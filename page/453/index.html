<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta name="generator" content="Hugo 0.133.1">
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="编程大咖的博客">
		<meta property="og:url" content="https://bcdaka.github.io/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="编程大咖">
  <meta property="og:description" content="编程大咖的博客">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="website">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	
	<link rel="alternate" type="application/rss+xml" href="/index.xml" title="编程大咖">

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main list" role="main">
	<article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/2674a78e7d18d1896926f495ef94cee7/" rel="bookmark">
			mac上添加路由配置
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		前言 特殊场景下，需要添加网络的路由
操作 在 Mac 上,可以使用 route 命令来添加静态路由。命令格式如下:
sudo route add -net [目标网络] -netmask [子网掩码] -interface [网络接口] -gateway [网关IP] 例如在linux服务器上 添加 路由配置
ip route add 10.233.12.0/18 via 192.168.10.100 在 Mac 上的等价命令为:
sudo route add -net 10.233.12.0 -netmask 255.255.192.0 -interface [网络接口] -gateway 192.168.10.100 其中 [网络接口] 需要替换为你的实际网络接口名称,可以使用 networksetup -listallhardwareports 命令查看。通常情况下,有线网络接口名为 en0,无线网络接口名为 en1。
例如,如果你要通过有线网络接口 en0 添加上述两条路由,命令应该是:
sudo route add -net 10.233.12.0 -netmask 255.255.192.0 -interface en0 -gateway 192.168.10.100 添加完成后,你可以使用 netstat -rn 命令查看路由表,确认路由已经添加成功。
需要注意的是,这些路由是临时的,重启系统后会丢失。如果你需要永久保存路由,可以将这些命令添加到 /etc/rc.
	</div>
	<div class="list__footer clearfix">
		<a class="list__footer-readmore btn" href="/posts/2674a78e7d18d1896926f495ef94cee7/">Read more…</a>
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/ac3c0d738d9b619e869d9f67cf24df36/" rel="bookmark">
			rabbitMq的status报错Error: unable to perform an operation on node ‘rabbit……
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		问题可能是由于 RabbitMQ CLI 工具的 Erlang Cookie 与服务器上的不匹配而导致连接问题。Erlang Cookie 在 RabbitMQ 节点之间进行身份验证和安全通信时起着重要作用。
看了网上很多帖子，最后我的原因是因为c盘中两个.erlang.cookie文件中的内容不一致导致的。大部分的人都是这个原因，可以先查看下是否为和我相同的原因，节省时间，可以在c盘搜索一下看下两个.erlang.cookie文件中的内容是否一致，不一致的话就改成一致的就可以了。
有问题可以在评论区找我
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/4ce81754aab06e632300047d800d78d4/" rel="bookmark">
			Linux 安装 kafka
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		文章目录 前言一、Kafka简介kafka核心概念 二、安装Kafka1.准备工作1.1 Java1.2 安装包下载 2.安装KafKa1. 解压安装包2. 配置kafka3 进入配置文件目录4 修改配置文件server.properties，添加下面内容5 配置zookeeper服务 zookeeper.properties6 创建启动和关闭的 kafka 执行脚本6.1 创建启动脚本6.2 创建关闭脚本 kafkaStop.sh 7 启动脚本，关闭脚本赋予权限8 创建生产者 topic 和 消费者 topic9 Spring boot集成Kafka 前言 例如：随着人工智能的不断发展，机器学习这门技术也越来越重要，很多人都开启了学习机器学习，本文就介绍了机器学习的基础内容。
提示：以下是本篇文章正文内容，下面案例可供参考
一、Kafka简介 Kafka是由Apache软件基金会开发的一个开源流处理平台，由Scala和Java编写。Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。 这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。 这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。 对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消息。
kafka核心概念 在深入了解 Kafka 的使用教程之前，让我们先介绍一些 Kafka 的核心概念，这些概念是理解 Kafka 的基础：
Broker： Kafka 集群中的每个服务器节点称为 Broker，它们负责存储和处理数据。
Topic： 消息发布的主题，是数据流的类别。生产者将消息发布到主题，消费者从主题中订阅消息。
Partition： 每个 Topic 可以分成多个 Partition，每个 Partition 是一个有序的消息队列。分区允许数据水平分布和并行处理。
Producer： 数据的发布者，将消息发送到一个或多个 Topic。
Consumer： 数据的订阅者，从一个或多个 Topic 中消费消息。
Consumer Group： 一组消费者的集合，共同消费一个 Topic 的消息。每个分区只能由一个消费者组中的一个消费者消费。
Offset： 每个消息在 Partition 中的唯一标识，消费者使用 Offset 来追踪已消费的消息。
	</div>
	<div class="list__footer clearfix">
		<a class="list__footer-readmore btn" href="/posts/4ce81754aab06e632300047d800d78d4/">Read more…</a>
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/6a202dc647655f5c8de309fbc9fe02aa/" rel="bookmark">
			uniapp小程序路由跳转，使用uni.navigateBack方法，实现页面返回上一页
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		在小程序中uni.navigateTo和uni.redirectTo是通过路由拼接的方式传参
uni.navigateTo({ url: 'test?id=1&amp;name=uniapp' }); uni.redirectTo({ url: 'test?id=1' }); uni.navigateBack() 是一个用于在uni-app中进行页面后退操作的API。它可以让你返回到上一个页面。在给定的示例中，uni.navigateBack() 被调用并传入了一个对象作为参数。该对象具有一个属性 delta，用于指定要后退的页面层数。通过设置 delta: 1，表示后退一层，即返回到上一个页面。
uni.navigateBack通过delta控制返回几层
uni.navigateBack({ delta: 2 }); 请注意，delta 的值必须是一个正整数，且不能超过当前页面栈的层数。如果 delta 的值超过了页面栈的层数，后退操作将不会生效。
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/4ad3a6f586900ed2a132761cba34bc38/" rel="bookmark">
			Nginx转发代理请求（http）&#43;转发mysql（stream）
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		1.Nginx能处理的不同类型模块 1.1 http 模块 http 模块用于处理 HTTP 和 HTTPS 协议的请求和响应，通常用于构建 Web 服务器和反向代理服务器。通过 http 模块配置的服务可以处理 Web 浏览器发起的 HTTP 请求，并向客户端提供 Web 页面、静态资源、API 接口等内容。在 http 块内部可以配置各种 HTTP 相关的指令，如监听端口、设置服务器名称、定义虚拟主机、配置请求处理、设置缓存、负载均衡等。 1.2 stream 模块 stream 模块用于处理 TCP 和 UDP 等传输层协议的请求和响应，通常用于构建网络代理、负载均衡器、TCP/UDP 代理等。通过 stream 模块配置的服务可以在传输层上转发请求和响应数据，而不需要解析应用层的协议内容。在 stream 块内部可以配置 TCP 或 UDP 代理、负载均衡、健康检查等。常用于处理数据库连接、邮件传输等。 1.3 mail 模块 mail 模块用于处理邮件服务相关的流量，例如 SMTP、POP3、IMAP 等邮件协议。通过 mail 模块配置的服务可以用作邮件代理、反向代理等。 1.4 upstream 模块： upstream 模块用于配置后端服务器集群，实现负载均衡和故障转移。通过 upstream 模块配置的服务器集群可以用于 HTTP、stream 或 mail 模块。 1.5 security 模块： security 模块提供了一些安全相关的功能，如防止恶意请求、拒绝服务攻击防护等。 1.6 limit_conn 模块： limit_conn 模块用于限制客户端的并发连接数。 1.
	</div>
	<div class="list__footer clearfix">
		<a class="list__footer-readmore btn" href="/posts/4ad3a6f586900ed2a132761cba34bc38/">Read more…</a>
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/2f0c379cbe261d9a415b0e898cb45378/" rel="bookmark">
			python pyinstaller打包常见问题（二）：No file找不到文件解决办法
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		项目场景： 我写了python程序，本地环境能正常运行，我打算打包成exe文件
方便发给朋友，让没有python环境也能正常运行
程序调用了wav文件，一个音效资源文件，程序调用的路径如下：
file = "猫咪吃东西.wav" 问题描述： 程序制作完成后，我开始打包
在需要打包的程序的目录上，我进入终端输入以下命令进行打包:
pyinstaller -cF xxx.py --add-data="猫咪吃东西.wav;猫咪吃东西.wav" -n="xxx.exe" 完成打包后，我在dist目录中找到生成的exe
运行失败直接闪退，我怀疑是缺少文件导致报错
为了捕捉原因，运行程序添加如下代码：
再次重复上述步骤进行打包，运行生成exe文件报错如下：
原因分析： 根据上面截图，可以看到exe文件运行失败的原因
是在当前目录上运行程序找不到所需资源wav文件
找不到资源文件导致的报错，一般这种情况有两个原因：
第一：打包的命令有问题，没有将资源文件打包进去
第二：运行程序的读取资源文件有问题，路径问题或者是权限问题（可能性小）
解决方案： 根据上述两个原因，制定两个计划：
第一：再运行一次exe文件，查看生成的临时文件夹有没有资源文件
第二：查看临时目录的资源文件的路径，确认路径与程序读取文件路径对应
一、查看临时文件夹 运行exe后，不关闭运行程序窗口就会出现想要的临时文件夹
window系统运行的程序生成的缓存文件，一般都在临时文件Temp中
而pyinstaller打包生成的临时文件在Temp中文件夹里一般是_ME开头的文件，例如：
二、修改打包命令，并打包 根据图片可以发现程序所需资源文件和第三方库的存在
但是路径与与程序读取文件的路径不一样
我们想要的只是wav文件，却多了个目录
了解pyinstaller打包命令后，发现错误原因如下：
附加文件指定的路径，不需要写名称
修改打包命令，终端输入以下命令打包：
pyinstaller -cF xxx.py --add-data="猫咪吃东西.wav;." -n="xxx.exe" 打包生成exe的运行后，依然报错如下：
三、修改运行程序调用文件的路径 打包的exe文件读取资源时的路径需要采用绝对路径
在运行程序中我添加了一些输出，具体原因你可以看截图：
根据截图你会发现
如果你写的是相对路径，那么程序寻找的资源路径与正确路径不一致
运行exe所在的目录就是工作目录，他会在当前目录寻找资源文件
所以说读取文件路径需要先获取临时工作目录加上资源所在路径
修改运行程序的路径，代码如下：
# 获取目录路径对象 SRC_PATH = Path.absolute(Path(__file__)).parent # 吃食物声音路径： path = str(SRC_PATH / "猫咪吃东西.wav") # 拼接获得文件绝对路径字符串 修改完程序后，终端再次输入命令打包：
	</div>
	<div class="list__footer clearfix">
		<a class="list__footer-readmore btn" href="/posts/2f0c379cbe261d9a415b0e898cb45378/">Read more…</a>
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/457a784809b2591511b7f9cdadb49415/" rel="bookmark">
			Android Studio 报错AAPT: error: resource android:attr/lStar not found.解决方法
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		r\.gradle\caches\transforms-2\files-2.1\930c42acd29d295ce5bc495c3b84423e\core-1.9.0\res\values\values.xml:104:5-113:25: AAPT: error: resource android:attr/lStar not found.
报错提示了 core-1.9.0
这个问题一般是由于 androidx.core 版本不兼容所致
解决方法：
方法1：全局搜索项目 androidx.core:core 或者 androidx.core:core-ktx，若没有设置具体版本的修改为具体版本就好，
方法2：若是第三方库使用没有设置具体版本，在app的build.gradle中添加如下，强制项目使用具体的版本，解决问题
configurations.all {
resolutionStrategy {
//force 'androidx.core:core-ktx:1.6.0'
force 'androidx.core:core:1.6.0'
}
}
或者提高到对应的sdk版本
core-1.9.0 对应 compileSdkVersion 31 修改到此版本 就可以了
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/7213aa201ed0fe2be19198521527509a/" rel="bookmark">
			SpringBoot集成Nacos
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		1. 概述 Nacos是一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。Spring Boot是一个用于创建独立、生产级别的基于Spring的应用的框架。集成Nacos到SpringBoot中，可以方便地实现服务发现、配置管理和动态配置刷新等功能。
在集成之前，确保你已经安装了Nacos服务器，并且它正在运行。你可以在Nacos的官方GitHub仓库中找到安装和配置Nacos服务器的指南。
2. 添加依赖 在你的SpringBoot项目的pom.xml文件中，添加Nacos的依赖。这里以Maven为例：
&lt;dependencies&gt; &lt;!-- Spring Cloud Alibaba Nacos Config Starter --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;version&gt;你的版本&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spring Cloud Alibaba Nacos Discovery Starter --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;version&gt;你的版本&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 其他依赖 --&gt; &lt;/dependencies&gt; 请确保你使用的是与你的SpringBoot版本兼容的Spring Cloud Alibaba版本。
3. 配置Nacos 在bootstrap.properties或bootstrap.yml文件中配置Nacos的相关信息。bootstrap配置文件在SpringBoot应用中会优先于application配置文件加载。
spring: cloud: nacos: config: server-addr: 127.0.0.1:8848 # Nacos服务器地址 namespace: your-namespace-id # 命名空间ID（可选） group: DEFAULT_GROUP # 配置分组（可选） data-id: your-data-id # 数据ID，用于区分不同的配置文件 file-extension: yaml # 配置文件格式，默认为properties extension-configs[0]: data-id: example.
	</div>
	<div class="list__footer clearfix">
		<a class="list__footer-readmore btn" href="/posts/7213aa201ed0fe2be19198521527509a/">Read more…</a>
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/2f46a08a2641fa6d27c23463f7d6f9ab/" rel="bookmark">
			Redis第10讲——Redis数据分片的三种算法
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		前面提到的Redis集群和各大厂商的集群方案，或多或少都提到了一些算法，比如Redis Cluster的哈希槽分区（crc16算法）、Redis客户端分片的一致性哈希算法以及我们常见的哈希算法等，这些算法在集群方案中也处于举足轻重的地位，下面我们一起来看看。
一、Hash取模分片 我们先从简单、经典的hash取模算法说起。
假设Redis集群现在有3个节点，使用经典的hash取模算法进行数据分片，实际就是一个节点一个数据分片，分为了3片，这是一种非常简单的分片方式。
但这种算法存在一个很严重的问题，就是对扩/缩容很不友好，假设现在再加一个节点，变成4个节点：
我们可以看到，原来的分片路由算法是：hash(key)%3，现在的分片路由算法是：hash(key)%4
分片路由的变更就意味着大量的key需要进行数据迁移，按上述例子来说，使用hash取模算法的话，要有75%的数据需要进行迁移。
那该怎么解决呢，有两个方案：一个方案是，如果非要用这种算法，建议采用多倍扩容的方式，这样就只需要迁移50%的数据。另一个方案则是采用一致性hash分片的算法。
总结一下：
优点：
简单粗暴，直接有效。
缺点：
数据节点伸缩时，会导致大量数据迁移(最少50%数据要迁移，一般有80%）。
二、一致性Hash算法分片 2.1 什么是一致性Hash算法 一致性Hash算法在1997年由麻省理工学院提出，引入了一个0~43亿的整数哈希环（0~2^32），把节点的ip和端口及其它信息作为字符串的对象进行散列计算，目的是为了解决分布式缓存数据变动和映射问题，简单地说就是当服务器个数发生变化的时候，尽量减少影响客户端到服务端的映射关系。
2.2 key分配过程 key值是如何经过一致性hash算法计算后分配到对应的redis节点的呢？这里就需要采用一种特殊得结构：Hash槽位环，一致性哈希算法把2^32个slot槽位虚拟成一个圆环，该环首尾相连（0=2^32），如下图：
假设现在有4个redis节点，那么这4个节点如何确定自己在环上的位置的呢？
将各个服务器进行一个哈希，具体可以选择服务器的IP或主机名作为关键字进行哈希(hash(ip))，这样每台机器就能确定其在哈希环上的位置。
假设经过计算后这4个节点的位置如下，每个节点负责存储一个slot分段：
那么key是如何分配的各个节点的呢？key分配过程核心有两个阶段：
第一阶段：进行slot槽位计算，每个key进行hash运算，被hash后的结果与2^32取模，获得slot槽位。
第二阶段：在hash槽位环上，按顺时针去找到最近的redis节点，那么这个key将会被保存在这个节点上。
2.3 三个经典场景 2.3.1 Key入环 key入环即key上述key分配过程，不再赘述。
2.3.2 新增redis节点 假设现在需要对redis节点进行扩容，经过hash(ip)后，redis5落在了redis1和redis2之间，如下图：
添加了redis5之后，会对所有redis2上的数据，进行重新检查：
如果redis2上的数据，顺时针方向最近的新节点不是redis2而是redis5的话（也就是hash计算后落在了redis1和redis5之间的key），那么这些数据将会被迁移至redis5上。
而其它节点redis1、redis3和redis4上的数据不会受影响。
2.3.3 删除redis节点 假设把redis2节点剔除了，会发生什么？如下图：
那么redis2节点上的数据将会被迁移到redis3上（顺时针最近），redis1和redis4不受影响，这也是一致性哈希算法存在的最大问题：数据倾斜问题。
那么如何解决这个问题呢？虚拟节点。
2.4 虚拟节点 虚拟节点可以理解为逻辑节点，不是物理节点。假设在hash环上，引入了32个虚拟redis节点，如下图：
现在还是4个redis节点，那么这32个虚拟节点到4个redis物理节点做映射： 假设redis2节点被移除，那么把redis2负责的逻辑节点，二次分配到其它三个物理节点就行了，上述只是一个简单的一个虚拟节点的映射方案，无论如何，通过虚拟节点，就会大大减少了一致性哈希算法的数据倾斜问题。
三、Redis cluster数据分片（crc16 哈希算法） 看似一致性哈希算法已经够完美了，但Redis cluster的分片并没有采用。
3.1 Redis cluster分片 在Redis的Cluster集群模式中，使用了哈希槽（hash slot）的方式来进行数据分片，将整个数据集划分为16384个槽，每个节点负责部分槽。客户端访问数据时，先计算出数据对应的槽，然后直接连接到该槽所在的节点进行操作，如下图：
ps：上图把16384个槽均匀分配给了三个节点，当然，如果各个节点机器的性能不一样，也可以用【cluster addslots】命令为每个节点自定义分配槽的数量。
在Redis的每个节点上，都有这么两个东西：
槽(slot)：它的取值范围是0~16384（2^14)。
cluster：可以理解为是一个集群管理的插件。
当我们存取key的时候，Redis会根据CRC16算法得出一个结果，然后把结果对16384取模，这样就得到了一个在0~16384范围之间的哈希槽，通过这个值，去找到对应负责该槽的节点，然后就可以进行存取操作了。
3.2 Redis节点的增加和删除 无论是增加还是删除节点，redis cluster都会让数据尽可能的均匀分布。比如，现在有三个节点：Redis1[0,5460]，Redis2[5461,10922]，Redis3[10924,16383]。
这时增加了一台Redis4，那么cluster就会从1、2、3的数据会迁移一部分到节点4上，实现4个节点数据均匀，这时每个节点的负责16384/4 = 4096个槽。减少节点也同理，假设删除Redis，那么Redis4节点上数据也会均匀地迁移到1、2、3，删除后，现在每个节点负责的槽位是：16384/3=6128。 3.
	</div>
	<div class="list__footer clearfix">
		<a class="list__footer-readmore btn" href="/posts/2f46a08a2641fa6d27c23463f7d6f9ab/">Read more…</a>
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/fd65f3948c41b99cef0ea7f89a9d481d/" rel="bookmark">
			电商人必备工具！5款免费AI商品图背景生成网站推荐
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		​电商行业竞争日渐激烈，如何让商品在众多竞争对手中脱颖而出，制作精美的主图是不可或缺的关键，在AI绘画如火如荼发展的今天，AI商品图是提升产品竞争力不可或缺的助力，今天这篇文章就给大家推荐5款免费的AI商品图生成工具，一起来看看吧。
一、易可图AI商品图
易可图是一家专门服务于电商领域的专业图片编辑工具网站，例如一键抠图、图片变清晰、AI商品图等常用的图片编辑功能一应俱全，最重要的是完全免费。
1.进入易可图官网，点击左侧功能列表的易可图AI，点击AI商品图功能
2.需要注意的是，易可图AI商品图功能每天每个账号有20次的免费生成点数，次日会刷新点数，也就是说多注册几个账号就可以无限次数的使用，非常的不错。
3.上传一张商品图，可以是扣好的图也可以是照片，如果传的是照片在上传的时候会自动将你的商品图抠出来，非常的省事，点击下方的推荐场景，然后点击立即生成，等待一会后，即可生成一张AI商品图。
4.并且这个网站的所有图片都是可以免费下载的，不用担心生成图片后下载需要收费这个问题，并且一次支持生成最多4张AI商品图，强烈推荐。
二、PIXELCUT
PIXELCUT是一款全能的AI图片编辑器，无论是想制作产品图、图像重新上色还是进行批量编辑，都可以了解下PIXELCUT。
使用PIXELCUT的AI生成产品图操作步骤简单，可选择固定模板或者自订背景生成产品图，图片生成后可直接点击下载按钮，也可点击编辑按钮跳转编辑页面，对图片进行二次编辑。
PS：自定义商品背景属于收费功能，所以只需要使用预设背景就可以直接白嫖
1.进入PIXELCUT官网，点击try plxelcut按钮
2.根据需求选择风格模板，上传图片，即可直接替换掉它的预设商品，换成你自己的，然后点击下载即可。
三、PhotoRoom
PhotoRoom的AI背景合成功能不支持自定输入关键词生成背景，但是其提供了各式各样的背景模板，你可以轻松选择合适的背景来更换图片背景。选择背景模板后会立即运用到你的产品图上，实时预览效果。
1.PhotoRoom官网，选择Instant Backgrounds功能
2.上传图片，PhotoRoom可以根据热点内容制作出产品预览图，可直接看到效果并点击应用到产品图中
3.除了热门推荐，还有大量固定模板可供选择
4.点击符合自己需求的模板生成背景图之后，可在右边编辑栏对图片背景进行适量调整，满意之后点击下载即可
四、PromeAI
PromeAI是一款借助先进AI算法的图像处理工具，它的AI商品图功能无需进行繁琐的手动操作，只需要输入所需的参数和指示，即可生成高质量的产品图。无论是从事设计、电商还是广告行业，这个功能都能给你带来很大的帮助。
1.访问PromeAI官网，选择AI背景合成功能
2.上传图片之后在图片下方选择符合你产品特色的背景模板，以及需要的尺寸之后点击立即生成
3.除了预设模板，你也可以点击自定义按钮，输入关键词生成背景图
4.每次生成三张产品图，生成之后可对产品图进行适当编辑，满意之后点击下载按钮即可
五、Pebblely
Pebblely是一款基于AI的产品创意图生成工具，Pebblely可以快速创作极具特色的产品创意图，帮助你在电商等方面进行多样化推广，不过每月只能免费制作40张场景图，聊胜于无吧。
1.进入Pebblely官网，上传图片
2.图片上传之后点击Save asset按钮跳转添加背景图页面（可以根据个人需求修改产品名称）
3.产品主题大小可根据需求进行调节，选择背景模板，点击生成按钮即可
4.每次生成4张图片，点击符合您需求的产品图，可对其进行编辑或下载
AI背景合成工具的出现，让我们进入了一个更加高效和便利的图像处理时代，通过 AI生成背景图，我们能够轻松实现图像的背景去除和替换，为图片注入新的创意和美感。让我们一起拥抱这个AI技术，让它成为你创作旅程中的得力助手吧！
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/065c22c736ee4e8e86947b3de7d0cea3/" rel="bookmark">
			【数据库管理操作】Mysql 创建学生数据库及对数据表进行修改
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		MySQL 创建学生成绩数据库 1.创建数据库 create database studentscore; 创建完成之后，如果需要使用该数据，使用use命令
use studentscore; 创建表前查看当前数据库中包含的表
show tables;	2.创建bclass表 create table bclass( class_id char(8) primary key, class_name varchar(20) not null unique, class_num int(6) default(0), major_id char(2), length char(1), depart_char char(2)); 3.创建bstudent表 create table bstudent( stud_id char(10) primary key, stud_name varchar(50) not null, stud_sex Enum('男','女'), birth Date default '1900-01-01', members varchar(40), family_place varchar(50), class_id char(8), constraint Fk_classid foreign key(class_id) references bclass(class_id) ); 注:上面代码中，通过外键约束，在bstudent表的班级代号列class_id上建立了与班级信息表bclass班级代号class_id的关联关系。
4.创建bcourse表 create table bcourse( course_id char(8) primary key, course_name varchar(40) not null, course_type Enum ('必修','选修'), hours int(6), credit int(4) ); 5.
	</div>
	<div class="list__footer clearfix">
		<a class="list__footer-readmore btn" href="/posts/065c22c736ee4e8e86947b3de7d0cea3/">Read more…</a>
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/d6fe6101feff22024013cb9dee6418fe/" rel="bookmark">
			stable diffusion 的 GPU 不足怎么解决
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		稳定扩散（stable diffusion）是一种用于图像处理和计算机视觉任务的图像滤波算法。
当使用Stable Diffusion过程中遇到GPU显示内存不足的问题时。解决这个问题的方法有以下几种：
目前，对我来说，就最后一点能够暂时解决当前的困境了
1. 降低图像分辨率 通过降低图像的分辨率，可以减少GPU的计算负载。这可以通过缩小图像尺寸或者使用图像金字塔等技术来实现。
2. 并行计算 利用GPU的并行计算能力，可以将图像分成多个块，并同时在多个GPU核心上进行计算。这样可以提高计算效率，减少GPU负载。
3. 优化算法 对稳定扩散算法进行优化，减少计算量和内存占用。例如，可以使用近似算法或者采样技术来减少计算量，或者使用稀疏矩阵等数据结构来减少内存占用。
4. 使用更高性能的GPU 如果GPU不足以处理大规模图像，可以考虑使用更高性能的GPU。例如，使用具有更多CUDA核心或更大显存的GPU。
5. 分布式计算 如果单个GPU无法满足需求，可以考虑使用多个GPU进行分布式计算。这可以通过使用GPU集群或者云计算平台来实现。
7.任务管理器 通过任务管理器查看GPU内存占用情况，并尝试关闭占用高的应用程序，以释放GPU内存。
8.去掉某些参数 调整Stable Diffusion的参数，可能需要去掉某些参数以减少显存占用。
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/2bf8dcb96ebb8b3c85bc0abb54bd2d5b/" rel="bookmark">
			把本地文件上传到HDFS上操作步骤
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		因为条件有限，我这里以虚拟机centos为例
实验条件：我在虚拟机上创建了三台节点，部署了hadoop，把笔记本上的数据上传到hdfs中
数据打包上传到虚拟机节点上
采用的是rz命令，可以帮我们上传数据
没有的话可以使用命令安装yum install lrzsz [root@node04 0007]# rz 解压
没有解压命令可以安装yum install unzip [root@node04 data]# unzip 0007.zip 启动hads服务 [root@node04 data]# start-dfs.sh 查看hdfs下的目录结构 [root@node04 data]# hadoop dfs -ls / 在hdfs上为我们的数据创建目录 hadoop dfs -mkdir /data 把数据放到hdfs中 hadoop dfs -put /opt/data/0007/ /data/ 查看结果
存放成功
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/26694beecaea7c0861496d202f3ba050/" rel="bookmark">
			ChatGPT写论文（疯狂版) -- 附正确使用方式
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		我是娜姐 @迪娜学姐 ，一个SCI医学期刊编辑，探索用AI工具提效论文写作和发表。
自2022年底，ChatGPT发布以来，被论文长期折磨的科研搬砖人，立马发现了这个强大工具，用的不亦乐乎。
美国田纳西大学一名放射科医生，Som Biswas，用ChatGPT一口气写了16篇论文，已发表了4篇。
我在PubMed查了一下，截止目前，Biswas已发表论文增加到7篇，6篇都是Q1-Q2区的，影响因子最高的是Q1区19.7分的Radiology。看一下这位大神的战绩： 不过，他发表的主题都是ChatGPT在研究领域的应用，都是Comment或Letter之类的论文。那ChatGPT能不能帮我们写学术类的综述或研究性论文呢？
答案是肯定的。以至于，这些作者们对着ChatGPT输出的内容，复制粘贴太爽了，在已上线的论文中，堂而皇之出现了一些跟论文内容无关的“AI马脚”。
比如最近热搜的这篇↓：
通讯作者是中国地质大学的刘教授。Introduction第一句“Certainly, here is a possible introduction for your topic”是标准的ChatGPT式回复。离谱的是，这句跟文章无关的话出现在引言开头啊，难道这篇文章通讯没看，Editor没看，审稿人没看，编辑部也没看？？ 这个期刊影响影子6.2分，Q1区。
再看这篇↓：
“作为一个人工智能语言模型，我没办法获得实时信息或患者的具体数据。。。”这个出现在Discussion最后一段。作者有点忘乎所以。
再看这篇↓：
“Regenerate response”重新生成，用过ChatGPT的人都知道，这是复制粘贴的时候多不小心啊。
好几起乌龙事件都发生在Elsevier旗下期刊上，这下出版商也坐不住了，不得不出面澄清： 各大出版商和期刊对于LLM的政策：
之前，娜姐写过一篇关于TOP100期刊和出版社关于AI的政策：
如何合理使用AI写论文？来看top 100学术期刊和出版社的最新调查结果
主要观点是，AI可以用于构思论文，启发观点，编辑润色等，但是，不能用于代替关键的写作任务，比如给出科学结论、提供临床建议等。此外，不能将AI列为作者之一。
还有，作者需要在论文的特定部分披露LLM的使用情况。
很多期刊在投稿系统中增加了一个环节，要求确认在论文写作过程中是否使用了AI。
目前市面上也有不少AI检测工具。但是，说实话，这些个大模型就是用自然语言训练出来的，如果提示语用的合适，其实是很难辨别出AI生成的内容和人写的内容。所以，大部分作者并没有披露AI的使用，期刊也无从查起。 网友笑称，学术界已经被ChatGPT攻陷了。这些已经发表的论文中露出的“AI马脚”只是冰山一角，更多的是大家心照不宣、悄摸的用ChatGPT来加速发论文。
可以预见，未来就业或评职称，需要的论文数量和IF点数，被这些用AI大模型来疯狂发论文的人卷上一个新高度。
学术论文作为一个载体，它的主要作用是交流学术发现和成果。其中作者的新发现、新成果才是其中最有价值的部分。
写论文对于每一个科研人，不论是新手还是发了几十篇的老手，都是一件劳心劳力的事。写这种固定格式的八股文，你需要“综述”一堆已知的文献结论，还要总结的全面，最新的、最经典的都不能漏。现在大家都在卷论文，每天都有层出不穷的新论文发表，光这一项，就让人薅掉不少头发；方法和结果部分相对好写，讨论部分有的结合已知文献结论，对你的结果逐一展开合理解释和延伸讨论，Discussion部分又难倒一大批人。
ChatGPT经过1700多亿大数据的训练，上知天文下知地理，对于你想要表达的观点和已知文献结论，它能迅速帮你理清逻辑，真的是解放广大科研党的“论文神器”。
但是，作为论文的主要负责人，你需要对AI生成的内容把关，逐字阅读核对。
如果你也想尝试用ChatGPT来写论文，但是，总是不得其法，它在你手里像个智障。欢迎你来学习SCI期刊编辑娜姐的课程“ChatGPT辅助论文写作到发表”，课程设计是针对论文写作的一整个工作流，每节课都有对应的提示语，用AI帮你写出top级别的论文。经过50+学员检验，好评如潮。 “光是润色一项就值回票价”
“它给我改的逻辑清晰，语法流畅”
“请北京的主任改一篇大几千，也就那样，还是会有错误。ChatGPT改的太完美了”
“导师看了我的论文，表扬我说，没想到你英语很不错嘛”
“之前我也用过流行的提示语，改的乱七八糟，不符合学术风格。娜姐的提示语改的太地道了”
写论文这件事，需要集中火力短平快。因为写作需要你大脑飞速运转，每次写作，进入状态就需要一会，还没开始写多少，又被打断，下一次再接上又需要分析一下之前的内容。这样下去，可能3-5个月还没啥进展。再而衰三而竭，最后干脆丢一边，不想写了。
如果你已经准备好了数据，在ChatGPT的加持下，集中几天的时间，快速分析、输出、润色搞定，投稿。
还是那句话，未来的世界是善用AI的人淘汰不用或者不会用的。这样的世界已经加速在实现，你要做的是赶紧跳上车，跟上大部队。 往期干货精选：
ChatGPT助力论文写作：
我真的建议大家，寒假期间完成一篇SCI论文
Discussion讨论部分被3个审稿人说没深度没逻辑，用这个AI工具三步拯救了我！
ChatGPT辅助论文文献综述，再加这个AI工具找论据，绝了！
如何借助Al工具高效精准回复审稿意见
论文投稿选刊，这样操作accept速度大大提升
快速把重复率从58%降到5%，这论文降重很给力!
ChatGPT助力论文选题，这样操作效果太棒了！
论文文献综述，这几款热门GPTs你用过没？
关于GPTs的基础知识和我创建的学术GPTs应用分享送
SCI论文写作与发表：
SCI论文审稿：有拒稿或大修的审稿意见，该如何修改才能顺利接收？
SCl论文发表:回复审稿意见的正确姿势及避坑指南
关于伦理批准，这些问题可能导致论文被拒/被撤稿。一文讲清楚正确操作流程和写法（建议收藏）
SCI论文审稿：审稿周期太长，该如何有效催稿又不招致reject？
SCI论文发表：校稿proof--如何正确使用论文正式发表前的最后一次修改机会？
SCl论文摘要Abstract部分的写作技巧及避坑指南
投稿中这些增加好感和败好感的细节，get就能大大提升投稿命中率
SCI论文图片Figure处理要求及避坑指南5000字原创干货分享
	</div>
	<div class="list__footer clearfix">
		<a class="list__footer-readmore btn" href="/posts/26694beecaea7c0861496d202f3ba050/">Read more…</a>
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/9016e3b36f5056e6f1acd4a3af79730b/" rel="bookmark">
			Android APK反编译就这么简单 详解（附图），2024年Android开发实战
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		dex2jar
作用：将apk反编译成Java源码（classes.dex转化成jar文件）
jd-gui
作用：查看APK中classes.dex转化成出的jar文件，即源码文件
反编译流程：
一、apk反编译得到程序的源代码、图片、XML配置、语言资源等文件
下载上述工具中的apktool，解压得到3个文件：aapt.exe，apktool.bat，apktool.jar ，将需要反编译的APK文件放到该目录下，
打开命令行界面（运行-CMD） ，定位到apktool文件夹，输入以下命令：apktool.bat d -f test.apk test
**
**
（命令中test.apk指的是要反编译的APK文件全名，test为反编译后资源文件存放的目录名称，即为：apktool.bat d -f [apk文件 ] [输出文件夹]）
说明获取成功，之后发现在文件夹下多了个test文件，点击便可以查看该应用的所有资源文件了。
如果你想将反编译完的文件重新打包成apk，那你可以：输入apktool.bat b test（你编译出来文件夹）便可，效果如下：
之后在之前的test文件下便可以发现多了2个文件夹:
build
dist(里面存放着打包出来的APK文件)
二、Apk反编译得到Java源代码
下载上述工具中的dex2jar和jd-gui ，解压
将要反编译的APK后缀名改为.rar或则 .zip，并解压，得到其中的额classes.dex文件（它就是java文件编译再通过dx工具打包而成的），将获取到的classes.dex放到之前解压出来的工具dex2jar-0.0.9.15 文件夹内，
在命令行下定位到dex2jar.bat所在目录，输入dex2jar.bat classes.dex，效果如下：
在改目录下会生成一个classes_dex2jar.jar的文件，然后打开工具jd-gui文件夹里的jd-gui.exe，之后用该工具打开之前生成的classes_dex2jar.jar文件，便可以看到源码了，效果如下：
被混淆过的效果图（类文件名称以及里面的方法名称都会以a,b,c…之类的样式命名）：
三、 图形化反编译apk（本人未使用过）
上述步骤一、二讲述了命令行反编译apk，现在提供一种图形化反编译工具：Androidfby
自我介绍一下，小编13年上海交大毕业，曾经在小公司待过，也去过华为、OPPO等大厂，18年进入阿里一直到现在。
深知大多数Android工程师，想要提升技能，往往是自己摸索成长或者是报班学习，但对于培训机构动则几千的学费，着实压力不小。自己不成体系的自学效果低效又漫长，而且极易碰到天花板技术停滞不前！
因此收集整理了一份《2024年Android移动开发全套学习资料》，初衷也很简单，就是希望能够帮助到想自学提升又不知道该从何学起的朋友，同时减轻大家的负担。
既有适合小白学习的零基础资料，也有适合3年以上经验的小伙伴深入学习提升的进阶课程，基本涵盖了95%以上Android开发知识点，真正体系化！
由于文件比较大，这里只是将部分目录大纲截图出来，每个节点里面都包含大厂面经、学习笔记、源码讲义、实战项目、讲解视频，并且后续会持续更新
如果你觉得这些内容对你有帮助，可以添加V：vip204888 备注Android获取（资料价值较高，非无偿）
结尾 我还总结出了互联网公司Android程序员面试涉及到的绝大部分面试题及答案，并整理做成了文档，以及系统的进阶学习视频资料分享给大家。
（包括Java在Android开发中应用、APP框架知识体系、高级UI、全方位性能调优，NDK开发，音视频技术，人工智能技术，跨平台技术等技术资料），希望能帮助到你面试前的复习，且找到一个好的工作，也节省大家在网上搜索资料的时间来学习。
开发中应用、APP框架知识体系、高级UI、全方位性能调优，NDK开发，音视频技术，人工智能技术，跨平台技术等技术资料），希望能帮助到你面试前的复习，且找到一个好的工作，也节省大家在网上搜索资料的时间来学习。**
[外链图片转存中…(img-X7ECS6eV-1711536357497)]
本文已被CODING开源项目：《Android学习笔记总结+移动架构视频+大厂面试真题+项目实战源码》收录
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/adc7ee28f0f147942d9b84b2ca539087/" rel="bookmark">
			Nginx限制上传（下载）大小限制
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		“ Nginx 基本 启停命令 可点击本链接查看 ”
一、client_max_body_size Nginx 限制文件大小可以通过 client_max_body_size 指令来设置，该指令通常在 http、server 或
location 块中设置，如果不设置，默认上传大小为1M。
1.在 http 块中全局设置：
http { client_max_body_size 10M; ... } 2.在 server 块中设置：
server { client_max_body_size 10M; ... } 3.在 location 块中设置：
location /upload { client_max_body_size 10M; ... } 二、client_body_buffer_size 在Nginx中，client_body_buffer_size是一个用于设置客户端请求体缓冲区大小的指令。它的作用是控制Nginx服务器接收客户端请求体的缓冲区大小。
当客户端发送请求数据时（例如POST请求中的表单数据、文件上传等），Nginx会将这些数据存储在内存中的缓冲区中，然后处理该数据。client_body_buffer_size指令用于设置这个缓冲区的大小。
如果设置的client_body_buffer_size小于实际的请求体数据大小，Nginx会返回一个"Request Entity Too Large"（413）错误。因此，适当地设置client_body_buffer_size是很重要的，它可以避免由于请求体过大而导致服务器性能下降或者内存溢出的问题。
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/8b1a3fa64be8876e0eb48dc6130f49cf/" rel="bookmark">
			数据结构——栈(C语言版)
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		前言：
在学习完数据结构顺序表和链表之后，其实我们就可以做很多事情了，后面的栈和队列，其实就是对前面的顺序表和链表的灵活运用，今天我们就来学习一下栈的原理和应用。
准备工作：本人习惯将文件放在test.c、SeqList.c、SeqList.h三个文件中来实现，其中test.c用来放主函数，SeqList.c用来放调用的函数，SeqList.h用来放头文件和函数声明
目录
什么是队列？
栈的节点结构
栈的基本操作
1、初始化
2、销毁
3、插入元素
4、判断栈顶元素是否为空
5、删除元素
6、返回栈顶元素
7、栈中元素个数
完整的栈实例
总结
什么是队列？ 队列中的数据是按照先进后出的顺序的，也就是说先进去的数字后出来
因为栈的这种性质，所以栈我们用顺序表来实现比链表方便很多，顺序表就可以实现尾插尾出，所以我们一般就采用顺序表来实现
栈的节点结构 队列采用的顺序表的结构，所以与顺序表差异不大
typedef int STDataType; typedef struct stack { STDataType* a; int top; //指向栈元素下一位 int capacity; }ST; 栈的结构很简单，定义一个整形指针，一个表示容量和一个表示尾部元素的整形变量即可
栈的基本操作 //初始化 void STInit(ST* pst); //销毁 void STDestroy(ST* pst); //插入元素 void STPush(ST* pst, STDataType x); //删除元素 void STPop(ST* pst); //判断栈顶元素是否为空 bool STEmpty(ST* pst); //找栈顶元素 STDataType STTop(ST* pst); //栈中元素个数 STDataType STSize(ST* pst); 看上面的函数声明部分我们就可以看到我们每一步要实现的内容，接下来，我们就来一步一步进行实现
	</div>
	<div class="list__footer clearfix">
		<a class="list__footer-readmore btn" href="/posts/8b1a3fa64be8876e0eb48dc6130f49cf/">Read more…</a>
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/4db53059a011dc1b65990df353263e0e/" rel="bookmark">
			Docker 搭建私人仓库
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		docker 搭建私人仓库有下面几种方式：
1、docker hub 官方私人镜像仓库2、本地私有仓库 官方私人镜像仓库搭建很简单(就是需要有魔法，否则就异步到第二种方法吧)，只需要 login、pull、tag、push 几种命令就完事了。而本地私人镜像仓库则比较麻烦一点而且只能存储在本地不能公开。
下面我们就来简单的介绍一下怎么搭建自己的私人仓库吧！
官方镜像仓库 首先，我们需要在 docker hub 这里申请注册一个账号并登录，同时也要在 Docker Desktop 中进行登录。完成登录以后，我们首先在命令行中使用命令进行登录
docker login 出现 Login Succeeded 就代表登录成功了！
注：你的身份验证凭证将被存储在本地目录的 .dockercfg 文件中
接着我们查看一下本地镜像，可以看到这里有一个 nginx，待会儿就拿这个进行测试吧！
docker images 首先对镜像打上 tag，也就是重命名。
注：重命名格式需要 用户名/镜像名:版本号
docker tag nginx codegetters/nginx:1.0.0 检查一下看看是否成功打上了 tag
到了这里我们就完成了大部分的工作了，只需要 push到自己的账号中就好了。
注：push 的镜像名要写全包括最后的版本号，否则就会失败
docker push codegetters/nginx:1.0.0 完成以后我们就可以删除本地镜像了，不过删除不是我们的重点就不过多展示了。我们接着进入 docker hub 并登录自己的账号。点击 Repositories 就可以看到我们的仓库中已经有了 nginx 了
到了这里我们就算完成了，如果想要从自己的仓库中拉下来就执行下面的命令即可
docker pull codegetters/nginx:1.0.0 本地私有仓库 为什么使用私用仓库？
公司的项目一般不予许我们上传到 Docker Hub 这类的公共仓库中，所有学会创建一个私有仓库也是非常必要的。虽然 hub.docker 上可以保存镜像，但是网速相对较慢，在内部环境中搭建一个私有的公共仓库是个更好的方案。
官方提供了 docker hub 来作为一个公开的集中仓库。然而，本地访问 Docker Hub 速度往往很慢，并且很多时候需要一个本地的私有仓库只供网内使用。
	</div>
	<div class="list__footer clearfix">
		<a class="list__footer-readmore btn" href="/posts/4db53059a011dc1b65990df353263e0e/">Read more…</a>
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/45ec1e2db3566cb089e0b3494c59542c/" rel="bookmark">
			GraalVM与OpenJDK:一场Java技术的较量
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		引言 在Java生态中,OpenJDK长期以来作为Java开发工具包的主流选择,而GraalVM则是一种新兴的高性能多语言虚拟机。本文将探讨这两者在不同方面的优劣对比,为开发者提供选择的参考。
1. 背景介绍 1.1 OpenJDK OpenJDK是开源的Java平台参考实现,由Oracle和Java社区共同维护。作为Java SE的官方实现,OpenJDK自诞生以来已发展多年,拥有庞大的社区支持和生态体系。
1.2 GraalVM GraalVM是一个高性能的多语言虚拟机,支持Java、JavaScript、Python等多种语言。它提供了先进的即时编译器(JIT)和静态提前编译器(AOT),旨在提高应用程序的性能和效率。
2. 性能比较 2.1 启动时间和响应速度 OpenJDK采用传统JIT编译模式,更适用于长时间运行的应用;而GraalVM支持AOT编译,可显著减少启动时间,提高短期任务的响应速度。
2.2 资源利用和吞吐量 OpenJDK拥有成熟的垃圾回收和优化策略,对大型应用更加友好;GraalVM在某些场景下的内存和CPU利用率更高,可提供更好的吞吐量。
3. 兼容性与生态系统 3.1 应用兼容性 作为Java标准实现,OpenJDK具有极高的应用兼容性;GraalVM力求与Java规范兼容,但在特定场景下可能存在细微差异。
3.2 社区支持与生态 OpenJDK拥有庞大的社区支持和广泛的生态体系;GraalVM社区正在快速发展,吸引了越来越多开发者的关注。
4. 适用场景 4.1 适合OpenJDK的场景 大型企业级应用,需要长期稳定运行已有大量基于OpenJDK构建的遗留系统 4.2 适合GraalVM的场景 需要快速启动和高效执行的微服务、云原生应用多语言混合编程环境对启动时间、内存利用率有苛刻要求的应用 5. 结论 虽然GraalVM在诸如启动时间、多语言支持等方面展现出优秀的性能,但OpenJDK作为Java生态中的中流砥柱,其稳定性和成熟的生态系统仍不可替代。两者的选择取决于具体的应用场景和需求。
6. 未来展望 随着GraalVM的不断发展和社区的壮大,预计未来Java生态将更加多元化,为开发者提供更多灵活的选择。OpenJDK和GraalVM或将在不同场景扮演不同的角色,共同推动Java生态的进步。
	</div>
</article><article class="list__item post">
	
	<header class="list__header">
		<h2 class="list__title post__title">
			<a href="/posts/d2fca639658358126650d9b1dc2d90df/" rel="bookmark">
			解决Python因卸载不彻底,产生卸载,安装时出现的0x80070643问题
			</a>
		</h2>
		
	</header>
	<div class="content list__excerpt post__content clearfix">
		Setup failed
One or more issues caused the setup to fail, Pease fx the issues ancthen retry setup, For more information see the log fle
0x80070643-安装时发生严重错误 卸载不彻底原因:
因发现电脑python文件有多个重复(因为不是第一次下载), 觉得占空间,就想全部删除,再重新下载。
删除过程中,使用了文件夹删除,Everything软件查找后的删除,还有一切能查到的有关python,pycharm文件的删除,总之很杂,不建议小伙伴们进行类似的操作。
想要有效且干净的删除,可以借鉴以下这位大神的文档Python怎么卸载，如何才能彻彻底底的卸载干净？_彻底卸载python-CSDN博客
在 我的电脑-应用-安装的应用 和 控制面板-程序-程序和功能 中,卸载时出现
因为还要再下载,怕影响正常安装,在网上找了很多种方法都没有解决卸载问题
就重新下载了python,想着如果解决安装问题,也就可以暂时忽略卸载问题
如果有哪位大神有如上解决卸载问题的方法,能否告诉我一下,感谢!
网上有很多安装时解决x80070643问题,试了几个,不行
1.以管理员权限运行cmd/命令行提示符
2.在cmd中,切换到Python安装包所在目录
3.然后键入python-3.12.2-amd64 /? 并回车 //注:"python-3.12.2-amd64"由你下载的文件名决定
4.弹出了一个简单解释命令行选项的窗口，记有”/passive"、"/quiet"、"/simple"、"/uninstall"、"/layout [directory]"、"/log [filename]“
有位网友同样是卸载不干净问题,他键入python-3.12.2-amd64 /uninstall并执行，等待进度条滚动结束，再重新运行python-3.12.2-amd64的安装包即可成功安装, 我却不行
就在官网上详细查看文档4. Using Python on Windows — Python 3.12.2 documentation
Installing Without UI All of the options available in the installer UI can also be specified from the command line, allowing scripted installers to replicate an installation on many machines without user interaction.
	</div>
	<div class="list__footer clearfix">
		<a class="list__footer-readmore btn" href="/posts/d2fca639658358126650d9b1dc2d90df/">Read more…</a>
	</div>
</article>
</main>

<div class="pagination">
	<a class="pagination__item pagination__item--prev btn" href="/page/452/">«</a>
	<span class="pagination__item pagination__item--current">453/621</span>
	<a class="pagination__item pagination__item--next btn" href="/page/454/">»</a>
</div>

			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>