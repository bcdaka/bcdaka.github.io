<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>通俗易懂的Stable Diffusion模型结构介绍 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/d3f3680278fa91c3fc4edba45651e172/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="通俗易懂的Stable Diffusion模型结构介绍">
  <meta property="og:description" content="目录
SD的发展历程
SD 模型的网络结构
ClipText 文本编码器
文本向量输入Unet
VAE模型
总结图
SD的发展历程 Stable Diffusion是一个的文本条件隐式扩散模型(text-conditioned latent diffusion model)，可以根据文字描述生成效果极好的图像。
2021年12月提出了隐式扩散模型（Latent Diffusion Models，LDMs）的text-to-image模型。这个研究使得用扩散模型进行文字生成图片任务可以在普通显卡上执行，并且耗时较短。为一年后现象级的稳定扩散（Stable Diffusion）诞生奠定了基础。
SD 模型的网络结构 主要包括三个部分： ClipText 文本编码器 ClipText 文本编码器：用于解析提示词的 Clip 模型
文本编码器负责将提示词转换成电脑可以识别的文本向量
Diffusion 扩散模型 Diffusion 扩散模型：用于生成图像的 U-Net 和 Scheduler
扩散模型负责根据文本向量生成图像
VAE 模型 VAE 模型：用于压缩和恢复的图像解码器
而图像编码器则用于将生成的图像信息进行解码，以生成最终的图像输出
简化网络结构图 详细网络结构图 ClipText 文本编码器 为了导入提示词，我们首先需要为文本创建数值表示形式。
为此，Stable Diffusion使用了一个名为CLIP的预训练Transformer模型。
CLIP的文本编码器会将文本描述转换为特征向量，该特征向量可用于与图像特征向量进行相似度比较。
因此，CLIP非常适合从文本描述中为图像创建有用的表征信息。输入的文本提示语首先会被分词（也就是基于一个很大的词汇库，将句子中的词语或短语转换为一个一个的token），然后被输入CLIP的文本编码器，从而为每个token（分词）产生一个768维（针对Stable Diffusion 1.x版本）或1024维（针对Stable Diffusion 2.x版本）的向量。
CLIP模型 CLIP模型是一个基于对比学习的多模态模型，主要包含Text Encoder和Image Encoder两个模型。
其中Text Encoder用来提取文本的特征，可以使用NLP中常用的text transformer模型作为Text Encoder；
而Image Encoder主要用来提取图像的特征，可以使用CNN/vision transformer模型（ResNet和ViT）作为Image Encoder。与此同时，他直接使用4亿个图片与标签文本对数据集进行训练，来学习图片与本文内容的对应关系。
文本向量输入Unet 文本提示词转换为向量后将被输入扩散模型，用于引导图像的生成，这里使用的扩散模型是Unet网络。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-28T19:39:09+08:00">
    <meta property="article:modified_time" content="2024-04-28T19:39:09+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">通俗易懂的Stable Diffusion模型结构介绍</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div style="margin-left:0in;"> 
 <div style="margin-left:0in;"> 
  <p id="main-toc"><strong>目录</strong></p> 
  <p id="SD%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B-toc" style="margin-left:0px;"><a href="#SD%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B" rel="nofollow">SD的发展历程</a></p> 
  <p id="SD%20%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-toc" style="margin-left:0px;"><a href="#SD%20%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84" rel="nofollow">SD 模型的网络结构</a></p> 
  <p id="ClipText%20%E6%96%87%E6%9C%AC%E7%BC%96%E7%A0%81%E5%99%A8-toc" style="margin-left:0px;"><a href="#ClipText%20%E6%96%87%E6%9C%AC%E7%BC%96%E7%A0%81%E5%99%A8" rel="nofollow">        </a><a href="#ClipText%20%E6%96%87%E6%9C%AC%E7%BC%96%E7%A0%81%E5%99%A8" rel="nofollow">ClipText 文本编码器</a></p> 
  <p id="%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%BE%93%E5%85%A5Unet-toc" style="margin-left:0px;"><a href="#%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%BE%93%E5%85%A5Unet" rel="nofollow">        </a><a href="#%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%BE%93%E5%85%A5Unet" rel="nofollow">文本向量输入Unet</a></p> 
  <p id="VAE%E6%A8%A1%E5%9E%8B-toc" style="margin-left:0px;"><a href="#VAE%E6%A8%A1%E5%9E%8B" rel="nofollow">        </a><a href="#VAE%E6%A8%A1%E5%9E%8B" rel="nofollow">VAE模型</a></p> 
  <p id="%E6%80%BB%E7%BB%93%E5%9B%BE-toc" style="margin-left:0px;"><a href="#%E6%80%BB%E7%BB%93%E5%9B%BE" rel="nofollow">总结图</a></p> 
  <hr> 
  <h2 id="SD%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B" style="margin-left:0px;"><strong>SD</strong><strong>的发展历程</strong></h2> 
  <p style="margin-left:0;"><strong>Stable Diffusion</strong>是一个的文本条件隐式扩散模型(text-conditioned latent diffusion model)，可以根据文字描述生成效果极好的图像。</p> 
  <p style="margin-left:0;">2021年12月提出了隐式扩散模型（Latent Diffusion Models，LDMs）的text-to-image模型。这个研究使得用扩散模型进行文字生成图片任务可以在普通显卡上执行，并且耗时较短。为一年后现象级的稳定扩散（Stable Diffusion）诞生奠定了基础。</p> 
  <p style="margin-left:0;"></p> 
  <h2 id="SD%20%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84" style="margin-left:0px;"><strong>SD </strong><strong>模型的网络结构</strong></h2> 
  <h3 id="%E4%B8%BB%E8%A6%81%E5%8C%85%E6%8B%AC%E4%B8%89%E4%B8%AA%E9%83%A8%E5%88%86%EF%BC%9A" style="margin-left:0px;"><strong>主要包括三个部分</strong>：</h3> 
  <h4 id="ClipText%20%E6%96%87%E6%9C%AC%E7%BC%96%E7%A0%81%E5%99%A8">ClipText 文本编码器</h4> 
  <blockquote> 
   <p style="margin-left:0px;">ClipText 文本编码器：用于解析提示词的 Clip 模型</p> 
  </blockquote> 
  <p>文本编码器负责将提示词转换成电脑可以识别的文本向量</p> 
  <h4 id="Diffusion%20%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B">Diffusion 扩散模型</h4> 
  <blockquote> 
   <p style="margin-left:0px;">Diffusion 扩散模型：用于生成图像的 U-Net 和 Scheduler</p> 
  </blockquote> 
  <p>扩散模型负责根据文本向量生成图像</p> 
  <h4 id="VAE%20%E6%A8%A1%E5%9E%8B">VAE 模型</h4> 
  <blockquote> 
   <p style="margin-left:0px;">VAE 模型：用于压缩和恢复的图像解码器</p> 
  </blockquote> 
  <p style="margin-left:0;">而图像编码器则用于将生成的图像信息进行解码，以生成最终的图像输出</p> 
  <p style="margin-left:0;"></p> 
  <h3 id="%E7%AE%80%E5%8C%96%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%9B%BE" style="margin-left:0px;"><strong>简化网络结构图</strong></h3> 
  <p class="img-center"><img alt="" height="395" src="https://images2.imgbox.com/cd/85/umAgFzbC_o.jpg" width="601"></p> 
  <h3 id="%E8%AF%A6%E7%BB%86%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%9B%BE" style="margin-left:0px;"><strong>详细网络结构图</strong></h3> 
  <p class="img-center"><img alt="" height="381" src="https://images2.imgbox.com/a0/c6/ZxA9kbwg_o.png" width="720"></p> 
  <p style="margin-left:0;"></p> 
  <p style="margin-left:0;"></p> 
  <h2 style="margin-left:0px;"><strong>ClipText 文本编码器</strong></h2> 
  <p style="margin-left:0px;">为了导入提示词，我们首先需要为文本创建数值表示形式。</p> 
  <p style="margin-left:0px;">为此，Stable Diffusion使用了一个名为CLIP的预训练Transformer模型。</p> 
  <p style="margin-left:0px;">CLIP的文本编码器会<strong>将文本描述转换为特征向量，该特征向量可用于与图像特征向量进行相似度比较。</strong></p> 
  <p class="img-center"><img alt="" height="378" src="https://images2.imgbox.com/a3/38/H1OzXtQa_o.png" width="768"></p> 
  <p style="margin-left:0;">因此，<strong>CLIP</strong>非常适合从文本描述中为图像创建有用的表征信息。输入的文本提示语首先会被分词（也就是基于一个很大的词汇库，将句子中的词语或短语转换为一个一个的token），然后被输入<strong>CLIP的文本编码器</strong>，从而为每个token（分词）产生一个768维（针对Stable Diffusion 1.x版本）或1024维（针对Stable Diffusion 2.x版本）的向量。</p> 
  <p style="margin-left:0;"></p> 
  <h3 id="CLIP%E6%A8%A1%E5%9E%8B" style="margin-left:0px;"><strong>CLIP模型</strong></h3> 
  <p style="margin-left:0;">CLIP模型是一个基于对比学习的多模态模型，主要包含Text Encoder和Image Encoder两个模型。</p> 
  <p style="margin-left:0;">其中Text Encoder用来提取文本的特征，可以使用NLP中常用的text transformer模型作为Text Encoder；</p> 
  <p style="margin-left:0;">而Image Encoder主要用来提取图像的特征，可以使用CNN/vision transformer模型（ResNet和ViT）作为Image Encoder。与此同时，他直接使用4亿个图片与标签文本对数据集进行训练，来学习图片与本文内容的对应关系。</p> 
  <p style="margin-left:0;"></p> 
  <p style="margin-left:0;"></p> 
  <h2 id="%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%BE%93%E5%85%A5Unet" style="margin-left:0px;"><strong>文本向量输入Unet</strong></h2> 
  <p style="margin-left:0;">文本提示词转换为向量后将被输入扩散模型，用于引导图像的生成，这里使用的扩散模型是Unet网络。</p> 
  <h3 id="%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E5%A6%82%E4%BD%95%E8%BE%93%E5%85%A5UNet%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B%EF%BC%9F" style="margin-left:0px;">文本向量如何输入UNet进行预测？</h3> 
  <blockquote> 
   <p style="margin-left:0;"><strong>交叉注意力(cross-attention)机制</strong></p> 
  </blockquote> 
  <p style="margin-left:0;">交叉注意力层贯穿了整个UNet结构，UNet中的每个空间位置都可以“注意”到文字条件中不同的token，以便从文本提示语中获取不同位置的相互关联信息。</p> 
  <p style="margin-left:0;">下图展示了UNet不同层之间信息的传递</p> 
  <p class="img-center"><img alt="" height="377" src="https://images2.imgbox.com/e6/8f/1CsHBGu8_o.png" width="516"></p> 
  <h3 id="%E4%BB%A5%E6%96%87%E6%9C%AC%E4%B8%BA%E7%94%9F%E6%88%90%E6%9D%A1%E4%BB%B6" style="margin-left:0px;"><strong>以文本为生成条件</strong></h3> 
  <p style="margin-left:0;">将提示信息输入UNet，实现对图像生成的定向引导，这种方法称为<strong>条件生成</strong>。</p> 
  <p style="margin-left:0;"></p> 
  <h3 id="UNet%E7%9A%84%E5%8E%9F%E7%90%86" style="margin-left:0px;">UNet的原理</h3> 
  <p style="margin-left:0;">在预测过程中，通过反复调用UNet迭代降噪，将UNet预测输出的noise slice从原有的噪声中去除，从而生成高质量图像。（具体细节可以看：<a href="https://blog.csdn.net/qq_61144763/article/details/138215463" title="扩散模型思想及数学原理-CSDN博客">扩散模型思想及数学原理-CSDN博客</a> ）</p> 
  <p style="margin-left:0;">对于给定的“带噪”图像，可以使模型<strong>基于提示信息来预测“去噪”后的图像</strong>。在推理阶段，我们可以输入期望图像的文本描述，并将纯噪声数据作为起点，然后模型便开始全力对噪声输入进行“去噪”，从而生成能够匹配文本描述的图像。</p> 
  <p style="margin-left:0;">具体到Stable Diffusion模型中，在推理阶段，我们可以输入期望图像的文本描述，并将纯噪声数据作为起点，然后模型便开始全力对噪声输入进行“去噪”，从而生成能够匹配文本描述的图像。</p> 
  <p class="img-center"><img alt="" height="442" src="https://images2.imgbox.com/a7/de/6yHJcXPY_o.png" width="442"></p> 
  <p style="margin-left:0;">文本编码过程：将输入的文本提示语转换为一系列的文本嵌入（即图中的ENCODER_HIDDEN_STATES），然后输入UNet作为生成条件。</p> 
  <p style="margin-left:0;"></p> 
  <h2 id="VAE%E6%A8%A1%E5%9E%8B" style="margin-left:0px;"><strong>VAE模型</strong></h2> 
  <p style="margin-left:0px;">由Latent Diffusion提出</p> 
  <p style="margin-left:0px;">当输入图像<strong>尺寸变大</strong>时，生成图片<strong>所需的计算能力也会随之增加</strong>。这种现象<strong>在自注意力(self-attention)机制下的影响尤为突出</strong>，因为操作数会随着输入量的增加以平方关系增加。</p> 
  <p style="margin-left:0;">例如：一张128×128像素的正方形图片拥有的像素数量是一张64×64像素的正方形图片的4倍，因此在自注意力层就需要16倍(42)于后者的内存和计算量。</p> 
  <blockquote> 
   <p style="margin-left:0;">这是高分辨率图像生成任务存在的普遍问题</p> 
  </blockquote> 
  <p style="margin-left:0;">为了解决这个问题，隐式扩散(Latent Diffusion)使用了一个独立的模型——<strong>VAE</strong>来压缩图片到一个更小的空间维度，VAE全称是 Variational Auto Encoder <strong>变分自动编码器</strong></p> 
  <p style="margin-left:0;"></p> 
  <h3 id="VAE%E5%8E%9F%E7%90%86" style="margin-left:0px;">VAE原理</h3> 
  <p style="margin-left:0;"><strong>图片通常包含大量冗余信息</strong>，因此我们可以训练一个VAE（对其使用大量的图片数据进行训练），使其可以<strong>将图片映射到一个较小的隐式表征</strong>，并<strong>将这个较小的隐式表征映射到原始图片</strong>。</p> 
  <p style="margin-left:0;">简单来说，它的作用就是<strong>将高维数据（像素空间）映射到低维空间（潜空间），从而实现数据的压缩和降维</strong>。</p> 
  <p style="margin-left:0;"></p> 
  <h3 id="VEA%E7%BB%84%E6%88%90" style="margin-left:0px;">VEA组成</h3> 
  <p style="margin-left:0;">它由<strong>编码器（Encoder）</strong>和<strong>解码器（Decoder）</strong>两部分组成。 <strong>编码器用于将图像信息降维并传入潜空间中，解码器将潜在数据表示转换回原始图像</strong>，而在潜在扩散模型的推理生成过程中我们只需用到 VAE 的解码器部分。</p> 
  <p class="img-center"><img alt="" height="209" src="https://images2.imgbox.com/b0/86/X5qHuXIB_o.png" width="555"></p> 
  <p style="margin-left:0;"></p> 
  <h3 id="SD%E5%AF%B9VAE%E7%9A%84%E5%BA%94%E7%94%A8" style="margin-left:0px;">SD对VAE的应用</h3> 
  <p style="margin-left:0;">Stable Diffusion中的VAE能够接收一张三通道图片作为输入，从而生成一个4通道的隐式表征，同时每一个空间维度都将减少为原来的八分之一。</p> 
  <p style="margin-left:0;">例如，一张512×512像素的正方形图片将被压缩到一个4×64×64的隐式表征上。</p> 
  <p style="margin-left:0;"><strong>作用</strong></p> 
  <p style="margin-left:0;">通过在隐式表征（而不是完整图像）上进行扩散，我们可以在使用更少的内存的同时减少UNet层数并加速图片的生成。与此同时，我们仍能把结果输入VAE的解码器，从而解码得到高分辨率图像。<strong>隐式表征极大降低了训练和推理成本</strong>。</p> 
  <p style="margin-left:0;"></p> 
  <h2 id="%E6%80%BB%E7%BB%93%E5%9B%BE" style="margin-left:0px;">总结图</h2> 
  <p class="img-center"><img alt="" height="304" src="https://images2.imgbox.com/bc/b5/kpGLryJc_o.png" width="608"></p> 
  <p style="margin-left:0;"></p> 
  <p style="margin-left:0;"></p> 
 </div> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ed01a79cae5be026c6c5c17a527578b3/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">YOLO目标检测项目--YOLOv4算法对交通标志的识别</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e6e870f8780306aa1e98724b7f3ab5b1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">conda怎么查看可以安装哪些版本的python</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>