<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ES入门十四：分词器 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/551e10799b2f6552779351d145cc42ca/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="ES入门十四：分词器">
  <meta property="og:description" content="我们存储到ES中数据大致分为以下两种：
全文本，例如文章内容、通知内容精确值，如实体Id 在对这两类值进行查询的时候，精确值类型会比较它们的二进制，其结果只有相等或者不想等。而对全文本类型进行等值比较是不太实现的，一般我们只会比较两个文本是否相似。根据上一讲的内容我们知道，要比较两个文本类型是否相似，使用相关性评分来评估的。而要得到相关性评分，我们就需要对全文本进行分词处理，然后得到统计数据才能进行评估
在es中，负责处理文本分词的是分词器，本文我们就来学习ES中分词器的组成和部分分词器的特性。
分词（Analysis）与分词器 分词是将全文本转换为一系列单词的过程，这些单词称为term或者token，而这个过程称为分词。
分词是通过**分词器（Analyzer）来实现的，**比如用于中文分词的IK分词器等。当然你也可以实现自己的分词器，例如可以简单将全文本以空格来实现分词。ES内置来一些常用的分词器，如果不能满足你的需求，你可以安装第三方的分词器或者定制化你自己的分词器。
**除了在写入的时候对数据进行分词，在对全文本进行查询的时候也需要使用相同的分词器对检索内存进行分析。例如，**查询Java Book的时候会分为java 和book两个单词，如下如所示：
分词器的组成 分词器主要由 3 部分组成。
Character Filter：注意对原文本进行格式处理，比如去除html标签Tokenizer：按照指定规则对文本进行切分，比如按空格来切分单词，同时页负责标记出每个单词的顺序、位置以及单词在原文本中开始和结束的偏移量Token Filter：对切分后的单词进行处理，如转换为小写、删除停顿词、增加同义词、词干化等 如下图就是分词器工作的流程，需要进行分词的文本依次通过Character Filter、Tokenizer、Token Filter，最后得出切分后的词项：
ES内置的分词器 为了方便用户使用，Es为用户提供了多个内置的分词器，常见的有以下8种。
Standard Analyzer：这个是默认的分词器，使用Unicode文本分割算法，将文本按单词切分并且转换为小写Simple Analyzer：按照非字母切分并且进行小写处理Stop Analyzer：与 Simple Analyzer 类似，但增加了停用词过滤（如 a、an、and、are、as、at、be、but 等）。Whitespace Analyzer：使用空格对文本进行切分，并不进行小写转换Pattern n Analyzer；使用正则表达式切分，默认使用 \W&#43; (非字符分隔)。支持小写转换和停用词删除。Keyword Analyzer：不进行分词Language Analyzer：提供了多种常见语言的分词器。如 Irish、Italian、Latvian 等。Customer Analyzer：自定义分词器 下面我们通过讲解Standard Analyzer来进一步熟悉分词器的工作流程，但在这之前我要先介绍一个Es提供的API：_analyze。
_analyze Api是一个非常有用的工具，它可以帮助我们查看分词器是如何工作的。_analyze API 提供了 3 种方式来查看分词器是如何工作的。
使用 _analyze API 时可以直接指定 Analyzer 来进行测试，示例如下： GET _analyze { &#34;analyzer&#34;: &#34;standard&#34;, &#34;text&#34;: &#34;Your cluster could be accessible to anyone.&#34; } # 结果 { &#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-07T23:43:19+08:00">
    <meta property="article:modified_time" content="2024-04-07T23:43:19+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ES入门十四：分词器</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-dracula">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>我们存储到ES中数据大致分为以下两种：</p> 
<ol><li>全文本，例如文章内容、通知内容</li><li>精确值，如实体Id</li></ol> 
<p>在对这两类值进行查询的时候，精确值类型会比较它们的二进制，其结果只有相等或者不想等。而对全文本类型进行等值比较是不太实现的，一般我们只会比较两个文本是否相似。根据上一讲的内容我们知道，要比较两个文本类型是否相似，使用相关性评分来评估的。<strong>而要得到相关性评分，我们就需要对全文本进行分词处理，然后得到统计数据才能进行评估</strong></p> 
<p>在es中，负责处理文本分词的是分词器，本文我们就来学习ES中分词器的组成和部分分词器的特性。</p> 
<p></p> 
<h3><a id="Analysis_11"></a><strong>分词（Analysis）与分词器</strong></h3> 
<p><strong>分词</strong>是将全文本转换为一系列单词的过程，这些单词称为<strong>term</strong>或者<strong>token</strong>，而这个过程称为分词。<br>分词是通过**分词器（Analyzer）来实现的，**比如用于中文分词的IK分词器等。当然你也可以实现自己的分词器，例如可以简单将全文本以空格来实现分词。ES内置来一些常用的分词器，如果不能满足你的需求，你可以安装第三方的分词器或者定制化你自己的分词器。</p> 
<p>**除了在写入的时候对数据进行分词，在对全文本进行查询的时候也需要使用相同的分词器对检索内存进行分析。例如，**查询Java Book的时候会分为java 和book两个单词，如下如所示：<br><img src="https://images2.imgbox.com/82/f0/SFQjMODA_o.png" alt="image.png"><br> </p> 
<h3><a id="_16"></a>分词器的组成</h3> 
<p>分词器主要由 3 部分组成。</p> 
<ul><li>Character Filter：注意对原文本进行格式处理，比如去除html标签</li><li>Tokenizer：按照指定规则对文本进行切分，比如按空格来切分单词，同时页负责标记出每个单词的顺序、位置以及单词在原文本中开始和结束的偏移量</li><li>Token Filter：对切分后的单词进行处理，如转换为小写、删除停顿词、增加同义词、词干化等</li></ul> 
<p>如下图就是分词器工作的流程，<strong>需要进行分词的文本依次通过Character Filter、Tokenizer、Token Filter，最后得出切分后的词项：</strong><br><img src="https://images2.imgbox.com/49/40/IWr8ogEG_o.png" alt="image.png"></p> 
<p></p> 
<h3><a id="ES_26"></a>ES内置的分词器</h3> 
<p>为了方便用户使用，Es为用户提供了多个内置的分词器，常见的有以下8种。</p> 
<ul><li>Standard Analyzer：这个是默认的分词器，使用Unicode文本分割算法，将文本按单词切分并且转换为小写</li><li>Simple Analyzer：按照非字母切分并且进行小写处理</li><li>Stop Analyzer：与 Simple Analyzer 类似，但增加了停用词过滤（如 a、an、and、are、as、at、be、but 等）。</li><li>Whitespace Analyzer：使用空格对文本进行切分，并不进行小写转换</li><li>Pattern n Analyzer；使用正则表达式切分，默认使用 \W+ (非字符分隔)。支持小写转换和停用词删除。</li><li>Keyword Analyzer：不进行分词</li><li>Language Analyzer：提供了多种常见语言的分词器。如 Irish、Italian、Latvian 等。</li><li>Customer Analyzer：自定义分词器</li></ul> 
<p>下面我们通过讲解Standard Analyzer来进一步熟悉分词器的工作流程，但在这之前我要先介绍一个Es提供的API：<strong>_analyze。</strong><br><strong>_analyze Api</strong>是一个非常有用的工具，它可以帮助我们查看分词器是如何工作的。<a href="https://link.juejin.cn/?target=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Freference%2Fcurrent%2Ftest-analyzer.html" rel="nofollow">_analyze</a> API 提供了 3 种方式来查看分词器是如何工作的。</p> 
<ol><li>使用 _analyze API 时可以直接指定 Analyzer 来进行测试，示例如下：</li></ol> 
<pre><code class="prism language-bash">GET _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"analyzer"</span><span class="token builtin class-name">:</span> <span class="token string">"standard"</span>,
  <span class="token string">"text"</span><span class="token builtin class-name">:</span> <span class="token string">"Your cluster could be accessible to anyone."</span>
<span class="token punctuation">}</span>

<span class="token comment"># 结果</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokens"</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span><span class="token builtin class-name">:</span> <span class="token string">"your"</span>,
      <span class="token string">"start_offset"</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
      <span class="token string">"end_offset"</span><span class="token builtin class-name">:</span> <span class="token number">4</span>,
      <span class="token string">"type"</span><span class="token builtin class-name">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span>,
      <span class="token string">"position"</span><span class="token builtin class-name">:</span> <span class="token number">0</span>
    <span class="token punctuation">}</span>,
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span><span class="token builtin class-name">:</span> <span class="token string">"cluster"</span>,
      <span class="token string">"start_offset"</span><span class="token builtin class-name">:</span> <span class="token number">5</span>,
      <span class="token string">"end_offset"</span><span class="token builtin class-name">:</span> <span class="token number">12</span>,
      <span class="token string">"type"</span><span class="token builtin class-name">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span>,
      <span class="token string">"position"</span><span class="token builtin class-name">:</span> <span class="token number">1</span>
    <span class="token punctuation">}</span>
    <span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>如上示例，在这段代码中我们可以看到它将text的内容用standard分词器进行分词，text的内容按单词进行了切分并且your转为了小写。</p> 
<ol start="2"><li>对指定的索引进行测试，示例如下：</li></ol> 
<pre><code class="prism language-bash"><span class="token comment"># 创建和设置索引</span>
PUT my-index
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"mappings"</span><span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"properties"</span><span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"my_text"</span><span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"type"</span><span class="token builtin class-name">:</span> <span class="token string">"text"</span>,
        <span class="token string">"analyzer"</span><span class="token builtin class-name">:</span> <span class="token string">"standard"</span>  <span class="token comment"># my_text字段使用了standard分词器</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

GET my-index/_analyze 
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"field"</span><span class="token builtin class-name">:</span> <span class="token string">"my_text"</span>, <span class="token comment"># 直接使用my_text字段已经设置的分词器</span>
  <span class="token string">"text"</span><span class="token builtin class-name">:</span>  <span class="token string">"Is this déjà vu?"</span>
<span class="token punctuation">}</span>

<span class="token comment"># 结果：</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokens"</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span><span class="token builtin class-name">:</span> <span class="token string">"is"</span>,
      <span class="token string">"start_offset"</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
      <span class="token string">"end_offset"</span><span class="token builtin class-name">:</span> <span class="token number">2</span>,
      <span class="token string">"type"</span><span class="token builtin class-name">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span>,
      <span class="token string">"position"</span><span class="token builtin class-name">:</span> <span class="token number">0</span>
    <span class="token punctuation">}</span>,
    <span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">}</span>

</code></pre> 
<ol start="3"><li>组合 tokenizer、filters、character filters 进行测试，示例如下：</li></ol> 
<pre><code class="prism language-bash">GET _analyze 
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokenizer"</span><span class="token builtin class-name">:</span> <span class="token string">"standard"</span>, <span class="token comment"># 指定一个tokenizer</span>
  <span class="token string">"filter"</span><span class="token builtin class-name">:</span>  <span class="token punctuation">[</span> <span class="token string">"lowercase"</span>, <span class="token string">"asciifolding"</span> <span class="token punctuation">]</span>, <span class="token comment"># 可以组合多个token filter</span>
  <span class="token comment"># "char_filter":"html_strip", 可以指定零个Character Filter</span>
  <span class="token string">"text"</span><span class="token builtin class-name">:</span> <span class="token string">"java app"</span>
<span class="token punctuation">}</span>

</code></pre> 
<p>从上面的示例可以看到，<strong>tokenizer使用了standard而token filter使用了lowercase和ascillfolding来对text的内容进行切分。用户可以组合一个tokenizer、零个或者多个token filter、零个或者多个character filter。</strong></p> 
<p></p> 
<h4><a id="Standard_Analyzer_122"></a>Standard Analyzer</h4> 
<p>Standard Analyzer 是 ES 默认的分词器，它会将输入的内容按词切分，并且将切分后的词进行小写转换，默认情况下停用词（Stop Word）过滤功能是关闭的。<br><img src="https://images2.imgbox.com/f5/b3/wgOW0FGh_o.png" alt="image.png"><br>可以试一下下面这个例子：</p> 
<pre><code class="prism language-bash">GET _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"analyzer"</span><span class="token builtin class-name">:</span> <span class="token string">"standard"</span>, <span class="token comment"># 设定分词器为 standard</span>
  <span class="token string">"text"</span><span class="token builtin class-name">:</span> <span class="token string">"Your cluster could be accessible to anyone."</span>
<span class="token punctuation">}</span>

<span class="token comment"># 结果</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokens"</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span><span class="token builtin class-name">:</span> <span class="token string">"your"</span>,
      <span class="token string">"start_offset"</span><span class="token builtin class-name">:</span> <span class="token number">0</span>,
      <span class="token string">"end_offset"</span><span class="token builtin class-name">:</span> <span class="token number">4</span>,
      <span class="token string">"type"</span><span class="token builtin class-name">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span>,
      <span class="token string">"position"</span><span class="token builtin class-name">:</span> <span class="token number">0</span>
    <span class="token punctuation">}</span>,
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span><span class="token builtin class-name">:</span> <span class="token string">"cluster"</span>,
      <span class="token string">"start_offset"</span><span class="token builtin class-name">:</span> <span class="token number">5</span>,
      <span class="token string">"end_offset"</span><span class="token builtin class-name">:</span> <span class="token number">12</span>,
      <span class="token string">"type"</span><span class="token builtin class-name">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span>,
      <span class="token string">"position"</span><span class="token builtin class-name">:</span> <span class="token number">1</span>
    <span class="token punctuation">}</span> 
    <span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">}</span>

</code></pre> 
<p>如上示例，从其结果中可以看出，单词You做了小写转换，停用词be没有被去掉，并且返回结果里记录了这个单词在原文本中的开始偏移量、结束偏移以及这个词出现的位置</p> 
<p></p> 
<h4><a id="_157"></a>自定义分词器</h4> 
<p>除了使用内置的分词器外，我们还可以通过组合 Tokenizer、Filters、Character Filters 来自定义分词器。其用例如下：</p> 
<pre><code class="prism language-bash">PUT my-index-001
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"settings"</span><span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"analysis"</span><span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"char_filter"</span><span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span> <span class="token comment"># 自定义char_filter</span>
        <span class="token string">"and_char_filter"</span><span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span>
          <span class="token string">"type"</span><span class="token builtin class-name">:</span> <span class="token string">"mapping"</span>,
          <span class="token string">"mappings"</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token string">"&amp; =&gt; and"</span><span class="token punctuation">]</span> <span class="token comment"># 将 '&amp;' 转换为 'and'</span>
        <span class="token punctuation">}</span>
      <span class="token punctuation">}</span>,
      <span class="token string">"filter"</span><span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span> <span class="token comment"># 自定义 filter</span>
        <span class="token string">"an_stop_filter"</span><span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span>
          <span class="token string">"type"</span><span class="token builtin class-name">:</span> <span class="token string">"stop"</span>,
          <span class="token string">"stopwords"</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token string">"an"</span><span class="token punctuation">]</span> <span class="token comment"># 设置 "an" 为停用词</span>
        <span class="token punctuation">}</span>
      <span class="token punctuation">}</span>,
      <span class="token string">"analyzer"</span><span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span> <span class="token comment"># 自定义分词器为 custom_analyzer</span>
        <span class="token string">"custom_analyzer"</span><span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span>
          <span class="token string">"type"</span><span class="token builtin class-name">:</span> <span class="token string">"custom"</span>,
          <span class="token comment"># 使用内置的html标签过滤和自定义的my_char_filter</span>
          <span class="token string">"char_filter"</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token string">"html_strip"</span>, <span class="token string">"and_char_filter"</span><span class="token punctuation">]</span>,
          <span class="token string">"tokenizer"</span><span class="token builtin class-name">:</span> <span class="token string">"standard"</span>,
          <span class="token comment"># 使用内置的lowercase filter和自定义的my_filter</span>
          <span class="token string">"filter"</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token string">"lowercase"</span>, <span class="token string">"an_stop_filter"</span><span class="token punctuation">]</span>
        <span class="token punctuation">}</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

GET my-index-001/_analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"analyzer"</span><span class="token builtin class-name">:</span> <span class="token string">"custom_analyzer"</span>,
  <span class="token string">"text"</span><span class="token builtin class-name">:</span> <span class="token string">"Tom &amp; Gogo bought an orange &lt;span&gt; at an orange shop"</span>
<span class="token punctuation">}</span>

</code></pre> 
<p>你可以在 Kibana 中运行上述的语句并且查看结果是否符合预期，Tom 和 Gogo 将会变成小写，而 &amp; 会转为 and，an 这个停用词和这个 html 标签将会被处理掉，但 at 不会。<br>ES 的内置分词器可以很方便地处理英文字符，但对于中文却并不那么好使，一般我们需要依赖第三方的分词器插件才能满足日常需求。<br> </p> 
<h3><a id="_199"></a>中文分词器</h3> 
<p>中文分词不像英文分词那样可以简单地以空格来分隔，而是要分成有含义的词汇，但相同的词汇在不同的语境下有不同的含义。社区中有很多优秀的分词器，这里列出几个日常用得比较多的。</p> 
<ul><li>analysis-icu是官方的插件，其将Lucene ICU module融入到es中。使用ICU函数库来处理提供处理Unicode的工具</li><li>IK：支持自定义词典和词典热更新</li><li>THULAC：其安装和使用官方文档中有详细的说明，本文就不再赘述了</li></ul> 
<p></p> 
<h4><a id="analysisicu_207"></a>analysis-icu分词器</h4> 
<p>analysis-icu 是官方的插件，<a href="https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Ftree%2Fmaster%2Fplugins%2Fanalysis-icu" rel="nofollow">项目在这里</a>。ICU 的安装如下：</p> 
<pre><code class="prism language-bash"><span class="token comment"># 进入脚本目录，参见ES 简介和安装一节我们把es安装在ES/es_node1</span>

<span class="token comment"># 有3个节点的需要分别进入3个节点目录进行安装 ！！！！！</span>

<span class="token builtin class-name">cd</span> ES/es_node1

bin/elasticsearch-plugin <span class="token function">install</span> https://artifacts.elastic.co/downloads/elasticsearch-plugins/analysis-icu/analysis-icu-7.13.0.zip

<span class="token comment"># 如果安装出错，并且提示你没有权限，请加上sudo：</span>

<span class="token function">sudo</span> bin/elasticsearch-plugin <span class="token function">install</span> https://artifacts.elastic.co/downloads/elasticsearch-plugins/analysis-icu/analysis-icu-7.13.0.zip

</code></pre> 
<p>ICU 的用例如下：</p> 
<pre><code class="prism language-bash">POST _analyze
<span class="token punctuation">{<!-- --></span>  
    <span class="token string">"analyzer"</span><span class="token builtin class-name">:</span> <span class="token string">"icu_analyzer"</span>,
    <span class="token string">"text"</span><span class="token builtin class-name">:</span> <span class="token string">"Linus 在90年代开发出了linux操作系统"</span>  
<span class="token punctuation">}</span>

<span class="token comment"># 结果</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokens"</span> <span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
    <span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token builtin class-name">:</span> <span class="token string">"开发"</span>,
      <span class="token string">"start_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">11</span>,
      <span class="token string">"end_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">13</span>,
      <span class="token string">"type"</span> <span class="token builtin class-name">:</span> <span class="token string">"&lt;IDEOGRAPHIC&gt;"</span>,
      <span class="token string">"position"</span> <span class="token builtin class-name">:</span> <span class="token number">4</span>
    <span class="token punctuation">}</span>,
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token builtin class-name">:</span> <span class="token string">"出了"</span>,
      <span class="token string">"start_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">13</span>,
      <span class="token string">"end_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">15</span>,
      <span class="token string">"type"</span> <span class="token builtin class-name">:</span> <span class="token string">"&lt;IDEOGRAPHIC&gt;"</span>,
      <span class="token string">"position"</span> <span class="token builtin class-name">:</span> <span class="token number">5</span>
    <span class="token punctuation">}</span>,
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token builtin class-name">:</span> <span class="token string">"linux"</span>,
      <span class="token string">"start_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">15</span>,
      <span class="token string">"end_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">20</span>,
      <span class="token string">"type"</span> <span class="token builtin class-name">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span>,
      <span class="token string">"position"</span> <span class="token builtin class-name">:</span> <span class="token number">6</span>
    <span class="token punctuation">}</span>
    <span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">}</span>

</code></pre> 
<p>通过在 Kibana 上运行上述查询语句，可以看到结果与 Standard Analyzer 是不一样的，同样你可以将得出的结果和下面的 IK 分词器做一下对比，看看哪款分词器更适合你的业务。更详细的 ICU 使用文档可以查看：<a href="https://link.juejin.cn/?target=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Felasticsearch%2Fplugins%2Fcurrent%2Fanalysis-icu-analyzer.html" rel="nofollow">ICU 文档</a><br> </p> 
<h4><a id="IK_263"></a>IK分词器</h4> 
<p>IK 的算法是基于词典的，其支持自定义词典和词典热更新。下面来安装 IK 分词器插件：</p> 
<pre><code class="prism language-bash"><span class="token comment"># 有3个节点的需要分别进入3个节点目录进行安装 ！！！！！</span>

<span class="token builtin class-name">cd</span> ES/es_node1

<span class="token comment"># 如果因为没有权限而安装失败的话，使用sudo ./bin/elasticsearch-plugin install url 来安装</span>

./bin/elasticsearch-plugin <span class="token function">install</span> https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.13.0/elasticsearch-analysis-ik-7.13.0.zip

</code></pre> 
<p><strong>在每个节点</strong>执行完上述指令后，需要<strong>重启服务才能使插件生效</strong>。重启后，可以在 Kibana 中测试一下 IK 中文分词器的效果了。</p> 
<pre><code class="prism language-bash">POST _analyze
<span class="token punctuation">{<!-- --></span>  
    <span class="token string">"analyzer"</span><span class="token builtin class-name">:</span> <span class="token string">"ik_max_word"</span>,
    <span class="token string">"text"</span><span class="token builtin class-name">:</span> <span class="token string">"Linus 在90年代开发出了linux操作系统"</span>  
<span class="token punctuation">}</span>

POST _analyze
<span class="token punctuation">{<!-- --></span>  
    <span class="token string">"analyzer"</span><span class="token builtin class-name">:</span> <span class="token string">"ik_smart"</span>,
    <span class="token string">"text"</span><span class="token builtin class-name">:</span> <span class="token string">"Linus 在90年代开发出了linux操作系统"</span>  
<span class="token punctuation">}</span>

</code></pre> 
<p>如上示例可以看到，<strong>IK 有两种模式：ik_max_word 和 ik_smart</strong>，它们的区别可总结为如下（以下是 IK 项目的原文）。</p> 
<ul><li><strong>ik_max_word：</strong> 会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国、中华人民、中华、华人、人民共和国、人民、人、民、共和国、共和、和、国国、国歌”，会穷尽各种可能的组合，适合 Term Query。</li><li><strong>ik_smart：</strong> 会做最粗粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国、国歌”，适合 Phrase 查询。</li></ul> 
<p>关于 IK 分词器插件更详细的使用信息，你可以参考 <a href="https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Fmedcl%2Felasticsearch-analysis-ik%2Ftree%2Fv7.13.0" rel="nofollow">IK 项目</a>的文档。<br> </p> 
<h3><a id="_298"></a></h3>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/167f99a712891cd82869c0ca82e786a0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">解决前端精度丢失问题：后端Long类型到前端的处理策略</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/cada58090ee2392867348bec867cbd39/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">前端入门（认识HTML，CSS，JavaScript三件套）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>