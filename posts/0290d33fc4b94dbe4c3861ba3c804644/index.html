<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Llama 3 模型微调的步骤 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/0290d33fc4b94dbe4c3861ba3c804644/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Llama 3 模型微调的步骤">
  <meta property="og:description" content="环境准备 操作系统：Ubuntu 22.04.5 LTS Anaconda3：Miniconda3-latest-Linux-x86_64 GPU： NVIDIA GeForce RTX 4090 24G Step 1. 准备conda环境 创建一个新的conda环境：
conda create --name llama_factory python=3.11 激活刚刚创建的conda环境：
conda activate llama_factory Step 2. 下载LLaMA-Factory的项目文件 下载LLama_Factory源码：
git clone https://github.com/hiyouga/LLaMA-Factory.git Step 3. 升级pip版本 建议在执行项目的依赖安装之前升级 pip 的版本，如果使用的是旧版本的 pip，可能无法安装一些最新的包，或者可能无法正确解析依赖关系。升级 pip 很简单，只需要运行命令如下命令：
python -m pip install --upgrade pip Step 4. 使用pip安装LLaMA-Factory项目代码运行的项目依赖 pip install -r requirements.txt --index-url https://mirrors.huaweicloud.com/repository/pypi/simple Step 5. Llama3模型下载 在~/ai-test/创建如下目录：
mkdir model 存放模型文件
cd model 可以从下面地址中下载模型文件，这里我们从ModelScope来下载
huggingface Llama3模型主页：
https://huggingface.co/meta-llama/
Github主页：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-28T17:17:51+08:00">
    <meta property="article:modified_time" content="2024-06-28T17:17:51+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Llama 3 模型微调的步骤</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_0"></a>环境准备</h3> 
<pre><code class="prism language-bash">操作系统：Ubuntu <span class="token number">22.04</span>.5 LTS
Anaconda3：Miniconda3-latest-Linux-x86_64
GPU： NVIDIA GeForce RTX <span class="token number">4090</span> 24G
</code></pre> 
<h3><a id="Step_1_conda_9"></a>Step 1. 准备conda环境</h3> 
<p>创建一个新的conda环境：</p> 
<pre><code class="prism language-bash">conda create <span class="token parameter variable">--name</span> llama_factory <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.11</span>
</code></pre> 
<p>激活刚刚创建的conda环境：</p> 
<pre><code class="prism language-bash">conda activate llama_factory
</code></pre> 
<h3><a id="Step_2_LLaMAFactory_22"></a>Step 2. 下载LLaMA-Factory的项目文件</h3> 
<p>下载LLama_Factory源码：</p> 
<pre><code class="prism language-bash"><span class="token function">git</span> clone https://github.com/hiyouga/LLaMA-Factory.git
</code></pre> 
<h3><a id="Step_3_pip_29"></a>Step 3. 升级pip版本</h3> 
<p>建议在执行项目的依赖安装之前升级 pip 的版本，如果使用的是旧版本的 pip，可能无法安装一些最新的包，或者可能无法正确解析依赖关系。升级 pip 很简单，只需要运行命令如下命令：</p> 
<pre><code class="prism language-bash">python <span class="token parameter variable">-m</span> pip <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> pip
</code></pre> 
<h3><a id="Step_4_pipLLaMAFactory_36"></a>Step 4. 使用pip安装LLaMA-Factory项目代码运行的项目依赖</h3> 
<pre><code class="prism language-bash">pip <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements.txt --index-url https://mirrors.huaweicloud.com/repository/pypi/simple
</code></pre> 
<h3><a id="Step_5_Llama3_42"></a>Step 5. Llama3模型下载</h3> 
<p>在~/ai-test/创建如下目录：</p> 
<pre><code class="prism language-bash"><span class="token function">mkdir</span> model
</code></pre> 
<p>存放模型文件</p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">cd</span> model
</code></pre> 
<p>可以从下面地址中下载模型文件，这里我们从ModelScope来下载</p> 
<p>huggingface Llama3模型主页：</p> 
<p>https://huggingface.co/meta-llama/</p> 
<p>Github主页：</p> 
<p>https://github.com/meta-llama/llama3/tree/main</p> 
<p>ModelScope Llama3-8b模型主页：</p> 
<p>https://www.modelscope.cn/models/LLM-Research/Meta-Llama-3-8B-Instruct/summary</p> 
<pre><code class="prism language-bash"><span class="token function">git</span> clone https://www.modelscope.cn/LLM-Research/Meta-Llama-3-8B-Instruct.git
</code></pre> 
<h3><a id="Step_6__74"></a>Step 6. 运行原始模型</h3> 
<p>切换到LLama_Factory目录下</p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">cd</span> ~/ai-test/LLaMA-Factory
</code></pre> 
<pre><code class="prism language-bash"><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span>  python src/web_demo.py <span class="token punctuation">\</span>
    <span class="token parameter variable">--model_name_or_path</span> /home/oneview/ai-test/model/Meta-Llama-3-8B-Instruct <span class="token punctuation">\</span>
    <span class="token parameter variable">--template</span> llama3 <span class="token punctuation">\</span>
    <span class="token parameter variable">--infer_backend</span> vllm <span class="token punctuation">\</span>
    <span class="token parameter variable">--vllm_enforce_eager</span>
</code></pre> 
<p>访问<code>http://127.0.0.1:8000</code>输入“你好，请介绍下你自己”，可以发现模型还不具备中文处理能力，后面我们将用中文数据集对模型进行微调。<br> <img src="https://images2.imgbox.com/a0/6d/64LJLFyy_o.png" alt="在这里插入图片描述"><br> 通过上述步骤就已经完成了LLaMA-Factory模型的完整私有化部署过程。</p> 
<p>接下来是微调的步骤</p> 
<p><img src="https://images2.imgbox.com/9c/de/uAmyi0t4_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Step_1__96"></a>Step 1. 查看微调中文数据集数据字典</h3> 
<p>我们找到./LLaMA-Factory目录下的data文件夹：<br> <img src="https://images2.imgbox.com/13/ee/tuH1Oi1x_o.png" alt="在这里插入图片描述"><br> 查看dataset_info.json:</p> 
<p><img src="https://images2.imgbox.com/c3/b7/k1SqcVhd_o.png" alt="在这里插入图片描述"></p> 
<p>找到当前数据集名称：alpaca_zh。数据集情况如下：<br> <img src="https://images2.imgbox.com/36/a3/5I6XbHcB_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Step_2__107"></a>Step 2. 创建微调脚本</h3> 
<p>切换到./LLaMA-Factory目录，创建一个名为single_lora_llama3.sh的脚本（脚本的名字可以自由命名）。</p> 
<pre><code class="prism language-bash"><span class="token shebang important">#!/bin/bash</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDA_DEVICE_MAX_CONNECTIONS</span><span class="token operator">=</span><span class="token number">1</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">NCCL_P2P_DISABLE</span><span class="token operator">=</span><span class="token string">"1"</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">NCCL_IB_DISABLE</span><span class="token operator">=</span><span class="token string">"1"</span>


<span class="token comment"># 如果是预训练，添加参数       --stage pt \</span>
<span class="token comment"># 如果是指令监督微调，添加参数  --stage sft \</span>
<span class="token comment"># 如果是奖励模型训练，添加参数  --stage rm \</span>
<span class="token comment"># 添加 --quantization_bit 4 就是4bit量化的QLoRA微调，不添加此参数就是LoRA微调 \</span>



<span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> python src/train_bash.py <span class="token punctuation">\</span>   <span class="token comment">## 单卡运行</span>
  <span class="token parameter variable">--stage</span> sft <span class="token punctuation">\</span>                                     <span class="token comment">## --stage pt （预训练模式）  --stage sft（指令监督模式）</span>
  <span class="token parameter variable">--do_train</span> True <span class="token punctuation">\</span>                                 <span class="token comment">## 执行训练模型</span>
  <span class="token parameter variable">--model_name_or_path</span> /home/oneview/ai-test/model/Meta-Llama-3-8B-Instruct <span class="token punctuation">\</span>     <span class="token comment">## 模型的存储路径</span>
  <span class="token parameter variable">--dataset</span> alpaca_zh <span class="token punctuation">\</span>                                <span class="token comment">## 训练数据的存储路径，存放在 LLaMA-Factory/data路径下</span>
  <span class="token parameter variable">--template</span> llama3 <span class="token punctuation">\</span>                                 <span class="token comment">## 选择Qwen模版</span>
  <span class="token parameter variable">--lora_target</span> q_proj,v_proj <span class="token punctuation">\</span>                     <span class="token comment">## 默认模块应作为</span>
  <span class="token parameter variable">--output_dir</span> /home/oneview/ai-test/Llama3/output <span class="token punctuation">\</span>        <span class="token comment">## 微调后的模型保存路径</span>
  <span class="token parameter variable">--overwrite_cache</span> <span class="token punctuation">\</span>                               <span class="token comment">## 是否忽略并覆盖已存在的缓存数据</span>
  <span class="token parameter variable">--per_device_train_batch_size</span> <span class="token number">2</span> <span class="token punctuation">\</span>                 <span class="token comment">## 用于训练的批处理大小。可根据 GPU 显存大小自行设置。</span>
  <span class="token parameter variable">--gradient_accumulation_steps</span> <span class="token number">64</span> <span class="token punctuation">\</span>                 <span class="token comment">##  梯度累加次数</span>
  <span class="token parameter variable">--lr_scheduler_type</span> cosine <span class="token punctuation">\</span>                      <span class="token comment">## 指定学习率调度器的类型</span>
  <span class="token parameter variable">--logging_steps</span> <span class="token number">5</span> <span class="token punctuation">\</span>                               <span class="token comment">## 指定了每隔多少训练步骤记录一次日志。这包括损失、学习率以及其他重要的训练指标，有助于监控训练过程。</span>
  <span class="token parameter variable">--save_steps</span> <span class="token number">100</span> <span class="token punctuation">\</span>                                <span class="token comment">## 每隔多少训练步骤保存一次模型。这是模型保存和检查点创建的频率，允许你在训练过程中定期保存模型的状态</span>
  <span class="token parameter variable">--learning_rate</span> 5e-5 <span class="token punctuation">\</span>                            <span class="token comment">## 学习率</span>
  <span class="token parameter variable">--num_train_epochs</span> <span class="token number">1.0</span> <span class="token punctuation">\</span>                          <span class="token comment">## 指定了训练过程将遍历整个数据集的次数。一个epoch表示模型已经看过一次所有的训练数据。</span>
  <span class="token parameter variable">--finetuning_type</span> lora <span class="token punctuation">\</span>                          <span class="token comment">## 参数指定了微调的类型，lora代表使用LoRA（Low-Rank Adaptation）技术进行微调。</span>
  <span class="token parameter variable">--fp16</span> <span class="token punctuation">\</span>                                          <span class="token comment">## 开启半精度浮点数训练</span>
  <span class="token parameter variable">--lora_rank</span> <span class="token number">4</span> <span class="token punctuation">\</span>                                   <span class="token comment">## 在使用LoRA微调时设置LoRA适应层的秩。</span>
</code></pre> 
<p>注：实际脚本文件最好不要出现中文备注，否则容易出现编辑格式导致的问题。</p> 
<p>然后为了保险起见，我们需要对齐格式内容进行调整，以满足Ubuntu操作系统运行需要（此前是从Windows系统上复制过去的文件，一般都需要进行如此操作）：</p> 
<pre><code class="prism language-bash"><span class="token function">sed</span> <span class="token parameter variable">-i</span> <span class="token string">'s/\r$//'</span> ./single_lora_llama3.sh
</code></pre> 
<h3><a id="Step_3__155"></a>Step 3. 运行微调脚本，获取模型微调权重</h3> 
<p>当我们准备好微调脚本之后，接下来即可围绕当前模型进行微调了。这里我们直接在命令行中执行sh文件即可，注意运行前需要为该文件增加权限：</p> 
<pre><code class="prism language-bash">  
<span class="token function">chmod</span> +x ./single_lora_llama3.sh
./single_lora_llama3.sh
</code></pre> 
<p>当微调结束之后，我们就可以在当前主目录下看到新的模型权重文件：</p> 
<p><img src="https://images2.imgbox.com/76/20/fbkEvmfm_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Step_4__170"></a>Step 4. 合并模型权重，获得微调模型</h3> 
<p>接下来我们需要将该模型权重文件和此前的原始模型权重文件进行合并，才能获得最终的微调模型。LlaMa-Factory中已经为我们提供了非常完整的模型合并方法，同样，我们只需要编写脚本文件来执行合并操作即可，即llama3_merge_model.sh。同样，该脚本文件也可以按照此前single_lora_llama3.sh脚本相类似的操作，就是将课件中提供的脚本直接上传到Jupyter主目录下，再复制到LlaMa-Factory主目录下进行运行。</p> 
<p>首先简单查看llama3_merge_model.sh脚本文件内容：</p> 
<pre><code class="prism language-bash"><span class="token shebang important">#!/bin/bash</span>

python src/export_model.py <span class="token punctuation">\</span>               <span class="token comment">## 用于执行合并功能的Python代码文件</span>
  <span class="token parameter variable">--model_name_or_path</span> /home/oneview/ai-test/model/Meta-Llama-3-8B-Instruct <span class="token punctuation">\</span>  <span class="token comment">## 原始模型文件</span>
  <span class="token parameter variable">--adapter_name_or_path</span> /home/oneview/ai-test/Llama3/output <span class="token punctuation">\</span>                <span class="token comment">## 微调模型权重文件</span>
  <span class="token parameter variable">--template</span> llama3 <span class="token punctuation">\</span>                        <span class="token comment">## 模型模板名称</span>
  <span class="token parameter variable">--finetuning_type</span> lora <span class="token punctuation">\</span>                 <span class="token comment">## 微调框架名称</span>
  <span class="token parameter variable">--export_dir</span>  /home/oneview/ai-test/Llama3/output_lora <span class="token punctuation">\</span>                          <span class="token comment">## 合并后新模型文件位置</span>
  <span class="token parameter variable">--export_size</span> <span class="token number">2</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--export_legacy_format</span> <span class="token boolean">false</span>
</code></pre> 
<p>注：实际脚本文件最好不要出现中文备注，否则容易出现编辑格式导致的问题。</p> 
<p>然后运行脚本，进行模型合并：</p> 
<pre><code class="prism language-bash">./llama3_merge_model.sh
</code></pre> 
<p>接下来即可查看刚刚获得的新的微调模型：</p> 
<p><img src="https://images2.imgbox.com/6d/8a/MtxqoyWy_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Step_5__202"></a>Step 5. 测试微调效果</h3> 
<p>切换到LLama_Factory目录下</p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">cd</span> ~/ai-test/LLaMA-Factory
</code></pre> 
<pre><code class="prism language-bash"><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span>  python src/web_demo.py <span class="token punctuation">\</span>
    <span class="token parameter variable">--model_name_or_path</span> /home/oneview/ai-test/Llama3/llama3_lora <span class="token punctuation">\</span>
    <span class="token parameter variable">--template</span> llama3 <span class="token punctuation">\</span>
    <span class="token parameter variable">--infer_backend</span> vllm <span class="token punctuation">\</span>
    <span class="token parameter variable">--vllm_enforce_eager</span>
</code></pre> 
<p>运行如下：端口可能有所不同</p> 
<p><img src="https://images2.imgbox.com/db/f3/o2dsykoX_o.png" alt="在这里插入图片描述"></p> 
<p>访问http://127.0.0.1:8000<br> <img src="https://images2.imgbox.com/fd/f5/NT2aoOIp_o.png" alt="在这里插入图片描述"></p> 
<p>可以看到，现在的回答已经是中文</p> 
<p>原文博客地址：<a href="https://www.cnblogs.com/hlgnet/articles/18148788" rel="nofollow">https://www.cnblogs.com/hlgnet/articles/18148788</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6db7e4f77553d1f5b81094bf1e4c6c23/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">什么是产线工控安全，如何保障产线设备的安全</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2a244a352b92e7323200cea0abeccaf8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">ElementUI的搭建</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>