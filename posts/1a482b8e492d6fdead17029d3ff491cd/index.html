<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>摸鱼大数据——Spark基础——Spark On Yarn环境配置和部署 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/1a482b8e492d6fdead17029d3ff491cd/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="摸鱼大数据——Spark基础——Spark On Yarn环境配置和部署">
  <meta property="og:description" content="1、Spark On Yarn的本质 Spark专注于分布式计算；Yarn专注于资源管理。
Spark将资源管理的工作交给了Yarn来负责!
2、环境搭建和启动 环境搭建 1.修改spark-env.sh
cd /export/server/spark/conf
cp spark-env.sh.template spark-env.sh
vim /export/server/spark/conf/spark-env.sh
在最后面添加以下内容:
HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop
YARN_CONF_DIR=/export/server/hadoop/etc/hadoop
SPARK_HISTORY_OPTS=&#34;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&#34;
2.修改hadoop的yarn-site.xml
node1修改
cd /export/server/hadoop-3.3.0/etc/hadoop/
vim /export/server/hadoop-3.3.0/etc/hadoop/yarn-site.xml
在&lt;configuration&gt;下面添加以下内容:
&lt;!-- 设置yarn集群的内存分配方案 --&gt;
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
&lt;value&gt;20480&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;
&lt;value&gt;2048&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;
&lt;value&gt;2.1&lt;/value&gt;
&lt;/property&gt;
将其同步到其他两台
cd /export/server/hadoop/etc/hadoop
scp -r yarn-site.xml node2:$PWD
scp -r yarn-site.xml node3:$PWD
3.Spark设置历史服务地址
cd /export/server/spark/conf
cp spark-defaults.conf.template spark-defaults.conf
vim spark-defaults.conf
添加以下内容:
spark.eventLog.enabled true
spark.eventLog.dir hdfs://node1:8020/sparklog/
spark.eventLog.compress true
spark.yarn.historyServer.address node1:18080">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-02T07:15:00+08:00">
    <meta property="article:modified_time" content="2024-07-02T07:15:00+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">摸鱼大数据——Spark基础——Spark On Yarn环境配置和部署</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h4>1、Spark On Yarn的本质</h4> 
<p>Spark专注于分布式计算；Yarn专注于资源管理。</p> 
<p>Spark将资源管理的工作交给了Yarn来负责!</p> 
<p></p> 
<p></p> 
<h4>2、环境搭建和启动</h4> 
<h5>环境搭建</h5> 
<p></p> 
<p>1.修改spark-env.sh</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="vertical-align:top;width:448pt;"> <p style="text-align:justify;">cd /export/server/spark/conf</p> <p style="text-align:justify;">cp spark-env.sh.template spark-env.sh</p> <p style="text-align:justify;">vim /export/server/spark/conf/spark-env.sh</p> <p style="text-align:justify;"></p> <p style="text-align:justify;">在最后面添加以下内容:</p> <p style="text-align:justify;">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop</p> <p style="text-align:justify;"><span style="background-color:#ffff00;">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span></p> <p style="text-align:justify;">SPARK_HISTORY_OPTS="-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true"</p> <p style="text-align:justify;"></p> <img alt="" height="170" src="https://images2.imgbox.com/e5/09/geBZG7FU_o.png" width="748"></td></tr></tbody></table> 
<p></p> 
<p>2.修改hadoop的yarn-site.xml</p> 
<p>node1修改</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="vertical-align:top;width:448pt;"> <p style="text-align:justify;">cd /export/server/hadoop-3.3.0/etc/hadoop/</p> <p style="text-align:justify;">vim /export/server/hadoop-3.3.0/etc/hadoop/yarn-site.xml</p> <p style="text-align:justify;"></p> <p style="text-align:justify;">在&lt;configuration&gt;下面添加以下内容:</p> <p style="text-align:justify;">&lt;!-- 设置yarn集群的内存分配方案 --&gt;</p> <p style="text-align:justify;">    <span style="color:#ff0000;">&lt;property&gt;</span></p> <p style="text-align:justify;"><span style="color:#ff0000;">        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span></p> <p style="text-align:justify;"><span style="color:#ff0000;">        &lt;value&gt;20480&lt;/value&gt;</span></p> <p style="text-align:justify;"><span style="color:#ff0000;">    &lt;/property&gt;</span></p> <p style="text-align:justify;">    <span style="color:#ff0000;">&lt;property&gt;</span></p> <p style="text-align:justify;"><span style="color:#ff0000;">        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span></p> <p style="text-align:justify;"><span style="color:#ff0000;">        &lt;value&gt;2048&lt;/value&gt;</span></p> <p style="text-align:justify;"><span style="color:#ff0000;">    &lt;/property&gt;</span></p> <p style="text-align:justify;">    <span style="color:#ff0000;">&lt;property&gt;</span></p> <p style="text-align:justify;"><span style="color:#ff0000;">        &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span></p> <p style="text-align:justify;"><span style="color:#ff0000;">        &lt;value&gt;2.1&lt;/value&gt;</span></p> <p style="text-align:justify;"><span style="color:#ff0000;">&lt;/property&gt;</span></p> <p style="text-align:justify;"></p> <img alt="" height="365" src="https://images2.imgbox.com/de/7e/XZh9T8oK_o.png" width="747"></td></tr></tbody></table> 
<p></p> 
<p>将其同步到其他两台</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="vertical-align:top;width:448pt;"> <p style="text-align:justify;">cd /export/server/hadoop/etc/hadoop</p> <p style="text-align:justify;">scp -r yarn-site.xml node2:$PWD</p> <p style="text-align:justify;">scp -r yarn-site.xml node3:$PWD</p> </td></tr></tbody></table> 
<p></p> 
<p>3.Spark设置历史服务地址</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="vertical-align:top;width:448pt;"> <p style="text-align:justify;">cd /export/server/spark/conf</p> <p style="text-align:justify;">cp spark-defaults.conf.template spark-defaults.conf</p> <p style="text-align:justify;">vim spark-defaults.conf</p> <p style="text-align:justify;"></p> <p style="text-align:justify;">添加以下内容:</p> <p style="text-align:justify;">spark.eventLog.enabled                  true</p> <p style="text-align:justify;">spark.eventLog.dir                      hdfs://node1:8020/sparklog/</p> <p style="text-align:justify;">spark.eventLog.compress                 true</p> <p style="text-align:justify;"><span style="background-color:#ffff00;">spark.yarn.historyServer.address        node1:18080</span></p> <p style="text-align:justify;"></p> <img alt="" height="270" src="https://images2.imgbox.com/fd/37/rcbRhJ89_o.png" width="747"><p style="text-align:justify;"></p> <p style="text-align:justify;"><span style="background-color:#ffff00;">配置后, 需要在HDFS上创建 sparklog目录</span></p> <p style="text-align:justify;">hdfs dfs -mkdir -p /sparklog</p> </td></tr></tbody></table> 
<p>设置日志级别:</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="vertical-align:top;width:448pt;"> <p style="text-align:justify;">cd /export/server/spark/conf</p> <p style="text-align:justify;">cp log4j.properties.template log4j.properties</p> <p style="text-align:justify;">vim log4j.properties</p> <p style="text-align:justify;"></p> <p style="text-align:justify;">修改以下内容:</p> <img alt="" height="104" src="https://images2.imgbox.com/6c/2c/SahlV4fj_o.png" width="643"></td></tr></tbody></table> 
<p></p> 
<p>4.配置依赖spark jar包</p> 
<p><span style="color:#c00000;">当Spark Application应用提交运行在YARN上时，默认情况下，每次提交应用都需要将依赖</span><span style="background-color:#ffff00;"><span style="color:#c00000;">Spark相关jar包上传到YARN 集群中</span></span><span style="color:#c00000;">，为了节省提交时间和存储空间，将Spark相关jar包上传到HDFS目录中，</span>设置属性告知Spark Application应用。</p> 
<p></p> 
<table border="1" cellspacing="0"><tbody><tr><td style="vertical-align:top;width:448pt;"> <p style="text-align:justify;">hadoop fs -mkdir -p /spark/jars/</p> <p style="text-align:justify;">hadoop fs -put /export/server/spark/jars/* /spark/jars/</p> </td></tr></tbody></table> 
<p>修改spark-defaults.conf</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="vertical-align:top;width:448pt;"> <p style="text-align:justify;">cd /export/server/spark/conf</p> <p style="text-align:justify;">vim spark-defaults.conf</p> <p style="text-align:justify;">添加以下内容:</p> <p style="text-align:justify;">spark.yarn.jars  hdfs://node1:8020/spark/jars/*</p> </td></tr></tbody></table> 
<p></p> 
<p>5.启动服务</p> 
<p></p> 
<p>Spark Application运行在YARN上时，上述配置完成</p> 
<p>启动服务：HDFS、YARN、MRHistoryServer和Spark HistoryServer，命令如下：</p> 
<p></p> 
<p></p> 
<table border="1" cellspacing="0"><tbody><tr><td style="vertical-align:top;width:448pt;"> <p style="text-align:justify;">## 重启HDFS和YARN服务，在node1执行命令</p> <p style="text-align:justify;">先stop-all.sh，再执行<span style="background-color:#ffff00;">start-all.sh</span></p> <p style="text-align:justify;"><span style="background-color:#ffff00;"><span style="color:#0000ff;">注意：在onyarn模式下不需要启动start-all.sh（jps查看一下看到worker和master）</span></span></p> <p style="text-align:justify;">## 启动MRHistoryServer服务，在node1执行命令</p> <p style="text-align:justify;"><span style="background-color:#ffff00;">mr-jobhistory-daemon.sh start historyserver</span></p> <img alt="" height="277" src="https://images2.imgbox.com/35/56/OU44sczs_o.png" width="439"><p style="text-align:justify;"></p> <p style="text-align:justify;">## 启动Spark HistoryServer服务，，在node1执行命令</p> <p style="text-align:justify;"><span style="background-color:#ffff00;">/export/server/spark/sbin/start-history-server.sh</span></p> <img alt="" height="295" src="https://images2.imgbox.com/b1/d3/7sI9nDiz_o.png" width="493"></td></tr></tbody></table> 
<p></p> 
<p>Spark HistoryServer服务WEB UI页面地址：</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="vertical-align:top;width:459.3pt;"> <p style="text-align:justify;"><a href="http://node1:18080/" rel="nofollow" title="http://node1:18080/">http://node1:18080/</a></p> </td></tr></tbody></table> 
<p></p> 
<p>6.提交测试</p> 
<p>先将圆周率PI程序提交运行在YARN上，命令如下：</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="vertical-align:top;width:459.3pt;"> <p style="text-align:justify;">SPARK_HOME=/export/server/spark</p> <p style="text-align:justify;">${SPARK_HOME}/bin/spark-submit \</p> <p style="text-align:justify;">--master yarn \</p> <p style="text-align:justify;">--conf "spark.pyspark.driver.python=/root/anaconda3/bin/python3" \</p> <p style="text-align:justify;">--conf "spark.pyspark.python=/root/anaconda3/bin/python3" \</p> <p style="text-align:justify;">${SPARK_HOME}/examples/src/main/python/pi.py \</p> <p style="text-align:justify;">10</p> </td></tr></tbody></table> 
<p></p> 
<p>运行完成在YARN 监控页面截图如下：</p> 
<p><img alt="" height="195" src="https://images2.imgbox.com/e0/fa/4QnCIf6k_o.png" width="747"></p> 
<p></p> 
<p>设置资源信息，提交运行pi程序至YARN上，命令如下：</p> 
<table border="1" cellspacing="0" style="margin-left:6.75pt;"><tbody><tr><td style="vertical-align:top;width:459.3pt;"> <p style="text-align:justify;">SPARK_HOME=/export/server/spark</p> <p style="text-align:justify;">${SPARK_HOME}/bin/spark-submit \</p> <p style="text-align:justify;">--master yarn \</p> <p style="text-align:justify;">--driver-memory 512m \</p> <p style="text-align:justify;">--executor-memory 512m \</p> <p style="text-align:justify;">--executor-cores 1 \</p> <p style="text-align:justify;">--num-executors 2 \</p> <p style="text-align:justify;">--queue default \</p> <p style="text-align:justify;">--conf "spark.pyspark.driver.python=/root/anaconda3/bin/python3" \</p> <p style="text-align:justify;">--conf "spark.pyspark.python=/root/anaconda3/bin/python3" \</p> <p style="text-align:justify;">${SPARK_HOME}/examples/src/main/python/pi.py \</p> <p style="text-align:justify;">10</p> </td></tr></tbody></table> 
<p></p> 
<p>当pi应用运行YARN上完成以后，从8080 WEB 页面点击应用历史服务连接，查看应用运行状态信息。</p> 
<p><img alt="" height="271" src="https://images2.imgbox.com/26/f6/VPGXqSqo_o.png" width="747"></p> 
<p></p> 
<p><strong>扩展：Spark On Yarn运行过程，为什么需要将Jar包上传到HDFS</strong></p> 
<p><img alt="" height="732" src="https://images2.imgbox.com/d7/cc/DwWkIipP_o.png" width="1124"></p> 
<p></p> 
<p><img alt="" height="591" src="https://images2.imgbox.com/90/02/5A1oXd0F_o.png" width="1200"></p> 
<blockquote> 
 <p>小技巧</p> 
 <p>1- vim相关快捷键：在文件中搜索关键词</p> 
 <p>输入/，然后输入关键词即可。按N进行跳转</p> 
 <p></p> 
 <p>2- 文件查找</p> 
 <p>find / -name spark-core_2.12-3.1.2.jar</p> 
 <p>在服务器上查找spark-core_2.12-3.1.2.jar文件,</p> 
 <p>默认配完环境后只有node1有,当node2,node3运行spark_on_yarn程序的时候,会自动从hdfs下载对应的jar包</p> 
</blockquote> 
<p></p> 
<h5>启动sparkonyarn</h5> 
<blockquote> 
 <p>注意:</p> 
 <p>配置完环境后需要先<strong>stop-all.sh</strong>停止,再重新启动服务生效</p> 
 <p>相比原理hadoop集群,需要多启动一个spark的自己的历史服务,它是依赖hadoop的历史服务的!</p> 
</blockquote> 
<pre>[root@node1 ~]# start-all.sh
[root@node1 ~]# mapred --daemon start historyserver
[root@node1 ~]# /export/server/spark/sbin/start-history-server.sh
[root@node1 ~]# jps
5570 HistoryServer
3555 DataNode
5460 JobHistoryServer
4199 NodeManager
4008 ResourceManager
3369 NameNode
29324 Jps
[root@node1 bin]# </pre> 
<p></p> 
<p></p> 
<h4>3、提交应用验证环境</h4> 
<p>测试提交官方示例圆周率</p> 
<pre>SPARK_HOME=/export/server/spark
${SPARK_HOME}/bin/spark-submit \
--master yarn \
--conf "spark.pyspark.driver.python=/root/anaconda3/bin/python3" \
--conf "spark.pyspark.python=/root/anaconda3/bin/python3" \
${SPARK_HOME}/examples/src/main/python/pi.py \
10</pre> 
<p>测试提交自定义python类</p> 
<blockquote> 
 <p>将编写的WordCount的代码提交到Yarn平台运行</p> 
 <ul><li> <p>注意1：需要修改setMaster的参数值为yarn，或者直接将setMaster代码删除</p> </li><li> <p>注意2：需要将代码中和文件路径相关的内容改成读写分布式文件系统，例如：HDFS。因为程序是分布式运行的，我们无法确保每台节点上面都有相同的本地文件路径，会导致报错。</p> </li></ul> 
</blockquote> 
<pre>cd /export/server/spark/bin
​
./spark-submit --master yarn \
--conf "spark.pyspark.driver.python=/root/anaconda3/bin/python3" \
--conf "spark.pyspark.python=/root/anaconda3/bin/python3" \
/export/data/spark_project/spark_base/05_词频统计案例_spark_on_yarn运行.py
​
# 注意:py代码文件路径改成自己的linux对应py文件路径!!!</pre> 
<p></p> 
<h4>4、如何查看日志</h4> 
<p><strong>日志用途：主要用于排查问题</strong></p> 
<ul><li> <p>通过Spark提供的18080日志服务器查看， <strong>推荐使用</strong></p> </li></ul> 
<p><img alt="" height="457" src="https://images2.imgbox.com/0e/ab/2era5Fqr_o.png" width="1119"></p> 
<p>直接查看对应executor的日志</p> 
<p><img alt="" height="530" src="https://images2.imgbox.com/b4/02/Z2wQRJ4K_o.png" width="1092"></p> 
<p></p> 
<p><img alt="" height="623" src="https://images2.imgbox.com/90/bc/c1oCKGqZ_o.png" width="1200"></p> 
<pre>发现不管是进程还是线程，日志内容都很少。原因如下：
1- 因为在进行环境部署的时候将日志级别，由INFO变成了WARN级别
2- 和部署方式有关。</pre> 
<p></p> 
<p>查看对应线程的日志</p> 
<p><img alt="" height="457" src="https://images2.imgbox.com/bf/fa/zpOp1YtT_o.png" width="1119"></p> 
<p><img alt="" height="543" src="https://images2.imgbox.com/8d/17/6mSXp0jT_o.png" width="950"></p> 
<p></p> 
<p><img alt="" height="530" src="https://images2.imgbox.com/48/d2/FuX0pwwT_o.png" width="905"></p> 
<p></p> 
<p><img alt="" height="298" src="https://images2.imgbox.com/f4/a6/VKZLz7UG_o.png" width="1101"></p> 
<p></p> 
<p></p> 
<p>还可以通过Yarn的8088界面查看日志</p> 
<p><img alt="" height="658" src="https://images2.imgbox.com/b2/79/ig1mw2pE_o.png" width="692"></p> 
<p><img alt="" height="352" src="https://images2.imgbox.com/fa/48/mLG0cOXN_o.png" width="1200"></p> 
<p></p> 
<p>点进去后， 如果日志比较多， 也可以点击查看详细日志</p> 
<p><img alt="" height="92" src="https://images2.imgbox.com/de/2e/JTsGWbUj_o.png" width="704"></p> 
<p></p> 
<h4>5、Spark On Yarn两种部署方式</h4> 
<p>Spark中有两种部署方式，Client和Cluster方式，默认是Client方式。这两种方式的本质区别，<strong>是Driver进程运行的地方不一样。</strong></p> 
<pre>Client部署方式: Driver进程运行在你提交程序的那台机器上
    优点: 将运行结果和运行日志全部输出到了提交程序的机器上，方便查看结果
    缺点: Driver进程和Yarn集群可能不在同一个集群中，会导致Driver和Executor进程间进行数据交换的时候，效率比较低
    使用: 一般用在开发和测试中
    
Cluster部署方式: Driver进程运行在集群中某个从节点上
    优点: Driver进程和Yarn集群在同一个集群中，Driver和Executor进程间进行数据交换的时候，效率比较高
    缺点: 需要去18080或者8088页面查看日志和运行结果
    使用: 一般用在生产环境使用</pre> 
<p></p> 
<p>演示操作：</p> 
<ul><li> <h5>client模式：</h5> </li></ul> 
<pre>./spark-submit --master yarn \
--deploy-mode client \
--conf "spark.pyspark.driver.python=/root/anaconda3/bin/python3" \
--conf "spark.pyspark.python=/root/anaconda3/bin/python3" \
/export/data/spark_project/spark_base/05_词频统计案例_spark_on_yarn运行.py
​
# 注意:py代码文件路径改成自己的linux虚拟机中的对应py文件路径!!!</pre> 
<p><img alt="" height="298" src="https://images2.imgbox.com/17/b8/QfftbR4Y_o.png" width="1056"></p> 
<p><img alt="" height="212" src="https://images2.imgbox.com/1e/d2/rZAsuP69_o.png" width="1200"></p> 
<p></p> 
<p></p> 
<ul><li> <h5>cluster模式</h5> </li></ul> 
<pre>./spark-submit --master yarn \
--deploy-mode cluster \
--conf "spark.pyspark.driver.python=/root/anaconda3/bin/python3" \
--conf "spark.pyspark.python=/root/anaconda3/bin/python3" \
/export/data/spark_project/spark_base/05_词频统计案例_spark_on_yarn运行.py
​
# 注意:py代码文件路径改成自己的linux对应py文件路径!!!</pre> 
<p><img alt="" height="205" src="https://images2.imgbox.com/30/79/g183ZmNt_o.png" width="1042"></p> 
<p><img alt="" height="267" src="https://images2.imgbox.com/7d/dc/geEPnI6Z_o.png" width="1200"></p> 
<p><img alt="" height="390" src="https://images2.imgbox.com/ac/bf/DGWvvrOM_o.png" width="950"></p> 
<p></p> 
<pre>观察两种模式结果区别?
​
client模式:  driver一直是node1(客户端)      运行输出结果在node1窗口直接输出
​
cluter模式:  driver是集群节点中切换           运行输出结果在driver对应stdout日志展示出</pre> 
<p></p> 
<p>如果输出内容不是直接打印而是到hdfs,有可能报错,看日志可以从以下位置找:</p> 
<p><img alt="" height="306" src="https://images2.imgbox.com/31/a2/VM24HwrP_o.png" width="1132"></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ada618c82c11640ab42b3b3a9fe4b490/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">（超详细）数据结构——“队列”的深度解析</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6acefca7a7be9a5bf3c56e16c2bb60bb/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python酷库之旅-第三方库openpyxl(20)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>