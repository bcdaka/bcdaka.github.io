<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【学习笔记】：Ubuntu 22 使用模型量化工具llama.cpp部署大模型 CPU&#43;GPU - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/2f3752806d0cec292e6e4a719a831089/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【学习笔记】：Ubuntu 22 使用模型量化工具llama.cpp部署大模型 CPU&#43;GPU">
  <meta property="og:description" content="学习笔记：Ubuntu 22 使用模型量化工具llama.cpp部署大模型 CPU&#43;GPU 前言1 下载并编译llama.cpp1.1 git下载llama.cpp仓库源码1.2 编译源码（make）1.2.1 选择一：仅在CPU上运行1.2.2 选择二：使用GPU，与cuBLAS编译 2 量化大模型2.1 准备大模型2.2 生成量化模型 3 加载模型3.1 CPU3.2 GPU 4 llama-cpp-python4.1 安装llama-cpp-python4.2 API 参考 前言 官方仓库：
llama.cpp
llama-cpp-python
环境：
CUDA Version: 12.2
Torch: 2.1.1
Python: 3.9
1 下载并编译llama.cpp 1.1 git下载llama.cpp仓库源码 由于服务器git上不去，先下载源码到本地再上传到服务器（带有.git隐藏文件）。
git clone https://github.com/ggerganov/llama.cpp 1.2 编译源码（make） 生成./main和./quantize等二进制文件。
cd llama.cpp 1.2.1 选择一：仅在CPU上运行 make 1.2.2 选择二：使用GPU，与cuBLAS编译 使用 Nvidia GPU 的 CUDA 内核提供 BLAS 加速，确保设备上有GPU&#43;CUDA。
make LLAMA_CUBLAS=1 LLAMA_CUDA_NVCC=/usr/local/cuda/bin/nvcc 如果遇到错误：
可尝试
make clean cd scripts sed -i &#39;s/\r//&#39; build-info.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-03-18T15:45:15+08:00">
    <meta property="article:modified_time" content="2024-03-18T15:45:15+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【学习笔记】：Ubuntu 22 使用模型量化工具llama.cpp部署大模型 CPU&#43;GPU</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>学习笔记：Ubuntu 22 使用模型量化工具llama.cpp部署大模型 CPU+GPU</h4> 
 <ul><li><a href="#_7" rel="nofollow">前言</a></li><li><a href="#1_llamacpp_20" rel="nofollow">1 下载并编译llama.cpp</a></li><li><ul><li><a href="#11_gitllamacpp_21" rel="nofollow">1.1 git下载llama.cpp仓库源码</a></li><li><a href="#12_make_29" rel="nofollow">1.2 编译源码（make）</a></li><li><ul><li><a href="#121_CPU_34" rel="nofollow">1.2.1 选择一：仅在CPU上运行</a></li><li><a href="#122_GPUcuBLAS_39" rel="nofollow">1.2.2 选择二：使用GPU，与cuBLAS编译</a></li></ul> 
  </li></ul> 
  </li><li><a href="#2__58" rel="nofollow">2 量化大模型</a></li><li><ul><li><a href="#21__59" rel="nofollow">2.1 准备大模型</a></li><li><a href="#22__63" rel="nofollow">2.2 生成量化模型</a></li></ul> 
  </li><li><a href="#3__77" rel="nofollow">3 加载模型</a></li><li><ul><li><a href="#31_CPU_79" rel="nofollow">3.1 CPU</a></li><li><a href="#32_GPU_96" rel="nofollow">3.2 GPU</a></li></ul> 
  </li><li><a href="#4_llamacpppython_104" rel="nofollow">4 llama-cpp-python</a></li><li><ul><li><a href="#41_llamacpppython_107" rel="nofollow">4.1 安装llama-cpp-python</a></li><li><a href="#42_API_117" rel="nofollow">4.2 API</a></li></ul> 
  </li><li><a href="#_150" rel="nofollow">参考</a></li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_7"></a>前言</h2> 
<p>官方仓库：<br>   <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a><br>   <a href="https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#high-level-api">llama-cpp-python</a><br> 环境：<br>   CUDA Version: 12.2<br>   Torch: 2.1.1<br>   Python: 3.9</p> 
<hr> 
<h2><a id="1_llamacpp_20"></a>1 下载并编译llama.cpp</h2> 
<h3><a id="11_gitllamacpp_21"></a>1.1 git下载llama.cpp仓库源码</h3> 
<p>由于服务器git上不去，先下载源码到本地再上传到服务器（带有.git隐藏文件）。</p> 
<pre><code class="prism language-powershell">git clone https:<span class="token operator">/</span><span class="token operator">/</span>github<span class="token punctuation">.</span>com/ggerganov/llama<span class="token punctuation">.</span><span class="token function">cpp</span>
</code></pre> 
<h3><a id="12_make_29"></a>1.2 编译源码（make）</h3> 
<p>生成./main和./quantize等二进制文件。</p> 
<pre><code class="prism language-powershell">cd llama<span class="token punctuation">.</span><span class="token function">cpp</span>
</code></pre> 
<h4><a id="121_CPU_34"></a>1.2.1 选择一：仅在CPU上运行</h4> 
<pre><code class="prism language-powershell">make
</code></pre> 
<h4><a id="122_GPUcuBLAS_39"></a>1.2.2 选择二：使用GPU，与cuBLAS编译</h4> 
<p>使用 Nvidia GPU 的 CUDA 内核提供 BLAS 加速，确保设备上有GPU+CUDA。</p> 
<pre><code class="prism language-powershell">make LLAMA_CUBLAS=1 LLAMA_CUDA_NVCC=<span class="token operator">/</span>usr/local/cuda/bin/nvcc
</code></pre> 
<p>如果遇到错误：<br> <img src="https://images2.imgbox.com/52/86/KoF8DrZc_o.png" alt="在这里插入图片描述"><br> 可尝试</p> 
<pre><code class="prism language-powershell">make clean
cd scripts
sed <span class="token operator">-</span>i <span class="token string">'s/\r//'</span> build-info<span class="token punctuation">.</span>sh
make LLAMA_CUBLAS=1 LLAMA_CUDA_NVCC=<span class="token operator">/</span>usr/local/cuda/bin/nvcc
</code></pre> 
<hr> 
<h2><a id="2__58"></a>2 量化大模型</h2> 
<h3><a id="21__59"></a>2.1 准备大模型</h3> 
<p>llama.cpp支持转换的模型格式有PyTorch 的.pth、huggingface的 .safetensors、还有之前 llamma.cpp 采用的 ggmlv3。<br> 在 huggingface 上找到合适格式的模型，下载至 llama.cpp 的 models目录下。<br> 或本地已下载的模型上传至models目录。</p> 
<h3><a id="22__63"></a>2.2 生成量化模型</h3> 
<p>quantize 提供各种精度的量化。量化会损失精度.（参考<a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/llamacpp_zh">WIKI最后部分</a>、<a href="https://zhuanlan.zhihu.com/p/666569378#:~:text=q4_0%EF%BC%9A%E6%A0%87%E5%87%86int4%E9%87%8F%E5%8C%96%E3%80%82,%E5%85%B7%E4%BD%93%E6%9D%A5%E8%AF%B4%EF%BC%8C%E6%AF%8F%E4%B8%AAchunk%E5%86%85%E9%83%A8%E6%9C%8932%E4%B8%AAint4%E6%9D%83%E9%87%8D%EF%BC%88%E8%BF%99%E6%A0%B7%E4%B8%80%E5%85%B1128bit%EF%BC%89%EF%BC%8C%E7%84%B6%E5%90%8E%E8%BF%98%E6%9C%89%E4%B8%80%E4%B8%AAfp32%E7%9A%84scale%EF%BC%8832bit%EF%BC%89%EF%BC%8C%E8%BF%99%E6%A0%B7%E4%B8%80%E5%85%B1160%E4%B8%AAbit%EF%BC%8C%E7%9B%B8%E5%BD%93%E4%BA%8E%E6%AF%8F%E4%B8%AA%E6%9D%83%E9%87%8D%E5%B9%B3%E5%9D%87%E5%8D%A05%E4%B8%AAbit%E3%80%82" rel="nofollow">LLM量化笔记</a>）<br> 仅需要近似于Q4_0或者Q4_1的效果（模型大小在3.5～3.9G），可以使用的是Q4_K_M，效果相比标准Q4_1模型变小了（3.9G-&gt;3.8G），ppl也变小了（0.1846-&gt;0.0535）。</p> 
<p><strong>先将模型转为GGUF的FP16格式</strong></p> 
<pre><code class="prism language-powershell">python3 convert<span class="token punctuation">.</span>py <span class="token punctuation">.</span><span class="token operator">/</span>models/chinese-alpaca-2-7b-hf/ 
</code></pre> 
<p><strong>再对FP16模型进行4-bit量化</strong></p> 
<pre><code class="prism language-powershell"><span class="token punctuation">.</span><span class="token operator">/</span>quantize <span class="token punctuation">.</span><span class="token operator">/</span>models/chinese-alpaca-2-7b-hf/ggml-model-f16<span class="token punctuation">.</span>gguf <span class="token punctuation">.</span><span class="token operator">/</span>models/chinese-alpaca-2-7b-hf/ggml-model-q4_0<span class="token punctuation">.</span>gguf Q4_0
</code></pre> 
<hr> 
<h2><a id="3__77"></a>3 加载模型</h2> 
<p>（涉及加载模型后部分模式的应用）</p> 
<h3><a id="31_CPU_79"></a>3.1 CPU</h3> 
<pre><code class="prism language-powershell"><span class="token punctuation">.</span><span class="token operator">/</span>main <span class="token operator">-</span>m <span class="token punctuation">.</span><span class="token operator">/</span>models/chinese-alpaca-2-7b-hf/ggml-model-q4_0<span class="token punctuation">.</span>gguf <span class="token operator">-</span>n 128 <span class="token operator">--</span>prompt <span class="token string">"Once upon a time"</span>
</code></pre> 
<p>main模式部分参数：</p> 
<ul><li>m 指定模型</li><li>ins 交互模式，可以连续对话，上下文会保留</li><li>c 控制上下文的长度，值越大越能参考更长的对话历史（默认：512）</li><li>n 控制回复生成的最大长度（默认：128） –temp 温度系数，值越低回复的随机性越小</li></ul> 
<pre><code class="prism language-powershell"><span class="token comment">#以交互式对话</span>
<span class="token punctuation">.</span><span class="token operator">/</span>main <span class="token operator">-</span>m <span class="token punctuation">.</span><span class="token operator">/</span>models/chinese-alpaca-2-7b-hf/ggml-model-q4_0<span class="token punctuation">.</span>gguf <span class="token operator">--</span>color <span class="token operator">-</span>f prompts/alpaca<span class="token punctuation">.</span>txt <span class="token operator">-</span>ins <span class="token operator">-</span>c 2048 <span class="token operator">--</span>temp 0<span class="token punctuation">.</span>2 <span class="token operator">-</span>n 256 <span class="token operator">--</span>repeat_penalty 1<span class="token punctuation">.</span>3
<span class="token comment">#chat with bob</span>
<span class="token punctuation">.</span><span class="token operator">/</span>main <span class="token operator">-</span>m <span class="token punctuation">.</span><span class="token operator">/</span>models/chinese-alpaca-2-7b-hf/ggml-model-q4_0<span class="token punctuation">.</span>gguf <span class="token operator">-</span>n 256 <span class="token operator">--</span>repeat_penalty 1<span class="token punctuation">.</span>0 <span class="token operator">--</span>color <span class="token operator">-</span>i <span class="token operator">-</span>r <span class="token string">"User:"</span> <span class="token operator">-</span>f prompts/chat-with-bob<span class="token punctuation">.</span>txt
</code></pre> 
<p><a href="https://zhuanlan.zhihu.com/p/651168655" rel="nofollow">参考</a></p> 
<h3><a id="32_GPU_96"></a>3.2 GPU</h3> 
<p>CPU处理速度较慢，可以选择使用GPU进行加速。添加<strong>n_gpu_layers</strong>参数，让一些层在GPU上跑，提升推理的速度，具体数值视情况而定。</p> 
<pre><code class="prism language-powershell"><span class="token punctuation">.</span><span class="token operator">/</span>main <span class="token operator">-</span>m <span class="token punctuation">.</span><span class="token operator">/</span>models/chinese-alpaca-2-7b-hf/ggml-model-q4_0<span class="token punctuation">.</span>gguf <span class="token operator">-</span>n 128 <span class="token operator">--</span>n_gpu_layers 40 <span class="token operator">--</span>prompt <span class="token string">"Once upon a time"</span>
</code></pre> 
<p><strong>其他详细examples内容请参考<a href="https://github.com/ggerganov/llama.cpp/tree/master/examples/main">官方文档</a>。</strong></p> 
<hr> 
<h2><a id="4_llamacpppython_104"></a>4 llama-cpp-python</h2> 
<p>可以借助<a href="https://github.com/abetlen/llama-cpp-python">llama-cpp-python</a>的API编写python程序，读取文件进行文本生成任务。</p> 
<h3><a id="41_llamacpppython_107"></a>4.1 安装llama-cpp-python</h3> 
<pre><code class="prism language-powershell">pip install llama-<span class="token function">cpp</span><span class="token operator">-</span>python
</code></pre> 
<p>注：若需要GPU加速，要在安装前设置环境变量：<strong>LLAMA_CUBLAS=on</strong>。若加速失败下滑见后文参考。</p> 
<pre><code class="prism language-powershell">CMAKE_ARGS=<span class="token string">"-DLLAMA_CUBLAS=on"</span> pip install llama-<span class="token function">cpp</span><span class="token operator">-</span>python
</code></pre> 
<h3><a id="42_API_117"></a>4.2 API</h3> 
<p>高级API使用实例可直接参考仓库<a href="https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#high-level-api">High-level API</a>部分，使用GPU加速修改n_gpu_layers值即可。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> llama_cpp <span class="token keyword">import</span> Llama
llm <span class="token operator">=</span> Llama<span class="token punctuation">(</span>
      model_path<span class="token operator">=</span><span class="token string">"./models/7B/llama-model.gguf"</span><span class="token punctuation">,</span>
      <span class="token comment"># n_gpu_layers=-1, # Uncomment to use GPU acceleration</span>
      <span class="token comment"># seed=1337, # Uncomment to set a specific seed</span>
      <span class="token comment"># n_ctx=2048, # Uncomment to increase the context window</span>
<span class="token punctuation">)</span>
output <span class="token operator">=</span> llm<span class="token punctuation">(</span>
      <span class="token string">"Q: Name the planets in the solar system? A: "</span><span class="token punctuation">,</span> <span class="token comment"># Prompt</span>
      max_tokens<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token comment"># Generate up to 32 tokens, set to None to generate up to the end of the context window</span>
      stop<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"Q:"</span><span class="token punctuation">,</span> <span class="token string">"\n"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token comment"># Stop generating just before the model would generate a new question</span>
      echo<span class="token operator">=</span><span class="token boolean">True</span> <span class="token comment"># Echo the prompt back in the output</span>
<span class="token punctuation">)</span> <span class="token comment"># Generate a completion, can also call create_completion</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">)</span>
</code></pre> 
<p><strong><font color="Red">若安装后不能成功使用GPU加速</font>，参考<a href="https://michaelriedl.com/2023/09/10/llama2-install-gpu.html" rel="nofollow">此解决方案</a>重新安装：</strong></p> 
<pre><code class="prism language-powershell">conda install <span class="token operator">-</span>c <span class="token string">"nvidia/label/cuda-11.8.0"</span> cuda-toolkit cuda-nvcc <span class="token operator">-</span>y <span class="token operator">--</span><span class="token function">copy</span>
CMAKE_ARGS=<span class="token string">"-DLLAMA_CUBLAS=on"</span> FORCE_CMAKE=1 pip install <span class="token operator">--</span>upgrade <span class="token operator">--</span>force-reinstall llama-<span class="token function">cpp</span><span class="token operator">-</span>python <span class="token operator">--</span>no-cache-<span class="token function">dir</span>
</code></pre> 
<hr> 
<h2><a id="_150"></a>参考</h2> 
<blockquote> 
 <p>[1] <a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/llama.cpp%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2">llama.cpp量化部署</a><br> [2]<a href="https://www.chenshaowen.com/blog/llama-cpp-that-is-a-llm-deployment-tool.html" rel="nofollow">大模型部署工具 llama.cpp</a><br> [3]<a href="https://michaelriedl.com/2023/09/10/llama2-install-gpu.html" rel="nofollow">Installing llama-cpp-python with GPU Support</a></p> 
</blockquote> 
<p>（个人学习参考笔记，如有不妥烦请告知）</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ecca7d8ceb9c7997b88276ad224d6cb2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">web期末作业网页设计——制作作业实例（网页源码 ——全网最全,建议收藏) HTML&#43;CSS&#43;JS (2）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f8006e864e9ffa0c466d426a5fcb72c3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">llama factory 参数体系EvaluationArguments、DataArguments、FinetuningArguments、FreezeArguments、LoraArgument</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>