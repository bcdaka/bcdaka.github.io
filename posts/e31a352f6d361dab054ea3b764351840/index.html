<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>python中的数据分析（juypter） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/e31a352f6d361dab054ea3b764351840/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="python中的数据分析（juypter）">
  <meta property="og:description" content="加载数据后的套路
df.head() df.info() df.describe() 选择部分数据
df[[要选中的列名的列表]] df.loc[,] df.iloc[,] df.query() 增加
df[新列名] = [新值] df.insert(loc , column,value =) 删除
df.drop() df.drop_duplicates() axis =0 可以改成1
inplace
修改数据
df.iloc[0,0] = 10 def func(x): return x s.apply(func) # 自定义处理，当修改的逻辑比较复杂的时候 df.apply(func ,axis = ) df.replace(to_replace = ,value=) 修改表结构
index
df.set_index() # 把df中一列数据变成行索引 df.reset_index() # 重置索引， 当前的索引会变成数据中的一列， 会添加一个从0开始计数的整数索引 df.rename(index = {}，columns={}) df.columns = [] df.index = [] df.replace/df.rename 共同的特点， 老的值没找到， 不会报错， 正常执行， 不会改值
df.insert 是修改数据的API中， 没有inplace 参数的一个 ， 直接在原来数据上进行修改">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-12T19:44:07+08:00">
    <meta property="article:modified_time" content="2024-06-12T19:44:07+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">python中的数据分析（juypter）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3></h3> 
<p>加载数据后的套路</p> 
<pre>df.head()
df.info()
df.describe()</pre> 
<p>选择部分数据</p> 
<pre>df[[要选中的列名的列表]]
df.loc[,]
df.iloc[,]
df.query()</pre> 
<p>增加</p> 
<pre>df[新列名] = [新值]
df.insert(loc , column,value =)</pre> 
<p>删除</p> 
<pre>df.drop()
df.drop_duplicates()</pre> 
<blockquote> 
 <p>axis =0 可以改成1</p> 
 <p>inplace</p> 
</blockquote> 
<p>修改数据</p> 
<pre>df.iloc[0,0] = 10
def func(x):
    return x
s.apply(func) # 自定义处理，当修改的逻辑比较复杂的时候
df.apply(func ,axis = )
df.replace(to_replace = ,value=)</pre> 
<p>修改表结构</p> 
<p>index</p> 
<pre>df.set_index() # 把df中一列数据变成行索引
df.reset_index() # 重置索引， 当前的索引会变成数据中的一列， 会添加一个从0开始计数的整数索引
df.rename(index = {}，columns={})
df.columns = []
df.index = []</pre> 
<blockquote> 
 <p>df.replace/df.rename 共同的特点， 老的值没找到， 不会报错， 正常执行， 不会改值</p> 
 <p>df.insert 是修改数据的API中， 没有inplace 参数的一个 ， 直接在原来数据上进行修改</p> 
</blockquote> 
<p>s.value_counts()</p> 
<p>s.unique()</p> 
<p>s.sort_values()</p> 
<p></p> 
<h3>1 常用排序和统计函数</h3> 
<h4>1 排序</h4> 
<p>sort_values()</p> 
<pre># by 指定排序的字段  可以传入列表，做多字段排序， 比如下面的例子， 当价格相同的时候， 价格相同的数据再按面积排序
# ascending 也可以传入列表，长度更by 的列表相同， 多字段排序的时候， 每个字段是升序还是降序排可以手动指定
df.sort_values(by=['价格','面积'],ascending=False)</pre> 
<p>nlargest()</p> 
<p>nsmallest()</p> 
<pre>df.nlargest(5,columns=['价格'])  # 价格 n个最大的
df.nsmallest(5,columns=['价格'])  # n个最小的</pre> 
<h4>2 统计函数</h4> 
<p>df.corr() 计算相关性， 判断两列数据是否同增同减</p> 
<ul><li> <p>如果两列变量 一个增加， 另外一个也增加， 一个减少另外一个也减少， 说明他们之间具备正相关性 ， 计算出来的相关系数&gt;0 相关系数&gt;0.7 强相关 0.3~0.7 之间 具有相关性 &lt;0.3 相关性比较弱</p> </li><li> <p>如果两列变量 一个增加， 另外一个减少， 一个减少另外一个增加， 说明他们之间具备负相关性</p> </li></ul> 
<p>相关性的应用场景</p> 
<ul><li> <p>用来判断不同数据之间是否有关联， 如果两列数据相关性比较强， 说明他们之间可能具有因果关系</p> </li><li> <p>在数据分析过程中， 归因分析是比较重要的， 做归因可以从计算相关性开始进行分析</p> </li><li> <p><strong>相关不等于因果</strong></p> </li></ul> 
<p>df.min() 最小 df.max() 最大 df.mean() 平均 df.std() 标准差 df.sum() 求和</p> 
<ul><li> <p>计算这些统计量的时候， 如果df中又多列数值型的数据， 既可以按行计算， 也可以按列计算， 通过传入0,1来进行控制</p> </li></ul> 
<h3>2 缺失值处理</h3> 
<h4>2.1 认识缺失值， Pandas如何表示缺失值</h4> 
<pre>from numpy import NaN,NAN,nan
</pre> 
<blockquote> 
 <p>Nan 是一个特殊的float类型的数据， 它和任何值都不相等</p> 
</blockquote> 
<h4>2.2 加载数据后， 如何判断缺失值数量</h4> 
<pre>pd.read_csv('C:/Develop/顺义48/day01/02_代码/data/city_day.csv',keep_default_na=True,na_values='Ahmedabad')
# keep_default_na 默认是True 数据中有空值会加载成NaN  如果改成False 空值就是空白内容  
# na_values='Ahmedabad' 可以指定 哪些值可以作为缺失值处理
</pre> 
<blockquote> 
 <p>如果没有特殊需求， 正常加载数据， 缺失会被加载成NAN</p> 
</blockquote> 
<pre>df.isnull() # 是空值 返回True 不是返回False
df.notnull()  # 不是空值 返回True 是返回False
</pre> 
<p>计算每一列缺失值的数量</p> 
<pre>df.isnull().sum()
</pre> 
<h4>2.3 缺失值的处理</h4> 
<h5>删除缺失值</h5> 
<ul><li> <p>使用<code>dropna</code>函数来删除空值，具体用法如下</p> <pre># 函数用法
df.dropna(    
    axis=0,     
    how='any',     
    inplace=True,     
    subset=['列名',...],    
    thresh=10
)

df.drop() # 按列删除
</pre> </li><li> <p><code>dropna</code>函数参数解释</p> 
  <ul><li> <p><code>axis=0</code></p> 
    <ul><li> <p>可选参数 ，默认为0按行删</p> </li><li> <p>0, or 'index'：删除包含缺失值的行</p> </li><li> <p>1, or 'columns'：删除包含缺失值的列</p> </li></ul></li><li> <p><code>how='any'</code></p> 
    <ul><li> <p>可选参数，默认为any</p> </li><li> <p>any: 如果存在NA值，则删除该行或列</p> </li><li> <p>all: 如果所有值都是NA，则删除该行或列</p> </li></ul></li><li> <p>inplace=False</p> 
    <ul><li> <p>可选参数，不建议使用这个参数</p> </li><li> <p>默认False, 不对原数据集进行修改</p> </li><li> <p>inplce=True，对原数据集进行修改</p> </li></ul></li><li> <p>subset接收一个列表</p> 
    <ul><li> <p>接收一个列表，列表中的元素为列名: 对特定的列进行缺失值删除处理</p> </li></ul></li><li> <p>thresh=n</p> 
    <ul><li> <p>可选参数</p> </li><li> <p>参数值为int类型，按行去除NaN值，去除NaN值后该行剩余数值的数量（列数）大于等于n，便保留这一行</p> </li></ul></li></ul></li></ul> 
<h5>缺失值填充</h5> 
<p>非时序数据</p> 
<ul><li> <p>考虑用默认值填充</p> </li><li> <p>使用统计量进行填充 均值， 中位数，众数</p> </li></ul> 
<pre>pm25_mean = df2['PM2.5'].mean()
df2['PM2.5'].fillna(pm25_mean)
</pre> 
<p>时序数据</p> 
<ul><li> <p>天气数据，股票的数据</p> </li><li> <p>跟时间相关， 前一个数据和后一个数据有一定关系</p> </li></ul> 
<p>时序数据填充可以考虑用前一个非空值， 后一个非空值填充，可以使用线性插值</p> 
<pre>s1 = df['Xylene'][54:64]

s1.fillna(method='ffill')

s1.fillna(method='bfill')

s1.interpolate()
</pre> 
<h3>3 数据类型转换</h3> 
<h4>3.1 数值型和字符串之间的转换</h4> 
<p>astype()</p> 
<p>pd.to_numeric()</p> 
<pre>import pandas as pd
df = pd.read_csv('C:/Develop/顺义48/day01/02_代码/data/city_day.csv')
df['PM2.5'] = df['PM2.5'].astype(object)
</pre> 
<pre>df2 = df.head().copy()
# 创造包含'missing'为缺失值的数据，批量替换第1、3、5行中NO列的值为字符串'missing'
df2.loc[::2, 'NO'] = 'missing'
df2.info()
df2['NO'].astype(float)
</pre> 
<blockquote> 
 <p>ValueError: could not convert string to float: 'missing'</p> 
</blockquote> 
<p>如果一列数据中， 有数值类型，也有字符串， astype转换为数值会报错，此时可以使用to_numeric()方法， 可以把字符串转换成空值</p> 
<pre>pd.to_numeric(df2['NO'],errors='coerce')
</pre> 
<h4>3.2 日期时间类型</h4> 
<p>datetime64 [ns]</p> 
<ul><li> <p>日期事件类型的列， 加载之后， 可以转换成日期事件类型， 就是datetime64[ns]</p> </li></ul> 
<pre>pd.to_datetime()
</pre> 
<ul><li> <p>也可以在加载的时候直接指定日期列， 直接加载成日期时间类型</p> </li></ul> 
<pre>pd.read_csv('',parse_dates = [日期列名])
</pre> 
<p>Timestamp 时间戳</p> 
<ul><li> <p>pd.to_datetime('时间点')</p> </li></ul> 
<p>timedelta64 时间差值</p> 
<p>两个时间相减</p> 
<p>使用的时候需要注意， 如果一列数据 Series 是datetime64 、 timedelta64 的时候， 获取时间维度的相关属性需要通过</p> 
<p>s.dt.XXX 来获取</p> 
<pre>print(df['Date'].dt.year) # 获取年
print(df['Date'].dt.month) # 月
print(df['Date'].dt.day) # 日
df['Date'].dt.quarter # 季度
</pre> 
<pre>s_time_delta = df['Date']-df['Date'].min()
s_time_delta.dt.days
</pre> 
<p></p> 
<p>datetime类型和timedelta类型都可以作为索引</p> 
<ul><li> <p>好处， 方便按照时间维度筛选数据</p> </li><li> <p>方便按照时间维度进行切片操作</p> </li></ul> 
<pre># Data 是日期时间类型列，  set_index 设置成行索引
df.set_index('Date', inplace=True)
# 在切片、按时间维度筛选数据之前， 先对日期时间索引排序
df = df.sort_index() # 对索引进行排序
df.loc['2018']
df.loc['2018-01-01']
df.loc['2018-02-01 22':'2018-02-02 23:59:59']
</pre> 
<blockquote> 
 <p>DatetimeIndex 类型 日期时间索引</p> 
</blockquote> 
<p>计算两列日期数据的差值， 得到timedelta类型的Series可以把它设置为index 索引， 得到的是 TimedeltaIndex</p> 
<pre>df2 = df.query('City == "Delhi"')
df2.index = df2['Date']-df2['Date'].min()
df2.index
df2['3 days':'4 days']
</pre> 
<p></p> 
<h3>4 分组聚合</h3> 
<p>df.grouppy([分组字段], as_index= )['聚合字段'].聚合方法()</p> 
<p>df.grouppy([分组字段], as_index= )['聚合字段'].agg(['聚合方法名'])</p> 
<p>df.grouppy([分组字段], as_index= ).agg({‘聚合字段名’:'聚合方法名','聚合字段名’:'聚合方法名'})</p> 
<ul><li> <p>分组字段，可以有1个多个， 默认分组的字段在分组的结果中会作为行索引， 如果设置了as_index = False ，分组字段会作为结果的列数据， 会使用从0开始的整数索引</p> </li><li> <p>聚合字段可以有1个多个， 可以通过agg来指定不同的字段， 使用不同的聚合方式</p> </li><li> <p>多个字段分组， 多个字段聚合， 得到的结果 MultiIndex 通过MultiIndex 做数据筛选， 传入的是元组</p> </li></ul> 
<pre>import pandas as pd
df = pd.read_csv('C:/Develop/顺义48/day01/02_代码/data/LJdata.csv')
#%%
df.groupby(['区域']).mean()
# FutureWarning 未来版本变化的提示， python 包版本升级之后， 可能一些方法， 或参数直接就变了， 或者删掉了
# 看到提示之后， 需要注意， 当前这份代码， pandas的版本不要升级
# python项目开发好之后， 开发时候用到的软件包， 不要升级


print(df.groupby('区域')['价格'].mean())
print(df.groupby('区域')[['价格']].mean())
df.groupby('区域')[['价格','面积']].mean()
print(df.groupby('区域')[['价格','面积']].agg(['mean','max']))
df_result= df.groupby('区域')[['价格','面积']].agg(['mean','max'])
df_result.columns
# MultiIndex([('价格', 'mean'),
#             ('价格',  'max'),
#             ('面积', 'mean'),
#             ('面积',  'max')],
#            )

print(df_result[[('价格', 'mean'),('价格',  'max')]])

df.groupby('区域').agg({'价格':'mean','面积':'max'})

df_result2 = df.groupby(['区域','户型']).agg({'价格':'mean','面积':'max'})

df_result2.index # 查看索引 当前对两个字段进行分组， 得到的是MultiIndex

df_result2.reset_index() # 重置索引

df.groupby(['区域','户型'],as_index=False).agg({'价格':'mean','面积':'max'})
</pre> 
<p>DataFrameGroupby对象 （了解）</p> 
<p>df.groupby('区域')['价格'].mean()</p> 
<ul><li> <p>df.groupby('区域') → 对象 DataFrameGroupby</p> </li><li> <pre>df_gb = df.groupby('区域')
df_gb.groups # 可以获取所有的分组 {'区域取值':[当前取值在数据中的行索引]}
df_gb.get_group('区域取值') → 这一组对应的DataFrame数据
</pre> </li><li> <p>df.groupby('区域')['价格'] → 对象 SeriesGroupby</p> </li></ul> 
<p></p> 
<p>自定义聚合函数</p> 
<ul><li> <p>df.groupby('分组字段')['聚合字段'].agg(自定义聚合函数对象)</p> </li></ul> 
<pre>## 自定义聚合函数
def my_mean(x):
    print(x)
    print('=====')
    return x.sum()/len(x)
df.groupby('区域')['价格'].agg(my_mean)
</pre> 
<h3>5 数据分组（分箱） pd.cut</h3> 
<p>使用场景：</p> 
<ul><li> <p>把年龄划分成少年， 青年， 中年， 老年</p> </li><li> <p>收入划分成， 低收入， 中收入， 高收入 。。</p> </li><li> <p>价格 便宜， 中等， 贵</p> </li></ul> 
<pre># 等距 每一组 边界差距， 尽量均匀
pd.cut(df['价格'],bins = 3) # bins 传入要分几组
# 可以自定义分组的边界
pd.cut(df['价格'],bins = [0,3000,8500,210000],labels=['便宜','中等','贵'])
# bins 默认区间是左开右闭合 (0,3000]  (3000,8500], (8500,210000] 需要注意最小的区间要比数据中的最小值小一些
# labels 可以指定分组之后， 每一组的名字，如果不指定默认使用的就是分组的边界的取值
</pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/35c62b94a415bfb663433342de374eb6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">基于C#开发web网页管理系统模板流程-登录界面和主界面</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/05f4caee103bb86e9d8877dbb5ffb0f5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">2024.6.12 作业 xyt</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>