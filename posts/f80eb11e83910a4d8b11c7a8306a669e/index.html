<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>多模态模型学习1——CLIP对比学习 语言-图像预训练模型 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/f80eb11e83910a4d8b11c7a8306a669e/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="多模态模型学习1——CLIP对比学习 语言-图像预训练模型">
  <meta property="og:description" content="多模态模型学习1——CLIP对比学习 语言-图像预训练模型 学习前言什么是CLIP模型代码下载CLIP实现思路一、网络结构介绍1、Image Encodera、Patch&#43;Position Embeddingb、Transformer EncoderI、Self-attention结构解析II、Self-attention的矩阵运算III、MultiHead多头注意力机制IV、TransformerBlock的构建。 c、整个VIT模型的构建 2、Text Encoder 二、训练部分 训练自己的CLIP模型一、数据集的准备二、数据集的格式三、开始网络训练四、训练结果预测 学习前言 学了一些多模态的知识，CLIP算是其中最重要也是最通用的一环，一起来看一下吧。
什么是CLIP模型 CLIP的全称是Contrastive Language-Image Pre-Training，中文是对比语言-图像预训练，是一个预训练模型，简称为CLIP。
该模型是 OpenAI 在 2021 年发布的，最初用于匹配图像和文本的预训练神经网络模型，这个任务在多模态领域比较常见，可以用于文本图像检索，CLIP是近年来在多模态研究领域的经典之作。该模型大量的成对互联网数据进行预训练，在很多任务表现上达到了目前最佳表现（SOTA） 。
CLIP的思想非常简单，只需要看懂这幅图就可以了，左边是训练的原理，CLIP一共有两个模态，一个是文本模态，一个是视觉模态，分别对应了Text Encoder和Image Encoder。
Text Encoder用于对文本进行编码，获得其Embedding；
Image Encoder用于对图片编码，获得其Embedding。
两个Embedding均为一定长度的单一向量。
在训练时，假设一个批次中有64个文本图像对，此时我们会同时获得64个图片和64个文本，首先我们从64个文本图像对中取出一个文本图像对，成对的文本图像对是天然的正样本，它们是配对的。
而对于这个样本的文本来讲，其它63个图像都为负样本，它们是不配对的。
而对于这个样本的图像来讲，其它63个文本都为负样本，它们是不配对的。
在这个批次中，64个文本图像对，可以获得的图像embedding和文本embedding为：
visual_embedding [64, embedding_size] text_embedding	[64, embedding_size] visual_embedding的第x行和text_embedding的第x行是成对的。
我们使用visual_embedding 叉乘 text_embedding，得到一个[64, 64]的矩阵，那么对角线上的值便是成对特征内积得到的，如果visual_embedding和对应的text_embedding越相似，那么它的值便越大。
我们选取[64, 64]矩阵中的第一行，代表第1个图片与64个文本的相似程度，其中第1个文本是正样本，我们将这一行的标签设置为1，那么我们就可以使用交叉熵进行训练，尽量把第1个图片和第一个文本的内积变得更大，那么它们就越相似。
每一行都做同样的工作，那么[64, 64]的矩阵，它的标签就是[1,2,3,4,5,6……,64]，在计算机中，标签从0开始，所以实际标签为[0,1,2,3,4,5……,63]。
代码下载 Github源码下载地址为：
https://github.com/bubbliiiing/clip-pytorch
复制该路径到地址栏跳转。
CLIP实现思路 一、网络结构介绍 1、Image Encoder a、Patch&#43;Position Embedding Patch&#43;Position Embedding的作用主要是对输入进来的图片进行分块处理，每隔一定的区域大小划分图片块。然后将划分后的图片块组合成序列。
该部分首先对输入进来的图片进行分块处理，处理方式其实很简单，使用的是现成的卷积。由于卷积使用的是滑动窗口的思想，我们只需要设定特定的步长，就可以输入进来的图片进行分块处理了。
在VIT中，我们常设置这个卷积的卷积核大小为16x16，步长也为16x16，此时卷积就会每隔16个像素点进行一次特征提取，由于卷积核大小为16x16，两个图片区域的特征提取过程就不会有重叠。当我们输入的图片是224, 224, 3的时候，我们可以获得一个14, 14, 768的特征层。
下一步就是将这个特征层组合成序列，组合的方式非常简单，就是将高宽维度进行平铺，14, 14, 768在高宽维度平铺后，获得一个196, 768的特征层。平铺完成后，我们会在图片序列中添加上Cls Token，该Token会作为一个单位的序列信息一起进行特征提取，图中的这个0*就是Cls Token，我们此时获得一个197, 768的特征层。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-04-05T15:14:19+08:00">
    <meta property="article:modified_time" content="2023-04-05T15:14:19+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">多模态模型学习1——CLIP对比学习 语言-图像预训练模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night-eighties">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>多模态模型学习1——CLIP对比学习 语言-图像预训练模型</h4> 
 <ul><li><a href="#_2" rel="nofollow">学习前言</a></li><li><a href="#CLIP_5" rel="nofollow">什么是CLIP模型</a></li><li><a href="#_33" rel="nofollow">代码下载</a></li><li><a href="#CLIP_38" rel="nofollow">CLIP实现思路</a></li><li><ul><li><a href="#_39" rel="nofollow">一、网络结构介绍</a></li><li><ul><li><a href="#1Image_Encoder_40" rel="nofollow">1、Image Encoder</a></li><li><ul><li><a href="#aPatchPosition_Embedding_41" rel="nofollow">a、Patch+Position Embedding</a></li><li><a href="#bTransformer_Encoder_89" rel="nofollow">b、Transformer Encoder</a></li><li><ul><li><a href="#ISelfattention_93" rel="nofollow">I、Self-attention结构解析</a></li><li><a href="#IISelfattention_110" rel="nofollow">II、Self-attention的矩阵运算</a></li><li><a href="#IIIMultiHead_156" rel="nofollow">III、MultiHead多头注意力机制</a></li><li><a href="#IVTransformerBlock_200" rel="nofollow">IV、TransformerBlock的构建。</a></li></ul> 
     </li><li><a href="#cVIT_229" rel="nofollow">c、整个VIT模型的构建</a></li></ul> 
    </li><li><a href="#2Text_Encoder_341" rel="nofollow">2、Text Encoder</a></li></ul> 
   </li><li><a href="#_407" rel="nofollow">二、训练部分</a></li></ul> 
  </li><li><a href="#CLIP_453" rel="nofollow">训练自己的CLIP模型</a></li><li><ul><li><a href="#_460" rel="nofollow">一、数据集的准备</a></li><li><a href="#_465" rel="nofollow">二、数据集的格式</a></li><li><a href="#_493" rel="nofollow">三、开始网络训练</a></li><li><a href="#_514" rel="nofollow">四、训练结果预测</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_2"></a>学习前言</h2> 
<p>学了一些多模态的知识，CLIP算是其中最重要也是最通用的一环，一起来看一下吧。<br> <img src="https://images2.imgbox.com/9b/04/mo0fhF5V_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="CLIP_5"></a>什么是CLIP模型</h2> 
<p>CLIP的全称是Contrastive Language-Image Pre-Training，中文是对比语言-图像预训练，是一个预训练模型，简称为CLIP。</p> 
<p>该模型是 OpenAI 在 2021 年发布的，最初用于匹配图像和文本的预训练神经网络模型，这个任务在多模态领域比较常见，可以用于文本图像检索，CLIP是近年来在多模态研究领域的经典之作。该模型大量的成对互联网数据进行预训练，在很多任务表现上达到了目前最佳表现（SOTA） 。</p> 
<p>CLIP的思想非常简单，只需要看懂这幅图就可以了，左边是训练的原理，CLIP一共有两个模态，一个是文本模态，一个是视觉模态，分别对应了Text Encoder和Image Encoder。<br> Text Encoder用于对文本进行编码，获得其Embedding；<br> Image Encoder用于对图片编码，获得其Embedding。<br> 两个Embedding均为一定长度的单一向量。</p> 
<p>在训练时，假设一个批次中有64个文本图像对，此时我们会同时获得64个图片和64个文本，首先我们从64个文本图像对中取出一个文本图像对，成对的文本图像对是天然的正样本，它们是配对的。</p> 
<p>而对于这个样本的文本来讲，其它63个图像都为负样本，它们是不配对的。<br> 而对于这个样本的图像来讲，其它63个文本都为负样本，它们是不配对的。</p> 
<p>在这个批次中，64个文本图像对，可以获得的图像embedding和文本embedding为：</p> 
<pre><code class="prism language-python">visual_embedding 	<span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">,</span> embedding_size<span class="token punctuation">]</span>
text_embedding		<span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">,</span> embedding_size<span class="token punctuation">]</span>
</code></pre> 
<p>visual_embedding的第x行和text_embedding的第x行是成对的。</p> 
<p>我们使用visual_embedding 叉乘 text_embedding，得到一个[64, 64]的矩阵，那么对角线上的值便是<strong>成对特征</strong>内积得到的，如果visual_embedding和对应的text_embedding越相似，那么它的值便越大。</p> 
<p><strong>我们选取[64, 64]矩阵中的第一行，代表第1个图片与64个文本的相似程度，其中第1个文本是正样本，我们将这一行的标签设置为1，那么我们就可以使用交叉熵进行训练，尽量把第1个图片和第一个文本的内积变得更大，那么它们就越相似。</strong></p> 
<p>每一行都做同样的工作，那么[64, 64]的矩阵，它的标签就是[1,2,3,4,5,6……,64]，在计算机中，标签从0开始，所以实际标签为[0,1,2,3,4,5……,63]。<br> <img src="https://images2.imgbox.com/e0/d3/8QBUfQMw_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_33"></a>代码下载</h2> 
<p>Github源码下载地址为：<br> https://github.com/bubbliiiing/clip-pytorch</p> 
<p>复制该路径到地址栏跳转。</p> 
<h2><a id="CLIP_38"></a>CLIP实现思路</h2> 
<h3><a id="_39"></a>一、网络结构介绍</h3> 
<h4><a id="1Image_Encoder_40"></a>1、Image Encoder</h4> 
<h5><a id="aPatchPosition_Embedding_41"></a>a、Patch+Position Embedding</h5> 
<p><img src="https://images2.imgbox.com/92/3f/w5DXkgQZ_o.png" alt="在这里插入图片描述"><br> Patch+Position Embedding的作用<strong>主要是对输入进来的图片进行分块处理</strong>，每隔一定的区域大小划分图片块。然后<strong>将划分后的图片块组合成序列</strong>。</p> 
<p>该部分首先对输入进来的图片进行分块处理，处理方式其实很简单，<strong>使用的是现成的卷积</strong>。由于卷积使用的是<strong>滑动窗口的思想</strong>，我们只需要<strong>设定特定的步长</strong>，就可以输入进来的图片进行<strong>分块处理了</strong>。</p> 
<p>在VIT中，<strong>我们常设置这个卷积的卷积核大小为16x16，步长也为16x16</strong>，此时卷积就会每隔16个像素点进行一次特征提取，由于卷积核大小为16x16，<strong>两个图片区域的特征提取过程就不会有重叠</strong>。<strong>当我们输入的图片是224, 224, 3的时候，我们可以获得一个14, 14, 768的特征层。</strong><br> <img src="https://images2.imgbox.com/30/3b/0aBQLvGZ_o.gif" alt="请添加图片描述"><br> 下一步就是将这个特征层组合成序列，<strong>组合的方式非常简单，就是将高宽维度进行平铺</strong>，14, 14, 768在高宽维度平铺后，<strong>获得一个196, 768的特征层</strong>。平铺完成后，我们会在图片序列中添加上Cls Token，<strong>该Token会作为一个单位的序列信息</strong>一起进行特征提取，<strong>图中的这个0*就是Cls Token</strong>，我们此时<strong>获得一个197, 768的特征层</strong>。<img src="https://images2.imgbox.com/bb/94/J9JyktfS_o.png" alt="在这里插入图片描述"><br> 添加完成Cls Token后，再<strong>为所有特征添加上位置信息</strong>，<strong>这样网络才有区分不同区域的能力</strong>。添加方式其实也非常简单，我们生成一个<strong>197, 768的参数矩阵</strong>，这个参数矩阵是可训练的，把这个矩阵加上<strong>197, 768的特征层</strong>即可。</p> 
<p>到这里，Patch+Position Embedding就构建完成了，构建代码如下：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">VisionTransformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_resolution<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> patch_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> width<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> layers<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> output_dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>input_resolution <span class="token operator">=</span> input_resolution
        self<span class="token punctuation">.</span>output_dim <span class="token operator">=</span> output_dim
        <span class="token comment">#-----------------------------------------------#</span>
        <span class="token comment">#   224, 224, 3 -&gt; 196, 768</span>
        <span class="token comment">#-----------------------------------------------#</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span>width<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span>patch_size<span class="token punctuation">,</span> stride<span class="token operator">=</span>patch_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

        scale <span class="token operator">=</span> width <span class="token operator">**</span> <span class="token operator">-</span><span class="token number">0.5</span>
        <span class="token comment">#--------------------------------------------------------------------------------------------------------------------#</span>
        <span class="token comment">#   class_embedding部分是transformer的分类特征。用于堆叠到序列化后的图片特征中，作为一个单位的序列特征进行特征提取。</span>
        <span class="token comment">#</span>
        <span class="token comment">#   在利用步长为16x16的卷积将输入图片划分成14x14的部分后，将14x14部分的特征平铺，一幅图片会存在序列长度为196的特征。</span>
        <span class="token comment">#   此时生成一个class_embedding，将class_embedding堆叠到序列长度为196的特征上，获得一个序列长度为197的特征。</span>
        <span class="token comment">#   在特征提取的过程中，class_embedding会与图片特征进行特征的交互。最终分类时，我们取出class_embedding的特征，利用全连接分类。</span>
        <span class="token comment">#--------------------------------------------------------------------------------------------------------------------#</span>
        <span class="token comment">#   196, 768 -&gt; 197, 768</span>
        self<span class="token punctuation">.</span>class_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>scale <span class="token operator">*</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>width<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment">#--------------------------------------------------------------------------------------------------------------------#</span>
        <span class="token comment">#   为网络提取到的特征添加上位置信息。</span>
        <span class="token comment">#   以输入图片为224, 224, 3为例，我们获得的序列化后的图片特征为196, 768。加上class_embedding后就是197, 768</span>
        <span class="token comment">#   此时生成的pos_Embedding的shape也为197, 768，代表每一个特征的位置信息。</span>
        <span class="token comment">#--------------------------------------------------------------------------------------------------------------------#</span>
        <span class="token comment">#   197, 768 -&gt; 197, 768</span>
        self<span class="token punctuation">.</span>positional_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>scale <span class="token operator">*</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>input_resolution <span class="token operator">//</span> patch_size<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> width<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># shape = [*, width, grid, grid]</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># shape = [*, width, grid ** 2]</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># shape = [*, grid ** 2, width]</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>class_embedding<span class="token punctuation">.</span>to<span class="token punctuation">(</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>x<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># shape = [*, grid ** 2 + 1, width]</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>positional_embedding<span class="token punctuation">.</span>to<span class="token punctuation">(</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
</code></pre> 
<h5><a id="bTransformer_Encoder_89"></a>b、Transformer Encoder</h5> 
<p><img src="https://images2.imgbox.com/10/2a/T2HB9hty_o.png" alt="在这里插入图片描述"><br> 在上一步<strong>获得shape为197, 768的序列信息</strong>后，将<strong>序列信息传入Transformer Encoder进行特征提取</strong>，这是Transformer特有的Multi-head Self-attention结构，<strong>通过自注意力机制，关注每个图片块的重要程度。</strong></p> 
<h6><a id="ISelfattention_93"></a>I、Self-attention结构解析</h6> 
<p>看懂Self-attention结构，其实看懂下面这个动图就可以了，动图中存在<strong>一个序列的三个单位输入</strong>，<strong>每一个序列单位的输入</strong>都可以通过<strong>三个处理（比如全连接）获得Query、Key、Value</strong>，Query是查询向量、Key是键向量、Value值向量。<br> <img src="https://images2.imgbox.com/2b/af/rGVQF6UH_o.gif" alt="请添加图片描述"><br> 如果我们想要获得input-1的输出，那么我们进行如下几步：<br> 1、利用<strong>input-1的查询向量</strong>，分别乘上<strong>input-1、input-2、input-3的键向量</strong>，此时我们获得了<strong>三个score</strong>。<br> 2、然后对<strong>这三个score取softmax</strong>，获得了<strong>input-1、input-2、input-3</strong>各自的重要程度。<br> 3、然后将这个重要程度乘上<strong>input-1、input-2、input-3</strong>的值向量，求和。<br> 4、此时我们获得了input-1的输出。</p> 
<p>如图所示，我们进行如下几步：<br> 1、<strong>input-1的查询向量为[1, 0, 2]</strong>，分别乘上<strong>input-1、input-2、input-3的键向量</strong>，获得三个score为2，4，4。<br> 2、然后对<strong>这三个score取softmax</strong>，获得了<strong>input-1、input-2、input-3</strong>各自的重要程度，获得三个重要程度为0.0，0.5，0.5。<br> 3、然后将这个重要程度乘上<strong>input-1、input-2、input-3</strong>的值向量，求和，即<br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         0.0 
        
       
         ∗ 
        
       
         [ 
        
       
         1 
        
       
         , 
        
       
         2 
        
       
         , 
        
       
         3 
        
       
         ] 
        
       
         + 
        
       
         0.5 
        
       
         ∗ 
        
       
         [ 
        
       
         2 
        
       
         , 
        
       
         8 
        
       
         , 
        
       
         0 
        
       
         ] 
        
       
         + 
        
       
         0.5 
        
       
         ∗ 
        
       
         [ 
        
       
         2 
        
       
         , 
        
       
         6 
        
       
         , 
        
       
         3 
        
       
         ] 
        
       
         = 
        
       
         [ 
        
       
         2.0 
        
       
         , 
        
       
         7.0 
        
       
         , 
        
       
         1.5 
        
       
         ] 
        
       
      
        0.0 * [1, 2, 3] + 0.5 * [2, 8, 0] + 0.5 * [2, 6, 3] = [2.0, 7.0, 1.5] 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0.0</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">3</span><span class="mclose">]</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0.5</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">8</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">0</span><span class="mclose">]</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0.5</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">6</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">3</span><span class="mclose">]</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord">2.0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">7.0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">1.5</span><span class="mclose">]</span></span></span></span></span>。<br> 4、此时我们获得了input-1的输出 [2.0, 7.0, 1.5]。</p> 
<p>上述的例子中，<strong>序列长度仅为3，每个单位序列的特征长度仅为3</strong>，在VIT的Transformer Encoder中，<strong>序列长度为197，每个单位序列的特征长度为768 // num_heads</strong>。但计算过程是一样的。在实际运算时，<strong>我们采用矩阵进行运算。</strong></p> 
<h6><a id="IISelfattention_110"></a>II、Self-attention的矩阵运算</h6> 
<p>实际的矩阵运算过程如下图所示。我以实际矩阵为例子给大家解析：<br> <img src="https://images2.imgbox.com/ef/43/GUi96F2b_o.png" alt="在这里插入图片描述"><br> 输入的Query、Key、Value如下图所示：<br> <img src="https://images2.imgbox.com/bf/02/xM0Q4TC6_o.png" alt="在这里插入图片描述"><br> 首先利用 <strong>查询向量query</strong> 叉乘 <strong>转置后的键向量key</strong>，这一步可以通俗的理解为，<strong>利用查询向量去查询序列的特征，获得序列每个部分的重要程度score。</strong></p> 
<p><strong>输出的每一行，都代表input-1、input-2、input-3，对当前input的贡献</strong>，我们对这个贡献值取一个softmax。<br> <img src="https://images2.imgbox.com/c6/e4/xlDWu5t5_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/98/db/3UsRn7oG_o.png" alt="在这里插入图片描述"><br> 然后利用 score 叉乘 value，<strong>这一步可以通俗的理解为，将序列每个部分的重要程度重新施加到序列的值上去。</strong><br> <img src="https://images2.imgbox.com/58/44/FqkMXSZi_o.png" alt="在这里插入图片描述"><br> 这个矩阵运算的代码如下所示，各位同学可以自己试试。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">def</span> <span class="token function">soft_max</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span><span class="token punctuation">:</span>
    t <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>z<span class="token punctuation">)</span>
    a <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>z<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> a

Query <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>

Key <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>

Value <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>

scores <span class="token operator">=</span> Query @ Key<span class="token punctuation">.</span>T
<span class="token keyword">print</span><span class="token punctuation">(</span>scores<span class="token punctuation">)</span>
scores <span class="token operator">=</span> soft_max<span class="token punctuation">(</span>scores<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>scores<span class="token punctuation">)</span>
out <span class="token operator">=</span> scores @ Value
<span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span>
</code></pre> 
<h6><a id="IIIMultiHead_156"></a>III、MultiHead多头注意力机制</h6> 
<p>多头注意力机制的示意图如图所示：<br> <img src="https://images2.imgbox.com/8a/81/GaUJmcqV_o.png" alt="在这里插入图片描述"><br> 这幅图给人的感觉略显迷茫，我们跳脱出这个图，<strong>直接从矩阵的shape入手会清晰很多。</strong></p> 
<p>在第一步进行图像的分割后，我们获得的特征层为197, 768。</p> 
<p>在施加多头的时候，<strong>我们直接对196, 768的最后一维度进行分割，比如我们想分割成12个头，那么矩阵的shepe就变成了196, 12, 64。</strong></p> 
<p>然后我们将196, 12, 64进行转置，将12放到前面去，获得的特征层为12, 196, 64。<strong>之后我们忽略这个12，把它和batch维度同等对待</strong>，<strong>只对196, 64进行处理</strong>，<strong>其实也就是上面的注意力机制的过程了</strong>。</p> 
<p>下面这个代码并未在CLIP中使用，CLIP直接使用了nn.MultiheadAttention模块计算多头注意力，下面这个代码是其他部分的Vision Transformer截取过来的，方便各位理解。</p> 
<pre><code class="prism language-python"><span class="token comment">#--------------------------------------------------------------------------------------------------------------------#</span>
<span class="token comment">#   Attention机制</span>
<span class="token comment">#   将输入的特征qkv特征进行划分，首先生成query, key, value。query是查询向量、key是键向量、v是值向量。</span>
<span class="token comment">#   然后利用 查询向量query 叉乘 转置后的键向量key，这一步可以通俗的理解为，利用查询向量去查询序列的特征，获得序列每个部分的重要程度score。</span>
<span class="token comment">#   然后利用 score 叉乘 value，这一步可以通俗的理解为，将序列每个部分的重要程度重新施加到序列的值上去。</span>
<span class="token comment">#--------------------------------------------------------------------------------------------------------------------#</span>
<span class="token keyword">class</span> <span class="token class-name">Attention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> num_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> qkv_bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> attn_drop<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span> proj_drop<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_heads  <span class="token operator">=</span> num_heads
        self<span class="token punctuation">.</span>scale      <span class="token operator">=</span> <span class="token punctuation">(</span>dim <span class="token operator">//</span> num_heads<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token operator">-</span><span class="token number">0.5</span>

        self<span class="token punctuation">.</span>qkv        <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>qkv_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attn_drop  <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>attn_drop<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj       <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj_drop  <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>proj_drop<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        B<span class="token punctuation">,</span> N<span class="token punctuation">,</span> C     <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        qkv         <span class="token operator">=</span> self<span class="token punctuation">.</span>qkv<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> N<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> C <span class="token operator">//</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
        q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v     <span class="token operator">=</span> qkv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>

        attn <span class="token operator">=</span> <span class="token punctuation">(</span>q @ k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>scale
        attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        attn <span class="token operator">=</span> self<span class="token punctuation">.</span>attn_drop<span class="token punctuation">(</span>attn<span class="token punctuation">)</span>

        x <span class="token operator">=</span> <span class="token punctuation">(</span>attn @ v<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B<span class="token punctuation">,</span> N<span class="token punctuation">,</span> C<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>proj_drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre> 
<h6><a id="IVTransformerBlock_200"></a>IV、TransformerBlock的构建。</h6> 
<p><img src="https://images2.imgbox.com/90/fb/P3n9y2e2_o.png" alt="在这里插入图片描述"></p> 
<p><strong>在完成MultiHeadSelfAttention的构建后，我们需要在其后加上两个全连接。就构建了整个TransformerBlock。</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">ResidualAttentionBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> n_head<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> attn_mask<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> nn<span class="token punctuation">.</span>MultiheadAttention<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> n_head<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ln_1 <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>mlp <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span>
            <span class="token punctuation">(</span><span class="token string">"c_fc"</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">(</span><span class="token string">"gelu"</span><span class="token punctuation">,</span> QuickGELU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">(</span><span class="token string">"c_proj"</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">,</span> d_model<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ln_2 <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attn_mask <span class="token operator">=</span> attn_mask

    <span class="token keyword">def</span> <span class="token function">attention</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>attn_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>attn_mask<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>x<span class="token punctuation">.</span>device<span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>attn_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> <span class="token boolean">None</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> need_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> attn_mask<span class="token operator">=</span>self<span class="token punctuation">.</span>attn_mask<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ln_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ln_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre> 
<h5><a id="cVIT_229"></a>c、整个VIT模型的构建</h5> 
<p><img src="https://images2.imgbox.com/6b/84/RaVYqZ9b_o.png" alt="在这里插入图片描述"><br> 整个VIT模型由一个Patch+Position Embedding加上多个TransformerBlock组成。典型的TransforerBlock的数量为12个。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> collections <span class="token keyword">import</span> OrderedDict

<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn


<span class="token keyword">class</span> <span class="token class-name">LayerNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Subclass torch's LayerNorm to handle fp16."""</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        orig_type <span class="token operator">=</span> x<span class="token punctuation">.</span>dtype
        ret <span class="token operator">=</span> <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>forward<span class="token punctuation">(</span>x<span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> ret<span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>orig_type<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">QuickGELU</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> x <span class="token operator">*</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token number">1.702</span> <span class="token operator">*</span> x<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">ResidualAttentionBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> n_head<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> attn_mask<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> nn<span class="token punctuation">.</span>MultiheadAttention<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> n_head<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ln_1 <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>mlp <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span>
            <span class="token punctuation">(</span><span class="token string">"c_fc"</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">(</span><span class="token string">"gelu"</span><span class="token punctuation">,</span> QuickGELU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">(</span><span class="token string">"c_proj"</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">,</span> d_model<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ln_2 <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attn_mask <span class="token operator">=</span> attn_mask

    <span class="token keyword">def</span> <span class="token function">attention</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>attn_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>attn_mask<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>x<span class="token punctuation">.</span>device<span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>attn_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> <span class="token boolean">None</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> need_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> attn_mask<span class="token operator">=</span>self<span class="token punctuation">.</span>attn_mask<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ln_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ln_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x


<span class="token keyword">class</span> <span class="token class-name">Transformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> width<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> layers<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> attn_mask<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>width <span class="token operator">=</span> width
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> layers
        self<span class="token punctuation">.</span>resblocks <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span>ResidualAttentionBlock<span class="token punctuation">(</span>width<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>resblocks<span class="token punctuation">(</span>x<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">VisionTransformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_resolution<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> patch_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> width<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> layers<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> output_dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>input_resolution <span class="token operator">=</span> input_resolution
        self<span class="token punctuation">.</span>output_dim <span class="token operator">=</span> output_dim
        <span class="token comment">#-----------------------------------------------#</span>
        <span class="token comment">#   224, 224, 3 -&gt; 196, 768</span>
        <span class="token comment">#-----------------------------------------------#</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span>width<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span>patch_size<span class="token punctuation">,</span> stride<span class="token operator">=</span>patch_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

        scale <span class="token operator">=</span> width <span class="token operator">**</span> <span class="token operator">-</span><span class="token number">0.5</span>
        <span class="token comment">#--------------------------------------------------------------------------------------------------------------------#</span>
        <span class="token comment">#   class_embedding部分是transformer的分类特征。用于堆叠到序列化后的图片特征中，作为一个单位的序列特征进行特征提取。</span>
        <span class="token comment">#</span>
        <span class="token comment">#   在利用步长为16x16的卷积将输入图片划分成14x14的部分后，将14x14部分的特征平铺，一幅图片会存在序列长度为196的特征。</span>
        <span class="token comment">#   此时生成一个class_embedding，将class_embedding堆叠到序列长度为196的特征上，获得一个序列长度为197的特征。</span>
        <span class="token comment">#   在特征提取的过程中，class_embedding会与图片特征进行特征的交互。最终分类时，我们取出class_embedding的特征，利用全连接分类。</span>
        <span class="token comment">#--------------------------------------------------------------------------------------------------------------------#</span>
        <span class="token comment">#   196, 768 -&gt; 197, 768</span>
        self<span class="token punctuation">.</span>class_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>scale <span class="token operator">*</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>width<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment">#--------------------------------------------------------------------------------------------------------------------#</span>
        <span class="token comment">#   为网络提取到的特征添加上位置信息。</span>
        <span class="token comment">#   以输入图片为224, 224, 3为例，我们获得的序列化后的图片特征为196, 768。加上class_embedding后就是197, 768</span>
        <span class="token comment">#   此时生成的pos_Embedding的shape也为197, 768，代表每一个特征的位置信息。</span>
        <span class="token comment">#--------------------------------------------------------------------------------------------------------------------#</span>
        <span class="token comment">#   197, 768 -&gt; 197, 768</span>
        self<span class="token punctuation">.</span>positional_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>scale <span class="token operator">*</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span>input_resolution <span class="token operator">//</span> patch_size<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> width<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ln_pre <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>width<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>transformer <span class="token operator">=</span> Transformer<span class="token punctuation">(</span>width<span class="token punctuation">,</span> layers<span class="token punctuation">,</span> heads<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>ln_post <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>width<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>scale <span class="token operator">*</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>width<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># shape = [*, width, grid, grid]</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># shape = [*, width, grid ** 2]</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># shape = [*, grid ** 2, width]</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>class_embedding<span class="token punctuation">.</span>to<span class="token punctuation">(</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>x<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># shape = [*, grid ** 2 + 1, width]</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>positional_embedding<span class="token punctuation">.</span>to<span class="token punctuation">(</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>ln_pre<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># NLD -&gt; LND</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># LND -&gt; NLD</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>ln_post<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>proj <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> x @ self<span class="token punctuation">.</span>proj

        <span class="token keyword">return</span> x
</code></pre> 
<h4><a id="2Text_Encoder_341"></a>2、Text Encoder</h4> 
<p><img src="https://images2.imgbox.com/67/93/QxUk8X9a_o.png" alt="在这里插入图片描述"><br> Text Encoder是一个基本的Bert，本质上也是由Self-Attention模块组成的，所以Text Encoder和Image Encoder的结构基本一样。</p> 
<p>在CLIP中，Text Encoder由12层的Transformer Encoder组成，由于文本信息相比于视觉信息更加简单，因此每一个规模的CLIP使用到的Text Encoder没有变化，大小都是一样的。</p> 
<p><img src="https://images2.imgbox.com/40/91/9rJwF8uZ_o.png" alt="在这里插入图片描述"><br> 在CLIP中，Text Encoder的宽度（embedding_size）为512，num_head值为512/64=8，层数为12，Transformer Encoder，如上图所hi由Self-Attention模块+FFN（Feed Foward Network，本质上就是俩全连接组成），结构非常简单。</p> 
<p>在Text Encoder中，我们会对每个句子增加一个Class Token，用于整合特征，以一个固定长度向量来代表输入句子。一般的Bert会将Class Token放在第0位，也就是最前面。而在CLIP中，Class Token被放在了文本的最后。</p> 
<p>以我的理解，放前面和放后面应该性能上没有很大的差别。</p> 
<p>构建代码如下：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> collections <span class="token keyword">import</span> OrderedDict

<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F

<span class="token comment">#--------------------------------------#</span>
<span class="token comment">#   Gelu激活函数的实现</span>
<span class="token comment">#   利用近似的数学公式</span>
<span class="token comment">#--------------------------------------#</span>
<span class="token keyword">class</span> <span class="token class-name">GELU</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>GELU<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token number">0.5</span> <span class="token operator">*</span> x <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> F<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>pi<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>x <span class="token operator">+</span> <span class="token number">0.044715</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
<span class="token keyword">class</span> <span class="token class-name">ResidualAttentionBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> n_head<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> attn_mask<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> nn<span class="token punctuation">.</span>MultiheadAttention<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> n_head<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ln_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>mlp <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span>
            <span class="token punctuation">(</span><span class="token string">"c_fc"</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">(</span><span class="token string">"gelu"</span><span class="token punctuation">,</span> GELU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">(</span><span class="token string">"c_proj"</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">,</span> d_model<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ln_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attn_mask <span class="token operator">=</span> attn_mask

    <span class="token keyword">def</span> <span class="token function">attention</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>attn_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>attn_mask<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>x<span class="token punctuation">.</span>device<span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>attn_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> <span class="token boolean">None</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> need_weights<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> attn_mask<span class="token operator">=</span>self<span class="token punctuation">.</span>attn_mask<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ln_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ln_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

<span class="token keyword">class</span> <span class="token class-name">Transformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> width<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> layers<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> attn_mask<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>width <span class="token operator">=</span> width
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> layers
        self<span class="token punctuation">.</span>resblocks <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span>ResidualAttentionBlock<span class="token punctuation">(</span>width<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>resblocks<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_407"></a>二、训练部分</h3> 
<p><img src="https://images2.imgbox.com/f2/bd/bTasgeqY_o.png" alt="在这里插入图片描述"><br> 训练部分的思路和前面介绍的一样。<br> 假设一个批次中有64个文本图像对，此时我们会同时获得64个图片和64个文本，首先我们从64个文本图像对中取出一个文本图像对，成对的文本图像对是天然的正样本，它们是配对的。</p> 
<p>而对于这个样本的文本来讲，其它63个图像都为负样本，它们是不配对的。<br> 而对于这个样本的图像来讲，其它63个文本都为负样本，它们是不配对的。</p> 
<p>在这个批次中，64个文本图像对，可以获得的图像embedding和文本embedding为：</p> 
<pre><code class="prism language-python">visual_embedding 	<span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">,</span> embedding_size<span class="token punctuation">]</span>
text_embedding		<span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">,</span> embedding_size<span class="token punctuation">]</span>
</code></pre> 
<p>visual_embedding的第x行和text_embedding的第x行是成对的。</p> 
<p>我们使用visual_embedding 叉乘 text_embedding，得到一个[64, 64]的矩阵，那么对角线上的值便是<strong>成对特征</strong>内积得到的，如果visual_embedding和对应的text_embedding越相似，那么它的值便越大。</p> 
<p><strong>我们选取[64, 64]矩阵中的第一行，代表第1个图片与64个文本的相似程度，其中第1个文本是正样本，我们将这一行的标签设置为1，那么我们就可以使用交叉熵进行训练，尽量把第1个图片和第一个文本的内积变得更大，那么它们就越相似。</strong></p> 
<p>每一行都做同样的工作，那么[64, 64]的矩阵，它的标签就是[1,2,3,4,5,6……,64]，在计算机中，标签从0开始，所以实际标签为[0,1,2,3,4,5……,63]。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> image<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>
    image_features  <span class="token operator">=</span> self<span class="token punctuation">.</span>encode_image<span class="token punctuation">(</span>image<span class="token punctuation">)</span>
    text_features   <span class="token operator">=</span> self<span class="token punctuation">.</span>encode_text<span class="token punctuation">(</span>text<span class="token punctuation">)</span>

    image_features  <span class="token operator">=</span> image_features <span class="token operator">/</span> image_features<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    text_features   <span class="token operator">=</span> text_features <span class="token operator">/</span> text_features<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

    logit_scale         <span class="token operator">=</span> self<span class="token punctuation">.</span>logit_scale<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span>
    logits_per_image    <span class="token operator">=</span> logit_scale <span class="token operator">*</span> image_features @ text_features<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span>
    logits_per_text     <span class="token operator">=</span> logits_per_image<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> logits_per_image<span class="token punctuation">,</span> logits_per_text
 
<span class="token comment"># 训练的代码如下，仅仅截取部分用于理解</span>
<span class="token keyword">def</span> <span class="token function">fit_one_epoch</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
	<span class="token comment"># 这里不使用logits_per_text是因为dp模式的划分有问题，所以使用logits_per_image出来的后转置。</span>
    logits_per_image<span class="token punctuation">,</span> _                 <span class="token operator">=</span> model_train<span class="token punctuation">(</span>images<span class="token punctuation">,</span> texts<span class="token punctuation">)</span>
    logits_per_text                     <span class="token operator">=</span> logits_per_image<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span>
    labels                              <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>logits_per_image<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>images<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
    loss_logits_per_image               <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>logits_per_image<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
    loss_logits_per_text                <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>logits_per_text<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
    loss                                <span class="token operator">=</span> loss_logits_per_image <span class="token operator">+</span> loss_logits_per_text
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
</code></pre> 
<h2><a id="CLIP_453"></a>训练自己的CLIP模型</h2> 
<p><strong>首先前往Github下载对应的仓库，下载完后利用解压软件解压，之后用编程软件打开文件夹。<br> 注意打开的根目录必须正确，否则相对目录不正确的情况下，代码将无法运行。</strong><br> 一定要注意打开后的根目录是文件存放的目录。<br> <img src="https://images2.imgbox.com/11/89/ZcCo9mrI_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_460"></a>一、数据集的准备</h3> 
<p><strong>本文使用json格式进行训练，训练前需要自己制作好数据集，如果没有自己的数据集，可以通过Github连接下载flickr8k的数据集尝试下。</strong><br> <strong>训练前将图片文件放在datasets/&lt;数据集名称&gt;中。<br> 训练前将标签文件放在datasets/&lt;标签名称&gt;.json。</strong><br> <img src="https://images2.imgbox.com/2c/21/HcJrxLsb_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_465"></a>二、数据集的格式</h3> 
<p>这里我提供了两个版本的数据集，一个版本是英文的、一个版本是中文的，开头分别为en和cn。标注文件为*.json文件，*.json的格式如下，image为图片的路径，caption为对应的文本，为一个列表，内容可以多条也可以单条：</p> 
<pre><code class="prism language-python"><span class="token punctuation">[</span>
  <span class="token punctuation">{<!-- --></span>
    <span class="token string">"image"</span><span class="token punctuation">:</span> <span class="token string">"flickr8k-images/2513260012_03d33305cf.jpg"</span><span class="token punctuation">,</span>
    <span class="token string">"caption"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>
      <span class="token string">"A black dog is running after a white dog in the snow ."</span><span class="token punctuation">,</span>
      <span class="token string">"Black dog chasing brown dog through snow"</span><span class="token punctuation">,</span>
      <span class="token string">"Two dogs chase each other across the snowy ground ."</span><span class="token punctuation">,</span>
      <span class="token string">"Two dogs play together in the snow ."</span><span class="token punctuation">,</span>
      <span class="token string">"Two dogs running through a low lying body of water ."</span>
    <span class="token punctuation">]</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token punctuation">{<!-- --></span>
    <span class="token string">"image"</span><span class="token punctuation">:</span> <span class="token string">"flickr8k-images/2903617548_d3e38d7f88.jpg"</span><span class="token punctuation">,</span>
    <span class="token string">"caption"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>
      <span class="token string">"A little baby plays croquet ."</span><span class="token punctuation">,</span>
      <span class="token string">"A little girl plays croquet next to a truck ."</span><span class="token punctuation">,</span>
      <span class="token string">"The child is playing croquette by the truck ."</span><span class="token punctuation">,</span>
      <span class="token string">"The kid is in front of a car with a put and a ball ."</span><span class="token punctuation">,</span>
      <span class="token string">"The little boy is playing with a croquet hammer and ball beside the car ."</span>
    <span class="token punctuation">]</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>
</code></pre> 
<p>而图片文件就放在datasets/&lt;数据集名称&gt;中即可。<br> <img src="https://images2.imgbox.com/b1/26/62HYjwia_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_493"></a>三、开始网络训练</h3> 
<p><strong>在train.py文件里，我们有一些参数需要设置</strong></p> 
<p>一般而言，需要注意的参数主要为：<br> model_path指向需要使用到的预训练权重。<br> phi指向需要使用到的模型。<br> datasets_path为数据集存放的路径<br> datasets_train_json_path为训练集的标签<br> datasets_val_json_path为验证机的标签。</p> 
<p>model_path和phi需要对应，本文当前支持三个模型，分别为：</p> 
<pre><code class="prism language-python"><span class="token string">"openai/VIT-B-16"</span>
<span class="token string">"openai/VIT-B-32"</span>
<span class="token string">"self-cn/VIT-B-32"</span>
</code></pre> 
<p>openai/VIT-B-16为openai公司开源的CLIP模型中，VIT-B-16规模的CLIP模型，英文文本与图片匹配，有公开预训练权重可用。<br> openai/VIT-B-32为openai公司开源的CLIP模型中，VIT-B-32规模的CLIP模型，英文文本与图片匹配，有公开预训练权重可用。<br> self-cn/VIT-B-32为自实现的模型，VIT-B-32规模的CLIP模型，英文文本与图片匹配，中文文本与图片匹配，无公开预训练权重可用，可以使用openai/VIT-B-32的Image Encoder初始化视觉部分，使用huggingface的<code>bert-base-chinese</code>初始化文本部分，进行训练时，model_path设置为<code>model_data/ViT-B-32-OpenAI.pth</code>即可。huggingface的<code>bert-base-chinese</code>会自动进行下载。</p> 
<p>准备好数据集之后就可以开始训练了。</p> 
<h3><a id="_514"></a>四、训练结果预测</h3> 
<p>训练结果预测需要用到两个文件，分别是clip.py和predict.py。<br> 我们首先需要去clip.py里面修改model_path，在clip.py文件里面，在如下部分修改model_path使其对应训练好的文件；<strong>model_path对应logs文件夹下面的权值文件</strong>。</p> 
<pre><code class="prism language-python">_defaults <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">#-------------------------------#</span>
    <span class="token comment">#   指向logs文件夹下的权值文件</span>
    <span class="token comment">#-------------------------------#</span>
    <span class="token string">"model_path"</span>        <span class="token punctuation">:</span> <span class="token string">'model_data/ViT-B-16-OpenAI.pth'</span><span class="token punctuation">,</span>
    <span class="token comment">#-------------------------------#</span>
    <span class="token comment">#   模型的种类</span>
    <span class="token comment">#   openai/VIT-B-16</span>
    <span class="token comment">#   openai/VIT-B-16</span>
    <span class="token comment">#   self-cn/VIT-B-32</span>
    <span class="token comment">#-------------------------------#</span>
    <span class="token string">"phi"</span>               <span class="token punctuation">:</span> <span class="token string">"openai/VIT-B-16"</span><span class="token punctuation">,</span>
    <span class="token comment">#--------------------------------------------------------------------#</span>
    <span class="token comment">#   该变量用于控制是否使用letterbox_image对输入图像进行不失真的resize</span>
    <span class="token comment">#   否则对图像进行CenterCrop</span>
    <span class="token comment">#--------------------------------------------------------------------#</span>
    <span class="token string">"letterbox_image"</span>   <span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    <span class="token comment">#-------------------------------#</span>
    <span class="token comment">#   是否使用Cuda</span>
    <span class="token comment">#   没有GPU可以设置成False</span>
    <span class="token comment">#-------------------------------#</span>
    <span class="token string">"cuda"</span>              <span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/ca/d8/TW1oNM3i_o.png" alt="在这里插入图片描述"></p> 
<p>设置好image_path和captions即可，后就可以运行predict.py进行检测了。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bf1060c1acbbacdbceef0a9e2172a345/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">数据清洗是什么？如何进行数据清洗？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c90c6845ac593316d9e66dcb23a1af55/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Vue实验】实验一：设计并实现一个网页版的汇率计算器</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>