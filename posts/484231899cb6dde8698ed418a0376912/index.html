<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【小白视角】大数据基础实践（七） Spark的基本操作_spark操作类型 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/484231899cb6dde8698ed418a0376912/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【小白视角】大数据基础实践（七） Spark的基本操作_spark操作类型">
  <meta property="og:description" content="RDD的 transformations 和 actions
点击这里
RDD 运行过程：
创建 RDD 对象；SparkContext 负责计算 RDD 之间的依赖关系，构建 DAGDAGScheduler 负责把 DAG 图分解成多个 Stage ，每个 Stage 中包含了多个Task ，每个 Task 会被 TaskScheduler 分发给各个 WorkerNode 上的 Executor 去执行。 3.4.3 Scala Scala 是一门现代的多范式编程语言，运行于 Java 平台（ JVM Java 虚拟机），并兼容现有的 Java 程序
Scala 的特性：
Scala 具备强大的并发性，支持函数式编程，可以更好地支持分布式系统Scala 语法简洁，能提供优雅的APIScala 兼容Java ，运行速度快，且能融合到 Hadoop 生态圈中Scala 是 Spark 的主要编程语言，但 Spark还支持 Java 、 Python 、R 作为编程语言Scala 的优势是提供了 REPL Read Eval Print Loop，交互式解释器 ），提高程序开发效率 4. SparkSQL Spark SQL在Hive兼容层面仅依赖HiveQL解析、Hive元数据，也就是说，从HQL被解析成抽象语法树（AST）起，就全部由Spark SQL接管了。Spark SQL执行计划生成和优化都由Catalyst（函数式关系查询优化框架）负责">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-13T17:16:00+08:00">
    <meta property="article:modified_time" content="2024-04-13T17:16:00+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【小白视角】大数据基础实践（七） Spark的基本操作_spark操作类型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>RDD的 <code>transformations</code> 和 <code>actions</code></p> 
<p><a href="" rel="nofollow">点击这里</a></p> 
<p>RDD 运行过程：</p> 
<ol><li>创建 RDD 对象；</li><li>SparkContext 负责计算 RDD 之间的依赖关系，构建 DAG</li><li>DAGScheduler 负责把 DAG 图分解成多个 Stage ，每个 Stage 中包含了多个Task ，每个 Task 会被 TaskScheduler 分发给各个 WorkerNode 上的 Executor 去执行。</li></ol> 
<h6><a id="343_Scala_14"></a>3.4.3 Scala</h6> 
<blockquote> 
 <p>Scala 是一门现代的多范式编程语言，运行于 Java 平台（ JVM Java 虚拟机），并兼容现有的 Java 程序</p> 
</blockquote> 
<p>Scala 的特性：</p> 
<ol><li>Scala 具备强大的并发性，支持函数式编程，可以更好地支持分布式系统</li><li>Scala 语法简洁，能提供优雅的<code>API</code></li><li>Scala 兼容<code>Java</code> ，运行速度快，且能融合到 <code>Hadoop</code> 生态圈中</li><li>Scala 是 Spark 的主要编程语言，但 <code>Spark</code>还支持 <code>Java</code> 、 <code>Python</code> 、<code>R</code> 作为编程语言</li><li>Scala 的优势是提供了 REPL <code>Read Eval Print Loop</code>，交互式解释器 ），提高程序开发效率</li></ol> 
<h4><a id="4_SparkSQL_35"></a>4. SparkSQL</h4> 
<blockquote> 
 <p>Spark SQL在Hive兼容层面仅依赖HiveQL解析、Hive元数据，也就是说，从HQL被解析成抽象语法树（AST）起，就全部由Spark SQL接管了。Spark SQL执行计划生成和优化都由Catalyst（函数式关系查询优化框架）负责</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/0a/c9/8d7WwiaV_o.png" alt="在这里插入图片描述"></p> 
<ul><li>Spark SQL增加了SchemaRDD（即带有Schema信息的RDD），使用户可以在Spark SQL中执行SQL语句，数据既可以来自RDD，也可以是Hive、HDFS、Cassandra等外部数据源，还可以是JSON格式的数据</li><li>Spark SQL目前支持Scala、Java、Python三种语言，支持SQL-92规范</li></ul> 
<p><img src="https://images2.imgbox.com/2c/0c/KHok3DML_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="5_Spark_56"></a>5. Spark编程实践</h4> 
<h5><a id="51__59"></a>5.1 编程环境</h5> 
<ul><li>操作系统：Linux（建议Ubuntu18.04或Ubuntu16.04）；</li><li>Hadoop版本：3.1.3或2.7.1；</li><li>JDK版本：1.8；</li><li>Hadoop伪分布式配置</li><li>Spark 2.4.8或自编译版本</li><li>Scala 2.11.8或2.8.0</li></ul> 
<h5><a id="52__70"></a>5.2 实验步骤：</h5> 
<h6><a id="521_Spark_73"></a>5.2.1 Spark环境配置</h6> 
<ol><li>检测java环境和hadoop环境。<br> <img src="https://images2.imgbox.com/73/8b/Q2HlkUsG_o.png" alt="在这里插入图片描述"></li><li>安装包下载<br> <img src="https://images2.imgbox.com/d1/86/rqO3zpxv_o.png" alt="在这里插入图片描述"><br> Scala： https://www.scala-lang.org/download/all.html<br> Spark： http://spark.apache.org/downloads.html<br> 关于 Spark 官网下载页面中 Choose a package type 几个选项说明：</li></ol> 
<ul><li>Source Code：spark 源码，需要编译才能使用，可以自由设置编译选项；</li><li>Pre-build with user-provide Hadoop：属于 Hadoop free 版本，用应用到任意 Hadoop 版本；</li><li>Pre-build for Hadoop 2.7、Pre-build for Hadoop 2.6：分别基于 Hadoop2.7、2.6 的预先编译版本，需要与本机安装的 Hadoop 版本对应使用；</li><li>Pre-build with Scala 2.12 and user provided Apache Hadoop：预先编译的版本，包含了 Scala2.12，可应用于任意 Hadoop 版本。</li></ul> 
<ol start="3"><li>安装scala</li></ol> 
<p>解压安装包（sudo tar -zxvf scala-2.11.8.tgz -C /usr/local/），并更改 scala<br> <img src="https://images2.imgbox.com/4d/13/XwqhaOw0_o.png" alt="在这里插入图片描述"></p> 
<p>所属用户和用户组为当前用户及所在组。<br> <img src="https://images2.imgbox.com/d3/cf/7FlyaheF_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/9d/3c/Kd0B06tg_o.png" alt="在这里插入图片描述"></p> 
<p>配置环境变量：添加</p> 
<p>S</p> 
<p>C</p> 
<p>A</p> 
<p>L</p> 
<p>A</p> 
<p>H</p> 
<p>O</p> 
<p>M</p> 
<p>E</p> 
<p>变</p> 
<p>量</p> 
<p>为</p> 
<p>s</p> 
<p>c</p> 
<p>a</p> 
<p>l</p> 
<p>a</p> 
<p>解</p> 
<p>压</p> 
<p>路</p> 
<p>径</p> 
<p>，</p> 
<p>并</p> 
<p>在</p> 
<p>SCALA_HOME 变量为 scala 解压路径，并在</p> 
<p>SCALAH​OME变量为scala解压路径，并在PATH 变量添加相应 的 bin 目录。<br> <img src="https://images2.imgbox.com/ca/11/Zc4llzCp_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/8e/31/UJl0pgx4_o.png" alt="在这里插入图片描述"></p> 
<p>使得环境生效<br> <img src="https://images2.imgbox.com/23/eb/13goCB8q_o.png" alt="在这里插入图片描述"></p> 
<p>查看是否安装成功<br> <img src="https://images2.imgbox.com/31/5c/BvPWPbbj_o.png" alt="在这里插入图片描述"><br> 已经成功了！</p> 
<ol start="4"><li>安装spark</li></ol> 
<p>解压安装包（sudo tar -zxvf spark-2.4.8-bin-without-hadoop.tgz -C /usr/local/），更改所属用户及用户组，并将目录重命名为 spark-2.4.8，方便后续配置：<br> <img src="https://images2.imgbox.com/b8/7f/Vah6Apir_o.png" alt="在这里插入图片描述"></p> 
<p>更改所属用户及用户组<br> <img src="https://images2.imgbox.com/9e/da/fneDa3Wg_o.png" alt="在这里插入图片描述"></p> 
<p>并将目录重命名为 spark-2.4.8<br> <img src="https://images2.imgbox.com/96/ff/q3iEir40_o.png" alt="在这里插入图片描述"></p> 
<p>配置环境变量，添加 SPARK_HOME 变量，并在 PATH 变量中添加相应的 bin 目录。<br> export SPARK_HOME=/usr/local/spark-2.4.8<br> export PATH=</p> 
<p>P</p> 
<p>A</p> 
<p>T</p> 
<p>H</p> 
<p>:</p> 
<p>PATH:</p> 
<p>PATH:SPARK_HOME/bin<img src="https://images2.imgbox.com/1a/49/QnfEh6q0_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/d6/56/ugSkBbSQ_o.png" alt="在这里插入图片描述"><br> Spark 配置文件配置：</p> 
<p><img src="https://images2.imgbox.com/f6/76/KVhDJtQz_o.png" alt="在这里插入图片描述"></p> 
<p>将 spark-env.sh.template 文件复制为 spark-env.sh 文件：<br> <img src="https://images2.imgbox.com/45/5c/SXao2tP3_o.png" alt="在这里插入图片描述"></p> 
<p>并配置内容如下：<br> <img src="https://images2.imgbox.com/31/5b/opJmAcfv_o.png" alt="在这里插入图片描述"></p> 
<p>启动 spark：启动 spark 之前要先启动 HDFS<br> <img src="https://images2.imgbox.com/13/b4/WteTU90C_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/1d/dc/E7prvabi_o.png" alt="在这里插入图片描述"></p> 
<p>启动之后网页访问 Master:8080 可以查看当前 Spark workers 状态。<br> <img src="https://images2.imgbox.com/6f/47/rGBILxFh_o.png" alt="在这里插入图片描述"></p> 
<p>Spark-shell 进入spark shell<br> <img src="https://images2.imgbox.com/84/05/Eplck77R_o.png" alt="在这里插入图片描述"></p> 
<p>会有这种错误<br> <img src="https://images2.imgbox.com/95/6f/XD6hxvXv_o.png" alt="在这里插入图片描述"></p> 
<p>但不需要慌张！不影响使用 scala 使用，如果要解决，可以通过添加系统环境变量。export TERM=xterm-color<br> <img src="https://images2.imgbox.com/ac/cb/eBMljRJ3_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/74/ba/IxvDiTt0_o.png" alt="在这里插入图片描述"></p> 
<p>就不会有了<br> <img src="https://images2.imgbox.com/ad/56/JM5jzi3q_o.png" alt="在这里插入图片描述"></p> 
<p>1.5 举个例子<br> 通 过 spark-submit 命令运行 spark 自 带 实 例 ， spark 自 带 实 例 都 在<br> SPARK_HOME/examples/jars/spark-examples_2.11-2.4.8.jar 中提供：</p> 
<pre><code>spark-submit --class org.apache.spark.examples.SparkPi examples/jars/spark-examples_2.11-2.4.8.jar

</code></pre> 
<p>注：在运行SparkPi实例时会输出很多运行日志，可以通过加 grep 命令进行过滤，显示关心的信息：</p> 
<p><img src="https://images2.imgbox.com/57/36/blbcugBJ_o.png" alt="在这里插入图片描述"></p> 
<h6><a id="522_spark_shellScala_304"></a>5.2.2 spark shell中编写Scala代码实现：</h6> 
<p>（1）分别从<code>本地文件</code>、<code>HDFS上的文件</code>以及<code>Spark Context的parallelized（）方法</code>生成分别生成RDD_1、RDD_2、RDD_3，要求本地文件格式为每行多个单词，以空格隔开；HDFS上的文本为每行1个单词，即单词以换行符隔开，每个RDD中都要包含1个或多个你的学号或者姓名拼音；<br> 1.1 本地创建in.txt<br> <img src="https://images2.imgbox.com/65/2c/lyGs1R8M_o.png" alt="在这里插入图片描述"><br> 写入内容<br> <img src="https://images2.imgbox.com/ca/8c/GDevoqDV_o.png" alt="在这里插入图片描述"><br> 上传到spark<br> <img src="https://images2.imgbox.com/28/dc/U1wLaf2p_o.png" alt="在这里插入图片描述"><br> 1.2 本地创建文件in0.txt<br> <img src="https://images2.imgbox.com/de/99/hgKTTBxD_o.png" alt="在这里插入图片描述"><br> 写入数据<br> <img src="https://images2.imgbox.com/d3/b0/1IOQh95M_o.png" alt="在这里插入图片描述"><br> 上传到hdfs中<br> <img src="https://images2.imgbox.com/ff/77/BHhO9xm6_o.png" alt="在这里插入图片描述"><br> 检查是否上传成功<br> <img src="https://images2.imgbox.com/24/a7/4A4xXz6p_o.png" alt="在这里插入图片描述">上传到spark<br> <img src="https://images2.imgbox.com/cb/84/fOH6TDR6_o.png" alt="在这里插入图片描述"></p> 
<p>1.3 spark创建文件<br> <img src="https://images2.imgbox.com/bd/06/HIXmNQKJ_o.png" alt="在这里插入图片描述"></p> 
<p>创建成功！</p> 
<p>(2) 输出RDD_1的第一行、RDD_2的所有内容、RDD_3的最大值；<br> 2.1RDD_1的第一行<br> <img src="https://images2.imgbox.com/e4/89/AGMcG2BG_o.png" alt="在这里插入图片描述"><br> 2.2 RDD_2的所有内容<br> <img src="https://images2.imgbox.com/25/7b/nGAqGzta_o.png" alt="在这里插入图片描述"></p> 
<p>2.3 RDD_3的最大值<br> <img src="https://images2.imgbox.com/eb/1c/Bsz9Xi7q_o.png" alt="在这里插入图片描述"></p> 
<p>(3) 统计 RDD_1 中“姓名拼音”、“学号”两个单词出现的次数；<br> <img src="https://images2.imgbox.com/2b/9c/w7cz27nl_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ba/24/JY7wBm53_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/f4/0a/9zBQsKLC_o.png" alt="在这里插入图片描述"></p> 
<p>结果：<br> zqc 有6个<br> 031904102 有 4个</p> 
<p>（4） 对去重后的 RDD_1再去掉RDD_2中的内容；<br> <img src="https://images2.imgbox.com/9a/92/KsQMVriF_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/79/61/PAAryEP7_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/23/ae/9UY0xhAV_o.png" alt="在这里插入图片描述"></p> 
<p>（5） 将上述结果与RDD_3合并，并将RDD_3分别写入本地文件系统和HDFS文件系统；<br> <img src="https://images2.imgbox.com/a4/e0/1bh7TNt7_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/49/ed/u6RCxGZu_o.png" alt="在这里插入图片描述"><br> 查看是否成功放入<br> <img src="https://images2.imgbox.com/6c/b5/ljyL189l_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/47/61/XvDmdoJ8_o.png" alt="在这里插入图片描述"><br> （6）编写scala代码实现写入任意内容到HDFS中，文件路径自定义，文件以”学号-姓名拼音.txt”命名。</p> 
<p>先创建一个文件<br> <img src="https://images2.imgbox.com/75/89/5lfX0fft_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2e/47/tleupUH6_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2b/bc/stGZqEVl_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/1e/d9/BXx6evLU_o.png" alt="在这里插入图片描述"><br> 在HDFS上查看<br> <img src="https://images2.imgbox.com/fb/52/GrsL3NOw_o.png" alt="在这里插入图片描述"></p> 
<h6><a id="523_Scala_376"></a>5.2.3 编写Scala独立应用程序：</h6> 
<p>使用 Scala 语言编写的 Spark 程序，需要使用 sbt 进行编译打包。Spark 中没有自带sbt，需要单独安装。可以到 官网 下载 sbt 安装文件，最新版即可<br> <img src="https://images2.imgbox.com/90/5d/L1N00fpg_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/a5/79/F0iYU9P2_o.png" alt="在这里插入图片描述"></p> 
<p>下载好<br> <img src="https://images2.imgbox.com/0c/e9/sBKS7pWv_o.png" alt="在这里插入图片描述"></p> 
<p>创建一个目录<br> <img src="https://images2.imgbox.com/1d/c5/FMpkeDD1_o.png" alt="在这里插入图片描述"></p> 
<p>这里我们把 sbt 安装到“/usr/local/sbt”目录下，执行如下命令：<br> <img src="https://images2.imgbox.com/ef/47/2mWKTtLs_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/26/3c/cuFLrOY3_o.png" alt="在这里插入图片描述"><br> 把 bin 目录下的 sbt-launch.jar 复制到 sbt 安装目录下<br> <img src="https://images2.imgbox.com/36/2b/4y3iQz4K_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/95/35/mnl4sr2e_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/cd/eb/zkDpiM5x_o.png" alt="在这里插入图片描述"><br> 新建一个文件然后将下列内容写下去</p> 
<pre><code>#!/bin/bash 
SBT\_OPTS="-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M" 
java $SBT\_OPTS -jar `dirname $0`/sbt-launch.jar "$@"

</code></pre> 
<p><img src="https://images2.imgbox.com/2a/1d/7ycoaAIn_o.png" alt="在这里插入图片描述"></p> 
<p>保存后，还需要为该 Shell 脚本文件增加可执行权限：<br> 然后，可以使用命令 sbt sbtVersion 查看 sbt 版本信息：<br> <img src="https://images2.imgbox.com/76/58/FARc5em2_o.png" alt="在这里插入图片描述"><br> 完成了，是有一点点慢！<br> <img src="https://images2.imgbox.com/64/c8/fpTLwzmN_o.png" alt="在这里插入图片描述"></p> 
<p>(1) 实现wordcount功能，并将结果写入本地文件；<br> 在本地创建目录<br> <img src="https://images2.imgbox.com/54/68/Hk9PH8U6_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/62/b8/wx9XwNXv_o.png" alt="在这里插入图片描述"></p> 
<p>创建这个文件。</p> 
<p><img src="https://images2.imgbox.com/e3/11/poKg1oFY_o.png" alt="在这里插入图片描述"></p> 
<p>写入数据。</p> 
<p><img src="https://images2.imgbox.com/2e/18/6qkpNxCD_o.png" alt="在这里插入图片描述"><br> 检查目录结构<br> <img src="https://images2.imgbox.com/e0/85/aMYTiHVT_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/95/5f/Ro27i3Vv_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e1/f1/Bsl7Fce6_o.png" alt="在这里插入图片描述"><br> （2）分别使用sbt打包上述程序；<br> <img src="https://images2.imgbox.com/e3/ab/netSruGH_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/04/fe/PDwL05bd_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/7f/6d/QiY0QUQk_o.png" alt="在这里插入图片描述"><br> （3）通过spark-submit执行生成的jar。<br> <img src="https://images2.imgbox.com/0e/54/EszUOHxa_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/bc/76/0bwklOFN_o.png" alt="在这里插入图片描述"></p> 
<ol start="4"><li> <p>编写Scala独立应用程序：</p> </li><li> <pre><code></code></pre> </li></ol> 
<p>实现生成任意RDD，并将结果写入文件；</p> 
<pre><code>

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210712001510469.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMwNDUwMw==,size_16,color_FFFFFF,t_70)  
 重命名并设置权限组  
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210712001526123.png)


在终端中执行如下命令创建一个文件夹 spark\_zqc\_maven\_scala 作为应用程序根，目录：  
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210712001541539.png)


写入下面内容


![在这里插入图片描述](https://img-blog.csdnimg.cn/20210712001555343.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMwNDUwMw==,size_16,color_FFFFFF,t_70)


(2) 分别使用maven打包上述程序；  
 该 程 序 依 赖 Spark Java API, 因 此 我 们 需 要 通 过 Maven 进 行 编 译 打 包 。 在./spark\_zqc\_maven\_scala 目录中新建文件 pom.xml，然后，在 pom.xml 文件中 添加如下内容，用来声明该独立应用程序的信息以及与 Spark 的依赖关系：  
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210712001622267.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMwNDUwMw==,size_16,color_FFFFFF,t_70)


为了保证 Maven 能够正常运行，先执行如下命令检查整个应用程序的文件结构，


![在这里插入图片描述](https://img-blog.csdnimg.cn/20210712001635821.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMwNDUwMw==,size_16,color_FFFFFF,t_70)


接下来，我们可以通过如下代码将整个应用程序打包成 JAR 包（注意：计算机需要保持连接网络的状态，而且首次运行打包命令时，Maven 会自动下载依赖包，需要消耗几分钟的时间）：


![在这里插入图片描述](https://img-blog.csdnimg.cn/20210712001643350.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMwNDUwMw==,size_16,color_FFFFFF,t_70)  
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210712001648506.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMwNDUwMw==,size_16,color_FFFFFF,t_70)


(3) 通过spark-submit执行生成的jar。  


**自我介绍一下，小编13年上海交大毕业，曾经在小公司待过，也去过华为、OPPO等大厂，18年进入阿里一直到现在。**

**深知大多数大数据工程师，想要提升技能，往往是自己摸索成长或者是报班学习，但对于培训机构动则几千的学费，着实压力不小。自己不成体系的自学效果低效又漫长，而且极易碰到天花板技术停滞不前！**

**因此收集整理了一份《2024年大数据全套学习资料》，初衷也很简单，就是希望能够帮助到想自学提升又不知道该从何学起的朋友。**
![img](https://img-blog.csdnimg.cn/img_convert/7c0b116b491b7efa2b39807e506ffad0.png)
![img](https://img-blog.csdnimg.cn/img_convert/ce9651aaa7570a256adef665bdc7a957.png)
![img](https://img-blog.csdnimg.cn/img_convert/4bdd7f44f78a5500735629a04be96778.png)
![img](https://img-blog.csdnimg.cn/img_convert/1fddb6a4938f5193aeed40b45e69c51b.png)
![img](https://img-blog.csdnimg.cn/img_convert/f46569e9bef98e6adaf774f8d20da044.png)

**既有适合小白学习的零基础资料，也有适合3年以上经验的小伙伴深入学习提升的进阶课程，基本涵盖了95%以上大数据开发知识点，真正体系化！**

**由于文件比较大，这里只是将部分目录大纲截图出来，每个节点里面都包含大厂面经、学习笔记、源码讲义、实战项目、讲解视频，并且后续会持续更新**

**如果你觉得这些内容对你有帮助，可以添加VX：vip204888 （备注大数据获取）**
![img](https://img-blog.csdnimg.cn/img_convert/e859f4c9e30eeb06648cebe09ebe4591.png)

**一个人可以走的很快，但一群人才能走的更远。不论你是正从事IT行业的老鸟或是对IT行业感兴趣的新人，都欢迎扫码加入我们的的圈子（技术交流、学习资源、职场吐槽、大厂内推、面试辅导），让我们一起学习成长！**


[外链图片转存中...(img-4nFP8brM-1712999691004)]
[外链图片转存中...(img-i03d7mEb-1712999691005)]
[外链图片转存中...(img-qIfQHXef-1712999691005)]
[外链图片转存中...(img-Migi4bEn-1712999691005)]

**既有适合小白学习的零基础资料，也有适合3年以上经验的小伙伴深入学习提升的进阶课程，基本涵盖了95%以上大数据开发知识点，真正体系化！**

**由于文件比较大，这里只是将部分目录大纲截图出来，每个节点里面都包含大厂面经、学习笔记、源码讲义、实战项目、讲解视频，并且后续会持续更新**

**如果你觉得这些内容对你有帮助，可以添加VX：vip204888 （备注大数据获取）**
[外链图片转存中...(img-5I8vgxu2-1712999691006)]

**一个人可以走的很快，但一群人才能走的更远。不论你是正从事IT行业的老鸟或是对IT行业感兴趣的新人，都欢迎扫码加入我们的的圈子（技术交流、学习资源、职场吐槽、大厂内推、面试辅导），让我们一起学习成长！**

</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2a67c1a092dad8edcabc25cfecde00db/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">人事|基于SpringBoot&#43;vue的人事管理系统设计与实现(源码&#43;数据库&#43;文档)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4bd829925db99310e181e4ce552cfd75/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【WebSocket连接异常】前端使用WebSocket子协议传递token时，Java后端的正确打开方式！！！</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>