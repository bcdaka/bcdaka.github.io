<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Transformers x SwanLab：可视化NLP模型训练 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/00303ffc5ef191c9ce5b389c23913b34/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Transformers x SwanLab：可视化NLP模型训练">
  <meta property="og:description" content="HuggingFace 的 Transformers 是目前最流行的深度学习训框架之一（100k&#43; Star），现在主流的大语言模型（LLaMa系列、Qwen系列、ChatGLM系列等）、自然语言处理模型（Bert系列）等，都在使用Transformers来进行预训练、微调和推理。
SwanLab是一个深度学习实验管理与训练可视化工具，由西安电子科技大学团队打造，融合了Weights &amp; Biases与Tensorboard的特点，能够方便地进行 训练可视化、多实验对比、超参数记录、大型实验管理和团队协作，并支持用网页链接的方式分享你的实验。
你可以使用Transformers快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。
下面将用一个Bert训练，来介绍如何将Transformers与SwanLab配合起来：
1. 代码中引入SwanLabCallback from swanlab.integration.huggingface import SwanLabCallback SwanLabCallback是适配于Transformers的日志记录类。
SwanLabCallback可以定义的参数有：
project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。你也可以在外部通过swanlab.init创建项目，集成会将实验记录到你在外部创建的项目中。 2. 传入Trainer from swanlab.integration.huggingface import SwanLabCallback from transformers import Trainer, TrainingArguments ... # 实例化SwanLabCallback swanlab_callback = SwanLabCallback() trainer = Trainer( ... # 传入callbacks参数 callbacks=[swanlab_callback], ) 3. 案例-Bert训练 查看在线实验过程：BERT-SwanLab
下面是一个基于Transformers框架，使用BERT模型在imdb数据集上做微调，同时用SwanLab进行可视化的案例代码
&#34;&#34;&#34; 用预训练的Bert模型微调IMDB数据集，并使用SwanLabCallback回调函数将结果上传到SwanLab。 IMDB数据集的1是positive，0是negative。 &#34;&#34;&#34; import torch from datasets import load_dataset from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments from swanlab.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-28T20:08:26+08:00">
    <meta property="article:modified_time" content="2024-05-28T20:08:26+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Transformers x SwanLab：可视化NLP模型训练</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>HuggingFace 的 Transformers 是目前最流行的深度学习训框架之一（100k+ Star），现在主流的大语言模型（LLaMa系列、Qwen系列、ChatGLM系列等）、自然语言处理模型（Bert系列）等，都在使用Transformers来进行预训练、微调和推理。</p> 
<p><img src="https://images2.imgbox.com/07/15/78pL5KXJ_o.png" alt="在这里插入图片描述"><br> SwanLab是一个深度学习实验管理与训练可视化工具，由西安电子科技大学团队打造，融合了Weights &amp; Biases与Tensorboard的特点，能够方便地进行 训练可视化、多实验对比、超参数记录、大型实验管理和团队协作，并支持用网页链接的方式分享你的实验。</p> 
<p><img src="https://images2.imgbox.com/d7/9c/Qv3VUBYl_o.png" alt="在这里插入图片描述"></p> 
<p>你可以使用Transformers快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。<br> 下面将用一个Bert训练，来介绍如何将Transformers与SwanLab配合起来：</p> 
<h3><a id="1_SwanLabCallback_9"></a>1. 代码中引入SwanLabCallback</h3> 
<pre><code class="prism language-python"><span class="token keyword">from</span> swanlab<span class="token punctuation">.</span>integration<span class="token punctuation">.</span>huggingface <span class="token keyword">import</span> SwanLabCallback
</code></pre> 
<p><strong>SwanLabCallback</strong>是适配于Transformers的日志记录类。</p> 
<p><strong>SwanLabCallback</strong>可以定义的参数有：</p> 
<ul><li>project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。</li><li>你也可以在外部通过<code>swanlab.init</code>创建项目，集成会将实验记录到你在外部创建的项目中。</li></ul> 
<h3><a id="2_Trainer_20"></a>2. 传入Trainer</h3> 
<pre><code class="prism language-python"><span class="token keyword">from</span> swanlab<span class="token punctuation">.</span>integration<span class="token punctuation">.</span>huggingface <span class="token keyword">import</span> SwanLabCallback
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> Trainer<span class="token punctuation">,</span> TrainingArguments
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

<span class="token comment"># 实例化SwanLabCallback</span>
swanlab_callback <span class="token operator">=</span> SwanLabCallback<span class="token punctuation">(</span><span class="token punctuation">)</span>

trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    <span class="token comment"># 传入callbacks参数</span>
    callbacks<span class="token operator">=</span><span class="token punctuation">[</span>swanlab_callback<span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="3_Bert_37"></a>3. 案例-Bert训练</h3> 
<p>查看在线实验过程：<a href="https://swanlab.cn/@ZeyiLin/BERT/charts" rel="nofollow">BERT-SwanLab</a></p> 
<p><img src="https://images2.imgbox.com/7b/2b/1BZaRRab_o.png" alt="在这里插入图片描述"></p> 
<p>下面是一个基于Transformers框架，使用BERT模型在imdb数据集上做微调，同时用SwanLab进行可视化的案例代码</p> 
<pre><code class="prism language-python"><span class="token triple-quoted-string string">"""
用预训练的Bert模型微调IMDB数据集，并使用SwanLabCallback回调函数将结果上传到SwanLab。
IMDB数据集的1是positive，0是negative。
"""</span>

<span class="token keyword">import</span> torch
<span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForSequenceClassification<span class="token punctuation">,</span> Trainer<span class="token punctuation">,</span> TrainingArguments
<span class="token keyword">from</span> swanlab<span class="token punctuation">.</span>integration<span class="token punctuation">.</span>huggingface <span class="token keyword">import</span> SwanLabCallback
<span class="token keyword">import</span> swanlab

<span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>text<span class="token punctuation">,</span> model<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> CLASS_NAME<span class="token punctuation">)</span><span class="token punctuation">:</span>
    inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>text<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>

    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span>
        logits <span class="token operator">=</span> outputs<span class="token punctuation">.</span>logits
        predicted_class <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>logits<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Input Text: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>text<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Predicted class: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span><span class="token builtin">int</span><span class="token punctuation">(</span>predicted_class<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>CLASS_NAME<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>predicted_class<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token builtin">int</span><span class="token punctuation">(</span>predicted_class<span class="token punctuation">)</span>
<span class="token comment"># 加载IMDB数据集</span>
dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">'imdb'</span><span class="token punctuation">)</span>

<span class="token comment"># 加载预训练的BERT tokenizer</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">)</span>

<span class="token comment"># 定义tokenize函数</span>
<span class="token keyword">def</span> <span class="token function">tokenize</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tokenizer<span class="token punctuation">(</span>batch<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 对数据集进行tokenization</span>
tokenized_datasets <span class="token operator">=</span> dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>tokenize<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 设置模型输入格式</span>
tokenized_datasets <span class="token operator">=</span> tokenized_datasets<span class="token punctuation">.</span>rename_column<span class="token punctuation">(</span><span class="token string">"label"</span><span class="token punctuation">,</span> <span class="token string">"labels"</span><span class="token punctuation">)</span>
tokenized_datasets<span class="token punctuation">.</span>set_format<span class="token punctuation">(</span><span class="token string">'torch'</span><span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">,</span> <span class="token string">'attention_mask'</span><span class="token punctuation">,</span> <span class="token string">'labels'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 加载预训练的BERT模型</span>
model <span class="token operator">=</span> AutoModelForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

<span class="token comment"># 设置训练参数</span>
training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span><span class="token string">'./results'</span><span class="token punctuation">,</span>
    eval_strategy<span class="token operator">=</span><span class="token string">'epoch'</span><span class="token punctuation">,</span>
    save_strategy<span class="token operator">=</span><span class="token string">'epoch'</span><span class="token punctuation">,</span>
    learning_rate<span class="token operator">=</span><span class="token number">2e-5</span><span class="token punctuation">,</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
    per_device_eval_batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
    logging_first_step<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>
    <span class="token comment"># 总的训练轮数</span>
    num_train_epochs<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
    weight_decay<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span>
    report_to<span class="token operator">=</span><span class="token string">"none"</span><span class="token punctuation">,</span>
    <span class="token comment"># 单卡训练</span>
<span class="token punctuation">)</span>

CLASS_NAME <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token number">0</span><span class="token punctuation">:</span> <span class="token string">"negative"</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span> <span class="token string">"positive"</span><span class="token punctuation">}</span>

<span class="token comment"># 设置swanlab回调函数</span>
swanlab_callback <span class="token operator">=</span> SwanLabCallback<span class="token punctuation">(</span>project<span class="token operator">=</span><span class="token string">'BERT'</span><span class="token punctuation">,</span>
                                   experiment_name<span class="token operator">=</span><span class="token string">'BERT-IMDB'</span><span class="token punctuation">,</span>
                                   config<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">'dataset'</span><span class="token punctuation">:</span> <span class="token string">'IMDB'</span><span class="token punctuation">,</span> <span class="token string">"CLASS_NAME"</span><span class="token punctuation">:</span> CLASS_NAME<span class="token punctuation">}</span><span class="token punctuation">)</span>

<span class="token comment"># 定义Trainer</span>
trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
    model<span class="token operator">=</span>model<span class="token punctuation">,</span>
    args<span class="token operator">=</span>training_args<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    eval_dataset<span class="token operator">=</span>tokenized_datasets<span class="token punctuation">[</span><span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    callbacks<span class="token operator">=</span><span class="token punctuation">[</span>swanlab_callback<span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment"># 训练模型</span>
trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 保存模型</span>
model<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span><span class="token string">'./sentiment_model'</span><span class="token punctuation">)</span>
tokenizer<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span><span class="token string">'./sentiment_model'</span><span class="token punctuation">)</span>

<span class="token comment"># 测试模型</span>
test_reviews <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">"I absolutely loved this movie! The storyline was captivating and the acting was top-notch. A must-watch for everyone."</span><span class="token punctuation">,</span>
    <span class="token string">"This movie was a complete waste of time. The plot was predictable and the characters were poorly developed."</span><span class="token punctuation">,</span>
    <span class="token string">"An excellent film with a heartwarming story. The performances were outstanding, especially the lead actor."</span><span class="token punctuation">,</span>
    <span class="token string">"I found the movie to be quite boring. It dragged on and didn't really go anywhere. Not recommended."</span><span class="token punctuation">,</span>
    <span class="token string">"A masterpiece! The director did an amazing job bringing this story to life. The visuals were stunning."</span><span class="token punctuation">,</span>
    <span class="token string">"Terrible movie. The script was awful and the acting was even worse. I can't believe I sat through the whole thing."</span><span class="token punctuation">,</span>
    <span class="token string">"A delightful film with a perfect mix of humor and drama. The cast was great and the dialogue was witty."</span><span class="token punctuation">,</span>
    <span class="token string">"I was very disappointed with this movie. It had so much potential, but it just fell flat. The ending was particularly bad."</span><span class="token punctuation">,</span>
    <span class="token string">"One of the best movies I've seen this year. The story was original and the performances were incredibly moving."</span><span class="token punctuation">,</span>
    <span class="token string">"I didn't enjoy this movie at all. It was confusing and the pacing was off. Definitely not worth watching."</span>
<span class="token punctuation">]</span>

model<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>
text_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> review <span class="token keyword">in</span> test_reviews<span class="token punctuation">:</span>
    label <span class="token operator">=</span> predict<span class="token punctuation">(</span>review<span class="token punctuation">,</span> model<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> CLASS_NAME<span class="token punctuation">)</span>
    text_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>swanlab<span class="token punctuation">.</span>Text<span class="token punctuation">(</span>review<span class="token punctuation">,</span> caption<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>label<span class="token punctuation">}</span></span><span class="token string">-</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>CLASS_NAME<span class="token punctuation">[</span>label<span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> text_list<span class="token punctuation">:</span>
    swanlab<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"predict"</span><span class="token punctuation">:</span> text_list<span class="token punctuation">}</span><span class="token punctuation">)</span>

swanlab<span class="token punctuation">.</span>finish<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/d6/4d/MXh1Tssg_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/6f/39/73aiJaG4_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/65/6b/I16TMxIQ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b0/1c/FmM8S5i3_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="4__155"></a>4. 相关链接</h3> 
<ul><li>Transformers文档：<a href="https://huggingface.co/docs/transformers/index" rel="nofollow">🤗 Transformers</a></li><li>SwanLab官网：<a href="https://swanlab.cn" rel="nofollow">SwanLab - 在线AI实验平台，一站式跟踪、比较、分享你的模型</a></li><li>SwanLab官方文档：<a href="https://docs.swanlab.cn" rel="nofollow">SwanLab官方文档 | 先进的AI团队协作与模型创新引擎</a></li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/26bc504490227eb88400ab0dba7ebe88/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Llama模型家族训练奖励模型Reward Model技术及代码实战（三） 使用 TRL 训练奖励模型</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/79f6ce210f045d038c20a85ed2a6e37d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">基于javaswing和mysql实现的员工工资管理系统</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>