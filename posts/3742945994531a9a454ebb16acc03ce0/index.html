<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Day15—热点搜索词统计 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/3742945994531a9a454ebb16acc03ce0/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Day15—热点搜索词统计">
  <meta property="og:description" content="一、要求 根据用户上网的搜索记录对每天的热点搜索词进行统计，以了解用户所关心的热点话题。
要求完成：统计每天搜索数量前3名的搜索词（同一天中同一用户多次搜索同一个搜索词视为1次）。
二、数据 三、配置scala环境 1.下载scala插件 Scala插件的安装有两种方式：在线与离线。我们学习在线安装方式。
启动IDEA，在欢迎界面中选择Configure→Plugins命令，搜索scala进行下载
2.配置scala环境 下载后的scala进行环境配置
在Project Settings-&gt;Libraries中添加下载好的Scala
3.创建scala class 4.编写scala代码 package org.example import org.apache.spark.rdd.RDD import org.apache.spark.sql.{Row, SparkSession} import org.apache.spark.sql.types._ import scala.collection.mutable.ListBuffer object keywords { def main(args: Array[String]): Unit = { //构建SparkSession // 构建SparkSession val spark = SparkSession.builder() .appName(&#34;YourAppName&#34;) // 设置应用程序的名称，显示在Spark UI中 .master(&#34;local[*]&#34;) // 设置Spark应用程序运行的主节点和资源 .getOrCreate() // 创建或获取已存在的SparkSession对象 //读取数据 val linesRDD: RDD[String] = spark.sparkContext.textFile(&#34;data/keywords.txt&#34;) // 使用map算子操作转换RDD中的每个元素 val transformedRDD = linesRDD.map(line =&gt; { val fields = line.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-20T15:19:26+08:00">
    <meta property="article:modified_time" content="2024-06-20T15:19:26+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Day15—热点搜索词统计</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>一、要求</h2> 
<p>根据用户上网的搜索记录对每天的热点搜索词进行统计，以了解用户所关心的热点话题。<br> 要求完成：统计每天搜索数量前3名的搜索词（同一天中同一用户多次搜索同一个搜索词视为1次）。</p> 
<h2><a id="_4"></a>二、数据</h2> 
<p><img src="https://images2.imgbox.com/1c/79/wLw95RaN_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="scala_8"></a>三、配置scala环境</h2> 
<h4><a id="1scala_9"></a>1.下载scala插件</h4> 
<p>Scala插件的安装有两种方式：在线与离线。我们学习在线安装方式。<br> 启动IDEA，在欢迎界面中选择Configure→Plugins命令，搜索scala进行下载</p> 
<h4><a id="2scala_13"></a>2.配置scala环境</h4> 
<p>下载后的scala进行环境配置<br> 在Project Settings-&gt;Libraries中添加下载好的Scala<br> <img src="https://images2.imgbox.com/5d/af/ttJpkxLn_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="3scala_class_19"></a>3.创建scala class</h4> 
<p><img src="https://images2.imgbox.com/8a/46/Hxn0aSC5_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/0c/2e/bvr7juci_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="4scala_24"></a>4.编写scala代码</h4> 
<pre><code>package org.example

import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.types._
import scala.collection.mutable.ListBuffer

object keywords {
  def main(args: Array[String]): Unit = {
    //构建SparkSession
    // 构建SparkSession
    val spark = SparkSession.builder()
      .appName("YourAppName") // 设置应用程序的名称，显示在Spark UI中
      .master("local[*]")    // 设置Spark应用程序运行的主节点和资源
      .getOrCreate()         // 创建或获取已存在的SparkSession对象

    //读取数据
    val linesRDD: RDD[String] = spark.sparkContext.textFile("data/keywords.txt")

    // 使用map算子操作转换RDD中的每个元素
    val transformedRDD = linesRDD.map(line =&gt; {
      val fields = line.split(",") // 按逗号分割每行数据
      val date = fields(0) // 日期
      val user = fields(1) // 用户
      val keyword = fields(2) // 搜索词
      ((date, keyword), user) // 结果创建一个新的元组，其中包含键和值
    })
    //根据关键词进行分组
    val groupedBy = transformedRDD.groupByKey() //将时间和搜索词相等的(键相等)划分为一组

    // 去除每个分组中的重复用户名称
    val distinctUsersPerGroup = groupedBy.map {
      case ((date, keyword), users) =&gt; ((date, keyword), users.toSeq.distinct)
    }
    // 使用map操作来转换RDD中的每个元素，计数
    val userCountsRDD = distinctUsersPerGroup.map {
      case ((date, keyword), users) =&gt; ((date, keyword), users.size)
    }

    val result = userCountsRDD.collect()
    println(result.mkString("\n"))


    val rowRDD: RDD[Row] = userCountsRDD.map(line =&gt; {
      Row(
        line._1._1, //日期
        line._1._2, //关键词
        line._2.toInt //搜索次数
      )
    })

    //构建DataFrame元数据
    val structType = StructType(Array(
      StructField("date", StringType, true),
      StructField("keyword", StringType, true),
      StructField("times", IntegerType, true)

    ))

    //将RDD[Row]转为DataFrame
    val df = spark.createDataFrame(rowRDD, structType)

    //使用开窗函数取每一天的搜索前3名
    df.createTempView("hot_times") //创建临时视图
    //执行SQL查询
    spark.sql(
      "select date,keyword,times,rank  from " +
        "(select date,keyword,times," +
        "row_number() over (partition by date order by times desc) rank " +
        "from hot_times) t " +
        "where t.rank&lt;=3"
    ).show()
  }
}
</code></pre> 
<h4><a id="5_103"></a>5.运行结果</h4> 
<p><img src="https://images2.imgbox.com/57/ac/2dVTD1cd_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Xshell_106"></a>二、在Xshell中运行</h3> 
<h4><a id="_107"></a>总体流程</h4> 
<p><img src="https://images2.imgbox.com/a0/ce/cm3OTfB5_o.png" alt="代码类似于上图"><br> 1.读取文件<br> <code>val rdd = sc.textFile("file:///root/data/keywords.txt")</code><br> <img src="https://images2.imgbox.com/89/90/NoplEXp0_o.png" alt="在这里插入图片描述"><br> 2.使用map算子，转换RDD中的每个元素</p> 
<pre><code class="prism language-bash">val rdd2 <span class="token operator">=</span> rdd.map<span class="token punctuation">(</span>line <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">{<!-- --></span>
     val fields <span class="token operator">=</span> line.split<span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">)</span> // 按逗号分割每行数据
     val <span class="token function">date</span> <span class="token operator">=</span> fields<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>         // 日期
     val user <span class="token operator">=</span> fields<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>         // 用户
     val keyword <span class="token operator">=</span> fields<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>      // 搜索词
     <span class="token punctuation">((</span>date, keyword<span class="token punctuation">)</span>, user<span class="token punctuation">)</span>     // 创建一个新的元组，其中包含键和值
     <span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/cb/84/4A4DkMQS_o.png" alt="在这里插入图片描述"><br> 3.根据关键词进行分组<br> 将时间和搜索词相等的(键相等)划分为一组</p> 
<pre><code class="prism language-bash">val <span class="token assign-left variable">rdd3GBy</span><span class="token operator">=</span> rdd2.groupByKey<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/ad/8c/N7IXST3b_o.png" alt="在这里插入图片描述"></p> 
<p>4.去除重复的值<br> <code>val rdd4 =rdd3GBy.map { case ((date, keyword), users) =&gt; ((date, keyword), users.toSeq.distinct) }</code></p> 
<p><img src="https://images2.imgbox.com/7e/03/1BIiVwk9_o.png" alt="在这里插入图片描述"><br> 5.使用map操作来转换RDD中的每个元素，计数</p> 
<pre><code class="prism language-bash">val rdd5 <span class="token operator">=</span> rdd4.map <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">case</span> <span class="token punctuation">((</span>date, keyword<span class="token punctuation">)</span>, <span class="token function">users</span><span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">((</span>date, keyword<span class="token punctuation">)</span>, users.size<span class="token punctuation">)</span>
      <span class="token punctuation">}</span>

</code></pre> 
<p>6.导入必要的包<br> import org.apache.spark.rdd.RDD<br> import org.apache.spark.sql.types._<br> import org.apache.spark.sql.{Row, SparkSession}</p> 
<p>7.在Apache Spark中，SparkSession 是一个核心对象，用于与Spark交互。它是执行Spark应用程序的入口点，负责创建DataFrame和Dataset，以及读写数据。</p> 
<pre><code class="prism language-bash">    val rowRDD: RDD<span class="token punctuation">[</span>Row<span class="token punctuation">]</span> <span class="token operator">=</span> rdd5.map<span class="token punctuation">(</span>line <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">{<!-- --></span>
      Row<span class="token punctuation">(</span>
        line._1._1, //日期
        line._1._2, //关键词
        line._2.toInt //搜索次数
      <span class="token punctuation">)</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<p>8.这段Scala代码是在使用Apache Spark的RDD（弹性分布式数据集）API来处理文本数据。</p> 
<pre><code class="prism language-bash">    val structType <span class="token operator">=</span> StructType<span class="token punctuation">(</span>Array<span class="token punctuation">(</span>

      StructField<span class="token punctuation">(</span><span class="token string">"date"</span>, StringType, <span class="token boolean">true</span><span class="token punctuation">)</span>,

      StructField<span class="token punctuation">(</span><span class="token string">"keyword"</span>, StringType, <span class="token boolean">true</span><span class="token punctuation">)</span>,

      StructField<span class="token punctuation">(</span><span class="token string">"times"</span>, IntegerType, <span class="token boolean">true</span><span class="token punctuation">)</span>

    <span class="token punctuation">))</span>

</code></pre> 
<p>9.将RDD[Row]转为DataFrame</p> 
<p><code>val df = spark.createDataFrame(rowRDD, structType)</code></p> 
<p>10.使用开窗函数取每一天的搜索前3名<br> <code>df.createTempView("hot_times") //创建临时视图</code></p> 
<pre><code class="prism language-bash">//执行SQL查询
spark.sql<span class="token punctuation">(</span>
  <span class="token string">"select date,keyword,times,rank  from "</span> +
    <span class="token string">"(select date,keyword,times,"</span> +
    <span class="token string">"row_number() over (partition by date order by times desc) rank "</span> +
    <span class="token string">"from hot_times) t "</span> +
    <span class="token string">"where t.rank&lt;=3"</span>
<span class="token punctuation">)</span>.show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_194"></a>结果展示</h3> 
<p><img src="https://images2.imgbox.com/bc/a8/CDkEDrg6_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_197"></a>小结</h3> 
<p>我们本次学习了使用spark sql来编写一个代码完成统计。在本次学习中，我们探索了如何利用Apache Spark SQL的强大功能来编写代码，以完成数据统计任务。通过Spark SQL，我们能够以一种声明式的方式处理数据集，使得数据分析变得更加直观和高效。我们学习了如何创建SparkSession，执行SQL查询，以及使用DataFrame API进行数据转换和分析。这些技能对于处理大规模数据集至关重要，能够帮助我们快速得到所需的统计结果。通过实践，我们发现Spark SQL不仅简化了代码编写，还提高了数据处理的性能。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d15b70c0297bb320f3fd23632f51f71d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【漏洞复现】红帆iOffice.net wssRtSyn接口处存在SQL注入</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/770e9f81d874596fda18a17bc36820e6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">矿大数据结构 实验三</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>