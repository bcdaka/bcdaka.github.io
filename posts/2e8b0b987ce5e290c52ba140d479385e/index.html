<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Stable Diffusion 基本原理 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/2e8b0b987ce5e290c52ba140d479385e/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Stable Diffusion 基本原理">
  <meta property="og:description" content="1 Diffusion Model的运作过程 输入一张和我们所需结果图尺寸一致的噪声图像，通过Denoise模块逐步减少noise，最终生成我们需要的效果图。
图中Denoise模块虽然是同一个，但是它会根据不同step的输入图像和代表noise严重程度的参数选择denoise的程度。
1.1 Denoise 模块的内部过程 根据我们输入带噪声的图像和去噪程度的参数，Denoise模块中的Noise pred模块会预测出图中的noise部分，此时输入图像和预测噪声的差即为该step的输出结果。
问：为何选择预测噪声做差而不是直接预测消除部分噪声后的图像？
由于预测噪声的难度更低，如果直接预测带噪声后的图像其实就已经相当于可以实现图像的生成了。
1.2 如何训练Noise_predictor 想要训练Noise_predictor预测出来噪声，我们需要提供噪声的Ground truth，这个如何获得？
我们从训练数据集中随机抽取一张图像，然后人为给其加噪声，我们人为添加的噪声即是Noise_predictor中的groundTruth，该添加噪声的过程也被称为foward process。
2 Stable Diffusion stable diffusion包括三大模块：TextEncoder、Generation Model、Decoder，三个模块独立训练，最终组合。
2.1 TextEncoder TextEncoder对结果的影响很大，远大于diffusion model，增大TextEncoder模型，效果明显，而增大diffusion model模型，效果则没那么显著。
2.1.1 FID的理解 将真实图片和生成图片分别输入到一个CNN分类器，假设他们都满足高斯分布，计算他们的距离，距离越小，说明生成的图片效果越好，距离越大说明生成的图片效果越差。注意，FID的计算需要充足数量的样本。
2.1.2 CLIP score的理解 clip分为TextEncoder和imageEncoder两个模块，将text输入到TextEncoder中获得的向量与将image送入到imageEncoder中获得的向量进行比较，如果输入的text和image是成对的，则结果向量越近越好，反之，越远越好。
2.2 Decoder 2.2.1 若中间产物为小图 直接训练一个输入为小图，输出为大图的模型。
2.2.2 若中间产物为latent representation 训练一个Auto-encoder，即将输入的图像通过encoder可以获得一个latent representation，随后再经历Decoder还原成一张图，该图与原图进行对比，即完成训练流程。最终取用其中的decoder模块即可。
2.3 Generation Model 将noise加到encoder后的latent representation上，通过noise predictor预测出噪声，得到去噪后的图像
3 Diffusion Model 数学原理 3.1 VAE与Diffusion Model区别 3.2 Diffusion Model训练过程原理： 随机选取一张干净的原始图像初始化一个迭代次数t初始化一个噪声利用Noise predictor预测加噪后图片的噪声与真实噪声之间的差距，约束其越小越好
3.3 Diffusion Model推理过程原理： 4 图像生成模型原理 4.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-01-05T21:01:13+08:00">
    <meta property="article:modified_time" content="2024-01-05T21:01:13+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Stable Diffusion 基本原理</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="1_Diffusion_Model_0"></a>1 Diffusion Model的运作过程</h2> 
<p>输入一张和我们所需结果图<strong>尺寸一致</strong>的噪声图像，通过Denoise模块逐步减少noise，最终生成我们需要的效果图。<br> 图中Denoise模块虽然是同一个，但是它会根据<strong>不同step的输入图像</strong>和<strong>代表noise严重程度的参数</strong>选择denoise的程度。<br> <img src="https://images2.imgbox.com/68/0b/J08te66W_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="11_Denoise__4"></a>1.1 Denoise 模块的内部过程</h4> 
<p>根据我们输入带噪声的图像和去噪程度的参数，Denoise模块中的<strong>Noise pred模块会预测出图中的noise部分，此时输入图像和预测噪声的差即为该step的输出结果。</strong></p> 
<p>问：为何选择预测噪声做差而不是直接预测消除部分噪声后的图像？<br> 由于预测噪声的难度更低，如果直接预测带噪声后的图像其实就已经相当于可以实现图像的生成了。<br> <img src="https://images2.imgbox.com/be/c4/fNZgEJzj_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="12_Noise_predictor_10"></a>1.2 如何训练Noise_predictor</h4> 
<p>想要训练Noise_predictor预测出来噪声，我们需要提供噪声的Ground truth，这个如何获得？<br> <img src="https://images2.imgbox.com/6b/a4/euzRdGV9_o.png" alt="在这里插入图片描述"><br> 我们从训练数据集中随机抽取一张图像，然后人为给其加噪声，<strong>我们人为添加的噪声即是Noise_predictor中的groundTruth</strong>，该添加噪声的过程也被称为foward process。<br> <img src="https://images2.imgbox.com/29/64/pNZzZHgI_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="2_Stable_Diffusion_15"></a>2 Stable Diffusion</h2> 
<p>stable diffusion包括三大模块：TextEncoder、Generation Model、Decoder，三个模块独立训练，最终组合。<br> <img src="https://images2.imgbox.com/cd/a4/Tf43XrRh_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="21_TextEncoder_18"></a>2.1 TextEncoder</h4> 
<p><strong>TextEncoder对结果的影响很大，远大于diffusion model，增大TextEncoder模型，效果明显，而增大diffusion model模型，效果则没那么显著。</strong><br> <img src="https://images2.imgbox.com/59/92/a9HgEnlB_o.png" alt="在这里插入图片描述"></p> 
<h6><a id="211_FID_21"></a>2.1.1 FID的理解</h6> 
<p>将真实图片和生成图片分别输入到一个CNN分类器，假设他们都满足高斯分布，计算他们的距离，距离越小，说明生成的图片效果越好，距离越大说明生成的图片效果越差。注意，FID的计算需要充足数量的样本。<br> <img src="https://images2.imgbox.com/3c/c9/9gs2BvtD_o.png" alt="在这里插入图片描述"></p> 
<h6><a id="212_CLIP_score_24"></a>2.1.2 CLIP score的理解</h6> 
<p>clip分为TextEncoder和imageEncoder两个模块，将text输入到TextEncoder中获得的向量与将image送入到imageEncoder中获得的向量进行比较，如果输入的text和image是成对的，则结果向量越近越好，反之，越远越好。<br> <img src="https://images2.imgbox.com/a2/10/nMqqnsR6_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="22_Decoder_27"></a>2.2 Decoder</h4> 
<h6><a id="221__28"></a>2.2.1 若中间产物为小图</h6> 
<p>直接训练一个输入为小图，输出为大图的模型。<br> <img src="https://images2.imgbox.com/43/79/4lMvSDb3_o.png" alt="在这里插入图片描述"></p> 
<h6><a id="222_latent_representation_31"></a>2.2.2 若中间产物为latent representation</h6> 
<p>训练一个Auto-encoder，即将输入的图像通过encoder可以获得一个latent representation，随后再经历Decoder还原成一张图，该图与原图进行对比，即完成训练流程。最终取用其中的decoder模块即可。<br> <img src="https://images2.imgbox.com/22/87/c63Zycwy_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="23_Generation_Model_34"></a>2.3 Generation Model</h4> 
<p>将noise加到encoder后的latent representation上，通过noise predictor预测出噪声，得到去噪后的图像<br> <img src="https://images2.imgbox.com/76/13/5RBZaGEU_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="3_Diffusion_Model__37"></a>3 Diffusion Model 数学原理</h2> 
<h4><a id="31_VAEDiffusion_Model_38"></a>3.1 VAE与Diffusion Model区别</h4> 
<p><img src="https://images2.imgbox.com/19/6a/LBcRFerm_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="32_Diffusion_Model_40"></a>3.2 Diffusion Model训练过程原理：</h4> 
<ol><li>随机选取一张干净的原始图像</li><li>初始化一个迭代次数t</li><li>初始化一个噪声</li><li>利用Noise predictor预测加噪后图片的噪声与真实噪声之间的差距，约束其越小越好<br> <img src="https://images2.imgbox.com/cd/b4/yMzZ40zl_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/8a/ed/nVYK74PE_o.png" alt="在这里插入图片描述"></li></ol> 
<h4><a id="33_Diffusion_Model_47"></a>3.3 Diffusion Model推理过程原理：</h4> 
<p><img src="https://images2.imgbox.com/b9/15/pPLUZJbM_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="4__49"></a>4 图像生成模型原理</h2> 
<h4><a id="41_Maximum_Likelihood_Estimation_50"></a>4.1 Maximum Likelihood Estimation</h4> 
<ol><li>在训练数据P<sub>data</sub>中取样x<sup>1</sup>, x<sup>2</sup>…x<sup>m</sup></li><li>计算p<sub>θ</sub>产生x<sub>i</sub>（x<sup>1</sup>, x<sup>2</sup>…x<sup>m</sup>）的几率</li><li>找一个θ，使得p<sub>θ</sub>产生x<sub>i</sub>（x<sup>1</sup>, x<sup>2</sup>…x<sup>m</sup>）的几率相乘结果越大越好<br> <img src="https://images2.imgbox.com/73/fa/2yp6mQ1T_o.png" alt="在这里插入图片描述"></li></ol> 
<h4><a id="42__55"></a>4.2 公式推导</h4> 
<p><img src="https://images2.imgbox.com/47/2a/7OnfbUmV_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="43_VAE_57"></a>4.3 VAE计算</h4> 
<p><img src="https://images2.imgbox.com/32/14/xCzPUQm1_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/08/9f/Ws4HnQhP_o.png" alt="在这里插入图片描述">DDPM<br> <img src="https://images2.imgbox.com/5b/0f/tJRYrYwj_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/f0/96/943ekzIo_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f89938e9d18e15df3b7c1d79374632e4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Android Studio初学者实例：ContentProvider读取手机通讯录</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/52178ccbb8b40eccebed55fd2f4e829d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">操作系统课程设计——文件管理模拟 C&#43;&#43;版</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>