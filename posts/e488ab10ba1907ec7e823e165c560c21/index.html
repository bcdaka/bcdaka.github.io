<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>国产版Sora复现——智谱AI开源CogVideoX-2b 本地部署复现实践教程 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/e488ab10ba1907ec7e823e165c560c21/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="国产版Sora复现——智谱AI开源CogVideoX-2b 本地部署复现实践教程">
  <meta property="og:description" content="目录 一、CogVideoX简介二、CogVideoX部署实践流程2.1、创建丹摩实例2.2、配置环境和依赖2.3、上传模型与配置文件2.4、开始运行 最后 一、CogVideoX简介 智谱AI在8月6日宣布了一个令人兴奋的消息：他们将开源视频生成模型CogVideoX。目前，其提示词上限为 226 个 token，视频长度为 6 秒，帧率为 8 帧 / 秒，视频分辨率为 720*480，而这仅仅是初代，性能更强参数量更大的模型正在路上。先看两个效果（均为个人本次实践复现得出）：
CogVideoX生成视频1
CogVideo生成视频2
首先简单介绍下原理，CogVideoX的核心在于它的3D变分自编码器，这项技术能够将视频数据压缩至原来的2%，极大地降低了模型处理视频时所需的计算资源，还巧妙地保持了视频帧与帧之间的连贯性，有效避免了视频生成过程中可能出现的闪烁问题。
为了进一步提升内容的连贯性，CogVideoX采用了3D旋转位置编码（3D RoPE）技术，使得模型在处理视频时能够更好地捕捉时间维度上的帧间关系，建立起视频中的长期依赖关系，从而生成更加流畅和连贯的视频序列。
在可控性方面，智谱AI研发了一款端到端的视频理解模型，这个模型能够为视频数据生成精确且与内容紧密相关的描述。这一创新极大地增强了CogVideoX对文本的理解和对用户指令的遵循能力，确保了生成的视频不仅与用户的输入高度相关，而且能够处理超长且复杂的文本提示。
代码仓库：https://github.com/THUDM/CogVideo模型下载：https://huggingface.co/THUDM/CogVideoX-2b技术报告：https://github.com/THUDM/CogVideo/blob/main/resources/CogVideoX.pdf丹摩智算平台：https://damodel.com/register?source=1D5686A0 本篇博客将详细介绍，使用丹摩服务器部署和初步使用CogVideoX的实践流程。
二、CogVideoX部署实践流程 2.1、创建丹摩实例 首先进入控制台，选择GPU云实例，点击创建实例：
由于CogVideoX在FP-16 精度下的推理至少需 18GB 显存，微调则需要 40GB 显存，我们这里可以选择L40S显卡（推荐）或者4090显卡，硬盘可以选择默认的100GB系统盘和50GB数据盘，镜像选择PyTorch2.3.0、Ubuntu-22.04，CUDA12.1镜像，创建并绑定密钥对，最后启动。
2.2、配置环境和依赖 进入JupyterLab后，打开终端，首先输入git clone https://github.com/THUDM/CogVideo.git，拉取CogVideo代码的仓库，如遇到github连接超时，可以使用本地下载压缩包然后上传到服务器解压，拉取成功后会显示CogVideo-main的文件夹如下：
其次，cd进入CogVideo-main文件夹，输入pip install -r requirements.txt安装对应依赖:
其中第一个依赖需要从GitHub仓库中安装特定版本的diffusers包，在安装中可能会出现Running command git clone --filter=blob:non过久：
这种情况可以选择使用本地下载diffusers-0.30.0.dev0-py3-none-any.whl文件（夸克网盘链接：https://pan.quark.cn/s/67d4bf445556）
再使用pip install diffusers-0.30.0.dev0-py3-none-any.whl安装对应版本的diffusers
然后再删除requirements.txt文件中的git&#43;https://github.com/huggingface/diffusers.git@878f609aa5ce4a78fea0f048726889debde1d7e8#egg=diffusers那一行
最后，继续使用pip安装剩下的依赖：
以上依赖安装好后，可以在终端输入python，然后输入以下代码进行测试：
import torch from diffusers import CogVideoXPipeline from diffusers.utils import export_to_video 显示如下状态，没有报错就说明依赖安装成功！
2.3、上传模型与配置文件 除了配置代码文件和项目依赖，还需要上传CogVideoX模型文件和对应的配置文件。打开huggingface，进入https://huggingface.co/THUDM/CogVideoX-2b/tree/main仓库，找到Files and versions目录，将其中的所有模型和配置文件全部下载到本地。
一共大概9GB&#43;大小的文件，下载完成后的目录如下：
然后点击丹摩控制台-文件存储-上传文件，将刚刚下载好的整个CogVideo文件夹上传，上传好后的文件存在实例的/root/shared-storage目录。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-06T22:32:42+08:00">
    <meta property="article:modified_time" content="2024-08-06T22:32:42+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">国产版Sora复现——智谱AI开源CogVideoX-2b 本地部署复现实践教程</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p></p> 
 <div class="toc"> 
  <h4>目录</h4> 
  <ul><li><a href="#CogVideoX_2" rel="nofollow">一、CogVideoX简介</a></li><li><a href="#CogVideoX_23" rel="nofollow">二、CogVideoX部署实践流程</a></li><li><ul><li><a href="#21_24" rel="nofollow">2.1、创建丹摩实例</a></li><li><a href="#22_29" rel="nofollow">2.2、配置环境和依赖</a></li><li><a href="#23_53" rel="nofollow">2.3、上传模型与配置文件</a></li><li><a href="#24_64" rel="nofollow">2.4、开始运行</a></li></ul> 
   </li><li><a href="#_113" rel="nofollow">最后</a></li></ul> 
 </div> 
 <p></p> 
</blockquote> 
<h2><a id="CogVideoX_2"></a>一、CogVideoX简介</h2> 
<p>智谱AI在8月6日宣布了一个令人兴奋的消息：他们将开源视频生成模型CogVideoX。目前，其提示词上限为 <strong>226 个 token，视频长度为 6 秒，帧率为 8 帧 / 秒，视频分辨率为 720*480</strong>，而这仅仅是初代，性能更强参数量更大的模型正在路上。先看两个效果（均为个人本次实践复现得出）：</p> 
<p></p> 
<div class="csdn-video-box" data-report-view='{"spm":"3001.10261","extra":{"id":"mcNXOLpy-1722952910790"}}'> 
 <iframe id="mcNXOLpy-1722952910790" frameborder="0" src="https://live.csdn.net/v/embed/415477" allowfullscreen="true" data-mediaembed="csdn"></iframe> 
 <p>CogVideoX生成视频1</p> 
</div> 
<p></p> 
<p></p> 
<div class="csdn-video-box" data-report-view='{"spm":"3001.10261","extra":{"id":"VTX9m1f5-1722952917312"}}'> 
 <iframe id="VTX9m1f5-1722952917312" frameborder="0" src="https://live.csdn.net/v/embed/415478" allowfullscreen="true" data-mediaembed="csdn"></iframe> 
 <p>CogVideo生成视频2</p> 
</div> 
<p></p> 
<p>首先简单介绍下原理，CogVideoX的核心在于它的<strong>3D变分自编码器</strong>，这项技术能够将视频数据压缩至原来的2%，极大地降低了模型处理视频时所需的计算资源，还巧妙地保持了视频帧与帧之间的连贯性，有效避免了视频生成过程中可能出现的闪烁问题。<br> <img src="https://images2.imgbox.com/0c/2c/oE4w2DOF_o.png" alt="在这里插入图片描述"><br> 为了进一步提升内容的连贯性，<strong>CogVideoX采用了3D旋转位置编码（3D RoPE）技术，使得模型在处理视频时能够更好地捕捉时间维度上的帧间关系，建立起视频中的长期依赖关系，从而生成更加流畅和连贯的视频序列。</strong></p> 
<p>在可控性方面，智谱AI研发了一款<strong>端到端的视频理解模型</strong>，这个模型能够为视频数据生成精确且与内容紧密相关的描述。这一创新极大地增强了CogVideoX对文本的理解和对用户指令的遵循能力，确保了生成的视频不仅与用户的输入高度相关，而且能够处理超长且复杂的文本提示。</p> 
<ul><li>代码仓库：<code>https://github.com/THUDM/CogVideo</code></li><li>模型下载：<code>https://huggingface.co/THUDM/CogVideoX-2b</code></li><li>技术报告：<code>https://github.com/THUDM/CogVideo/blob/main/resources/CogVideoX.pdf</code></li><li>丹摩智算平台：<code>https://damodel.com/register?source=1D5686A0</code></li></ul> 
<p>本篇博客将详细介绍，<strong>使用丹摩服务器部署和初步使用CogVideoX的实践流程</strong>。</p> 
<h2><a id="CogVideoX_23"></a>二、CogVideoX部署实践流程</h2> 
<h3><a id="21_24"></a>2.1、创建丹摩实例</h3> 
<p>首先进入控制台，选择GPU云实例，点击创建实例：<br> <img src="https://images2.imgbox.com/51/7b/aMA4wynS_o.png" alt="在这里插入图片描述"><br> 由于CogVideoX在FP-16 精度下的推理至少需 18GB 显存，微调则需要 40GB 显存，我们这里可以选择L40S显卡（推荐）或者4090显卡，硬盘可以选择默认的100GB系统盘和50GB数据盘，镜像选择PyTorch2.3.0、Ubuntu-22.04，CUDA12.1镜像，创建并绑定密钥对，最后启动。<br> <img src="https://images2.imgbox.com/f4/67/4AwmxMq2_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="22_29"></a>2.2、配置环境和依赖</h3> 
<p>进入JupyterLab后，打开终端，首先输入<code>git clone https://github.com/THUDM/CogVideo.git</code>，拉取CogVideo代码的仓库，如遇到github连接超时，可以使用本地下载压缩包然后上传到服务器解压，拉取成功后会显示CogVideo-main的文件夹如下：<br> <img src="https://images2.imgbox.com/a5/cc/xCjz9C0A_o.png" alt="在这里插入图片描述"><br> 其次，cd进入CogVideo-main文件夹，输入<code>pip install -r requirements.txt</code>安装对应依赖:<br> <img src="https://images2.imgbox.com/fa/b1/fnH8UbXe_o.png" alt="在这里插入图片描述"><br> 其中第一个依赖需要从GitHub仓库中安装特定版本的diffusers包，在安装中可能会出现<code>Running command git clone --filter=blob:non</code>过久：<br> <img src="https://images2.imgbox.com/f8/7c/hNLO6CIK_o.png" alt="在这里插入图片描述"><br> 这种情况可以选择使用本地下载<a href="https://download.csdn.net/download/air__Heaven/89618817">diffusers-0.30.0.dev0-py3-none-any.whl文件</a>（夸克网盘链接：https://pan.quark.cn/s/67d4bf445556）</p> 
<p>再使用<code>pip install diffusers-0.30.0.dev0-py3-none-any.whl</code>安装对应版本的diffusers</p> 
<p>然后再删除requirements.txt文件中的<code>git+https://github.com/huggingface/diffusers.git@878f609aa5ce4a78fea0f048726889debde1d7e8#egg=diffusers</code>那一行</p> 
<p>最后，继续使用pip安装剩下的依赖：<br> <img src="https://images2.imgbox.com/cc/fd/l74OVotW_o.png" alt="在这里插入图片描述"><br> 以上依赖安装好后，可以在终端输入python，然后输入以下代码进行测试：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> diffusers <span class="token keyword">import</span> CogVideoXPipeline
<span class="token keyword">from</span> diffusers<span class="token punctuation">.</span>utils <span class="token keyword">import</span> export_to_video
</code></pre> 
<p>显示如下状态，没有报错就说明依赖安装成功！<br> <img src="https://images2.imgbox.com/97/ea/C2suK2He_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="23_53"></a>2.3、上传模型与配置文件</h3> 
<p>除了配置代码文件和项目依赖，还需要上传CogVideoX模型文件和对应的配置文件。打开huggingface，进入<code>https://huggingface.co/THUDM/CogVideoX-2b/tree/main</code>仓库，找到Files and versions目录，将其中的所有模型和配置文件全部下载到本地。<br> <img src="https://images2.imgbox.com/59/8e/WZzYmm11_o.png" alt="在这里插入图片描述"></p> 
<p>一共大概9GB+大小的文件，下载完成后的目录如下：<br> <img src="https://images2.imgbox.com/dc/59/zMYSmJUz_o.png" alt="在这里插入图片描述"></p> 
<p>然后点击丹摩控制台-文件存储-上传文件，将刚刚下载好的整个CogVideo文件夹上传，上传好后的文件存在实例的/root/shared-storage目录。<br> <img src="https://images2.imgbox.com/4b/5a/1gtNVv0H_o.png" alt="在这里插入图片描述"><br> 上传完成后，继续打开终端，cd进入/root/shared-storage，输入ls可以看到刚刚上传好的CogVideo的文件夹已经显示在文件目录中。<br> <img src="https://images2.imgbox.com/b7/50/HkQvHeVw_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="24_64"></a>2.4、开始运行</h3> 
<p>上传完成后，在CogVideo-main文件新建一个test.py文件<br> <img src="https://images2.imgbox.com/cc/4a/krHXlsk7_o.png" alt="在这里插入图片描述"><br> test.py代码内容如下，主要使用diffusers库中的CogVideoXPipeline模型，加载了一个预训练的CogVideo模型，然后根据一个详细的文本描述（prompt），生成对应视频。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> diffusers <span class="token keyword">import</span> CogVideoXPipeline
<span class="token keyword">from</span> diffusers<span class="token punctuation">.</span>utils <span class="token keyword">import</span> export_to_video

<span class="token comment"># prompt里写自定义想要生成的视频内容</span>
prompt <span class="token operator">=</span> <span class="token string">"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance."</span>

pipe <span class="token operator">=</span> CogVideoXPipeline<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    <span class="token string">"root/shared-storage/CogVideo"</span><span class="token punctuation">,</span> <span class="token comment"># 这里填CogVideo模型存放的位置，此处是放在了丹摩实例的共享空间，也可以放到项目文件夹里</span>
    torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16
<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>

<span class="token comment"># 参数do_classifier_free_guidance设置为True可以启用无分类器指导，增强生成内容一致性和多样性</span>
<span class="token comment"># num_videos_per_prompt控制每个prompt想要生成的视频数量</span>
<span class="token comment"># max_sequence_length控制输入序列的最大长度</span>
prompt_embeds<span class="token punctuation">,</span> _ <span class="token operator">=</span> pipe<span class="token punctuation">.</span>encode_prompt<span class="token punctuation">(</span>
    prompt<span class="token operator">=</span>prompt<span class="token punctuation">,</span>
    do_classifier_free_guidance<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    num_videos_per_prompt<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    max_sequence_length<span class="token operator">=</span><span class="token number">226</span><span class="token punctuation">,</span>
    device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">,</span>
    dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">,</span>
<span class="token punctuation">)</span>

video <span class="token operator">=</span> pipe<span class="token punctuation">(</span>
    num_inference_steps<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span>
    guidance_scale<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span>
    prompt_embeds<span class="token operator">=</span>prompt_embeds<span class="token punctuation">,</span>
<span class="token punctuation">)</span><span class="token punctuation">.</span>frames<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

export_to_video<span class="token punctuation">(</span>video<span class="token punctuation">,</span> <span class="token string">"output.mp4"</span><span class="token punctuation">,</span> fps<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>
</code></pre> 
<p>运行成功后，可以在当前文件夹中找到对应prompt生成的output.mp4视频：<br> <img src="https://images2.imgbox.com/f2/0a/PXlJD3rD_o.png" alt="在这里插入图片描述"><br> 最近正好奥运会，我还试着生成了一些比较有难度的运动员的视频，感觉效果还不错，后面再研究研究视频的prompt怎么写。</p> 
<p></p> 
<div class="csdn-video-box" data-report-view='{"spm":"3001.10261","extra":{"id":"iKU5t0nd-1722954699245"}}'> 
 <iframe id="iKU5t0nd-1722954699245" frameborder="0" src="https://live.csdn.net/v/embed/415482" allowfullscreen="true" data-mediaembed="csdn"></iframe> 
 <p>CogVideo生成视频4</p> 
</div> 
<p></p> 
<p><img src="https://images2.imgbox.com/8f/5c/czpPVOsQ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/82/16/9xWiAUoz_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_113"></a>最后</h2> 
<p>💖 个人简介：人工智能领域硕士毕业，某央企AI工程师</p> 
<p>📝 关注我：<a href="https://blog.csdn.net/air__Heaven">中杯可乐多加冰</a></p> 
<p>🎉 支持我：点赞👍+收藏⭐️+留言📝</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7fa027e1542f9a04e0f85f5bc257f293/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">程序编译及链接</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/089eab8d7370c5296cbea5e77edbcf40/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">如何在生成式AI里使用 Ray Data 进行大规模 RAG 应用的 Embedding Inference</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>