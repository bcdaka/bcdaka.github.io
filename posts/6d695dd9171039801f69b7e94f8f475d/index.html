<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AutoGen 检索增强生成（RAG）功能解析 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/6d695dd9171039801f69b7e94f8f475d/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="AutoGen 检索增强生成（RAG）功能解析">
  <meta property="og:description" content="目录
一、什么是检索增强（RAG） ？
二、AutoGen 检索增强（RAG）
三、实例
本文主要对 AutoGen 检索增强生成（RAG）功能进行解析，并通过两个实例来说明。
一、什么是检索增强（RAG） ？ 检索增强生成 (RAG) 是大模型的一种扩展技术，它将大模型与外部知识检索相结合，以提高生成的响应的质量和相关性，例如：FastGPT。
RAG架构图如下所示。
主要流程如下所示：
（1）用户（User）提出问题（Query）;
（2）在数据库（Data Source）中查询与问题（Query）相关的内容，这些内容将作为 LLM 的上下文（Text），将问题（Query）和上下文（Text）一起传给 LLM；
（3）LLM 根据 Data Source 中检索到的内容组织回答，最终生成回答，返回给用户；
这样就很大程度上避免了 LLM 幻觉问题，充分利用了 LLM 语言生成的能力。
二、AutoGen 检索增强（RAG） AutoGen 虽然是多代理协作的框架，AutoGen 同样也支持 RAG 功能，AutoGen 是通过 AssistantAgent 和 RetrieveUserProxyAgent 类构建代理聊天实现。
2.1 RetrieveUserProxyAgent 函数
RetrieveUserProxyAgent 根据问题的嵌入检索文档块，并将它们与问题一起发送给检索增强助手，该类继承自 UserProxyAgent。
class RetrieveUserProxyAgent(UserProxyAgent) 主要函数为 __init__，如下所示。
def __init__(name=&#34;RetrieveChatAgent&#34;, human_input_mode: Literal[&#34;ALWAYS&#34;, &#34;NEVER&#34;, &#34;TERMINATE&#34;] = &#34;ALWAYS&#34;, is_termination_msg: Optional[Callable[[Dict], bool]] = None, retrieve_config: Optional[Dict] = None, **kwargs) 参数介绍：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-09-01T11:09:18+08:00">
    <meta property="article:modified_time" content="2024-09-01T11:09:18+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AutoGen 检索增强生成（RAG）功能解析</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%EF%BC%88RAG%EF%BC%89%C2%A0%EF%BC%9F-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%EF%BC%88RAG%EF%BC%89%C2%A0%EF%BC%9F" rel="nofollow">一、什么是检索增强（RAG） ？</a></p> 
<p id="%E4%BA%8C%E3%80%81AutoGen%20%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%EF%BC%88RAG%EF%BC%89-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81AutoGen%20%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%EF%BC%88RAG%EF%BC%89" rel="nofollow">二、AutoGen 检索增强（RAG）</a></p> 
<p id="%E4%B8%89%E3%80%81%E5%AE%9E%E4%BE%8B-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81%E5%AE%9E%E4%BE%8B" rel="nofollow">三、实例</a></p> 
<hr id="hr-toc"> 
<p>本文主要对 AutoGen 检索增强生成（RAG）功能进行解析，并通过两个实例来说明。</p> 
<h2 id="%E4%B8%80%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%EF%BC%88RAG%EF%BC%89%C2%A0%EF%BC%9F">一、什么是检索增强（RAG） ？</h2> 
<p>检索增强生成 (RAG) 是大模型的一种扩展技术，它将大模型与外部知识检索相结合，以提高生成的响应的质量和相关性，例如：FastGPT。</p> 
<p>RAG架构图如下所示。</p> 
<p><img alt="" height="900" src="https://images2.imgbox.com/c3/12/HfCIJBkv_o.png" width="1200"></p> 
<p>主要流程如下所示：</p> 
<p>（1）用户（User）提出问题（Query）;</p> 
<p>（2）在数据库（Data Source）中查询与问题（Query）相关的内容，这些内容将作为 LLM 的上下文（Text），将问题（Query）和上下文（Text）一起传给 LLM；</p> 
<p>（3）LLM 根据 Data Source 中检索到的内容组织回答，最终生成回答，返回给用户；</p> 
<p>这样就很大程度上避免了 LLM 幻觉问题，充分利用了 LLM 语言生成的能力。</p> 
<p></p> 
<h2 id="%E4%BA%8C%E3%80%81AutoGen%20%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%EF%BC%88RAG%EF%BC%89">二、AutoGen 检索增强（RAG）</h2> 
<p>AutoGen 虽然是多代理协作的框架，AutoGen 同样也支持 RAG 功能，AutoGen 是通过 AssistantAgent 和 RetrieveUserProxyAgent 类构建代理聊天实现。</p> 
<p>2.1 RetrieveUserProxyAgent 函数</p> 
<p>RetrieveUserProxyAgent 根据问题的嵌入检索文档块，并将它们与问题一起发送给检索增强助手，该类继承自 UserProxyAgent。</p> 
<pre><code class="language-python">class RetrieveUserProxyAgent(UserProxyAgent)</code></pre> 
<p> 主要函数为 __init__，如下所示。</p> 
<pre><code class="language-python">def __init__(name="RetrieveChatAgent",
             human_input_mode: Literal["ALWAYS", "NEVER",
                                       "TERMINATE"] = "ALWAYS",
             is_termination_msg: Optional[Callable[[Dict], bool]] = None,
             retrieve_config: Optional[Dict] = None,
             **kwargs)</code></pre> 
<p><strong>参数介绍：</strong></p> 
<p><strong>name:</strong>  代理名称；</p> 
<p><strong>human_input_mode：</strong>详细介绍见<a class="link-info" href="https://blog.csdn.net/nyist_zxp/article/details/141613763" title="AutoGen ConversableAgent 基类解析">AutoGen ConversableAgent 基类解析</a>中关于 <strong>human_input_mode</strong> 的介绍；</p> 
<p><strong>is_termination_msg：</strong>同 <strong>human_input_mode </strong>字段介绍；</p> 
<p><strong>retrieve_config：</strong>dict or None 类型，这个字段是<strong>重点！</strong>默认是 None，可以包含很多参数，下面列举下主要的参数：</p> 
<p><strong>（1）task：</strong>检索增强任务类型，可选项：<strong>code</strong>、<strong>qa</strong>、<strong>default</strong>。默认值是 <strong>default</strong>，同时支持 <strong>code</strong> 和 <strong>qa</strong>，并提供来源。</p> 
<p><strong>（2）chunk_token_size：</strong>分块大小（通常将检索的原文切分为多个块，通过 Embedding 模型对比问题和分块的相似度，从而判断是否相似），默认值 <strong>max_tokens * 0.4</strong>；</p> 
<p><strong>（3）model：</strong>用于检索聊天的模型，默认值 <strong>gpt-4</strong>；</p> 
<p><strong>（4）embedding_model：</strong>用于检索中文本的向量化，默认值 <strong>all-MiniLM-L6-v2</strong>。可用模型列表：<a href="https://www.sbert.net/docs/pretrained_models.html" rel="nofollow" title="https://www.sbert.net/docs/pretrained_models.html">https://www.sbert.net/docs/pretrained_models.html</a>，官方推荐使用 <strong>all-mpnet-base-v2</strong> 模型；</p> 
<p><strong>（5）docs_path：</strong>文档目录的路径，可以是单个文件的路径、单个文件的url或目录、文件和url的列表。</p> 
<p><strong>（6）custom_text_types：docs_path</strong> 字段中要处理的文件类型列表。默认值为 <strong>autogen.retrieve_utils.TEXT_FORMATS</strong> 中支持的类型。</p> 
<p>下面通过两个实例进行说明。</p> 
<p></p> 
<h2 id="%E4%B8%89%E3%80%81%E5%AE%9E%E4%BE%8B">三、实例</h2> 
<p>因为我这边 LLM 采用的是开源大模型 llama-3.1-405b 中文支持感觉不太好，所以下面的实例中还是采用英文进行介绍。</p> 
<p>实例主要是询问大模型“介绍下AutoGen”，并提供了 AutoGen 在线文档进行内容检索，代码如下所示。</p> 
<pre><code class="language-python">import os
import chromadb
from autogen import AssistantAgent, config_list_from_json
from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent

# 配置LLM
config_list = config_list_from_json(
    env_or_file="OAI_CONFIG_LIST",
)

# 助手
assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant.",
    llm_config={
        "timeout": 600,
        "cache_seed": 42,
        "config_list": config_list,
    },
)

# 检索增强代理
ragproxyagent = RetrieveUserProxyAgent(
    name="ragproxyagent",
    human_input_mode="NEVER",
    max_consecutive_auto_reply=3,
    retrieve_config={
        "task": "code",
        "docs_path": [
            "https://github.com/microsoft/autogen/blob/main/website/docs/Getting-Started.mdx",
            "https://github.com/microsoft/autogen/blob/main/website/docs/tutorial/introduction.ipynb",
            os.path.join(os.path.abspath(""), "..", "website", "docs"),
        ],
        "custom_text_types": ["mdx"],
        "chunk_token_size": 2000,
        "model": config_list[0]["model"],
        "client": chromadb.PersistentClient(path="/tmp/chromadb"),
        "embedding_model": "all-mpnet-base-v2",
        "get_or_create": True,  # set to False if you don't want to reuse an existing collection, but you'll need to remove the collection manually
    },
    code_execution_config=False,  # set to False if you don't want to execute the code
)


code_problem = "Please introduce AutoGen."

# 开始对话
ragproxyagent.initiate_chat(
    assistant, 
    message=ragproxyagent.message_generator,
    problem=code_problem,
    search_string="AutoGen"
)  # search_string is used as an extra filter for the embeddings search, in this case, we only want to search documents that contain "spark".</code></pre> 
<p>输出如下所示。</p> 
<pre><code class="language-python">(autogenstudy) D:\code\autogenstudio_images\example&gt;python RetrieveUserProxyAgent.py
D:\Software\anaconda3\envs\autogenstudy\lib\site-packages\transformers\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Trying to create collection.
2024-08-31 14:00:25,791 - autogen.agentchat.contrib.retrieve_user_proxy_agent - INFO - Use the existing collection `autogen-docs`.
File D:\code\autogenstudio_images\example\..\website\docs does not exist. Skipping.
2024-08-31 14:00:29,757 - autogen.agentchat.contrib.retrieve_user_proxy_agent - INFO - Found 3 chunks.
2024-08-31 14:00:29,764 - autogen.agentchat.contrib.vectordb.chromadb - INFO - No content embedding is provided. Will use the VectorDB's embedding function to generate the content embedding.
Number of requested results 20 is greater than number of elements in index 7, updating n_results = 7
VectorDB returns doc_ids:  [['99947a71', 'b947501f']]
Model meta/llama-3.1-405b-instruct not found. Using cl100k_base encoding.
Adding content of doc 99947a71 to context.
Model meta/llama-3.1-405b-instruct not found. Using cl100k_base encoding.
ragproxyagent (to assistant):

You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the
context provided by the user.
If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.
For code generation, you must obey the following rules:
Rule 1. You MUST NOT install any packages because all the packages needed are already installed.
Rule 2. You must follow the formats below to write your code:
```language
# your code
```

User's question is: Please introduce AutoGen.

Context is: ### Main Features

* AutoGen enables building next\-gen LLM applications based on [multi\-agent
conversations](/microsoft/autogen/blob/main/docs/Use-Cases/agent_chat) with minimal effort. It simplifies
the orchestration, automation, and optimization of a complex LLM workflow. It
maximizes the performance of LLM models and overcomes their weaknesses.
* It supports [diverse conversation
patterns](/microsoft/autogen/blob/main/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns)
for complex workflows. With customizable and conversable agents, developers can
use AutoGen to build a wide range of conversation patterns concerning
conversation autonomy, the number of agents, and agent conversation topology.
* It provides a collection of working systems with different complexities. These
systems span a [wide range of
applications](/microsoft/autogen/blob/main/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen)
from various domains and complexities. This demonstrates how AutoGen can
easily support diverse conversation patterns.

AutoGen is powered by collaborative [research studies](/microsoft/autogen/blob/main/docs/Research) from
Microsoft, Penn State University, and University of Washington.

### Quickstart

```
pip install pyautogen
```

```
import os
from autogen import AssistantAgent, UserProxyAgent

llm_config = {"model": "gpt-4", "api_key": os.environ["OPENAI_API_KEY"]}
assistant = AssistantAgent("assistant", llm_config=llm_config)
user_proxy = UserProxyAgent("user_proxy", code_execution_config=False)

# Start the chat
user_proxy.initiate_chat(
    assistant,
    message="Tell me a joke about NVDA and TESLA stock prices.",
)
```

```
&lt;/TabItem&gt;
&lt;TabItem value="local" label="Local execution" default&gt;

```

:::warning
When asked, be sure to check the generated code before continuing to ensure it is safe to run.
:::

```
import os
import autogen
from autogen import AssistantAgent, UserProxyAgent

llm_config = {"model": "gpt-4", "api_key": os.environ["OPENAI_API_KEY"]}
assistant = AssistantAgent("assistant", llm_config=llm_config)

user_proxy = UserProxyAgent(
    "user_proxy", code_execution_config={"executor": autogen.coding.LocalCommandLineCodeExecutor(work_dir="coding")}
)

# Start the chat
user_proxy.initiate_chat(
    assistant,
    message="Plot a chart of NVDA and TESLA stock price change YTD.",
)
```

```
&lt;/TabItem&gt;
&lt;TabItem value="docker" label="Docker execution" default&gt;

```

```
import os
import autogen
from autogen import AssistantAgent, UserProxyAgent

llm_config = {"model": "gpt-4", "api_key": os.environ["OPENAI_API_KEY"]}

with autogen.coding.DockerCommandLineCodeExecutor(work_dir="coding") as code_executor:
    assistant = AssistantAgent("assistant", llm_config=llm_config)
    user_proxy = UserProxyAgent(
        "user_proxy", code_execution_config={"executor": code_executor}
    )

    # Start the chat
    user_proxy.initiate_chat(
        assistant,
        message="Plot a chart of NVDA and TESLA stock price change YTD. Save the plot to a file called plot.png",
    )
```

Open `coding/plot.png` to see the generated plot.

```
&lt;/TabItem&gt;

```

:::tip
Learn more about configuring LLMs for agents [here](/microsoft/autogen/blob/main/docs/topics/llm_configuration).
:::

#### Multi\-Agent Conversation Framework

Autogen enables the next\-gen LLM applications with a generic multi\-agent conversation framework. It offers customizable and conversable agents which integrate LLMs, tools, and humans.
By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. For [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),

The figure below shows an example conversation flow with AutoGen.

[![Agent Chat Example](/microsoft/autogen/raw/main/img/chat_example.png)](/microsoft/autogen/blob/main/img/chat_example.png)

### Where to Go Next?

* Go through the [tutorial](/microsoft/autogen/blob/main/docs/tutorial/introduction) to learn more about the core concepts in AutoGen
* Read the examples and guides in the [notebooks section](/microsoft/autogen/blob/main/docs/notebooks)
* Understand the use cases for [multi\-agent conversation](/microsoft/autogen/blob/main/docs/Use-Cases/agent_chat) and [enhanced LLM inference](/microsoft/autogen/blob/main/docs/Use-Cases/enhanced_inference)
* Read the [API](/microsoft/autogen/blob/main/docs/reference/agentchat/conversable_agent) docs
* Learn about [research](/microsoft/autogen/blob/main/docs/Research) around AutoGen
* Chat on [Discord](https://aka.ms/autogen-dc)
* Follow on [Twitter](https://twitter.com/pyautogen)
* See our [roadmaps](https://aka.ms/autogen-roadmap)

If you like our project, please give it a [star](https://github.com/microsoft/autogen/stargazers) on GitHub. If you are interested in contributing, please read [Contributor's Guide](/microsoft/autogen/blob/main/docs/contributor-guide/contributing).

\&lt;iframe
 src\="[https://ghbtns.com/github\-btn.html?user\=microsoft\&amp;repo\=autogen\&amp;type\=star\&amp;count\=true\&amp;size\=large](https://ghbtns.com/github-btn.html?user=microsoft&amp;repo=autogen&amp;type=star&amp;count=true&amp;size=large)"
 frameborder\="0"
 scrolling\="0"
 width\="170"
 height\="30"
 title\="GitHub"
\&gt;\&lt;/iframe\&gt;


Footer
------

 © 2024 GitHub, Inc.


### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

 You can’t perform that action at this time.



--------------------------------------------------------------------------------
assistant (to ragproxyagent):

AutoGen is an open-source Python library developed by Microsoft that simplifies the process of building next-gen LLM (Large Language Model) applications based on multi-agent conversations. It enables developers to create customizable and conversable agents that can integrate LLMs, tools, and humans, allowing for easy automation and optimization of complex LLM workflows.

AutoGen supports diverse conversation patterns, including conversation autonomy, multiple agents, and agent conversation topology, making it suitable for a wide range of applications. The library provides a collection of working systems with different complexities and is powered by collaborative research studies from Microsoft, Penn State University, and the University of Washington.

Key features of AutoGen include:

1. Multi-agent conversation framework: AutoGen enables next-gen LLM applications with a generic multi-agent conversation framework, allowing for customization and conversability.
2. Customizable agents: Developers can create agents that integrate LLMs, tools, and humans, making it easy to automate and optimize complex LLM workflows.
3. Diverse conversation patterns: AutoGen supports various conversation patterns, including conversation autonomy, multiple agents, and agent conversation topology.
4. Working systems: The library provides a collection of working systems with different complexities, demonstrating its versatility.

AutoGen is easy to use and provides a quickstart guide, with examples of local, Docker, and API execution. The library also includes extensive documentation, tutorials, and guides, making it accessible to developers of all levels.

Overall, AutoGen is a powerful library that enables the creation of advanced LLM applications with ease, making it an exciting tool for developers and researchers working with large language models.

--------------------------------------------------------------------------------</code></pre> 
<p></p> 
<p><strong>参考链接：</strong></p> 
<p>[1] <a href="https://microsoft.github.io/autogen/docs/topics/retrieval_augmentation/" rel="nofollow" title="Retrieval Augmentation | AutoGen">Retrieval Augmentation | AutoGen</a></p> 
<p>[2] <a href="https://microsoft.github.io/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent/" rel="nofollow" title="agentchat.contrib.retrieve_user_proxy_agent | AutoGen">agentchat.contrib.retrieve_user_proxy_agent | AutoGen</a> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a6d89fb0f5a7759fb7600df84cb71644/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">RabbitMQ简单介绍</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/919996ff17381bf4c81ee6c45cc8a473/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">html,css学习记录</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>