<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>使用LangChain和Llama-Index实现多重检索RAG - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/946059ceab4a71766b565873aee4b74b/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="使用LangChain和Llama-Index实现多重检索RAG">
  <meta property="og:description" content="大家好，在信息检索的世界里，查询扩展技术正引领着一场效率革命。本文将介绍这一技术的核心多查询检索，以及其是如何在LangChain和Llama-Index中得到应用的。
1.查询扩展 查询扩展是一种信息检索技术，通过在原始查询的基础上增加相关或同义的词汇和短语来优化搜索结果。这种方法能够丰富查询的语义，提高检索系统的准确性和相关性。
在查询扩展的众多策略中，多查询检索是其中的一种。它通过生成多个相关的查询请求，从而拓宽搜索范围，帮助用户更全面地获取所需信息。这种技术尤其适用于处理复杂的查询需求，能够有效提高信息检索的效率和质量。
2.机制 系统在接到查询请求后，会先通过高级语言模型生成一个与原查询相近的新查询。这个新查询随后用于在Llama-Index中检索相关文档，从而获取与原查询高度相关的信息，增强上下文理解，确保结果更精准、更符合用户的实际需求。
2次LLM交互：为精确生成查询，流程包括向大型语言模型（LLM）并行发出两次请求：初次使用gpt3模型，之后可能提升至gpt4或其他高级模型，以获取更丰富的查询结果。
3.实现方法 3.1 LangChain loader = UnstructuredPDFLoader(FILE_NAME) docs = loader.load() text_splitter = SentenceTransformersTokenTextSplitter() texts = text_splitter.split_documents(docs) emb = OpenAIEmbeddings(openai_api_key=openai.api_key) vec_db = Chroma.from_documents(documents=texts, embedding=emb) lc_model = ChatOpenAI(openai_api_key=openai.api_key, temperature=1.5) base_retriever = vec_db.as_retriever(k=K) final_retriever = MultiQueryRetriever.from_llm(base_retriever, lc_model) tmpl = &#34;&#34;&#34; You are an assistant to answer a question from user with a context. Context: {context} Question: {question} The response should be presented as a list of key points, after creating the title of the content, formatted in HTML with appropriate markup for clarity and organization.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-18T17:57:26+08:00">
    <meta property="article:modified_time" content="2024-04-18T17:57:26+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">使用LangChain和Llama-Index实现多重检索RAG</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>大家好，在信息检索的世界里，查询扩展技术正引领着一场效率革命。本文将介绍这一技术的核心多查询检索，以及其是如何在LangChain和Llama-Index中得到应用的。</p> 
<h3>1.查询扩展</h3> 
<p>查询扩展是一种信息检索技术，通过在原始查询的基础上增加相关或同义的词汇和短语来优化搜索结果。这种方法能够丰富查询的语义，提高检索系统的准确性和相关性。</p> 
<p>在查询扩展的众多策略中，多查询检索是其中的一种。它通过生成多个相关的查询请求，从而拓宽搜索范围，帮助用户更全面地获取所需信息。这种技术尤其适用于处理复杂的查询需求，能够有效提高信息检索的效率和质量。</p> 
<h3>2.机制</h3> 
<p>系统在接到查询请求后，会先通过高级语言模型生成一个与原查询相近的新查询。这个新查询随后用于在Llama-Index中检索相关文档，从而获取与原查询高度相关的信息，增强上下文理解，确保结果更精准、更符合用户的实际需求。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="230" src="https://images2.imgbox.com/1c/c8/RKWUaMty_o.png" width="647"></p> 
<p>2次LLM交互：为精确生成查询，流程包括向大型语言模型（LLM）并行发出两次请求：初次使用gpt3模型，之后可能提升至gpt4或其他高级模型，以获取更丰富的查询结果。</p> 
<h3>3.实现方法</h3> 
<h4>3.1 LangChain</h4> 
<pre><code class="language-python">loader = UnstructuredPDFLoader(FILE_NAME)
docs = loader.load()

text_splitter = SentenceTransformersTokenTextSplitter()
texts = text_splitter.split_documents(docs)

emb = OpenAIEmbeddings(openai_api_key=openai.api_key)
vec_db = Chroma.from_documents(documents=texts, embedding=emb)

lc_model = ChatOpenAI(openai_api_key=openai.api_key, temperature=1.5)
base_retriever = vec_db.as_retriever(k=K)
final_retriever = MultiQueryRetriever.from_llm(base_retriever, lc_model)

tmpl = """
You are an assistant to answer a question from user with a context.

Context:
{context}

Question:
{question}

The response should be presented as a list of key points, after creating the title of the content,
    formatted in HTML with appropriate markup for clarity and organization.
"""
prompt = ChatPromptTemplate.from_template(tmpl)
chain = {"question": RunnablePassthrough(), "context": final_retriever} \
        | prompt \
        | lc_model \
        | StrOutputParser() \

result = chain.invoke("Waht is the doc talking about?")
</code></pre> 
<p>MultiQueryRetriever 通过提供一套完整的类库，简化了任务执行过程。其核心机制是配备一个基础检索器，能够自动产生最多三个定制化的查询。这一过程中，检索操作的安全性和封装性得到了保障。</p> 
<h4>3.2 Llama-Index</h4> 
<p>实现Llama-Index颇具挑战，因为要求我们不仅要手动创建“生成的查询”，还得自行实现这些查询的检索流程。面对多个查询的需求，这里采用了必要的协程机制来确保过程的顺利进行。</p> 
<pre><code class="language-python">vector_index: BaseIndex = VectorStoreIndex.from_documents(
    docs,
    service_context=service_context, 
    show_progress=True,
)

base_retriever = vector_index.as_retriever(similarity_top_k=K)

class MultiQueriesRetriever(BaseRetriever):
    def __init__(self, base_retriever: BaseRetriever, model:OpenAI):
        self.template = PromptTemplate("""You are an AI language model assistant. Your task is to generate Five
    different versions of the given user question to retrieve relevant documents from a vector
    database. By generating multiple perspectives on the user question, your goal is to help
    the user overcome some of the limitations of the distance-based similarity search.
    Provide these alternative questions seperated by newlines.
    Original question: {question}""")
        self._retrievers = [base_retriever]
        self.base_retriever = base_retriever
        self.model = model
    
    def gen_queries(self, query) -&gt; List[str]:
        gen_queries_model = OpenAI(model="gpt-3-turbo", temperature=1.5)
        prompt = self.template.format(question=query)
        res = self.model.complete(prompt)
        return res.text.split("\n")

    async def run_gen_queries(self,generated_queries: List[str]) -&gt; List[NodeWithScore]:
        tasks = list(map(lambda q: self.base_retriever.aretrieve(q), generated_queries)) 
        res = await tqdm.gather(*tasks)
        return res[0]

    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        return list()

    async def _aretrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        query = query_bundle.query_str
        generated_queries = self.gen_queries(query)
        query_res = await self.run_gen_queries(generated_queries)
        return query_res


    
mr = MultiQueriesRetriever(base_retriever, li_model)

final_res = await RetrieverQueryEngine(mr).aquery(query_text)
</code></pre> 
<p>重要的是通过继承BaseRetriever类，将其功能与基础检索器相融合，以便根据生成的查询来检索相应的信息。由于这些生成的查询是通过协程机制来实现的，因此需要对_aretrieve方法进行重写。这部分内容就不再展开详细说明了。</p> 
<h4>3.3 子问题查询引擎</h4> 
<p>Llama-Index包含一个名为SubQuestionQueryEngine的类，它基本上能够满足我们的需求。这个类的特点是将复杂查询分解成多个子问题，而不是创建一个与原查询“相似”的新查询。根据文档说明，可以通过以下代码来使用这个类：</p> 
<pre><code class="language-python">query_engine_tools = [
    QueryEngineTool(
        query_engine=vector_query_engine,
        metadata=ToolMetadata(
            name="pg_essay",
            description="Paul Graham essay on What I Worked On",
        ),
    ),
]

query_engine = SubQuestionQueryEngine.from_defaults(
    query_engine_tools=query_engine_tools,
    use_async=True,
)

response = query_engine.query(
    "How was Paul Grahams life different before, during, and after YC?"
)
</code></pre> 
<p>SubQuestionQueryEngine的工作原理是将复杂的原始查询拆分成多个子问题，每个子问题都针对特定的数据源。这些子问题的答案不仅提供了必要的上下文信息，还为构建最终答案做出了贡献。每个子问题专门设计用来从相应的数据源中抽取关键信息。综合这些子问题的答案，可以得到对原始查询的完整回应。</p> 
<p>此外，SubQuestionQueryEngine能够将一个复杂的查询细化为多个子问题，并为每个子问题指定相应的查询引擎进行处理。一旦所有子问题都得到解答，这些答案将被汇总并传递给响应合成器，以生成最终的答案。SubQuestionQueryEngine根据SubQuestion中的tool_name属性来确定应该使用哪个QueryEngineTool来处理每个子问题。</p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/353e6aed733dda12f5b4e09536458df3/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">福彩双色球开奖数据爬虫工具-Python彩票数据爬虫实实战（附上所有代码）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/fb85f90d02cf25ddf845ae766fa9dfad/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">选择不再困难：Rust Web框架Actix、Rocket与Warp深度对比</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>