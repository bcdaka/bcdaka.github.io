<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Spark SQL编程初级实践 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/b6ded26c1ec7837d8d1dfcac1c7a93f0/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Spark SQL编程初级实践">
  <meta property="og:description" content="一、实验目的 （1）通过实验掌握Spark SQL的基本编程方法；
（2）熟悉RDD到DataFrame的转化方法；
（3）熟悉利用Spark SQL管理来自不同数据源的数据。
二、实验平台 操作系统：Ubuntu20.04
Spark版本：3.4.0
Python版本：3.8.3
数据库：MySQL
三、实验内容和过程 1．Spark SQL基本操作
将下列JSON格式数据复制到Ubuntu系统中，并保存命名为employee.json。
{ &#34;id&#34;:1 , &#34;name&#34;:&#34; Ella&#34; , &#34;age&#34;:36 }
{ &#34;id&#34;:2, &#34;name&#34;:&#34;Bob&#34;,&#34;age&#34;:29 }
{ &#34;id&#34;:3 , &#34;name&#34;:&#34;Jack&#34;,&#34;age&#34;:29 }
{ &#34;id&#34;:4 , &#34;name&#34;:&#34;Jim&#34;,&#34;age&#34;:28 }
{ &#34;id&#34;:4 , &#34;name&#34;:&#34;Jim&#34;,&#34;age&#34;:28 }
{ &#34;id&#34;:5 , &#34;name&#34;:&#34;Damon&#34; }
{ &#34;id&#34;:5 , &#34;name&#34;:&#34;Damon&#34; }
为employee.json创建DataFrame，并写出Python语句完成下列操作：
查询所有数据；查询所有数据，并去除重复的数据；查询所有数据，打印时去除id字段；筛选出age&gt;30的记录；将数据按age分组；将数据按name升序排列；取出前3行数据；查询所有记录的name列，并为其取别名为username；查询年龄age的平均值；查询年龄age的最小值。 实验步骤：
首先在目录/usr/local/spark/mycode/sparksql下，用命令“gedit employee.json”创建文件，并把数据复制进去。然后调用 Builder 对象的 getOrCreate() 方法，创建一个 SparkSession 实例，最后创建DataFrame。
&gt;&gt;&gt; from pyspark.sql import SparkSession
&gt;&gt;&gt; spark = SparkSession.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-12T17:10:10+08:00">
    <meta property="article:modified_time" content="2024-04-12T17:10:10+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Spark SQL编程初级实践</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h4 style="margin-left:0px;text-align:justify;">一、<strong>实验目的</strong></h4> 
<p style="margin-left:0;text-align:left;">（1）通过实验掌握Spark SQL的基本编程方法；</p> 
<p style="margin-left:0;text-align:left;">（2）熟悉RDD到DataFrame的转化方法；</p> 
<p style="margin-left:0;text-align:left;">（3）熟悉利用Spark SQL管理来自不同数据源的数据。</p> 
<h4 style="margin-left:0px;text-align:justify;"><strong>二、实验平台</strong></h4> 
<p style="margin-left:0;text-align:justify;">操作系统：Ubuntu20.04</p> 
<p style="margin-left:0;text-align:justify;">Spark版本：3.4.0</p> 
<p style="margin-left:0;text-align:justify;">Python版本：3.8.3</p> 
<p style="margin-left:0;text-align:justify;">数据库：MySQL</p> 
<h4 style="margin-left:0px;text-align:justify;"><strong>三、实验内容和</strong><strong>过程</strong></h4> 
<p>1．Spark SQL基本操作</p> 
<p style="margin-left:0;text-align:left;">将下列JSON格式数据复制到Ubuntu系统中，并保存命名为employee.json。</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:426.1pt;"> <p style="margin-left:0;text-align:left;">{ "id":1 , "name":" Ella" , "age":36 }</p> <p style="margin-left:0;text-align:left;">{ "id":2, "name":"Bob","age":29 }</p> <p style="margin-left:0;text-align:left;">{ "id":3 , "name":"Jack","age":29 }</p> <p style="margin-left:0;text-align:left;">{ "id":4 , "name":"Jim","age":28 }</p> <p style="margin-left:0;text-align:left;">{ "id":4 , "name":"Jim","age":28 }</p> <p style="margin-left:0;text-align:left;">{ "id":5 , "name":"Damon" }</p> <p style="margin-left:0;text-align:left;">{ "id":5 , "name":"Damon" }</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">为employee.json创建DataFrame，并写出Python语句完成下列操作：</p> 
<ol><li style="text-align:left;">查询所有数据；</li><li style="text-align:left;">查询所有数据，并去除重复的数据；</li><li style="text-align:left;">查询所有数据，打印时去除id字段；</li><li style="text-align:left;">筛选出age&gt;30的记录；</li><li style="text-align:left;">将数据按age分组；</li><li style="text-align:left;">将数据按name升序排列；</li><li style="text-align:left;">取出前3行数据；</li><li style="text-align:left;">查询所有记录的name列，并为其取别名为username；</li><li style="text-align:left;">查询年龄age的平均值；</li><li style="text-align:left;">查询年龄age的最小值。</li></ol> 
<p style="margin-left:0;text-align:left;">实验步骤：</p> 
<p style="margin-left:0;text-align:left;">首先在目录/usr/local/spark/mycode/sparksql下，用命令“gedit employee.json”创建文件，并把数据复制进去。然后调用 Builder 对象的 getOrCreate() 方法，创建一个 SparkSession 实例，最后创建DataFrame。</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">&gt;&gt;&gt; from pyspark.sql import SparkSession</p> <p style="margin-left:0;text-align:left;">&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()</p> <p style="margin-left:0;text-align:left;">&gt;&gt;&gt; df = spark.read.json("file:///usr/local/spark/mycode/sparksql/employee.json")</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">基于上面的数据，用Python语句完成下列操作：</p> 
<p style="margin-left:0;text-align:left;">（1）查询所有数据</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">&gt;&gt;&gt; df = spark.read.json("file:///usr/local/spark/mycode/sparksql/employee.json")</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">运行结果：</p> 
<p style="margin-left:0;text-align:center;"><img src="https://images2.imgbox.com/82/09/iiGFMona_o.png" alt="1724eaffe8104ebb8182fe5e2411a2b6.png"></p> 
<p style="margin-left:0;text-align:left;">（2）查询所有数据，并去除重复的数据</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">&gt;&gt;&gt; df.distinct().show()</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">运行结果：</p> 
<p style="margin-left:0;text-align:center;"><img src="https://images2.imgbox.com/23/06/ZrOxHvRg_o.png" alt="e0b9cb141fda4c2cafa8737a16ad3546.png"></p> 
<p style="margin-left:0;text-align:left;">（3）查询所有数据，打印时去除id字段</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">&gt;&gt;&gt; df.drop("id").show()</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">运行结果：</p> 
<p style="margin-left:0;text-align:center;"><img src="https://images2.imgbox.com/aa/d1/BHzDAQKM_o.png" alt="d7bc3c5c92da46afb9a1216c376be42d.png"></p> 
<p style="margin-left:0;text-align:left;">（4）筛选age&gt;30的记录</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">df.filter(df.age &gt; 30 ).show()</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">运行结果：</p> 
<p style="margin-left:0;text-align:center;"><img src="https://images2.imgbox.com/f7/c9/WkOeDfvk_o.png" alt="30421dda73ac4fa4a0cc01e778c17ad4.png"></p> 
<p style="margin-left:0;text-align:left;">（5）将数据按“name”分组</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">&gt;&gt;&gt; df.groupBy("name").count().show()</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">运行结果：</p> 
<p style="margin-left:0;text-align:center;"><img src="https://images2.imgbox.com/3c/a2/j4G4KLW9_o.png" alt="ea5a6883e38647a4a81577c0a78af3b0.png"></p> 
<p style="margin-left:0;text-align:left;">（6）将数据按name升序排列</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">&gt;&gt;&gt; df.sort(df.name.asc()).show()</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">运行结果：</p> 
<p style="margin-left:0;text-align:center;"><img src="https://images2.imgbox.com/2d/5e/DPZyP8y8_o.png" alt="f47107449abe4c0a9bfbe661397dadfe.png"></p> 
<p style="margin-left:0;text-align:left;">（7）取出前3行数据</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">&gt;&gt;&gt; df.take(3) 或python&gt; df.head(3)</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">运行结果：</p> 
<p style="margin-left:0;text-align:center;"><img src="https://images2.imgbox.com/83/c2/pjSQQjhn_o.png" alt="9fce5b8c6e304423bbb9c73084297be3.png"></p> 
<p style="margin-left:0;text-align:left;">（8）查询所有记录的name列，并为其取别名为username</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">&gt;&gt;&gt; df.select(df.name.alias("username")).show()</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">运行结果：</p> 
<p style="margin-left:0;text-align:center;"><img src="https://images2.imgbox.com/a9/26/Qc0QVsj6_o.png" alt="b2a58104f0164bc3ab2a5bc500cca7a8.png"></p> 
<p style="margin-left:0;text-align:left;">（9）查询年龄age的平均值</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">&gt;&gt;&gt; df.agg({"age": "mean"}).show()</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">运行结果：</p> 
<p style="margin-left:0;text-align:center;"><img src="https://images2.imgbox.com/ed/9e/tgmAqYft_o.png" alt="253d0ecaa05b43e586cf9e7f858cb50e.png"></p> 
<p style="margin-left:0;text-align:left;">（10）查询年龄age的最大值</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">&gt;&gt;&gt; df.agg({"age": "max"}).show()</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">运行结果：</p> 
<p style="margin-left:0;text-align:center;"><img src="https://images2.imgbox.com/5b/6a/u9yM5w2b_o.png" alt="807acef5d0a24c9e964f1f9a6528d06c.png"></p> 
<p style="margin-left:0;text-align:left;"> </p> 
<p><strong>2</strong><strong>．编程实现将</strong><strong>RDD</strong><strong>转换为</strong><strong>DataFrame</strong></p> 
<p style="margin-left:0;text-align:left;">源文件内容如下（包含id,name,age）：</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:426.1pt;"> <p style="margin-left:0;text-align:left;">1,Ella,36</p> <p style="margin-left:0;text-align:left;">2,Bob,29</p> <p style="margin-left:0;text-align:left;">3,Jack,29</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">       先将数据复制保存到Ubuntu系统中，命名为employee.txt，实现从RDD转换得到DataFrame，并按“id:1,name:Ella,age:36”的格式打印出DataFrame的所有数据。请写出程序代码。</p> 
<p style="margin-left:0;text-align:left;"> </p> 
<p style="margin-left:0;text-align:left;">实验过程：</p> 
<p style="margin-left:0;text-align:left;">        假设当前目录为/usr/local/spark/mycode/rddtodf，在当前目录下新建一个目录mkdir -p src/main/python，然后在目录/usr/local/spark/mycode/rddtodf/src/main/python下新建一个rddtodf.py，复制下面代码；（下列两种方式任选其一）</p> 
<p style="margin-left:0;text-align:left;">方法一：利用反射来推断包含特定类型对象的RDD的schema，适用对已知数据结构的RDD转换；</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">from pyspark.sql import SparkSession</p> <p style="margin-left:0;text-align:left;">from pyspark import SparkContext</p> <p style="margin-left:0;text-align:left;">from pyspark.sql.types import Row</p> <p style="margin-left:0;text-align:left;"> </p> <p style="margin-left:0;text-align:left;">if __name__ == "__main__":</p> <p style="margin-left:0;text-align:left;">    # 创建 SparkSession 实例</p> <p style="margin-left:0;text-align:left;">    spark = SparkSession.builder.appName("Simple App").getOrCreate()</p> <p style="margin-left:0;text-align:left;">    sc = spark.sparkContext  # 获取当前 SparkSession 的 SparkContext</p> <p style="margin-left:0;text-align:left;"> </p> <p style="margin-left:0;text-align:left;">    # 读取文件</p> <p style="margin-left:0;text-align:left;">    peopleRDD = sc.textFile("file:///usr/local/spark/mycode/sparksql/employee.txt")</p> <p style="margin-left:0;text-align:left;">    # 将每一行转换为 Row 对象</p> <p style="margin-left:0;text-align:left;">    rowRDD = peopleRDD.map(lambda line: line.split(",")).map(lambda attributes: Row(int(attributes[0]), attributes[1], int(attributes[2])))</p> <p style="margin-left:0;text-align:left;">    # 将 RDD 转换为 DataFrame</p> <p style="margin-left:0;text-align:left;">    df = rowRDD.toDF()</p> <p style="margin-left:0;text-align:left;">    # 创建临时视图</p> <p style="margin-left:0;text-align:left;">    df.createOrReplaceTempView("employee")</p> <p style="margin-left:0;text-align:left;">    # 执行 SQL 查询</p> <p style="margin-left:0;text-align:left;">    personsDF = spark.sql("select * from employee")</p> <p style="margin-left:0;text-align:left;">    # 打印结果</p> <p style="margin-left:0;text-align:left;">    personsDF.rdd.map(lambda t: "id:{}, Name:{}, age:{}".format(t[0], t[1], t[2])).foreach(print)</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">利用命令运行程序</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">python3 ./ rddtodf.py</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">运行结果：</p> 
<p style="margin-left:0;text-align:left;"><img src="https://images2.imgbox.com/57/2a/9MyZ0mAR_o.png" alt="79992574c0be4699ae9aa8a77e0f8ddc.png"></p> 
<p style="margin-left:0;text-align:left;">方法二：使用编程接口，构造一个schema并将其应用在已知的RDD上。</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">from pyspark.sql.types import Row</p> <p style="margin-left:0;text-align:left;">from pyspark.sql.types import StructType</p> <p style="margin-left:0;text-align:left;">from pyspark.sql.types import StructField</p> <p style="margin-left:0;text-align:left;">from pyspark.sql.types import StringType</p> <p style="margin-left:0;text-align:left;">from pyspark.conf import SparkConf</p> <p style="margin-left:0;text-align:left;">from pyspark.sql.session import SparkSession</p> <p style="margin-left:0;text-align:left;">from pyspark import SparkContext</p> <p style="margin-left:0;text-align:left;"> </p> <p style="margin-left:0;text-align:left;">if __name__ == "__main__":</p> <p style="margin-left:0;text-align:left;">    conf = SparkConf().setAppName("Simple App").setMaster("local")</p> <p style="margin-left:0;text-align:left;">    sc = SparkContext(conf=conf)</p> <p style="margin-left:0;text-align:left;">    spark = SparkSession.builder.config(conf=conf).getOrCreate()  # 创建 SparkSession 实例</p> <p style="margin-left:0;text-align:left;"> </p> <p style="margin-left:0;text-align:left;">    peopleRDD = sc.textFile("file:///usr/local/spark/mycode/sparksql/employee.txt")</p> <p style="margin-left:0;text-align:left;">    schemaString = "id name age"</p> <p style="margin-left:0;text-align:left;">    fields = [StructField(field, StringType(), True) for field in schemaString.split()]</p> <p style="margin-left:0;text-align:left;">    schema = StructType(fields)</p> <p style="margin-left:0;text-align:left;">    rowRDD = peopleRDD.map(lambda line: line.split(",")).map(lambda attributes: Row(id=attributes[0], name=attributes[1], age=int(attributes[2])))</p> <p style="margin-left:0;text-align:left;">    employeeDF = spark.createDataFrame(rowRDD, schema)</p> <p style="margin-left:0;text-align:left;">    employeeDF.createOrReplaceTempView("employee")</p> <p style="margin-left:0;text-align:left;">    results = spark.sql("SELECT * FROM employee")</p> <p style="margin-left:0;text-align:left;">    results.rdd.map(lambda t: "id:{}, Name:{}, age:{}".format(t.id, t.name, t.age)).foreach(print)</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">利用命令运行程序</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">python3 ./ rddtodf2.py</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">运行结果：</p> 
<p style="margin-left:0;text-align:left;"><img src="https://images2.imgbox.com/8b/30/iLkU9fva_o.png" alt="280484b403a24354aad4de475b2c36e6.png"></p> 
<p style="margin-left:0;text-align:left;"> </p> 
<p style="margin-left:0;text-align:left;"><strong>3. </strong><strong>编程实现利用</strong><strong>DataFrame</strong><strong>读写</strong><strong>MySQL</strong><strong>的数据</strong></p> 
<p style="margin-left:0;text-align:left;">（1）在MySQL数据库中新建数据库sparktest，再创建表employee，包含如表5-2所示的两行数据。</p> 
<p style="margin-left:0;text-align:center;"><strong>表</strong><strong>5-2 employee</strong><strong>表原有数据</strong></p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#bfbfbf;vertical-align:top;width:106.5pt;"> <p style="margin-left:0;text-align:left;">id</p> </td><td style="border-color:#bfbfbf;vertical-align:top;width:106.5pt;"> <p style="margin-left:0;text-align:left;">name</p> </td><td style="border-color:#bfbfbf;vertical-align:top;width:106.55pt;"> <p style="margin-left:0;text-align:left;">gender</p> </td><td style="border-color:#bfbfbf;vertical-align:top;width:106.55pt;"> <p style="margin-left:0;text-align:left;">Age</p> </td></tr><tr><td style="border-color:#bfbfbf;vertical-align:top;width:106.5pt;"> <p style="margin-left:0;text-align:left;">1</p> </td><td style="vertical-align:top;width:106.5pt;"> <p style="margin-left:0;text-align:left;">Alice</p> </td><td style="vertical-align:top;width:106.55pt;"> <p style="margin-left:0;text-align:left;">F</p> </td><td style="vertical-align:top;width:106.55pt;"> <p style="margin-left:0;text-align:left;">22</p> </td></tr><tr><td style="border-color:#bfbfbf;vertical-align:top;width:106.5pt;"> <p style="margin-left:0;text-align:left;">2</p> </td><td style="vertical-align:top;width:106.5pt;"> <p style="margin-left:0;text-align:left;">John</p> </td><td style="vertical-align:top;width:106.55pt;"> <p style="margin-left:0;text-align:left;">M</p> </td><td style="vertical-align:top;width:106.55pt;"> <p style="margin-left:0;text-align:left;">25</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;"> </p> 
<p style="margin-left:0;text-align:left;">（2）配置Spark通过JDBC连接数据库MySQL，编程实现利用DataFrame插入如表5-3所示的两行数据到MySQL中，最后打印出age的最大值和age的总和。</p> 
<p style="margin-left:0;text-align:center;"><strong>表</strong><strong>5-3 employee</strong><strong>表新增数据</strong></p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#bfbfbf;vertical-align:top;width:106.5pt;"> <p style="margin-left:0;text-align:left;">id</p> </td><td style="border-color:#bfbfbf;vertical-align:top;width:106.5pt;"> <p style="margin-left:0;text-align:left;">name</p> </td><td style="border-color:#bfbfbf;vertical-align:top;width:106.55pt;"> <p style="margin-left:0;text-align:left;">gender</p> </td><td style="border-color:#bfbfbf;vertical-align:top;width:106.55pt;"> <p style="margin-left:0;text-align:left;">age</p> </td></tr><tr><td style="border-color:#bfbfbf;vertical-align:top;width:106.5pt;"> <p style="margin-left:0;text-align:left;">3</p> </td><td style="vertical-align:top;width:106.5pt;"> <p style="margin-left:0;text-align:left;">Mary</p> </td><td style="vertical-align:top;width:106.55pt;"> <p style="margin-left:0;text-align:left;">F</p> </td><td style="vertical-align:top;width:106.55pt;"> <p style="margin-left:0;text-align:left;">26</p> </td></tr><tr><td style="border-color:#bfbfbf;vertical-align:top;width:106.5pt;"> <p style="margin-left:0;text-align:left;">4</p> </td><td style="vertical-align:top;width:106.5pt;"> <p style="margin-left:0;text-align:left;">Tom</p> </td><td style="vertical-align:top;width:106.55pt;"> <p style="margin-left:0;text-align:left;">M</p> </td><td style="vertical-align:top;width:106.55pt;"> <p style="margin-left:0;text-align:left;">23</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;"> </p> 
<p style="margin-left:0;text-align:left;">实验过程：</p> 
<p style="margin-left:0;text-align:left;">（1）首先启动MySQL数据库</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">sudo service mysql start    # 输入虚拟机密码</p> <p style="margin-left:0;text-align:left;">mysql -u root -p  #输入MySQL密码</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">在MySQL Shell环境中，新建数据库sparktest，再建表employee，并插入数据</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">create database sparktest;</p> <p style="margin-left:0;text-align:left;">use sparktest;</p> <p style="margin-left:0;text-align:left;">create table employee (id int(4), name char(20), gender char(4), age int(4));</p> <p style="margin-left:0;text-align:left;">insert into employee values(1,'Alice','F',22);</p> <p style="margin-left:0;text-align:left;">insert into employee values(2,'John','M',25);</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;"> </p> 
<p style="margin-left:0;text-align:left;">（2）在目录为/usr/local/spark/mycode/testmysql下，新建一个目录mkdir -p src/main/python，然后在目录/usr/local/spark/mycode/testmysql/src/main/python下新建一个testmysql.py，复制下面代码：</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">from pyspark.sql import SparkSession</p> <p style="margin-left:0;text-align:left;">from pyspark.sql.types import Row</p> <p style="margin-left:0;text-align:left;">from pyspark.sql.types import StructType</p> <p style="margin-left:0;text-align:left;">from pyspark.sql.types import StructField</p> <p style="margin-left:0;text-align:left;">from pyspark.sql.types import StringType</p> <p style="margin-left:0;text-align:left;">from pyspark.sql.types import IntegerType</p> <p style="margin-left:0;text-align:left;"> </p> <p style="margin-left:0;text-align:left;">if __name__ == "__main__":</p> <p style="margin-left:0;text-align:left;">    spark = SparkSession.builder \</p> <p style="margin-left:0;text-align:left;">        .appName("MySQL JDBC Example") \</p> <p style="margin-left:0;text-align:left;">        .config("spark.driver.extraClassPath", "/usr/local/spark/jars/mysql-connector-j-8.0.33.jar") \      #自己JDBC的路径，自行修改</p> <p style="margin-left:0;text-align:left;">        .getOrCreate()</p> <p style="margin-left:0;text-align:left;"> </p> <p style="margin-left:0;text-align:left;">    jdbcDF = spark.read.format("jdbc") \</p> <p style="margin-left:0;text-align:left;">        .option("url", "jdbc:mysql://localhost:3306/sparktest") \</p> <p style="margin-left:0;text-align:left;">        .option("driver", "com.mysql.cj.jdbc.Driver") \</p> <p style="margin-left:0;text-align:left;">        .option("dbtable", "employee") \</p> <p style="margin-left:0;text-align:left;">        .option("user", "root") \</p> <p style="margin-left:0;text-align:left;">        .option("password", "自己MySQL密码") \</p> <p style="margin-left:0;text-align:left;">        .load()</p> <p style="margin-left:0;text-align:left;"> </p> <p style="margin-left:0;text-align:left;">    jdbcDF.filter(jdbcDF.age &gt; 20).collect()  # 检测是否连接成功</p> <p style="margin-left:0;text-align:left;"> </p> <p style="margin-left:0;text-align:left;">    studentRDD = spark.sparkContext.parallelize(["3 Mary F 26", "4 Tom M 23"]).map(lambda line: line.split(" "))</p> <p style="margin-left:0;text-align:left;">    schema = StructType([</p> <p style="margin-left:0;text-align:left;">        StructField("id", IntegerType(), True),</p> <p style="margin-left:0;text-align:left;">        StructField("name", StringType(), True),</p> <p style="margin-left:0;text-align:left;">        StructField("gender", StringType(), True),</p> <p style="margin-left:0;text-align:left;">        StructField("age", IntegerType(), True)</p> <p style="margin-left:0;text-align:left;">    ])</p> <p style="margin-left:0;text-align:left;">    rowRDD = studentRDD.map(lambda p: Row(int(p[0]), p[1].strip(), p[2].strip(), int(p[3])))</p> <p style="margin-left:0;text-align:left;">    employeeDF = spark.createDataFrame(rowRDD, schema)</p> <p style="margin-left:0;text-align:left;"> </p> <p style="margin-left:0;text-align:left;">    prop = {<!-- --></p> <p style="margin-left:0;text-align:left;">        'user': 'root',</p> <p style="margin-left:0;text-align:left;">        'password': '自己MySQL密码',</p> <p style="margin-left:0;text-align:left;">        'driver': "com.mysql.cj.jdbc.Driver"</p> <p style="margin-left:0;text-align:left;">    }</p> <p style="margin-left:0;text-align:left;"> </p> <p style="margin-left:0;text-align:left;">    employeeDF.write.jdbc("jdbc:mysql://localhost:3306/sparktest", 'employee', 'append', prop)</p> <p style="margin-left:0;text-align:left;"> </p> <p style="margin-left:0;text-align:left;">    jdbcDF.collect()</p> <p style="margin-left:0;text-align:left;">    jdbcDF.agg({"age": "max"}).show()</p> <p style="margin-left:0;text-align:left;">    jdbcDF.agg({"age": "sum"}).show()</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">注意：这里要下载MySQL的JDBC数据库驱动程序，确保虚拟机中有这个，才能运行，需要将代码中的SQL用户配置改成自己的，JDBC也要改成自己的路径</p> 
<p style="margin-left:0;text-align:left;">执行命令：</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:left;">python3 testmysql.py</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:left;">运行结果：</p> 
<p style="margin-left:0;text-align:left;"><img src="https://images2.imgbox.com/e8/8f/Zye8rJ6E_o.png" alt="1cfef78e3ef545bc850df761d52bf78f.png"></p> 
<p style="margin-left:0;text-align:left;"> </p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a4942c39da28fe07c13b3025af534fae/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">记账本|基于SSM的家庭记账本小程序设计与实现(源码&#43;数据库&#43;文档)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0ef26b02558957784c8b36b50b52704b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Sql Server远程连接_远程ip 登录 sql server</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>