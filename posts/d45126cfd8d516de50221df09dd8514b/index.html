<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>[NLP] 使用Llama.cpp和LangChain在CPU上使用大模型-RAG - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/d45126cfd8d516de50221df09dd8514b/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="[NLP] 使用Llama.cpp和LangChain在CPU上使用大模型-RAG">
  <meta property="og:description" content="一 准备工作 下面是构建这个应用程序时将使用的软件工具:
1.Llama-cpp-python
下载llama-cpp, llama-cpp-python
[NLP] Llama2模型运行在Mac机器-CSDN博客
2、LangChain
LangChain是一个提供了一组广泛的集成和数据连接器，允许我们链接和编排不同的模块。可以常见聊天机器人、数据分析和文档问答等应用。
3、sentence-transformer
sentence-transformer提供了简单的方法来计算句子、文本和图像的嵌入。它能够计算100多种语言的嵌入。我们将在这个项目中使用开源的all-MiniLM-L6-v2模型。
4、FAISS
Facebook AI相似度搜索(FAISS)是一个为高效相似度搜索和密集向量聚类而设计的库。
给定一组嵌入，我们可以使用FAISS对它们进行索引，然后利用其强大的语义搜索算法在索引中搜索最相似的向量。
虽然它不是传统意义上的成熟的向量存储(如数据库管理系统)，但它以一种优化的方式处理向量的存储，以实现有效的最近邻搜索。
二 LLM应用架构：以文档Q&amp;A为例 假设我们想基于自己部署的Llama2模型，构建一个用于回答针对特定文档内容提问的聊天机器人。文档的内容可能属于特定领域或者特定组织内的文档，因此不属于任何Llama2进行预训练和微调的数据集。一个直觉的做法是in-context-learning：将文档作为Prompt提供给模型，从而模型能够根据所提供的Context进行回答。直接将文档作为Context可能遇到的问题是：
文档的长度超出了模型的Context长度限制，原版Llama2的Context长度为4096个Tokens。对于较长的Context，模型可能会Lost in the Middle，无法准确从Context中获取关键信息。 因此，我们希望在构建Prompt时，只输入与用户的问题最相关的文档内容。
以下是构建文档Q&amp;A应用的常用架构：
文档处理与存储：将原始文本进行分块 (Splitting)，并使用语言模型对每块文本进行embedding，得到文本的向量表示，最后将文本向量存储在支持相似性搜索的向量数据库中。用户询问和Prompt构建：根据用户输入的询问，使用相似性搜索在向量数据库中提取出与询问最相关的一些文档分块，并将用户询问&#43;文档一起构建Prompt，随后输入LLM并得到回答。 三 实际使用 LangChain
在上节中描述了以文档Q&amp;A为例的LLM应用Pipeline架构。LangChain是构建该类大模型应用的框架，其提供了模块化组件（例如上文图中的Document loader, Text splitter, Vector storage）的抽象和实现，并支持集成第三方的实现（例如可以使用不同第三方提供的Vector Storage服务)。通过LangChain可以将大模型与自定义的数据源结合起来构建Pipeline。
https://github.com/langchain-ai/langchain​github.com/langchain-ai/langchain
构建步骤 我们已经了解了各种组件，接下来让逐步介绍如何构建文档问答应用程序。
由于已经有许多教程了，所以我们不会深入到复杂和一般的文档问答组件的细节(例如，文本分块，矢量存储设置)。在本文中，我们将把重点放在开源LLM和CPU推理方面。
使用llama-cpp-python启动llama2 api服务 python3 -m llama_cpp.server --model TheBloke--Chinese-Alpaca-2-7B-GGUF/chinese-alpaca-2-7b.Q4_K_M.gguf --n_gpu_layers 1 INFO: Started server process [63148] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://localhost:8000 (Press CTRL&#43;C to quit) 使用LangChain调用本地部署的Llama2 下面的示例将使用LangChain的API调用本地部署的Llama2模型。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-12-21T15:24:37+08:00">
    <meta property="article:modified_time" content="2023-12-21T15:24:37+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">[NLP] 使用Llama.cpp和LangChain在CPU上使用大模型-RAG</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2><strong>一 准备工作</strong></h2> 
<p>下面是构建这个应用程序时将使用的软件工具:</p> 
<p><strong>1.Llama-cpp-python</strong></p> 
<p> 下载llama-cpp, llama-cpp-python</p> 
<p><a href="https://blog.csdn.net/zwqjoy/article/details/134233461?spm=1001.2014.3001.5501" title="[NLP] Llama2模型运行在Mac机器-CSDN博客">[NLP] Llama2模型运行在Mac机器-CSDN博客</a></p> 
<p><strong>2、LangChain</strong></p> 
<p>LangChain是一个提供了一组广泛的集成和数据连接器，允许我们链接和编排不同的模块。可以常见聊天机器人、数据分析和文档问答等应用。</p> 
<p><strong>3、sentence-transformer</strong></p> 
<p>sentence-transformer提供了简单的方法来计算句子、文本和图像的嵌入。它能够计算100多种语言的嵌入。我们将在这个项目中使用开源的all-MiniLM-L6-v2模型。</p> 
<p><strong>4、FAISS</strong></p> 
<p>Facebook AI相似度搜索(FAISS)是一个为高效相似度搜索和密集向量聚类而设计的库。</p> 
<p>给定一组嵌入，我们可以使用FAISS对它们进行索引，然后利用其强大的语义搜索算法在索引中搜索最相似的向量。</p> 
<p>虽然它不是传统意义上的成熟的向量存储(如数据库管理系统)，但它以一种优化的方式处理向量的存储，以实现有效的最近邻搜索。</p> 
<p></p> 
<h2>二 LLM应用架构：以文档Q&amp;A为例</h2> 
<p><img alt="" height="820" src="https://images2.imgbox.com/3d/f0/Ftm4eccp_o.png" width="1200"></p> 
<p>假设我们想基于自己部署的Llama2模型，构建一个用于回答针对特定文档内容提问的聊天机器人。文档的内容可能属于特定领域或者特定组织内的文档，因此不属于任何Llama2进行预训练和微调的数据集。一个直觉的做法是in-context-learning：将文档作为Prompt提供给模型，从而模型能够根据所提供的Context进行回答。直接将文档作为Context可能遇到的问题是：</p> 
<ul><li>文档的长度超出了模型的Context长度限制，原版Llama2的Context长度为4096个Tokens。</li><li>对于较长的Context，模型可能会<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2307.03172" rel="nofollow" title="Lost in the Middle">Lost in the Middle</a>，无法准确从Context中获取关键信息。</li></ul> 
<p>因此，我们希望在构建Prompt时，只输入与用户的问题最相关的文档内容。</p> 
<p>以下是构建文档Q&amp;A应用的常用架构：</p> 
<p><img alt="" height="944" src="https://images2.imgbox.com/40/4b/LhNj1HcG_o.png" width="1200"></p> 
<ul><li>文档处理与存储：将原始文本进行分块 (Splitting)，并使用语言模型对每块文本进行embedding，得到文本的向量表示，最后将文本向量存储在支持相似性搜索的向量数据库中。</li><li>用户询问和Prompt构建：根据用户输入的询问，使用相似性搜索在向量数据库中提取出与询问最相关的一些文档分块，并将用户询问+文档一起构建Prompt，随后输入LLM并得到回答。</li></ul> 
<h3></h3> 
<h2>三 实际使用</h2> 
<p><strong>LangChain</strong></p> 
<p>在上节中描述了以文档Q&amp;A为例的LLM应用Pipeline架构。<strong>LangChain</strong>是构建该类大模型应用的框架，其提供了模块化组件（例如上文图中的Document loader, Text splitter, Vector storage）的抽象和实现，并支持集成第三方的实现（例如可以使用不同第三方提供的Vector Storage服务)。通过LangChain可以将大模型与自定义的数据源结合起来构建Pipeline。</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/langchain-ai/langchain" title="https://github.com/langchain-ai/langchain​github.com/langchain-ai/langchain">https://github.com/langchain-ai/langchain​github.com/langchain-ai/langchain</a></p> 
<p><img alt="" height="918" src="https://images2.imgbox.com/60/22/0i2AD2F3_o.png" width="1200"></p> 
<h3>构建步骤</h3> 
<p>我们已经了解了各种组件，接下来让逐步介绍如何构建文档问答应用程序。</p> 
<p>由于已经有许多教程了，所以我们不会深入到复杂和一般的文档问答组件的细节(例如，文本分块，矢量存储设置)。在本文中，我们将把重点放在开源LLM和CPU推理方面。</p> 
<p></p> 
<h3><span style="color:#1a439c;"><strong>使用llama-cpp-python启动llama2 api服务</strong></span></h3> 
<pre><code>python3 -m llama_cpp.server --model TheBloke--Chinese-Alpaca-2-7B-GGUF/chinese-alpaca-2-7b.Q4_K_M.gguf   --n_gpu_layers 1

INFO:     Started server process [63148]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://localhost:8000 (Press CTRL+C to quit)</code></pre> 
<p></p> 
<h3><strong>使用LangChain调用本地部署的Llama2</strong></h3> 
<p>下面的示例将使用LangChain的API调用本地部署的Llama2模型。</p> 
<pre><code>from langchain.chat_models import ChatOpenAI

chat_model = ChatOpenAI(openai_api_key = "EMPTY", openai_api_base = "http://localhost:8000/v1", max_tokens=256)</code></pre> 
<ul><li>由于本地部署的llama-cpp-python提供了类OpenAI的API，因此可以直接使用<code>ChatOpenAI</code>接口，这将调用<code>/v1/chat/completions</code> API。</li><li>由于并没有使用真正的OpenAI API，因此<code>open_ai_key</code>可以任意提供。<code>openai_pi_base</code>为模型API的Base URL。<code>max_tokens</code>限制了模型回答的长度。</li></ul> 
<pre><code class="language-python">from langchain.chat_models import ChatOpenAI

chat_model = ChatOpenAI(openai_api_key = "EMPTY", openai_api_base = "http://localhost:8000/v1", max_tokens=256)

from langchain.schema import AIMessage, HumanMessage, SystemMessage

system_text = "You are a helpful assistant."
human_text1 = "What is the capital of France?"
assistant_text = "Paris."
human_text2 = "How about England?"

messages = [SystemMessage(content=system_text), 
            HumanMessage(content=human_text1), 
            AIMessage(content=assistant_text), 
            HumanMessage(content=human_text2)]

chat_model.predict_messages(messages)</code></pre> 
<p><img alt="" height="768" src="https://images2.imgbox.com/3c/10/5PyLbg1x_o.png" width="1200"></p> 
<p></p> 
<h2>四 文档Q&amp;A应用</h2> 
<p><span style="color:#fe2c24;"><strong>这里将演示如何使用LangChain构建一个简单的文档Q&amp;A应用Pipeline：</strong></span></p> 
<ul><li><strong>Document Loading +</strong> <strong>Text Splitting</strong></li><li><strong>Text Embeddings + Vector Storage</strong></li><li><strong>Text Retrieval</strong>  <strong>+</strong> <strong>Query LLM</strong></li></ul> 
<p></p> 
<h3><strong>1 数据加载与处理 Document Loading +</strong> <strong>Text Splitting</strong></h3> 
<p>本实验使用llama.cpp的<a href="https://link.zhihu.com/?target=https%3A//github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md" rel="nofollow" title="README.md">README.md</a>作为我们需要进行询问的文档。LangChain提供了一系列不同格式文档的Loader，包括用于加载Markdown文档的<code>UnstructuredMarkdownLoader</code>:</p> 
<p><code>UnstructuredMarkdownLoader</code>默认会将文档中的不同Elements（各级标题，正文等）组合起来，去掉了<code>#</code>等字符。</p> 
<p><code>RecursiveCharacterTextSplitter</code>是对常见文本进行Split的推荐选择：</p> 
<p><code>RecursiveCharacterTextSplitter</code>递归地在文本中寻找能够进行分割的字符（默认为<code>["\n\n", "\n", " ", ""]</code>)。这将尽可能地保留完整的段落，句子和单词。</p> 
<ul><li><code>chunk_size</code>: 文本进行Split后每个分块的最大长度，所有分块将在这个限制以内</li><li><code>chunk_overlap</code>: 前后分块overlap的长度，overlap是为了保持前后两个分块之间的语义连续性</li><li><code>length_function</code>: 度量文本长度的方法</li></ul> 
<pre><code class="language-python">from langchain.document_loaders import UnstructuredMarkdownLoader

loader = UnstructuredMarkdownLoader("./README.md")
text = loader.load()


from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 2000,
    chunk_overlap  = 400,
    length_function = len,
    is_separator_regex = False
)
all_splits = text_splitter.split_documents(text)
</code></pre> 
<p><img alt="" height="786" src="https://images2.imgbox.com/b2/b1/7uJe4qdP_o.png" width="1200"></p> 
<h3></h3> 
<h3><strong>2.矢量存储 Text Embeddings + Vector Storage</strong></h3> 
<p>这一步的任务是：将文本分割成块，加载嵌入模型，然后通过FAISS 进行向量的存储</p> 
<p><strong>Vector Storage</strong></p> 
<p>这里以<code>FAISS</code>向量数据库作为示例， <code>FAISS</code>基于Facebook AI Similarity Search(Faiss)库 。</p> 
<pre><code>pip install faiss-cpu</code></pre> 
<pre><code class="language-python">

from langchain.embeddings import HuggingFaceEmbeddings 
# Load embeddings model 
embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', 
                                   model_kwargs={'device': 'cpu'}) 

from langchain.vectorstores import FAISS
vectorstore = FAISS.from_documents(all_splits, embeddings) 
vectorstore.save_local('vectorstore/db_faiss')

#question = "How to run the program in interactive mode?"
#docs = vectorstore.similarity_search(question, k=1)
</code></pre> 
<p><img alt="" height="820" src="https://images2.imgbox.com/25/e1/C6oLfGvD_o.png" width="1200"></p> 
<p>运行上面的Python脚本后，向量存储将被生成并保存在名为'vectorstore/db_faiss'的本地目录中，并为语义搜索和检索做好准备。</p> 
<p></p> 
<h3><strong>3.文本检索与LLM查询 Text Retrieval</strong>  <strong>+</strong> <strong>Query LLM</strong></h3> 
<pre><code class="language-python">from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

chat_model = ChatOpenAI(openai_api_key = "EMPTY", openai_api_base = "http://localhost:8000/v1", max_tokens=256)

qa_chain = RetrievalQA.from_chain_type(chat_model, retriever=vectorstore.as_retriever(search_kwargs={"k": 1}))
qa_chain({"query": "How to run the program in interactive mode?"})</code></pre> 
<p><img alt="" height="500" src="https://images2.imgbox.com/74/e6/JatVViDD_o.png" width="1200"></p> 
<p>构造<code>RetrievalQA</code>需要提供一个LLM的实例，我们提供基于本地部署的Llama2构造的<code>ChatOpenAI</code>；还需要提供一个文本的Retriever，我们提供<code>FAISS</code>向量数据库作为一个Retriever，参数<code>search_kwargs={"k":1}</code>设置了Retriever提取的文档分块的数量，决定了最终Prompt包含的文档内容的数量，在这里我们设置为1。 向Chain中传入询问，即可得到LLM根据Retriever提取的文档做出的回答。</p> 
<p></p> 
<p></p> 
<p><strong>自定义<code>RetrievalQA</code>的Prompt:</strong></p> 
<pre><code class="language-python">from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

template = """Use the following pieces of context to answer the question at the end. 
If you don't know the answer, just say that you don't know, don't try to make up an answer. 
Use three sentences maximum and keep the answer as concise as possible. 
Always say "thanks for asking!" at the end of the answer. 
{context}
Question: {question}
Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

qa_chain = RetrievalQA.from_chain_type(
    chat_model,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 1}),
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
)
qa_chain({"query": "What is --interactive option used for?"})</code></pre> 
<p><img alt="" height="432" src="https://images2.imgbox.com/55/d4/Pp6TakQc_o.png" width="1200"></p> 
<p></p> 
<p></p> 
<pre><code class="language-python">from langchain.chains import LLMChain
from langchain_core.runnables import RunnablePassthrough
llm_chain = LLMChain(llm=chat, prompt=prompt)

query = "What is --interactive option used for?" 

retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

rag_chain = ( 
 {"context": retriever, "question": RunnablePassthrough()}
    | llm_chain
)

rag_chain.invoke(query)</code></pre> 
<p><img alt="" height="103" src="https://images2.imgbox.com/32/f4/NkJrobcV_o.png" width="1065"></p> 
<p></p> 
<p></p> 
<p></p> 
<h3>结论</h3> 
<p>本文介绍了如何在MacBook Pro本地环境使用llama.cpp部署ggml量化格式的Llama2语言模型，并演示如何使用LangChain简单构造了一个文档Q&amp;A应用。</p> 
<h3>References</h3> 
<p>[1] <a href="https://link.zhihu.com/?target=https%3A//github.com/ggerganov/llama.cpp" title="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a></p> 
<p>[2] <a href="https://link.zhihu.com/?target=https%3A//python.langchain.com/docs/use_cases/question_answering/" rel="nofollow" title="QA over Documents | ️ Langchain">QA over Documents | ️ Langchain</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/649366179" rel="nofollow" title="在MacBook Pro部署Llama2语言模型并基于LangChain构建LLM应用 - 知乎 (zhihu.com)">在MacBook Pro部署Llama2语言模型并基于LangChain构建LLM应用 - 知乎 (zhihu.com)</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/644701608" rel="nofollow" title="使用GGML和LangChain在CPU上运行量化的llama2 - 知乎 (zhihu.com)">使用GGML和LangChain在CPU上运行量化的llama2 - 知乎 (zhihu.com)</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/63da03b0ef03c58f7ca13360ce578417/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python中导入Excel数据：全面解析与实践</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/992c80271ac0369e44121a8089314417/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Qwen-14B Ai新手部署开源模型安装到本地</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>