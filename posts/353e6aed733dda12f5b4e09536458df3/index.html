<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>福彩双色球开奖数据爬虫工具-Python彩票数据爬虫实实战（附上所有代码） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/353e6aed733dda12f5b4e09536458df3/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="福彩双色球开奖数据爬虫工具-Python彩票数据爬虫实实战（附上所有代码）">
  <meta property="og:description" content="简介 这个Python脚本用于从中国福利彩票官方网站爬取双色球开奖结果，并将数据保存到CSV文件中。该脚本使用requests库发送HTTP请求，使用pandas库处理和保存数据，仅供参考学习使用。
爬虫，也称为网络爬虫或网页爬虫，是一种自动化的网络机器人，其主要原理是按照一定的规则自动浏览万维网并抓取信息的程序或脚本。以下是爬虫的基本工作原理：
初始URL：爬虫开始工作时，通常会有一组初始的URL列表，这些是爬虫开始抓取数据的起点。
请求网页：爬虫通过HTTP或HTTPS协议向特定的URL发送请求，就像普通用户使用浏览器访问网页一样。
解析内容：一旦网页响应，爬虫会下载网页内容，并从中提取有用的信息。这通常涉及到HTML和CSS的解析。
数据提取：爬虫会根据预设的规则解析HTML文档，提取出需要的数据，如文本、图片、链接等。
数据存储：提取出的数据会被存储起来，常见的存储方式包括数据库、文件系统或者内存中。
遵循链接：爬虫会分析当前页面的链接，并将这些链接添加到待爬取的队列中，以便接下来访问。
去重复：为了效率和避免重复工作，爬虫会检查新发现的URL是否已经被访问过，通常使用一个大型的集合数据结构来存储已访问的URL。
遵守规则：爬虫在抓取过程中会遵守robots.txt文件中的规则，这是网站管理员指定哪些部分可以被爬虫访问的标准。
错误处理：在爬取过程中，爬虫会遇到各种异常情况，如404错误、服务器错误等，良好的爬虫设计会包括错误处理机制。
用户代理：爬虫通常会设置一个用户代理（User-Agent），模仿浏览器的行为，有时候也会进行伪装，以避免被服务器识别为爬虫。
爬虫的设计和实现可以根据需求的不同而有很大的变化，但上述是大多数爬虫的基本工作流程。在设计爬虫时，应当考虑到效率、并发、网络延迟、数据处理和法律等多方面的因素。
环境要求 Python 3.xrequests库pandas库 安装依赖 在运行脚本之前，需要安装必要的Python库。可以使用以下命令安装：
pip install requests pandas 文件结构 catch(i): 一个函数，用于爬取指定页码的开奖数据。try...except: 主逻辑块，用于循环调用catch函数，并处理数据。 使用方法 确保所有依赖已安装。在命令行或IDE中运行脚本。脚本会自动爬取数据，并将反序后的数据保存到reversed_data.csv文件中。 参数说明 url: 目标网站的URL。params: 用于请求的参数字典。header: 自定义的HTTP头部信息。pageNo: 当前爬取的页码。pageSize: 每页显示的记录数。 注意事项 请合理设置爬取间隔，避免对服务器造成过大压力。确保遵守目标网站的爬虫政策和法律法规。脚本中的time.sleep(1)被注释掉了，建议根据实际情况取消注释以设置合理的请求间隔。 代码附上：
# coding=gbk import time import requests import pandas as pd url = &#34;https://www.cwl.gov.cn/cwl_admin/front/cwlkj/search/kjxx/findDrawNotice&#34; params = { &#39;name&#39;: &#39;ssq&#39;, &#39;issueCount&#39;: &#39;&#39;, &#39;issueStart&#39;: &#39;&#39;, &#39;issueEnd&#39;: &#39;&#39;, &#39;dayStart&#39;: &#39;&#39;, &#39;dayEnd&#39;: &#39;&#39;, &#39;pageNo&#39;: 1, &#39;pageSize&#39;: 30, &#39;week&#39;: &#39;&#39;, &#39;systemType&#39;: &#39;PC&#39;, } header = { &#39;Accept&#39;: &#39;application/json, text/javascript, */*; q=0.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-18T17:51:10+08:00">
    <meta property="article:modified_time" content="2024-04-18T17:51:10+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">福彩双色球开奖数据爬虫工具-Python彩票数据爬虫实实战（附上所有代码）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>简介</h3> 
<p>        这个Python脚本用于从中国福利彩票官方网站爬取双色球开奖结果，并将数据保存到CSV文件中。该脚本使用<code>requests</code>库发送HTTP请求，使用<code>pandas</code>库处理和保存数据，仅供参考学习使用。</p> 
<p>        爬虫，也称为网络爬虫或网页爬虫，是一种自动化的网络机器人，其主要原理是按照一定的规则自动浏览万维网并抓取信息的程序或脚本。以下是爬虫的基本工作原理：</p> 
<ol><li> <p><strong>初始URL</strong>：爬虫开始工作时，通常会有一组初始的URL列表，这些是爬虫开始抓取数据的起点。</p> </li><li> <p><strong>请求网页</strong>：爬虫通过HTTP或HTTPS协议向特定的URL发送请求，就像普通用户使用浏览器访问网页一样。</p> </li><li> <p><strong>解析内容</strong>：一旦网页响应，爬虫会下载网页内容，并从中提取有用的信息。这通常涉及到HTML和CSS的解析。</p> </li><li> <p><strong>数据提取</strong>：爬虫会根据预设的规则解析HTML文档，提取出需要的数据，如文本、图片、链接等。</p> </li><li> <p><strong>数据存储</strong>：提取出的数据会被存储起来，常见的存储方式包括数据库、文件系统或者内存中。</p> </li><li> <p><strong>遵循链接</strong>：爬虫会分析当前页面的链接，并将这些链接添加到待爬取的队列中，以便接下来访问。</p> </li><li> <p><strong>去重复</strong>：为了效率和避免重复工作，爬虫会检查新发现的URL是否已经被访问过，通常使用一个大型的集合数据结构来存储已访问的URL。</p> </li><li> <p><strong>遵守规则</strong>：爬虫在抓取过程中会遵守<code>robots.txt</code>文件中的规则，这是网站管理员指定哪些部分可以被爬虫访问的标准。</p> </li><li> <p><strong>错误处理</strong>：在爬取过程中，爬虫会遇到各种异常情况，如404错误、服务器错误等，良好的爬虫设计会包括错误处理机制。</p> </li><li> <p><strong>用户代理</strong>：爬虫通常会设置一个用户代理（User-Agent），模仿浏览器的行为，有时候也会进行伪装，以避免被服务器识别为爬虫。</p> </li></ol> 
<p>爬虫的设计和实现可以根据需求的不同而有很大的变化，但上述是大多数爬虫的基本工作流程。在设计爬虫时，应当考虑到效率、并发、网络延迟、数据处理和法律等多方面的因素。</p> 
<h3>环境要求</h3> 
<ul><li>Python 3.x</li><li>requests库</li><li>pandas库</li></ul> 
<h3>安装依赖</h3> 
<p>在运行脚本之前，需要安装必要的Python库。可以使用以下命令安装：</p> 
<pre><code class="hljs">pip install requests pandas
</code></pre> 
<h3>文件结构</h3> 
<ul><li><code>catch(i)</code>: 一个函数，用于爬取指定页码的开奖数据。</li><li><code>try...except</code>: 主逻辑块，用于循环调用<code>catch</code>函数，并处理数据。</li></ul> 
<h3>使用方法</h3> 
<ol><li>确保所有依赖已安装。</li><li>在命令行或IDE中运行脚本。</li><li>脚本会自动爬取数据，并将反序后的数据保存到<code>reversed_data.csv</code>文件中。</li></ol> 
<h3>参数说明</h3> 
<ul><li><code>url</code>: 目标网站的URL。</li><li><code>params</code>: 用于请求的参数字典。</li><li><code>header</code>: 自定义的HTTP头部信息。</li><li><code>pageNo</code>: 当前爬取的页码。</li><li><code>pageSize</code>: 每页显示的记录数。</li></ul> 
<h3>注意事项</h3> 
<ul><li>请合理设置爬取间隔，避免对服务器造成过大压力。</li><li>确保遵守目标网站的爬虫政策和法律法规。</li><li>脚本中的<code>time.sleep(1)</code>被注释掉了，建议根据实际情况取消注释以设置合理的请求间隔。</li></ul> 
<p>代码附上：</p> 
<pre><code class="language-python"># coding=gbk
import time
import requests
import pandas as pd

url = "https://www.cwl.gov.cn/cwl_admin/front/cwlkj/search/kjxx/findDrawNotice"
params = {
    'name': 'ssq',
    'issueCount': '',
    'issueStart': '',
    'issueEnd': '',
    'dayStart': '',
    'dayEnd': '',
    'pageNo': 1,
    'pageSize': 30,
    'week': '',
    'systemType': 'PC',
}

header = {
    'Accept': 'application/json, text/javascript, */*; q=0.01',
    'Accept-Encoding': 'gzip, deflate, br',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
    'Connection': 'keep-alive',
    'Cookie': 'HMF_CI=c5e50c6669ec97153ed140ea30f3eb070d8faea96fbcdf670ed7e2d8931d52333b32cced986c96d2e15287f95c0b17c77e9928eadb1275b99291225d5bbd4ca230; 21_vq=5',
    'Host': 'www.cwl.gov.cn',
    'Referer': 'https://www.cwl.gov.cn/ygkj/wqkjgg/ssq/',
    'Sec-Ch-Ua': '"Chromium";v="122", "Not(A:Brand";v="24", "Microsoft Edge";v="122"',
    'Sec-Ch-Ua-Mobile': '?0',
    'Sec-Ch-Ua-Platform': '"Windows"',
    'Sec-Fetch-Dest': 'empty',
    'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'same-origin',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0',
    'X-Requested-With': 'XMLHttpRequest',
}


def catch(i):
    params['pageNo'] = i
    try:
        response = requests.get(url=url, params=params, headers=header)
        if response.status_code == 200:
            result = response.json()['result']
            if not result:  # 如果结果为空，说明没有更多数据
                return None
            data = [[int(a['code'])] + [int(k) for k in a['red'].split(',')] + [int(a['blue'])] for a in result]
            return pd.DataFrame(data, columns=['code', 'r1', 'r2', 'r3', 'r4', 'r5', 'r6', 'b1'])
        else:
            print('请求错误：', response.status_code)
    except Exception as e:
        print('发生异常：', e)

try:
    data_frames = []  # 创建一个空列表来存储数据帧
    page = 1
    while True:
        print("正在爬取第{}页数据....".format(page))
        new_data = catch(page)  # 假设 catch() 函数返回一个数据帧
        if new_data is not None:
            print("爬取成功，正在整合数据...")
            data_frames.append(new_data)  # 将新数据帧添加到列表中
        else:
            print("已经没有更多数据，爬取结束。")
            break  # 如果没有新数据，则退出循环
        page += 1
        # time.sleep(1)
    df = pd.concat(data_frames, ignore_index=True)  # 循环结束后一次性合并所有数据帧
    # 反序DataFrame
    df = df.iloc[::-1]
    # 保存反序后的DataFrame到新的CSV文件
    df.to_csv('reversed_data.csv', index=False, encoding='gbk')
except Exception as e:
    print('出错或到达页数最底层：', e)
</code></pre> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/fc039fa479b5a16ef09fb463c2316f82/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Stable Diffusion XL优化终极指南</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/946059ceab4a71766b565873aee4b74b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">使用LangChain和Llama-Index实现多重检索RAG</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>