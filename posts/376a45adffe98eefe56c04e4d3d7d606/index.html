<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>在autodl平台使用llama-factory微调Qwen1.5-7B - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/376a45adffe98eefe56c04e4d3d7d606/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="在autodl平台使用llama-factory微调Qwen1.5-7B">
  <meta property="og:description" content="1 部署环境 step 1. 使用24GB显存以上的显卡创建环境
step 2. 创建好环境之后，关闭环境，使用无卡模式开机（有钱可忽略）
step 3. 安装LLaMA-Factory
git clone https://github.com/hiyouga/LLaMA-Factory.git # conda create -n llama_factory python=3.10 # conda activate llama_factory cd LLaMA-Factory pip install -e .[metrics] step 4. 配置ModelScope下载模型环境
export USE_MODELSCOPE_HUB=1 # 更改模型缓存地址，否则默认会缓存到/root/.cache，导致系统盘爆满 export MODELSCOPE_CACHE=/root/autodl-tmp/models/modelscope # 学术资源加速 source /etc/network_turbo pip install modelscope vllm # 安装vllm时可能导致进程killed，需要降低内存安装 # pip install modelscope vllm --no-cache-dir step 5. 使用ModelScope下载模型
#模型下载 from modelscope import snapshot_download model_dir = snapshot_download(&#39;qwen/Qwen1.5-7B&#39;) step 6. 切换到 llama-factory 工作目录">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-02T16:57:38+08:00">
    <meta property="article:modified_time" content="2024-05-02T16:57:38+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">在autodl平台使用llama-factory微调Qwen1.5-7B</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="1__0"></a>1 部署环境</h3> 
<p>step 1. 使用24GB显存以上的显卡创建环境</p> 
<p>step 2. 创建好环境之后，关闭环境，使用无卡模式开机（有钱可忽略）</p> 
<p>step 3. 安装<a href="https://github.com/hiyouga/LLaMA-Factory/tree/main">LLaMA-Factory</a></p> 
<pre><code class="prism language-bash"><span class="token function">git</span> clone https://github.com/hiyouga/LLaMA-Factory.git
<span class="token comment"># conda create -n llama_factory python=3.10</span>
<span class="token comment"># conda activate llama_factory</span>
<span class="token builtin class-name">cd</span> LLaMA-Factory
pip <span class="token function">install</span> <span class="token parameter variable">-e</span> .<span class="token punctuation">[</span>metrics<span class="token punctuation">]</span>
</code></pre> 
<p>step 4. 配置ModelScope下载模型环境</p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">export</span> <span class="token assign-left variable">USE_MODELSCOPE_HUB</span><span class="token operator">=</span><span class="token number">1</span>
<span class="token comment"># 更改模型缓存地址，否则默认会缓存到/root/.cache，导致系统盘爆满</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">MODELSCOPE_CACHE</span><span class="token operator">=</span>/root/autodl-tmp/models/modelscope
<span class="token comment"># 学术资源加速</span>
<span class="token builtin class-name">source</span> /etc/network_turbo
pip <span class="token function">install</span> modelscope vllm
<span class="token comment"># 安装vllm时可能导致进程killed，需要降低内存安装</span>
<span class="token comment"># pip install modelscope vllm --no-cache-dir</span>
</code></pre> 
<p>step 5. 使用ModelScope下载模型</p> 
<pre><code class="prism language-python"><span class="token comment">#模型下载</span>
<span class="token keyword">from</span> modelscope <span class="token keyword">import</span> snapshot_download
model_dir <span class="token operator">=</span> snapshot_download<span class="token punctuation">(</span><span class="token string">'qwen/Qwen1.5-7B'</span><span class="token punctuation">)</span>
</code></pre> 
<p>step 6. 切换到 llama-factory 工作目录</p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">cd</span> <span class="token punctuation">\</span>root<span class="token punctuation">\</span>LLaMA-Factory
</code></pre> 
<p>step 7. 使用web部署</p> 
<pre><code class="prism language-bash"><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> python src/web_demo.py <span class="token punctuation">\</span>
    <span class="token parameter variable">--model_name_or_path</span> /root/autodl-tmp/models/modelscope/qwen/Qwen1___5-7B <span class="token punctuation">\</span>
    <span class="token parameter variable">--template</span> qwen <span class="token punctuation">\</span>
    <span class="token parameter variable">--infer_backend</span> vllm <span class="token punctuation">\</span>
	<span class="token parameter variable">--vllm_enforce_eager</span>
<span class="token comment"># 默认端口为7860</span>
</code></pre> 
<p>step 8.本地访问</p> 
<p>点击自定义服务，下载<code>AutoDL SSH隧道工具</code></p> 
<p><img src="https://images2.imgbox.com/43/29/mb7MSKwa_o.png" alt="在这里插入图片描述"></p> 
<p>解压后打开工具，并输入登录指令和密码</p> 
<p><img src="https://images2.imgbox.com/40/61/iTu2PsgB_o.png" alt="在这里插入图片描述"></p> 
<p>点击图中连接就可以访问了</p> 
<p><img src="https://images2.imgbox.com/f1/3f/lo6qPbrT_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="2__70"></a>2 添加数据集</h3> 
<p>step 1.如果没有自定义数据集可以在开源网站中获取（<a href="https://openxlab.org.cn/" rel="nofollow">OpenXLab</a>）</p> 
<pre><code class="prism language-bash">pip <span class="token function">install</span> openxlab <span class="token comment">#安装</span>

pip <span class="token function">install</span> <span class="token parameter variable">-U</span> openxlab <span class="token comment">#版本升级</span>

openxlab login <span class="token comment">#进行登录，输入对应的AK/SK</span>

<span class="token comment"># Access Key: xxx</span>

<span class="token comment"># Secret Key: xxx</span>
openxlab dataset get --dataset-repo OpenDataLab/COIG-CQIA --target-path /root/autodl-tmp/Data/openxlab <span class="token comment">#数据集下载</span>
</code></pre> 
<p>step 2. 将数据格式转化为llama-factory支持的格式</p> 
<p>转换脚本可以利用GPT进行编写。</p> 
<blockquote> 
 <p>请编写python脚本完成jsonl数据到jsonl数据的格式转换，要求如下：<br> 1）原始jsonl每行格式为：<br> {<!-- --><br> “instruction”: “天下没有不散的筵席，那么我们相聚的意义又是什么”,<br> “input”: “”,<br> “output”: “尽管我们相聚的时光有限，但是相聚的意义在于创造美好的回忆和珍贵的关系。相聚让我们感受到彼此的关怀、支持和友情。我们可以一起分享喜悦、快乐和困难，互相支持和激励。相聚也可以是一个机会，让我们相互了解、学习和成长。最重要的是，相聚能够带给我们真实的人际交往和情感交流，让我们感受到生活的丰富和美好。所以，即使相聚的时间有限，我们仍然要珍惜这份意义和与彼此相聚的时光。”,<br> “task_type”: {<!-- --><br> “major”: [<br> “问答”<br> ],<br> “minor”: [<br> “逻辑问答”,<br> “隐喻理解”<br> ]<br> },<br> “domain”: [<br> “通用”<br> ],<br> “metadata”: “暂无元数据信息”,<br> “answer_from”: “llm”,<br> “human_verified”: true,<br> “copyright”: “暂无版权及作者信息”<br> }</p> 
 <p>2）目标json每行格式为：</p> 
 <p>{<!-- --></p> 
 <p>​ “instruction”: “”,<br> ​ “input”: “天下没有不散的筵席，那么我们相聚的意义又是什么”,<br> ​ “output”: “尽管我们相聚的时光有限，但是相聚的意义在于创造美好的回忆和珍贵的关系。相聚让我们感受到彼此的关怀、支持和友情。我们可以一起分享喜悦、快乐和困难，互相支持和激励。相聚也可以是一个机会，让我们相互了解、学习和成长。最重要的是，相聚能够带给我们真实的人际交往和情感交流，让我们感受到生活的丰富和美好。所以，即使相聚的时间有限，我们仍然要珍惜这份意义和与彼此相聚的时光。”<br> }</p> 
 <p>3）按照从1）到2）的格式转换示例编写转换脚本<br> 4）将转换后的数据保存为新的json，中文不要转义</p> 
</blockquote> 
<p>GPT给出的代码如下，需要修改一下数据路径：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> json

<span class="token keyword">def</span> <span class="token function">convert_jsonl_to_target_format</span><span class="token punctuation">(</span>input_file<span class="token punctuation">,</span> output_file<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>input_file<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        jsonl_data <span class="token operator">=</span> f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span>

    converted_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token keyword">for</span> line <span class="token keyword">in</span> jsonl_data<span class="token punctuation">:</span>
        json_data <span class="token operator">=</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        converted_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>
            <span class="token string">"instruction"</span><span class="token punctuation">:</span> <span class="token string">""</span><span class="token punctuation">,</span>
            <span class="token string">"input"</span><span class="token punctuation">:</span> json_data<span class="token punctuation">[</span><span class="token string">"instruction"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token string">"output"</span><span class="token punctuation">:</span> json_data<span class="token punctuation">[</span><span class="token string">"output"</span><span class="token punctuation">]</span>
        <span class="token punctuation">}</span><span class="token punctuation">)</span>

    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>output_file<span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        <span class="token keyword">for</span> item <span class="token keyword">in</span> converted_data<span class="token punctuation">:</span>
            json<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>item<span class="token punctuation">,</span> f<span class="token punctuation">,</span> ensure_ascii<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
            f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span>

<span class="token comment"># 调用转换函数</span>

convert_jsonl_to_target_format<span class="token punctuation">(</span><span class="token string">"/root/autodl-tmp/Data/openxlab/OpenDataLab___COIG-CQIA/ruozhiba/ruozhiba_ruozhiba.jsonl"</span><span class="token punctuation">,</span> <span class="token string">"/root/autodl-tmp/Data/LF/ruozhiba.json"</span><span class="token punctuation">)</span>
</code></pre> 
<p>step 3. 编写 <code>dataset_info.json</code> 文件</p> 
<p>首先计算 <code>ruozhiba.json</code> 文件的sha1sum, <code>sha1sum /root/autodl-tmp/Data/LF/ruozhiba.json</code></p> 
<p>添加自定义数据集的配置信息, 把 ruozhiba.json 文件的sha1 值添加到文件中，<code>"ruozhiba"</code> 为该数据集名；</p> 
<pre><code class="prism language-json"><span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"ruozhiba"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"file_name"</span><span class="token operator">:</span> <span class="token string">"ruozhiba.json"</span><span class="token punctuation">,</span>
    <span class="token string-property property">"file_sha1"</span><span class="token operator">:</span> xxx
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre> 
<h4><a id="LLaMAFactory_171"></a>使用LLaMA-Factory微调</h4> 
<p>step 1. 重新打开有卡的服务器，并在终端进入 <code>LLaMA-Factory</code> 文件夹中</p> 
<p>step 2. 使用 LoRA 微调</p> 
<pre><code class="prism language-bash"><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> python src/train_bash.py <span class="token punctuation">\</span>
<span class="token parameter variable">--stage</span> sft <span class="token punctuation">\</span>
<span class="token parameter variable">--do_train</span> <span class="token punctuation">\</span>
<span class="token parameter variable">--model_name_or_path</span> /root/autodl-tmp/models/modelscope/qwen/Qwen1___5-7B <span class="token punctuation">\</span>
<span class="token parameter variable">--dataset</span> ruozhiba <span class="token punctuation">\</span>
<span class="token parameter variable">--dataset_dir</span> /root/autodl-tmp/Data/LF <span class="token punctuation">\</span>
<span class="token parameter variable">--template</span> qwen <span class="token punctuation">\</span>
<span class="token parameter variable">--finetuning_type</span> lora <span class="token punctuation">\</span>
<span class="token parameter variable">--lora_target</span> q_proj,v_proj <span class="token punctuation">\</span>
<span class="token parameter variable">--output_dir</span> /root/autodl-tmp/checkpoints/llama_factory_demo/qwen/lora/sft <span class="token punctuation">\</span>
<span class="token parameter variable">--overwrite_cache</span> <span class="token punctuation">\</span>
<span class="token parameter variable">--per_device_train_batch_size</span> <span class="token number">4</span> <span class="token punctuation">\</span>
<span class="token parameter variable">--gradient_accumulation_steps</span> <span class="token number">4</span> <span class="token punctuation">\</span>
<span class="token parameter variable">--lr_scheduler_type</span> cosine <span class="token punctuation">\</span>
<span class="token parameter variable">--logging_steps</span> <span class="token number">10</span> <span class="token punctuation">\</span>
<span class="token parameter variable">--save_strategy</span> epoch <span class="token punctuation">\</span>
<span class="token parameter variable">--learning_rate</span> 5e-5 <span class="token punctuation">\</span>
<span class="token parameter variable">--num_train_epochs</span> <span class="token number">50.0</span> <span class="token punctuation">\</span>
<span class="token parameter variable">--plot_loss</span> <span class="token punctuation">\</span>
<span class="token parameter variable">--fp16</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/25/95/Ab95LnEK_o.png" alt="在这里插入图片描述"></p> 
<p>step 2. 微调后将adapter和原来模型合并</p> 
<pre><code class="prism language-bash"><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> python src/export_model.py <span class="token punctuation">\</span>
    <span class="token parameter variable">--model_name_or_path</span> qwen/Qwen-7B <span class="token punctuation">\</span>
    <span class="token parameter variable">--adapter_name_or_path</span> /root/autodl-tmp/checkpoints/llama_factory_demo/qwen/lora/sft/checkpoint-750 <span class="token punctuation">\</span>
    <span class="token parameter variable">--model_name_or_path</span> /root/autodl-tmp/models/modelscope/qwen/Qwen1___5-7B <span class="token punctuation">\</span>
    <span class="token parameter variable">--template</span> qwen <span class="token punctuation">\</span>
    <span class="token parameter variable">--finetuning_type</span> lora <span class="token punctuation">\</span>
    <span class="token parameter variable">--export_dir</span> /root/autodl-tmp/merged_model/qwen <span class="token punctuation">\</span>
    <span class="token parameter variable">--export_size</span> <span class="token number">2</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--export_legacy_format</span> False
</code></pre> 
<p>step 3. 推理合并后的模型</p> 
<pre><code class="prism language-bash"><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> python src/web_demo.py <span class="token punctuation">\</span>
    <span class="token parameter variable">--model_name_or_path</span> /root/autodl-tmp/merged_model/qwen <span class="token punctuation">\</span>
    <span class="token parameter variable">--template</span> qwen <span class="token punctuation">\</span>
    <span class="token parameter variable">--infer_backend</span> vllm <span class="token punctuation">\</span>
	<span class="token parameter variable">--vllm_enforce_eager</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f5986557f87d669f6ac46a594c003d07/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">161、Rust与Python互操作：性能与简易性的完美结合</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/26004bebb9b654dd6721ff738b411bcf/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">RabbitMQ 中的 VirtualHost 该如何理解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>