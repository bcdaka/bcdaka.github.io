<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LLaMA-Factory添加adalora - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/ad09f834aa695dd2b99f080b71679135/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="LLaMA-Factory添加adalora">
  <meta property="og:description" content="感谢https://github.com/tsingcoo/LLaMA-Efficient-Tuning/commit/f3a532f56b4aa7d4200f24d93fade4b2c9042736和https://github.com/huggingface/peft/issues/432的帮助。
在LLaMA-Factory中添加adalora 1. 修改src/llmtuner/hparams/finetuning_args.py代码
在FinetuningArguments中修改finetuning_type，添加target_r和init_r
修改__post_init__函数
2. 修改src/llmtuner/tuner/core/adapter.py代码
添加AdaLoraConfig
在init_adapter函数中添加一个if判断，添加位置在如红框所示：
if finetuning_args.finetuning_type == &#34;adalora&#34;: logger.info(&#34;Fine-tuning method: AdaLoRA&#34;) latest_checkpoint = None if model_args.checkpoint_dir is not None: if (is_trainable and finetuning_args.resume_lora_training) or (not is_mergeable): # continually fine-tuning checkpoints_to_merge, latest_checkpoint = model_args.checkpoint_dir[:-1], model_args.checkpoint_dir[-1] else: checkpoints_to_merge = model_args.checkpoint_dir for checkpoint in checkpoints_to_merge: model = PeftModel.from_pretrained(model, checkpoint) model = model.merge_and_unload() if len(checkpoints_to_merge) &gt; 0: logger.info(&#34;Merged {} model checkpoint(s).&#34;.format(len(checkpoints_to_merge))) if latest_checkpoint is not None: # resume lora training or quantized inference model = PeftModel.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-01-12T18:32:42+08:00">
    <meta property="article:modified_time" content="2024-01-12T18:32:42+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">LLaMA-Factory添加adalora</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>感谢<a href="https://img-home.csdnimg.cn/images/20230724024159.png?origin_url=https%3A%2F%2Fgithub.com%2Ftsingcoo%2FLLaMA-Efficient-Tuning%2Fcommit%2Ff3a532f56b4aa7d4200f24d93fade4b2c9042736&amp;pos_id=img-s9vyCxVD-1705053400275" rel="nofollow">https://github.com/tsingcoo/LLaMA-Efficient-Tuning/commit/f3a532f56b4aa7d4200f24d93fade4b2c9042736</a>和<a href="https://github.com/huggingface/peft/issues/432">https://github.com/huggingface/peft/issues/432</a>的帮助。</p> 
<h3><a id="LLaMAFactoryadalora_2"></a>在LLaMA-Factory中添加adalora</h3> 
<p><strong>1. 修改src/llmtuner/hparams/finetuning_args.py代码</strong><br> 在FinetuningArguments中修改finetuning_type，添加target_r和init_r<br> <img src="https://images2.imgbox.com/05/c0/nzHvJWu7_o.png" alt="在这里插入图片描述"><br> 修改__post_init__函数<br> <img src="https://images2.imgbox.com/f2/6f/EcW3QLUS_o.png" alt="在这里插入图片描述"></p> 
<p><strong>2. 修改src/llmtuner/tuner/core/adapter.py代码</strong><br> 添加AdaLoraConfig<br> <img src="https://images2.imgbox.com/7c/a3/s7EyTyoQ_o.png" alt="在这里插入图片描述"><br> 在init_adapter函数中添加一个if判断，添加位置在如红框所示：<br> <img src="https://images2.imgbox.com/91/80/2gDxMcYh_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python">    <span class="token keyword">if</span> finetuning_args<span class="token punctuation">.</span>finetuning_type <span class="token operator">==</span> <span class="token string">"adalora"</span><span class="token punctuation">:</span>
        logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"Fine-tuning method: AdaLoRA"</span><span class="token punctuation">)</span>
        latest_checkpoint <span class="token operator">=</span> <span class="token boolean">None</span>

        <span class="token keyword">if</span> model_args<span class="token punctuation">.</span>checkpoint_dir <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token punctuation">(</span>is_trainable <span class="token keyword">and</span> finetuning_args<span class="token punctuation">.</span>resume_lora_training<span class="token punctuation">)</span> <span class="token keyword">or</span> <span class="token punctuation">(</span><span class="token keyword">not</span> is_mergeable<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># continually fine-tuning</span>
                checkpoints_to_merge<span class="token punctuation">,</span> latest_checkpoint <span class="token operator">=</span> model_args<span class="token punctuation">.</span>checkpoint_dir<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> model_args<span class="token punctuation">.</span>checkpoint_dir<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                checkpoints_to_merge <span class="token operator">=</span> model_args<span class="token punctuation">.</span>checkpoint_dir

            <span class="token keyword">for</span> checkpoint <span class="token keyword">in</span> checkpoints_to_merge<span class="token punctuation">:</span>
                model <span class="token operator">=</span> PeftModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model<span class="token punctuation">,</span> checkpoint<span class="token punctuation">)</span>
                model <span class="token operator">=</span> model<span class="token punctuation">.</span>merge_and_unload<span class="token punctuation">(</span><span class="token punctuation">)</span>

            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>checkpoints_to_merge<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
                logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"Merged {} model checkpoint(s)."</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>checkpoints_to_merge<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

            <span class="token keyword">if</span> latest_checkpoint <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span> <span class="token comment"># resume lora training or quantized inference</span>
                model <span class="token operator">=</span> PeftModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model<span class="token punctuation">,</span> latest_checkpoint<span class="token punctuation">,</span> is_trainable<span class="token operator">=</span>is_trainable<span class="token punctuation">)</span>


        <span class="token keyword">if</span> is_trainable <span class="token keyword">and</span> latest_checkpoint <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span> <span class="token comment"># create new lora weights while training</span>
            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>finetuning_args<span class="token punctuation">.</span>lora_target<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span> <span class="token keyword">and</span> finetuning_args<span class="token punctuation">.</span>lora_target<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">"all"</span><span class="token punctuation">:</span>
                target_modules <span class="token operator">=</span> find_all_linear_modules<span class="token punctuation">(</span>model<span class="token punctuation">,</span> model_args<span class="token punctuation">.</span>quantization_bit<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                target_modules <span class="token operator">=</span> finetuning_args<span class="token punctuation">.</span>lora_target
                
            lora_config <span class="token operator">=</span> AdaLoraConfig<span class="token punctuation">(</span>
                task_type<span class="token operator">=</span>TaskType<span class="token punctuation">.</span>CAUSAL_LM<span class="token punctuation">,</span>
                inference_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                target_r<span class="token operator">=</span>finetuning_args<span class="token punctuation">.</span>target_r<span class="token punctuation">,</span>
                init_r<span class="token operator">=</span>finetuning_args<span class="token punctuation">.</span>init_r<span class="token punctuation">,</span>
                r<span class="token operator">=</span>finetuning_args<span class="token punctuation">.</span>lora_rank<span class="token punctuation">,</span>
                target_modules<span class="token operator">=</span>target_modules<span class="token punctuation">,</span>
                lora_alpha<span class="token operator">=</span>finetuning_args<span class="token punctuation">.</span>lora_alpha<span class="token punctuation">,</span>
                lora_dropout<span class="token operator">=</span>finetuning_args<span class="token punctuation">.</span>lora_dropout<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>

            model <span class="token operator">=</span> get_peft_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> lora_config<span class="token punctuation">)</span>
            <span class="token keyword">if</span> <span class="token builtin">id</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>peft_config<span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token builtin">id</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>peft_config<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># https://github.com/huggingface/peft/issues/923</span>
                model<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>peft_config <span class="token operator">=</span> model<span class="token punctuation">.</span>peft_config

</code></pre> 
<p><strong>3. 修改src/llmtuner/tuner/core/parser.py的代码</strong><br> 这边建议所有有关finetuning_args.finetuning_type==/!= "lora"的都改成图片所示<br> <img src="https://images2.imgbox.com/97/93/IEfS2D5d_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="transformer_64"></a>修改transformer源码</h3> 
<p>按照上面的改完之后虽然可以训练，但是其实并没有实现adalora的秩的调整。</p> 
<p>我是通过在update_and_allocate函数中设置断点发现模型训练没有调用update_and_allocate函数，update_and_allocate函数位于python3.10/site-packages/peft/tuners/adalora.py中。</p> 
<p><strong>1. 修改python3.10/site-packages/transformers/trainer.py代码</strong></p> 
<pre><code class="prism language-python">                    <span class="token keyword">from</span> peft <span class="token keyword">import</span> PeftModel
                    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> PeftModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
                            <span class="token keyword">if</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>base_model<span class="token punctuation">,</span> <span class="token string">"update_and_allocate"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                                model<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>update_and_allocate<span class="token punctuation">(</span>total_batched_samples<span class="token punctuation">)</span>
</code></pre> 
<p>把上面的代码复制到train函数中，具体的位置应该是整个文件的第二个model.zero_grad()上面，不同transformers的位置可能不一样<br> <img src="https://images2.imgbox.com/16/51/7V6uIqXG_o.png" alt="在这里插入图片描述"><br> <strong>2. 设置adalora的总迭代次数</strong><br> 两个方法一个是在adaloraconfig定义的时候设定(我没试)，另外一个就是一样修改train.py，如下：<br> 在for epoch in range(epochs_trained, num_train_epochs):上面一行设置</p> 
<pre><code class="prism language-python">        <span class="token comment"># 设置总迭代数</span>
        model<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>peft_config<span class="token punctuation">[</span>model<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>trainable_adapter_name<span class="token punctuation">]</span><span class="token punctuation">.</span>total_step <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/ed/b6/N4eb2wOd_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_88"></a>训练启动</h3> 
<p><img src="https://images2.imgbox.com/e1/46/lzfiSX5f_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/401120a6b9fc0430103d5a41e8f34370/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Win11上安卓设备与MTKClient连接失败，问题汇总，终极方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2954c4dec2bde901af5025b8b160d7ec/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python-动态烟花【附完整源码】</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>