<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Qwen2本地web Demo - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/fcd2bcffdcb4072118e9d5fef5efe1c8/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Qwen2本地web Demo">
  <meta property="og:description" content="Qwen2的web搭建(streamlit) 千问2前段时间发布了，个人觉得千问系列是我用过最好的中文开源大模型，所以这里基于streamlit进行一个千问2的web搭建，来进行模型的测试
一、硬件要求 该文档中使用的千问模型为7B-Instruct，需要5g以上的显存，如果是轻薄本不建议进行本地测试（下图为测试时的实际显存占用）
二、环境准备 对于环境的基本要求
transformers torch streamlit sentencepiece accelerate transformers_stream_generator 上述是基础的环境准备，可以用conda创建一个新的环境来进行配置。在下载库时可以使用清华大学的镜像进行加速，如下所示
pip install transformers -i https://pypi.tuna.tsinghua.edu.cn/simple 三、模型下载 这里推荐使用huggingface镜像网站进行下载，因为在下载中断后，再次请求时会从上次中断的地方继续，而不是重新下载。
https://hf-mirror.com
以千问为例，在终端的下载请求为
huggingface-cli download --resume-download Qwen/Qwen2-7B-Instruct --local-dir ./qwen2 四、web代码编写 from transformers import AutoTokenizer,AutoModelForCausalLM import torch import streamlit as st #在侧边栏创建标题 with st.sidebar: st.markdown(&#34;qwen2&#34;) &#34;hello world&#34; #创建滑块，默认值为512，范围在0到1024之间 max_length = st.slider(&#34;max_length&#34;,0,1024,512,step=1) #创建标题和副标题 st.title(&#34;qwen2 chatbot&#34;) st.caption(&#34;test&#34;) #你下载到本地的模型路径 model_path = &#34;../models/qwen2-1.5b-Instruct&#34; #@streamlit.cache_resource 是一个用于缓存昂贵或频繁调用的资源（如大型文件、网络资源、或数据库连接）的装饰器。这个装饰器可以帮助你提高应用的性能，通过缓存那些不经常变更但加载需要大量时间或计算资源的数据。 #定义的函数来获取tokenizer和model @st.cache_resource def get_model(): tokenizer = AutoTokenizer.from_pretrained(model_path,use_fast=False) model = AutoModelForCausalLM.from_pretrained(model_path,torch_dtype=torch.float16,device_map=&#39;auto&#39;) return tokenizer,model tokenizer,model = get_model() #如果没有消息，则创建默认的消息列表 if &#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-24T19:27:57+08:00">
    <meta property="article:modified_time" content="2024-06-24T19:27:57+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Qwen2本地web Demo</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="Qwen2webstreamlit_0"></a>Qwen2的web搭建(streamlit)</h2> 
<p>千问2前段时间发布了，个人觉得千问系列是我用过最好的中文开源大模型，所以这里基于streamlit进行一个千问2的web搭建，来进行模型的测试</p> 
<h3><a id="_4"></a>一、硬件要求</h3> 
<p>该文档中使用的千问模型为<code>7B-Instruct</code>，需要5g以上的显存，如果是轻薄本不建议进行本地测试（下图为测试时的实际显存占用）</p> 
<p><img src="https://images2.imgbox.com/0a/63/mnTBYjDs_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_11"></a>二、环境准备</h3> 
<p>对于环境的基本要求</p> 
<pre><code class="prism language-bash">transformers
torch
streamlit
sentencepiece
accelerate
transformers_stream_generator
</code></pre> 
<p>上述是基础的环境准备，可以用<code>conda</code>创建一个新的环境来进行配置。在下载库时可以使用清华大学的镜像进行加速，如下所示</p> 
<pre><code class="prism language-bash">pip <span class="token function">install</span> transformers <span class="token parameter variable">-i</span> https://pypi.tuna.tsinghua.edu.cn/simple
</code></pre> 
<h3><a id="_30"></a>三、模型下载</h3> 
<p>这里推荐使用huggingface镜像网站进行下载，因为在下载中断后，再次请求时会从上次中断的地方继续，而不是重新下载。</p> 
<p>https://hf-mirror.com</p> 
<p>以千问为例，在终端的下载请求为</p> 
<pre><code class="prism language-bash">huggingface-cli download --resume-download Qwen/Qwen2-7B-Instruct --local-dir ./qwen2
</code></pre> 
<h3><a id="web_42"></a>四、web代码编写</h3> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span>AutoModelForCausalLM
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> streamlit <span class="token keyword">as</span> st

<span class="token comment">#在侧边栏创建标题</span>
<span class="token keyword">with</span> st<span class="token punctuation">.</span>sidebar<span class="token punctuation">:</span>
    st<span class="token punctuation">.</span>markdown<span class="token punctuation">(</span><span class="token string">"qwen2"</span><span class="token punctuation">)</span>
    <span class="token string">"hello world"</span>
 	<span class="token comment">#创建滑块，默认值为512，范围在0到1024之间</span>
    max_length <span class="token operator">=</span> st<span class="token punctuation">.</span>slider<span class="token punctuation">(</span><span class="token string">"max_length"</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1024</span><span class="token punctuation">,</span><span class="token number">512</span><span class="token punctuation">,</span>step<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment">#创建标题和副标题</span>
st<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"qwen2 chatbot"</span><span class="token punctuation">)</span>
st<span class="token punctuation">.</span>caption<span class="token punctuation">(</span><span class="token string">"test"</span><span class="token punctuation">)</span>

<span class="token comment">#你下载到本地的模型路径</span>
model_path <span class="token operator">=</span> <span class="token string">"../models/qwen2-1.5b-Instruct"</span>

<span class="token comment">#@streamlit.cache_resource 是一个用于缓存昂贵或频繁调用的资源（如大型文件、网络资源、或数据库连接）的装饰器。这个装饰器可以帮助你提高应用的性能，通过缓存那些不经常变更但加载需要大量时间或计算资源的数据。</span>
<span class="token comment">#定义的函数来获取tokenizer和model</span>
<span class="token decorator annotation punctuation">@st<span class="token punctuation">.</span>cache_resource</span>
<span class="token keyword">def</span> <span class="token function">get_model</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_path<span class="token punctuation">,</span>use_fast<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_path<span class="token punctuation">,</span>torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">,</span>device_map<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> tokenizer<span class="token punctuation">,</span>model

tokenizer<span class="token punctuation">,</span>model <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment">#如果没有消息，则创建默认的消息列表</span>
<span class="token keyword">if</span> <span class="token string">"messages"</span> <span class="token keyword">not</span> <span class="token keyword">in</span> st<span class="token punctuation">.</span>session_state<span class="token punctuation">:</span>
    st<span class="token punctuation">.</span>session_state<span class="token punctuation">[</span><span class="token string">'messages'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span><span class="token string">'role'</span><span class="token punctuation">:</span><span class="token string">"assistant"</span><span class="token punctuation">,</span><span class="token string">"content"</span><span class="token punctuation">:</span><span class="token string">"有什么可以帮到您？"</span><span class="token punctuation">}</span><span class="token punctuation">]</span>

<span class="token comment">#便利session_state中的消息并显示在聊天界面上</span>
<span class="token keyword">for</span> msg <span class="token keyword">in</span> st<span class="token punctuation">.</span>session_state<span class="token punctuation">.</span>messages<span class="token punctuation">:</span>
    st<span class="token punctuation">.</span>chat_message<span class="token punctuation">(</span>msg<span class="token punctuation">[</span><span class="token string">"role"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span>msg<span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment">## 如果用户在聊天输入框中输入了内容，则执行下述操作</span>
<span class="token keyword">if</span> prompt <span class="token operator">:=</span> st<span class="token punctuation">.</span>chat_input<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment">#将用户输入添加到message列表中</span>
    st<span class="token punctuation">.</span>session_state<span class="token punctuation">.</span>messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span><span class="token string">"user"</span><span class="token punctuation">,</span><span class="token string">"content"</span><span class="token punctuation">:</span>prompt<span class="token punctuation">}</span><span class="token punctuation">)</span>
    <span class="token comment">#在聊天界面上显示用户输入</span>
    st<span class="token punctuation">.</span>chat_message<span class="token punctuation">(</span><span class="token string">"user"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span>
    
    
	<span class="token comment">#构建输入</span>
    input_ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>apply_chat_template<span class="token punctuation">(</span>st<span class="token punctuation">.</span>session_state<span class="token punctuation">.</span>messages<span class="token punctuation">,</span>tokenize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>add_generation_prompt<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    model_inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token punctuation">[</span>input_ids<span class="token punctuation">]</span><span class="token punctuation">,</span>return_tensors<span class="token operator">=</span><span class="token string">'pt'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">)</span>
    <span class="token comment">#模型生成输出id</span>
    generated_ids <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>model_inputs<span class="token punctuation">.</span>input_ids<span class="token punctuation">,</span>max_new_tokens<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">)</span>
    generated_ids <span class="token operator">=</span> <span class="token punctuation">[</span>
        output_ids<span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token keyword">for</span> input_ids<span class="token punctuation">,</span>output_ids <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>model_inputs<span class="token punctuation">.</span>input_ids<span class="token punctuation">,</span>generated_ids<span class="token punctuation">)</span>
    <span class="token punctuation">]</span>
    <span class="token comment">#将生成的id转换成文字</span>
    response <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>batch_decode<span class="token punctuation">(</span>generated_ids<span class="token punctuation">,</span>skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

    st<span class="token punctuation">.</span>session_state<span class="token punctuation">.</span>messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span><span class="token string">"assistant"</span><span class="token punctuation">,</span><span class="token string">"content"</span><span class="token punctuation">:</span>response<span class="token punctuation">}</span><span class="token punctuation">)</span>
	<span class="token comment">#在界面上显示输出</span>
    st<span class="token punctuation">.</span>chat_message<span class="token punctuation">(</span><span class="token string">"assistant"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span>response<span class="token punctuation">)</span>
</code></pre> 
<blockquote> 
 <p>由于qwen2模型并没有自带流式输出函数，会报错<code>AttributeError: 'Qwen2Model' object has no attribute 'stream_chat'</code>，后续改进考虑对其进行流式输出增强用户可读性</p> 
</blockquote> 
<h3><a id="_104"></a>五、终端启动</h3> 
<p>在该文件目录下，终端输入</p> 
<pre><code class="prism language-bash">streamlit run your_file_name.py
</code></pre> 
<p>之后就会进入web界面</p> 
<h3><a id="_113"></a>六、调试</h3> 
<p>streamlit这样的web形式不能直接通过打断点进行debug，所以需要进行一些处理：</p> 
<p><img src="https://images2.imgbox.com/4c/70/gfocMOIL_o.png" alt="在这里插入图片描述"></p> 
<p>红框中进行下图配置，<code>script</code>框中的路径是你配置的模型环境中，streamlit所在的绝对路径；parameters框就是<code>run your_file_name.py</code>，这样处理后就是终端输入<code>streamlit run your_file_name.py</code>的效果，之后就能进行断点调试了</p> 
<p><img src="https://images2.imgbox.com/51/30/AekIOuxn_o.png" alt="在这里插入图片描述"></p> 
<hr> 
<h2><a id="Reference_127"></a>Reference</h2> 
<p>[1] <a href="https://qwen.readthedocs.io/zh-cn/latest/index.html" rel="nofollow">qwen官方文档</a></p> 
<p>[2] <a href="https://github.com/datawhalechina/self-llm/blob/master/Qwen2/03-Qwen2-7B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md">qwen2 webDemo部署</a></p> 
<p>[3] <a href="https://stackoverflow.com/questions/60172282/how-to-run-debug-a-streamlit-application-from-an-ide" rel="nofollow">streamlit断点调试</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/61af2ce5776b61baba5b4a1c565bcd7d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Pytorch之视频流猫狗识别</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/844a249ff478b47ebde938177503ac50/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">最小生成树模板（prim,heap-prim,kruskal）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>