<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【人工智能】Transformers之Pipeline（一）：音频分类（audio-classification） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/4fe2b4f9d98f66b27a0391202f6700f2/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【人工智能】Transformers之Pipeline（一）：音频分类（audio-classification）">
  <meta property="og:description" content="​​​​​​​
目录
一、引言 二、音频分类（audio-classification）
2.1 概述
2.2 技术原理
2.2.1 Wav2vec 2.0模型
2.2.1 HuBERT模型
2.3 pipeline参数
2.3.1 pipeline对象实例化参数
2.3.2 pipeline对象使用参数 2.4 pipeline实战
2.4.1 指令识别（默认模型）
2.4.2 情感识别
2.5 模型排名
三、总结
一、引言 pipeline（管道）是huggingface transformers库中一种极简方式使用大模型推理的抽象，将所有大模型分为音频（Audio）、计算机视觉（Computer vision）、自然语言处理（NLP）、多模态（Multimodal）等4大类，28小类任务（tasks）。共计覆盖32万个模型
今天介绍Audio音频的第一篇，音频分类（audio-classification），在huggingface库内共有2500个音频分类模型。
二、音频分类（audio-classification） 2.1 概述 音频分类，顾名思义就是将音频打标签或分配类别的任务。主要应用场景有语音情绪分类、语音命令分类、说话人分类、音乐风格判别、语言判别等。
2.2 技术原理 音频分类，主要思想就是将音频的音谱切分成25ms-60ms的片段，通过CNN等卷积神经网络模型提取特征并进行embedding化，基于transformer与文本类别对齐训练。下面介绍2个代表模型：
2.2.1 Wav2vec 2.0模型 Wav2vec 2.0是 Meta在2020年发表的无监督语音预训练模型。它的核心思想是通过向量量化（Vector Quantization，VQ）构造自建监督训练目标，对输入做大量掩码后利用对比学习损失函数进行训练。模型结构如图，基于卷积网络（Convoluational Neural Network，CNN）的特征提取器将原始音频编码为帧特征序列，通过 VQ 模块把每帧特征转变为离散特征 Q，并作为自监督目标。同时，帧特征序列做掩码操作后进入 Transformer [5] 模型得到上下文表示 C。最后通过对比学习损失函数，拉近掩码位置的上下文表示与对应的离散特征 q 的距离，即正样本对。
2.2.1 HuBERT模型 HuBERT是Meta在2021年发表的模型，模型结构类似 Wav2vec 2.0，不同的是训练方法。Wav2vec 2.0 是在训练时将语音特征离散化作为自监督目标，而 HuBERT 则通过在 MFCC 特征或 HuBERT 特征上做 K-means 聚类，得到训练目标。HuBERT 模型采用迭代训练的方式，BASE 模型第一次迭代在 MFCC 特征上做聚类，第二次迭代在第一次迭代得到的 HuBERT 模型的中间层特征上做聚类，LARGE 和 XLARGE 模型则用 BASE 模型的第二次迭代模型提取特征做聚类。从原始论文实验结果来看，HuBERT 模型效果要优于 Wav2vec 2.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-12T22:08:31+08:00">
    <meta property="article:modified_time" content="2024-07-12T22:08:31+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【人工智能】Transformers之Pipeline（一）：音频分类（audio-classification）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="%C2%A0%E2%80%8B%E7%BC%96%E8%BE%91">​​​​​​​<img alt="" height="402" src="https://images2.imgbox.com/bd/fd/18YB1YNA_o.png" width="1200"></p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E5%BC%95%E8%A8%80%C2%A0-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E5%BC%95%E8%A8%80%C2%A0" rel="nofollow">一、引言 </a></p> 
<p id="%E4%BA%8C%E3%80%81Tokenizer-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81Tokenizer" rel="nofollow">二、音频分类（audio-classification）</a></p> 
<p id="2.1%20%E6%A6%82%E8%BF%B0-toc" style="margin-left:40px;"><a href="#2.1%20%E6%A6%82%E8%BF%B0" rel="nofollow">2.1 概述</a></p> 
<p id="2.2%20%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95-toc" style="margin-left:40px;"><a href="#2.2%20%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95" rel="nofollow">2.2 技术原理</a></p> 
<p id="2.2.1%C2%A0Wav2vec%202.0%E6%A8%A1%E5%9E%8B-toc" style="margin-left:80px;"><a href="#2.2.1%C2%A0Wav2vec%202.0%E6%A8%A1%E5%9E%8B" rel="nofollow">2.2.1 Wav2vec 2.0模型</a></p> 
<p id="%C2%A02.2.1%20HuBERT%E6%A8%A1%E5%9E%8B-toc" style="margin-left:80px;"><a href="#%C2%A02.2.1%20HuBERT%E6%A8%A1%E5%9E%8B" rel="nofollow"> 2.2.1 HuBERT模型</a></p> 
<p id="2.3%20pipeline%E5%8F%82%E6%95%B0-toc" style="margin-left:40px;"><a href="#2.3%20pipeline%E5%8F%82%E6%95%B0" rel="nofollow">2.3 pipeline参数</a></p> 
<p id="2.3.1%20pipeline%E5%AF%B9%E8%B1%A1%E5%AE%9E%E4%BE%8B%E5%8C%96%E5%8F%82%E6%95%B0-toc" style="margin-left:80px;"><a href="#2.3.1%20pipeline%E5%AF%B9%E8%B1%A1%E5%AE%9E%E4%BE%8B%E5%8C%96%E5%8F%82%E6%95%B0" rel="nofollow">2.3.1 pipeline对象实例化参数</a></p> 
<p id="2.3.2%20pipeline%E5%AF%B9%E8%B1%A1%E4%BD%BF%E7%94%A8%E5%8F%82%E6%95%B0%C2%A0-toc" style="margin-left:80px;"><a href="#2.3.2%20pipeline%E5%AF%B9%E8%B1%A1%E4%BD%BF%E7%94%A8%E5%8F%82%E6%95%B0%C2%A0" rel="nofollow">2.3.2 pipeline对象使用参数 </a></p> 
<p id="2.4%C2%A0pipeline%E5%AE%9E%E6%88%98-toc" style="margin-left:40px;"><a href="#2.4%C2%A0pipeline%E5%AE%9E%E6%88%98" rel="nofollow">2.4 pipeline实战</a></p> 
<p id="2.4.1%20%E6%8C%87%E4%BB%A4%E8%AF%86%E5%88%AB%EF%BC%88%E9%BB%98%E8%AE%A4%E6%A8%A1%E5%9E%8B%EF%BC%89-toc" style="margin-left:80px;"><a href="#2.4.1%20%E6%8C%87%E4%BB%A4%E8%AF%86%E5%88%AB%EF%BC%88%E9%BB%98%E8%AE%A4%E6%A8%A1%E5%9E%8B%EF%BC%89" rel="nofollow">2.4.1 指令识别（默认模型）</a></p> 
<p id="%C2%A02.4.2%C2%A0%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB-toc" style="margin-left:80px;"><a href="#%C2%A02.4.2%C2%A0%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB" rel="nofollow"> 2.4.2 情感识别</a></p> 
<p id="2.5%C2%A0%E6%A8%A1%E5%9E%8B%E6%8E%92%E5%90%8D-toc" style="margin-left:40px;"><a href="#2.5%C2%A0%E6%A8%A1%E5%9E%8B%E6%8E%92%E5%90%8D" rel="nofollow">2.5 模型排名</a></p> 
<p id="2.2.1%C2%A0%E5%AE%89%E8%A3%85timm%E5%BA%93-toc" style="margin-left:0px;"><a href="#2.2.1%C2%A0%E5%AE%89%E8%A3%85timm%E5%BA%93" rel="nofollow">三、总结</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E4%B8%80%E3%80%81%E5%BC%95%E8%A8%80%C2%A0">一、引言 </h2> 
<p> pipeline（管道）是huggingface transformers库中一种极简方式使用大模型推理的抽象，将所有大模型分为音频（Audio）、计算机视觉（Computer vision）、自然语言处理（NLP）、多模态（Multimodal）等4大类，28小类任务（tasks）。共计覆盖32万个模型</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/eb/28/LaXfqmBs_o.jpg" width="1065"></p> 
<p>今天介绍Audio音频的第一篇，音频分类（audio-classification），在huggingface库内共有2500个音频分类模型。</p> 
<h2 id="%E4%BA%8C%E3%80%81Tokenizer">二、音频分类（audio-classification）</h2> 
<h3 id="2.1%20%E6%A6%82%E8%BF%B0">2.1 概述</h3> 
<p>音频分类，顾名思义就是将音频打标签或分配类别的任务。主要应用场景有<strong>语音情绪分类</strong>、<strong>语音命令分类</strong>、<strong>说话人分类</strong>、<strong>音乐风格判别</strong>、<strong>语言判别</strong>等。</p> 
<p><img alt="" height="604" src="https://images2.imgbox.com/aa/23/pHS6ULk5_o.png" width="1200"></p> 
<h3 id="2.2%20%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95">2.2 技术原理</h3> 
<p>音频分类，主要思想就是将音频的音谱切分成25ms-60ms的片段，通过CNN等卷积神经网络模型提取特征并进行embedding化，基于transformer与文本类别对齐训练。下面介绍2个代表模型：</p> 
<h4 id="2.2.1%C2%A0Wav2vec%202.0%E6%A8%A1%E5%9E%8B">2.2.1 Wav2vec 2.0模型</h4> 
<p>Wav2vec 2.0是 Meta在2020年发表的无监督语音预训练模型。它的核心思想是通过向量量化（Vector Quantization，VQ）构造自建监督训练目标，对输入做大量掩码后利用对比学习损失函数进行训练。模型结构如图，基于卷积网络（Convoluational Neural Network，CNN）的特征提取器将原始音频编码为帧特征序列，通过 VQ 模块把每帧特征转变为离散特征 Q，并作为自监督目标。同时，帧特征序列做掩码操作后进入 Transformer [5] 模型得到上下文表示 C。最后通过对比学习损失函数，拉近掩码位置的上下文表示与对应的离散特征 q 的距离，即正样本对。</p> 
<p><img alt="" height="830" src="https://images2.imgbox.com/a6/f2/MWNkpL0z_o.png" width="1200"></p> 
<h4 id="%C2%A02.2.1%20HuBERT%E6%A8%A1%E5%9E%8B"> 2.2.1 HuBERT模型</h4> 
<p>HuBERT是Meta在2021年发表的模型，模型结构类似 Wav2vec 2.0，不同的是训练方法。Wav2vec 2.0 是在训练时将语音特征离散化作为自监督目标，而 HuBERT 则通过在 MFCC 特征或 HuBERT 特征上做 K-means 聚类，得到训练目标。HuBERT 模型采用迭代训练的方式，BASE 模型第一次迭代在 MFCC 特征上做聚类，第二次迭代在第一次迭代得到的 HuBERT 模型的中间层特征上做聚类，LARGE 和 XLARGE 模型则用 BASE 模型的第二次迭代模型提取特征做聚类。从原始论文实验结果来看，HuBERT 模型效果要优于 Wav2vec 2.0，特别是下游任务有监督训练数据极少的情况，如 1 小时、10 分钟。</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/e8/23/G8ilqfFP_o.png" width="1200"></p> 
<h3 id="2.3%20pipeline%E5%8F%82%E6%95%B0">2.3 pipeline参数</h3> 
<h4 id="2.3.1%20pipeline%E5%AF%B9%E8%B1%A1%E5%AE%9E%E4%BE%8B%E5%8C%96%E5%8F%82%E6%95%B0">2.3.1 pipeline对象实例化参数</h4> 
<blockquote> 
 <ul style="margin-left:0;"><li><strong>模型</strong>（<a href="https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/model#transformers.PreTrainedModel" rel="nofollow" title="PreTrainedModel">PreTrainedModel</a>或<a href="https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/model#transformers.TFPreTrainedModel" rel="nofollow" title="TFPreTrainedModel">TFPreTrainedModel</a>）— 管道将使用其进行预测的模型。 对于 PyTorch，这需要从<a href="https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/model#transformers.PreTrainedModel" rel="nofollow" title="PreTrainedModel">PreTrainedModel</a>继承；对于 TensorFlow，这需要从<a href="https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/model#transformers.TFPreTrainedModel" rel="nofollow" title="TFPreTrainedModel继承。">TFPreTrainedModel继承。</a></li><li><strong>feature_extractor</strong> ( <a href="https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor" rel="nofollow" title="SequenceFeatureExtractor">SequenceFeatureExtractor</a> ) — 管道将使用的特征提取器来为模型编码数据。此对象继承自 <a href="https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor" rel="nofollow" title="SequenceFeatureExtractor">SequenceFeatureExtractor</a>。</li><li><strong>modelcard</strong>（<code>str</code>或<code>ModelCard</code>，<em>可选</em>） — 属于此管道模型的模型卡。</li><li><strong>framework</strong>（<code>str</code>，<em>可选</em>）— 要使用的框架，<code>"pt"</code>适用于 PyTorch 或<code>"tf"</code>TensorFlow。必须安装指定的框架。 <p style="margin-left:0;"></p> <p style="margin-left:0;">如果未指定框架，则默认为当前安装的框架。如果未指定框架且安装了两个框架，则默认为 的框架<code>model</code>，如果未提供模型，则默认为 PyTorch。</p> </li><li><strong>任务</strong>（<code>str</code>，默认为<code>""</code>）— 管道的任务标识符。</li><li><strong>num_workers</strong>（<code>int</code>，<em>可选</em>，默认为 8）— 当管道将使用<em>DataLoader</em>（传递数据集时，在 Pytorch 模型的 GPU 上）时，要使用的工作者数量。</li><li><strong>batch_size</strong>（<code>int</code>，<em>可选</em>，默认为 1）— 当管道将使用<em>DataLoader</em>（传递数据集时，在 Pytorch 模型的 GPU 上）时，要使用的批次的大小，对于推理来说，这并不总是有益的，请阅读<a href="https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching" rel="nofollow" title="使用管道进行批处理">使用管道进行批处理</a>。</li><li><strong>args_parser</strong>（<a href="https://huggingface.co/docs/transformers/v4.42.0/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler" rel="nofollow" title="ArgumentHandler">ArgumentHandler</a>，<em>可选</em>） - 引用负责解析提供的管道参数的对象。</li><li><strong>设备</strong>（<code>int</code>，<em>可选</em>，默认为 -1）— CPU/GPU 支持的设备序号。将其设置为 -1 将利用 CPU，设置为正数将在关联的 CUDA 设备 ID 上运行模型。您可以传递本机<code>torch.device</code>或<code>str</code>太</li><li><strong>torch_dtype</strong>（<code>str</code>或<code>torch.dtype</code>，<em>可选</em>） - 直接发送<code>model_kwargs</code>（只是一种更简单的快捷方式）以使用此模型的可用精度（<code>torch.float16</code>，，<code>torch.bfloat16</code>...或<code>"auto"</code>）</li><li><strong>binary_output</strong>（<code>bool</code>，<em>可选</em>，默认为<code>False</code>）——标志指示管道的输出是否应以序列化格式（即 pickle）或原始输出数据（例如文本）进行。</li></ul> 
</blockquote> 
<h4 id="2.3.2%20pipeline%E5%AF%B9%E8%B1%A1%E4%BD%BF%E7%94%A8%E5%8F%82%E6%95%B0%C2%A0">2.3.2 pipeline对象使用参数 </h4> 
<blockquote> 
 <ul style="margin-left:0;"><li><strong>输入</strong>（<code>np.ndarray</code>或<code>bytes</code>或<code>str</code>或<code>dict</code>） — 输入可以是： 
   <ul style="margin-left:0;"><li><code>str</code>这是音频文件的文件名，将以正确的采样率读取该文件以使用<em>ffmpeg</em>获取波形。这需要在系统上安装<em>ffmpeg 。</em></li><li><code>bytes</code>它应该是音频文件的内容，并以相同的方式由<em>ffmpeg进行解释。</em></li><li>（<code>np.ndarray</code>形状为（n，）类型为<code>np.float32</code>或<code>np.float64</code>）正确采样率的原始音频（不再进行进一步检查）</li><li><code>dict</code>形式可用于传递任意采样的原始音频<code>sampling_rate</code>，并让此管道进行重新采样。字典必须采用 或 格式<code>{"sampling_rate": int, "raw": np.array}</code>，<code>{"sampling_rate": int, "array": np.array}</code>其中键<code>"raw"</code>或 <code>"array"</code>用于表示原始音频波形。</li></ul></li><li><strong>top_k</strong>（<code>int</code>，<em>可选</em>，默认为 None）— 管道将返回的顶部标签数。如果提供的数字等于<code>None</code>或高于模型配置中可用的标签数，则将默认为标签数。</li></ul> 
</blockquote> 
<h3 id="2.4%C2%A0pipeline%E5%AE%9E%E6%88%98">2.4 pipeline实战</h3> 
<h4 id="2.4.1%20%E6%8C%87%E4%BB%A4%E8%AF%86%E5%88%AB%EF%BC%88%E9%BB%98%E8%AE%A4%E6%A8%A1%E5%9E%8B%EF%BC%89">2.4.1 指令识别（默认模型）</h4> 
<p>pipeline对于audio-classification的默认模型时superb/wav2vec2-base-superb-ks，使用pipeline时，如果仅设置task=audio-classification，不设置模型，则下载并使用默认模型。</p> 
<pre><code class="language-python">import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["CUDA_VISIBLE_DEVICES"] = "2"

from transformers import pipeline

speech_file = "./output_video_enhanced.mp3"
pipe = pipeline(task="audio-classification")
result = pipe(speech_file)
print(result)</code></pre> 
<p> 这是一个上下左右yes及no的指令识别模型，感觉像是训练动物。</p> 
<pre><code class="language-python">[{'score': 0.9988580942153931, 'label': '_unknown_'}, {'score': 0.000909291033167392, 'label': 'down'}, {'score': 9.889943612506613e-05, 'label': 'no'}, {'score': 7.015655864961445e-05, 'label': 'yes'}, {'score': 5.134344974067062e-05, 'label': 'stop'}]</code></pre> 
<h4 id="%C2%A02.4.2%C2%A0%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB"> 2.4.2 情感识别</h4> 
<p>我们指定模型为情感识别模型ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition，具体代码为：</p> 
<pre><code class="language-python">import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["CUDA_VISIBLE_DEVICES"] = "2"

from transformers import pipeline

speech_file = "./output_video_enhanced.mp3"
pipe = pipeline(task="audio-classification",model="ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition")
result = pipe(speech_file)
print(result)</code></pre> 
<p>输入为一段mp3格式的语音，输出为</p> 
<pre><code class="language-python">[{'score': 0.13128453493118286, 'label': 'angry'}, {'score': 0.12990005314350128, 'label': 'calm'}, {'score': 0.1262471228837967, 'label': 'happy'}, {'score': 0.12568499147891998, 'label': 'surprised'}, {'score': 0.12327362596988678, 'label': 'disgust'}]</code></pre> 
<h3 id="2.5%C2%A0%E6%A8%A1%E5%9E%8B%E6%8E%92%E5%90%8D">2.5 模型排名</h3> 
<p>在huggingface上，我们筛选音频分类模型，并按下载量从高到低排序：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/c9/08/I4M1vaWJ_o.png" width="1200"></p> 
<h2 id="2.2.1%C2%A0%E5%AE%89%E8%A3%85timm%E5%BA%93"><strong>三、总结</strong></h2> 
<p>本文对transformers之pipeline的音频分类（audio-classification）从概述、技术原理、pipeline参数、pipeline实战、模型排名等方面进行介绍，读者可以基于pipeline使用文中的代码极简的进行音频分类推理，应用于音频情感识别、音乐曲风判断等业务场景。</p> 
<p></p> 
<p>期待您的3连+关注，如何还有时间，欢迎阅读我的其他文章：</p> 
<p>《Transformers-Pipeline概述》</p> 
<p id="articleContentId"><a class="link-info" href="https://blog.csdn.net/weixin_48007632/article/details/140319929?spm=1001.2014.3001.5501" title="【人工智能】Transformers之Pipeline（概述）：30w+大模型极简应用">【人工智能】Transformers之Pipeline（概述）：30w+大模型极简应用</a></p> 
<p>《Transformers-Pipeline 第一章：音频（Audio）篇》</p> 
<p><a class="link-info" href="https://blog.csdn.net/weixin_48007632/article/details/140360594?spm=1001.2014.3001.5501" title="【人工智能】Transformers之Pipeline（一）：音频分类（audio-classification）">【人工智能】Transformers之Pipeline（一）：音频分类（audio-classification）</a>​​​​​​​</p> 
<p>【人工智能】Transformers之Pipeline（二）：自动语音识别（automatic-speech-recognition）</p> 
<p>【人工智能】Transformers之Pipeline（三）：文本转音频（text-to-audio）</p> 
<p>【人工智能】Transformers之Pipeline（四）：零样本音频分类（zero-shot-audio-classification）</p> 
<p>《Transformers-Pipeline 第二章：计算机视觉（CV）篇》</p> 
<p>【人工智能】Transformers之Pipeline（五）：深度估计（depth-estimation）</p> 
<p>【人工智能】Transformers之Pipeline（六）：图像分类（image-classification）</p> 
<p>【人工智能】Transformers之Pipeline（七）：图像分割（image-segmentation）</p> 
<p>【人工智能】Transformers之Pipeline（八）：图生图（image-to-image）</p> 
<p>【人工智能】Transformers之Pipeline（九）：物体检测（object-detection）</p> 
<p>【人工智能】Transformers之Pipeline（十）：视频分类（video-classification）</p> 
<p>【人工智能】Transformers之Pipeline（十一）：零样本图片分类（zero-shot-image-classification）</p> 
<p>【人工智能】Transformers之Pipeline（十二）：零样本物体检测（zero-shot-object-detection）</p> 
<p>《Transformers-Pipeline 第三章：自然语言处理（NLP）篇》</p> 
<p>【人工智能】Transformers之Pipeline（十三）：填充蒙版（fill-mask）</p> 
<p>【人工智能】Transformers之Pipeline（十四）：问答（question-answering）</p> 
<p>【人工智能】Transformers之Pipeline（十五）：总结（summarization）</p> 
<p>【人工智能】Transformers之Pipeline（十六）：表格问答（table-question-answering）</p> 
<p>【人工智能】Transformers之Pipeline（十七）：文本分类（text-classification）</p> 
<p>【人工智能】Transformers之Pipeline（十八）：文本生成（text-generation）</p> 
<p>【人工智能】Transformers之Pipeline（十九）：文生文（text2text-generation）</p> 
<p>【人工智能】Transformers之Pipeline（二十）：令牌分类（token-classification）</p> 
<p>【人工智能】Transformers之Pipeline（二十一）：翻译（translation）</p> 
<p>【人工智能】Transformers之Pipeline（二十二）：零样本文本分类（zero-shot-classification）</p> 
<p>《Transformers-Pipeline 第四章：多模态（Multimodal）篇》</p> 
<p>【人工智能】Transformers之Pipeline（二十三）：文档问答（document-question-answering）</p> 
<p>【人工智能】Transformers之Pipeline（二十四）：特征抽取（feature-extraction）</p> 
<p>【人工智能】Transformers之Pipeline（二十五）：图片特征抽取（image-feature-extraction）</p> 
<p>【人工智能】Transformers之Pipeline（二十六）：图片转文本（image-to-text）</p> 
<p>【人工智能】Transformers之Pipeline（二十七）：掩码生成（mask-generation）</p> 
<p>【人工智能】Transformers之Pipeline（二十八）：视觉问答（visual-question-answering）</p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ad30903fba05c08653d9928b40900f2a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">M3U8工作原理以及key解密视频流详解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/41cfa969e32b95f9d4b43a427161daae/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python数据分析-Excel和 Text 文件的读写操作</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>