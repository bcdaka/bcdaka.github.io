<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Hive 调优(包含hive所有调优方式 看这一篇就够了) - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/69bba8a70a676ca363f5d67d7d625253/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Hive 调优(包含hive所有调优方式 看这一篇就够了)">
  <meta property="og:description" content="文章目录 Hive 调优1、调优方式2、hive数据压缩2.1 压缩对比2.2 开启压缩 3、hive数据存储3.1 行列存储原理3.2 存储压缩比 4、fetch抓取5、本地模式6、join的优化操作7、列裁剪8、分区裁剪9、group by 操作10、count(distinct)11、笛卡尔积12、动态分区13、如何调整map和reduce的数量14、并行执行15、严格模式16、JVM 重用17、推测执行18、执行计划explain Hive 调优 hive官方配置url: https://cwiki.apache.org/confluence/display/Hive/Configuration&#43;Properties
1、调优方式 hive参数配置的意义: 开发Hive应用/调优时，不可避免地需要设定Hive的参数。设定Hive的参数可以调优HQL代码的执行效率，或帮助定位问题。然而实践中经常遇到的一个问题是，为什么我设定的参数没有起作用？这是对hive参数配置几种方式不了解导致的! hive参数设置范围（从大到小）: 配置文件 &gt; 命令行参数 &gt; set参数声明 hive参数设置优先级（从高优先级到低优先级）: set参数声明 &gt; 命令行参数 &gt; 配置文件 注意: 在工作中，推荐使用set参数声明，因为最简单最方便。而且同一个大数据集群除了你了还有其他的人或者项目组在使用。 2、hive数据压缩 Hive底层是运行MapReduce，所以Hive支持什么压缩格式本质上取决于MapReduce。
2.1 压缩对比 在后续可能会使用GZ(GZIP), 保证压缩后的数据更小, 同时压缩和解压的速度比较OK的,
但是大部分的选择主要会选择另一种压缩方案, snappy, 此种方案可以保证在合理的压缩比下, 拥有更高的解压缩的速度，在大数据领域中主要是关注数据的处理速度
http://google.github.io/snappy/
On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-15T23:24:14+08:00">
    <meta property="article:modified_time" content="2024-05-15T23:24:14+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Hive 调优(包含hive所有调优方式 看这一篇就够了)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#Hive__1" rel="nofollow">Hive 调优</a></li><li><ul><li><a href="#1_7" rel="nofollow">1、调优方式</a></li><li><a href="#2hive_20" rel="nofollow">2、hive数据压缩</a></li><li><ul><li><a href="#21__26" rel="nofollow">2.1 压缩对比</a></li><li><a href="#22__43" rel="nofollow">2.2 开启压缩</a></li></ul> 
   </li><li><a href="#3hive_83" rel="nofollow">3、hive数据存储</a></li><li><ul><li><a href="#31__85" rel="nofollow">3.1 行列存储原理</a></li><li><a href="#32__121" rel="nofollow">3.2 存储压缩比</a></li></ul> 
   </li><li><a href="#4fetch_213" rel="nofollow">4、fetch抓取</a></li><li><a href="#5_267" rel="nofollow">5、本地模式</a></li><li><a href="#6join_310" rel="nofollow">6、join的优化操作</a></li><li><a href="#7_369" rel="nofollow">7、列裁剪</a></li><li><a href="#8_391" rel="nofollow">8、分区裁剪</a></li><li><a href="#9group_by__414" rel="nofollow">9、group by 操作</a></li><li><a href="#10countdistinct_433" rel="nofollow">10、count(distinct)</a></li><li><a href="#11_473" rel="nofollow">11、笛卡尔积</a></li><li><a href="#12_492" rel="nofollow">12、动态分区</a></li><li><a href="#13mapreduce_513" rel="nofollow">13、如何调整map和reduce的数量</a></li><li><a href="#14_572" rel="nofollow">14、并行执行</a></li><li><a href="#15_599" rel="nofollow">15、严格模式</a></li><li><a href="#16JVM__621" rel="nofollow">16、JVM 重用</a></li><li><a href="#17_631" rel="nofollow">17、推测执行</a></li><li><a href="#18explain_646" rel="nofollow">18、执行计划explain</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="Hive__1"></a>Hive 调优</h2> 
<p>hive官方配置url: https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties</p> 
<h3><a id="1_7"></a>1、调优方式</h3> 
<pre><code class="prism language-properties">hive参数配置的意义: 开发Hive应用/调优时，不可避免地需要设定Hive的参数。设定Hive的参数可以调优HQL代码的执行效率，或帮助定位问题。然而实践中经常遇到的一个问题是，为什么我设定的参数没有起作用？这是对hive参数配置几种方式不了解导致的!

hive参数设置范围（从大到小）: 配置文件 &gt; 命令行参数 &gt; set参数声明
hive参数设置优先级（从高优先级到低优先级）: set参数声明 &gt; 命令行参数 &gt; 配置文件

注意: 在工作中，推荐使用set参数声明，因为最简单最方便。而且同一个大数据集群除了你了还有其他的人或者项目组在使用。
</code></pre> 
<h3><a id="2hive_20"></a>2、hive数据压缩</h3> 
<p><strong>Hive底层是运行MapReduce，所以Hive支持什么压缩格式本质上取决于MapReduce。</strong></p> 
<h4><a id="21__26"></a>2.1 压缩对比</h4> 
<blockquote> 
 <p>在后续可能会使用GZ(GZIP), 保证压缩后的数据更小, 同时压缩和解压的速度比较OK的,</p> 
 <p>但是大部分的选择主要会选择另一种压缩方案, <strong>snappy, 此种方案可以保证在合理的压缩比下, 拥有更高的解压缩的速度，在大数据领域中主要是关注数据的处理速度</strong></p> 
 <p>http://google.github.io/snappy/<br> On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about <strong>250 MB</strong>/sec or more and decompresses at about <strong>500 MB</strong>/sec or more.</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/57/ab/HYV7PII8_o.png" alt="在这里插入图片描述![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/608c2d9be2b04498a在这里插入图片描述
28905816eb8a9f2.png)"></p> 
<h4><a id="22__43"></a>2.2 开启压缩</h4> 
<blockquote> 
 <p>开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量. 当Hive将输出写入到表中时，输出内容同样可以进行压缩。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。</p> 
</blockquote> 
<pre><code class="prism language-sql">
<span class="token comment">-- 创建数据库</span>
<span class="token keyword">create</span> <span class="token keyword">database</span> hive05<span class="token punctuation">;</span>
<span class="token comment">-- 使用库</span>
<span class="token keyword">use</span> hive05<span class="token punctuation">;</span>


<span class="token comment">-- 开启压缩(map阶段或者reduce阶段)</span>
<span class="token comment">--开启hive支持中间结果的压缩方案</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>compress<span class="token punctuation">.</span>intermediate<span class="token punctuation">;</span> <span class="token comment">-- 查看默认</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>compress<span class="token punctuation">.</span>intermediate<span class="token operator">=</span><span class="token boolean">true</span> <span class="token punctuation">;</span>
<span class="token comment">--开启hive支持最终结果压缩</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>compress<span class="token punctuation">.</span>output<span class="token punctuation">;</span> <span class="token comment">-- 查看默认</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>compress<span class="token punctuation">.</span>output<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>

<span class="token comment">--开启MR的map端压缩操作</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>map<span class="token punctuation">.</span>output<span class="token punctuation">.</span>compress<span class="token punctuation">;</span> <span class="token comment">-- 查看默认</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>map<span class="token punctuation">.</span>output<span class="token punctuation">.</span>compress<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token comment">--设置mapper端压缩的方案</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>map<span class="token punctuation">.</span>output<span class="token punctuation">.</span>compress<span class="token punctuation">.</span>codec<span class="token punctuation">;</span> <span class="token comment">-- 查看默认</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>map<span class="token punctuation">.</span>output<span class="token punctuation">.</span>compress<span class="token punctuation">.</span>codec<span class="token operator">=</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>compress<span class="token punctuation">.</span>SnappyCodec<span class="token punctuation">;</span>

<span class="token comment">-- 开启MR的reduce端的压缩方案</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>output<span class="token punctuation">.</span>fileoutputformat<span class="token punctuation">.</span>compress<span class="token punctuation">;</span> <span class="token comment">-- 查看默认</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>output<span class="token punctuation">.</span>fileoutputformat<span class="token punctuation">.</span>compress<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token comment">-- 设置reduce端压缩的方案</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>output<span class="token punctuation">.</span>fileoutputformat<span class="token punctuation">.</span>compress<span class="token punctuation">.</span>codec<span class="token punctuation">;</span> <span class="token comment">-- 查看默认</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>output<span class="token punctuation">.</span>fileoutputformat<span class="token punctuation">.</span>compress<span class="token punctuation">.</span>codec <span class="token operator">=</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>compress<span class="token punctuation">.</span>SnappyCodec<span class="token punctuation">;</span>
<span class="token comment">--设置reduce的压缩类型</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>output<span class="token punctuation">.</span>fileoutputformat<span class="token punctuation">.</span>compress<span class="token punctuation">.</span><span class="token keyword">type</span><span class="token punctuation">;</span> <span class="token comment">-- 查看默认</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>output<span class="token punctuation">.</span>fileoutputformat<span class="token punctuation">.</span>compress<span class="token punctuation">.</span><span class="token keyword">type</span><span class="token operator">=</span>BLOCK<span class="token punctuation">;</span>
</code></pre> 
<h3><a id="3hive_83"></a>3、hive数据存储</h3> 
<h4><a id="31__85"></a>3.1 行列存储原理</h4> 
<p><img src="https://images2.imgbox.com/b2/c5/BLFczcnG_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/8c/a2/ZRG7vJNs_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-properties">行存储的特点: 将数据以行的形式整体进行存放
列存储的特点: 将相同字段的值作为整体放在一起进行存放

行存储:
	优点: 如果要查询整行数据内容，速度比较快。适合进行数据的insert/update操作
	缺点: 如果数据分析的时候，只想针对某几个字段进行处理，那么这个效率低。因为会将不需要的字段内容也会加载出来
	使用: textfile和sequencefile
	
列存储: 
	优点: 如果数据分析的时候，只想针对某几个字段进行处理，那么效率高，因为你要什么，我就给你返回什么
	缺点: 如果要查询整行数据内容，速度比较慢。不适合进行数据的insert/update操作
	使用: orc和parquet。推荐使用orc
	
注意: 在工作中推荐ORC+Snappy压缩一起使用，更加能够减少HDFS的磁盘占用。
格式:
	stored as orc -- 设置数据的存储格式
	tblproperties ("orc.compress"="SNAPPY"); -- 设置数据的压缩格式
</code></pre> 
<p>ORC</p> 
<p><img src="https://images2.imgbox.com/6e/a4/gwzz61Rb_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="32__121"></a>3.2 存储压缩比</h4> 
<pre><code class="prism language-sql"><span class="token comment">-- 数据的存储和压缩方式</span>
<span class="token comment">-- textfile：占用空间18.13 MB（如果是HDFS需要乘以3）</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> log_text<span class="token punctuation">(</span>
    track_time string<span class="token punctuation">,</span>
    url string<span class="token punctuation">,</span>
    session_id string<span class="token punctuation">,</span>
    referer string<span class="token punctuation">,</span>
    ip string<span class="token punctuation">,</span>
    end_user_id string<span class="token punctuation">,</span>
    city_id string
<span class="token punctuation">)</span>
<span class="token keyword">ROW</span> FORMAT DELIMITED <span class="token keyword">FIELDS</span> <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> <span class="token string">'\t'</span>
stored <span class="token keyword">as</span> textfile<span class="token punctuation">;</span> <span class="token comment">-- 表默认的存储格式就是不压缩并且使用行式存储中的textfile存储</span>

<span class="token comment">-- 导入数据</span>
<span class="token keyword">load</span> <span class="token keyword">data</span> inpath <span class="token string">'/dir/log.data'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> log_text<span class="token punctuation">;</span>

<span class="token comment">-- 数据验证</span>
<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> log_text<span class="token punctuation">;</span>
<span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">as</span> cnt <span class="token keyword">from</span> log_text<span class="token punctuation">;</span>

<span class="token comment">-- 列式存储：ORC，占用空间2.78 MB</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> log_orc<span class="token punctuation">(</span>
    track_time string<span class="token punctuation">,</span>
    url string<span class="token punctuation">,</span>
    session_id string<span class="token punctuation">,</span>
    referer string<span class="token punctuation">,</span>
    ip string<span class="token punctuation">,</span>
    end_user_id string<span class="token punctuation">,</span>
    city_id string
<span class="token punctuation">)</span>
<span class="token keyword">ROW</span> FORMAT DELIMITED <span class="token keyword">FIELDS</span> <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> <span class="token string">'\t'</span>
stored <span class="token keyword">as</span> orc<span class="token punctuation">;</span>

<span class="token comment">-- 加载数据</span>
<span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> log_orc <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> log_text<span class="token punctuation">;</span>
<span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">as</span> cnt <span class="token keyword">from</span> log_orc<span class="token punctuation">;</span>

<span class="token comment">-- 列式存储：ORC+snappy。占用空间3.75 MB，这里相对ORC反而多占用了1MB空间，是因为数据量太小，同时压缩以后，压缩文件里面需要存储和压缩相关元数据信息（例如：使用的压缩算法具体是哪一个）</span>
<span class="token comment">-- https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> log_orc_snappy<span class="token punctuation">(</span>
    track_time string<span class="token punctuation">,</span>
    url string<span class="token punctuation">,</span>
    session_id string<span class="token punctuation">,</span>
    referer string<span class="token punctuation">,</span>
    ip string<span class="token punctuation">,</span>
    end_user_id string<span class="token punctuation">,</span>
    city_id string
<span class="token punctuation">)</span>
<span class="token keyword">ROW</span> FORMAT DELIMITED <span class="token keyword">FIELDS</span> <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> <span class="token string">'\t'</span>
stored <span class="token keyword">as</span> orc <span class="token comment">-- 设置数据的存储格式</span>
tblproperties <span class="token punctuation">(</span><span class="token string">"orc.compress"</span><span class="token operator">=</span><span class="token string">"SNAPPY"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">-- 设置数据的压缩格式</span>

<span class="token comment">-- 加载数据</span>
<span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> log_orc_snappy <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> log_text<span class="token punctuation">;</span>
<span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">as</span> cnt <span class="token keyword">from</span> log_orc_snappy<span class="token punctuation">;</span>

<span class="token comment">-- 列式存储：parquet。占用空间13.09MB</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> log_orc_parquet<span class="token punctuation">(</span>
    track_time string<span class="token punctuation">,</span>
    url string<span class="token punctuation">,</span>
    session_id string<span class="token punctuation">,</span>
    referer string<span class="token punctuation">,</span>
    ip string<span class="token punctuation">,</span>
    end_user_id string<span class="token punctuation">,</span>
    city_id string
<span class="token punctuation">)</span>
<span class="token keyword">ROW</span> FORMAT DELIMITED <span class="token keyword">FIELDS</span> <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> <span class="token string">'\t'</span>
stored <span class="token keyword">as</span> parquet<span class="token punctuation">;</span>

<span class="token comment">-- 加载数据</span>
<span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> log_orc_parquet <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> log_text<span class="token punctuation">;</span>
<span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">as</span> cnt <span class="token keyword">from</span> log_orc_parquet<span class="token punctuation">;</span>
</code></pre> 
<blockquote> 
 <p><strong>拓展dfs -du -h</strong></p> 
 <pre><code class="prism language-properties">-- 查看hdfs文件大小除了去页面查看,还可以通过命令
dfs -du -h '/user/hive/warehouse/hive05.db/log_text/log.data' ;
dfs -du -h '/user/hive/warehouse/hive05.db/log_orc/000000_0' ;
dfs -du -h '/user/hive/warehouse/hive05.db/log_orc_snappy/000000_0' ;
dfs -du -h '/user/hive/warehouse/hive05.db/log_parquet/000000_0' ;
</code></pre> 
</blockquote> 
<h3><a id="4fetch_213"></a>4、fetch抓取</h3> 
<p>核心点: 在执行SQL, 能不走MR, 尽量不走MR</p> 
<p>回想, 在之前执行什么样查询的SQL的时候,没有走MR呢?</p> 
<pre><code class="prism language-properties">1) 全表扫描
2) 查询某个列数据
3) 执行一些简单查询操作
4) 执行limit操作
</code></pre> 
<p>而这些操作, 没有走MR原因, 就是hive默认以及开启本地抓取的策略方案:</p> 
<pre><code class="prism language-properties">hive.fetch.task.conversion:   设置本地抓取策略
可选:
	more (默认值): 可以保证在执行全表扫描、查询某几个列、简单的limit查询、简单的条件过滤不会变成MapReduce
	minimal: 可以保证在执行全表扫描、查询某几个列、简单的limit查询不会变成MapReduce
	none: 全部的HiveSQL语句都要变成MapReduce
</code></pre> 
<p>示例:</p> 
<pre><code class="prism language-sql"><span class="token comment">-- 不会变成MapReduce几类SQL演示</span>
<span class="token comment">-- 默认是more,底层自动调优</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">fetch</span><span class="token punctuation">.</span>task<span class="token punctuation">.</span>conversion<span class="token punctuation">;</span>  <span class="token comment">-- 默认值more</span>
<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> log_text<span class="token punctuation">;</span> <span class="token comment">-- 全表扫描</span>
<span class="token keyword">select</span> url <span class="token keyword">from</span> log_text<span class="token punctuation">;</span> <span class="token comment">-- 查询某个列的数据</span>
<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> log_text <span class="token keyword">where</span> city_id<span class="token operator">=</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">;</span> <span class="token comment">-- 简单的where过滤查询</span>
<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> log_text <span class="token keyword">limit</span> <span class="token number">10</span><span class="token punctuation">;</span> <span class="token comment">-- 执行limit程序</span>

<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> log_text <span class="token keyword">order</span> <span class="token keyword">by</span> url <span class="token keyword">desc</span> <span class="token keyword">limit</span> <span class="token number">100</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">;</span> <span class="token comment">-- 复杂limit会变成MapReduce</span>

<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">fetch</span><span class="token punctuation">.</span>task<span class="token punctuation">.</span>conversion<span class="token operator">=</span>minimal<span class="token punctuation">;</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">fetch</span><span class="token punctuation">.</span>task<span class="token punctuation">.</span>conversion<span class="token punctuation">;</span>
<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> log_text<span class="token punctuation">;</span> <span class="token comment">-- 全表扫描</span>
<span class="token keyword">select</span> url <span class="token keyword">from</span> log_text<span class="token punctuation">;</span> <span class="token comment">-- 查询某个列的数据</span>
<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> log_text <span class="token keyword">limit</span> <span class="token number">10</span><span class="token punctuation">;</span> <span class="token comment">-- 执行limit程序</span>
<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> log_text <span class="token keyword">where</span> city_id<span class="token operator">=</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">;</span> <span class="token comment">-- 简单的where过滤查询</span>


<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">fetch</span><span class="token punctuation">.</span>task<span class="token punctuation">.</span>conversion<span class="token operator">=</span>none<span class="token punctuation">;</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">fetch</span><span class="token punctuation">.</span>task<span class="token punctuation">.</span>conversion<span class="token punctuation">;</span> <span class="token comment">-- 这里也要变成MapReduce</span>
<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> log_text<span class="token punctuation">;</span> <span class="token comment">-- 全表扫描</span>
<span class="token keyword">select</span> url <span class="token keyword">from</span> log_text<span class="token punctuation">;</span> <span class="token comment">-- 查询某个列的数据</span>
<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> log_text <span class="token keyword">limit</span> <span class="token number">10</span><span class="token punctuation">;</span> <span class="token comment">-- 执行limit程序</span>
<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> log_text <span class="token keyword">where</span> city_id<span class="token operator">=</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">;</span> <span class="token comment">-- 简单的where过滤查询</span>
</code></pre> 
<h3><a id="5_267"></a>5、本地模式</h3> 
<p>核心点:让MR能走本地模式, 尽量走本地MR(大白话: 小活能自己干就不要麻烦别人)</p> 
<pre><code class="prism language-properties">如何开启:
	set hive.exec.mode.local.auto=true;  默认值为: false

开启本地模式后, 在什么情况下执行本地MR:   只有当输入的数据满足以下两个特性后, 才会执行本地MR
	set hive.exec.mode.local.auto.inputbytes.max=51234560;    
		默认为128M。设置本地MR最大处理的数据量
	set hive.exec.mode.local.auto.input.files.max=10; 
		默认值为4。设置本地MR最大处理的文件的数量
</code></pre> 
<p>示例:</p> 
<pre><code class="prism language-sql"><span class="token comment">-- 4.开启本地mr(默认false,如果小数据任务进行调优开启,小任务能自己干就自己干)</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span><span class="token keyword">mode</span><span class="token punctuation">.</span><span class="token keyword">local</span><span class="token punctuation">.</span>auto<span class="token punctuation">;</span>  <span class="token comment">-- 默认关闭</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span><span class="token keyword">mode</span><span class="token punctuation">.</span><span class="token keyword">local</span><span class="token punctuation">.</span>auto<span class="token operator">=</span><span class="token boolean">false</span><span class="token punctuation">;</span> <span class="token comment">-- 手动关闭</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span><span class="token keyword">mode</span><span class="token punctuation">.</span><span class="token keyword">local</span><span class="token punctuation">.</span>auto<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment">-- 手动开启</span>

<span class="token comment">--设置local mr的最大输入数据量，当输入数据量小于这个值时采用local  mr的方式，默认为134217728，即128M</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span><span class="token keyword">mode</span><span class="token punctuation">.</span><span class="token keyword">local</span><span class="token punctuation">.</span>auto<span class="token punctuation">.</span>inputbytes<span class="token punctuation">.</span>max<span class="token punctuation">;</span><span class="token comment">-- 查看</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span><span class="token keyword">mode</span><span class="token punctuation">.</span><span class="token keyword">local</span><span class="token punctuation">.</span>auto<span class="token punctuation">.</span>inputbytes<span class="token punctuation">.</span>max<span class="token operator">=</span><span class="token number">134217728</span><span class="token punctuation">;</span>

<span class="token comment">--设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span><span class="token keyword">mode</span><span class="token punctuation">.</span><span class="token keyword">local</span><span class="token punctuation">.</span>auto<span class="token punctuation">.</span>input<span class="token punctuation">.</span>files<span class="token punctuation">.</span>max<span class="token punctuation">;</span><span class="token comment">-- 查看</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span><span class="token keyword">mode</span><span class="token punctuation">.</span><span class="token keyword">local</span><span class="token punctuation">.</span>auto<span class="token punctuation">.</span>input<span class="token punctuation">.</span>files<span class="token punctuation">.</span>max<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">;</span>

<span class="token comment">-- 执行sql语句</span>
<span class="token comment">-- 没有开启本地执行24秒,开启后1.5秒</span>
<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> log_text <span class="token keyword">order</span> <span class="token keyword">by</span> city_id <span class="token keyword">desc</span><span class="token punctuation">;</span>

<span class="token comment">-- 注意: 有的同学开启本地模式后执行上述sql语句,会报code 2...错误</span>
<span class="token comment">-- 错误:[08S01][2] Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask</span>
<span class="token comment">-- 如何解决? /export/server/hive/hive-env.sh  修改 export HADOOP_HEAPSIZE=2048</span>
<span class="token comment">-- 注意: 修改完后需要重启hive服务</span>
</code></pre> 
<h3><a id="6join_310"></a>6、join的优化操作</h3> 
<p>思考: 在通过hive执行多表查询的SQL, 最终hive翻译的MR,是如何完成join的工作的呢?</p> 
<p><strong>默认的join行为, 基于reduce端的join工作</strong></p> 
<p>思考, 请问上述join存在哪些问题?</p> 
<pre><code class="prism language-properties">1) 导致reduce的压力剧增, 所有的数据全部都打向reduce中
2) 当有了多个reduce后, 如果某个join字段的值出现大量的重复, 会导致大量key发往同一个reduce, 从而导致数据倾斜
</code></pre> 
<p><strong>那么如何解决reduce端join遇到问题? 可以通过底层map 端 join实现,还可以sql语句join之前提前过滤数据或者转换数据实现</strong></p> 
<pre><code class="prism language-properties">通过 map join 即可解决掉 reduce join所出现的所有的问题, 也可以这么 mapjoin有解决数据倾斜的作用

存在什么弊端: 
	小表数据需要存储在内存中, 随着mapTask越多, 存储在内存的小表数据份数也会越多
		当这个小表数据比较大的, 可能无法放置到内存中

所以说, mapJoin有一定使用范围: 仅适用于小表 和大表 进行join的情况
</code></pre> 
<ul><li> 
  <ol><li>大表和小表进行join:</li></ol> 
  <ul><li> <p>解决方案: map join</p> </li><li> <p>如何开启这种操作呢?</p> <pre><code class="prism language-properties">set hive.auto.convert.join = true; -- 默认为true  开启mapJoin支持 
set hive.mapjoin.smalltable.filesize= 25000000;  设置 小表的最大的数据量  23.84m
</code></pre> </li><li> <p>在执行SQL, 应该将小表放置前面呢, 还是大表放置在前面呢,还是都可以呢 ?</p> <pre><code class="prism language-properties">(hive1) :  要求 必须将小表在前大表在后 ,只有这样才可能走Map Join
(hive2):  无所谓, 谁在前, 谁在后, 没有任何的影响, hive会自动判断
</code></pre> </li></ul> </li><li> 
  <ol start="3"><li>大表和大表join</li></ol> 
  <ul><li> <p>解决方案:</p> <pre><code class="prism language-properties">1) 能在join之前过滤操作, 一定要在join前过滤, 以减少join的数据量, 从而提升效率
2) 如果join字段上, 有很多的空值null值,获取其他无效数据, 这些值越多 就会导致出现数据倾斜
	方案一: 提前过滤掉
		select * from (select * from 表1 where 字段名 is not null) 别名1 join (select * from 表2 where 			字段名 is not null) 别名2 on 关联条件;
	方案二: 将null值替换为随机数, 从而减少数据倾斜影响
		select * from (select case when 字段名 is null then rand() else 字段名 end  from 表1) 别名1 join 			(select case when 字段名 is null then rand() else 字段名 end  from 表2) 别名2 on 关联条件;
3) 基于分桶表(大文件分为多个小文件)
</code></pre> </li></ul> </li></ul> 
<h3><a id="7_369"></a>7、列裁剪</h3> 
<p>Hive在读数据的时候，可以只读取查询中所需要用到的列，而忽略其他列</p> 
<p>例如:</p> 
<pre><code class="prism language-properties">假设有一个表A: a b c d e   5个字段, 请查看以下SQL
select  a,b from A where a=xxx;

在这条SQL, 发现没有使用c d e 字段, 在from A表时候, 读取数据, 只需要将a列 和 b列数据读取出来即可, 不需要读取cde列字段, 这样可以减少读取的数据量, 从而提升效率
</code></pre> 
<p>如何配置呢?</p> 
<pre><code class="prism language-sql"><span class="token comment">-- -=【列裁剪(只读取sql语句需要的字段,节省读取开销,提升效率)</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>cp<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>  <span class="token comment">-- 默认就是true  (在hive 2.x中无需在配置了, 直接为固定值: true)</span>
</code></pre> 
<h3><a id="8_391"></a>8、分区裁剪</h3> 
<p>执行查询SQL的时候, 能在join之前提前进行条件过滤的操作, 一定要提前过滤, 不要在join后进行过滤操作</p> 
<p>如果操作的表是一张分区表, 那么建议一定要带上分区字段, 以减少扫描的数据量, 从而提升效率,</p> 
<p>例如:</p> 
<pre><code class="prism language-sql"><span class="token keyword">select</span>  <span class="token operator">*</span> <span class="token keyword">from</span> A <span class="token keyword">join</span> B <span class="token keyword">where</span> A<span class="token punctuation">.</span>id<span class="token operator">=</span>xxx<span class="token punctuation">;</span>
优化后:
<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> A <span class="token keyword">where</span> id<span class="token operator">=</span> xxx<span class="token punctuation">)</span> A <span class="token keyword">join</span> B<span class="token punctuation">;</span>
</code></pre> 
<p>如何配置呢?</p> 
<pre><code class="prism language-sql"><span class="token comment">-- 7.分区裁剪</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>pruner<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment">--默认为就是true (在hive 2.x中无需在配置了, 直接为固定值: true)</span>
</code></pre> 
<h3><a id="9group_by__414"></a>9、group by 操作</h3> 
<pre><code class="prism language-properties">方案一:  
    （1）是否在Map端进行聚合，默认为True
    set hive.map.aggr = true;
    （2）在Map端进行聚合操作的条目数目
    set hive.groupby.mapaggr.checkinterval = 100000;

方案二:  官方称为 负载均衡
    （3）有数据倾斜的时候进行负载均衡（默认是false）
    set hive.groupby.skewindata = true;
    第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；
    第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。

</code></pre> 
<h3><a id="10countdistinct_433"></a>10、count(distinct)</h3> 
<p>说明 : count(distinct) 在数据量比较大的情况下, 效率并不高</p> 
<p>思考: 你知道是为什么吗?</p> 
<pre><code class="prism language-properties">原因如下: 
	请问1: 执行count操作的时候, hive翻译的MR, reduce数量是否可以有多个? 必然不会有多个, 只能有一个, 因为全局求最终结果
	此时如果执行统计的时候, 需要进行去重,那么去重工作是由reduce执行去重操作,  由于reduce只有一个, 所有的数据都在一个reduce中, 此时reduce的压力比较大
		
	希望执行去重工作可能有多个reduce一起来执行操作, 此时可以将SQL优化: 
		原有:
			select count(distinct ip) from ip_tab;
		优化: 
			select 
			    count(ip)
			from
			   (select ip from ip_tab group by ip) tmp;
	
		请注意: 这样的做法, 虽然会运行两个MR, 但是当数据量足够庞大的时候, 此操作绝对是值得的, 如果数据量比较少, 此操作效率更低

</code></pre> 
<pre><code class="prism language-sql"><span class="token comment">-- count(distinct)优化</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>countdistinct<span class="token punctuation">;</span> <span class="token comment">-- 默认就是true</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>countdistinct <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token comment">/*
SELECT count(DISTINCT id) FROM 大表;
结果：
SELECT count(id) FROM (SELECT id FROM 大表 GROUP BY id) a;
*/</span>
<span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token keyword">distinct</span> devicetype<span class="token punctuation">)</span> <span class="token keyword">from</span> device1<span class="token punctuation">;</span>
<span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span>devicetype<span class="token punctuation">)</span> <span class="token keyword">from</span> <span class="token punctuation">(</span><span class="token keyword">select</span> devicetype <span class="token keyword">from</span> device1 <span class="token keyword">group</span> <span class="token keyword">by</span> devicetype<span class="token punctuation">)</span> a<span class="token punctuation">;</span>
<span class="token comment">-- 注意: 小表拆分两个mr反而效率低,以后大表的时候才会真正提升效率</span>
</code></pre> 
<h3><a id="11_473"></a>11、笛卡尔积</h3> 
<p>什么是笛卡尔积呢? 在进行join的时候, 两个表乘积之后结果就是笛卡尔积的结果</p> 
<p>比如: 一个表有5条, 一个表有3条数据, 笛卡尔积结果就有15条数据 , 笛卡尔积中有大量数据都是无用数据</p> 
<p>什么时候会产生笛卡尔积呢?</p> 
<p>在多表join的时候, 关联条件缺少或者使用错误的关联条件以及将关联条件放置在where中都会导致笛卡尔积</p> 
<h3><a id="12_492"></a>12、动态分区</h3> 
<p>需求: 请将下面的一个分区表数据, 拷贝到另一个分区表, 保证对应区数据放置到另一个表的对应区下</p> 
<p>如何配置呢?</p> 
<pre><code class="prism language-properties">作用:  帮助一次性灌入多个分区的数据
参数: 
	set hive.exec.dynamic.partition.mode=nonstrict;  -- 开启非严格模式 默认为 strict(严格模式)
	set hive.exec.dynamic.partition=true;  -- 开启动态分区支持, 默认就是true
	
可选的参数:
	set  hive.exec.max.dynamic.partitions=1000; -- 在所有执行MR的节点上，最大一共可以创建多少个动态分区。
	set hive.exec.max.dynamic.partitions.pernode=100; -- 每个执行MR的节点上，最大可以创建多少个动态分区
	set hive.exec.max.created.files=100000; -- 整个MR Job中，最大可以创建多少个HDFS文件
</code></pre> 
<h3><a id="13mapreduce_513"></a>13、如何调整map和reduce的数量</h3> 
<blockquote> 
 <p>1&gt;<strong>是不是map数越多越好？</strong><br> 答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。</p> 
 <p>2&gt;<strong>是不是保证每个map处理接近128m的文件块，就高枕无忧了？</strong><br> 答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。</p> 
 <p>3&gt;<strong>是不是reduce数越多越好？</strong><br> 答案是否定的。如果reduce设置的过大，对整个作业会产生一定的影响。<br> ①过多的启动和初始化reduce也会消耗时间和资源；<br> ②另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</p> 
 <p>4&gt;<strong>在什么情况下, 只能有一个reduce呢?</strong></p> 
 <p>以下几种, 不管如何设置, 最终翻译后reduce只能有一个<br> ​ 1) 执行order by操作<br> ​ 2) 执行不需要group by直接聚合的操作<br> ​ 3) 执行笛卡尔积</p> 
</blockquote> 
<ul><li> <p>如何调整mapTask数量:</p> <p>小文件场景:当input的文件都很小,把小文件进行合并归档,减少map数, 设置map数量:</p> <pre><code class="prism language-sql"><span class="token comment">-- 每个Map最大输入大小(这个值决定了合并后文件的数量)</span>
    <span class="token keyword">set</span> mapred<span class="token punctuation">.</span>max<span class="token punctuation">.</span>split<span class="token punctuation">.</span>size<span class="token operator">=</span><span class="token number">256000000</span><span class="token punctuation">;</span>
<span class="token comment">-- 一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)</span>
    <span class="token keyword">set</span> mapred<span class="token punctuation">.</span>min<span class="token punctuation">.</span>split<span class="token punctuation">.</span>size<span class="token punctuation">.</span>per<span class="token punctuation">.</span>node<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span>
<span class="token comment">-- 一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并)</span>
    <span class="token keyword">set</span> mapred<span class="token punctuation">.</span>min<span class="token punctuation">.</span>split<span class="token punctuation">.</span>size<span class="token punctuation">.</span>per<span class="token punctuation">.</span>rack<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span>
<span class="token comment">-- 执行Map前进行小文件合并默认CombineHiveInputFormat</span>
    <span class="token keyword">set</span> hive<span class="token punctuation">.</span>input<span class="token punctuation">.</span>format<span class="token operator">=</span>org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>io<span class="token punctuation">.</span>CombineHiveInputFormat<span class="token punctuation">;</span>
</code></pre> <p>大文件场景:当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。</p> 
  <blockquote> 
   <p>举例:如果表a只有一个文件，大小为120M，但包含几千万的记录，如果用1个map去完成这个任务，肯定是比较耗时的，<br> ​ 这种情况下，我们要考虑将这一个文件合理的拆分成多个，这样就可以用多个map任务去完成。<br> ​ set mapred.reduce.tasks=10;<br> ​ create table a_1 as select * from tab_info distribute by rand(123);<br> ​ 这样会将a表的记录，随机的分散到包含10个文件的a_1表中，再用a_1代替上面sql中的a表，则会用10个map任务去完成。</p> 
  </blockquote> </li><li> <p>如何reduce的数量:</p> <pre><code class="prism language-sql"><span class="token comment">-- 查看reduces数量</span>
<span class="token comment">-- 该值默认为-1，由hive自己根据任务情况进行判断。</span>
<span class="token keyword">set</span> mapred<span class="token punctuation">.</span>reduce<span class="token punctuation">.</span>tasks<span class="token punctuation">;</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>reduces<span class="token punctuation">;</span>
<span class="token comment">-- （1）每个Reduce处理的数据量默认是256MB左右</span>
<span class="token keyword">set</span>	hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>reducers<span class="token punctuation">.</span>bytes<span class="token punctuation">.</span>per<span class="token punctuation">.</span>reducer<span class="token operator">=</span><span class="token number">256000000</span><span class="token punctuation">;</span>
<span class="token comment">-- （2）每个任务最大的reduce数，默认为1009;</span>
<span class="token keyword">set</span>	hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>reducers<span class="token punctuation">.</span>max<span class="token operator">=</span><span class="token number">1009</span><span class="token punctuation">;</span>
</code></pre> </li></ul> 
<h3><a id="14_572"></a>14、并行执行</h3> 
<p>​ 在执行一个SQL语句的时候, SQL会被翻译为MR, 一个SQL有可能被翻译成多个MR, 那么在多个MR之间, 有些MR之间可能不存在任何的关联, 此时可以设置让这些没有关联的MR 并行执行, 从而提升效率 , 默认是 一个一个来</p> 
<p>如何配置:</p> 
<pre><code class="prism language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>parallel<span class="token operator">=</span><span class="token boolean">false</span><span class="token punctuation">;</span>              <span class="token comment">--打开任务并行执行,默认关闭</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>thread<span class="token punctuation">.</span>number<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">;</span>  <span class="token comment">--同一个sql允许最大并行度，默认为8。</span>

前提:
	服务器必须有资源<span class="token punctuation">,</span> 如果没有 即使支持并行<span class="token punctuation">,</span> 也没有任何作用
</code></pre> 
<p>案例:</p> 
<pre><code class="prism language-SQL">select  * from A ....
union all
select * from B ...;

例如:
	select from (select * from A group by ...) tmp1 join (select * from B group by xxx) on ...
</code></pre> 
<h3><a id="15_599"></a>15、严格模式</h3> 
<p>​ hive提供一种严格模式, 主要目的, 是为了限制一些 效率极低的SQL 放置其执行时间过长, 影响其他的操作</p> 
<pre><code class="prism language-properties">屏蔽一下操作:
1) 执行order by 不加 limit
2) 出现笛卡尔积的现象SQL
3) 查询分区表, 不带分区字段

前提: 数据量足够大, 如果数据量比较少, 严格模式对此三项内容不生效
</code></pre> 
<p>如何配置:</p> 
<pre><code class="prism language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span>mapred<span class="token punctuation">.</span><span class="token keyword">mode</span> <span class="token operator">=</span> strict<span class="token punctuation">;</span>  <span class="token comment">--开启严格模式 </span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span>mapred<span class="token punctuation">.</span><span class="token keyword">mode</span> <span class="token operator">=</span> nostrict<span class="token punctuation">;</span> <span class="token comment">--开启非严格模式   最新默认</span>
</code></pre> 
<h3><a id="16JVM__621"></a>16、JVM 重用</h3> 
<p>此操作, 在hive2.x已经不需要配置了, 默认支持</p> 
<pre><code class="prism language-properties">jvm重用: 默认情况下, container资源容器  只能使用一次,不能重复使用, 开启JVM重用, 运行container容器可以被重复使用,在hive2.x已经默认支持了
</code></pre> 
<h3><a id="17_631"></a>17、推测执行</h3> 
<pre><code class="prism language-properties">Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。
hadoop中默认两个阶段都开启了推测执行机制。
hive本身也提供了配置项来控制reduce-side的推测执行：

set hive.mapred.reduce.tasks.speculative.execution=true;

关于调优推测执行机制，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大。

</code></pre> 
<h3><a id="18explain_646"></a>18、执行计划explain</h3> 
<pre><code class="prism language-properties">使用EXPLAIN关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是如何处理你的SQL语句的。帮助我们了解底层原理,hive调优,排查数据倾斜等有很有帮助 

使用示例：explain [...]  sql查询语句;

explain sql语句: 查看执行计划的基本信息

</code></pre> 
<p><img src="https://images2.imgbox.com/15/24/wBU4UrRY_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-properties">（1）stage dependencies：各个stage之间的依赖性
包含多个stage阶段,例如 Stage-1和Stage-0，Stage-1 是根stage，Stage-0 依赖 Stage-1，
（2）stage plan：各个stage的执行计划
包含两部分: map端执行计划树和reduce端执行计划树
</code></pre> 
<p>ks.speculative.execution=true;</p> 
<p>关于调优推测执行机制，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大。</p> 
<pre><code>## 18、执行计划explain

使用EXPLAIN关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是如何处理你的SQL语句的。帮助我们了解底层原理,hive调优,排查数据倾斜等有很有帮助 

使用示例：explain [...]  sql查询语句;

explain sql语句: 查看执行计划的基本信息

</code></pre> 
<p>[外链图片转存中…(img-33z4aUG4-1715785755032)]</p> 
<pre><code class="prism language-properties">（1）stage dependencies：各个stage之间的依赖性
包含多个stage阶段,例如 Stage-1和Stage-0，Stage-1 是根stage，Stage-0 依赖 Stage-1，
（2）stage plan：各个stage的执行计划
包含两部分: map端执行计划树和reduce端执行计划树
</code></pre> 
<p><img src="https://images2.imgbox.com/95/7f/3fii5nUG_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/65b8db539c2ae9587cb49ba84ded940a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【数据结构】时间、空间复杂度实例分析</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/713299e49385554a5399905123cdac06/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">除自身以外数组的乘积[中等]</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>