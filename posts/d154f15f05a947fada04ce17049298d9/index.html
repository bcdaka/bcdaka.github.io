<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>PCA（主成分分析）的理解与应用（学习笔记） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/d154f15f05a947fada04ce17049298d9/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="PCA（主成分分析）的理解与应用（学习笔记）">
  <meta property="og:description" content="PCA
主成分分析（Principal Component Analysis, PCA）是一种线性降维算法，也是一种常用的数据预处理（Pre-Processing）方法。它的目标是是用方差（Variance）来衡量数据的差异性，并将差异性较大的高维数据投影到低维空间中进行表示。绝大多数情况下，我们希望获得两个主成分因子：分别是从数据差异性最大和次大的方向提取出来的，称为PC1(Principal Component 1) 和 PC2（Principal Component 2）。
PCA的具体实现
随机选取的六名学生的数学和语文考试成绩 student_id123456scores1927095737287scores2748770929774 制作为散点图：
图中每个点代表了一个学生，X轴代表语文成绩，Y轴代表数学成绩。然后分别取所有样本的X平均值和Y平均值，并将这两个值变为X、Y坐标，在图中画出这个点（用五角星表示）:
按照图中箭头所示方向，将整个坐标系平移，使原点与五角星重叠。这样就获得了一个新的平面直角坐标系：
尽管此时坐标系和每个点的值都发生了变化，点与点之间的相对位置仍保持一致。找到这些点的最优拟合线（Line of Best Fit），也就找到了PC1，再通过原点做PC1的垂线，就找到PC2：
处理三维数组时便会产生第三个因子（PC3），以此类推，数据的维度越大，因子的数量也就越多。当维度大于等于4的时候，我们是无法想象出图像的，但PC4确实存在；假设有x个维度，便可以做x-1条垂线，就能得到PCx。接下来要做的便是选取最能代表数据差异性的两个因子，作为PC1和PC2。
按照下图所示，将点A投影到PC1上（六角星的位置），并计算其与原点之间的距离称为d1：
其余的五个点也做同样操作，得出d2至d5，再求这六个距离的平方和，称为PC1的特征值（Eigenvalue）。然后将PC1的特征值除以总样本数量减一（n-1），就计算出了PC1的差异值（Variation）。 以此类推，并选择差异值最大的两个因子作为PC1 和 PC2。假设在某个三维数组中，获得了PC1、PC2和PC3的差异值分别为18，7，5。通过计算（18&#43;7）/ (18&#43;7&#43;5) ≈ 83.3% 得到结论：PC1 和 PC2 代表了这个三维数组83.3%的差异性。在本次分析的13个因子中，PC1和PC2描述了整组数据约81%的差异性： 最后，再通过选中的PC1和PC2将样本映射回本身所在的坐标，就可以得到降维后的图像（PCA Plot）。
协方差矩阵基本知识点： 矩阵中的数据按行排列和按列排列求出的协方差矩阵是不同的，这里默认数据是按行排列。即每一行是一个observation（样本），那么每一列就是一个随机变量（特征）。
举个例子，矩阵X按行排列：
1.求每个维度的平均值
2.将X的每一列减去平均值
3.计算协方差矩阵
矩阵特征值和特征向量计算方法：
计算A的特征值和特征向量 计算行列式得
化简得：
得到特征值：
化简得：
令得到特征矩阵：
同理，当得：
令得到特征矩阵：
代码实现（依次由繁到简实现效果）
方法一：
# -*- coding: utf-8 -*- &#34;&#34;&#34; @author: 绯雨千叶 用PCA求样本矩阵X的K阶降维矩阵Z 请保证输入的样本矩阵X shape=(m, n)，m行样例，n个特征 &#34;&#34;&#34; import numpy as np class PCA(object): def __init__(self, X, K): self.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-02-20T19:05:03+08:00">
    <meta property="article:modified_time" content="2023-02-20T19:05:03+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PCA（主成分分析）的理解与应用（学习笔记）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><strong>PCA</strong></p> 
<p>        主成分分析（Principal Component Analysis, PCA）是一种线性降维算法，也是一种常用的数据预处理（Pre-Processing）方法。它的目标是是用方差（Variance）来衡量数据的差异性，并将差异性较大的高维数据投影到低维空间中进行表示。绝大多数情况下，我们希望获得两个主成分因子：分别是从数据差异性最大和次大的方向提取出来的，称为PC1(Principal Component 1) 和 PC2（Principal Component 2）。</p> 
<hr> 
<p><strong>PCA的具体实现</strong></p> 
<table align="center" border="1" cellpadding="1" cellspacing="1" style="width:500px;"><caption>
   随机选取的六名学生的数学和语文考试成绩 
 </caption><tbody><tr><td>student_id</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td></tr><tr><td>scores1</td><td>92</td><td>70</td><td>95</td><td>73</td><td>72</td><td>87</td></tr><tr><td>scores2</td><td>74</td><td>87</td><td>70</td><td>92</td><td>97</td><td>74</td></tr></tbody></table> 
<p><strong>制作为散点图：</strong></p> 
<p><img alt="" height="326" src="https://images2.imgbox.com/a8/ba/rlSACeDn_o.png" width="536"></p> 
<p>         图中每个点代表了一个学生，X轴代表语文成绩，Y轴代表数学成绩。然后分别取所有样本的X平均值和Y平均值，并将这两个值变为X、Y坐标，在图中画出这个点（用五角星表示）:</p> 
<p><img alt="" height="329" src="https://images2.imgbox.com/62/de/iOPVTDqd_o.png" width="542"></p> 
<p>         按照图中箭头所示方向，<strong>将整个坐标系平移，使原点与五角星重叠</strong>。这样就获得了一个新的平面直角坐标系：</p> 
<p><img alt="" height="332" src="https://images2.imgbox.com/a8/5f/EWoHzig0_o.png" width="550"></p> 
<p>         <strong>尽管此时坐标系和每个点的值都发生了变化，点与点之间的相对位置仍保持一致</strong>。找到这些点的最优拟合线（Line of Best Fit），也就找到了PC1，再通过原点做PC1的垂线，就找到PC2：</p> 
<p><img alt="" height="333" src="https://images2.imgbox.com/59/ae/x8bHdVF3_o.png" width="562"></p> 
<p>         处理三维数组时便会产生第三个因子（PC3），以此类推，数据的维度越大，因子的数量也就越多。当维度大于等于4的时候，我们是无法想象出图像的，但PC4确实存在；假设有x个维度，便可以做x-1条垂线，就能得到PCx。接下来要做的便是选取最能代表数据差异性的两个因子，作为PC1和PC2。</p> 
<p>按照下图所示，<strong>将点A投影到PC1上（六角星的位置），并计算其与原点之间的距离称为d1</strong>：</p> 
<p><img alt="" height="329" src="https://images2.imgbox.com/0b/1a/qHIzcyc1_o.png" width="566"></p> 
<p>        其余的五个点也做同样操作，得出d2至d5，再求这六个距离的平方和，称为PC1的<strong>特征值（Eigenvalue）。</strong>然后将PC1的特征值除以总样本数量减一（n-1），就计算出了PC1的<strong>差异值（Variation）。</strong> </p> 
<p><img alt="" height="106" src="https://images2.imgbox.com/fc/1c/7seLcyFs_o.png" width="519"></p> 
<p>        以此类推，并<strong>选择差异值最大的两个因子作为PC1 和 PC2</strong>。假设在某个三维数组中，获得了PC1、PC2和PC3的差异值分别为18，7，5。通过计算（18+7）/ (18+7+5) ≈ 83.3% 得到结论：PC1 和 PC2 代表了这个三维数组83.3%的差异性。<strong>在本次分析的13个因子中，PC1和PC2描述了整组数据约81%的差异性</strong>： </p> 
<p><img alt="" height="448" src="https://images2.imgbox.com/dc/ec/pChaJ3fn_o.png" width="575"></p> 
<p>         最后，再通过选中的PC1和PC2<strong>将样本映射回本身所在的坐标，就可以得到降维后的图像（PCA Plot）。</strong></p> 
<p><img alt="" height="374" src="https://images2.imgbox.com/77/7b/QwgWpTz6_o.png" width="588"></p> 
<hr> 
<p><strong>协方差矩阵基本知识点： </strong></p> 
<p>矩阵中的数据按行排列和按列排列求出的协方差矩阵是不同的，这里默认数据是按行排列。即每一行是一个observation（样本），那么每一列就是一个随机变量（特征）。</p> 
<p>举个例子，矩阵X按行排列：</p> 
<p><img alt="" height="88" src="https://images2.imgbox.com/e4/30/v4Ex4y9r_o.png" width="402"></p> 
<p><strong>1.求每个维度的平均值</strong></p> 
<p><img alt="" height="66" src="https://images2.imgbox.com/7a/2d/WHxttC4d_o.png" width="410"></p> 
<p><strong>2.将X的每一列减去平均值</strong></p> 
<p> <img alt="" height="89" src="https://images2.imgbox.com/3b/07/WcK5xDIj_o.png" width="310"></p> 
<p><strong>3.计算协方差矩阵</strong></p> 
<p><img alt="" height="115" src="https://images2.imgbox.com/11/99/7avj8BCr_o.png" width="479"></p> 
<hr> 
<p><strong> 矩阵特征值和特征向量计算方法：</strong></p> 
<p><img alt="" height="184" src="https://images2.imgbox.com/cc/03/i7ZX95gF_o.png" width="331"></p> 
<p><strong>计算A的特征值和特征向量 </strong></p> 
<p><img alt="" src="https://images2.imgbox.com/aa/5d/TxdCmN9m_o.png"></p> 
<p>计算行列式得</p> 
<p><img alt="" height="24" src="https://images2.imgbox.com/90/f2/Q8PL22wY_o.png" width="696"></p> 
<p>化简得：</p> 
<p><img alt="" height="31" src="https://images2.imgbox.com/69/d5/c6DVqfGn_o.png" width="119"></p> 
<p>得到特征值：</p> 
<p><img alt="" height="40" src="https://images2.imgbox.com/1c/c9/1xmcxij0_o.png" width="203"><img alt="" height="41" src="https://images2.imgbox.com/ed/bc/evRatLlI_o.png" width="262"></p> 
<p><img alt="" height="116" src="https://images2.imgbox.com/27/2d/3crdtzJX_o.png" width="158"><img alt="" height="109" src="https://images2.imgbox.com/a5/aa/IFLzu3fh_o.png" width="267">化简得：<img alt="" height="119" src="https://images2.imgbox.com/a2/3e/cB0vrw2t_o.png" width="128"></p> 
<p></p> 
<p><img alt="" height="105" src="https://images2.imgbox.com/4a/dd/gBNyFkIT_o.png" width="323"><img alt="" height="97" src="https://images2.imgbox.com/c8/d9/T92KVBOK_o.png" width="154"></p> 
<p>令<img alt="" height="29" src="https://images2.imgbox.com/84/3d/RL7VfYJJ_o.png" width="59">得到特征矩阵：<img alt="" height="118" src="https://images2.imgbox.com/aa/e9/eGKo4uyy_o.png" width="102"></p> 
<p>同理，当<img alt="" height="29" src="https://images2.imgbox.com/26/83/P3BTWEhf_o.png" width="115">得：</p> 
<p><img alt="" height="104" src="https://images2.imgbox.com/d9/73/psxDaha2_o.png" width="317"><img alt="" height="94" src="https://images2.imgbox.com/98/6d/H8nwAXDZ_o.png" width="195"></p> 
<p>令<img alt="" src="https://images2.imgbox.com/e8/a6/6FJUNifw_o.png">得到特征矩阵：<img alt="" height="122" src="https://images2.imgbox.com/80/31/FWhMISzH_o.png" width="163"></p> 
<hr> 
<p><strong> 代码实现（依次由繁到简实现效果）</strong></p> 
<p><strong>方法一：</strong></p> 
<pre><code class="language-python"># -*- coding: utf-8 -*-
"""
@author: 绯雨千叶

用PCA求样本矩阵X的K阶降维矩阵Z
请保证输入的样本矩阵X shape=(m, n)，m行样例，n个特征
"""
import numpy as np

class PCA(object):
    def __init__(self, X, K):
        self.X = X  # 训练样本矩阵X
        self.K = K  # X的降维矩阵的阶数，即X要特征降维成k阶
        self.centrX = []  # 矩阵X的中心化
        self.C = []  # 样本集的协方差矩阵C
        self.U = []  # 样本矩阵X的降维转换矩阵
        self.Z = []  # 样本矩阵X的降维矩阵Z

        self.centrX = self._centralized()
        self.C = self._conv()
        self.U = self._U()
        self.Z = self._Z()  # Z=XU求得

    def _centralized(self):  # 矩阵X的中心化
        print('样本矩阵X:\n', self.X)
        centrX = []
        mean = np.array([np.mean(attr) for attr in self.X.T])  # mean()函数功能：求取均值;样本集的特征均值（每一列的平均数）
        centrX = self.X - mean  # 样本集的中心化(减去他那行的特征均值)
        print('样本集的特征均值:\n', mean)
        print('样本矩阵X的中心化centrX:\n', centrX)
        return centrX

    def _conv(self):  # 求样本矩阵X的协方差矩阵C
        ns = np.shape(self.centrX)[0]  # 样本集的样例总数
        C = np.dot(self.centrX.T, self.centrX) / (ns - 1)  # 样本矩阵的协方差矩阵C;.dot向量点积和矩阵乘法
        print('样本矩阵X的协方差矩阵C:\n', C)
        return C

    def _U(self):  # 求X的降维转换矩阵U, shape=(n,k), n是X的特征维度总数，k是降维矩阵的特征维度
        a, b = np.linalg.eig(self.C)  # 第一个返回值是X的协方差矩阵C的特征值，第二个返回值是特征向量
        print('样本集的协方差矩阵C的特征值:\n', a)
        print('样本集的协方差矩阵C的特征向量:\n', b)
        ind = np.argsort(-1 * a)  # 给出特征值降序的topK的索引序列
        UT = [b[:, ind[i]] for i in range(self.K)]  # 构建K阶降维的降维转换矩阵U
        U = np.transpose(UT)
        print('%d阶降维转换矩阵U:\n' % self.K, U)
        return U

    def _Z(self):  # 按照Z=XU求降维矩阵Z, shape=(m,k), n是样本总数，k是降维矩阵中特征维度总数
        Z = np.dot(self.X, self.U)
        print('X shape:', np.shape(self.X))
        print('U shape:', np.shape(self.U))
        print('Z shape:', np.shape(Z))
        print('样本矩阵X的降维矩阵Z:\n', Z)
        return Z

if __name__ == '__main__':
    '10样本3特征的样本集, 行为样例，列为特征维度'
    X = np.array([[10, 15, 29],
                  [15, 46, 13],
                  [23, 21, 30],
                  [11, 9, 35],
                  [42, 45, 11],
                  [9, 48, 5],
                  [11, 21, 14],
                  [8, 5, 15],
                  [11, 12, 21],
                  [21, 20, 25]])
    K = np.shape(X)[1] - 1
    print('样本集(10行3列，10个样例，每个样例3个特征):\n', X)
    pca = PCA(X, K)</code></pre> 
<p><strong>效果展示：</strong></p> 
<p><img alt="" height="303" src="https://images2.imgbox.com/38/5a/ikcu2y53_o.png" width="376"><img alt="" height="303" src="https://images2.imgbox.com/2d/80/63qFBgYJ_o.png" width="141"><img alt="" height="356" src="https://images2.imgbox.com/ef/e5/vOC8YEWW_o.png" width="228"><img alt="" height="374" src="https://images2.imgbox.com/a6/7b/JOfecmVv_o.png" width="452"></p> 
<p> <img alt="" height="383" src="https://images2.imgbox.com/4c/be/Enadv3Ju_o.png" width="310"></p> 
<hr> 
<p><strong>方法二：</strong></p> 
<pre><code class="language-python"># coding=utf-8
"""
@author: 绯雨千叶
"""
import numpy as np

class PCA():
    def __init__(self, n_components):
        self.n_components = n_components

    def fit_transform(self, X):
        self.n_features_ = X.shape[1]
        # 求协方差矩阵
        X = X - X.mean(axis=0)
        self.covariance = np.dot(X.T, X) / X.shape[0]
        # 求协方差矩阵的特征值和特征向量
        eig_vals, eig_vectors = np.linalg.eig(self.covariance)
        # 获得降序排列特征值的序号
        idx = np.argsort(-eig_vals)
        # 降维矩阵
        self.components_ = eig_vectors[:, idx[:self.n_components]]
        # 对X进行降维
        return np.dot(X, self.components_)

# 调用
pca = PCA(n_components=2)
X = np.array(
    [[-1, 2, 66, -1], [-2, 6, 58, -1], [-3, 8, 45, -2], [1, 9, 36, 1], [2, 10, 62, 1], [3, 5, 83, 2]])  # 导入数据，维度为4
newX = pca.fit_transform(X)
print(newX)  # 输出降维后的数据</code></pre> 
<p><strong>效果展示： </strong></p> 
<p><img alt="" height="178" src="https://images2.imgbox.com/7f/2c/eE0Vtpvn_o.png" width="304"></p> 
<hr> 
<p><strong>方法三： </strong></p> 
<pre><code class="language-python">#coding=utf-8
"""
@author: 绯雨千叶
"""
import numpy as np
from sklearn.decomposition import PCA
X = np.array([[-1,2,66,-1], [-2,6,58,-1], [-3,8,45,-2], [1,9,36,1], [2,10,62,1], [3,5,83,2]])  #导入数据，维度为4
pca = PCA(n_components=2)   #降到2维
pca.fit(X)                  #训练
newX=pca.fit_transform(X)   #降维后的数据
PCA(copy=True, n_components=2, whiten=False)
print(pca.explained_variance_ratio_)  #输出贡献率
print(newX)                  #输出降维后的数据</code></pre> 
<p><strong>效果展示：</strong></p> 
<p><img alt="" height="204" src="https://images2.imgbox.com/15/7e/6xjoNGtl_o.png" width="358"></p> 
<hr> 
<blockquote> 
 <pre><strong>sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False)</strong></pre> 
 <p><strong>参数：</strong></p> 
 <p><strong>n_components:  </strong></p> 
 <blockquote> 
  <p>意义：PCA算法中所要保留的主成分个数n，也即保留下来的特征个数n</p> 
  <p>类型：int 或者 string，缺省时默认为None，所有成分被保留。</p> 
  <p>          赋值为int，比如n_components=1，将把原始数据降到一个维度。</p> 
  <p>          赋值为string，比如n_components='mle'，将自动选取特征个数n，使得满足所要求的方差百分比。</p> 
 </blockquote> 
 <p><strong>copy:</strong></p> 
 <blockquote>
   类型：bool，True或者False，缺省时默认为True。 
  <p>意义：表示是否在运行算法时，将原始训练数据复制一份。若为True，则运行PCA算法后，原始训练数据的值不            会有任何改变，因为是在原始数据的副本上进行运算；若为False，则运行PCA算法后，原始训练数据的              值会改，因为是在原始数据上进行降维计算。</p> 
 </blockquote> 
 <p><strong>whiten:</strong></p> 
 <blockquote> 
  <p>类型：bool，缺省时默认为False</p> 
  <p>意义：白化，使得每个特征具有相同的方差。</p> 
 </blockquote> 
</blockquote> 
<h4><a name="t1"></a>PCA属性：</h4> 
<ul><li><strong>components_ </strong>：返回具有最大方差的成分。</li><li><strong>explained_variance_ratio_</strong>：返回 所保留的n个成分各自的方差百分比。</li><li><strong>n_components_</strong>：返回所保留的成分个数n。</li><li><strong>mean_</strong>：</li><li><strong>noise_variance_：</strong></li></ul> 
<h4><a name="t2"></a>PCA方法：</h4> 
<p><strong>1、fit(X,y=None)</strong></p> 
<p>fit(X)，表示用数据X来<strong>训练</strong>PCA模型。</p> 
<p>函数返回值：调用fit方法的对象本身。比如pca.fit(X)，表示用X对pca这个对象进行训练。</p> 
<p><u><em>拓展：fit()可以说是scikit-learn中通用的方法，每个需要训练的算法都会有fit()方法，它其实就是算法中的“训练”这一步骤。因为PCA是无监督学习算法，此处y自然等于None。</em></u></p> 
<p><strong>2、fit_transform(X)</strong></p> 
<p>用X来<strong>训练</strong>PCA模型，同时返回<strong>降维</strong>后的数据。</p> 
<p>newX=pca.fit_transform(X)，newX就是降维后的数据。</p> 
<p><strong>3、inverse_transform()</strong></p> 
<p>将降维后的数据转换成原始数据，X=pca.inverse_transform(newX)</p> 
<p><strong>4、transform(X)</strong></p> 
<p>将数据X转换成<strong>降维</strong>后的数据。当模型训练好后，对于新输入的数据，都可以用transform方法来降维。</p> 
<p></p> 
<p>此外，还有get_covariance()、get_precision()、get_params(deep=True)、score(X, y=None)等方法，以后用到再补充吧。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4f0107422918afbf88e47112a7c38189/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">基于html&#43;js实现轮播图（自动轮播、左右按钮、小圆点点击及切换图片）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f9a7347cb3271ae951949b1c73e45f9a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【python绘图（一）】Python数据分析和可视化</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>