<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Stable Diffusion Lora模型训练详细教程 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/bad5efe9e26af706f424f4921d5a372c/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Stable Diffusion Lora模型训练详细教程">
  <meta property="og:description" content="1. 介绍 通过Lora小模型可以控制很多特定场景的内容生成。
但是那些模型是别人训练好的，你肯定很好奇，我也想训练一个自己的专属模型（也叫炼丹～_～）。
甚至可以训练一个专属家庭版的模型（family model），非常有意思。
将自己的训练好的Lora模型放到stableDiffusion lora 目录中，同时配上美丽的封面图。
2. 模型训练步骤 2.1 训练环境搭建 WebUI或者Diffuser
https://github.com/AUTOMATIC1111/stable-diffusion-webui
Lora训练环境
https://github.com/kohya-ss/sd-scripts
2.2 数据准备 从网上爬取一些想要的角色图片，或者直接去截图。
这次我做的初春，找一些图片就行了，不需要很多，20张就可以，各个角度，全身，大头尽量都有些。
角色是这样子的，头上一定得有花，这个是角色属性。
这些是我找的图：
2.3 数据清洗打标 得到数据后，第一步就是清洗，所谓清洗主要是把爬的垃圾数据删了，并且该抠图的抠图（角色的Lora其实不扣也不太影响，真人是尽力抠图，功能性的lora基本上是要手动p图的）。
这里我给大家一个抠图脚本，用的阿里云API，非常方便：
链接：https://pan.baidu.com/s/1PdF2ocgqOBtRmQqtmij6RA?pwd=bjf4 提取码：bjf4 总之清晰后数据就是干净的，最好是扣过的图，只有人物主体。
然后我们就要打标了，如果是真人或者风格类lora，可以学youtube里那些人做法，直接用BLIP做image caption，然后手动修改一些。但是二次元强烈建议直接上deepbooru，这是因为二次元SD的base model源头是NovelAI泄露的，而当时的模型就是这个风格的标签，所以二次元特别适合用booru风格的描述。具体操作如下：
首先打开SD webui，找到如下地方：
至于说分辨率，其实512就可以了，可以调大一些，如果你显存够大，我用3090发现基本上拉到1024分辨率后就没有收益了，而低于512明显效果不好。理论上图片分辨率高一些好，此外图片质量解析力也应该高。
打完勾后，就可以process了。
当程序自动走完流程我们就在目标文件夹里得到一组由image和text文件组成的对：
其中txt文件里就是对一幅图的描述，如下：
1girl, blurry, solo, depth_of_field, hairband, blurry_background, building, torii 然后就进行最重要的步骤，打标。
所谓打标就是监督学习，告诉SD我们要它学什么。这里我们就是想学一个角色，这个角色有很多特征，比如她头上有花花，短发，眼睛样子等。
因此我们要把这些角色特征的描述词，从txt文件中的描述中删除。
。。。。。。
然后我们再加入一个特定的角色名，用来表示这个角色。让这个角色名学到刚才我们删除的特征。
换句话说，如果我们不删除那些特征，模型是不会把这些特征学给这个角色名的，而是专门学给具体的描述词。比如如果我们留下来头上的花，那么头上有花这个概念就不会学给初春这个角色名。当我们生成图像时候，初春这个词是不会生成头上有花的女孩，但是初春必须头上有花！所以我们要干掉这个词，这样模型就认为初春头上必然有花。
其实就很简单，控制变量法，我们留下来的标签就是我们想让模型学习的。
但是编辑这玩意如果一个一个文件走会很烦，我这里有个简单的步骤：
首先下载一个webui的插件“
stable-diffusion-webui-dataset-tag-editor-main
（请去Github搜，然后放进Webui的Extensions目录下），然后我们可以进行批量编辑了：
这就是可以批量编辑的地方，我们进行批量remove，即批量删除我们不想要的特征词：
这些词我认为都是初春特征，所以我都打勾删除，记得删除后保存下。
等到这些删除完了后，我们在每句描述前面都加上uiharu_kazari，这个是初春的触发词。一旦我们在prompts写上这个词，初春就会出现！我认为很重要，当然有人说可以不加，其实我训练几十个lora很多方案都试过，现在我感觉也是可加可不加的，但是我喜欢加上，里面原因很难直接说，就是加上后出图效果会好一些。但是不一定，有时候会变差，我说的这个方案其实是有些晋级的了，新人建议全都不删除tags，标签出来直接都拿去train，最简单稳定。
为什么放在第一位，实际上tags是有顺序的，一般最开始的tags的权重最大，越靠后越小，所以我们要把我们最想学的这个人放在第一位，像是没有意义的概念，比如透明背景我们尽量往后放。
此外我们需要看每张图的描述，有一些描述是含有明显错误的，因为毕竟是打标器，很多都是错的。我们一定要删除错误的标签，这些错误是会明显影响我们训练结果的（当然图多时候其实不影响了，但是话又说回来，图片质量&gt;图片数量，风格、功能性Lora&gt;真人&gt;二次元，二次元是最简单的lora）。
2.4 训练 我们开始写一个训练脚本参数。
首先是Base Model概念，这个应该是最重要概念之一，就是我们是在什么模型基础上训练的。
比如我们训练真人，女性，社区目前首选Chilloutmix系列。如果是汽车之类的物体，首选可以是SD1.5 2.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-04-22T16:21:45+08:00">
    <meta property="article:modified_time" content="2023-04-22T16:21:45+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Stable Diffusion Lora模型训练详细教程</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="1__0"></a>1. 介绍</h2> 
<p>通过Lora小模型可以控制很多特定场景的内容生成。</p> 
<p>但是那些模型是别人训练好的，你肯定很好奇，我也想训练一个自己的专属模型（也叫炼丹～_～）。</p> 
<p>甚至可以训练一个专属家庭版的模型（family model），非常有意思。</p> 
<p>将自己的训练好的Lora模型放到stableDiffusion lora 目录中，同时配上美丽的封面图。<br> <img src="https://images2.imgbox.com/a6/eb/fgS1WVP6_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/23/86/kpJQnGBs_o.png" alt=""></p> 
<h2><a id="2__10"></a>2. 模型训练步骤</h2> 
<h3><a id="21__11"></a>2.1 训练环境搭建</h3> 
<ul><li> <p>WebUI或者Diffuser<br> https://github.com/AUTOMATIC1111/stable-diffusion-webui</p> </li><li> <p>Lora训练环境<br> https://github.com/kohya-ss/sd-scripts</p> </li></ul> 
<h3><a id="22__19"></a>2.2 数据准备</h3> 
<p>从网上爬取一些想要的角色图片，或者直接去截图。</p> 
<p>这次我做的初春，找一些图片就行了，不需要很多，20张就可以，各个角度，全身，大头尽量都有些。</p> 
<p>角色是这样子的，头上一定得有花，这个是角色属性。<br> <img src="https://images2.imgbox.com/f7/4b/1nVGDPEs_o.png" alt="在这里插入图片描述"><br> 这些是我找的图：<br> <img src="https://images2.imgbox.com/ec/01/SpumWM79_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="23__28"></a>2.3 数据清洗打标</h3> 
<p>得到数据后，第一步就是清洗，所谓清洗主要是把爬的垃圾数据删了，并且该抠图的抠图（角色的Lora其实不扣也不太影响，真人是尽力抠图，功能性的lora基本上是要手动p图的）。</p> 
<p>这里我给大家一个抠图脚本，用的阿里云API，非常方便：</p> 
<pre><code class="prism language-bash">链接：https://pan.baidu.com/s/1PdF2ocgqOBtRmQqtmij6RA?pwd<span class="token operator">=</span>bjf4 
提取码：bjf4 
</code></pre> 
<p>总之清晰后数据就是干净的，最好是扣过的图，只有人物主体。</p> 
<p>然后我们就要打标了，如果是真人或者风格类lora，可以学youtube里那些人做法，直接用BLIP做image caption，然后手动修改一些。但是二次元强烈建议直接上deepbooru，这是因为二次元SD的base model源头是NovelAI泄露的，而当时的模型就是这个风格的标签，所以二次元特别适合用booru风格的描述。具体操作如下：</p> 
<p>首先打开SD webui，找到如下地方：<br> <img src="https://images2.imgbox.com/c5/61/eRwhbdTD_o.png" alt="在这里插入图片描述"><br> 至于说分辨率，其实512就可以了，可以调大一些，如果你显存够大，我用3090发现基本上拉到1024分辨率后就没有收益了，而低于512明显效果不好。理论上图片分辨率高一些好，此外图片质量解析力也应该高。</p> 
<p>打完勾后，就可以process了。<br> <img src="https://images2.imgbox.com/52/82/ubvrHPky_o.png" alt="在这里插入图片描述"><br> 当程序自动走完流程我们就在目标文件夹里得到一组由image和text文件组成的对：</p> 
<p><img src="https://images2.imgbox.com/d5/5d/TIsjUkMm_o.png" alt="在这里插入图片描述"><br> 其中txt文件里就是对一幅图的描述，如下：</p> 
<pre><code class="prism language-c"><span class="token number">1</span>girl<span class="token punctuation">,</span> blurry<span class="token punctuation">,</span> solo<span class="token punctuation">,</span> depth_of_field<span class="token punctuation">,</span> hairband<span class="token punctuation">,</span> blurry_background<span class="token punctuation">,</span> building<span class="token punctuation">,</span> torii
</code></pre> 
<p>然后就进行最重要的步骤，打标。</p> 
<p>所谓打标就是监督学习，告诉SD我们要它学什么。这里我们就是想学一个角色，这个角色有很多特征，比如她头上有花花，短发，眼睛样子等。</p> 
<p>因此我们要把这些角色特征的描述词，从txt文件中的描述中删除。</p> 
<p>。。。。。。</p> 
<p>然后我们再加入一个特定的角色名，用来表示这个角色。让这个角色名学到刚才我们删除的特征。</p> 
<p>换句话说，如果我们不删除那些特征，模型是不会把这些特征学给这个角色名的，而是专门学给具体的描述词。比如如果我们留下来头上的花，那么头上有花这个概念就不会学给初春这个角色名。当我们生成图像时候，初春这个词是不会生成头上有花的女孩，但是初春必须头上有花！所以我们要干掉这个词，这样模型就认为初春头上必然有花。</p> 
<p>其实就很简单，控制变量法，我们留下来的标签就是我们想让模型学习的。</p> 
<p>但是编辑这玩意如果一个一个文件走会很烦，我这里有个简单的步骤：</p> 
<p>首先下载一个webui的插件“<br> stable-diffusion-webui-dataset-tag-editor-main<br> （请去Github搜，然后放进Webui的Extensions目录下），然后我们可以进行批量编辑了：<br> <img src="https://images2.imgbox.com/84/73/dwykihv2_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b7/81/Eazi7hWg_o.png" alt="在这里插入图片描述"><br> 这就是可以批量编辑的地方，我们进行批量remove，即批量删除我们不想要的特征词：<br> <img src="https://images2.imgbox.com/de/09/33Lp25F2_o.png" alt="在这里插入图片描述"><br> 这些词我认为都是初春特征，所以我都打勾删除，记得删除后保存下。</p> 
<p>等到这些删除完了后，我们在每句描述前面都加上uiharu_kazari，这个是初春的触发词。一旦我们在prompts写上这个词，初春就会出现！我认为很重要，当然有人说可以不加，其实我训练几十个lora很多方案都试过，现在我感觉也是可加可不加的，但是我喜欢加上，里面原因很难直接说，就是加上后出图效果会好一些。但是不一定，有时候会变差，我说的这个方案其实是有些晋级的了，新人建议全都不删除tags，标签出来直接都拿去train，最简单稳定。</p> 
<p><strong>为什么放在第一位，实际上tags是有顺序的，一般最开始的tags的权重最大，越靠后越小，所以我们要把我们最想学的这个人放在第一位，像是没有意义的概念，比如透明背景我们尽量往后放。</strong></p> 
<p>此外我们需要看每张图的描述，有一些描述是含有明显错误的，因为毕竟是打标器，很多都是错的。我们一定要删除错误的标签，这些错误是会明显影响我们训练结果的（当然图多时候其实不影响了，但是话又说回来，图片质量&gt;图片数量，风格、功能性Lora&gt;真人&gt;二次元，二次元是最简单的lora）。</p> 
<h3><a id="24__87"></a>2.4 训练</h3> 
<p>我们开始写一个训练脚本参数。</p> 
<p>首先是Base Model概念，这个应该是最重要概念之一，就是我们是在什么模型基础上训练的。</p> 
<p>比如我们训练真人，女性，社区目前首选Chilloutmix系列。如果是汽车之类的物体，首选可以是SD1.5 2.1等官方模型。如果是二次元，我们这次首选NovelAI系列的模型，我这里选的Acertain，因为它画风比较朴素，接近动漫效果，我感觉用来训练比较好。</p> 
<p>我这次直接把脚本参数都列出来了：</p> 
<pre><code class="prism language-python"><span class="token comment"># LoRA train script by @Akegarasu</span>
<span class="token comment"># Train data path | 设置训练用模型、图片</span>
$pretrained_model<span class="token operator">=</span><span class="token string">"D:\workspace\stable-diffusion-webui\models\Stable-diffusion\ACertainModel.ckpt"</span> <span class="token comment"># base model path | 底模路径</span>
$train_data_dir<span class="token operator">=</span><span class="token string">"C:\Users\NUOSEN\Desktop\data\lora\Kazari"</span>
$reg_data_dir <span class="token operator">=</span> <span class="token string">""</span> <span class="token comment"># directory for regularization images | 正则化数据集路径，默认不使用正则化图像。</span>

<span class="token comment"># Train related params | 训练相关参数</span>
$resolution <span class="token operator">=</span> <span class="token string">"768,768"</span> <span class="token comment"># image resolution w,h. 图片分辨率，宽,高。支持非正方形，但必须是 64 倍数。</span>
$batch_size <span class="token operator">=</span> <span class="token number">2</span> <span class="token comment"># batch size</span>
$max_train_epoches <span class="token operator">=</span> <span class="token number">10</span> <span class="token comment"># max train epoches | 最大训练 epoch</span>
$save_every_n_epochs <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment"># save every n epochs | 每 N 个 epoch 保存一次</span>
$network_dim <span class="token operator">=</span> <span class="token number">64</span> <span class="token comment"># network dim | 常用 4~128，不是越大越好</span>
$network_alpha <span class="token operator">=</span> <span class="token number">32</span> <span class="token comment"># network alpha | 常用与 network_dim 相同的值或者采用较小的值，如 network_dim的一半 防止下溢。默认值为 1，使用较小的 alpha 需要提升学习率。</span>
$clip_skip <span class="token operator">=</span> <span class="token number">2</span> <span class="token comment"># clip skip | 玄学 一般用 2</span>
$train_unet_only <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment"># train U-Net only | 仅训练 U-Net，开启这个会牺牲效果大幅减少显存使用。6G显存可以开启</span>
$train_text_encoder_only <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment"># train Text Encoder only | 仅训练 文本编码器</span>

<span class="token comment"># Learning rate | 学习率</span>
$lr <span class="token operator">=</span> <span class="token string">"5e-5"</span>
$unet_lr <span class="token operator">=</span> <span class="token string">"5e-5"</span>
$text_encoder_lr <span class="token operator">=</span> <span class="token string">"6e-6"</span>
$lr_scheduler <span class="token operator">=</span> <span class="token string">"cosine_with_restarts"</span> <span class="token comment"># "linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"</span>
$lr_warmup_steps <span class="token operator">=</span> <span class="token number">50</span> <span class="token comment"># warmup steps | 仅在 lr_scheduler 为 constant_with_warmup 时需要填写这个值</span>
$lr_restart_cycles <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment"># cosine_with_restarts restart cycles | 余弦退火重启次数，仅在 lr_scheduler 为 cosine_with_restarts 时起效。</span>

<span class="token comment"># Output settings | 输出设置</span>
$output_name <span class="token operator">=</span> <span class="token string">"Kazari_v1"</span> <span class="token comment"># output model name | 模型保存名称</span>
$save_model_as <span class="token operator">=</span> <span class="token string">"safetensors"</span> <span class="token comment"># model save ext | 模型保存格式 ckpt, pt, safetensors</span>

<span class="token comment"># 其他设置</span>
$network_weights <span class="token operator">=</span> <span class="token string">""</span> <span class="token comment"># pretrained weights for LoRA network | 若需要从已有的 LoRA 模型上继续训练，请填写 LoRA 模型路径。</span>
<span class="token comment"># $network_weights = "D:\workspace\stable-diffusion-webui\models\Lora\koreanDollLikeness_v10.safetensors" # pretrained weights for LoRA network | 若需要从已有的 LoRA 模型上继续训练，请填写 LoRA 模型路径。</span>
$min_bucket_reso <span class="token operator">=</span> <span class="token number">256</span> <span class="token comment"># arb min resolution | arb 最小分辨率</span>
$max_bucket_reso <span class="token operator">=</span> <span class="token number">1024</span> <span class="token comment"># arb max resolution | arb 最大分辨率</span>
$persistent_data_loader_workers <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment"># persistent dataloader workers | 容易爆内存，保留加载训练集的worker，减少每个 epoch 之间的停顿</span>

<span class="token comment"># 优化器设置</span>
$use_8bit_adam <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment"># use 8bit adam optimizer | 使用 8bit adam 优化器节省显存，默认启用。部分 10 系老显卡无法使用，修改为 0 禁用。</span>
$use_lion <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment"># use lion optimizer | 使用 Lion 优化器</span>


<span class="token comment"># ============= DO NOT MODIFY CONTENTS BELOW | 请勿修改下方内容 =====================</span>
<span class="token comment"># Activate python venv</span>
<span class="token punctuation">.</span>\venv\Scripts\activate

$Env<span class="token punctuation">:</span>HF_HOME <span class="token operator">=</span> <span class="token string">"huggingface"</span>
$ext_args <span class="token operator">=</span> <span class="token punctuation">[</span>System<span class="token punctuation">.</span>Collections<span class="token punctuation">.</span>ArrayList<span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token punctuation">:</span>new<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> <span class="token punctuation">(</span>$train_unet_only<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  <span class="token punctuation">[</span>void<span class="token punctuation">]</span>$ext_args<span class="token punctuation">.</span>Add<span class="token punctuation">(</span><span class="token string">"--network_train_unet_only"</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span>

<span class="token keyword">if</span> <span class="token punctuation">(</span>$train_text_encoder_only<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  <span class="token punctuation">[</span>void<span class="token punctuation">]</span>$ext_args<span class="token punctuation">.</span>Add<span class="token punctuation">(</span><span class="token string">"--network_train_text_encoder_only"</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span>

<span class="token keyword">if</span> <span class="token punctuation">(</span>$network_weights<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  <span class="token punctuation">[</span>void<span class="token punctuation">]</span>$ext_args<span class="token punctuation">.</span>Add<span class="token punctuation">(</span><span class="token string">"--network_weights="</span> <span class="token operator">+</span> $network_weights<span class="token punctuation">)</span>
<span class="token punctuation">}</span>

<span class="token keyword">if</span> <span class="token punctuation">(</span>$reg_data_dir<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  <span class="token punctuation">[</span>void<span class="token punctuation">]</span>$ext_args<span class="token punctuation">.</span>Add<span class="token punctuation">(</span><span class="token string">"--reg_data_dir="</span> <span class="token operator">+</span> $reg_data_dir<span class="token punctuation">)</span>
<span class="token punctuation">}</span>

<span class="token keyword">if</span> <span class="token punctuation">(</span>$use_8bit_adam<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  <span class="token punctuation">[</span>void<span class="token punctuation">]</span>$ext_args<span class="token punctuation">.</span>Add<span class="token punctuation">(</span><span class="token string">"--use_8bit_adam"</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span>

<span class="token keyword">if</span> <span class="token punctuation">(</span>$use_lion<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  <span class="token punctuation">[</span>void<span class="token punctuation">]</span>$ext_args<span class="token punctuation">.</span>Add<span class="token punctuation">(</span><span class="token string">"--use_lion_optimizer"</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span>

<span class="token keyword">if</span> <span class="token punctuation">(</span>$persistent_data_loader_workers<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  <span class="token punctuation">[</span>void<span class="token punctuation">]</span>$ext_args<span class="token punctuation">.</span>Add<span class="token punctuation">(</span><span class="token string">"--persistent_data_loader_workers"</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span>

<span class="token comment"># run train</span>
accelerate launch <span class="token operator">-</span><span class="token operator">-</span>num_cpu_threads_per_process<span class="token operator">=</span><span class="token number">8</span> <span class="token string">"./sd-scripts/train_network.py"</span> `
  <span class="token operator">-</span><span class="token operator">-</span>enable_bucket `
  <span class="token operator">-</span><span class="token operator">-</span>pretrained_model_name_or_path<span class="token operator">=</span>$pretrained_model `
  <span class="token operator">-</span><span class="token operator">-</span>train_data_dir<span class="token operator">=</span>$train_data_dir `
  <span class="token operator">-</span><span class="token operator">-</span>output_dir<span class="token operator">=</span><span class="token string">"./output"</span> `
  <span class="token operator">-</span><span class="token operator">-</span>logging_dir<span class="token operator">=</span><span class="token string">"./logs"</span> `
  <span class="token operator">-</span><span class="token operator">-</span>resolution<span class="token operator">=</span>$resolution `
  <span class="token operator">-</span><span class="token operator">-</span>network_module<span class="token operator">=</span>networks<span class="token punctuation">.</span>lora `
  <span class="token operator">-</span><span class="token operator">-</span>max_train_epochs<span class="token operator">=</span>$max_train_epoches `
  <span class="token operator">-</span><span class="token operator">-</span>learning_rate<span class="token operator">=</span>$lr `
  <span class="token operator">-</span><span class="token operator">-</span>unet_lr<span class="token operator">=</span>$unet_lr `
  <span class="token operator">-</span><span class="token operator">-</span>text_encoder_lr<span class="token operator">=</span>$text_encoder_lr `
  <span class="token operator">-</span><span class="token operator">-</span>lr_scheduler<span class="token operator">=</span>$lr_scheduler `
  <span class="token operator">-</span><span class="token operator">-</span>lr_warmup_steps<span class="token operator">=</span>$lr_warmup_steps `
  <span class="token operator">-</span><span class="token operator">-</span>lr_scheduler_num_cycles<span class="token operator">=</span>$lr_restart_cycles `
  <span class="token operator">-</span><span class="token operator">-</span>network_dim<span class="token operator">=</span>$network_dim `
  <span class="token operator">-</span><span class="token operator">-</span>network_alpha<span class="token operator">=</span>$network_alpha `
  <span class="token operator">-</span><span class="token operator">-</span>output_name<span class="token operator">=</span>$output_name `
  <span class="token operator">-</span><span class="token operator">-</span>train_batch_size<span class="token operator">=</span>$batch_size `
  <span class="token operator">-</span><span class="token operator">-</span>save_every_n_epochs<span class="token operator">=</span>$save_every_n_epochs `
  <span class="token operator">-</span><span class="token operator">-</span>mixed_precision<span class="token operator">=</span><span class="token string">"fp16"</span> `
  <span class="token operator">-</span><span class="token operator">-</span>save_precision<span class="token operator">=</span><span class="token string">"fp16"</span> `
  <span class="token operator">-</span><span class="token operator">-</span>seed<span class="token operator">=</span><span class="token string">"1337"</span> `
  <span class="token operator">-</span><span class="token operator">-</span>cache_latents `
  <span class="token operator">-</span><span class="token operator">-</span>clip_skip<span class="token operator">=</span>$clip_skip `
  <span class="token operator">-</span><span class="token operator">-</span>prior_loss_weight<span class="token operator">=</span><span class="token number">1</span> `
  <span class="token operator">-</span><span class="token operator">-</span>max_token_length<span class="token operator">=</span><span class="token number">225</span> `
  <span class="token operator">-</span><span class="token operator">-</span>caption_extension<span class="token operator">=</span><span class="token string">".txt"</span> `
  <span class="token operator">-</span><span class="token operator">-</span>save_model_as<span class="token operator">=</span>$save_model_as `
  <span class="token operator">-</span><span class="token operator">-</span>min_bucket_reso<span class="token operator">=</span>$min_bucket_reso `
  <span class="token operator">-</span><span class="token operator">-</span>max_bucket_reso<span class="token operator">=</span>$max_bucket_reso `
  <span class="token operator">-</span><span class="token operator">-</span>xformers <span class="token operator">-</span><span class="token operator">-</span>shuffle_caption $ext_args

Write<span class="token operator">-</span>Output <span class="token string">"Train finished"</span>
Read<span class="token operator">-</span>Host <span class="token operator">|</span> Out<span class="token operator">-</span>Null <span class="token punctuation">;</span>
</code></pre> 
<p>train_data_dir是我们放图片的目录，我们把刚才处理好的图片文件夹（里面有图片和文字对）放在这个目录下。</p> 
<p>我的目录结构是这样的（其中2_images就是我们处理好的图片文件夹，为什么这么命名一会讲）：</p> 
<pre><code class="prism language-python">C<span class="token punctuation">:</span>\Users\NUOSEN\Desktop\data\lora\Kazari\2_uiharu_kazari
</code></pre> 
<p>所以我在上面的脚本里填写的目录为</p> 
<pre><code class="prism language-python">C<span class="token punctuation">:</span>\Users\NUOSEN\Desktop\data\lora\Kazari
</code></pre> 
<p>训练的时候脚本自动在Kazari目录下找图片文件夹，它判定的是[NUM]_XXX命名的文件夹就是要输入给dataloader的图片文件夹。</p> 
<p>我们一定要按照这个规则明明我们的文件夹，NUM表示我们一个epoch要遍历几遍这个目录，即repeats，而XXX就是我们默认的触发词。</p> 
<p>而我们还会在脚本设置max_train_epoches这个参数，这个就表示我们要训练多少个epoch。</p> 
<p>所以训练时候优化的次数Step就是num_images X repeats X max_train_epoches / batch_size</p> 
<p>batch_size就是一次处理几张图片，也是我们在脚本里设置的，显卡有能力就别设置1。</p> 
<p>更重要的参数就是学习率Lr了：</p> 
<p>$lr=“5e-5”</p> 
<p>$unet_lr=“5e-5”</p> 
<p>$text_encoder_lr=“6e-6”</p> 
<p>这个我们可以先设置成这样，然后看loss函数下降了没，再进行调整。</p> 
<p>训练图片分辨率resolution这个参数也是非常重要的，它是我们训练时候图片的分辨率，显卡有能力设置大一些比较好，建议不要低于512，会很差。</p> 
<p>network_dim表示我们训练出来的Lora模型大小，一般不要大于128，因为没收益，小一些的dim可以抗过拟合。</p> 
<p>clip_skip这个参数在二次元模型里就设置2或者3，但真人模型可以考虑设置1，但是需要测试效果，一般就2。这个参数其实是CLIP这个模型结构的原因，它其实是层级概念，比如上一层是“人”，那么下一层就是“男人”“女人”，逐渐细化。而二次元这个从NovelAI起源的模型，都沿用了2这个设定。我们也设置成2。</p> 
<p>use_lion这个我设置成1，是因为我想启用Lion这个优化器，因为测试效果发现这个优化器泛化性好一些。</p> 
<p>save_every_n_epochs这个我设置成1，就是每个epoch都保存模型，这样最后我有10个模型了。</p> 
<p>设置好参数我们就一键运行脚本开始训练。<br> <img src="https://images2.imgbox.com/19/e6/69Ljo3rP_o.png" alt="在这里插入图片描述"><br> 训练完毕。</p> 
<h3><a id="25__256"></a>2.5 检测结果</h3> 
<p>训练完的lora应该看下不同epoch的结果，排除欠拟合和过拟合的模型，用最合适的模型。</p> 
<p>我们直接用简单的prompts遍历所有的模型和weights：<br> <img src="https://images2.imgbox.com/ac/2b/QFuJmZD8_o.png" alt="在这里插入图片描述"><br> 能出一张xy表：</p> 
<p><img src="https://images2.imgbox.com/c9/0b/4vu2yBHZ_o.png" alt="在这里插入图片描述"><br> 我最后选了8epoch的模型，因为我感觉它在各种环境下最稳定。</p> 
<h3><a id="26__266"></a>2.6 生成图片</h3> 
<p>训练好的Lora就可以出图啦。<br> <img src="https://images2.imgbox.com/c8/45/3j689BH9_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/467866c4aa62d676da5596770a076164/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Java基础——学生成绩信息管理系统（简单实现）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c8fb0865b1f2c962e3080d280e8407d6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">4.2 图书借阅系统数据库设计 --MySQL</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>