<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Stable Diffusion1.5网络结构-超详细原创 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/e4773f6b334b72565487b0a34aef9f96/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Stable Diffusion1.5网络结构-超详细原创">
  <meta property="og:description" content="目录
1 Unet
1.0 介绍　1.1详细整体结构
1.2 缩小版整体结构
1.3 时间步编码
1.4 CrossAttnDownBlock2D
1.4.1 ResnetBlock2D
1.4.2 Transformer2DModel
1.4.2.1 BasicTransformerBlock
1.4.2.1.1 SelfAttention
1.4.2.1.2 CrossAttention
1.4.2.1.3 FeedForward
1.4.3 DownSample2D
1.5 DownBlock2D
1.6 UnetMidBlock2DCrossAttn
1.7 UpBlock2D
1.7.1 UpSample2D
1.8 CrossAttnUpBlock2D
2 VAE
2.0 介绍
2.1 AE
2.2 VAE
2.3 整体结构 2.4 DownEncoderBlock2D
2.4.1 ResnetBlock2D
2.4.2 UpSample2D
2.5 UnetMidBlock2D
2.6 Sample
2.7 UpDecoderBlock2D
2.7.1 UpSample2D
3 CLIP
3.1 CLIPTextEmbeddings
3.2 CLIPEncoderLayer
绘制软件：ProcessOn，以下图片保存可高清查看
1 Unet 1.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-11-20T00:14:08+08:00">
    <meta property="article:modified_time" content="2023-11-20T00:14:08+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Stable Diffusion1.5网络结构-超详细原创</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"></p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="1%20Unet-toc" style="margin-left:40px;"><a href="#1%20Unet" rel="nofollow">1 Unet</a></p> 
<p id="1.0%20%E4%BB%8B%E7%BB%8D%E3%80%80-toc" style="margin-left:80px;"><a href="#1.0%20%E4%BB%8B%E7%BB%8D%E3%80%80" rel="nofollow">1.0 介绍　</a></p> 
<p id="1.1%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84-toc" style="margin-left:80px;"><a href="#1.1%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84" rel="nofollow">1.1详细整体结构</a></p> 
<p id="1.2%20%E7%BC%A9%E5%B0%8F%E7%89%88%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84-toc" style="margin-left:80px;"><a href="#1.2%20%E7%BC%A9%E5%B0%8F%E7%89%88%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84" rel="nofollow">1.2 缩小版整体结构</a></p> 
<p id="1.3%20%E6%97%B6%E9%97%B4%E6%AD%A5%E7%BC%96%E7%A0%81-toc" style="margin-left:80px;"><a href="#1.3%20%E6%97%B6%E9%97%B4%E6%AD%A5%E7%BC%96%E7%A0%81" rel="nofollow">1.3 时间步编码</a></p> 
<p id="1.4%C2%A0CrossAttnDownBlock2D-toc" style="margin-left:80px;"><a href="#1.4%C2%A0CrossAttnDownBlock2D" rel="nofollow">1.4 CrossAttnDownBlock2D</a></p> 
<p id="1.4.1%C2%A0ResnetBlock2D-toc" style="margin-left:120px;"><a href="#1.4.1%C2%A0ResnetBlock2D" rel="nofollow">1.4.1 ResnetBlock2D</a></p> 
<p id="1.4.2%C2%A0Transformer2DModel-toc" style="margin-left:120px;"><a href="#1.4.2%C2%A0Transformer2DModel" rel="nofollow">1.4.2 Transformer2DModel</a></p> 
<p id="1.4.2.1%C2%A0BasicTransformerBlock-toc" style="margin-left:160px;"><a href="#1.4.2.1%C2%A0BasicTransformerBlock" rel="nofollow">1.4.2.1 BasicTransformerBlock</a></p> 
<p id="1.4.2.1.1%C2%A0SelfAttention-toc" style="margin-left:200px;"><a href="#1.4.2.1.1%C2%A0SelfAttention" rel="nofollow">1.4.2.1.1 SelfAttention</a></p> 
<p id="1.4.2.1.2%C2%A0CrossAttention-toc" style="margin-left:200px;"><a href="#1.4.2.1.2%C2%A0CrossAttention" rel="nofollow">1.4.2.1.2 CrossAttention</a></p> 
<p id="1.4.2.1.3%C2%A0FeedForward-toc" style="margin-left:200px;"><a href="#1.4.2.1.3%C2%A0FeedForward" rel="nofollow">1.4.2.1.3 FeedForward</a></p> 
<p id="1.4.3%C2%A0DownSample2D-toc" style="margin-left:120px;"><a href="#1.4.3%C2%A0DownSample2D" rel="nofollow">1.4.3 DownSample2D</a></p> 
<p id="1.5%C2%A0DownBlock2D-toc" style="margin-left:80px;"><a href="#1.5%C2%A0DownBlock2D" rel="nofollow">1.5 DownBlock2D</a></p> 
<p id="1.6%C2%A0UnetMidBlock2DCrossAttn-toc" style="margin-left:80px;"><a href="#1.6%C2%A0UnetMidBlock2DCrossAttn" rel="nofollow">1.6 UnetMidBlock2DCrossAttn</a></p> 
<p id="1.7%C2%A0UpBlock2D-toc" style="margin-left:80px;"><a href="#1.7%C2%A0UpBlock2D" rel="nofollow">1.7 UpBlock2D</a></p> 
<p id="1.7.1%C2%A0UpSample2D-toc" style="margin-left:120px;"><a href="#1.7.1%C2%A0UpSample2D" rel="nofollow">1.7.1 UpSample2D</a></p> 
<p id="1.8%C2%A0CrossAttnUpBlock2D-toc" style="margin-left:80px;"><a href="#1.8%C2%A0CrossAttnUpBlock2D" rel="nofollow">1.8 CrossAttnUpBlock2D</a></p> 
<p id="2%20VAE-toc" style="margin-left:40px;"><a href="#2%20VAE" rel="nofollow">2 VAE</a></p> 
<p id="2.0%20%E4%BB%8B%E7%BB%8D-toc" style="margin-left:80px;"><a href="#2.0%20%E4%BB%8B%E7%BB%8D" rel="nofollow">2.0 介绍</a></p> 
<p id="2.1%20AE-toc" style="margin-left:80px;"><a href="#2.1%20AE" rel="nofollow">2.1 AE</a></p> 
<p id="2.2%20VAE-toc" style="margin-left:80px;"><a href="#2.2%20VAE" rel="nofollow">2.2 VAE</a></p> 
<p id="2.3%20%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84%C2%A0-toc" style="margin-left:80px;"><a href="#2.3%20%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84%C2%A0" rel="nofollow">2.3 整体结构 </a></p> 
<p id="2.4%C2%A0DownEncoderBlock2D-toc" style="margin-left:80px;"><a href="#2.4%C2%A0DownEncoderBlock2D" rel="nofollow">2.4 DownEncoderBlock2D</a></p> 
<p id="2.4.1%C2%A0%C2%A0ResnetBlock2D-toc" style="margin-left:120px;"><a href="#2.4.1%C2%A0%C2%A0ResnetBlock2D" rel="nofollow">2.4.1  ResnetBlock2D</a></p> 
<p id="2.4.2%C2%A0UpSample2D-toc" style="margin-left:120px;"><a href="#2.4.2%C2%A0UpSample2D" rel="nofollow">2.4.2 UpSample2D</a></p> 
<p id="2.5%C2%A0UnetMidBlock2D-toc" style="margin-left:80px;"><a href="#2.5%C2%A0UnetMidBlock2D" rel="nofollow">2.5 UnetMidBlock2D</a></p> 
<p id="2.6%C2%A0Sample-toc" style="margin-left:80px;"><a href="#2.6%C2%A0Sample" rel="nofollow">2.6 Sample</a></p> 
<p id="2.7%C2%A0UpDecoderBlock2D-toc" style="margin-left:80px;"><a href="#2.7%C2%A0UpDecoderBlock2D" rel="nofollow">2.7 UpDecoderBlock2D</a></p> 
<p id="%C2%A02.7.1%C2%A0%C2%A0UpSample2D-toc" style="margin-left:120px;"><a href="#%C2%A02.7.1%C2%A0%C2%A0UpSample2D" rel="nofollow"> 2.7.1  UpSample2D</a></p> 
<p id="3%20CLIP-toc" style="margin-left:40px;"><a href="#3%20CLIP" rel="nofollow">3 CLIP</a></p> 
<p id="3.1%C2%A0%20CLIPTextEmbeddings-toc" style="margin-left:80px;"><a href="#3.1%C2%A0%20CLIPTextEmbeddings" rel="nofollow">3.1  CLIPTextEmbeddings</a></p> 
<p id="3.2%C2%A0%C2%A0CLIPEncoderLayer-toc" style="margin-left:80px;"><a href="#3.2%C2%A0%C2%A0CLIPEncoderLayer" rel="nofollow">3.2  CLIPEncoderLayer</a></p> 
<hr id="hr-toc"> 
<p></p> 
<p><span style="color:#fe2c24;">绘制软件：ProcessOn，以下图片保存可高清查看</span></p> 
<h3 id="1%20Unet">1 Unet</h3> 
<h4 id="1.0%20%E4%BB%8B%E7%BB%8D%E3%80%80">1.0 介绍　</h4> 
<p>        负责预测噪声</p> 
<h4 id="1.1%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84">1.1详细整体结构</h4> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/e4/ab/jKy6pf7P_o.png"></p> 
<p style="text-align:center;"></p> 
<p style="text-align:center;"></p> 
<h4 id="1.2%20%E7%BC%A9%E5%B0%8F%E7%89%88%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84">1.2 缩小版整体结构</h4> 
<p><img alt="" src="https://images2.imgbox.com/cb/e9/cqMYK50G_o.png"></p> 
<p></p> 
<h4 id="1.3%20%E6%97%B6%E9%97%B4%E6%AD%A5%E7%BC%96%E7%A0%81">1.3 时间步编码</h4> 
<p style="text-align:center;"><img alt="" height="490" src="https://images2.imgbox.com/5f/d2/jmJ9uUlw_o.png" width="500"></p> 
<p></p> 
<h4 id="1.4%C2%A0CrossAttnDownBlock2D">1.4 CrossAttnDownBlock2D</h4> 
<p><strong>每个ResnetBlock2D的输入有两个</strong></p> 
<p>1，一个是来自上一层的输出lattent,</p> 
<p>2，另一个来自时间步编码模块的输出time_embeds ( shape=[2, 1280], 后面省略说明，默认[2, 1280]这种写法是tersor的形状)</p> 
<p><strong>每个Transformer2DModel输入有两个</strong></p> 
<p>１，上一层的输出</p> 
<p>２， CLIP text_encoder的文本编码text embedding，或者叫提示词编码prompt embedding，其shape=[2, 77, 768]</p> 
<p></p> 
<p>后面凡是有ResnetBlock2D和Transformer2DModel的模块，其输入形式都是如此，为了方便，后面有些模块的time_embeds和prompt  embedding这两个输入就默认不画了，例如UnetMidBlock2DCrossAttn、UpBlock2D、CrossAttnUpBlock2D</p> 
<p style="text-align:center;"></p> 
<p class="img-center"><img alt="" height="565" src="https://images2.imgbox.com/02/7e/RbFsYJce_o.png" width="400"></p> 
<h5 id="1.4.1%C2%A0ResnetBlock2D">1.4.1 ResnetBlock2D</h5> 
<p>需要注意的点</p> 
<p>1, ResnetBlock2D的输入有两个，一个是来自上一层的lattent，另一个来自时间步编码模块的输出time_embeds ( shape=[2, 1280], 后面省略说明，默认[2, 1280]这种写法是tersor的形状)</p> 
<p>2, Conv3x3和Linear的输入输出Channel，不同层会不一样</p> 
<p>3, 输入输出通道数不一致的时候，残差连接会用一个1x1的卷积</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/35/92/QRPgrL3f_o.png"></p> 
<h5 id="1.4.2%C2%A0Transformer2DModel">1.4.2 Transformer2DModel</h5> 
<p>Transformer2DModel输入有两个</p> 
<p>１，上一层的输入</p> 
<p>２， CLIP text_encoder的文本编码text embedding，或者叫提示词编码prompt embedding，其shape=[2, 77, 768]</p> 
<p class="img-center"><img alt="" height="551" src="https://images2.imgbox.com/49/59/3BCRTujm_o.png" width="400"></p> 
<h6 id="1.4.2.1%C2%A0BasicTransformerBlock">1.4.2.1 BasicTransformerBlock</h6> 
<p class="img-center"><img alt="" height="738" src="https://images2.imgbox.com/64/2a/CsebCEbv_o.png" width="400"></p> 
<h6 id="1.4.2.1.1%C2%A0SelfAttention">1.4.2.1.1 SelfAttention</h6> 
<p class="img-center"><img alt="" height="1200" src="https://images2.imgbox.com/49/fb/e5Za56wy_o.png" width="800"></p> 
<h6 id="1.4.2.1.2%C2%A0CrossAttention">1.4.2.1.2 CrossAttention</h6> 
<p class="img-center"><img alt="" height="1200" src="https://images2.imgbox.com/fb/2c/kGkqHQCG_o.png" width="800"></p> 
<h6 id="1.4.2.1.3%C2%A0FeedForward">1.4.2.1.3 FeedForward</h6> 
<p class="img-center"><img alt="" height="489" src="https://images2.imgbox.com/87/3b/Oz6doQw2_o.png" width="400"></p> 
<h5 id="1.4.3%C2%A0DownSample2D">1.4.3 DownSample2D</h5> 
<p class="img-center"><img alt="" height="465" src="https://images2.imgbox.com/70/26/orbGNFMX_o.png" width="300"></p> 
<h4 id="1.5%C2%A0DownBlock2D">1.5 DownBlock2D</h4> 
<p class="img-center"><img alt="" height="333" src="https://images2.imgbox.com/59/29/dg8fWHXK_o.png" width="300"></p> 
<h4 id="1.6%C2%A0UnetMidBlock2DCrossAttn">1.6 UnetMidBlock2DCrossAttn</h4> 
<p></p> 
<p class="img-center"><img alt="" height="427" src="https://images2.imgbox.com/8d/9d/WI1SW9sA_o.png" width="350"></p> 
<h4 id="1.7%C2%A0UpBlock2D">1.7 UpBlock2D</h4> 
<p>UNet右边部分的ResnetBlock2D模块，其输入除了有来自上一层的输出和time_embedd之外，还有自UNet左边部分输入，具体做法是将上一层的输出和UNet左边部分输入进行concat之后送进ResnetBlock2D模块，然后和time_embedd相加，后面的CrossAttnUpBlock2D也是如此，具体查看<strong>1.1 详细整体结构</strong></p> 
<p class="img-center"><img alt="" height="422" src="https://images2.imgbox.com/88/70/yEyyg6K3_o.png" width="400"></p> 
<h5 id="1.7.1%C2%A0UpSample2D">1.7.1 UpSample2D</h5> 
<p></p> 
<p class="img-center"><img alt="" height="391" src="https://images2.imgbox.com/19/c0/Tnji2Ca1_o.png" width="300"></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<h4 id="1.8%C2%A0CrossAttnUpBlock2D">1.8 <strong>CrossAttnUpBlock2D</strong></h4> 
<p>注意, 最后一个CrossAttnUpBlock2D没有UpSample2D模块,该模块具体输入输出shape看<strong>1.1 详细整体结构</strong></p> 
<p class="img-center"><img alt="" height="677" src="https://images2.imgbox.com/e4/1c/DW8INDdG_o.png" width="350"></p> 
<p></p> 
<h3 id="2%20VAE">2 VAE</h3> 
<h4 id="2.0%20%E4%BB%8B%E7%BB%8D">2.0 介绍</h4> 
<p>将扩散过程从512x512的图像空间映射降维到64x64的潜空间，内存和运算量减小64倍</p> 
<h4 id="2.1%20AE">2.1 AE</h4> 
<p><strong>注意：下面说的特征向量，编码向量，潜变量，code是同一个意思</strong></p> 
<p>普通的自编码器，分为编码器和解码器，编码器Encoder负责将编码图像，把图像从高维映射到低维，得到特征向量，例如把3x512x512的图像编码成4x64x64的特征向量，这个特征向量可以表示原始图像，包含原始图像的特征，比如颜色，纹理等其他抽象特征，解码器Decoder负责把低维的特征向量还原回原始图像．但这种编码是固定的，一张图片只能编码成一个固定的向量，反过来，解码也是唯一的，一个固定的编码向量只会被解码成一张确定的图像，如果来一张你训练集没见过的图片，对其编码后再解码，生成的图像大概率是个无意义的图像，因此AE是一个单值映射关系，潜变量（即编码向量）具有不连续性，潜变量是确定性值，选择一个随机的潜在变量可能会产生垃圾输出，潜在空间缺乏生成能力(即潜空间不是所有的潜变量都是有效的)，</p> 
<p>举个李宏毅老师的例子：</p> 
<p>假设我们训练好的AE将“新月”图片encode成code=1（这里假设code只有1维），将其decode能得到“新月”的图片；将“满月”encode成code=10，同样将其decode能得到“满月”图片。这时候如果我们给AE一个code=5，我们希望是能得到“半月”的图片，但由于之前训练时并没有将“半月”的图片编码，或者将一张非月亮的图片编码为5，那么我们就不太可能得到“半月”的图片。</p> 
<p class="img-center"><img alt="" height="363" src="https://images2.imgbox.com/f4/54/k1B8j8Vb_o.png" width="412"></p> 
<p></p> 
<p></p> 
<p><strong>其他博客解释：</strong></p> 
<p>        AE的Encoder是将图片映射成“数值编码”，Decoder是将“数值编码”映射成图片。这样存在的问题是，在训练过程中，随着不断降低输入图片与输出图片之间的误差，模型会过拟合，泛化性能不好。也就是说对于一个训练好的AE，输入某个图片，就只会将其编码为某个确定的code，输入某个确定的code就只会输出某个确定的图片，并且如果这个code来自于没见过的图片，那么生成的图片也不会好。</p> 
<p>        自动编码器是数据相关的（data-specific 或 data-dependent），这意味着自动编码器只能压缩那些与训练数据类似的数据，反过也是一类数据对应一种编码器，无法拓展一种编码器去应用于另一类数据。</p> 
<h4 id="2.2%20VAE">2.2 VAE</h4> 
<p>而VAE不是将图像编码成一个固定的值，而是编码成一个连续的分布，这样，只要满足这个分布，我就能重建原始图像，满足这个分布的值会有很多，不再是一个固定的code了，例如，我把编码器的输出约束成一个标准正态分布，重建图像我只需要从标准正态分布采样在送到就解码器即可，</p> 
<p>这里其实不一定非得要是标准正态分布，只要是连续的分布理论上都可以，但由于标准正态分布有良好的性质：１，高维正态分布采样的向量模长近似一致，两两近似正交，两点之间的欧式距离与期望值近似，这是一个很好的约束，对模型的学习有利;2,良好的数学性质，但你对多个分布进行运算时（像Diffusion），正态分布会带来很大的便利,可以通过标准化,变换等方式处理，便于模型的训练和优化;3,用标准正态分布对潜变量进行建模，能利用正态分布的性质进行随机采样和重构（重参数化）</p> 
<p>举个例子：来自<a href="https://www.cnblogs.com/amazingter/p/14686450.html" rel="nofollow" title="https://www.cnblogs.com/amazingter/p/14686450.html">https://www.cnblogs.com/amazingter/p/14686450.html</a></p> 
<p>针对上面的半月问题，我们转变思路，不将图片映射成“数值编码”，而将其映射成“分布”。我们将“新月”图片映射成μ=1的正态分布，那么就相当于在1附近加了噪声，此时不仅1表示“新月”，1附近的数值也表示“新月”，只是1的时候最像“新月”。将"满月"映射成μ=10的正态分布，10的附近也都表示“满月”。那么code=5时，就同时拥有了“新月”和“满月”的特点，那么这时候decode出来的大概率就是“半月”了。这就是VAE的思想。</p> 
<p class="img-center"><img alt="" height="541" src="https://images2.imgbox.com/eb/d7/daO99Uuc_o.png" width="412"></p> 
<p>关于VAE还可以参考<a href="https://blog.csdn.net/a312863063/article/details/87953517" title="【VAE学习笔记】全面通透地理解VAE(Variational Auto Encoder)_vae的作用-CSDN博客">【VAE学习笔记】全面通透地理解VAE(Variational Auto Encoder)_vae的作用-CSDN博客</a></p> 
<p>写的很好，接下来正式上图</p> 
<h4 id="2.3%20%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84%C2%A0">2.3 整体结构 </h4> 
<p>由于csdn查看大图很捞，可双击另存为查看高清大图</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/13/ad/FF0Ep0Ol_o.png"></p> 
<h4 id="2.4%C2%A0DownEncoderBlock2D">2.4 DownEncoderBlock2D</h4> 
<p>注意：VAE<strong>的ResnetBlock2D模块是没有时间步的输入的，即time_embeds=None, 其他的和上面Unet中的ResnetBlock2D一致</strong></p> 
<p class="img-center"><img alt="" height="431" src="https://images2.imgbox.com/cb/53/UmRyMf8V_o.png" width="350"></p> 
<h5 id="2.4.1%C2%A0%C2%A0ResnetBlock2D">2.4.1  ResnetBlock2D</h5> 
<p class="img-center"><img alt="" height="560" src="https://images2.imgbox.com/b6/31/rPEI6YeT_o.png" width="412"></p> 
<h5 id="2.4.2%C2%A0UpSample2D">2.4.2 UpSample2D</h5> 
<p></p> 
<p class="img-center"><img alt="" height="390" src="https://images2.imgbox.com/98/d7/0x25bMZ9_o.png" width="300"></p> 
<h4 id="2.5%C2%A0UnetMidBlock2D">2.5 UnetMidBlock2D</h4> 
<p>SelfAttention参考Unet中的</p> 
<p></p> 
<p class="img-center"><img alt="" height="644" src="https://images2.imgbox.com/7e/ff/O7MdVt5Z_o.png" width="412"></p> 
<h4 id="2.6%C2%A0Sample">2.6 Sample</h4> 
<p>重参数化技巧：</p> 
<p>现在均值mean和方差var(实际是log方差，这里先以方差为例)是网络的输出，也就是现在编码器的输出服从N~(mean, var)这个正态分布，我们要从这个正态分布采样出一个样本，送进解码器，这个样本是服从N~(mean, var)正态分布的，但采样这个操作是不可导的，采样操作是指从某个概率分布中随机抽取样本的过程，因为梯度是损失函数关于模型参数的变化率，而采样操作的非确定性使得无法直接计算关于参数的精确梯度。如果将采样操作视为一个具有参数的函数，并尝试计算其导数，通常会遇到两个问题：</p> 
<ol><li> <p><strong>不可导性：</strong> 由于采样操作引入了离散性和不可导性，它们在大多数情况下是不可导的。导数描述的是函数在某一点上的变化率，而采样操作在这方面表现得很突变和不连续，因此没有明确定义的导数。</p> </li><li> <p><strong>梯度的方差：</strong> 即使我们忽略不可导性，尝试使用梯度信息来更新参数，由于采样引入的随机性，梯度的方差可能会非常高，导致不稳定的优化过程。</p> </li></ol> 
<p>所以无法对mean和var求偏导，因为这里mean和var就是网络的参数，这个时候可以从标准正态分布N（0,1）采样一个sample(一般会用概率密度的分布函数去采样), 这个时候sample就是一个确定的值，把sample当成常数，std是标准差，等于var开根号</p> 
<p>令z=mean + sample * std，这个时候z就可以对mean和std求偏导了，z就等价从N~(mean, var)这个分布采样，可以对z求期望和方差，根据高斯分布的性质以及期望和方差的公式，能得出z也服从正态分布，切均值=mean, 方差=std的平方=var，这样做其实是把随机性转移到了sample这个常数</p> 
<p>但代码里是log方差，是因为方差是正的，加log让它取值范围也能取负值，这样就不用加激活函数了，所以后面再用exp还原方差</p> 
<p class="img-center"><img alt="" height="476" src="https://images2.imgbox.com/e0/45/DIaKB8Vv_o.png" width="512"></p> 
<h4 id="2.7%C2%A0UpDecoderBlock2D">2.7 UpDecoderBlock2D</h4> 
<p></p> 
<p class="img-center"><img alt="" height="614" src="https://images2.imgbox.com/8d/eb/QdHplJc0_o.png" width="412"></p> 
<h5 id="%C2%A02.7.1%C2%A0%C2%A0UpSample2D"> 2.7.1  UpSample2D</h5> 
<p></p> 
<p class="img-center"><img alt="" height="455" src="https://images2.imgbox.com/26/02/nUwJuTQA_o.png" width="350"></p> 
<p></p> 
<h3 id="3%20CLIP">3 CLIP</h3> 
<p>CLIP这里用到的是文本编码器，结构如下</p> 
<p class="img-center"><img alt="" height="717" src="https://images2.imgbox.com/b6/a4/hFi0t4mH_o.png" width="350"></p> 
<h4 id="3.1%C2%A0%20CLIPTextEmbeddings">3.1  CLIPTextEmbeddings</h4> 
<p>灰色的框框text_inoputs和embedding不是操作，是表示输入输出</p> 
<p class="img-center"><img alt="" height="466" src="https://images2.imgbox.com/b2/44/2k5woYAz_o.png" width="300"></p> 
<h4 id="3.2%C2%A0%C2%A0CLIPEncoderLayer">3.2  CLIPEncoderLayer</h4> 
<p>CLIPEncoderLayer就是普通的Transformer的Encoder层了</p> 
<p class="img-center"><img alt="" height="745" src="https://images2.imgbox.com/5a/24/SjnSRJ8n_o.png" width="350"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/817e6627271658dd71149a1c25b7b6ba/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【腾讯云 HAI域探秘】——即时职场生存指南小游戏以及【自行搭建Stable Diffusion图片AI绘制 | ChatGLM2-6B AI进行智能对话 | Pytorch2.0 AI框架视频处理】</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/af09d3ad8d7eacfcc504ea08eb60c49b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">语音识别神器 Whisper 的几个小技巧</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>