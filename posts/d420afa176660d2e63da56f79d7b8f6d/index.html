<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于PyTorch学AI——Dataset与DataLoader - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/d420afa176660d2e63da56f79d7b8f6d/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="基于PyTorch学AI——Dataset与DataLoader">
  <meta property="og:description" content="概况 训练模型离不开数据，PyTorch通过Dataset和DataLoader两个类，提供了灵活且高效的数据读取机制，实现了数据集代码与模型训练代码的解耦。
Dataset数据集负责处理单样本及其相应的标签，既可以使用内置于Pytorch的数据集，也可以使用自己的数据集。
DataLoader在数据集周围包装了一个可迭代项，进一步为模型训练提供了相应的功能。
Dataset Dataset类似一个字典，负责处理索引(index)到样本(sample)的映射。
Dataset可以对样本数据进行预处理，并利用getitem方法返回一个样本。
Dataset有两种类型：map-style datasets和iterable-style datasets。
其中，map-style datasets是实现__getitem__（）和__len__（）协议的数据集，表示idx/key到数据样本的map。该类型数据集使用dataset[idx]访问，返回索引为idx的sample及其标签。
iterable-style datasets是实现__iter_（）协议的IterableDataset的子类的实例，可在数据样本上迭代。这种类型的数据集用于不适合随机读取的情况，以及批量大小取决于提取的数据的情况。这种数据集通过iter(dataset)读取。
简单看下Dataset类的源码，由于是抽象类，官方实现的很简单，只定义了两个方法。
Dataset是抽象类，使用者根据自己的需求实现一个子类，需要实现以下3个方法：
init()：初始化方法。getitem()：基于index获取数据集的一个sample，包括data和label。len()：返回数据集的长度。 举一个Dataset的极简例子：
from torch.utils.data import Dataset class MyDataset(Dataset): def __init__(self, data, labels): self.x = data self.y = labels def __len__(self): return len(self.x) def __getitem__(self, index): return self.x[index], self.y[index] DataLoader DataLoader提供了数据的批量加载、多线程/进程加载、数据打乱等常用功能。
DataLoader类的实现细节较多，后面单独一节详细了解。
举一个DataLoader的极简例子：
from torch.utils.data import DataLoader # 创建dataset dataset = MyDataset(data, targets) # 创建Dataloader dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2) # 使用Dataloader加载数据 for batch_x, batch_y in dataloader: # 在这里进行模型的训练或验证操作 pass 加载一个Dataset 官方文档中举了一个从TorchVision加载Fashion MNIST数据集的示例。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-20T12:20:59+08:00">
    <meta property="article:modified_time" content="2024-06-20T12:20:59+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于PyTorch学AI——Dataset与DataLoader</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_1"></a>概况</h2> 
<p>训练模型离不开数据，PyTorch通过Dataset和DataLoader两个类，提供了灵活且高效的数据读取机制，实现了数据集代码与模型训练代码的解耦。<br> Dataset数据集负责处理单样本及其相应的标签，既可以使用内置于Pytorch的数据集，也可以使用自己的数据集。<br> DataLoader在数据集周围包装了一个可迭代项，进一步为模型训练提供了相应的功能。</p> 
<h2><a id="Dataset_5"></a>Dataset</h2> 
<p>Dataset类似一个字典，负责处理索引(index)到样本(sample)的映射。<br> Dataset可以对样本数据进行预处理，并利用getitem方法返回一个样本。</p> 
<p>Dataset有两种类型：map-style datasets和iterable-style datasets。<br> 其中，map-style datasets是实现__getitem__（）和__len__（）协议的数据集，表示idx/key到数据样本的map。该类型数据集使用dataset[idx]访问，返回索引为idx的sample及其标签。<br> iterable-style datasets是实现__iter_（）协议的IterableDataset的子类的实例，可在数据样本上迭代。这种类型的数据集用于不适合随机读取的情况，以及批量大小取决于提取的数据的情况。这种数据集通过iter(dataset)读取。</p> 
<p>简单看下Dataset类的源码，由于是抽象类，官方实现的很简单，只定义了两个方法。<br> <img src="https://images2.imgbox.com/5d/14/TKxfMdRP_o.png" alt="在这里插入图片描述"><br> Dataset是抽象类，使用者根据自己的需求实现一个子类，需要实现以下3个方法：</p> 
<ol><li><strong>init</strong>()：初始化方法。</li><li><strong>getitem</strong>()：基于index获取数据集的一个sample，包括data和label。</li><li><strong>len</strong>()：返回数据集的长度。</li></ol> 
<p>举一个Dataset的极简例子：</p> 
<pre><code>from torch.utils.data import Dataset

class MyDataset(Dataset):
    def __init__(self, data, labels):
        self.x = data
        self.y = labels

    def __len__(self):
        return len(self.x)

    def __getitem__(self, index):
        return self.x[index], self.y[index]
</code></pre> 
<h2><a id="DataLoader_36"></a>DataLoader</h2> 
<p>DataLoader提供了数据的批量加载、多线程/进程加载、数据打乱等常用功能。<br> DataLoader类的实现细节较多，后面单独一节详细了解。</p> 
<p>举一个DataLoader的极简例子：</p> 
<pre><code>from torch.utils.data import DataLoader

# 创建dataset
dataset = MyDataset(data, targets)

# 创建Dataloader
dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2)

# 使用Dataloader加载数据
for batch_x, batch_y in dataloader:
    # 在这里进行模型的训练或验证操作
    pass
</code></pre> 
<h2><a id="Dataset_55"></a>加载一个Dataset</h2> 
<p>官方文档中举了一个从TorchVision加载Fashion MNIST数据集的示例。<br> Fashion MNIST是Zalando文章图像的数据集，由60000个训练示例和10000个测试示例组成。每个示例包括一个28×28灰度图像和来自10个类别之一的相关标签。</p> 
<pre><code>import torch
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt


training_data = datasets.FashionMNIST(
    root="data",
    train=True,
    download=True,
    transform=ToTensor()
)

test_data = datasets.FashionMNIST(
    root="data",
    train=False,
    download=True,
    transform=ToTensor()
)
</code></pre> 
<p>相关参数如下：</p> 
<ul><li>root：数据文件的路径</li><li>train：指定是训练数据集还是测试数据集</li><li>download=True：如果不指定root，是否自动下载数据</li><li>transform和target_transform：指定特征和标签的转换函数</li></ul> 
<p>对于加载到Dataset的数据，可以通过index提取数据，也可以利用matplotlib可视化。</p> 
<pre><code>labels_map = {
    0: "T-Shirt",
    1: "Trouser",
    2: "Pullover",
    3: "Dress",
    4: "Coat",
    5: "Sandal",
    6: "Shirt",
    7: "Sneaker",
    8: "Bag",
    9: "Ankle Boot",
}
figure = plt.figure(figsize=(8, 8))
cols, rows = 3, 3
for i in range(1, cols * rows + 1):
    sample_idx = torch.randint(len(training_data), size=(1,)).item()
    img, label = training_data[sample_idx]
    figure.add_subplot(rows, cols, i)
    plt.title(labels_map[label])
    plt.axis("off")
    plt.imshow(img.squeeze(), cmap="gray")
plt.show()
</code></pre> 
<p>输出如下图：<br> <img src="https://images2.imgbox.com/6a/87/6oh6YcgB_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="Dataset_115"></a>自定义Dataset</h2> 
<p>下面代码自定义CustomImageDataset，通过本地文件加载Dataset，其中，图片数据存储在img_dir目录，标签数据存储在CSV文件：annotations_file。</p> 
<pre><code>import os
import pandas as pd
from torchvision.io import read_image

class CustomImageDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = read_image(img_path)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label
</code></pre> 
<p>上述代码唯一值得一提的是Dataset通过transform和target_transform两个方法，处理样本数据和标签数据，默认是none。<br> getitem方法中，image和label数据在返回之前，分别调用这两个方法进行了处理。<br> 这是常用的封装技巧，给外围调用者提供类似回调机制，方便调用者有机会对数据进行自定义处理。</p> 
<h2><a id="DataLoader_148"></a>轮到DataLoader登场了</h2> 
<p>Dataset的主要任务是处理单个样本，但在实际训练的时候肯定不能一条一条数据的训练，而是一批一批的训练，包括每轮训练完后是否需要打乱（reshuffle ）再训练下一轮，另外为了提高训练效率有可能还需要考虑多进程，诸如此类的功能，都封装在DataLoader类解决。</p> 
<p>创建DataLoader代码：</p> 
<pre><code>from torch.utils.data import DataLoader

train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)
</code></pre> 
<p>创建DataLoader的时候，需要传入Dataset对象，并指定batch的大小，以及是否需要reshuffle。<br> test数据一般不需要reshuffle。</p> 
<p>一旦我们把Dataset加载到DataLoader中，就可以根据需要遍历Dataset。</p> 
<pre><code># Display image and label.
train_features, train_labels = next(iter(train_dataloader))
print(f"Feature batch shape: {train_features.size()}")
print(f"Labels batch shape: {train_labels.size()}")
img = train_features[0].squeeze()
label = train_labels[0]
plt.imshow(img, cmap="gray")
plt.show()
print(f"Label: {label}")
</code></pre> 
<p>通过iter函数返回迭代器，然后传给next函数，按批次返回样本数据和标签。<br> 上述代码打印效果如下：<br> <img src="https://images2.imgbox.com/6c/7e/TzaiV6k6_o.png" alt="在这里插入图片描述"><br> 下面就一步一步解析DataLoader源码，看看内部是如何实现这个过程的。</p> 
<h2><a id="DataLoader_177"></a>DataLoader源码解析</h2> 
<h3><a id="__init___178"></a>__init__方法</h3> 
<p>先看看DataLoader的源码中的__init__方法。<br> 首先对参数进行校验并赋值给属性。<br> <img src="https://images2.imgbox.com/4d/6f/rIOjLjjn_o.png" alt="在这里插入图片描述"></p> 
<ul><li>dataset： 要传入的Dataset实例，也就是待训练的数据。</li><li>batch_size：批次大小，默认为1。</li><li>shuffle：每轮训练后，是否打乱数据。</li><li>sampler：如何对数据进行采样，可以自定义。</li><li>batch_sampler：一次返回一批样本。</li><li>num_workers：进程数，默认为0，也就是单进程。</li><li>collate_fn：聚集函数，可以对一个batch的样本进行后处理。</li><li>pin_memory：是否在GPU中执行。</li><li>drop_last： 如果总样本数据不能被batch_size整除，最后剩下的样本是否丢弃。默认为false。</li></ul> 
<p>获取样本的方式有多种，可以以默认的shuffle的方式，由官方定义的采样方法获取样本，也可以以自定义sample或batch_sampler的方式获取样本，两种方式二选一。<br> 看以下源码：</p> 
<pre><code>        if sampler is not None and shuffle:
            raise ValueError('sampler option is mutually exclusive with '
                             'shuffle')
</code></pre> 
<p>可以看出，如果同时指定了sample参数和shuffle参数，直接报错，两个参数是互斥的。</p> 
<p>同理，batch_sampler也有类似的逻辑。</p> 
<pre><code>        if batch_sampler is not None:
            # auto_collation with custom batch_sampler
            if batch_size != 1 or shuffle or sampler is not None or drop_last:
                raise ValueError('batch_sampler option is mutually exclusive '
                                 'with batch_size, shuffle, sampler, and '
                                 'drop_last')
            batch_size = None
            drop_last = False
        elif batch_size is None:
            # no auto_collation
            if drop_last:
                raise ValueError('batch_size=None option disables auto-batching '
                                 'and is mutually exclusive with drop_last')
</code></pre> 
<p>如果设置了batch_sampler ，就不需要设置batch_size 、shuffle，sampler、drop_last，否则直接报错，相当于batch_sampler就把所有问题都解决了。另外，<br> 如果没有设置batch_size且drop_last为true，也会报错，很好理解，既然不用批次，就不会有drop_last的问题。</p> 
<p>继续：</p> 
<pre><code>        if sampler is None:  # give default samplers
            if self._dataset_kind == _DatasetKind.Iterable:
                # See NOTE [ Custom Samplers and IterableDataset ]
                sampler = _InfiniteConstantSampler()
            else:  # map-style
                if shuffle:
                    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]
                else:
                    sampler = SequentialSampler(dataset)  # type: ignore[arg-type]
</code></pre> 
<p>如果没有指定sampler参数，则使用内置的采样器。<br> 首先判断dataset类型是iter还是map，对于iter采用内置的_InfiniteConstantSampler采样器，对于map类型，如果shuffle为true，则使用内置的随机采样器RandomSampler，否则内置的序列采样器SequentialSampler，也就是按照原来的顺序采样。</p> 
<hr> 
<p>这里插入一点细节，了解这两个类的实现。</p> 
<h4><a id="RandomSampler_237"></a>RandomSampler实现</h4> 
<p>init方法：</p> 
<pre><code>    def __init__(self, data_source: Sized, replacement: bool = False,
                 num_samples: Optional[int] = None, generator=None) -&gt; None:
        self.data_source = data_source
        self.replacement = replacement
        self._num_samples = num_samples
        self.generator = generator
        ...
</code></pre> 
<ul><li>data_source (Dataset): 样本数据源</li><li>replacement (bool): 样本是否按需替换</li><li>num_samples (int): 抽取样本数</li><li>generator (Generator): 用于样本抽取的方法</li></ul> 
<pre><code>  def __iter__(self) -&gt; Iterator[int]:
        n = len(self.data_source)
        if self.generator is None: # 如果没有指定generator，则用随机种子抽取数据
            seed = int(torch.empty((), dtype=torch.int64).random_().item())
            generator = torch.Generator()
            generator.manual_seed(seed)
        else:
            generator = self.generator

        if self.replacement:
            for _ in range(self.num_samples // 32):
                yield from torch.randint(high=n, size=(32,), dtype=torch.int64, generator=generator).tolist()
            yield from torch.randint(high=n, size=(self.num_samples % 32,), dtype=torch.int64, generator=generator).tolist()
        else:
            for _ in range(self.num_samples // n):
                yield from torch.randperm(n, generator=generator).tolist()
            yield from torch.randperm(n, generator=generator).tolist()[:self.num_samples % n]
</code></pre> 
<p>看返回，通过torch.randperm方法返回n个索引的随机排列，达到随机的效果。</p> 
<h4><a id="SequentialSampler_274"></a>SequentialSampler实现</h4> 
<p>这个代码很简单。</p> 
<pre><code>    def __iter__(self) -&gt; Iterator[int]:
        return iter(range(len(self.data_source)))
</code></pre> 
<p>按照样本原有的顺序抽取数据。</p> 
<hr> 
<p>细节插入结束。<br> 回到DataLoader的源码。</p> 
<pre><code>    if batch_size is not None and batch_sampler is None:
            # auto_collation without custom batch_sampler
            batch_sampler = BatchSampler(sampler, batch_size, drop_last)
</code></pre> 
<p>批量采样用到了BatchSampler类，再次插入该类的介绍。</p> 
<hr> 
<h4><a id="BatchSampler_295"></a>BatchSampler实现</h4> 
<p>直接看代码+注释。</p> 
<pre><code>    def __iter__(self) -&gt; Iterator[List[int]]:
        # Implemented based on the benchmarking in https://github.com/pytorch/pytorch/pull/76951
        if self.drop_last:  # 如果drop_last为true，不需要考虑最后一个批次的问题
            sampler_iter = iter(self.sampler)
            while True:
                try:
                    batch = [next(sampler_iter) for _ in range(self.batch_size)]
                    yield batch
                except StopIteration: # except说明不够一个batch_size，直接break，抛弃最后小部分数据
                    break
        else:
            batch = [0] * self.batch_size # 用 0 初始化batch_size 个元素的数组
            idx_in_batch = 0  # 利用该变量记录已采样的批次样本数
            for idx in self.sampler:
                batch[idx_in_batch] = idx  # 实际返回的还是idx数组
                idx_in_batch += 1
                if idx_in_batch == self.batch_size: 
                    yield batch # 达到批次数量，返回
                    idx_in_batch = 0  # 清零已采样数
                    batch = [0] * self.batch_size  # 重新初始化batch数组
            if idx_in_batch &gt; 0:
                yield batch[:idx_in_batch]   # 最后遗留的部分数据，单独返回
</code></pre> 
<hr> 
<p>再次回到DataLoader类。</p> 
<pre><code>        if collate_fn is None:
            if self._auto_collation:
                collate_fn = _utils.collate.default_collate
            else:
                collate_fn = _utils.collate.default_convert
</code></pre> 
<p>根据_auto_collation决定使用那个collate函数。</p> 
<pre><code>   @property
    def _auto_collation(self):
        return self.batch_sampler is not None
</code></pre> 
<p>如果设置了batch_sampler，则_auto_collation为true。<br> 通过查看default_collate源码，可以看到其内部对数据做了校验并返回，本质上没有太多有价值的功能。</p> 
<p>总结一下DataLoader的init方法，主要完成了以下功能：</p> 
<ul><li>校验参数并给属性赋值</li><li>构建sampler对象，用于采集数据</li><li>构建collate方法，用于样本数据后处理</li></ul> 
<h3><a id="__iter___346"></a>__iter__方法</h3> 
<p>DataLoader实现了__iter__方法，可以实现迭代器调用。</p> 
<pre><code>    def __iter__(self) -&gt; '_BaseDataLoaderIter':
        # When using a single worker the returned iterator should be
        # created everytime to avoid resetting its state
        # However, in the case of a multiple workers iterator
        # the iterator is only created once in the lifetime of the
        # DataLoader object so that workers can be reused
        if self.persistent_workers and self.num_workers &gt; 0:
            if self._iterator is None:
                self._iterator = self._get_iterator()
            else:
                self._iterator._reset(self)
            return self._iterator
        else:
            return self._get_iterator()
</code></pre> 
<p>该方法的逻辑很简单，调用_get_iterator()方法并返回。</p> 
<pre><code>    def _get_iterator(self) -&gt; '_BaseDataLoaderIter':
        if self.num_workers == 0:
            return _SingleProcessDataLoaderIter(self)
        else:
            self.check_worker_number_rationality()
            return _MultiProcessingDataLoaderIter(self)
</code></pre> 
<p>根据是否多线程，返回DataLoaderIter对象。</p> 
<p>下面以_SingleProcessDataLoaderIter为例，简单了解DataLoaderIter对象。<br> 该类主要作用是创建fetcher对象：</p> 
<pre><code>self._dataset_fetcher = _DatasetKind.create_fetcher(
            self._dataset_kind, self._dataset, self._auto_collation, self._collate_fn, self._drop_last)
</code></pre> 
<p>create_fetcher方法如下：</p> 
<pre><code>    @staticmethod
    def create_fetcher(kind, dataset, auto_collation, collate_fn, drop_last):
        if kind == _DatasetKind.Map:
            return _utils.fetch._MapDatasetFetcher(dataset, auto_collation, collate_fn, drop_last)
        else:
            return _utils.fetch._IterableDatasetFetcher(dataset, auto_collation, collate_fn, drop_last)
</code></pre> 
<p>根据Dataset的类型分别创建fetcher对象。<br> fetcher对象只实现了fatch方法。<br> 例如_MapDatasetFetcher类：</p> 
<pre><code>class _MapDatasetFetcher(_BaseDatasetFetcher):
    def fetch(self, possibly_batched_index):
        if self.auto_collation:
            if hasattr(self.dataset, "__getitems__") and self.dataset.__getitems__:
                data = self.dataset.__getitems__(possibly_batched_index)
            else:
                data = [self.dataset[idx] for idx in possibly_batched_index]
        else:
            data = self.dataset[possibly_batched_index]
        return self.collate_fn(data)
</code></pre> 
<p>上面的代码逻辑很清晰，就是根据不同情况获取dataset的样本数据。</p> 
<p>再次回到_SingleProcessDataLoaderIter类，还有个关键方法：_next_data。</p> 
<pre><code>    def _next_data(self):
        index = self._next_index()  # may raise StopIteration
        data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
        if self._pin_memory:
            data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)
        return data
</code></pre> 
<p>该方法在哪里调用的呢？<br> _SingleProcessDataLoaderIter基类_BaseDataLoaderIter的__next__方法：</p> 
<pre><code>    def __next__(self) -&gt; Any:
        with torch.autograd.profiler.record_function(self._profile_name):
            if self._sampler_iter is None:
                # TODO(https://github.com/pytorch/pytorch/issues/76750)
                self._reset()  # type: ignore[call-arg]
            data = self._next_data()  # 在这里！！！
            ......
</code></pre> 
<p>通过以上的逻辑，整个逻辑全通了！<br> 或者，全乱了~~~</p> 
<p>还记得通过Dataloader获取数据的代码吗？</p> 
<pre><code>train_features, train_labels = next(iter(train_dataloader))
</code></pre> 
<p>总结一下，整个流程就是通过__iter__ 和__next__ 两个魔法方法实现，然后通过next(iter(train_dataloader))这种形式优雅的串联了数据采样流程。</p> 
<h2><a id="_438"></a>总结</h2> 
<p>本文总结了Dataset和DataLoader两个核心类，是模型训练绕不开的基础类，希望阅读本文能带来收获。</p> 
<p>另外，阅读源码确实就像盗梦空间的层层梦境一样，不知道这种行文方式是否方便大家阅读，有什么好的建议欢迎留言。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/95314eeaa78e2c7695f7b7f9f5a32f84/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">性能巅峰对决：Rust vs C&#43;&#43; —— 速度、安全与权衡的艺术</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6de5652266f3b063276e10617480a84b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Android】构建 Android Automotive OS：适合初学者的指南</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>