<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>OpenAI、微软、智谱AI 等全球 16 家公司共同签署前沿人工智能安全承诺 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/f9033084aab4ef3bce8f3c2065e46c46/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="OpenAI、微软、智谱AI 等全球 16 家公司共同签署前沿人工智能安全承诺">
  <meta property="og:description" content="人工智能（AI）的安全问题，正以前所未有的关注度在全球范围内被讨论。
日前，OpenAI 联合创始人、首席科学家 Ilya Sutskever 与 OpenAI 超级对齐团队共同领导人 Jan Leike 相继离开 OpenAI，Leike 甚至在 X 发布了一系列帖子，称 OpenAI 及其领导层忽视安全而偏爱光鲜亮丽的产品。这在业界引起了广泛关注，在一定程度上凸显了当前 AI 安全问题的严峻性。
5 月 21 日，图灵奖得主 Yoshua Bengio、Geoffrey Hinton 和姚期智联合国内外数十位业内专家和学者，在权威科学期刊 Science 上刊文，呼吁世界各国领导人针对 AI 风险采取更有力的行动，并警告说，“近六个月所取得的进展还不够”。
他们认为，AI 的无节制发展很有可能最终导致生命和生物圈的大规模损失，以及人类的边缘化或灭绝。（点击查看详情）
在他们看来，AI 模型的安全问题，已经上升到足够威胁人类未来生存的水平。
同样，AI 模型的安全问题，也已经是可以影响每一个人、每一个人都有必要关心的话题。
**5 月 22 日，注定是人工智能史上的一个重大时刻：**OpenAI、谷歌、微软和智谱AI 等来自不同国家和地区的公司共同签署了前沿人工智能安全承诺（Frontier AI Safety Commitments）；欧盟理事会正式批准了《人工智能法案》（AI Act），全球首部 AI 全面监管法规即将生效。
再一次，AI 的安全问题在政策层面被提及。
人工智能首尔峰会“宣言”
在以“安全、创新、包容”为议题的“人工智能首尔峰会”（AI Seoul Summit）上，来自北美、亚洲、欧洲和中东地区的 16 家公司就 AI 开发的安全承诺达成一致，共同签署了前沿人工智能安全承诺，包括以下要点：
确保前沿 AI 安全的负责任治理结构和透明度；
基于人工智能安全框架，负责任地说明将如何衡量前沿 AI 模型的风险；
建立前沿 AI 安全模型风险缓解机制的明确流程。
图灵奖得主 Yoshua Bengio 认为，前沿人工智能安全承诺的签署“标志着在建立国际治理制度以促进人工智能安全方面迈出了重要一步”。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-23T22:04:59+08:00">
    <meta property="article:modified_time" content="2024-05-23T22:04:59+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">OpenAI、微软、智谱AI 等全球 16 家公司共同签署前沿人工智能安全承诺</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>人工智能（AI）的安全问题，正以前所未有的关注度在全球范围内被讨论。</p> 
<p>日前，OpenAI 联合创始人、首席科学家 Ilya Sutskever 与 OpenAI 超级对齐团队共同领导人 Jan Leike 相继离开 OpenAI，Leike 甚至在 X 发布了一系列帖子，称 OpenAI 及其领导层忽视安全而偏爱光鲜亮丽的产品。这在业界引起了广泛关注，<strong>在一定程度上凸显了当前 AI 安全问题的严峻性</strong>。</p> 
<p>5 月 21 日，图灵奖得主 Yoshua Bengio、Geoffrey Hinton 和姚期智联合国内外数十位业内专家和学者，在权威科学期刊 Science 上刊文，呼吁世界各国领导人针对 AI 风险采取更有力的行动，并警告说，<strong>“近六个月所取得的进展还不够”</strong>。</p> 
<p><img src="https://images2.imgbox.com/01/47/KLAfIqFT_o.png" alt="图片"></p> 
<p>他们认为，<strong>AI 的无节制发展很有可能最终导致生命和生物圈的大规模损失，以及人类的边缘化或灭绝</strong>。<a href="https://mp.weixin.qq.com/s?__biz=Mzg4MDE3OTA5NA==&amp;mid=2247586440&amp;idx=1&amp;sn=dbae31a2f3d508d4f393721894a2a288&amp;scene=21#wechat_redirect" rel="nofollow">（点击查看详情）</a></p> 
<p>在他们看来，AI 模型的安全问题，已经上升到足够威胁人类未来生存的水平。</p> 
<p>同样，AI 模型的安全问题，也已经是可以影响每一个人、每一个人都有必要关心的话题。</p> 
<p>**5 月 22 日，注定是人工智能史上的一个重大时刻：**OpenAI、谷歌、微软和智谱AI 等来自不同国家和地区的公司共同签署了前沿人工智能安全承诺（Frontier AI Safety Commitments）；欧盟理事会正式批准了《人工智能法案》（AI Act），全球首部 AI 全面监管法规即将生效。</p> 
<p>再一次，AI 的安全问题在政策层面被提及。</p> 
<p><strong>人工智能首尔峰会“宣言”</strong></p> 
<p>在以“安全、创新、包容”为议题的“人工智能首尔峰会”（AI Seoul Summit）上，来自北美、亚洲、欧洲和中东地区的 16 家公司就 AI 开发的安全承诺达成一致，共同签署了前沿人工智能安全承诺，包括以下要点：</p> 
<ul><li> <p>确保前沿 AI 安全的负责任治理结构和透明度；</p> </li><li> <p>基于人工智能安全框架，负责任地说明将如何衡量前沿 AI 模型的风险；</p> </li><li> <p>建立前沿 AI 安全模型风险缓解机制的明确流程。</p> </li></ul> 
<p>图灵奖得主 Yoshua Bengio 认为，前沿人工智能安全承诺的签署“<strong>标志着在建立国际治理制度以促进人工智能安全方面迈出了重要一步</strong>”。</p> 
<p>作为来自中国的大模型公司，智谱 AI 也签署了这一新的前沿人工智能安全承诺，完整签署方名单如下：</p> 
<p><img src="https://images2.imgbox.com/ba/c7/QzEyBDLu_o.jpg" alt="图片"></p> 
<p>对此，OpenAI 全球事务副总裁 Anna Makanju 表示，“前沿人工智能安全承诺是促进更广泛地实施先进 AI 系统安全实践的重要一步。” Google DeepMind 总法律顾问兼治理主管 Tom Lue 说道，“这些承诺将有助于在领先开发者之间建立重要的前沿 AI 安全最佳实践。” 智谱AI 首席执行官张鹏表示，“伴随着先进技术而来的是确保 AI 安全的重要责任。”</p> 
<p>日前，智谱AI 也受邀亮相 AI 顶会 ICLR 2024，并在题为“The ChatGLM’s Road to AGI”的主旨演讲中分享了他们针对 AI 安全的具体做法。</p> 
<p>他们认为，超级对齐（Superalignment）技术将协助提升大模型的安全性，并已经启动了类似 OpenAI 的 Superalignment 计划，希望让机器学会自己学习、自己判断，从而实现学习安全的内容。</p> 
<p><img src="https://images2.imgbox.com/9a/1f/SbqjZ3yW_o.png" alt="图片"></p> 
<p>他们透露，GLM-4V 即内置了这些安全措施，以防止产生有害或不道德的行为，同时保护用户隐私和数据安全；而 GLM-4 的后续升级版本即 GLM-4.5 及其升级模型，也应当基于超级智能（Superintelligence）和超级对齐技术。</p> 
<p>我们也发现，在一篇近期发表的论文中，智谱AI、清华团队介绍了一种通过利用大量自生成的否定词而实现的无反馈（feedback-free）大型语言模型对齐方法——Self-Contrast。</p> 
<p>据论文描述，在只有监督微调（SFT）目标的情况下，Self-Contrast 就可以利用 LLM 本身生成大量不同的候选词，并利用预先训练的嵌入模型根据文本相似性过滤多个否定词。</p> 
<p><img src="https://images2.imgbox.com/81/a8/E5lmbleS_o.png" alt="图片"></p> 
<p>论文链接：https://arxiv.org/abs/2404.00604</p> 
<p>在三个数据集上进行的直接偏好优化（DPO）实验表明，Self-Contrast 可以持续大幅超越 SFT 和标准 DPO 训练。而且，随着自生成的负样本数量增加，Self-Contrast 的表现也在不断提高。</p> 
<p><img src="https://images2.imgbox.com/f4/6f/oIN6gikQ_o.png" alt="图片"></p> 
<p>总的来说，这一研究为偏好数据缺失情况下的对齐（如 RLHF 方法）提供了一种新的方法。在偏好数据标注代价昂贵且难以获得的情况下，可以利用未标注的 SFT 数据构建语法偏好数据，通过增加负样本的数量来弥补因正样本不足造成的性能损失。</p> 
<p><strong>欧盟理事会正式批准《人工智能法案》</strong></p> 
<p>同日，欧盟理事会也于同日正式批准了《人工智能法案》（AI Act），这是全球首部 AI 全面监管法规，<strong>这一具有里程碑意义的人工智能法规将于下月生效</strong>，目前仅适用于欧盟法律范围内的领域，或将为商业和日常生活中使用的技术设定一个潜在的全球基准。</p> 
<p>“这部具有里程碑意义的法规是世界上第一部此类法规，它解决了一个全球性的技术挑战，同时也为我们的社会和经济创造了机遇，” 比利时数字化大臣 Mathieu Michel 在一份声明中说。</p> 
<p>这一综合性的 AI 立法采用“基于风险”的方法，意味着<strong>对社会造成伤害的风险越高，规则就越严格</strong>。例如，不构成系统性风险的通用目的 AI 模型将承担一些有限的要求，但那些具有系统性风险的则需要遵守更严格的规定。</p> 
<p>对违反《人工智能法案》中行为的罚款，该法案设定为违规公司前一个财年全球年营业额的百分比或预定的金额，以较高者为准。</p> 
<p>如今，无论是小到科技公司，还是大到政府机构，都已经将预防、解决 AI 安全问题提上日程。正如牛津大学工程科学系教授 Philip Torr 所言：</p> 
<p>“在上一次人工智能峰会上，全世界一致认为我们需要采取行动，但<strong>现在是时候从模糊的建议转变为具体的承诺了</strong>。”</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/98d3aaaea65ad5a6be0da00678916427/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">centos7上安装MySQL并配置Hive</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/38375a1eeaf0acc3e5d4bf565d388dbf/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">动态地控制kafka的消费速度，从而满足业务要求</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>