<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Llama网络结构介绍 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/6bc81ef8fd03b413dbed63a44b8d7ce9/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Llama网络结构介绍">
  <meta property="og:description" content="LLaMA现在已经是开源社区里炙手可热的模型了，但是原文中仅仅介绍了其和标准Transformer的差别，并没有一个全局的模型介绍。因此打算写篇文章，争取让读者不参考任何其他资料把LLaMA的模型搞懂。
结构 如图所示为LLaMA的示意图，由Attention和MLP层堆叠而成
LLaMA模型主要由Attention和MLP层堆叠而成，具有以下特点：
1、前置的RMSNorm：RMSNorm是一种归一化技术，用于稳定模型的训练过程，提高模型的收敛速度。
2、Q、K上的RoPE旋转式位置编码：位置编码用于捕捉序列中的位置信息，RoPE旋转式位置编码能够有效地处理长序列，提高模型的性能。
3、Causal mask：该机制保证每个位置只能看到前面的tokens，确保了模型的自回归性质。
4、使用了Group Query Attention：通过使用分组查询注意力（GQA），LLaMA能够在保持性能的同时，降低模型的计算复杂度，提高推理速度。
5、MLP表达式：down(up(x) * SILU(gate(x)))，其中down, up, gate都是线性层
LLaMA各个不同大小的结构设置如下表所示。其中最大的65B的LLaMA用了2048张80GB的A100，batch size为4百万，训练一次需要21天。
Group Query Attention(V2 only) 自回归模型生成回答时，需要前面生成的KV缓存起来，来加速计算。多头注意力机制(MHA)需要的缓存量很大，Multi-Query Attention指出多个头之间可以共享KV对。Group Query Attention没有像MQA一样极端，将query分组，组内共享KV，效果接近MHA，速度上与MQA可比较。p.s. 这个技术falcon已经用上了，当时falcon说自己用的是multi query attention，因为当group=1时，GQA和MQA是等价的。falcon支持设置不同的G。
RMSNorm 这是在BERT、GPT等模型中广泛使用的LayerNorm：
RMSNorm(root mean square)发现LayerNorm的中心偏移没什么用(减去均值等操作)。将其去掉之后，效果几乎不变，但是速度提升了40%。最终公式为：
注意除了没有减均值，加偏置以外，分母上求的RMS而不是方差。
LLaMA在 Attention Layer和MLP的输入上使用了RMSNorm，相比在输出上使用，训练会更加稳定。
SwiGLU LLaMA没有使用ReLU，而是使用了SwiGLU，有时也被称为SiLU。公式为：
，效果类似平滑版的ReLU：
RoPE LLaMA使用了Rotary Position Embedding。对于Q的第m个位置向量q，通过以下方法注入位置编码：
class LlamaRotaryEmbedding(torch.nn.Module): def __init__(self, dim, max_position_embeddings=2048, base=10000): super().__init__() theta = 1.0 / (base ** (torch.arange(0, dim, 2) / dim)) t = torch.arange(max_position_mbeddings) freqs = torch.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-23T17:30:06+08:00">
    <meta property="article:modified_time" content="2024-04-23T17:30:06+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Llama网络结构介绍</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>LLaMA现在已经是开源社区里炙手可热的模型了，但是原文中仅仅介绍了其和标准Transformer的差别，并没有一个全局的模型介绍。因此打算写篇文章，争取让读者不参考任何其他资料把LLaMA的模型搞懂。</p> 
<h3><a id="_1"></a>结构</h3> 
<p>如图所示为LLaMA的示意图，由Attention和MLP层堆叠而成<br> <img src="https://images2.imgbox.com/3e/d1/JGmKtVP7_o.png" alt="在这里插入图片描述"><br> LLaMA模型主要由Attention和MLP层堆叠而成，具有以下特点：<br> 1、前置的RMSNorm：RMSNorm是一种归一化技术，用于稳定模型的训练过程，提高模型的收敛速度。<br> 2、Q、K上的RoPE旋转式位置编码：位置编码用于捕捉序列中的位置信息，RoPE旋转式位置编码能够有效地处理长序列，提高模型的性能。<br> 3、Causal mask：该机制保证每个位置只能看到前面的tokens，确保了模型的自回归性质。<br> 4、使用了Group Query Attention：通过使用分组查询注意力（GQA），LLaMA能够在保持性能的同时，降低模型的计算复杂度，提高推理速度。<br> 5、MLP表达式：down(up(x) * SILU(gate(x)))，其中down, up, gate都是线性层<br> LLaMA各个不同大小的结构设置如下表所示。其中最大的65B的LLaMA用了2048张80GB的A100，batch size为4百万，训练一次需要21天。</p> 
<h3><a id="Group_Query_AttentionV2_only_12"></a>Group Query Attention(V2 only)</h3> 
<p>自回归模型生成回答时，需要前面生成的KV缓存起来，来加速计算。多头注意力机制(MHA)需要的缓存量很大，Multi-Query Attention指出多个头之间可以共享KV对。Group Query Attention没有像MQA一样极端，将query分组，组内共享KV，效果接近MHA，速度上与MQA可比较。p.s. 这个技术falcon已经用上了，当时falcon说自己用的是multi query attention，因为当group=1时，GQA和MQA是等价的。falcon支持设置不同的G。<br> <img src="https://images2.imgbox.com/db/93/EnGEP48j_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="RMSNorm_15"></a>RMSNorm</h3> 
<p>这是在BERT、GPT等模型中广泛使用的LayerNorm：<br> <img src="https://images2.imgbox.com/83/74/39cqG5PT_o.png" alt="在这里插入图片描述"><br> RMSNorm(root mean square)发现LayerNorm的中心偏移没什么用(减去均值等操作)。将其去掉之后，效果几乎不变，但是速度提升了40%。最终公式为：<br> <img src="https://images2.imgbox.com/1c/9e/lia4ZZHP_o.png" alt="在这里插入图片描述"><br> 注意除了没有减均值，加偏置以外，分母上求的RMS而不是方差。</p> 
<p>LLaMA在 Attention Layer和MLP的输入上使用了RMSNorm，相比在输出上使用，训练会更加稳定。</p> 
<h3><a id="SwiGLU_24"></a>SwiGLU</h3> 
<p>LLaMA没有使用ReLU，而是使用了SwiGLU，有时也被称为SiLU。公式为：<br> ，效果类似平滑版的ReLU：<br> <img src="https://images2.imgbox.com/ef/c4/k3KuXtAe_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="RoPE_29"></a>RoPE</h3> 
<p>LLaMA使用了Rotary Position Embedding。对于Q的第m个位置向量q，通过以下方法注入位置编码：<br> <img src="https://images2.imgbox.com/ec/a1/s45hhqQJ_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">LlamaRotaryEmbedding</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> max_position_embeddings<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> base<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        theta <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span>base <span class="token operator">**</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> dim<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">/</span> dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        t <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>max_position_mbeddings<span class="token punctuation">)</span>
        freqs <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"i,j-&gt;ij"</span><span class="token punctuation">,</span> t<span class="token punctuation">,</span> theta<span class="token punctuation">)</span>

        emb <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>freqs<span class="token punctuation">,</span> freqs<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"cos_cached"</span><span class="token punctuation">,</span> emb<span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"sin_cached"</span><span class="token punctuation">,</span> emb<span class="token punctuation">.</span>sin<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> seq_len<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>cos_cached<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>seq_len<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>sin_cached<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>seq_len<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span>

<span class="token comment"># 在LlamaAttention通过以下命令调用：</span>
cos<span class="token punctuation">,</span> sin <span class="token operator">=</span> self<span class="token punctuation">.</span>rotary_emb<span class="token punctuation">(</span>seq_len<span class="token operator">=</span>kv_seq_len<span class="token punctuation">)</span>
</code></pre> 
<p>以下代码将q沿着最后一个维度劈成两半，将后一半乘-1，然后连接在第一半之前，就得到了上式第三项。</p> 
<pre><code class="prism language-python"><span class="token comment"># 在接下来的apply_rotary_pos_emb函数里调用</span>

<span class="token keyword">def</span> <span class="token function">rotate_half</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x1 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">]</span>
    x2 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> <span class="token number">2</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span>x2<span class="token punctuation">,</span> x1<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<p>最后通过以下代码得到结合了位置编码的Q,K(K和Q使用同样的方式进行位置编码)。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">apply_rotary_pos_emb</span><span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> cos<span class="token punctuation">,</span> sin<span class="token punctuation">,</span> position_ids<span class="token punctuation">)</span><span class="token punctuation">:</span>
    q_embed <span class="token operator">=</span> <span class="token punctuation">(</span>q <span class="token operator">*</span> cos<span class="token punctuation">[</span>position_ids<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>rotate_half<span class="token punctuation">(</span>q<span class="token punctuation">)</span> <span class="token operator">*</span> sin<span class="token punctuation">[</span>position_ids<span class="token punctuation">]</span><span class="token punctuation">)</span>
    k_embed <span class="token operator">=</span> <span class="token punctuation">(</span>k <span class="token operator">*</span> cos<span class="token punctuation">[</span>position_ids<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>rotate_half<span class="token punctuation">(</span>k<span class="token punctuation">)</span> <span class="token operator">*</span> sin<span class="token punctuation">[</span>position_ids<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> q_embed<span class="token punctuation">,</span> k_embed

<span class="token comment"># 在LlamaAttention中通过以下命令调用：</span>
query_states<span class="token punctuation">,</span> key_states <span class="token operator">=</span> apply_rotary_pos_emb<span class="token punctuation">(</span>query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">,</span> cos<span class="token punctuation">,</span> sin<span class="token punctuation">,</span> position_ids<span class="token punctuation">)</span>

</code></pre> 
<p>绝对位置编码的优点是计算速度快等，缺点是拓展长度比较麻烦，且绝对位置并没有什么实际意义。而相对位置编码对学习token之间的关系很有意义，比如距离的很远的两个token之间的关联大概率很小，使用相对位置编码往往能够获得更好的效果。此外拓展长度也更容易，因为不论context size多长，只需关注最长距离以内的输入即可。相对位置编码的缺点是没有绝对位置编码计算速度快。</p> 
<p>当我们计算Attention时，RoPE可以变成相对位置编码。<br> <img src="https://images2.imgbox.com/09/91/jr91qqgD_o.png" alt="在这里插入图片描述"><br> 从上面这个公式可以看出，q和k的attention依赖相对距离m-n。因此RoPE为q、k注入的绝对位置编码，计算得到的attention，却变成了相对位置编码。妙的很，我这里为了不参考其他文章就很容易搞懂LLaMA的结构，简化了很多东西，推荐大家看一看RoPE原作者<strong>苏剑林</strong>的博客了解更多信息。</p> 
<p>本文只关注LLaMA缺失的模型结构方面的介绍，对于文章的翻译可以参考其他的文章，<br> 例如：靳伟，LLaMA大模型是如何炼成的，<br> 其他参考文章：https://zhuanlan.zhihu.com/p/636784644<br> 原文：https://arxiv.org/pdf/2302.13971.pdf。<br> 文中参考的代码是huggingface的transformers库实现的版本，并不是Meta官方的代码。<br> 备注说明：受笔者水平限制，如果哪里讲的不对，或者不够清晰易懂，欢迎在评论区与我交流。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c2c4cc7f9c41824949e28e20f72b9d4d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">AI测试干货！实例讲解AI自动生成测试用例</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e16b538247b6da2b0a08148b2fc7fb8d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【机器学习技术系列】FM系列算法详解(FM、FFM、DeepFM)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>