<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Stable Diffusion基础：ControlNet之线稿成图 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/bc646c4fd2ee5b8a01af8db6e30fc5d3/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Stable Diffusion基础：ControlNet之线稿成图">
  <meta property="og:description" content="今天继续给大家分享Stable Diffusiion的基础能力：ControlNet之线稿成图。
所谓线稿就是由一条条的线段组成的图形，主要用于绘画和设计领域的打底稿、表达构想和预见最终效果。
所谓线稿成图就是利用 Stable Diffusion &#43; ControlNet 的能力，依照线稿中的图形设计生成创意图片。借助ControlNet，我们可以直接使用线稿图生成图片，也可以先从其它图片中提取线稿，然后再生成图片。
下面我将介绍ControlNet中四个优秀的线稿模型，以及它们的使用方法和注意事项。
安装ControlNet 工欲善其事必先利其器，ControlNet 还是先要安装好的，已经安装好的请跳过这一步。
使用方法 Canny 基于精细的边缘检测，准确还原图片的结构和特征。Canny边缘图中的线条没有粗细、深浅的区分。
因为是真人图片，所以这里选择了一个现实视觉的大模型：realisticVisionV51，提示词可以手写，也可以找个反推工具反推。
如果是生成真人图片，建议把“面部修复”勾选上。
ControlNet的设置是重点：
1、展开ControlNet控制面板。
2、在Unit 0中上传一张需要提取边缘的图片。
3、勾选“启用”，启用当前ControlNet Unit。
4、勾选“完美匹配像素”，自动设置预处理器的分辨率。
5、勾选“允许预览”，这样就可以预览边缘检测的效果。
6、选择“Canny”
7、预处理器和模型会自动加载，暂时不更改它们。
预处理器可以检测图片中人物和物体的边缘，绘制出边缘图。点击预处理器后边的星火图标可以在预览区看到预处理的效果图。
预处理器器还有两个选项：
无：不使用预处理器，直接在ControlNet中上传一张线稿图。颜色反转：ControlNet中能够处理的线稿图需要线条是白色， 其它区域是黑色，这和现实中的线图颜色是相反的。所以通过这个预处理器，我们可以直接反转一张现实世界的线稿图，供ControlNet模型进行生图处理。不适合普通的彩色图片。 8、控制权重：ControlNet模型在生成图片时的权重，降低这个权重，线稿对绘图的约束就会变弱。
启动控制的步数：Canny介入的时机。介入时机越早，越能保证构图依照边缘图，一般从0开始。
结束控制的步数：Canny退出的时机。退出时机越晚，细节保留程度越大，如果要多变换一些细节，可以提前退出。
9、这两个阈值是canny预处理器提取线稿图时使用的，通过调整阈值可以控制边缘图中保留细节的多少。
Canny Low Threshold：去掉过细的线段。大于低阈值的线段被认定为边缘。
Canny High Threshold：去掉零散的线段。大于高阈值的线段被认定为强边缘，全部保留；高阈值和低阈值之间的线段认定为弱边缘，只保留强边缘相邻的弱边缘。
10、控制模式：ControlNet的通用设置，以提示词为主，还是以ControlNet模型为主。
11、缩放模式：ControlNet的通用设置，参考图与要生成的图片尺寸不一致时如何处理。拉伸有变形的问题，一般使用裁剪和填充。
Lineart 使用线条艺术生成训练，通常用于黑线白底图的上色，也可以从各种图片中提取线条，然后再生成图片。
Lineart的线条有粗细深浅的区别，相比Canny，除了能够控制构图，还可以更好的还原图片深度。
下图的几个设置和上面Canny控制器的设置差不多，只是上传的图片换成了一张黑白线稿图。
重点看下这几个设置：
Lineart：线稿控制网络。
预处理器：从图片提取线稿图的处理器。这里有多个预处理器，简单介绍下：
none：不使用预处理器，需要直接上传一张处理好白线黑景线稿图。lineart_anime：适合从动漫图片中提取线稿图。lineart_anime_denoise：适合从动漫图片中提取线稿图，并去掉噪音点。lineart_coarse：从图片中粗略提取线稿图，忽略不突出的细节，生图时自由度更高。lineart_realistic：从真实视觉的图片中提取线稿图。lineart_standard：从图片提取线稿图的标准版处理器。颜色反转：反色图片，适合从白色背景、黑色线条的图片中提取线稿图。 模型：根据线稿图生成图片的ControlNet模型。
control_v11p_sd15_lineart_fp16：通用线稿图生成模型。control_v11p_sd15s2_lineart_anime_fp16：二次元线稿图生成模型。 SoftEdge 使用软边缘图像生成训练，平滑的边缘、更好的深度，忽略内部细节，方便创作更具绘画特征或艺术风格的图片。
相比Canny、Lineart，SoftEdge除了稳定构图，更好的图片深度控制，还拥有了更多的自由度。
重点看下ControlNet的这几个设置：
预处理器：从图片中提取软边缘图。
softedge_hed：合成柔边图。
softedge_hedsafe：安全的合成柔边图，边缘更清晰，稳定性更高。
softedge_pidinet：简笔柔边图，描绘的细节更少，出图的细节更自由，姿势更稳定。
softedge_pidinet：安全的简笔柔边图，边缘更清晰，稳定性更高。
模型：control_v11p_sd15_softedge_fp16，参与生成图片的软边缘模型，只有这一个。
Scribble 这是一个称之为涂鸦成图的ControlNet，生成图片的自由度更高。
丁老头大家都画过吧，我这里把它生成一个毛绒玩具的图片。
注意这几个参数：
预处理器：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-29T09:46:44+08:00">
    <meta property="article:modified_time" content="2024-04-29T09:46:44+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Stable Diffusion基础：ControlNet之线稿成图</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><img src="https://images2.imgbox.com/d2/3f/8E0zV4uX_o.png" alt="image.png"> 今天继续给大家分享Stable Diffusiion的基础能力：ControlNet之线稿成图。</p> 
<p>所谓线稿就是由一条条的线段组成的图形，主要用于绘画和设计领域的打底稿、表达构想和预见最终效果。</p> 
<p>所谓线稿成图就是利用 Stable Diffusion + ControlNet 的能力，依照线稿中的图形设计生成创意图片。借助ControlNet，我们可以直接使用线稿图生成图片，也可以先从其它图片中提取线稿，然后再生成图片。</p> 
<p>下面我将介绍ControlNet中四个优秀的线稿模型，以及它们的使用方法和注意事项。</p> 
<h3><a id="ControlNet_9"></a>安装ControlNet</h3> 
<p>工欲善其事必先利其器，ControlNet 还是先要安装好的，已经安装好的请跳过这一步。</p> 
<h3><a id="_16"></a>使用方法</h3> 
<h4><a id="Canny_19"></a>Canny</h4> 
<p>基于精细的边缘检测，准确还原图片的结构和特征。Canny边缘图中的线条没有粗细、深浅的区分。</p> 
<p><img src="https://images2.imgbox.com/e9/ac/t1PXOwKV_o.png" alt=""></p> 
<p>因为是真人图片，所以这里选择了一个现实视觉的大模型：realisticVisionV51，提示词可以手写，也可以找个反推工具反推。</p> 
<p><img src="https://images2.imgbox.com/68/0d/jZWwHcuP_o.png" alt=""></p> 
<p>如果是生成真人图片，建议把“面部修复”勾选上。</p> 
<p><img src="https://images2.imgbox.com/f2/43/0ANMPSB5_o.png" alt=""></p> 
<p>ControlNet的设置是重点：</p> 
<p>1、展开ControlNet控制面板。</p> 
<p>2、在Unit 0中上传一张需要提取边缘的图片。</p> 
<p>3、勾选“启用”，启用当前ControlNet Unit。</p> 
<p>4、勾选“完美匹配像素”，自动设置预处理器的分辨率。</p> 
<p>5、勾选“允许预览”，这样就可以预览边缘检测的效果。</p> 
<p><img src="https://images2.imgbox.com/75/ea/UVJT49cH_o.png" alt=""></p> 
<p>6、选择“Canny”</p> 
<p>7、预处理器和模型会自动加载，暂时不更改它们。</p> 
<p>预处理器可以检测图片中人物和物体的边缘，绘制出边缘图。点击预处理器后边的星火图标可以在预览区看到预处理的效果图。</p> 
<p>预处理器器还有两个选项：</p> 
<ul><li>无：不使用预处理器，直接在ControlNet中上传一张线稿图。</li><li>颜色反转：ControlNet中能够处理的线稿图需要线条是白色， 其它区域是黑色，这和现实中的线图颜色是相反的。所以通过这个预处理器，我们可以直接反转一张现实世界的线稿图，供ControlNet模型进行生图处理。不适合普通的彩色图片。</li></ul> 
<p>8、控制权重：ControlNet模型在生成图片时的权重，降低这个权重，线稿对绘图的约束就会变弱。</p> 
<p>启动控制的步数：Canny介入的时机。介入时机越早，越能保证构图依照边缘图，一般从0开始。</p> 
<p>结束控制的步数：Canny退出的时机。退出时机越晚，细节保留程度越大，如果要多变换一些细节，可以提前退出。</p> 
<p>9、这两个阈值是canny预处理器提取线稿图时使用的，通过调整阈值可以控制边缘图中保留细节的多少。</p> 
<p>Canny Low Threshold：去掉过细的线段。大于低阈值的线段被认定为边缘。</p> 
<p>Canny High Threshold：去掉零散的线段。大于高阈值的线段被认定为强边缘，全部保留；高阈值和低阈值之间的线段认定为弱边缘，只保留强边缘相邻的弱边缘。</p> 
<p>10、控制模式：ControlNet的通用设置，以提示词为主，还是以ControlNet模型为主。</p> 
<p>11、缩放模式：ControlNet的通用设置，参考图与要生成的图片尺寸不一致时如何处理。拉伸有变形的问题，一般使用裁剪和填充。</p> 
<p><img src="https://images2.imgbox.com/91/d8/3ZOXnMIi_o.png" alt=""></p> 
<h4><a id="Lineart_76"></a>Lineart</h4> 
<p>使用线条艺术生成训练，通常用于黑线白底图的上色，也可以从各种图片中提取线条，然后再生成图片。</p> 
<p>Lineart的线条有粗细深浅的区别，相比Canny，除了能够控制构图，还可以更好的还原图片深度。</p> 
<p><img src="https://images2.imgbox.com/56/bb/eDKVTu8d_o.png" alt=""></p> 
<p><img src="https://images2.imgbox.com/f8/a9/SffTxnb3_o.png" alt=""></p> 
<p>下图的几个设置和上面Canny控制器的设置差不多，只是上传的图片换成了一张黑白线稿图。</p> 
<p><img src="https://images2.imgbox.com/7f/d0/9qrOvoow_o.png" alt=""></p> 
<p>重点看下这几个设置：</p> 
<ul><li> <p>Lineart：线稿控制网络。</p> </li><li> <p>预处理器：从图片提取线稿图的处理器。这里有多个预处理器，简单介绍下：</p> </li><li> 
  <ul><li>none：不使用预处理器，需要直接上传一张处理好白线黑景线稿图。</li><li>lineart_anime：适合从动漫图片中提取线稿图。</li><li>lineart_anime_denoise：适合从动漫图片中提取线稿图，并去掉噪音点。</li><li>lineart_coarse：从图片中粗略提取线稿图，忽略不突出的细节，生图时自由度更高。</li><li>lineart_realistic：从真实视觉的图片中提取线稿图。</li><li>lineart_standard：从图片提取线稿图的标准版处理器。</li><li>颜色反转：反色图片，适合从白色背景、黑色线条的图片中提取线稿图。</li></ul> </li></ul> 
<p><img src="https://images2.imgbox.com/95/32/hljpswT8_o.png" alt=""></p> 
<ul><li> <p>模型：根据线稿图生成图片的ControlNet模型。</p> </li><li> 
  <ul><li>control_v11p_sd15_lineart_fp16：通用线稿图生成模型。</li><li>control_v11p_sd15s2_lineart_anime_fp16：二次元线稿图生成模型。</li></ul> </li></ul> 
<p><img src="https://images2.imgbox.com/b9/b8/AvOpo78F_o.png" alt=""></p> 
<h4><a id="SoftEdge_112"></a>SoftEdge</h4> 
<p>使用软边缘图像生成训练，平滑的边缘、更好的深度，忽略内部细节，方便创作更具绘画特征或艺术风格的图片。</p> 
<p>相比Canny、Lineart，SoftEdge除了稳定构图，更好的图片深度控制，还拥有了更多的自由度。</p> 
<p><img src="https://images2.imgbox.com/a2/81/s8Qa6TyQ_o.png" alt=""></p> 
<p>重点看下ControlNet的这几个设置：</p> 
<p><img src="https://images2.imgbox.com/d0/19/uveZp4gu_o.png" alt=""></p> 
<p>预处理器：从图片中提取软边缘图。</p> 
<p>softedge_hed：合成柔边图。</p> 
<p>softedge_hedsafe：安全的合成柔边图，边缘更清晰，稳定性更高。</p> 
<p>softedge_pidinet：简笔柔边图，描绘的细节更少，出图的细节更自由，姿势更稳定。</p> 
<p>softedge_pidinet：安全的简笔柔边图，边缘更清晰，稳定性更高。</p> 
<p><img src="https://images2.imgbox.com/a7/42/HG3pM0uG_o.png" alt=""></p> 
<p>模型：control_v11p_sd15_softedge_fp16，参与生成图片的软边缘模型，只有这一个。</p> 
<h4><a id="Scribble_138"></a>Scribble</h4> 
<p>这是一个称之为涂鸦成图的ControlNet，生成图片的自由度更高。</p> 
<p>丁老头大家都画过吧，我这里把它生成一个毛绒玩具的图片。</p> 
<p><img src="https://images2.imgbox.com/2d/11/ycxWlHOj_o.png" alt=""></p> 
<p>注意这几个参数：</p> 
<p><img src="https://images2.imgbox.com/7f/93/Ac1ZLqjY_o.png" alt=""></p> 
<p>预处理器：</p> 
<p>无：不对参考图进行预处理，需要自己上传处理好的涂鸦图，白色代表线条。</p> 
<p>scribble_hed: 粗线合成涂鸦图，用来生成随机性较强的图。</p> 
<p>scribble_pidinet: 粗线简笔合成涂鸦图，生成图片的自由度比 scribble_hed 弱。</p> 
<p>scribble_xdog：从图片中提取更多的细节生成涂鸦图，结构和轮廓的限制更多。</p> 
<p>颜色反转：简单的反转图片中的黑白色，适合黑白图的处理。</p> 
<p><img src="https://images2.imgbox.com/5d/a1/tfCgYZ2e_o.png" alt=""></p> 
<p>模型：control_v11p_sd15_scribble_fp16，参与生成图片的涂鸦模型，只有这一个。</p> 
<p>Scribble 也可以用来处理真实的图片，与Canny、Lineart、SoftEdge相比，最大的变化就是，构图会发生一些变化，而且它没有对图片深度的处理。</p> 
<p><img src="https://images2.imgbox.com/6b/cd/pOkGOUG2_o.png" alt=""></p> 
<h4><a id="_170"></a>效果对比</h4> 
<p>考虑到 Scribble 对原图的变化过大，这里只对比前三个：</p> 
<p>可以看到 SoftEdge 对构图和深度处理的最好，当然细节上也更自由。</p> 
<p><img src="https://images2.imgbox.com/e0/42/ZkEuAVPP_o.png" alt=""></p> 
<h3><a id="_178"></a>资源下载</h3> 
<p>本文使用的模型、插件，生成的图片，都已经上传到了我整理的SD绘画资源中，后续也会持续更新，如有需要，请添加下方领取！</p> 
<hr> 
<h3><a id="_188"></a>写在最后</h3> 
<p><font face="幼圆" size="4" color="red">感兴趣的小伙伴，赠送全套AIGC学习资料，包含AI绘画、AI人工智能等前沿科技教程和软件工具，具体看这里。<br> ​<img src="https://images2.imgbox.com/41/df/i3m71D6A_o.jpg"></font></p> 
<p>AIGC技术的未来发展前景广阔，随着人工智能技术的不断发展，AIGC技术也将不断提高。未来，AIGC技术将在游戏和计算领域得到更广泛的应用，使游戏和计算系统具有更高效、更智能、更灵活的特性。同时，AIGC技术也将与人工智能技术紧密结合，在更多的领域得到广泛应用，对程序员来说影响至关重要。未来，AIGC技术将继续得到提高，同时也将与人工智能技术紧密结合，在更多的领域得到广泛应用。<br>  <br> <img src="https://images2.imgbox.com/88/c3/AgoDZpuh_o.png" alt="在这里插入图片描述"></p> 
<p><strong>一、AIGC所有方向的学习路线</strong></p> 
<p>AIGC所有方向的技术点做的整理，形成各个领域的知识点汇总，它的用处就在于，你可以按照下面的知识点去找对应的学习资源，保证自己学得较为全面。</p> 
<p><img src="https://images2.imgbox.com/5e/2d/iUM6oz5y_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/61/9b/YbcylJlt_o.png" alt="在这里插入图片描述"></p> 
<p><strong>二、AIGC必备工具</strong></p> 
<p>工具都帮大家整理好了，安装就可直接上手！<br> <img src="https://images2.imgbox.com/4c/04/lZQWjPe1_o.png" alt="在这里插入图片描述"></p> 
<p><strong>三、最新AIGC学习笔记</strong></p> 
<p>当我学到一定基础，有自己的理解能力的时候，会去阅读一些前辈整理的书籍或者手写的笔记资料，这些笔记详细记载了他们对一些技术点的理解，这些理解是比较独到，可以学到不一样的思路。<br> <img src="https://images2.imgbox.com/1f/14/TeyeNXu9_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ab/2f/gfRjzI45_o.png" alt="在这里插入图片描述"></p> 
<p><strong>四、AIGC视频教程合集</strong></p> 
<p>观看全面零基础学习视频，看视频学习是最快捷也是最有效果的方式，跟着视频中老师的思路，从基础到深入，还是很容易入门的。</p> 
<p><img src="https://images2.imgbox.com/1b/4e/j26PyZzO_o.png" alt="在这里插入图片描述"></p> 
<p><strong>五、实战案例</strong></p> 
<p>纸上得来终觉浅，要学会跟着视频一起敲，要动手实操，才能将自己的所学运用到实际当中去，这时候可以搞点实战案例来学习。<br> <img src="https://images2.imgbox.com/77/74/Douq1kNC_o.png" alt="在这里插入图片描述"><br> ​<img src="https://images2.imgbox.com/52/d4/KKqJb5Gq_o.jpg"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/cc608e5263ff2297f91d9f399984a8f6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">扩展阅读-穿越Redis单线程迷雾：从面试场景到技术内核的解读</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0b3e6b0ec0d52dde943c6bf2a80c2931/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Stable Diffusion从入门到卸载，一站式服务为你的AI绘画保驾护航！</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>