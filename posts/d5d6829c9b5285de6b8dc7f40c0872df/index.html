<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Spark实时（五）：InputSource数据源案例演示 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/d5d6829c9b5285de6b8dc7f40c0872df/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Spark实时（五）：InputSource数据源案例演示">
  <meta property="og:description" content="文章目录
InputSource数据源案例演示
一、​​​​​​​File Source
1、读取text文件
2、读取csv文件
3、读取json文件
二、Socket Source 三、Rate Source
InputSource数据源案例演示 在Spark2.0版本之后，DataFrame和Dataset可以表示静态有边界的数据，也可以表示无边界的流式数据。在Structured Streaming中我们可以使用SparkSession针对流式数据源创建对应的Dataset或者DataFrame，并可以像处理批数据一样使用各种Operators操作处理流式数据。
Structured Streaming的数据源目前支持File Source 、Socket Source 、Rate Source、Kafka Source ，与Kafka的整合在后续整理，这里对其他三种数据源分别演示。
一、​​​​​​​​​​​​​​File Source Sturctured Streaming可以读取写入目录的文件作为数据流，文件将按照文件修改时间的顺序进行处理，文件必须原子性的存入到监控目录中，支持的格式有text、csv、json、orc、parquet。
1、读取text文件 Scala代码如下：
package com.lanson.structuredStreaming.source import org.apache.spark.sql.streaming.StreamingQuery import org.apache.spark.sql.{DataFrame, Dataset, SparkSession} /** * Structured Streaming监控目录 text格式数据 */ object SSReadTextData { def main(args: Array[String]): Unit = { //1.创建对象 val spark: SparkSession = SparkSession.builder().master(&#34;local&#34;) .appName(&#34;SSReadTextData&#34;) .config(&#34;spark.sql.shuffle.partitions&#34;, 1) .getOrCreate() import spark.implicits._ spark.sparkContext.setLogLevel(&#34;Error&#34;) //2.监控目录 val ds: Dataset[String] = spark.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-25T23:49:05+08:00">
    <meta property="article:modified_time" content="2024-07-25T23:49:05+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Spark实时（五）：InputSource数据源案例演示</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/c8/87/Y84woCMl_o.jpg"></p> 
<p id="main-toc"><strong>文章目录</strong></p> 
<p id="InputSource%E6%95%B0%E6%8D%AE%E6%BA%90%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA-toc" style="margin-left:0px;"><a href="#InputSource%E6%95%B0%E6%8D%AE%E6%BA%90%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA" rel="nofollow">InputSource数据源案例演示</a></p> 
<p id="%E4%B8%80%E3%80%81%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8BFile%20Source-toc" style="margin-left:40px;"><a href="#%E4%B8%80%E3%80%81%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8BFile%20Source" rel="nofollow">一、​​​​​​​File Source</a></p> 
<p id="1%E3%80%81%E8%AF%BB%E5%8F%96text%E6%96%87%E4%BB%B6-toc" style="margin-left:80px;"><a href="#1%E3%80%81%E8%AF%BB%E5%8F%96text%E6%96%87%E4%BB%B6" rel="nofollow">1、读取text文件</a></p> 
<p id="2%E3%80%81%E8%AF%BB%E5%8F%96csv%E6%96%87%E4%BB%B6-toc" style="margin-left:80px;"><a href="#2%E3%80%81%E8%AF%BB%E5%8F%96csv%E6%96%87%E4%BB%B6" rel="nofollow">2、读取csv文件</a></p> 
<p id="3%E3%80%81%E8%AF%BB%E5%8F%96json%E6%96%87%E4%BB%B6-toc" style="margin-left:80px;"><a href="#3%E3%80%81%E8%AF%BB%E5%8F%96json%E6%96%87%E4%BB%B6" rel="nofollow">3、读取json文件</a></p> 
<p id="%E4%BA%8C%E3%80%81Socket%20Source%C2%A0-toc" style="margin-left:40px;"><a href="#%E4%BA%8C%E3%80%81Socket%20Source%C2%A0" rel="nofollow">二、Socket Source </a></p> 
<p id="%E4%B8%89%E3%80%81Rate%20Source-toc" style="margin-left:40px;"><a href="#%E4%B8%89%E3%80%81Rate%20Source" rel="nofollow">三、Rate Source</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2><strong>InputSource数据源案例演示</strong></h2> 
<p style="margin-left:.0001pt;text-align:justify;">在Spark2.0版本之后，DataFrame和Dataset可以表示静态有边界的数据，也可以表示无边界的流式数据。在Structured Streaming中我们可以使用SparkSession针对流式数据源创建对应的Dataset或者DataFrame，并可以像处理批数据一样使用各种Operators操作处理流式数据。</p> 
<p style="margin-left:.0001pt;text-align:justify;">Structured Streaming的数据源目前支持File Source 、Socket Source 、Rate Source、Kafka Source ，与Kafka的整合在后续整理，这里对其他三种数据源分别演示。</p> 
<p></p> 
<h3 id="%E4%B8%80%E3%80%81%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8BFile%20Source"><strong>一、​​​​​​​​​​​​​​File Source</strong></h3> 
<p style="margin-left:.0001pt;text-align:justify;">Sturctured Streaming可以读取写入目录的文件作为数据流，文件将按照文件修改时间的顺序进行处理，文件必须原子性的存入到监控目录中，支持的格式有text、csv、json、orc、parquet。</p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<h4 id="1%E3%80%81%E8%AF%BB%E5%8F%96text%E6%96%87%E4%BB%B6" style="text-align:justify;"><strong><strong>1、读取text文件</strong></strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">Scala代码如下：</p> 
<pre><code class="language-Scala">package com.lanson.structuredStreaming.source

import org.apache.spark.sql.streaming.StreamingQuery
import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}

/**
  *  Structured Streaming监控目录 text格式数据
  */
object SSReadTextData {
  def main(args: Array[String]): Unit = {

    //1.创建对象
    val spark: SparkSession = SparkSession.builder().master("local")
      .appName("SSReadTextData")
      .config("spark.sql.shuffle.partitions", 1)
      .getOrCreate()

    import  spark.implicits._

    spark.sparkContext.setLogLevel("Error")

    //2.监控目录
    val ds: Dataset[String] = spark.readStream.textFile("./data/")

    val result: DataFrame = ds.map(line =&gt; {
      val arr: Array[String] = line.split("-")
      (arr(0).toInt, arr(1), arr(2).toInt)
    }).toDF("id", "name", "age")

    val query: StreamingQuery = result.writeStream
      .format("console")
      .start()

    query.awaitTermination()

  }

}
</code></pre> 
<p> 结果：</p> 
<p><img alt="" height="630" src="https://images2.imgbox.com/77/94/jE0dYAwG_o.png" width="1200"></p> 
<p>Java代码如下：</p> 
<pre><code class="language-java">package com.lanson.structuredStreaming.source;

import java.util.concurrent.TimeoutException;
import org.apache.spark.api.java.function.MapFunction;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.streaming.StreamingQueryException;
import scala.Tuple3;

public class SSReadTextData01 {
    public static void main(String[] args) throws TimeoutException, StreamingQueryException {
        //1.创建对象
        SparkSession spark = SparkSession.builder().master("local")
                .appName("SSReadSocketData01")
                .config("spark.sql.shuffle.partitions", 1)
                .getOrCreate();

        spark.sparkContext().setLogLevel("Error");

        Dataset&lt;String&gt; ds = spark.readStream().textFile("./data/");

        Dataset&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; ds2 = ds.map(new MapFunction&lt;String, Tuple3&lt;Integer, String, Integer&gt;&gt;() {
            @Override
            public Tuple3&lt;Integer, String, Integer&gt; call(String line) throws Exception {
                String[] arr = line.split("-");

                return new Tuple3&lt;&gt;(Integer.valueOf(arr[0]), arr[1],Integer.valueOf(arr[2]) );
            }
        }, Encoders.tuple(Encoders.INT(), Encoders.STRING(), Encoders.INT()));

        Dataset&lt;Row&gt; result = ds2.toDF("id", "name", "age");

        result.writeStream()
                .format("console")
                .start()
                .awaitTermination();

    }
}
</code></pre> 
<p> 结果：</p> 
<p><img alt="" height="627" src="https://images2.imgbox.com/67/29/qlqoeXCy_o.png" width="1200"></p> 
<p style="margin-left:.0001pt;text-align:justify;">以上代码编写完成之后，向监控的目录“./data”中不断写入含有以下内容的文件，可以看到控制台有对应的流数据输出，这里一定是原子性的将文件复制到对应目录下。文件内容如下：</p> 
<pre><code class="language-java">1-zhangsan-18
2-lisi-19
3-ww-20</code></pre> 
<p></p> 
<h4 id="2%E3%80%81%E8%AF%BB%E5%8F%96csv%E6%96%87%E4%BB%B6"><strong>2、读取csv文件</strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">Scala代码如下：</p> 
<pre><code class="language-Scala">package com.lanson.structuredStreaming.source

import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}
import org.apache.spark.sql.streaming.StreamingQuery
import org.apache.spark.sql.types.StructType

/**
  * Structured Streaming 读取CSV数据
  */
object SSReadCsvData {
  def main(args: Array[String]): Unit = {
    //1.创建对象
    val spark: SparkSession = SparkSession.builder().master("local")
      .appName("SSReadCsvData")
      .config("spark.sql.shuffle.partitions", 1)
      .getOrCreate()

    import  spark.implicits._

    spark.sparkContext.setLogLevel("Error")

    //2.创建CSV数据schema
    val userSchema: StructType = new StructType().add("id", "integer")
      .add("name", "string")
      .add("gender", "string")
      .add("age", "integer")


    val result: DataFrame = spark.readStream
      .option("sep", ",")
      .schema(userSchema)
      .csv("./data/")

    val query: StreamingQuery = result.writeStream
      .format("console")
      .start()

    query.awaitTermination()

  }

}
</code></pre> 
<p>结果：</p> 
<p><img alt="" height="725" src="https://images2.imgbox.com/bd/77/zu2LrK2p_o.png" width="1200"></p> 
<p>Java代码如下</p> 
<pre><code class="language-Scala">package com.lanson.structuredStreaming.source;

import java.util.concurrent.TimeoutException;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.streaming.StreamingQueryException;
import org.apache.spark.sql.types.StructType;

/**
 * Structured Streaming 读取CSV数据
 */

public class SSReadCsvData01 {
    public static void main(String[] args) throws TimeoutException, StreamingQueryException {
        //1.创建对象
        SparkSession spark = SparkSession.builder().master("local")
                .appName("SSReadCsvData")
                .config("spark.sql.shuffle.partitions", 1)
                .getOrCreate();

        spark.sparkContext().setLogLevel("Error");

        StructType userSchema = new StructType()
                .add("id", "integer")
                .add("name", "string")
                .add("gender", "string")
                .add("age", "integer");
        Dataset&lt;Row&gt; result = spark.readStream()
                .option("sep", ",")
                .schema(userSchema)
                .csv("./data/");

        result.writeStream()
                .format("console")
                .start()
                .awaitTermination();

    }
}
</code></pre> 
<p> 结果：</p> 
<p><img alt="" height="732" src="https://images2.imgbox.com/ce/e7/XFRaD9ml_o.png" width="1200"></p> 
<p>以上代码运行之后向对应监控的目录下原子性写入含有数据的csv文件，在控制台可以看到实时监控内容。文件内容如下：</p> 
<pre><code class="language-Scala">1,zhangsan,一班,100
2,lisi,二班,200
3,wangwu,一班,300
4,maliu,二班,100
5,tianqi,三班,100
6,gaoba,三班,50
7,zs2,四班,50</code></pre> 
<p></p> 
<h4 id="3%E3%80%81%E8%AF%BB%E5%8F%96json%E6%96%87%E4%BB%B6"><strong>3、读取json文件</strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;">Scala代码如下：</p> 
<pre><code class="language-Scala">package com.lanson.structuredStreaming.source

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.streaming.StreamingQuery
import org.apache.spark.sql.types.StructType

/**
  *  Structured Streaming 监控Json格式数据
  */
object SSReadJsonData {
  def main(args: Array[String]): Unit = {
    //1.创建对象
    val spark: SparkSession = SparkSession.builder().master("local")
      .appName("SSReadCsvData")
      .config("spark.sql.shuffle.partitions", 1)
      .getOrCreate()

    import  spark.implicits._

    spark.sparkContext.setLogLevel("Error")

    //2.创建 json 数据schema
    val userSchema: StructType = new StructType().add("id", "integer")
      .add("name", "string")
      .add("age", "integer")



    val result: DataFrame = spark.readStream
      .schema(userSchema)
      .json("./data/")

    val query: StreamingQuery = result.writeStream
      .format("console")
      .start()

    query.awaitTermination()

  }

}
</code></pre> 
<p>结果：</p> 
<p><img alt="" height="394" src="https://images2.imgbox.com/fc/7a/CqJGVE0Q_o.png" width="1200"></p> 
<p></p> 
<p style="margin-left:.0001pt;text-align:justify;">Java代码如下</p> 
<pre><code class="language-java">package com.lanson.structuredStreaming.source;


import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.streaming.StreamingQueryException;
import org.apache.spark.sql.types.StructType;
import java.util.concurrent.TimeoutException;

/**
 * Structured Streaming实时监控目录中json文件作为数据流
 */
public class SSReadJsonData01 {
    public static void main(String[] args) throws TimeoutException, StreamingQueryException {
        //1.创建对象
        SparkSession spark = SparkSession.builder().appName("File Source test")
            .master("local")
            .getOrCreate();

        //2.设置日志
        spark.sparkContext().setLogLevel("Error");

        //3.设置Schema
        StructType userSchema = new StructType().add("id", "integer")
            .add("name", "string")
            .add("age", "integer");

        //4.指定监控目录读取数据json数据
        Dataset&lt;Row&gt; ds = spark.readStream()
            .option("sep", ",")
            .schema(userSchema)
            .json("./data/");

        //5.打印数据到控制台
        StreamingQuery query =ds.writeStream()
            .format("console")
            .start();

        query.awaitTermination();

    }
}
</code></pre> 
<p>结果：</p> 
<p><img alt="" height="357" src="https://images2.imgbox.com/75/e1/5O06egy7_o.png" width="1061"></p> 
<p>以上代码启动之后，向监控的目录“./data”下原子写入含有以下内容的json文件，在控制台可以看到实时监控内容。json文件内容如下：</p> 
<pre><code class="language-java">{"id":1,"name":"zs","age":18}
{"id":2,"name":"ls","age":19}
{"id":3,"name":"ww","age":20}
{"id":4,"name":"ml","age":21}
</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="color:#0d0016;"><strong>注意：</strong>实时监控json格式数据时，创建的Schema 中的字段需要与Json中的属性保持一致，否则在映射成表时，Schema中含有但在Json中没有的属性的字段对应的数据会为null。</span></p> 
<p></p> 
<h3 id="%E4%BA%8C%E3%80%81Socket%20Source%C2%A0"><strong>二、Socket Source </strong></h3> 
<p style="margin-left:.0001pt;text-align:justify;">读取Socket方式需要指定对应的host和port，读取Socket数据源多用于测试场景，这里不再演示。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong>可以参考案例：</strong></p> 
<p style="margin-left:.0001pt;text-align:justify;"><a href="https://blog.csdn.net/xiaoweite1/article/details/140649090?spm=1001.2014.3001.5501" title="Spark实时（三）：Structured Streaming入门案例-CSDN博客">Spark实时（三）：Structured Streaming入门案例-CSDN博客</a></p> 
<p></p> 
<h3 id="%E4%B8%89%E3%80%81Rate%20Source"><strong>三、Rate Source</strong></h3> 
<p style="margin-left:.0001pt;text-align:justify;">Rate Source是以每秒指定的行数生成数据，每个输出行包含一个timestamp和value，其中timestamp是一个Timestamp含有信息分配的时间类型，value是从0开始的Long类型的数据，Rate Source式多用于测试。</p> 
<p style="margin-left:.0001pt;text-align:justify;">scala代码如下：</p> 
<pre><code class="language-Scala">package com.lanson.structuredStreaming.source

import org.apache.spark.sql.{DataFrame, SparkSession}

/**
  * SSRateSource
  */
object SSRateSource {
  def main(args: Array[String]): Unit = {
    //1.创建对象
    val spark: SparkSession = SparkSession.builder().master("local")
      .appName("rate test")
//      .config("spark.sql.shuffle.partitions", 1)
      .getOrCreate()

    val result: DataFrame = spark.readStream
      .format("rate")
      // 配置每秒生成多少行数据，默认1行
      .option("rowsPerSecond", "10")
      .option("numPartitions", 5)
      .load()
    result.writeStream
      .format("console")
      .option("numRows","100")
      .option("truncate","false")
      .start()
      .awaitTermination()

  }

}
</code></pre> 
<p>结果：</p> 
<p><img alt="" height="621" src="https://images2.imgbox.com/4c/b2/v1ZHMr5l_o.png" width="1200"></p> 
<p>Java代码如下：</p> 
<pre><code class="language-java">package com.lanson.structuredStreaming.source;

import java.util.concurrent.TimeoutException;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.streaming.StreamingQueryException;

public class ssratesource01 {
    public static void main(String[] args) throws TimeoutException, StreamingQueryException {
        //1.创建对象
       SparkSession spark = SparkSession.builder().master("local")
                .appName("rate test")
                .getOrCreate();
       spark.sparkContext().setLogLevel("Error");

        Dataset&lt;Row&gt; result = spark.readStream()
                .format("rate")
                // 配置每秒生成多少行数据，默认1行
                .option("rowsPerSecond", "10")
                .option("numPartitions", 5)
                .load();

        result.writeStream()
                .format("console")
                .option("numRows","100")
                .option("truncate","false")
                .start()
                .awaitTermination();
    }
}
</code></pre> 
<p>结果： </p> 
<p><img alt="" height="485" src="https://images2.imgbox.com/6a/be/WOmSpbJ9_o.png" width="1200"></p> 
<p><img alt="" height="515" src="https://images2.imgbox.com/32/09/bP1Dx3rr_o.png" width="655"> </p> 
<hr> 
<ul><li>📢博客主页：<a href="https://lansonli.blog.csdn.net/" rel="nofollow" title="https://lansonli.blog.csdn.net">https://lansonli.blog.csdn.net</a></li><li>📢欢迎点赞 👍 收藏 ⭐留言 📝 如有错误敬请指正！</li><li>📢本文由 Lansonli 原创，首发于 CSDN博客🙉</li><li>📢停下休息的时候不要忘了别人还在奔跑，希望大家抓紧时间学习，全力奔赴更美好的生活✨</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/375243dc869d0e04bf9f7af653394298/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【人工智能】AI绘画：科技与艺术交汇的新时代</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/34e220bb1566faa62f8f551a49759748/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">KubeSphere介绍及一键安装k8s</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>