<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AI论文阅读笔记 | Timer: Generative Pre-trained Transformers Are Large Time Series Models(ICML 2024) - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/9d5c7a846a979bc50646b828ef4079f5/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="AI论文阅读笔记 | Timer: Generative Pre-trained Transformers Are Large Time Series Models(ICML 2024)">
  <meta property="og:description" content="一、基本信息 题目：Timer: Generative Pre-trained Transformers Are Large Time Series Models会议：ICML 2024原文：https://arxiv.org/abs/2402.02368源码：​​​​​​​https://github.com/thuml/Timer 二、基本内容 1、解决什么问题 虽然深度学习对时间序列的分析做出了显著的贡献，但在数据稀缺的真实世界场景中，深度模型可能会遇到性能瓶颈，而这些瓶颈可能由于当前基准测试中小模型的性能饱和而被掩盖。现有的小模型缺乏大模型所展示的少样本泛化（few-shot generalization）、可扩展性（scalability）和任务通用性（task generality）。 2、怎么解决 构建了统一的时间序列数据集 （Unified Time Series Dataset ——UTSD） 本文建立了筛选高质量数据和堆叠的时间序列语料库的层次标准，通过对公开的时间序列数据集进行聚合，并对聚合后的数据进行协同处理，构建了具有层次化能力的统一时间序列数据集（UTSD)如图所示。包含7个域，10亿个时间点（UTSD-12G）,涵盖了时间序列分析的典型场景。
数据集链接：https://huggingface.co/datasets/thuml/UTSD
训练策略——单序列序列（S3）格式 采用生成式预训练方法，将单序列序列（S3）作为不同时间序列的标准语句，如下图所示。不同于自然语言 ，序列的幅度、频率、平稳性、数据集的变量数、序列长度等差异使得构造统一的时间序列并非易事。本文提出将异构时间序列转换为单序列序列（S3），保留了具有统一上下文长度的序列变化模式。模型观察了不同时间段、不同数据集的序列，增加了预训练的难度，使模型更关注时间变化。S3不需要时间对齐，这适用于广泛的单变量和不规则时间序列。
采用生成式的预训练目标 采用了GPT风格的生成式预训练目标，通过预测下一个时间点来进行预训练。将时间序列的预测、插补和异常检测转化为一个统一的生成式任务，允许模型学习时间序列的生成过程，从而在下游任务中展现出更好的泛化能力。 Timer（a large-scale pre-trained Time Series Transformer）的提出 基于以上几点，论文提出了Timer，采用与大语言模型类似的解码器结构，具备灵活的上下文长度和自回归生成能力。在多个数据集和多种任务中表现出色。
3、取得了什么效果 1、时间序列预测 数据集：ETT、ECL、Traffic、Weather、PEMS
SOTA: iTransformer （ICLR 2024）
实验：在不同的数据稀缺性（100%，75%，50%，25%，20%.......1%）下 ,对比了从头开始训练的Timer（虚线)和在UTSD（USTD-12G）上预训练后的Timer（实线）的性能。
结果：
Timer的性能受数据量减少的影响小于基线，随着数据数量的下降，Timer的性能均超过基线，且预训练之后的模型表现比从头训练的好。
2、 插补 数据集：ETT、ECL、Traffic、Weather、PEMS
baseline: TimesNet （ICLR 2023）
实验：在数据稀缺度为5%，20%，100%的情况下，Timer在44个插补任务中表现分别优于TimesNet100%，86.4%，56.8%。
对于预训练的有效性，以插补错误减少率来表示促进作用，其中，预训练在下游样本只有5%的情况下始终产生积极影响。
3、 异常检测 数据集：UCR Anomaly Archive(包含250个时间序列数据集)
baseline: TimesNet （ICLR 2023）、Anomaly Transformer（ICLR 2022）
实验：首先在训练集上训练一个预测模型，并计算预测序列和真实值之间的MSE。将置信度高于α的分段标记为异常的潜在位置。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-26T16:17:49+08:00">
    <meta property="article:modified_time" content="2024-07-26T16:17:49+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AI论文阅读笔记 | Timer: Generative Pre-trained Transformers Are Large Time Series Models(ICML 2024)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>一、基本信息</h2> 
<ul><li>题目：Timer: Generative Pre-trained Transformers Are Large Time Series Models</li><li>会议：ICML 2024</li><li>原文：<a href="https://arxiv.org/abs/2402.02368" rel="nofollow" title="https://arxiv.org/abs/2402.02368">https://arxiv.org/abs/2402.02368</a></li><li>源码：<a class="link-info" href="https://github.com/thuml/Timer" title="​​​​​​​​​​​​​​https://github.com/thuml/Timer">​​​​​​​https://github.com/thuml/Timer</a></li></ul> 
<p></p> 
<h2>二、基本内容 </h2> 
<h3>1、解决什么问题</h3> 
<ul><li>虽然深度学习对时间序列的分析做出了显著的贡献，但在<strong>数据稀缺</strong>的真实世界场景中，<strong>深度模型可能会遇到性能瓶颈</strong>，而这些瓶颈可能由于当前基准测试中小模型的性能饱和而被掩盖。</li><li>现有的小模型缺乏大模型所展示的<strong>少样本泛化</strong>（few-shot generalization）、<strong>可扩展性</strong>（scalability）和<strong>任务通用性</strong>（task generality）。</li></ul> 
<h3>2、怎么解决</h3> 
<ul><li><strong>构建了统一的时间序列数据集</strong> （Unified Time Series Dataset ——UTSD）</li></ul> 
<p>         本文建立了筛选高质量数据和堆叠的时间序列语料库的层次标准，通过对公开的时间序列数据集进行聚合，并对聚合后的数据进行协同处理，构建了具有层次化能力的统一时间序列数据集（UTSD)如图所示。包含7个域，10亿个时间点（UTSD-12G）,涵盖了时间序列分析的典型场景。</p> 
<p class="img-center"><img alt="" height="645" src="https://images2.imgbox.com/e3/20/k5JMLTsz_o.png" width="1200"></p> 
<p> 数据集链接：<a class="link-info" href="https://huggingface.co/datasets/thuml/UTSD" rel="nofollow" title="https://huggingface.co/datasets/thuml/UTSD">https://huggingface.co/datasets/thuml/UTSD</a></p> 
<ul><li><strong>训练策略——单序列序列（S3）格式 </strong></li></ul> 
<p>        <strong>采用生成式预训练方法，将单序列序列（S3）作为不同时间序列的标准语句，如下图所示。</strong>不同于自然语言 ，序列的幅度、频率、平稳性、数据集的变量数、序列长度等差异使得构造统一的时间序列并非易事。本文提出将异构时间序列转换为单序列序列（S3），保留了具有统一上下文长度的序列变化模式。模型观察了不同时间段、不同数据集的序列，增加了预训练的难度，使模型更关注时间变化。S3不需要时间对齐，这适用于广泛的单变量和不规则时间序列。</p> 
<p class="img-center"><img alt="" height="521" src="https://images2.imgbox.com/61/88/iLaKhWPG_o.png" width="867"></p> 
<p></p> 
<ul><li><strong>采用生成式的预训练目标</strong></li></ul> 
<p>        采用了GPT风格的生成式预训练目标，通过预测下一个时间点来进行预训练。将时间序列的预测、插补和异常检测转化为一个<strong>统一的生成式任务</strong>，允许模型学习时间序列的生成过程，从而在下游任务中展现出更好的泛化能力。 </p> 
<p class="img-center"><img alt="" height="489" src="https://images2.imgbox.com/b1/84/uXFn7QFo_o.png" width="855"></p> 
<ul><li><strong>Timer（a large-scale pre-trained Time Series Transformer）的提出</strong></li></ul> 
<p>        基于以上几点，论文提出了Timer，采用与大语言模型类似的解码器结构，具备灵活的上下文长度和自回归生成能力。在多个数据集和多种任务中表现出色。</p> 
<h3>3、取得了什么效果 </h3> 
<h4>        1、时间序列预测</h4> 
<p>            数据集：ETT、ECL、Traffic、Weather、PEMS</p> 
<p>            SOTA:  iTransformer （ICLR 2024）</p> 
<p>            实验：在不同的数据稀缺性（100%，75%，50%，25%，20%.......1%）下 ,对比了从头开始训练的Timer（<strong>虚线</strong>)和在UTSD（USTD-12G）上预训练后的Timer（<strong>实线</strong>）的性能。</p> 
<p>            结果：</p> 
<p class="img-center"><img alt="" height="994" src="https://images2.imgbox.com/5e/de/1N94MDRG_o.png" width="760"></p> 
<p></p> 
<p>        <strong>Timer的性能受数据量减少的影响小于基线，随着数据数量的下降，Timer的性能均超过基线，且预训练之后的模型表现比从头训练的好。</strong></p> 
<h4>        2、 插补</h4> 
<p>            数据集：ETT、ECL、Traffic、Weather、PEMS</p> 
<p>            baseline: TimesNet （ICLR 2023）</p> 
<p>            实验：在数据稀缺度为5%，20%，100%的情况下，Timer在44个插补任务中表现分别优于TimesNet100%，86.4%，56.8%。</p> 
<p class="img-center"><img alt="" height="360" src="https://images2.imgbox.com/e6/81/biIxX81M_o.png" width="539"></p> 
<p>           对于预训练的有效性，以插补错误减少率来表示促进作用，其中，预训练在下游样本只有5%的情况下始终产生积极影响。</p> 
<p class="img-center"><img alt="" height="402" src="https://images2.imgbox.com/3a/de/fCiH9S8o_o.png" width="727"></p> 
<h4>        3、 异常检测</h4> 
<p>            数据集：UCR Anomaly Archive(包含250个时间序列数据集)</p> 
<p>            baseline: TimesNet （ICLR 2023）、Anomaly Transformer（ICLR 2022）</p> 
<p>            实验：首先在训练集上训练一个预测模型，并计算预测序列和真实值之间的MSE。将置信度高于α的分段标记为异常的潜在位置。</p> 
<p>            结果：图7右侧表面，Timer优于其他两个异常检测模型。此外，图9左图显示了预训练模型的有效性。</p> 
<p class="img-center"><img alt="" height="353" src="https://images2.imgbox.com/3d/83/qqDhJIwh_o.png" width="933"></p> 
<p></p> 
<p class="img-center"><img alt="" height="358" src="https://images2.imgbox.com/ec/0b/OR84H0Ss_o.png" width="636"></p> 
<p> 未完待续......</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0ecaabf3f597d4939cff6d3e82d49f8f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">A*(star)算法原理讲解以及程序实现(基于Python)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/915b8d7c37c9ee189792f056dba586c6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">masscan 端口扫描——（Golang 简单使用总结）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>