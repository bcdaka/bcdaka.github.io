<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Spark编程实验五：Spark Structured Streaming编程 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/20b9c5de08f98b3381e343b89f5cd1d6/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Spark编程实验五：Spark Structured Streaming编程">
  <meta property="og:description" content="目录
一、目的与要求
二、实验内容
三、实验步骤
1、Syslog介绍
2、通过Socket传送Syslog到Spark
3、Syslog日志拆分为DateFrame
4、对Syslog进行查询
四、结果分析与实验体会
一、目的与要求 1、通过实验掌握Structured Streaming的基本编程方法；
2、掌握日志分析的常规操作，包括拆分日志方法和分析场景。
二、实验内容 1、通过Socket传送Syslog到Spark
日志分析是一个大数据分析中较为常见的场景。在Unix类操作系统里，Syslog广泛被应用于系统或者应用的日志记录中。Syslog通常被记录在本地文件内，也可以被发送给远程Syslog服务器。Syslog日志内一般包括产生日志的时间、主机名、程序模块、进程名、进程ID、严重性和日志内容。
日志一般会通过Kafka等有容错保障的源发送，本实验为了简化，直接将Syslog通过Socket源发送。新建一个终端，执行如下命令：
$ tail -n&#43;1 -f /var/log/syslog | nc -lk 9988 “tail -n&#43;1 -f /var/log/syslog”表示从第一行开始打印文件syslog的内容。“-f”表示如果文件有增加则持续输出最新的内容。然后，通过管道把文件内容发送到nc程序（nc程序可以进一步把数据发送给Spark）。
如果/var/log/syslog内的内容增长速度较慢，可以再新开一个终端（计作“手动发送日志终端”），手动在终端输入如下内容来增加日志信息到/var/log/syslog内：
$ logger ‘I am a test error log message.’ 2、对Syslog进行查询
由Spark接收nc程序发送过来的日志信息，然后完成以下任务：
（1）统计CRON这个进程每小时生成的日志数，并以时间顺序排列，水印设置为1分钟。
（2）统计每小时的每个进程或者服务分别产生的日志总数，水印设置为1分钟。
（3）输出所有日志内容带error的日志。
三、实验步骤 1、Syslog介绍 分析日志是一个大数据分析中较为常见的场景。在Unix类操作系统里，Syslog广泛被应用于系统或者应用的日志记录中。Syslog通常被记录在本地文件内，也可以被发送给远程Syslog服务器。Syslog日志内一般包括产生日志的时间、主机名、程序模块、进程名、进程ID、严重性和日志内容。
2、通过Socket传送Syslog到Spark 日志一般会通过kafka等有容错保障的源发送，本实验为了简化，直接将syslog通过Socket源发送。新开一个终端，命令为“tail终端”，输入
tail -n&#43;1 -f /var/log/syslog | nc -lk 9988 tail命令加-n&#43;1代表从第一行开始打印文件内容。-f代表如果文件有增加则持续输出最新的内容。通过管道发送到nc命令起的在本地9988上的服务上。
如果/var/log/syslog内的内容增长速度较慢，可以再新开一个终端，命名为“手动发送log终端”，手动在终端输入
logger ‘I am a test error log message.’ 来增加日志信息到/var/log/syslog内。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-02-13T10:35:21+08:00">
    <meta property="article:modified_time" content="2024-02-13T10:35:21+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Spark编程实验五：Spark Structured Streaming编程</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E7%9B%AE%E7%9A%84%E4%B8%8E%E8%A6%81%E6%B1%82-toc" style="margin-left:40px;"><a href="#%E4%B8%80%E3%80%81%E7%9B%AE%E7%9A%84%E4%B8%8E%E8%A6%81%E6%B1%82" rel="nofollow">一、目的与要求</a></p> 
<p id="%E4%BA%8C%E3%80%81%E5%AE%9E%E9%AA%8C%E5%86%85%E5%AE%B9-toc" style="margin-left:40px;"><a href="#%E4%BA%8C%E3%80%81%E5%AE%9E%E9%AA%8C%E5%86%85%E5%AE%B9" rel="nofollow">二、实验内容</a></p> 
<p id="%E4%B8%89%E3%80%81%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4-toc" style="margin-left:40px;"><a href="#%E4%B8%89%E3%80%81%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4" rel="nofollow">三、实验步骤</a></p> 
<p id="1%E3%80%81Syslog%E4%BB%8B%E7%BB%8D-toc" style="margin-left:80px;"><a href="#1%E3%80%81Syslog%E4%BB%8B%E7%BB%8D" rel="nofollow">1、Syslog介绍</a></p> 
<p id="2%E3%80%81%E9%80%9A%E8%BF%87Socket%E4%BC%A0%E9%80%81Syslog%E5%88%B0Spark-toc" style="margin-left:80px;"><a href="#2%E3%80%81%E9%80%9A%E8%BF%87Socket%E4%BC%A0%E9%80%81Syslog%E5%88%B0Spark" rel="nofollow">2、通过Socket传送Syslog到Spark</a></p> 
<p id="3%E3%80%81Syslog%E6%97%A5%E5%BF%97%E6%8B%86%E5%88%86%E4%B8%BADateFrame-toc" style="margin-left:80px;"><a href="#3%E3%80%81Syslog%E6%97%A5%E5%BF%97%E6%8B%86%E5%88%86%E4%B8%BADateFrame" rel="nofollow">3、Syslog日志拆分为DateFrame</a></p> 
<p id="4%E3%80%81%E5%AF%B9Syslog%E8%BF%9B%E8%A1%8C%E6%9F%A5%E8%AF%A2-toc" style="margin-left:80px;"><a href="#4%E3%80%81%E5%AF%B9Syslog%E8%BF%9B%E8%A1%8C%E6%9F%A5%E8%AF%A2" rel="nofollow">4、对Syslog进行查询</a></p> 
<p id="%E5%9B%9B%E3%80%81%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E9%AA%8C%E4%BD%93%E4%BC%9A-toc" style="margin-left:40px;"><a href="#%E5%9B%9B%E3%80%81%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E9%AA%8C%E4%BD%93%E4%BC%9A" rel="nofollow">四、结果分析与实验体会</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h3 id="%E4%B8%80%E3%80%81%E7%9B%AE%E7%9A%84%E4%B8%8E%E8%A6%81%E6%B1%82">一、目的与要求</h3> 
<p style="margin-left:.0001pt;text-align:justify;">1、通过实验掌握Structured Streaming的基本编程方法；<br> 2、掌握日志分析的常规操作，包括拆分日志方法和分析场景。</p> 
<h3 id="%E4%BA%8C%E3%80%81%E5%AE%9E%E9%AA%8C%E5%86%85%E5%AE%B9" style="margin-left:.0001pt;text-align:justify;">二、实验内容</h3> 
<p style="margin-left:.0001pt;text-align:justify;"><strong>1、通过Socket传送Syslog到Spark</strong></p> 
<p style="margin-left:.0001pt;text-align:justify;">        日志分析是一个大数据分析中较为常见的场景。在Unix类操作系统里，Syslog广泛被应用于系统或者应用的日志记录中。Syslog通常被记录在本地文件内，也可以被发送给远程Syslog服务器。Syslog日志内一般包括产生日志的时间、主机名、程序模块、进程名、进程ID、严重性和日志内容。</p> 
<p style="margin-left:.0001pt;text-align:justify;">        日志一般会通过Kafka等有容错保障的源发送，本实验为了简化，直接将Syslog通过Socket源发送。新建一个终端，执行如下命令：</p> 
<pre><code class="language-bash">$ tail -n+1 -f /var/log/syslog | nc -lk 9988</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;">        “tail -n+1 -f /var/log/syslog”表示从第一行开始打印文件syslog的内容。“-f”表示如果文件有增加则持续输出最新的内容。然后，通过管道把文件内容发送到nc程序（nc程序可以进一步把数据发送给Spark）。</p> 
<p style="margin-left:.0001pt;text-align:justify;">        如果/var/log/syslog内的内容增长速度较慢，可以再新开一个终端（计作“手动发送日志终端”），手动在终端输入如下内容来增加日志信息到/var/log/syslog内：</p> 
<pre><code class="language-bash">$ logger ‘I am a test error log message.’</code></pre> 
<p style="text-align:justify;"><strong><strong><strong>2、对Syslog进行查询</strong></strong></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;">由Spark接收nc程序发送过来的日志信息，然后完成以下任务：</p> 
<p style="margin-left:.0001pt;text-align:justify;">（1）统计CRON这个进程每小时生成的日志数，并以时间顺序排列，水印设置为1分钟。<br> （2）统计每小时的每个进程或者服务分别产生的日志总数，水印设置为1分钟。<br> （3）输出所有日志内容带error的日志。</p> 
<h3 id="%E4%B8%89%E3%80%81%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4" style="margin-left:.0001pt;text-align:justify;">三、实验步骤</h3> 
<h4 id="1%E3%80%81Syslog%E4%BB%8B%E7%BB%8D" style="margin-left:.0001pt;text-align:justify;">1、Syslog介绍</h4> 
<p style="margin-left:.0001pt;text-align:justify;">        分析日志是一个大数据分析中较为常见的场景。在Unix类操作系统里，Syslog广泛被应用于系统或者应用的日志记录中。Syslog通常被记录在本地文件内，也可以被发送给远程Syslog服务器。Syslog日志内一般包括产生日志的时间、主机名、程序模块、进程名、进程ID、严重性和日志内容。</p> 
<h4 id="2%E3%80%81%E9%80%9A%E8%BF%87Socket%E4%BC%A0%E9%80%81Syslog%E5%88%B0Spark" style="margin-left:.0001pt;text-align:justify;">2、通过Socket传送Syslog到Spark</h4> 
<p style="margin-left:.0001pt;text-align:justify;">        日志一般会通过kafka等有容错保障的源发送，本实验为了简化，直接将syslog通过Socket源发送。新开一个终端，命令为“tail终端”，输入</p> 
<pre><code class="language-bash">tail -n+1 -f /var/log/syslog | nc -lk 9988</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;">        tail命令加-n+1代表从第一行开始打印文件内容。-f代表如果文件有增加则持续输出最新的内容。通过管道发送到nc命令起的在本地9988上的服务上。<br>         如果/var/log/syslog内的内容增长速度较慢，可以再新开一个终端，命名为“手动发送log终端”，手动在终端输入</p> 
<pre><code class="language-bash">logger ‘I am a test error log message.’</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;">来增加日志信息到/var/log/syslog内。</p> 
<h4 id="3%E3%80%81Syslog%E6%97%A5%E5%BF%97%E6%8B%86%E5%88%86%E4%B8%BADateFrame" style="margin-left:.0001pt;text-align:justify;">3、Syslog日志拆分为DateFrame</h4> 
<p style="margin-left:.0001pt;text-align:justify;">        Syslog每行的数据类似以下：</p> 
<blockquote> 
 <p>Nov 24 13:17:01 spark CRON[18455]: (root) CMD (cd / &amp;&amp; run-parts --report /etc/cron.hourly)</p> 
</blockquote> 
<p style="margin-left:.0001pt;text-align:justify;">        最前面为时间，接着是主机名，进程名，可选的进程ID，冒号后是日志内容。在Spark内，可以使用正则表达式对syslog进行拆分成结构化字段，以下是示例代码：</p> 
<pre><code class="language-python"> # 定义一个偏应用函数，从固定的pattern获取日志内匹配的字段
    fields = partial(
        regexp_extract, str="value", pattern="^(\w{3}\s*\d{1,2} \d{2}:\d{2}:\d{2}) (.*?) (.*?)\[*\d*\]*: (.*)$"
    )

    words = lines.select(
        to_timestamp(format_string('2019 %s', fields(idx=1)), 'yy MMM d H:m:s').alias("timestamp"),
        fields(idx=2).alias("hostname"),
        fields(idx=3).alias("tag"),
        fields(idx=4).alias("content"),
    )</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;">        to_timestamp(format_string('2018 %s', fields(idx=1)), 'yy MMM d H:m:s').alias("timestamp"),这句是对Syslog格式的一个修正，因为系统默认的Syslog日期是没有年的字段，所以使用format_string函数强制把拆分出来的第一个字段前面加上2019年，再根据to_timestamp格式转换成timestamp字段。在接下来的查询应当以这个timestamp作为事件时间。</p> 
<h4 id="4%E3%80%81%E5%AF%B9Syslog%E8%BF%9B%E8%A1%8C%E6%9F%A5%E8%AF%A2" style="margin-left:.0001pt;text-align:justify;">4、对Syslog进行查询</h4> 
<p style="margin-left:.0001pt;text-align:justify;">由Spark接收nc程序发送过来的日志信息，然后完成以下任务。</p> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">（1）统计CRON这个进程每小时生成的日志数，并以时间顺序排列，水印设置为1分钟。</p> 
</blockquote> 
<p style="margin-left:.0001pt;text-align:justify;">        在新开的终端内输入 vi spark_exercise_testsyslog1.py ，贴入如下代码并运行。运行之前需要关闭“tail终端”内的tail命令并重新运行tail命令，否则多次运行测试可能导致没有新数据生成。</p> 
<pre><code class="language-python">#!/usr/bin/env python3

from functools import partial

from pyspark.sql import SparkSession
from pyspark.sql.functions import *


if __name__ == "__main__":
    spark = SparkSession \
        .builder \
        .appName("StructuredSyslog") \
        .getOrCreate()

    lines = spark \
        .readStream \
        .format("socket") \
        .option("host", "localhost") \
        .option("port", 9988) \
        .load()

    # Nov 24 13:17:01 spark CRON[18455]: (root) CMD (   cd / &amp;&amp; run-parts --report /etc/cron.hourly)
    # 定义一个偏应用函数，从固定的pattern获取日志内匹配的字段
    fields = partial(
        regexp_extract, str="value", pattern="^(\w{3}\s*\d{1,2} \d{2}:\d{2}:\d{2}) (.*?) (.*?)\[*\d*\]*: (.*)$"
    )

    words = lines.select(
        to_timestamp(format_string('2019 %s', fields(idx=1)), 'yy MMM d H:m:s').alias("timestamp"),
        fields(idx=2).alias("hostname"),
        fields(idx=3).alias("tag"),
        fields(idx=4).alias("content"),
    )

    # (1).  统计CRON这个进程每小时生成的日志数，并以时间顺序排列，水印设置为1分钟。
    windowedCounts1 = words \
        .filter("tag = 'CRON'") \
        .withWatermark("timestamp", "1 minutes") \
        .groupBy(window('timestamp', "1 hour")) \
        .count() \
        .sort(asc('window'))

    # 开始运行查询并在控制台输出
    query = windowedCounts1 \
        .writeStream \
        .outputMode("complete") \
        .format("console") \
        .option('truncate', 'false')\
        .trigger(processingTime="3 seconds") \
        .start()

    query.awaitTermination()</code></pre> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">（2）统计每小时的每个进程或者服务分别产生的日志总数，水印设置为1分钟。</p> 
</blockquote> 
<p style="margin-left:.0001pt;text-align:justify;">        在新开的终端内输入 vi spark_exercise_testsyslog2.py ，贴入如下代码并运行。运行之前需要关闭“tail终端”内的tail命令并重新运行tail命令，否则多次运行测试可能导致没有新数据生成。</p> 
<pre><code class="language-python">#!/usr/bin/env python3

from functools import partial

from pyspark.sql import SparkSession
from pyspark.sql.functions import *


if __name__ == "__main__":
    spark = SparkSession \
        .builder \
        .appName("StructuredSyslog") \
        .getOrCreate()

    lines = spark \
        .readStream \
        .format("socket") \
        .option("host", "localhost") \
        .option("port", 9988) \
        .load()

    # Nov 24 13:17:01 spark CRON[18455]: (root) CMD (   cd / &amp;&amp; run-parts --report /etc/cron.hourly)
    # 定义一个偏应用函数，从固定的pattern获取日志内匹配的字段
    fields = partial(
        regexp_extract, str="value", pattern="^(\w{3}\s*\d{1,2} \d{2}:\d{2}:\d{2}) (.*?) (.*?)\[*\d*\]*: (.*)$"
    )

    words = lines.select(
        to_timestamp(format_string('2019 %s', fields(idx=1)), 'yy MMM d H:m:s').alias("timestamp"),
        fields(idx=2).alias("hostname"),
        fields(idx=3).alias("tag"),
        fields(idx=4).alias("content"),
    )

    # (2).  统计每小时的每个进程或者服务分别产生的日志总数，水印设置为1分钟。
    windowedCounts2 = words \
        .withWatermark("timestamp", "1 minutes") \
        .groupBy('tag', window('timestamp', "1 hour")) \
        .count() \
        .sort(asc('window'))

    # 开始运行查询并在控制台输出
    query = windowedCounts2 \
        .writeStream \
        .outputMode("complete") \
        .format("console") \
        .option('truncate', 'false')\
        .trigger(processingTime="3 seconds") \
        .start()

    query.awaitTermination()</code></pre> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">（3）输出所有日志内容带error的日志。</p> 
</blockquote> 
<p style="margin-left:.0001pt;text-align:justify;">        在新开的终端内输入 vi spark_exercise_testsyslog3.py ，贴入如下代码并运行。运行之前需要关闭“tail终端”内的tail命令并重新运行tail命令，否则多次运行测试可能导致没有新数据生成。</p> 
<pre><code class="language-python">#!/usr/bin/env python3

from functools import partial

from pyspark.sql import SparkSession
from pyspark.sql.functions import *


if __name__ == "__main__":
    spark = SparkSession \
        .builder \
        .appName("StructuredSyslog") \
        .getOrCreate()

    lines = spark \
        .readStream \
        .format("socket") \
        .option("host", "localhost") \
        .option("port", 9988) \
        .load()

    # Nov 24 13:17:01 spark CRON[18455]: (root) CMD (   cd / &amp;&amp; run-parts --report /etc/cron.hourly)
    # 定义一个偏应用函数，从固定的pattern获取日志内匹配的字段
    fields = partial(
        regexp_extract, str="value", pattern="^(\w{3}\s*\d{1,2} \d{2}:\d{2}:\d{2}) (.*?) (.*?)\[*\d*\]*: (.*)$"
    )

    words = lines.select(
        to_timestamp(format_string('2019 %s', fields(idx=1)), 'yy MMM d H:m:s').alias("timestamp"),
        fields(idx=2).alias("hostname"),
        fields(idx=3).alias("tag"),
        fields(idx=4).alias("content"),
    )

    # (3).  输出所有日志内容带error的日志。
    windowedCounts3 = words \
        .filter("content like '%error%'")

    # 开始运行查询并在控制台输出
    query = windowedCounts3 \
        .writeStream \
        .outputMode("update") \
        .format("console") \
        .option('truncate', 'false')\
        .trigger(processingTime="3 seconds") \
        .start()

    query.awaitTermination()</code></pre> 
<h3 id="%E5%9B%9B%E3%80%81%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E9%AA%8C%E4%BD%93%E4%BC%9A">四、结果分析与实验体会</h3> 
<p>        Spark Structured Streaming 是 Spark 提供的用于实时流处理的 API，它提供了一种统一的编程模型，使得批处理和流处理可以共享相同的代码逻辑，让开发者更容易地实现复杂的实时流处理任务。通过对 Structured Streaming 的实验，有以下体会：</p> 
<ol><li> <p><strong>简单易用</strong>: Structured Streaming 提供了高级抽象的 DataFrame 和 Dataset API，使得流处理变得类似于静态数据处理，降低了学习成本和编程复杂度。</p> </li><li> <p><strong>容错性强大</strong>: Structured Streaming 内置了端到端的 Exactly-Once 语义，能够保证在发生故障时数据处理的准确性，给开发者提供了更可靠的数据处理保障。</p> </li><li> <p><strong>灵活性和扩展性</strong>: Structured Streaming 支持丰富的数据源和数据接收器，可以方便地与其他数据存储和处理系统集成，同时也支持自定义数据源和输出操作，满足各种不同场景的需求。</p> </li><li> <p><strong>优化性能</strong>: Structured Streaming 内置了优化器和调度器，能够根据任务的特性自动优化执行计划，提升处理性能，同时还可以通过调整配置参数和优化代码来进一步提高性能。</p> </li><li> <p><strong>监控和调试</strong>: Structured Streaming 提供了丰富的监控指标和集成的调试工具，帮助开发者实时监控作业运行状态、诊断问题，并进行性能调优。</p> </li></ol> 
<p>        通过实验和实践，更深入地理解 Structured Streaming 的特性和工作原理，掌握实时流处理的开发技巧和最佳实践，为构建稳健可靠的实时流处理应用打下坚实基础。</p> 
<p>        Syslog 是一种常用的日志标准，它定义了一个网络协议，用于在计算机系统和网络设备之间传递事件消息和警报。通过对 Syslog 的实验，有以下体会：</p> 
<ol><li> <p><strong>灵活性</strong>: Syslog 可以用于收集各种类型的事件和日志信息，包括系统日志、安全事件、应用程序消息等等，具有很高的灵活性和可扩展性。</p> </li><li> <p><strong>可靠性</strong>: Syslog 提供了可靠的传输和存储机制，确保事件和日志信息不会丢失或损坏，在故障恢复和安全审计方面非常重要。</p> </li><li> <p><strong>标准化</strong>: Syslog 是一种通用的日志标准，已经被广泛采用和支持，可以与各种操作系统、应用程序、设备和服务集成，提供了统一的数据格式和接口。</p> </li><li> <p><strong>安全性</strong>: Syslog 支持基于 TLS 和 SSL 的加密和身份认证机制，确保传输的信息不会被窃听或篡改，保证了日志传输的安全性。</p> </li><li> <p><strong>可视化</strong>: 通过将 Syslog 收集到集中式的日志管理系统中，可以方便地进行搜索、分析和可视化，使日志信息变得更加易于理解和利用。</p> </li></ol> 
<p>        通过实验和实践，更深入地了解 Syslog 的工作原理和应用场景，学会如何配置和使用 Syslog，掌握日志收集、存储、分析和可视化的技巧和最佳实践，为构建高效、可靠、安全的日志管理系统打下坚实基础。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5095ef4ad2c3460ea09805157b869ccd/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">宿舍报修|宿舍报修小程序|基于微信小程序的宿舍报修系统的设计与实现(源码&#43;数据库&#43;文档)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2796b9f3171d924ca978bc83f1131d73/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">解决python setup.py egg_info did not run successfully.问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>