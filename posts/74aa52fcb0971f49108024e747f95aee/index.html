<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>text-generation-webui在linux服务器上的部署和运行（保姆教程/踩坑记录） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/74aa52fcb0971f49108024e747f95aee/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="text-generation-webui在linux服务器上的部署和运行（保姆教程/踩坑记录）">
  <meta property="og:description" content="目录
text-generation-webui部署（Linux服务器）：
项目下载：
Conda环境配置：
text-generation-webui运行：
本地运行：
服务器转发：
模型载入（CodeLLama-7b为例）：
最近在学习LLMs（大语言模型）相关的论文和代码实践，想要借助text-generation-webui作为部署和微调的界面工具。由于本地的算力和资源有限，我想要将项目都部署在linux命令行式服务器上，由于在CSDN上没有完整的教程，故个人摸索了很久，最终成功部署并运行。其中遇到的主要问题有：
如何将服务器上的内容转发至本地网页打开；环境配置问题：模型加载过程中出现报错：ImportError:.../flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol:_ZN3c104cuda9SetDeviceEi。 本篇将以部署CodeLLama-7b模型为例，手把手记录该webui的部署实践过程。
text-generation-webui部署（Linux服务器）： text-generation-webui简介：一款帮助LLMs实现本地化部署和微调的GUI界面式工具。
项目下载： github：https://github.com/oobabooga/text-generation-webui
首先需要从github上下载text-generation-webui项目至服务器，可以先本地下载再通过Xftp等工具传输（git下载因网络原因失败的话，可以尝试先从本地下载），也可以直接从服务器上进行下载，git指令如下：
git clone https://github.com/oobabooga/text-generation-webui.git 下载至服务器后，得到该项目的zip压缩包，通过unzip指令解压至所需目录即可得到项目文件。
unzip text-generation-webui-main Conda环境配置： 项目下载至服务器后，需要对项目所需环境进行配置（需要首先下载配置Anaconda，相关教程很多），该项目需要依赖python版本为3.10或3.11，其中[env_name]为环境名，自己命名一个即可。
conda create --name [env_name] python=3.11 进入到text-generation-webui目录中，激活该环境，下载该项目所需的依赖包。
conda activate [env_name] pip install -r requirements.txt 安装时间较长，安装好之后，同时需要安装该项目所需的启动文件，这里我们在linux服务器上运行，所以需要的文件是start_linux.sh，不同操作系统选择不同的安装文件。
bash start_linux.sh 至此，所需安装部分完成。
text-generation-webui运行： 本地运行： 通过运行server.py文件或start_linux.sh文件均可运行该项目：
python server.py ./start_linux.sh 可以看到，项目ui被转发至127.0.0.1:7860，倘若是图形化界面可以直接本地打开网页进入URL地址为127.0.0.1:7860即可在本地打开该webui界面。
服务器转发： 但这里我是在命令行服务器上运行，需要转发至本地PC网页中打开，所以这里需要添加--listen参数，转发并进行监听。
python server.py --listen 接下来就可以通过 [服务器地址：端口号] 进行访问了，但在这边我用的是学校内网的服务器，因此还需要服务器管理员帮我做一次转发，转发后即可通过[地址:端口号]在网页成功访问webui的网页界面了。
模型载入（CodeLLama-7b为例）： 可以看到，文件中有文件名为models的目录，我们需要把我们想运行的模型下载至models目录的文件夹下：
下载方式有很多，例如从huggingface中进行下载，具体可以参考这篇博客：【已解决】如何在服务器中下载huggingface模型，解决huggingface无法连接
下载完成后，打开webui中的models选项，然后选择该模型，点击load按钮：
顺利的话可以看到如下界面，模型被成功加载：
但不幸的是，我报错了（这里我用的别人的报错内容，但报错信息基本一致）：
Traceback (most recent call last): File &#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-22T22:10:48+08:00">
    <meta property="article:modified_time" content="2024-07-22T22:10:48+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">text-generation-webui在linux服务器上的部署和运行（保姆教程/踩坑记录）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="text-generation-webui%E9%83%A8%E7%BD%B2%EF%BC%88Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%EF%BC%89%EF%BC%9A-toc" style="margin-left:40px;"><a href="#text-generation-webui%E9%83%A8%E7%BD%B2%EF%BC%88Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%EF%BC%89%EF%BC%9A" rel="nofollow">text-generation-webui部署（Linux服务器）：</a></p> 
<p id="%E9%A1%B9%E7%9B%AE%E4%B8%8B%E8%BD%BD%EF%BC%9A-toc" style="margin-left:80px;"><a href="#%E9%A1%B9%E7%9B%AE%E4%B8%8B%E8%BD%BD%EF%BC%9A" rel="nofollow">项目下载：</a></p> 
<p id="Conda%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%EF%BC%9A-toc" style="margin-left:80px;"><a href="#Conda%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%EF%BC%9A" rel="nofollow">Conda环境配置：</a></p> 
<p id="text-generation-webui%E8%BF%90%E8%A1%8C%EF%BC%9A-toc" style="margin-left:40px;"><a href="#text-generation-webui%E8%BF%90%E8%A1%8C%EF%BC%9A" rel="nofollow">text-generation-webui运行：</a></p> 
<p id="%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C%EF%BC%9A-toc" style="margin-left:80px;"><a href="#%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C%EF%BC%9A" rel="nofollow">本地运行：</a></p> 
<p id="%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BD%AC%E5%8F%91%EF%BC%9A-toc" style="margin-left:80px;"><a href="#%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BD%AC%E5%8F%91%EF%BC%9A" rel="nofollow">服务器转发：</a></p> 
<p id="%E6%A8%A1%E5%9E%8B%E8%BD%BD%E5%85%A5%EF%BC%88CodeLLama-7b%E4%B8%BA%E4%BE%8B%EF%BC%89%EF%BC%9A-toc" style="margin-left:80px;"><a href="#%E6%A8%A1%E5%9E%8B%E8%BD%BD%E5%85%A5%EF%BC%88CodeLLama-7b%E4%B8%BA%E4%BE%8B%EF%BC%89%EF%BC%9A" rel="nofollow">模型载入（CodeLLama-7b为例）：</a></p> 
<hr> 
<p></p> 
<p>最近在学习LLMs（大语言模型）相关的论文和代码实践，想要借助text-generation-webui作为部署和微调的界面工具。由于本地的算力和资源有限，我想要将项目都部署在linux命令行式服务器上，由于在CSDN上没有完整的教程，故个人摸索了很久，最终成功部署并运行。其中遇到的主要问题有：</p> 
<ol><li>如何将服务器上的内容转发至本地网页打开；</li><li>环境配置问题：模型加载过程中出现报错：ImportError:.../flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol:_ZN3c104cuda9SetDeviceEi。</li></ol> 
<p>本篇将以部署CodeLLama-7b模型为例，手把手记录该webui的部署实践过程。</p> 
<p></p> 
<h3 id="text-generation-webui%E9%83%A8%E7%BD%B2%EF%BC%88Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%EF%BC%89%EF%BC%9A">text-generation-webui部署（Linux服务器）：</h3> 
<p>text-generation-webui简介：一款帮助LLMs实现本地化部署和微调的GUI界面式工具。</p> 
<h4 id="%E9%A1%B9%E7%9B%AE%E4%B8%8B%E8%BD%BD%EF%BC%9A">项目下载：</h4> 
<p>github：<a class="link-info" href="https://github.com/oobabooga/text-generation-webui" title="https://github.com/oobabooga/text-generation-webui">https://github.com/oobabooga/text-generation-webui</a></p> 
<p>首先需要从github上下载text-generation-webui项目至服务器，可以先本地下载再通过Xftp等工具传输（git下载因网络原因失败的话，可以尝试先从本地下载），也可以直接从服务器上进行下载，git指令如下：</p> 
<pre><code>git clone https://github.com/oobabooga/text-generation-webui.git</code></pre> 
<p>下载至服务器后，得到该项目的zip压缩包，通过unzip指令解压至所需目录即可得到项目文件。</p> 
<pre><code>unzip text-generation-webui-main</code></pre> 
<h4 id="%E2%80%8B%E7%BC%96%E8%BE%91"><img alt="" height="58" src="https://images2.imgbox.com/ef/e0/UeHqIcGp_o.png" width="764"></h4> 
<h4 id="Conda%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%EF%BC%9A">Conda环境配置：</h4> 
<p>项目下载至服务器后，需要对项目所需环境进行配置（需要首先下载配置Anaconda，相关教程很多），该项目需要依赖python版本为3.10或3.11，其中[env_name]为环境名，自己命名一个即可。</p> 
<pre><code>conda create --name [env_name] python=3.11</code></pre> 
<p>进入到text-generation-webui目录中，激活该环境，下载该项目所需的依赖包。</p> 
<pre><code>conda activate [env_name]
pip install -r requirements.txt</code></pre> 
<p>安装时间较长，安装好之后，同时需要安装该项目所需的启动文件，这里我们在linux服务器上运行，所以需要的文件是start_linux.sh，不同操作系统选择不同的安装文件。</p> 
<pre><code>bash start_linux.sh</code></pre> 
<p>至此，所需安装部分完成。</p> 
<p></p> 
<h3 id="text-generation-webui%E8%BF%90%E8%A1%8C%EF%BC%9A">text-generation-webui运行：</h3> 
<h4 id="%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C%EF%BC%9A">本地运行：</h4> 
<p><img alt="" height="125" src="https://images2.imgbox.com/eb/27/9pWKNAVd_o.png" width="721"></p> 
<p>通过运行server.py文件或start_linux.sh文件均可运行该项目：</p> 
<pre><code>python server.py</code></pre> 
<pre><code>./start_linux.sh</code></pre> 
<p><img alt="" height="85" src="https://images2.imgbox.com/24/05/M2qy0YF3_o.png" width="713"></p> 
<p>可以看到，项目ui被转发至127.0.0.1:7860，倘若是图形化界面可以直接本地打开网页进入URL地址为127.0.0.1:7860即可在本地打开该webui界面。</p> 
<h4 id="%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BD%AC%E5%8F%91%EF%BC%9A">服务器转发：</h4> 
<p>但这里我是在命令行服务器上运行，需要转发至本地PC网页中打开，所以这里需要添加--listen参数，转发并进行监听。</p> 
<pre><code>python server.py --listen</code></pre> 
<p><img alt="" height="145" src="https://images2.imgbox.com/e7/43/UPfy3bor_o.png" width="697"></p> 
<p>接下来就可以通过 [服务器地址：端口号] 进行访问了，但在这边我用的是学校内网的服务器，因此还需要服务器管理员帮我做一次转发，转发后即可通过[地址:端口号]在网页成功访问webui的网页界面了。</p> 
<p><img alt="" height="349" src="https://images2.imgbox.com/31/fe/xEOm2acS_o.png" width="694"></p> 
<p></p> 
<h4 id="%E6%A8%A1%E5%9E%8B%E8%BD%BD%E5%85%A5%EF%BC%88CodeLLama-7b%E4%B8%BA%E4%BE%8B%EF%BC%89%EF%BC%9A">模型载入（CodeLLama-7b为例）：</h4> 
<p>可以看到，文件中有文件名为models的目录，我们需要把我们想运行的模型下载至models目录的文件夹下：</p> 
<p><img alt="" height="122" src="https://images2.imgbox.com/8b/08/FxWdhoW9_o.png" width="720"></p> 
<p>下载方式有很多，例如从huggingface中进行下载，具体可以参考这篇博客：<a class="link-info" href="https://blog.csdn.net/a61022706/article/details/134887159?spm=1001.2014.3001.5506" title="【已解决】如何在服务器中下载huggingface模型，解决huggingface无法连接">【已解决】如何在服务器中下载huggingface模型，解决huggingface无法连接</a></p> 
<p><img alt="" height="233" src="https://images2.imgbox.com/aa/80/eWpjnrmr_o.png" width="751"></p> 
<p>下载完成后，打开webui中的models选项，然后选择该模型，点击load按钮：</p> 
<p><img alt="" height="358" src="https://images2.imgbox.com/fe/b3/0wTmJ9SP_o.png" width="711"></p> 
<p>顺利的话可以看到如下界面，模型被成功加载：</p> 
<p><img alt="" height="354" src="https://images2.imgbox.com/6b/60/2wRmeFPz_o.png" width="703"></p> 
<p>但不幸的是，我报错了（这里我用的别人的报错内容，但报错信息基本一致）：</p> 
<pre><code>Traceback (most recent call last):
 
File "C:\Users\Ma\AppData\Roaming\Python\Python310\site-packages\transformers\utils\import_utils.py", line 1353, in _get_module
 
 
return importlib.import_module("." + module_name, self.__name__)
File "D:\Anaconda\Anaconda\envs\codellama\lib\importlib_init_.py", line 126, in import_module
 
 
return _bootstrap._gcd_import(name[level:], package, level)
File "", line 1050, in _gcd_import
 
File "", line 1027, in _find_and_load
 
File "", line 1006, in _find_and_load_unlocked
 
File "", line 688, in _load_unlocked
 
File "", line 883, in exec_module
 
File "", line 241, in _call_with_frames_removed
 
File "C:\Users\Ma\AppData\Roaming\Python\Python310\site-packages\transformers\models\llama\modeling_llama.py", line 48, in
 
 
from flash_attn import flash_attn_func, flash_attn_varlen_func
File "C:\Users\Ma\AppData\Roaming\Python\Python310\site-packages\flash_attn_init_.py", line 3, in
 
 
from flash_attn.flash_attn_interface import (
File "C:\Users\Ma\AppData\Roaming\Python\Python310\site-packages\flash_attn\flash_attn_interface.py", line 8, in
 
 
import flash_attn_2_cuda as flash_attn_cuda
ImportError: DLL load failed while importing flash_attn_2_cuda: 找不到指定的模块。
 
The above exception was the direct cause of the following exception:
 
Traceback (most recent call last):
 
File "E:\模型\text-generation-webui\text-generation-webui\modules\ui_model_menu.py", line 209, in load_model_wrapper
 
 
shared.model, shared.tokenizer = load_model(shared.model_name, loader)
File "E:\模型\text-generation-webui\text-generation-webui\modules\models.py", line 85, in load_model
 
 
output = load_func_map[loader](model_name)
File "E:\模型\text-generation-webui\text-generation-webui\modules\models.py", line 155, in huggingface_loader
 
 
model = LoaderClass.from_pretrained(path_to_model, **params)
File "C:\Users\Ma\AppData\Roaming\Python\Python310\site-packages\transformers\models\auto\auto_factory.py", line 565, in from_pretrained
 
 
model_class = _get_model_class(config, cls._model_mapping)
File "C:\Users\Ma\AppData\Roaming\Python\Python310\site-packages\transformers\models\auto\auto_factory.py", line 387, in _get_model_class
 
 
supported_models = model_mapping[type(config)]
File "C:\Users\Ma\AppData\Roaming\Python\Python310\site-packages\transformers\models\auto\auto_factory.py", line 740, in getitem
 
 
return self._load_attr_from_module(model_type, model_name)
File "C:\Users\Ma\AppData\Roaming\Python\Python310\site-packages\transformers\models\auto\auto_factory.py", line 754, in _load_attr_from_module
 
 
return getattribute_from_module(self._modules[module_name], attr)
File "C:\Users\Ma\AppData\Roaming\Python\Python310\site-packages\transformers\models\auto\auto_factory.py", line 698, in getattribute_from_module
 
 
if hasattr(module, attr):
File "C:\Users\Ma\AppData\Roaming\Python\Python310\site-packages\transformers\utils\import_utils.py", line 1343, in getattr
 
 
module = self._get_module(self._class_to_module[name])
File "C:\Users\Ma\AppData\Roaming\Python\Python310\site-packages\transformers\utils\import_utils.py", line 1355, in _get_module
 
 
raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
 
DLL load failed while importing flash_attn_2_cuda: 找不到指定的模块。</code></pre> 
<p>其中我的核心报错内容如下：</p> 
<p>ImportError:.../flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol:_ZN3c104cuda9SetDeviceEi</p> 
<p><img alt="" height="112" src="https://images2.imgbox.com/99/5a/CXkIV5mR_o.png" width="718"></p> 
<p>这里我根据报错信息在网上搜索了很久，为数不多的解决方法提出了这是cuda和pytorch版本不匹配导致的，具体可以参考这篇文章：</p> 
<p><a class="link-info" href="https://blog.csdn.net/dandandancpop/article/details/134729988?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522172130426816800182176928%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=172130426816800182176928&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-134729988-null-null.142%5Ev100%5Epc_search_result_base4&amp;utm_term=ImportError%3A%20%2Fdata%2Fhome%2FShehongyu%2Fanaconda3%2Fenvs%2Fwebui%2Flib%2Fpython3.11%2Fsite-packages%2Fflash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so%3A%20undefined%20symbol%3A%20_ZN3c104cuda9SetDeviceEi&amp;spm=1018.2226.3001.4187" title="text-generation-webui加载codellama报错DLL load failed while importing flash_attn_2_cuda: 找不到指定的模块。">text-generation-webui加载codellama报错DLL load failed while importing flash_attn_2_cuda: 找不到指定的模块。</a></p> 
<p>按照这样去同步cuda和torch后，我的报错并没有得到解决，随后我发现这是flash_attn的版本问题，flash_attn作为加速组件，同样需要能够匹配cuda和torch的正确版本，很离谱的是，我按照该项目中requirement.txt文件所默认下载的flash_attn版本是不匹配默认下载的cuda和torch版本的，导致报错，这也是我找了很久才找到的原因。</p> 
<p>知道了报错原因，解决方法就很简单了，那就是重新装过合适的flash_attn版本依赖:</p> 
<p>flash_attn下载地址：<a class="link-info" href="https://github.com/Dao-AILab/flash-attention/releases" title="https://github.com/Dao-AILab/flash-attention/releases">https://github.com/Dao-AILab/flash-attention/releases</a></p> 
<p>从中可以发现报错原因，默认下载的flash_attn版本为2.6.1，只能支持torch2.0/2.1，而该项目的torch版本为2.3，支持版本不匹配导致报错。于是我下载了匹配环境torch2.3和匹配我的python环境的cp311版本的flash_attn。</p> 
<p><img alt="" height="239" src="https://images2.imgbox.com/8b/09/ookQ561a_o.png" width="704"></p> 
<p>下载至服务器，安装至conda运行环境中：</p> 
<pre><code>pip install flash_attn-2.6.0.post1+cu118torch2.3cxx11abiFALSE-cp311-cp311-linux_x86_64.whl</code></pre> 
<p>发现successfully load，加载成功！！</p> 
<p><img alt="" height="360" src="https://images2.imgbox.com/f3/c5/GPZ0UNk5_o.png" width="716"></p> 
<p>至此可以成功用服务器运行，转发至本地PC上使用text-generation-webui运行CodeLLama模型。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b74dd3073abb56a2e888b544137c93d8/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">QtQuick-QML类型系统-对象类型</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3bf58206da2a5e927859f1b90333e15f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">AV1技术学习：Transform Coding</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>