<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AIGC面经大全（持续更新） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/10e7a011da295d1fc1543b4877934f6b/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="AIGC面经大全（持续更新）">
  <meta property="og:description" content="目录
DDPM算法原理部分：
DDIM算法原理部分： ⾼阶采样⽅案：
特征编码篇：
Stable Diffusion篇： SDXL篇：
⼤模型微调篇：
控制模型篇：
适配器篇：
DDPM算法原理部分： 简述DDPM的算法流程：
初始化：从带噪声的图像开始。正向扩散：逐步向数据添加高斯噪声，直到数据完全转化为无结构的噪声。反向去噪：通过模型预测并逐渐去掉每一步加入的噪声，还原得到无噪声的图像。训练：使用反向传播算法更新模型参数，以最小化正向和反向过程之间的差异。测试：对新的高噪声图像应用训练好的模型进行去噪。 实现DDPM是否需要什么条件：
马尔可夫链：DDPM使用马尔可夫链来描述数据的扩散过程。马尔可夫链是一个随机过程，具有无记忆性，即在给定当前状态的情况下，未来的状态只依赖于当前状态。微小变化：DDPM通过逐步添加微小的高斯噪声来扩散数据。这些微小的变化是在数据中引入随机性的关键步骤。高斯噪声变化：DDPM使用高斯噪声来模拟数据的扩散过程。高斯噪声是一种常见的随机噪声，也称为正态分布噪声。 为什么DDPM加噪声的幅度是不⼀致的？
前期加噪少是为了保持数据结构的完整性，后期加噪多是为了加速扩散过程，使得模型能够更快地从噪声中恢复出清晰的数据。
DDPM预测噪声还是预测当前分布？
预测噪声，预测分布只是中间过程
DDIM算法原理部分： DDIM是怎么实现加速采样的？
DDIM通过保证DDPM的三项前向条件不变：前向⾼斯噪声&#43;⻢尔可夫链，实现逆向递推公式优化，减少逆向推理步骤
DDIM是不是确定性⽣成，为什么 是确定性⽣成。因为在逆向去噪声过程中，DDIM的逆推公式，将随机噪声的部分置为0
Score-Based-diffusion-model
提供了⼀种解释扩散模型的等价⽅式，其中降噪过程可以看作是沿着分数（梯度）前进
⾼阶采样⽅案： 是否了解DPM&#43;&#43;等加速采样⽅案
通过ODE对扩散模型进⾏建模，通过解析解的形式解构扩散模型求解步骤
特征编码篇： 介绍⼀下CLIP编码：
构建⼤规模的图像-⽂本数据构建（⽂本，图像）pair对，在其他下游⼦任务中取得极⾼的zero-shot指标
CLIP编码特征的优缺点
优点：泛化性能强，特征在同⼀空间下衡量，模型简单不需要额外训练。
缺陷：⽂本描述简单“A photo of a xxx”，图⽂理解能⼒偏弱
介绍⼀下BLIP/BLIP2的原理
BLIP：通过多路损失函数，以及图像分快理解策略等算法，构建⾼质量的图像理解模型。
BLIP2：在BLIP基础上，利用Q-Former构建图像与⼤语⾔模型之间的桥梁，充分利⽤⼤语⾔模型⾃身的预训练能⼒
为什么BLIP/BLIP2的特征没法直接⽤
因为受到⽂图⼀致性等隐形损失约束，相关特征不再同⼀个特征空间下（⽆法直接⽤距离衡量⽂图特征的相似性）。因此⽆法像CLIP⼀样“直接”接⼊模型中使⽤ Stable Diffusion篇： Stable Diffusion 的核⼼优化是什么？
通过VAE将特征映射到Latent Space，⼤幅减少运算量的同时还能保证⽣成质量。
通过Unet实现对⽣成内容的引导
Stable Diffusion是怎么训练的？
从训练集中选取一张加噪过的图片和噪声强度输入unet，让unet预测噪声图计算和真正的噪声图之间的误差通过反向传播更新unet的参数 VAE为什么会导致图像变模糊 VAE编解码整体是⼀个有损过程，可以选择减少损失，⽐如优化模型结构，提升采样效率等。完全不减少损失的⽅案就是原图反贴 介绍⼀下SD，Dall-E2两者的异同
Dalle2通过自回归的方式逐个预测像素点，最终生成符合描述的图像。
SD加⼊了Latent-Space（⼤幅降低特征维度），以及交叉注意⼒机制&#43;Unet的步骤，更精细更可控
介绍下classifier-free guidance和Classifier Guidance
Classifier Guidance的一般流程如下：首先单独预训练一个噪声鲁棒的分类器模型。然后训练一个普通的无条件Diffusion模型。Diffusion模型生成图像的采样过程中,利 用预训练好的分类器来提供条件信号。具体来说,就是每个采样步骤都计算分类器的输 出,获得条件影响项,加入到Diffusion模型的更新公式中。这样就可以利用分类器的条 件信号,实现Diffusion模型在推理阶段条件生成图像的目的。Classifier-Free Guidance 中，⽣成模型不仅仅学习如何根据给定的条件⽣成数据，⽽且还学习如何在没有任何条件输⼊的情况下⽣成数据。换句话说，模型既能进⾏条件⽣成，也能进⾏⽆条件⽣成。CFG的训练过程其实就是对提供的条件输入做随机的dropout，这样就可以得到一个无条件和条件提示的两个输出，然后学习二者之间的方向差指导采样过程。在⽣成过程中，Classifier-Free Guidance 允许我们在没有显式使⽤分类器或判别器的情况下调节条件⽣成的强度。这是通过“调节”（或“混合”）条件⽣成和⽆条件⽣成的输出来实现的，以此来控制⽣成内容的相关性和多样性noise_pred = noise_pred_uncond &#43; guidance_scale * (noise_pred_text - noise_pred_uncond) guidance scale是一个放缩系数，越大，生成的结果越倾向于输入条件，多样性会下降。越小，多样性越大。 Stable Diffusion 怎么实现⽂本和图像的条件控制的">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-02-01T16:39:00+08:00">
    <meta property="article:modified_time" content="2024-02-01T16:39:00+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AIGC面经大全（持续更新）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="DDPM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E9%83%A8%E5%88%86%EF%BC%9A-toc" style="margin-left:0px;"><a href="#DDPM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E9%83%A8%E5%88%86%EF%BC%9A" rel="nofollow">DDPM算法原理部分：</a></p> 
<p id="DDIM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E9%83%A8%E5%88%86%EF%BC%9A%C2%A0-toc" style="margin-left:0px;"><a href="#DDIM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E9%83%A8%E5%88%86%EF%BC%9A%C2%A0" rel="nofollow">DDIM算法原理部分： </a></p> 
<p id="%C2%A0%E2%BE%BC%E9%98%B6%E9%87%87%E6%A0%B7%E2%BD%85%E6%A1%88%EF%BC%9A-toc" style="margin-left:0px;"><a href="#%C2%A0%E2%BE%BC%E9%98%B6%E9%87%87%E6%A0%B7%E2%BD%85%E6%A1%88%EF%BC%9A" rel="nofollow"> ⾼阶采样⽅案：</a></p> 
<p id="%E7%89%B9%E5%BE%81%E7%BC%96%E7%A0%81%E7%AF%87%EF%BC%9A-toc" style="margin-left:0px;"><a href="#%E7%89%B9%E5%BE%81%E7%BC%96%E7%A0%81%E7%AF%87%EF%BC%9A" rel="nofollow">特征编码篇：</a></p> 
<p id="Stable%20Diffusion%E7%AF%87%EF%BC%9A%C2%A0-toc" style="margin-left:0px;"><a href="#Stable%20Diffusion%E7%AF%87%EF%BC%9A%C2%A0" rel="nofollow">Stable Diffusion篇： </a></p> 
<p id="%C2%A0SDXL%E7%AF%87%EF%BC%9A-toc" style="margin-left:0px;"><a href="#%C2%A0SDXL%E7%AF%87%EF%BC%9A" rel="nofollow"> SDXL篇：</a></p> 
<p id="%C2%A0%E2%BC%A4%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E7%AF%87%EF%BC%9A-toc" style="margin-left:0px;"><a href="#%C2%A0%E2%BC%A4%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E7%AF%87%EF%BC%9A" rel="nofollow"> ⼤模型微调篇：</a></p> 
<p id="%E6%8E%A7%E5%88%B6%E6%A8%A1%E5%9E%8B%E7%AF%87%EF%BC%9A-toc" style="margin-left:0px;"><a href="#%E6%8E%A7%E5%88%B6%E6%A8%A1%E5%9E%8B%E7%AF%87%EF%BC%9A" rel="nofollow">控制模型篇：</a></p> 
<p id="%C2%A0%E9%80%82%E9%85%8D%E5%99%A8%E7%AF%87%EF%BC%9A-toc" style="margin-left:0px;"><a href="#%C2%A0%E9%80%82%E9%85%8D%E5%99%A8%E7%AF%87%EF%BC%9A" rel="nofollow"> 适配器篇：</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="DDPM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E9%83%A8%E5%88%86%EF%BC%9A" style="margin-left:.0001pt;text-align:justify;"><strong><strong>DDPM算法原理部分：</strong></strong></h2> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="text-align:justify;"><strong><strong>简述DDPM的算法流程：</strong></strong></p> 
<blockquote> 
 <ol><li style="text-align:justify;">初始化：从带噪声的图像开始。</li><li style="text-align:justify;">正向扩散：逐步向数据添加高斯噪声，直到数据完全转化为无结构的噪声。</li><li style="text-align:justify;">反向去噪：通过模型预测并逐渐去掉每一步加入的噪声，还原得到无噪声的图像。</li><li style="text-align:justify;">训练：使用反向传播算法更新模型参数，以最小化正向和反向过程之间的差异。</li><li style="text-align:justify;">测试：对新的高噪声图像应用训练好的模型进行去噪。</li></ol> 
</blockquote> 
<p style="text-align:justify;"><strong><strong>实现DDPM是否需要什么条件：</strong></strong></p> 
<blockquote> 
 <ol><li style="text-align:justify;">马尔可夫链：DDPM使用马尔可夫链来描述数据的扩散过程。马尔可夫链是一个随机过程，具有无记忆性，即在给定当前状态的情况下，未来的状态只依赖于当前状态。</li><li style="text-align:justify;">微小变化：DDPM通过逐步添加微小的高斯噪声来扩散数据。这些微小的变化是在数据中引入随机性的关键步骤。</li><li style="text-align:justify;">高斯噪声变化：DDPM使用高斯噪声来模拟数据的扩散过程。高斯噪声是一种常见的随机噪声，也称为正态分布噪声。</li></ol> 
</blockquote> 
<p style="text-align:justify;"><strong><strong>为什么DDPM加噪声的幅度是不⼀致的？</strong></strong></p> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">前期加噪少是为了保持数据结构的完整性，后期加噪多是为了加速扩散过程，使得模型能够更快地从噪声中恢复出清晰的数据。</p> 
</blockquote> 
<p style="text-align:justify;"><strong><strong>DDPM预测噪声还是预测当前分布？</strong></strong></p> 
<blockquote> 
 <p style="text-align:justify;">预测噪声，预测分布只是中间过程</p> 
</blockquote> 
<h2 id="DDIM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E9%83%A8%E5%88%86%EF%BC%9A%C2%A0"><strong><strong>DDIM算法原理部分：</strong></strong> </h2> 
<p style="text-align:justify;"><strong><strong>DDIM是怎么实现加速采样的？</strong></strong></p> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">DDIM通过保证DDPM的三项前向条件不变：前向⾼斯噪声+⻢尔可夫链，实现逆向递推公式优化，减少逆向推理步骤</p> 
</blockquote> 
<p><strong><strong>DDIM是不是确定性⽣成，为什么</strong></strong> </p> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">是确定性⽣成。因为在逆向去噪声过程中，DDIM的逆推公式，将随机噪声的部分置为0</p> 
</blockquote> 
<blockquote> 
 <p> <img alt="" height="392" src="https://images2.imgbox.com/0d/fa/e3eJgPxs_o.png" width="554"></p> 
</blockquote> 
<p style="text-align:justify;"><strong><strong>Score-Based-diffusion-model</strong></strong></p> 
<blockquote> 
 <p style="text-align:justify;">提供了⼀种解释扩散模型的等价⽅式，其中降噪过程可以看作是沿着分数（梯度）前进</p> 
</blockquote> 
<h2 id="%C2%A0%E2%BE%BC%E9%98%B6%E9%87%87%E6%A0%B7%E2%BD%85%E6%A1%88%EF%BC%9A"> <strong><strong>⾼阶采样⽅案：</strong></strong></h2> 
<p></p> 
<p style="text-align:justify;"><strong><strong>是否了解DPM++等加速采样⽅案</strong></strong></p> 
<blockquote> 
 <p style="text-align:justify;"></p> 
 <p style="text-align:justify;">通过ODE对扩散模型进⾏建模，通过解析解的形式解构扩散模型求解步骤</p> 
</blockquote> 
<h2 id="%E7%89%B9%E5%BE%81%E7%BC%96%E7%A0%81%E7%AF%87%EF%BC%9A"><strong><strong>特征编码篇：</strong></strong></h2> 
<h2></h2> 
<p style="text-align:justify;"><strong><strong>介绍⼀下CLIP编码：</strong></strong></p> 
<blockquote> 
 <p style="text-align:justify;">构建⼤规模的图像-⽂本数据构建（⽂本，图像）pair对，在其他下游⼦任务中取得极⾼的zero-shot指标</p> 
</blockquote> 
<p><strong><strong>CLIP编码特征的优缺点</strong></strong></p> 
<blockquote> 
 <p></p> 
 <p style="text-align:justify;">优点：泛化性能强，特征在同⼀空间下衡量，模型简单不需要额外训练。</p> 
 <p style="margin-left:.0001pt;text-align:justify;">缺陷：⽂本描述简单“A photo of a xxx”，图⽂理解能⼒偏弱</p> 
</blockquote> 
<p><strong><strong>介绍⼀下BLIP/BLIP2的原理</strong></strong></p> 
<blockquote> 
 <p></p> 
 <p style="text-align:justify;">BLIP：通过多路损失函数，以及图像分快理解策略等算法，构建⾼质量的图像理解模型。</p> 
 <p style="margin-left:.0001pt;text-align:justify;">BLIP2：在BLIP基础上，利用Q-Former构建图像与⼤语⾔模型之间的桥梁，充分利⽤⼤语⾔模型⾃身的预训练能⼒</p> 
 <p></p> 
</blockquote> 
<p><strong><strong>为什么BLIP/BLIP2的特征没法直接⽤</strong></strong></p> 
<blockquote> 
 <p>因为受到⽂图⼀致性等隐形损失约束，相关特征不再同⼀个特征空间下（⽆法直接⽤距离衡量⽂图特征的相似性）。因此⽆法像CLIP⼀样“直接”接⼊模型中使⽤ </p> 
</blockquote> 
<h2 id="Stable%20Diffusion%E7%AF%87%EF%BC%9A%C2%A0"><strong><strong>Stable Diffusion篇：</strong></strong> </h2> 
<p style="text-align:justify;"><strong><strong>Stable Diffusion 的核⼼优化是什么？</strong></strong></p> 
<blockquote> 
 <p style="text-align:justify;">通过VAE将特征映射到Latent Space，⼤幅减少运算量的同时还能保证⽣成质量。</p> 
 <p style="margin-left:.0001pt;text-align:justify;">通过Unet实现对⽣成内容的引导</p> 
</blockquote> 
<p style="text-align:justify;"><strong>Stable Diffusion</strong><strong>是怎么训练的？</strong></p> 
<blockquote> 
 <ul><li style="text-align:justify;">从训练集中选取一张加噪过的图片和噪声强度</li><li style="text-align:justify;">输入unet，让unet预测噪声图</li><li style="text-align:justify;">计算和真正的噪声图之间的误差</li><li style="text-align:justify;">通过反向传播更新unet的参数</li></ul> 
</blockquote> 
<p><strong><strong>VAE为什么会导致图像变模糊</strong></strong> </p> 
<blockquote> 
 <p>VAE编解码整体是⼀个有损过程，可以选择减少损失，⽐如优化模型结构，提升采样效率等。完全不减少损失的⽅案就是原图反贴 </p> 
</blockquote> 
<p><strong><strong>介绍⼀下SD，Dall-E2两者的异同</strong></strong></p> 
<blockquote> 
 <p></p> 
 <p style="text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#191a24;">Dalle2通过自回归的方式逐个预测像素点，最终生成符合描述的图像。</span></span></p> 
 <p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#191a24;">SD加⼊了Latent-Space（⼤幅降低特征维度），以及交叉注意⼒机制+Unet的步骤，更精细更可控</span></span></p> 
</blockquote> 
<p><strong><span style="background-color:#ffffff;"><span style="color:#191a24;"><strong>介绍下classifier-free guidance</strong></span></span></strong><strong><span style="background-color:#ffffff;"><span style="color:#191a24;"><strong>和Classifier Guidance</strong></span></span></strong></p> 
<blockquote> 
 <ul><li style="text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#191a24;">Classifier Guidance的一般流程如下：首先单独预训练一个噪声鲁棒的分类器模型。</span></span><span style="background-color:#ffffff;"><span style="color:#191a24;">然后训练一个普通的无条件Diffusion模型。Diffusion模型生成图像的采样过程中,利</span></span> <span style="background-color:#ffffff;"><span style="color:#191a24;">用预训练好的分类器来提供条件信号。具体来说,就是每个采样步骤都计算分类器的输</span></span> <span style="background-color:#ffffff;"><span style="color:#191a24;">出,获得条件影响项,加入到Diffusion模型的更新公式中。这样就可以利用分类器的条</span></span> <span style="background-color:#ffffff;"><span style="color:#191a24;">件信号,实现Diffusion模型在推理阶段条件生成图像的目的。</span></span></li><li style="text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#191a24;">Classifier-Free Guidance 中，⽣成模型不仅仅学习如何根据给定的条件⽣成数据，⽽且还学习如何在没有任何条件输⼊的情况下⽣成数据。换句话说，模型既能进⾏条件⽣成，也能进⾏⽆条件⽣成。CFG的训练过程其实就是对提供的条件输入做随机的dropout，这样就可以得到一个无条件和条件提示的两个输出，然后学习二者之间的方向差指导采样过程</span></span><span style="background-color:#ffffff;"><span style="color:#191a24;">。</span></span><span style="background-color:#ffffff;"><span style="color:#191a24;">在⽣成过程中，Classifier-Free Guidance 允许我们在没有显式使⽤分类器或判别器的情况下调节条件⽣成的强度。这是通过“调节”（或“混合”）条件⽣成和⽆条件⽣成的输出来实现的，以此来控制⽣成内容的相关性和多样性</span></span></li><li style="text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#191a24;">noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)</span></span>      <span style="background-color:#ffffff;"><span style="color:#191a24;">guidance scale是一个放缩系数，越大，生成的结果越倾向于输入条件，多样性会下降。越小，多样性越大。</span></span></li></ul> 
</blockquote> 
<p><strong><span style="background-color:#ffffff;"><span style="color:#191a24;"><strong>Stable Diffusion 怎么实现⽂本和图像的条件控制的</strong></span></span></strong></p> 
<blockquote> 
 <p><span style="background-color:#ffffff;"><span style="color:#191a24;">⽂本/图像编码器将⽂本/图像信息编码，然后通过交叉注意⼒机制将信息引⼊扩散模型。SD 的 U-Net 既用到了自注意力，也用到了交叉注意力。自注意力用于图像特征自己内部信息聚合。交叉注意力用于让生成图像对齐文本，其 Q 来自图像特征，K, V 来自文本编码</span></span> </p> 
</blockquote> 
<p><strong><strong>扩散模型添加时间步timestep信息</strong></strong></p> 
<blockquote> 
 <p> <span style="color:#262626;">通过类似于Transformer中的位置编码⽅法，将常数转换为向量并添加到输⼊图像中</span></p> 
</blockquote> 
<p><strong><strong>Noise Scheduler了解吗</strong></strong> </p> 
<blockquote> 
 <p>Noise Scheduler定义了⼀个⾼斯分布，其均值和⽅差随着时间步的变化⽽变化，以控制噪声的添加量 </p> 
</blockquote> 
<p><strong><strong>Stable Diffusion核⼼模块有哪些</strong></strong></p> 
<blockquote> 
 <p></p> 
 <p style="text-align:justify;">VAE：将图像特征/⽂本特征，映射到Latent Space。</p> 
 <p style="text-align:justify;">LDM相关：Diffusion Model +Unet，去噪声核⼼步骤</p> 
 <p style="margin-left:.0001pt;text-align:justify;">Conditioning：作⽤于Unet的 Cross-Attention位置，实现对输出结果的控制</p> 
</blockquote> 
<p><strong><strong>为什么原⽣SD的控制效果不太好，需要引⼊如ControlNet的控制模型</strong></strong> </p> 
<blockquote> 
 <p>因为控制是⼀个隐性控制模型，通过CrossAttention的权重隐性引导⽣成结果，并不是完全控制 </p> 
</blockquote> 
<h2 id="%C2%A0SDXL%E7%AF%87%EF%BC%9A"> <strong><strong>SDXL篇：</strong></strong></h2> 
<p><strong><strong>SDXL的核⼼优化</strong></strong></p> 
<blockquote> 
 <p></p> 
 <p style="text-align:justify;">接⼊级联的refiner模型+微调⽹络结构，⼤幅度提升⽣成质量。</p> 
 <p style="margin-left:.0001pt;text-align:justify;">多样化的训练策略，⼤幅提升基础模型表达能⼒</p> 
</blockquote> 
<p style="text-align:justify;"><strong><strong>SDXL的训练策略：</strong></strong></p> 
<blockquote> 
 <p style="text-align:left;"><span style="color:#262626;">图像尺⼨条件化：把图像的尺⼨编码后作为信息输⼊到模型中。 </span></p> 
 <p style="text-align:left;"><span style="color:#262626;">裁剪参数化训练：裁剪坐标也和尺⼨⼀样送⼊模型中。 </span></p> 
 <p style="text-align:left;"><span style="color:#262626;">多尺度训练：多尺度+分桶 </span></p> 
 <p style="margin-left:.0001pt;text-align:justify;"><span style="color:#262626;">噪声偏置：针对冷⻔⾊域，加⼊初始化噪声偏置</span></p> 
</blockquote> 
<h2 id="%C2%A0%E2%BC%A4%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E7%AF%87%EF%BC%9A"> <strong><span style="color:#262626;"><strong>⼤模型微调篇：</strong></span></strong></h2> 
<p> <strong><strong>Lora：</strong></strong></p> 
<blockquote> 
 <p></p> 
 <p style="text-align:left;"><span style="color:#262626;">核⼼解读关键词：低秩展开，即插即⽤ </span></p> 
 <p style="margin-left:.0001pt;text-align:justify;"><span style="color:#191b1f;">通过矩阵低秩展开，使⽤“外接”低秩展开后的⽹络对原模型进⾏更新</span></p> 
 <p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="286" src="https://images2.imgbox.com/b0/9a/ylvTzDpu_o.png" width="490"></p> 
</blockquote> 
<p><strong><strong>Lora有没有什么优化⽅案</strong></strong></p> 
<blockquote> 
 <p></p> 
 <p style="text-align:justify;">Locon/loha，分别进⾏细节质量和速度存储空间的优化</p> 
</blockquote> 
<p><strong><strong>DreamBooth</strong></strong></p> 
<blockquote> 
 <p></p> 
 <p style="text-align:left;"><span style="color:#191b1f;">核⼼解读关键词：正则化微调整个⽹络，训练数据混合</span></p> 
 <p style="margin-left:.0001pt;text-align:justify;"><span style="color:#262626;">因为使⽤正则化，只在预训练⽹络上微调某类特定的case。 所以速度反⽽⽐Lora快得多</span></p> 
 <p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="433" src="https://images2.imgbox.com/90/9b/HKqk6SgR_o.png" width="554"></p> 
 <p></p> 
</blockquote> 
<p><strong><strong>Textual Inversion</strong></strong></p> 
<blockquote> 
 <p></p> 
 <ul><li style="text-align:left;"><span style="color:#191b1f;">关键词：⽂本embedding，Transformer </span></li><li style="text-align:left;"><span style="color:#191b1f;">核⼼总结：通过对Embedding层的特殊编码，实现通过不同输⼊⽂本，来影响模型最终的⽣成结果。影响的是Embedding的部分</span></li><li style="text-align:left;">首先需要定义一个在现有模型中没有的关键词，新的关键词会和其他的关键词一样，生成Tokenizer(用不同的数字表示)；然后将其转换为embedding；text transformer会映射出对于新给的关键词最好的embedding向量。不用改变模型，可以看作在模型中寻找新的表征来表示新的关键字<img alt="" height="405" src="https://images2.imgbox.com/ff/26/7TvuQ8Ri_o.png" width="466"></li></ul> 
</blockquote> 
<p><strong><strong>Lora/Dreambooth/Textual Inversion，核⼼差异点</strong></strong></p> 
<blockquote> 
 <p></p> 
 <ul><li style="text-align:left;"><span style="color:#262626;">Lora：是⼩模型即插即⽤微调。 </span></li><li style="text-align:left;"><span style="color:#262626;">Dreambooth：⼤模型特化全量微调 </span> 
   <ul><li style="margin-left:.0001pt;text-align:justify;"><span style="color:#262626;">Textual Inversion：Text-embedding 编码修改</span></li></ul></li></ul> 
</blockquote> 
<h2 id="%E6%8E%A7%E5%88%B6%E6%A8%A1%E5%9E%8B%E7%AF%87%EF%BC%9A"><strong><strong>控制模型篇：</strong></strong></h2> 
<p><strong><strong>介绍⼀下ControlNet的核⼼原理</strong></strong></p> 
<blockquote> 
 <p></p> 
 <ul><li style="text-align:left;"><span style="color:#262626;">复制原⽣Unet⼀样的模型结构，前半部分</span><span style="color:#262626;">encoder</span><span style="color:#262626;">训练，后半部分⽤</span><span style="color:#191b1f;">Zero Convolution 承接，</span><span style="color:#191b1f;">decoder部分</span><span style="color:#191b1f;">接⼊到模型Unet的⽹络层中</span></li><li style="text-align:left;"><span style="color:#191b1f;">“Zero Convolution”即零卷积：是带有零初始化权重和偏差的1×1卷积。在进⾏⾃⼰的模型训练开始之前，所有零卷积输出都是零，此时模型仍然是原始的Stable Diffusion Model</span><img alt="" height="486" src="https://images2.imgbox.com/bf/3b/r88eBn31_o.png" width="550"></li></ul> 
</blockquote> 
<h2 id="%C2%A0%E9%80%82%E9%85%8D%E5%99%A8%E7%AF%87%EF%BC%9A"> <strong><strong>适配器篇：</strong></strong></h2> 
<p><strong><strong>T2I Adapter</strong></strong></p> 
<blockquote> 
 <p></p> 
 <p style="text-align:justify;">每张条件图片都会别额外编码，编码信息会被加入到 UNET 噪声预测中</p> 
 <p style="text-align:justify;">训练时候，冻结了原先的 unet，只对 Adapter 部分进行单独训练</p> 
 <p style="text-align:justify;"><img alt="" height="313" src="https://images2.imgbox.com/a1/36/5ln2GFP9_o.png" width="555"></p> 
</blockquote> 
<p> <strong><strong>IP-Adapter</strong></strong></p> 
<blockquote> 
 <p>IP-Adapter 通过带有解耦交叉注意力的适配模块，将文本特征的 Cross-Attention 和图像特征的 Cross-Attention 区分开来，在 Unet 的模块中新增了一路 Cross-Attention 模块，用于引入图像特征</p> 
 <p><img alt="" height="358" src="https://images2.imgbox.com/f0/69/4OApbwsD_o.png" width="554"></p> 
 <p><img alt="" height="281" src="https://images2.imgbox.com/10/a4/L6Rnclko_o.png" width="554"></p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1b431d519fde82d36a5a4997fbc9062f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">SpringBoot Test详解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/002e921f69a77a9714b2297486fd9dcc/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">一文通透位置编码：从标准位置编码、旋转位置编码RoPE到ALiBi、LLaMA 2 Long</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>