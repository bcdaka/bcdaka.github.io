<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>5、DataX（DataX简介、DataX架构原理、DataX部署、使用、同步MySQL数据到HDFS、同步HDFS数据到MySQL） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/98030e4b1a8daf8525d533999ba20274/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="5、DataX（DataX简介、DataX架构原理、DataX部署、使用、同步MySQL数据到HDFS、同步HDFS数据到MySQL）">
  <meta property="og:description" content="1、DataX简介 1.1 DataX概述 DataX 是阿里巴巴开源的一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。 源码地址：https://github.com/alibaba/DataX
1.2 DataX支持的数据源 DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入，目前支持数据如下图。
2、DataX架构原理 2.1 DataX设计理念 为了解决异构数据源同步问题，DataX将复杂的网状的同步链路变成了星型数据链路，DataX作为中间传输载体负责连接各种数据源。当需要接入一个新的数据源的时候，只需要将此数据源对接到DataX，便能跟已有的数据源做到无缝数据同步。
2.2 DataX框架设计 DataX本身作为离线数据同步框架，采用Framework &#43; plugin架构构建。将数据源读取和写入抽象成为Reader/Writer插件，纳入到整个同步框架中。
Reader：数据采集模块，负责采集数据源的数据，将数据发送给Framework。
Writer：数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。
Framework：用于连接Reader和Writer，作为两者的数据传输通道，并处理缓存，流控，并发，数据转换等核心技术问题。
2.3 DataX运行流程 下面用一个DataX作业生命周期的时序图说明DataX的运行流程、核心概念以及每个概念之间的关系。
2.4 DataX调度决策思路 举例来说，用户提交了一个DataX作业，并且配置了总的并发度为20，目的是对一个有100张分表的mysql数据源进行同步。DataX的调度决策思路是：
1）DataX Job根据分库分表切分策略，将同步工作分成100个Task。
2）根据配置的总的并发度20，以及每个Task Group的并发度5，DataX计算共需要分配4个TaskGroup。
3）4个TaskGroup平分100个Task，每一个TaskGroup负责运行25个Task。
2.5 DataX和Sqoop对比 3、DataX部署 1、下载DataX安装包并上传到hadoop102的/opt/software
下载地址：http://datax-opensource.oss-cn-hangzhou.aliyuncs.com/datax.tar.gz
2、解压datax.tar.gz到/opt/module
tar -zxvf datax.tar.gz -C /opt/module/ 3、自检，执行如下命令
python /opt/module/datax/bin/datax.py /opt/module/datax/job/job.json 4、出现如下内容，则表明安装成功
4、DataX使用 4.1 DataX使用概述 4.1.1 DataX任务提交命令 Datax的使用十分简单，用户只需要根据自己同步数据的数据源和目的地选择相应的Reader和Writer，并将Reader和Writer的信息配置在一个json文件中，然后执行如下命令提交数据同步任务即可。
python bin/datax.py path/to/your/job.json 4.1.2 DataX配置文件格式 可以使用如下命名查看DataX配置文件模板。
python bin/datax.py -r mysqlreader -w hdfswriter 配置文件模板如下，json最外层是一个job，job包含setting和content两部分，其中setting用于对整个job进行配置，content用户配置数据源和目的地。
4.2 同步MySQL数据到HDFS案例 案例要求：同步gmall数据库中base_province表数据到HDFS的/base_province目录">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-06-29T19:27:27+08:00">
    <meta property="article:modified_time" content="2023-06-29T19:27:27+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">5、DataX（DataX简介、DataX架构原理、DataX部署、使用、同步MySQL数据到HDFS、同步HDFS数据到MySQL）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="1DataX_0"></a>1、DataX简介</h3> 
<h5><a id="11_DataX_1"></a>1.1 DataX概述</h5> 
<pre><code>DataX 是阿里巴巴开源的一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。
</code></pre> 
<p>源码地址：https://github.com/alibaba/DataX</p> 
<h5><a id="12_DataX_5"></a>1.2 DataX支持的数据源</h5> 
<p>DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入，目前支持数据如下图。<br> <img src="https://images2.imgbox.com/a1/59/Y2iK1dEH_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="2DataX_9"></a>2、DataX架构原理</h3> 
<h5><a id="21_DataX_10"></a>2.1 DataX设计理念</h5> 
<p>为了解决异构数据源同步问题，DataX将复杂的网状的同步链路变成了星型数据链路，DataX作为中间传输载体负责连接各种数据源。当需要接入一个新的数据源的时候，只需要将此数据源对接到DataX，便能跟已有的数据源做到无缝数据同步。<br> <img src="https://images2.imgbox.com/20/18/urQQaSlS_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="22_DataX_13"></a>2.2 DataX框架设计</h5> 
<p>DataX本身作为离线数据同步框架，采用Framework + plugin架构构建。将数据源读取和写入抽象成为Reader/Writer插件，纳入到整个同步框架中。<br> Reader：数据采集模块，负责采集数据源的数据，将数据发送给Framework。<br> Writer：数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。<br> Framework：用于连接Reader和Writer，作为两者的数据传输通道，并处理缓存，流控，并发，数据转换等核心技术问题。</p> 
<h5><a id="23_DataX_19"></a>2.3 DataX运行流程</h5> 
<p>下面用一个DataX作业生命周期的时序图说明DataX的运行流程、核心概念以及每个概念之间的关系。<br> <img src="https://images2.imgbox.com/7a/fc/xOvsqG3j_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="24_DataX_22"></a>2.4 DataX调度决策思路</h5> 
<p>举例来说，用户提交了一个DataX作业，并且配置了总的并发度为20，目的是对一个有100张分表的mysql数据源进行同步。DataX的调度决策思路是：<br> 1）DataX Job根据分库分表切分策略，将同步工作分成100个Task。<br> 2）根据配置的总的并发度20，以及每个Task Group的并发度5，DataX计算共需要分配4个TaskGroup。<br> 3）4个TaskGroup平分100个Task，每一个TaskGroup负责运行25个Task。</p> 
<h5><a id="25_DataXSqoop_28"></a>2.5 DataX和Sqoop对比</h5> 
<p><img src="https://images2.imgbox.com/c7/47/Upp9rdsb_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="3DataX_30"></a>3、DataX部署</h3> 
<p>1、下载DataX安装包并上传到hadoop102的/opt/software<br> 下载地址：http://datax-opensource.oss-cn-hangzhou.aliyuncs.com/datax.tar.gz<br> 2、解压datax.tar.gz到/opt/module</p> 
<pre><code class="prism language-bash"> <span class="token function">tar</span> -zxvf datax.tar.gz -C /opt/module/
</code></pre> 
<p>3、自检，执行如下命令</p> 
<pre><code class="prism language-bash"> python /opt/module/datax/bin/datax.py /opt/module/datax/job/job.json
</code></pre> 
<p>4、出现如下内容，则表明安装成功<br> <img src="https://images2.imgbox.com/01/98/abSJ12nQ_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="4DataX_45"></a>4、DataX使用</h3> 
<h5><a id="41_DataX_46"></a>4.1 DataX使用概述</h5> 
<h6><a id="411_DataX_47"></a>4.1.1 DataX任务提交命令</h6> 
<p>Datax的使用十分简单，用户只需要根据自己同步数据的数据源和目的地选择相应的Reader和Writer，并将Reader和Writer的信息配置在一个json文件中，然后执行如下命令提交数据同步任务即可。</p> 
<pre><code class="prism language-bash"> python bin/datax.py path/to/your/job.json
</code></pre> 
<h6><a id="412_DataX_53"></a>4.1.2 DataX配置文件格式</h6> 
<p>可以使用如下命名查看DataX配置文件模板。</p> 
<pre><code class="prism language-bash">python bin/datax.py -r mysqlreader -w hdfswriter
</code></pre> 
<p>配置文件模板如下，json最外层是一个job，job包含setting和content两部分，其中setting用于对整个job进行配置，content用户配置数据源和目的地。<br> <img src="https://images2.imgbox.com/af/7b/e1vYvHvj_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="42_MySQLHDFS_61"></a>4.2 同步MySQL数据到HDFS案例</h5> 
<p>案例要求：同步gmall数据库中base_province表数据到HDFS的/base_province目录<br> 需求分析：要实现该功能，需选用MySQLReader和HDFSWriter，MySQLReader具有两种模式分别是TableMode和QuerySQLMode，前者使用table，column，where等属性声明需要同步的数据；后者使用一条SQL查询语句声明需要同步的数据。<br> 下面分别使用两种模式进行演示。</p> 
<h6><a id="421_MySQLReaderTableMode_65"></a>4.2.1 MySQLReader之TableMode</h6> 
<p>1、编写配置文件<br> （1）创建配置文件base_province.json</p> 
<pre><code class="prism language-bash"><span class="token function">vim</span> /opt/module/datax/job/base_province.json
</code></pre> 
<p>（2）配置文件内容如下</p> 
<pre><code>{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "column": [
                            "id",
                            "name",
                            "region_id",
                            "area_code",
                            "iso_code",
                            "iso_3166_2"
                        ],
                        "where": "id&gt;=3",
                        "connection": [
                            {
                                "jdbcUrl": [
                                    "jdbc:mysql://hadoop102:3306/gmall"
                                ],
                                "table": [
                                    "base_province"
                                ]
                            }
                        ],
                        "password": "000000",
                        "splitPk": "",
                        "username": "root"
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "column": [
                            {
                                "name": "id",
                                "type": "bigint"
                            },
                            {
                                "name": "name",
                                "type": "string"
                            },
                            {
                                "name": "region_id",
                                "type": "string"
                            },
                            {
                                "name": "area_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_3166_2",
                                "type": "string"
                            }
                        ],
                        "compress": "gzip",
                        "defaultFS": "hdfs://hadoop102:8020",
                        "fieldDelimiter": "\t",
                        "fileName": "base_province",
                        "fileType": "text",
                        "path": "/base_province",
                        "writeMode": "append"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": 1
            }
        }
    }
}
</code></pre> 
<p>2、配置文件说明<br> （1）Reader参数说明<br> <img src="https://images2.imgbox.com/83/a2/ujiaOZ2T_o.png" alt="在这里插入图片描述"><br> （2）Writer参数说明<br> <img src="https://images2.imgbox.com/d3/8c/Lo2jvgSS_o.png" alt="在这里插入图片描述"><br> 注意事项：<br> HFDS Writer并未提供nullFormat参数：也就是用户并不能自定义null值写到HFDS文件中的存储格式。默认情况下，HFDS Writer会将null值存储为空字符串（‘’），而Hive默认的null值存储格式为\N。所以后期将DataX同步的文件导入Hive表就会出现问题。<br> （3）Setting参数说明<br> <img src="https://images2.imgbox.com/6b/c2/vzP44QC7_o.png" alt="在这里插入图片描述"><br> 3、提交任务<br> （1）在HDFS创建/base_province目录<br> 使用DataX向HDFS同步数据时，需确保目标路径已存在</p> 
<pre><code class="prism language-bash">hadoop fs -mkdir /base_province
</code></pre> 
<p>（2）进入DataX根目录<br> （3）执行如下命令</p> 
<pre><code class="prism language-bash"> python bin/datax.py job/base_province.json 
</code></pre> 
<p>4、查看结果<br> （1）DataX打印日志<br> <img src="https://images2.imgbox.com/28/f6/cbLVMirW_o.png" alt="在这里插入图片描述"><br> （2）查看HDFS文件</p> 
<pre><code class="prism language-bash">hadoop fs -cat /base_province/* <span class="token operator">|</span> zcat
</code></pre> 
<h6><a id="422_MySQLReaderQuerySQLMode_184"></a>4.2.2 MySQLReader之QuerySQLMode</h6> 
<p>1、编写配置文件<br> （1）修改配置文件base_province.json<br> （2）配置文件内容如下</p> 
<pre><code>{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "connection": [
                            {
                                "jdbcUrl": [
                                    "jdbc:mysql://hadoop102:3306/gmall"
                                ],
                                "querySql": [
                                    "select id,name,region_id,area_code,iso_code,iso_3166_2 from base_province where id&gt;=3"
                                ]
                            }
                        ],
                        "password": "000000",
                        "username": "root"
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "column": [
                            {
                                "name": "id",
                                "type": "bigint"
                            },
                            {
                                "name": "name",
                                "type": "string"
                            },
                            {
                                "name": "region_id",
                                "type": "string"
                            },
                            {
                                "name": "area_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_3166_2",
                                "type": "string"
                            }
                        ],
                        "compress": "gzip",
                        "defaultFS": "hdfs://hadoop102:8020",
                        "fieldDelimiter": "\t",
                        "fileName": "base_province",
                        "fileType": "text",
                        "path": "/base_province",
                        "writeMode": "append"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": 1
            }
        }
    }
}
</code></pre> 
<p>2、配置文件说明<br> （1）Reader参数说明<br> <img src="https://images2.imgbox.com/90/d6/qRxzB1Oo_o.png" alt="在这里插入图片描述"><br> 3、提交任务<br> （1）清空历史数据</p> 
<pre><code class="prism language-bash"> hadoop fs -rm -r -f /base_province/*
</code></pre> 
<p>（2）进入DataX根目录<br> （3）执行如下命令</p> 
<pre><code class="prism language-bash">python bin/datax.py job/base_province.json
</code></pre> 
<p>4、查看结果<br> （1）DataX打印日志<br> <img src="https://images2.imgbox.com/2e/92/RGb6zbDp_o.png" alt="在这里插入图片描述"><br> （2）查看HDFS文件</p> 
<pre><code class="prism language-bash">hadoop fs -cat /base_province/* <span class="token operator">|</span> zcat
</code></pre> 
<h6><a id="423_DataX_283"></a>4.2.3 DataX传参</h6> 
<p>通常情况下，离线数据同步任务需要每日定时重复执行，故HDFS上的目标路径通常会包含一层日期，以对每日同步的数据加以区分，也就是说每日同步数据的目标路径不是固定不变的，因此DataX配置文件中HDFS Writer的path参数的值应该是动态的。为实现这一效果，就需要使用DataX传参的功能。<br> DataX传参的用法如下，在JSON配置文件中使用${param}引用参数，在提交任务时使用-p"-Dparam=value"传入参数值，具体示例如下。<br> 1、编写配置文件<br> （1）修改配置文件base_province.json</p> 
<p>（2）配置文件内容如下</p> 
<pre><code>{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "connection": [
                            {
                                "jdbcUrl": [
                                    "jdbc:mysql://hadoop102:3306/gmall"
                                ],
                                "querySql": [
                                    "select id,name,region_id,area_code,iso_code,iso_3166_2 from base_province where id&gt;=3"
                                ]
                            }
                        ],
                        "password": "000000",
                        "username": "root"
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "column": [
                            {
                                "name": "id",
                                "type": "bigint"
                            },
                            {
                                "name": "name",
                                "type": "string"
                            },
                            {
                                "name": "region_id",
                                "type": "string"
                            },
                            {
                                "name": "area_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_3166_2",
                                "type": "string"
                            }
                        ],
                        "compress": "gzip",
                        "defaultFS": "hdfs://hadoop102:8020",
                        "fieldDelimiter": "\t",
                        "fileName": "base_province",
                        "fileType": "text",
                        "path": "/base_province/${dt}",
                        "writeMode": "append"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": 1
            }
        }
    }
}
</code></pre> 
<p>2、提交任务<br> （1）创建目标路径</p> 
<pre><code class="prism language-bash"> hadoop fs -mkdir /base_province/2020-06-14
</code></pre> 
<p>（2）进入DataX根目录<br> （3）执行如下命令</p> 
<pre><code class="prism language-bash"> python bin/datax.py -p<span class="token string">"-Ddt=2020-06-14"</span> job/base_province.json
</code></pre> 
<p>3、查看结果</p> 
<pre><code class="prism language-bash">hadoop fs -ls /base_province
</code></pre> 
<h5><a id="43_HDFSMySQL_379"></a>4.3 同步HDFS数据到MySQL案例</h5> 
<p>案例要求：同步HDFS上的/base_province目录下的数据到MySQL gmall 数据库下的test_province表。<br> 需求分析：要实现该功能，需选用HDFSReader和MySQLWriter。<br> 1、编写配置文件<br> （1）创建配置文件test_province.json<br> （2）配置文件内容如下</p> 
<pre><code>{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "hdfsreader",
                    "parameter": {
                        "defaultFS": "hdfs://hadoop102:8020",
                        "path": "/base_province",
                        "column": [
                            "*"
                        ],
                        "fileType": "text",
                        "compress": "gzip",
                        "encoding": "UTF-8",
                        "nullFormat": "\\N",
                        "fieldDelimiter": "\t",
                    }
                },
                "writer": {
                    "name": "mysqlwriter",
                    "parameter": {
                        "username": "root",
                        "password": "000000",
                        "connection": [
                            {
                                "table": [
                                    "test_province"
                                ],
                                "jdbcUrl": "jdbc:mysql://hadoop102:3306/gmall?useUnicode=true&amp;characterEncoding=utf-8"
                            }
                        ],
                        "column": [
                            "id",
                            "name",
                            "region_id",
                            "area_code",
                            "iso_code",
                            "iso_3166_2"
                        ],
                        "writeMode": "replace"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": 1
            }
        }
    }
}
</code></pre> 
<p>2、配置文件说明<br> （1）Reader参数说明<br> <img src="https://images2.imgbox.com/14/d8/qAlIlSaj_o.png" alt="在这里插入图片描述"><br> （2）Writer参数说明<br> <img src="https://images2.imgbox.com/56/18/SvitIH5l_o.png" alt="在这里插入图片描述"><br> 3、提交任务<br> （1）在MySQL中创建gmall.test_province表</p> 
<pre><code class="prism language-bash">DROP TABLE IF EXISTS <span class="token variable"><span class="token variable">`</span>test_province<span class="token variable">`</span></span><span class="token punctuation">;</span>
CREATE TABLE <span class="token variable"><span class="token variable">`</span>test_province<span class="token variable">`</span></span>  <span class="token punctuation">(</span>
  <span class="token variable"><span class="token variable">`</span><span class="token function">id</span><span class="token variable">`</span></span> bigint<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> NOT NULL,
  <span class="token variable"><span class="token variable">`</span>name<span class="token variable">`</span></span> varchar<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  <span class="token variable"><span class="token variable">`</span>region_id<span class="token variable">`</span></span> varchar<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  <span class="token variable"><span class="token variable">`</span>area_code<span class="token variable">`</span></span> varchar<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  <span class="token variable"><span class="token variable">`</span>iso_code<span class="token variable">`</span></span> varchar<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  <span class="token variable"><span class="token variable">`</span>iso_3166_2<span class="token variable">`</span></span> varchar<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  PRIMARY KEY <span class="token punctuation">(</span><span class="token variable"><span class="token variable">`</span><span class="token function">id</span><span class="token variable">`</span></span><span class="token punctuation">)</span>
<span class="token punctuation">)</span> ENGINE <span class="token operator">=</span> InnoDB CHARACTER SET <span class="token operator">=</span> utf8 COLLATE <span class="token operator">=</span> utf8_general_ci ROW_FORMAT <span class="token operator">=</span> Dynamic<span class="token punctuation">;</span>
</code></pre> 
<p>（2）进入DataX根目录<br> （3）执行如下命令</p> 
<pre><code class="prism language-bash"> python bin/datax.py job/test_province.json 
</code></pre> 
<p>4、查看结果<br> （1）DataX打印日志<br> （2）查看MySQL目标表数据<br> <img src="https://images2.imgbox.com/87/09/rKbegFtg_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="5DataX_470"></a>5、DataX优化</h3> 
<h5><a id="51__471"></a>5.1 速度控制</h5> 
<p>DataX3.0提供了包括通道(并发)、记录流、字节流三种流控模式，可以随意控制你的作业速度，让你的作业在数据库可以承受的范围内达到最佳的同步速度。<br> <img src="https://images2.imgbox.com/ca/2b/n572fVlh_o.png" alt="在这里插入图片描述"><br> 注意事项：<br> 1.若配置了总record限速，则必须配置单个channel的record限速<br> 2.若配置了总byte限速，则必须配置单个channe的byte限速<br> 3.若配置了总record限速和总byte限速，channel并发数参数就会失效。因为配置了总record限速和总byte限速之后，实际channel并发数是通过计算得到的：<br> 计算公式为:<br> min(总byte限速/单个channel的byte限速，总record限速/单个channel的record限速)</p> 
<h5><a id="52__480"></a>5.2 内存调整</h5> 
<p>当提升DataX Job内Channel并发数时，内存的占用会显著增加，因为DataX作为数据交换通道，在内存中会缓存较多的数据。例如Channel中会有一个Buffer，作为临时的数据交换的缓冲区，而在部分Reader和Writer的中，也会存在一些Buffer，为了防止OOM等错误，需调大JVM的堆内存。<br> 建议将内存设置为4G或者8G，这个也可以根据实际情况来调整。<br> 调整JVM xms xmx参数的两种方式：一种是直接更改datax.py脚本；另一种是在启动的时候，加上对应的参数，如下：<br> python datax/bin/datax.py --jvm=“-Xms8G -Xmx8G” /path/to/your/job.json</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/847a3144eb6e10f060c1957407a61794/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Google Colab免费GPU大揭晓：超详细使用攻略</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4252d9787064982903e85343bbc1f724/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">详细解决linux安装mysql后登录报错:Can‘t connect to local MySQL server through socket ‘/tmp/mysql.sock‘ (2)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>