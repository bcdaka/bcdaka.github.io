<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>新架构Mamba-2正式发布！！真实版“man, what can i say”！！ - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/1136d1a1780bc0c5df26a1c1b42e1a61/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="新架构Mamba-2正式发布！！真实版“man, what can i say”！！">
  <meta property="og:description" content="Mamba 面世以来，社区反应热烈，获得了广泛关注。
遗憾的是，Mamba 的论文竟然被 ICLR 拒稿，这让许多研究者颇感意外。
仅仅六个月之后，原作者团队带来更加改进和强大的 Mamba 2 正式发布啦！！
Mamba-2，其状态空间扩大了8倍，训练速度提升了50%！
——Transformer的新挑战者，最新架构Mamba刚刚推出了其第二代版本！
论文地址： Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality GitHub 地址： https://github.com/state-spaces/mamba 更令人惊讶的是，团队的研究发现Transformer和状态空间模型（SSM）实际上是亲近关系。
这两大主流的序列建模架构，终于在此实现了统一。
OpenAI发布GPT-4o一夜创历史，超越所有AIhttps://www.zhihu.com/pin/1773645611381747712
没体验过OpenAI最新版GPT-4o？快戳最详细升级教程，几分钟搞定：
手把手升级ChatGPT-4o Turbo详细步骤教程https://www.zhihu.com/pin/1768399982598909952
01 状态空间模型SSM 没错，这篇论文揭示了一个重大的发现：Transformer中的注意力机制与状态空间模型（SSM）之间存在着非常紧密的数学联系。
团队通过提出一个名为结构化状态空间二元性（Structured State Space Duality, SSD）的理论框架，将这两大模型家族统一了起来。
值得一提的是，Mamba一代论文在年初被ICLR拒稿，曾引发学术界广泛热议，许多学者因此纷纷表态。
此次推出的二代论文在理论和实验上都有了更丰富的内容，并成功入选ICML 2024。
论文的作者依然是Albert Gu和Tri Dao两位。
他们透露，论文题目中的“Transformers are SSMs”是向四年前那篇经典的线性注意力论文“Transformers are RNNs”致敬。
Transformer的核心组件是注意力机制，而SSM模型的核心则是一个线性时变系统。
乍看之下，两者似乎毫无关联，但论文指出：它们都可以表示成可半分离矩阵（Semiseparable Matrices）的变换。
让我们先从SSM的角度来看。
SSM本身定义了一个线性映射，恰好可对应于一个半可分离矩阵。
半可分离矩阵具有特殊的低秩结构，而这种结构恰好与SSM模型中的状态变量相对应。
因此，矩阵乘法就相当于SSM的线性时变系统。带选择性的SSM实际上是一种广义线性注意力机制。
从注意力的角度来看，又会有什么发现呢？
团队试图通过更抽象的方式来描述注意力机制的本质，毕竟“Softmax自注意力”只是其中的一种形式。
更一般地说，任何带有掩码的注意力机制，都可以表示为4个张量的缩并（Contraction）。
其中，QKV分别对应注意力中的query、key、value，L则对应掩码矩阵。 借助这一联系，团队在线性注意力的基础上提出了结构化掩码注意力（Structured Masked Attention，SMA）。
当注意力的掩码矩阵是半可分离的，就与SSM等价。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-04T20:56:49+08:00">
    <meta property="article:modified_time" content="2024-06-04T20:56:49+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">新架构Mamba-2正式发布！！真实版“man, what can i say”！！</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <blockquote> 
 <p>Mamba 面世以来，社区反应热烈，获得了广泛关注。<br><br> 遗憾的是，Mamba 的论文竟然被 ICLR 拒稿，这让许多研究者颇感意外。</p> 
</blockquote> 
<p></p> 
<p></p> 
<p></p> 
<p>仅仅六个月之后，原作者团队带来更加改进和强大的 Mamba 2 正式发布啦！！</p> 
<p><strong>Mamba-2，其状态空间扩大了8倍，训练速度提升了50%！</strong></p> 
<p>——Transformer的新挑战者，最新架构Mamba刚刚推出了其第二代版本！<img alt="" height="690" src="https://images2.imgbox.com/45/c8/3W3GQ0XZ_o.png" width="1200"></p> 
<p></p> 
<blockquote>
  论文地址： 
 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2405.21060" rel="nofollow" title="Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality">Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality</a> 
 <br> 
 <br> GitHub 地址： 
 <a href="https://link.zhihu.com/?target=https%3A//github.com/state-spaces/mamba" title="https://github.com/state-spaces/mamba">https://github.com/state-spaces/mamba</a> 
</blockquote> 
<p></p> 
<p style="text-align:center;">更令人惊讶的是，团队的研究发现Transformer和状态空间模型（SSM）实际上是亲近关系。<img alt="" src="https://images2.imgbox.com/67/20/QhmWrHoY_o.png"></p> 
<p></p> 
<p></p> 
<p>这两大主流的序列建模架构，终于在此实现了统一。</p> 
<p><a class="has-card" href="https://www.zhihu.com/pin/1773645611381747712" rel="nofollow" title="OpenAI发布GPT-4o一夜创历史，超越所有AI"><span class="link-card-box"><span class="link-title">OpenAI发布GPT-4o一夜创历史，超越所有AI</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/c3/c8/9pNj9dh4_o.png" alt="icon-default.png?t=N7T8">https://www.zhihu.com/pin/1773645611381747712</span></span></a></p> 
<p>没体验过OpenAI最新版GPT-4o？快戳最详细升级教程，几分钟搞定：</p> 
<p><a class="has-card" href="https://www.zhihu.com/pin/1768399982598909952" rel="nofollow" title="手把手升级ChatGPT-4o Turbo详细步骤教程"><span class="link-card-box"><span class="link-title">手把手升级ChatGPT-4o Turbo详细步骤教程</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/9b/48/MR9gMjGT_o.png" alt="icon-default.png?t=N7T8">https://www.zhihu.com/pin/1768399982598909952</span></span></a></p> 
<p></p> 
<h3 id="h_701642387_0">01 状态空间模型SSM</h3> 
<p>没错，这篇论文揭示了一个重大的发现：Transformer中的注意力机制与<strong>状态空间模型</strong>（SSM）之间存在着非常紧密的数学联系。</p> 
<p>团队通过提出一个名为<strong>结构化状态空间二元性</strong>（Structured State Space Duality, SSD）的理论框架，将这两大模型家族统一了起来。</p> 
<p><strong>值得一提的是，Mamba一代论文在年初被ICLR拒稿，曾引发学术界广泛热议，许多学者因此纷纷表态。</strong></p> 
<p>此次推出的二代论文在理论和实验上都有了更丰富的内容，<strong>并成功入选ICML 2024。</strong></p> 
<p>论文的作者依然是Albert Gu和Tri Dao两位。</p> 
<p><img alt="" height="528" src="https://images2.imgbox.com/62/cc/WdahLHtJ_o.png" width="1200"></p> 
<p></p> 
<p></p> 
<p>他们透露，论文题目中的“Transformers are SSMs”是向四年前那篇经典的线性注意力论文“Transformers are RNNs”致敬。</p> 
<p>Transformer的核心组件是注意力机制，而SSM模型的核心则是一个线性时变系统。</p> 
<p>乍看之下，两者似乎毫无关联，但论文指出：它们都可以表示成可半分离矩阵（Semiseparable Matrices）的变换。</p> 
<p>让我们先从SSM的角度来看。</p> 
<p><img alt="" height="684" src="https://images2.imgbox.com/2c/6e/Lb5iyohK_o.png" width="1200"></p> 
<p></p> 
<p>SSM本身定义了一个线性映射，恰好可对应于一个半可分离矩阵。</p> 
<p>半可分离矩阵具有特殊的低秩结构，而这种结构恰好与SSM模型中的状态变量相对应。</p> 
<p>因此，矩阵乘法就相当于SSM的线性时变系统。带选择性的SSM实际上是一种广义线性注意力机制。</p> 
<p>从注意力的角度来看，又会有什么发现呢？</p> 
<p>团队试图通过更抽象的方式来描述注意力机制的本质，毕竟“Softmax自注意力”只是其中的一种形式。</p> 
<p><strong>更一般地说，任何带有掩码的注意力机制，都可以表示为4个张量的缩并（Contraction）。</strong></p> 
<p>其中，QKV分别对应注意力中的query、key、value，L则对应掩码矩阵。 借助这一联系，团队在线性注意力的基础上提出了结构化掩码注意力（Structured Masked Attention，SMA）。</p> 
<p><strong>当注意力的掩码矩阵是半可分离的，就与SSM等价</strong>。</p> 
<p><img alt="" height="765" src="https://images2.imgbox.com/b0/b2/RUo2hvbt_o.png" width="1200"></p> 
<p></p> 
<p>基于这一发现，作者进一步推导出了两种等价的计算形式，这就是论文核心思想——<strong>“状态空间二元性</strong>”（Structured State Space Duality，SSD）的由来。</p> 
<p></p> 
<h3 id="h_701642387_1">02 状态空间二元性SSD</h3> 
<p>基于SSD思想的全新算法，Mamba-2 支持更大的状态维度，从16扩展到了256，从而能学习到更强的表示能力。</p> 
<p></p> 
<p>新方法采用了基于块分解的矩阵乘法，利用了GPU的存储层次结构，从而提升了训练速度。</p> 
<p></p> 
<p><img alt="" height="564" src="https://images2.imgbox.com/d8/23/CQt5cyS8_o.png" width="1200"></p> 
<p>在架构设计上，Mamba-2简化了块的设计，同时汲取了注意力机制的启发，做出了一些改动。</p> 
<p>借鉴多头注意力机制，创建了多输入SSM。</p> 
<p>通过与注意力机制之间的联系，<strong>SSD能够轻松将Transformer架构多年来积累的优化方法引入SSM。</strong><img alt="" height="543" src="https://images2.imgbox.com/71/ee/vDnKKGQy_o.png" width="1200"></p> 
<p></p> 
<p></p> 
<p>例如，引入张量并行和序列并行，以扩展到更大的模型和更长的序列。</p> 
<p>同时，还可以引入可变序列长度，以实现更快的微调和推理。</p> 
<p>Mamba-2的SSD层比Mamba-1中的关联扫描快了很多，这使团队能够增加状态维度并提高模型质量。<img alt="" height="428" src="https://images2.imgbox.com/cf/93/HKcLBCUT_o.png" width="1200"></p> 
<p></p> 
<p></p> 
<p>在实验中，<strong>Mamba-2在3B参数规模上训练了300B tokens，超越了相同规模的Mamba-1和Transformer。</strong></p> 
<p><img alt="" height="487" src="https://images2.imgbox.com/31/61/QTKCxE09_o.png" width="1200"></p> 
<p></p> 
<p></p> 
<p></p> 
<p><img alt="" height="417" src="https://images2.imgbox.com/dd/65/MFkA3raM_o.png" width="1200"></p> 
<p></p> 
<p>尤其在需要更大状态容量的任务上，如硬关联召回任务（MQAR），<strong>Mamba-2相较于Mamba-1有了显著的改进。</strong><img alt="" height="500" src="https://images2.imgbox.com/1b/79/dZp4QzMG_o.png" width="1200"></p> 
<p></p> 
<p></p> 
<h3 id="h_701642387_2">03 小结</h3> 
<p></p> 
<p><strong>man, what can i say!</strong></p> 
<p></p> 
<p></p> 
<p>推荐阅读：</p> 
<p><a href="https://blog.csdn.net/ElevenSakura/article/details/139245672" title="如何免费使用GPT-4o？如何升级GPT4.0 Turbo？（内附详细步骤教程）">如何免费使用GPT-4o？如何升级GPT4.0 Turbo？（内附详细步骤教程）<br></a><br><br><a href="https://blog.csdn.net/ElevenSakura/article/details/139452297?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22139452297%22%2C%22source%22%3A%22ElevenSakura%22%7D" title="最新版手把手升级GPT-4o、GPT-4 Turbo详细步骤教程！！ 【2024年6月】">最新版手把手升级GPT-4o、GPT-4 Turbo详细步骤教程！！ 【2024年6月】</a><br>  </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7bdc7a90f8328e6f6c2ff9961657530a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Mac M1安装编译FFmpeg教程</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/11361a2b3dfeeb81e97d23e3441b5cb2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【香橙派 AIpro 开发板】AI 应用部署测评：视频目标检测&#43;Linux镜像&#43;vscode远程连接&#43;全细节试用</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>