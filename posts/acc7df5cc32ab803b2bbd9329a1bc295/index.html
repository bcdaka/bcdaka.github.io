<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>论文研读｜针对文生图模型的AIGC检测 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/acc7df5cc32ab803b2bbd9329a1bc295/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="论文研读｜针对文生图模型的AIGC检测">
  <meta property="og:description" content="前言：人工智能生成内容的鉴别（AIGC检测）算是当前的研究热点之一，本篇文章介绍几篇针对文生图模型的 AIGC 检测相关工作。
相关文章：AIGC溯源相关研究详见此篇文章
目录 1. Towards Universal Fake Image Detectors that Generalize Across Generative Models（CVPR 2023）2. DIRE for Diffusion-Generated Image Detection（ICCV 2023）3. AEROBLADE: Training-Free Detection of Latent Diffusion Images Using Autoencoder Reconstruction Error（CVPR, 2024）4. Seeing is not always believing: Benchmarking Human and Model Perception of AI-Generated Images（NeurIPS, 2023） 1. Towards Universal Fake Image Detectors that Generalize Across Generative Models（CVPR 2023） 作者：Utkarsh Ojha, et al. University of Wisconsin-Madison, USA">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-18T15:34:42+08:00">
    <meta property="article:modified_time" content="2024-06-18T15:34:42+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">论文研读｜针对文生图模型的AIGC检测</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p><strong>前言</strong>：人工智能生成内容的鉴别（AIGC检测）算是当前的研究热点之一，本篇文章介绍几篇针对文生图模型的 AIGC 检测相关工作。</p> 
</blockquote> 
<p>相关文章：AIGC溯源相关研究详见<a href="https://blog.csdn.net/qq_36332660/article/details/137335043">此篇文章</a></p> 
<hr> 
<p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#1_Towards_Universal_Fake_Image_Detectors_that_Generalize_Across_Generative_ModelsCVPR_2023_9" rel="nofollow">1. Towards Universal Fake Image Detectors that Generalize Across Generative Models（CVPR 2023）</a></li><li><a href="#2_DIRE_for_DiffusionGenerated_Image_DetectionICCV_2023_21" rel="nofollow">2. DIRE for Diffusion-Generated Image Detection（ICCV 2023）</a></li><li><a href="#3_AEROBLADE_TrainingFree_Detection_of_Latent_Diffusion_Images_Using_Autoencoder_Reconstruction_ErrorCVPR_2024_38" rel="nofollow">3. AEROBLADE: Training-Free Detection of Latent Diffusion Images Using Autoencoder Reconstruction Error（CVPR, 2024）</a></li><li><a href="#4_Seeing_is_not_always_believing_Benchmarking_Human_and_Model_Perception_of_AIGenerated_ImagesNeurIPS_2023_55" rel="nofollow">4. Seeing is not always believing: Benchmarking Human and Model Perception of AI-Generated Images（NeurIPS, 2023）</a></li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="1_Towards_Universal_Fake_Image_Detectors_that_Generalize_Across_Generative_ModelsCVPR_2023_9"></a>1. Towards Universal Fake Image Detectors that Generalize Across Generative Models（CVPR 2023）</h2> 
<p>作者：Utkarsh Ojha, et al. University of Wisconsin-Madison, USA<br> 代码链接：<a href="https://github.com/Yuheng-Li/UniversalFakeDetect">https://github.com/Yuheng-Li/UniversalFakeDetect</a><br> 核心思想：作者首先发现基于GAN生成数据训练的分类器只能记住 GAN 生成的 fake image，其他的全部归类为 real image，包括扩散模型生成的图像。这就导致决策边界发生偏离，如下图所示。因此，之前基于GAN数据训练分类器的检测方法无法有效检测扩散模型生成的图像。<br> <img src="https://images2.imgbox.com/ed/d8/7cMIUSpM_o.png" alt="在这里插入图片描述" height="250"><br> 基于上述发现，作者提出基于预训练模型特征提取的方法来进行real/fake的检测，首先通过预训练模型提取出待检测图像特征，然后 1）<strong>基于相似度衡量（K近邻）的检测</strong>：通过比较待检测图像特征分别与真实图像特征和虚假图像特征的距离，检测图像的真实性。2）<strong>基于分类器的检测</strong>：将提取出的特征送入轻量级分类器训练，得到预测标签。</p> 
<p>为了保证图像特征提取的质量，选择 CLIP-ViT，是由于其训练数据足够庞大（400M），因此表征空间足够大。（实验证明该提取器的特征提取效果最好）<br> <img src="https://images2.imgbox.com/98/6d/QYvP9sVM_o.png" alt="在这里插入图片描述" height="175"><br> 个人评价：做得快，正值风口，瞄准检测方法的通用性和泛化性，方法其实很简单。</p> 
<hr> 
<h2><a id="2_DIRE_for_DiffusionGenerated_Image_DetectionICCV_2023_21"></a>2. DIRE for Diffusion-Generated Image Detection（ICCV 2023）</h2> 
<p>作者：Zhendong Wang, et al. 中科大。<br> 代码链接：<a href="https://github.com/ZhendongWang6/DIRE">https://github.com/ZhendongWang6/DIRE</a><br> 核心思想：作者发现<strong>生成图像重建前后的距离</strong>比<strong>真实图像重建前后的距离</strong>要小。<br> <img src="https://images2.imgbox.com/66/56/qjRxivRG_o.png" alt="在这里插入图片描述" height="175"><br> 基于上述观察，作者提出基于DDIM 重建误差的检测方法。</p> 
<p><img src="https://images2.imgbox.com/da/6b/ehYV3ZCC_o.png" alt="在这里插入图片描述" height="30"><br> <img src="https://images2.imgbox.com/5a/97/Mfmts0ga_o.png" alt="在这里插入图片描述" height="175"></p> 
<p>注意：通过DIRE得到重建结果之后，把DIRE作为输入，训练一个简单的二分类器，通过交叉熵损失约束。</p> 
<p><img src="https://images2.imgbox.com/47/86/1FTdBLrG_o.png" alt="在这里插入图片描述" height="55"></p> 
<p>个人评价：基于重建损失的检测手段，在GAN时期已经出现，这篇文章把它运用到扩散模型上取得了同样的效果。另一篇工作<a href="https://arxiv.org/abs/2403.17465" rel="nofollow">LaRE^2 (CVPR 2024)</a>是对DIRE的改进。</p> 
<hr> 
<h2><a id="3_AEROBLADE_TrainingFree_Detection_of_Latent_Diffusion_Images_Using_Autoencoder_Reconstruction_ErrorCVPR_2024_38"></a>3. AEROBLADE: Training-Free Detection of Latent Diffusion Images Using Autoencoder Reconstruction Error（CVPR, 2024）</h2> 
<p>作者：Jonas Ricker, et al. Ruhr University Bochum, Germany<br> 代码链接：<a href="https://github.com/jonasricker/aeroblade">https://github.com/jonasricker/aeroblade</a><br> 核心思想：与上一个工作相似，不过这篇文章针对 Latent Diffusion Model，使用 AutoEncoder 重建损失进行鉴别。先来直观感受下真实图像和SD生成图像分别在使用AutoEncoder重建前后的误差：<br> <img src="https://images2.imgbox.com/ba/03/iozaeooD_o.png" alt="在这里插入图片描述" height="135"><br> 重建损失就是重建前后的距离，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          Δ 
         
         
         
           A 
          
          
          
            E 
           
          
            i 
           
          
         
        
       
      
        \Delta_{AE_{i}} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9334em; vertical-align: -0.2501em;"></span><span class="mord"><span class="mord">Δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0576em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3281em;"><span class="" style="top: -2.357em; margin-left: -0.0576em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2501em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 表示使用第 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         i 
        
       
      
        i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span></span> 个 AutoEncoder 重建前后的图像距离。（因为这是真伪检测任务，所以要使用尽可能多的 AutoEncoder 覆盖尽可能多的生成模型）。</p> 
<p><img src="https://images2.imgbox.com/86/69/Alrz60wI_o.png" alt="在这里插入图片描述" height="45"><br> 然后在众多的 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          Δ 
         
         
         
           A 
          
          
          
            E 
           
          
            i 
           
          
         
        
       
      
        \Delta_{AE_{i}} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9334em; vertical-align: -0.2501em;"></span><span class="mord"><span class="mord">Δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0576em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3281em;"><span class="" style="top: -2.357em; margin-left: -0.0576em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2501em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 中，挑选出一个最小的距离，作为判别根据。<br> <img src="https://images2.imgbox.com/f8/21/3xCwDk8B_o.png" alt="在这里插入图片描述" height="80"><br> 这个方法中，距离函数的选择十分重要。这篇文章使用 LPIPS 作为距离的衡量标准。</p> 
<p><img src="https://images2.imgbox.com/b0/5b/gzhUEduV_o.png" alt="在这里插入图片描述" height="150"></p> 
<p>个人评价：这篇文章和DIRE不同点就在于，把DIRE的后续分类器步骤拿掉，变成 training-free 了，这样一来就提高了检测方法的可扩展性，不需要每次有新的模型进来都重训一遍。注意这篇文章的检测模型仅限于在图像生成阶段使用 AutoEncoder 的生成模型，判定阈值的选取也十分重要。</p> 
<hr> 
<h2><a id="4_Seeing_is_not_always_believing_Benchmarking_Human_and_Model_Perception_of_AIGenerated_ImagesNeurIPS_2023_55"></a>4. Seeing is not always believing: Benchmarking Human and Model Perception of AI-Generated Images（NeurIPS, 2023）</h2> 
<p>作者：Zeyu Lu, et al. 上海交大。<br> 代码链接：<a href="https://github.com/Inf-imagine/Sentry">https://github.com/Inf-imagine/Sentry</a><br> 核心思想：眼见不一定为实，这篇文章的主要贡献是提出了一个AIGC图像检测的 Benchmark 数据集 Fake2M，并在文中分别对人工检测和模型检测两类AIGC检测方法进行了评估。评估结果表明 AIGC 检测仍然任重道远啊……</p> 
<p><img src="https://images2.imgbox.com/12/cc/z3jFWcG3_o.png" alt="在这里插入图片描述" height="250"></p> 
<blockquote> 
 <p><strong>后记</strong>：本文介绍的这几种针对文生图模型生成图像的检测方法，或是借助强大的预训练特征提取器进行二分类，或是从生成模型的内在机制出发，将重建损失作为判别标准进行检测。本质上来讲，这些方法都利用了距离信息作为衡量依据，不论是第一篇工作中的K近邻，还是待检测图像本身的重建误差，都在向我们传达一个信息，那就是，纵然生成模型生成的内容再逼真，也总会留下一些可以认定其为虚假的痕迹，等着我们去探索，去发现。</p> 
</blockquote> 
<hr> 
<p><strong>参考文献</strong></p> 
<ol><li>Towards Universal Fake Image Detectors that Generalize Across Generative Models. CVPR 2023.</li><li>DIRE for Diffusion-Generated Image Detection. ICCV 2023.</li><li>LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection. CVPR, 2024.</li><li>AEROBLADE: Training-Free Detection of Latent Diffusion Images Using Autoencoder Reconstruction Error. CVPR, 2024.</li><li>Seeing is not always believing: Benchmarking Human and Model Perception of AI-Generated Images. NeurIPS, 2023.</li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6ea09684e7b916c1d067ce34ee981b93/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">ChatGLM3-6B使用lora微调实体抽取，工具LLaMA-Factory，医学数据集CMeEE</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/47618827e61e99487448b0e7262e4af4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">如何将文件从Android传输到 iPhone [7 种最佳方式]</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>