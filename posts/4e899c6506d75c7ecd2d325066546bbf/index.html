<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ollama &#43; fastgpt搭建本地私有AI大模型智能体工作流（AI Agent Flow）-- windows环境 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/4e899c6506d75c7ecd2d325066546bbf/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="ollama &#43; fastgpt搭建本地私有AI大模型智能体工作流（AI Agent Flow）-- windows环境">
  <meta property="og:description" content="首先我们来探究下为什么要设计大模型&#43;工作流 一、大模型&#43;工作流的优势主要有四点： 1. 降低任务门槛：工作流可以将复杂任务分解成多个小任务，降低每个任务的复杂度，从而减少对提示词和大模型推理能力的依赖。这样可以提升大模型处理复杂任务的性能和容错能力。
2. 提升任务效率：工作流可以实现自动化处理，减少重复劳动和纠正。只需提供必要元素，工作流就可以直接输出结果，提高效率和稳定性。
3. 提高任务的一致性：工作流可以确保任务的执行是一致的，减少人为错误和偏差，提高任务的可靠性和质量。
4. 实现任务的可重复性：工作流可以实现任务的可重复性，减少重复劳动和开发时间，提高开发效率和生产力。
好了接下来我们开始正式实践。
二、安装docker desktop &#43; wsl2 dockerdesktop下载地址：Docker Desktop: The #1 Containerization Tool for Developers | Docker
安装步骤就省略咯，可以百度。
推荐博客：0基础基于最新WSL2的Window Docker安装及搭建内网穿透保姆级教程_wsl 镜像网络-CSDN博客
三、安装ollama 1、安装ollama 下载ollama官方windows安装程序，下载后直接双击应用程序安装。
地址：Ollama
安装完成后进行更换ollama的存储位置
（设置环境变量--不明白就百度一下windows设置环境变量，这里就不过多讲解。）：
设置完成后重启电脑。
2、下载模型 ollama官网顶部Models，进入模型列表界面。
复制完成后进入cmd，并开始下载模型，将复制的模型 粘贴并回车。
上图为拉取进度，完成后会自动进行编译，然后本地cmd方式即可进行对话。
经过以上步骤本地大模型llama3以安装完成，接下来我们通过dockerdesktop进行搭建fastgpt。
四、安装部署Fastgpt 1、部署fastgpt到dockerdesktop Fastgpt开源项目地址：https://github.com/labring/FastGPT
Fastgpt官方docker模式部署地址：Docker Compose 快速部署 | FastGPT
方法如下：
启动 dockerdesktop。
依次执行下面命令，创建 FastGPT 文件并复制docker-compose.yml和config.json的内容并创建两个一模一样的文件，执行完后目录下会有 2 个文件。
文件创建好并拷贝出git上对应的两个文件内容，并cmd进入当前文件夹执行代码：docker-compose up -d
然后等待安装完成（由于我电脑已经完成安装，就不演示了）。完成后dockerdesktop会显示如下信息：
文件夹内容：
dockerdesktop内容：
2、启动并配置one api模型： 访问本地部署好的one api：http://localhost:3001/channel
初始化的 用户名:root 密码：123456">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-18T21:23:37+08:00">
    <meta property="article:modified_time" content="2024-05-18T21:23:37+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ollama &#43; fastgpt搭建本地私有AI大模型智能体工作流（AI Agent Flow）-- windows环境</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>首先我们来探究下为什么要设计大模型+工作流</h2> 
<h3>一、大模型+工作流的优势主要有四点：</h3> 
<p><strong>1. 降低任务门槛</strong>：工作流可以将复杂任务分解成多个小任务，降低每个任务的复杂度，从而减少对提示词和大模型推理能力的依赖。这样可以提升大模型处理复杂任务的性能和容错能力。</p> 
<p><strong>2. 提升任务效率</strong>：工作流可以实现自动化处理，减少重复劳动和纠正。只需提供必要元素，工作流就可以直接输出结果，提高效率和稳定性。</p> 
<p><strong>3. 提高任务的一致性</strong>：工作流可以确保任务的执行是一致的，减少人为错误和偏差，提高任务的可靠性和质量。</p> 
<p><strong>4. 实现任务的可重复性</strong>：工作流可以实现任务的可重复性，减少重复劳动和开发时间，提高开发效率和生产力。</p> 
<p>好了接下来我们开始正式实践。</p> 
<h3>二、安装docker desktop + wsl2</h3> 
<p>dockerdesktop下载地址：<a href="https://www.docker.com/products/docker-desktop/" rel="nofollow" title="Docker Desktop: The #1 Containerization Tool for Developers | Docker">Docker Desktop: The #1 Containerization Tool for Developers | Docker</a></p> 
<p>安装步骤就省略咯，可以百度。</p> 
<p>推荐博客：<a href="https://blog.csdn.net/weixin_70050636/article/details/134599891" title="0基础基于最新WSL2的Window Docker安装及搭建内网穿透保姆级教程_wsl 镜像网络-CSDN博客">0基础基于最新WSL2的Window Docker安装及搭建内网穿透保姆级教程_wsl 镜像网络-CSDN博客</a></p> 
<h3>三、安装ollama</h3> 
<h4>1、安装ollama</h4> 
<p>下载ollama官方windows安装程序，下载后直接双击应用程序安装。</p> 
<p>地址：<a href="https://ollama.com/" rel="nofollow" title="Ollama">Ollama</a></p> 
<p><img alt="" height="493" src="https://images2.imgbox.com/a8/f4/G8hwBQWz_o.png" width="676"></p> 
<p><img alt="" height="611" src="https://images2.imgbox.com/72/07/sluXETwS_o.png" width="682"></p> 
<p>安装完成后进行更换ollama的存储位置</p> 
<p>（设置环境变量--不明白就百度一下windows设置环境变量，这里就不过多讲解。）：</p> 
<p><img alt="" height="922" src="https://images2.imgbox.com/45/ca/HP9xm4uU_o.png" width="1200"></p> 
<p>设置完成后重启电脑。</p> 
<h4>2、下载模型</h4> 
<p>ollama官网顶部Models，进入模型列表界面。</p> 
<p><img alt="" height="631" src="https://images2.imgbox.com/f5/dc/IrDK0bYN_o.png" width="1111"></p> 
<p><img alt="" height="798" src="https://images2.imgbox.com/2f/45/ow4Zfgty_o.png" width="1024"></p> 
<p><img alt="" height="735" src="https://images2.imgbox.com/66/fa/XEe39u5i_o.png" width="863"></p> 
<p>复制完成后进入cmd，并开始下载模型，将复制的模型 粘贴并回车。</p> 
<p><img alt="" height="392" src="https://images2.imgbox.com/10/af/SLBQDduW_o.png" width="987"></p> 
<p><img alt="" height="345" src="https://images2.imgbox.com/41/ed/hvpfX8Fi_o.png" width="841"></p> 
<p><img alt="" height="194" src="https://images2.imgbox.com/69/ac/syObPYOF_o.png" width="975"></p> 
<p><img alt="" height="270" src="https://images2.imgbox.com/ef/17/k8iYjcKv_o.png" width="804"></p> 
<p><img alt="" height="528" src="https://images2.imgbox.com/d9/a3/elda9T0x_o.png" width="1002"></p> 
<p><img alt="" height="403" src="https://images2.imgbox.com/72/27/NGl6lhx7_o.png" width="803"></p> 
<p>上图为拉取进度，完成后会自动进行编译，然后本地cmd方式即可进行对话。</p> 
<p>经过以上步骤本地大模型llama3以安装完成，接下来我们通过dockerdesktop进行搭建fastgpt。</p> 
<h3>四、安装部署Fastgpt</h3> 
<h4>1、部署fastgpt到dockerdesktop</h4> 
<p>Fastgpt开源项目地址：<a href="https://github.com/labring/FastGPT" title="https://github.com/labring/FastGPT">https://github.com/labring/FastGPT</a></p> 
<p>Fastgpt官方docker模式部署地址：<a href="https://doc.fastai.site/docs/development/docker/" rel="nofollow" title="Docker Compose 快速部署 | FastGPT">Docker Compose 快速部署 | FastGPT</a></p> 
<p>方法如下：</p> 
<p>启动 dockerdesktop。</p> 
<p>依次执行下面命令，创建 FastGPT 文件并复制<code>docker-compose.yml</code>和<code>config.json的内容并创建两个一模一样的文件</code>，执行完后目录下会有 2 个文件。</p> 
<p>文件创建好并拷贝出git上对应的两个文件内容，并cmd进入当前文件夹执行代码：docker-compose up -d</p> 
<p><img alt="" height="683" src="https://images2.imgbox.com/db/4e/Y2KUr1C4_o.png" width="992"></p> 
<p><img alt="" height="302" src="https://images2.imgbox.com/89/10/EF28cyCI_o.png" width="983"></p> 
<p><img alt="" height="557" src="https://images2.imgbox.com/fd/c9/CKXn84nE_o.png" width="788"></p> 
<p><img alt="" height="363" src="https://images2.imgbox.com/da/77/qrQqQuAM_o.png" width="727"></p> 
<p>然后等待安装完成（由于我电脑已经完成安装，就不演示了）。完成后dockerdesktop会显示如下信息：</p> 
<p>文件夹内容：</p> 
<p><img alt="" height="417" src="https://images2.imgbox.com/e6/f4/Utzk9rFh_o.png" width="864"></p> 
<p>dockerdesktop内容：</p> 
<p><img alt="" height="758" src="https://images2.imgbox.com/2b/a7/ckTDIA7s_o.png" width="1200"></p> 
<h4 style="background-color:transparent;">2、启动并配置one api模型：</h4> 
<p>访问本地部署好的one api：<a href="http://localhost:3001/channel" rel="nofollow" title="http://localhost:3001/channel">http://localhost:3001/channel</a></p> 
<p>初始化的    用户名:root   密码：123456</p> 
<p><img alt="" height="599" src="https://images2.imgbox.com/cc/44/Xbz31e2P_o.png" width="1042"></p> 
<p>进入后会叫修改密码之类的，完成后点击“渠道”，配置你的大语言模型。首次登录进来渠道是空的，需要自己添加自己的大模型。</p> 
<p><img alt="" height="616" src="https://images2.imgbox.com/56/53/NCNWwef2_o.png" width="1200"></p> 
<p>那接下来我们怎么配置刚刚安装的本地大模型llama3呢？</p> 
<p>第一步：先点击 底部“添加新的渠道”，然后选择“Ollama”。</p> 
<p><img alt="" height="947" src="https://images2.imgbox.com/a0/a5/oEaGolbB_o.png" width="1200"></p> 
<p>第二步：配置本地llama3:8b大模型</p> 
<p><img alt="" height="851" src="https://images2.imgbox.com/a2/3d/Xtf8GNJ6_o.png" width="977"></p> 
<p>查询本地ollama中的模型名称，填入模型的时候用到</p> 
<p><img alt="" height="226" src="https://images2.imgbox.com/1c/9c/oBD6Gehr_o.png" width="559"></p> 
<p>完整的配置界面</p> 
<p><img alt="" height="850" src="https://images2.imgbox.com/02/c0/4kS09AF2_o.png" width="862"></p> 
<p>完成后测试通过，下图中有时间返回，就说明模型链接成功。</p> 
<p><img alt="" height="650" src="https://images2.imgbox.com/53/04/yMuYah94_o.png" width="1200"></p> 
<p>oneapi还有最后一步，就是将模型加入到咱们的令牌中，点击顶部“令牌”导航进入。选默认的令牌信息，并添加llama3:laest 模型。</p> 
<p><img alt="" height="726" src="https://images2.imgbox.com/af/4e/4Lchh8ww_o.png" width="1200"></p> 
<h3>五、配置Fastgpt</h3> 
<p>接下来配置Fastgpt并使用咱们导入的本地大模型llama3来进行创建chat 或者 agent了。</p> 
<h4>1、配置fastgpt的模型</h4> 
<p>进入最开始的fastgpt文件夹。找到config.json,配置llama3模型，位置和代码如下：</p> 
<p>完成配置后需要重新启动dockerdesktop里面的oneapi和fastgpt。</p> 
<p><img alt="" height="818" src="https://images2.imgbox.com/20/7e/wvaIN8vA_o.png" width="613"></p> 
<h4><img alt="" height="568" src="https://images2.imgbox.com/ab/31/PpbMVFfb_o.png" width="1020"></h4> 
<h4>2、访问fastgpt</h4> 
<p>地址：<a href="http://localhost:3000/login" rel="nofollow" title="http://localhost:3000/login">http://localhost:3000/login</a>，默认用户名：root  密码：1234</p> 
<p><img alt="" height="849" src="https://images2.imgbox.com/da/e5/VjDJujRl_o.png" width="1134"></p> 
<p>2、配置应用chat、agent等：</p> 
<p><img alt="" height="615" src="https://images2.imgbox.com/f7/a4/xSePZpuB_o.png" width="1155"></p> 
<p>输入chat名字，选择类型。</p> 
<p><img alt="" height="601" src="https://images2.imgbox.com/89/c5/p7Lg0Fjf_o.png" width="991"></p> 
<p>选择一个大模型，配置到你创建的应用中。</p> 
<p><img alt="" height="1012" src="https://images2.imgbox.com/39/97/bYiISptX_o.png" width="1200"></p> 
<p>完成模型选择后进行 发布，发布后即可在右变边的聊天框中输入信息，进行对话。</p> 
<p><img alt="" height="1042" src="https://images2.imgbox.com/77/d9/Rh7QQo3t_o.png" width="1200"></p> 
<p>第一个聊天应用就搞定了。后续还有知识库上传和导入，导入完成后即可配置本地RAG检索agent配置了。</p> 
<p>下期再写吧，感谢各位耐心观看。</p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c0059054d831dcec72ec5a33fae9627d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">继承详解——C&#43;&#43;深度学习解析</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/dd8a608608839324b169f8a8a8850348/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">数据结构堆排序（c语言版）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>