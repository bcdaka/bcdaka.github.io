<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>使用Amazon SageMaker构建高质量AI作画模型Stable Diffusion_sagemaker ai绘图 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/15e4e8df1bc99abef84c0ab463baaad2/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="使用Amazon SageMaker构建高质量AI作画模型Stable Diffusion_sagemaker ai绘图">
  <meta property="og:description" content="目前人工智能模型可以分为两大类别，包括判别模型 (Discriminative Model) 与生成模型 (Generative Model)。判别模型根据一组输入数据，例如文本、X 射线图像或者游戏画面，经过一系列计算得到相应目标输出结果，例如单词翻译结果、X 光图像的诊断结果或游戏中下一时刻要执行的动作。判别模型可能是我们最熟悉的一类 AI 模型，其目的是在一组输入变量和目标输出之间创建映射。
而生成模型，并不会不会对输入变量计算分数或标签，而是通过学习输入和输出之间的关系生成新的数据样本，这类模型可以接受与实际值无关的向量(甚至是随机向量)，生成复杂输出，例如文本、音乐或图像。人工智能生成( Artificial Intelligence Generated Content, AIGC) 内容泛指指利用机器学习和自然语言处理技术，让计算机生成人类可理解的文本、音频、图像等内容，主要由深度学习算法和神经网络构成，可以通过学习大量的数据来模拟人类的思维和创造力，从而产生高质量的内容。下图是使用 stable diffusion 模型生成的图像，可以看出生成的图像不仅具有很高的质量，同时能够很好的契合给定的输入描述。
AIGC 通过机器学习方法从原始数据中学习数据特征，进而生成全新的、原创的数据，这些数据与训练数据保持相似，而非简单复制原始数据。AIGC 已经取得了重大进展，并在各个领域得到广泛应用：
内容创作：可以辅助创作者完成图画、文章、小说、音乐等内容的创作设计：可以帮助设计师进行平面设计、UI 设计等游戏：可以生成游戏中的角色、道具等元素视频制作：可以生成特效、动画等内容智能客服：可以生成自然语言对话，实现智能客服等应用 AIGC 可以视为未来的战略技术，其将极大加速人工智能生成数据的速度，正在深刻改变人类社会，推动人类创作活动，包括写作、绘画、编程等，甚至也将推动科学研究，例如生成科学假设和科学现象等。AIGC 是一个快速发展的领域，将为各个行业带来革命性的变化。未来，通过学术界和工业界持续探索新的算法和技术，将进一步提高生成内容的质量和多样性。
总的来说，判别模型关注的是输入和输出之间的关系，直接预测输出结果，而生成模型则关注数据的分布，通过学习数据的统计特征来生成新的样本数据。判别模型推动了人工智能前数十年的发展，而生成模型将成为人工智能未来十年的重点发展方向。
2.2 Stable Diffusion 介绍 最近 AI 作画取得如此巨大进展的原因很大程度上可以归功于开源模型 Stable Diffusion，Stable diffusion 是一个基于潜在扩散模型 (Latent Diffusion Models, LDM) 的文图生成 (text-to-image) 模型，经过训练可以逐步对随机高斯噪声进行去噪以获得感兴趣的数据样本，该模型使用来自 LAION-5B 数据库 (LAION-5B 是目前最大、可自由访问的多模态数据集)子集的 512x512 图像进行训练，使用这个模型，可以生成包括人脸在内的任何图像。在使用 Stable Diffusion 生成高质量图像之前，我们首先介绍该模型的原理与架构，Stable Diffusion 模型架构如下图所示：
Diffusion model 相比生成对抗网络 (Generative Adversarial Network, GAN) 具有更好的图片生成效果，但由于该模型是一种自回归模型，需要反复迭代计算，因此训练和推理代价都很高，主要原因是它们在像素空间中运行，特别是在生成高分辨率图像时，需要消耗大量内存。Latent diffusion 通过在较低维度的潜空间上应用扩散过程而不是使用实际的像素空间来减少内存和计算成本，所以 Stable Diffusion 引入了 Latent diffusion 的方式来解决计算代价昂贵的问题，能够极大地减少计算复杂度，同时可以生成质量较高的图像，Latent Diffusion 的主要包括以下三个组成部分：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-30T16:33:42+08:00">
    <meta property="article:modified_time" content="2024-04-30T16:33:42+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">使用Amazon SageMaker构建高质量AI作画模型Stable Diffusion_sagemaker ai绘图</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>目前人工智能模型可以分为两大类别，包括判别模型 (<code>Discriminative Model</code>) 与生成模型 (<code>Generative Model</code>)。判别模型根据一组输入数据，例如文本、X 射线图像或者游戏画面，经过一系列计算得到相应目标输出结果，例如单词翻译结果、X 光图像的诊断结果或游戏中下一时刻要执行的动作。判别模型可能是我们最熟悉的一类 AI 模型，其目的是在一组输入变量和目标输出之间创建映射。<br> 而生成模型，并不会不会对输入变量计算分数或标签，而是通过学习输入和输出之间的关系生成新的数据样本，这类模型可以接受与实际值无关的向量(甚至是随机向量)，生成复杂输出，例如文本、音乐或图像。人工智能生成( <code>Artificial Intelligence Generated Content</code>, <code>AIGC</code>) 内容泛指指利用机器学习和自然语言处理技术，让计算机生成人类可理解的文本、音频、图像等内容，主要由深度学习算法和神经网络构成，可以通过学习大量的数据来模拟人类的思维和创造力，从而产生高质量的内容。下图是使用 <code>stable diffusion</code> 模型生成的图像，可以看出生成的图像不仅具有很高的质量，同时能够很好的契合给定的输入描述。</p> 
<p><img src="https://images2.imgbox.com/56/b1/gUO78th2_o.png" alt="AIGC"><br> <code>AIGC</code> 通过机器学习方法从原始数据中学习数据特征，进而生成全新的、原创的数据，这些数据与训练数据保持相似，而非简单复制原始数据。<code>AIGC</code> 已经取得了重大进展，并在各个领域得到广泛应用：</p> 
<ul><li>内容创作：可以辅助创作者完成图画、文章、小说、音乐等内容的创作</li><li>设计：可以帮助设计师进行平面设计、<code>UI</code> 设计等</li><li>游戏：可以生成游戏中的角色、道具等元素</li><li>视频制作：可以生成特效、动画等内容</li><li>智能客服：可以生成自然语言对话，实现智能客服等应用</li></ul> 
<p><code>AIGC</code> 可以视为未来的战略技术，其将极大加速人工智能生成数据的速度，正在深刻改变人类社会，推动人类创作活动，包括写作、绘画、编程等，甚至也将推动科学研究，例如生成科学假设和科学现象等。<code>AIGC</code> 是一个快速发展的领域，将为各个行业带来革命性的变化。未来，通过学术界和工业界持续探索新的算法和技术，将进一步提高生成内容的质量和多样性。<br> 总的来说，判别模型关注的是输入和输出之间的关系，直接预测输出结果，而生成模型则关注数据的分布，通过学习数据的统计特征来生成新的样本数据。判别模型推动了人工智能前数十年的发展，而生成模型将成为人工智能未来十年的重点发展方向。</p> 
<h5><a id="22_Stable_Diffusion__19"></a>2.2 Stable Diffusion 介绍</h5> 
<p>最近 <code>AI</code> 作画取得如此巨大进展的原因很大程度上可以归功于开源模型 <code>Stable Diffusion</code>，<code>Stable diffusion</code> 是一个基于潜在扩散模型 (<code>Latent Diffusion Models</code>, <code>LDM</code>) 的文图生成 (<code>text-to-image</code>) 模型，经过训练可以逐步对随机高斯噪声进行去噪以获得感兴趣的数据样本，该模型使用来自 <code>LAION-5B</code> 数据库 (<code>LAION-5B</code> 是目前最大、可自由访问的多模态数据集)子集的 <code>512x512</code> 图像进行训练，使用这个模型，可以生成包括人脸在内的任何图像。在使用 <code>Stable Diffusion</code> 生成高质量图像之前，我们首先介绍该模型的原理与架构，<code>Stable Diffusion</code> 模型架构如下图所示：</p> 
<p><img src="https://images2.imgbox.com/90/91/IbaMeoBB_o.png" alt="模型架构"><br> <code>Diffusion model</code> 相比生成对抗网络 (<code>Generative Adversarial Network</code>, <code>GAN</code>) 具有更好的图片生成效果，但由于该模型是一种自回归模型，需要反复迭代计算，因此训练和推理代价都很高，主要原因是它们在像素空间中运行，特别是在生成高分辨率图像时，需要消耗大量内存。<code>Latent diffusion</code> 通过在较低维度的潜空间上应用扩散过程而不是使用实际的像素空间来减少内存和计算成本，所以 <code>Stable Diffusion</code> 引入了 <code>Latent diffusion</code> 的方式来解决计算代价昂贵的问题，能够极大地减少计算复杂度，同时可以生成质量较高的图像，<code>Latent Diffusion</code> 的主要包括以下三个组成部分：</p> 
<ul><li>变分自编码器 (<code>Variational autoEncoder</code>, <code>VAE</code>)<br> <code>VAE</code> 模型主要包含两个部分：编码器和解码器，其中编码器用于将图像转换为低维的潜在表示，得到的低维潜在表示将作为 <code>U-Net</code> 模型的输入，而解码器用于将潜在表示转换回图像。在 <code>Latent diffusion</code> 训练期间，编码器用于获得正向扩散过程的图像的潜在表示，正向扩散在每一步中逐步使用越来越多的噪声。在推断过程中，使用 <code>VAE</code> 解码器将反向扩散过程生成的去噪潜在表示转换回图像</li><li><code>U-Net</code><br> <code>U-Net</code> 同样包含编码器和解码器两部分，且都由 <code>ResNet</code> 块组成，编码器将图像表示压缩成较低分辨率的图像表示，解码器将较低分辨率图像表示解码回原始较高分辨率的图像表示。为了防止 <code>U-Net</code> 在下采样时丢失重要信息，通常在编码器的下采样 <code>ResNet</code> 和解码器的上采样 <code>ResNet</code> 之间添加捷径连接 (<code>Short-cut Connections</code>)，此外，<code>U-Net</code> 能够通过 <code>cross-attention</code> 层将其输出条件设置在文本嵌入上，<code>cross-attention</code> 层被添加到 <code>U-Net</code> 的编码器和解码器部分，通常用在 <code>ResNet</code> 块之间</li><li><code>Text-Encoder</code><br> <code>Text-Encoder</code> 负责将输入提示转换到 <code>U-Net</code> 可以理解的嵌入空间，它通常是一个简单的基于 <code>Transformer</code> 的编码器，将输入分词序列映射到潜在文本嵌入序列。受 <code>Imagen</code> 的启发，<code>Stable Diffusion</code> 在训练过程中并不会训练 <code>Text-Encoder</code>，只使用 <code>CLIP</code> 预训练过的 <code>Text-Encoder</code>——<code>CLIPTextModel</code></li></ul> 
<p>但是，纵然由于 <code>Latent diffusion</code> 可以在低维潜在空间上进行操作，与像素空间扩散模型相比，它极大的降低了内存和计算需求，但如果需要生成高质量照片，模型仍然需要在 <code>16GB</code> 以上 <code>GPU</code> 上运行，具体而言，在本地计算机上搭建 <code>Stable Diffusion</code> 模型会遇到以下困难：</p> 
<ul><li>软件环境：<code>Stable Diffusion</code> 模型的构建需要使用特定的软件和库，在本地计算机上搭建软件环境可能会遇到版本不兼容、依赖关系复杂等问题，需要花费大量时间和精力进行调试和解决</li><li>数据处理：<code>Stable Diffusion</code> 模型训练需要处理大量的高质量图像数据，在本地计算机上处理大量数据可能会导致内存不足、速度慢等问题</li><li>计算资源限制：<code>Stable Diffusion</code> 模型训练需要大量的计算资源，包括高显存 <code>GPU</code> 和大量内存，如果本地计算机的计算资源不足，将无法训练模型</li><li>超参数：<code>Stable Diffusion</code> 模型需要设置大量参数，如扩散系数、边界条件、学习率等，这些超参数的选择需要经过大量调试，否则可能会导致模型不收敛或者收敛速度过慢</li><li>模型验证：<code>Stable Diffusion</code> 模型需要进行大量模型验证和测试，以确保模型的正确性和可靠性</li></ul> 
<p>综上所述，搭建 <code>Stable Diffusion</code> 模型需要克服计算资源限制、软件兼容问题、数据处理和超参数选择等困难。因此，选择云计算平台来简化这些工作便成为自然的选择，而 <code>Amazon SageMaker</code> 作为完全托管的机器学习服务成为构建、训练与部署复杂模型(例如 <code>Stable Diffusion</code> )的首选。</p> 
<h4><a id="3__Amazon_SageMaker__Stable_Diffusion__50"></a>3. 使用 Amazon SageMaker 创建 Stable Diffusion 模型</h4> 
<p>在本节中，我们将介绍基于 <code>Amazon SageMaker</code> 使用 <code>Amazon SageMaker Notebook</code> 实例测试、验证 <code>AIGC</code> 模型并<br> 部署 <code>AIGC</code> 模型至 <code>Amazon SageMaker Inference Endpoint</code>。</p> 
<h5><a id="31__57"></a>3.1 准备工作</h5> 
<p>为了确保能够将 <code>AIGC</code> 模型部署至 <code>Amazon SageMaker Inference Endpoint</code>，需要确保有足够的限额。为此，我们首先需要通过<a href="https://bbs.csdn.net/topics/618545628">服务配额页面</a>检查配额，在搜索框中输入 <code>ml.g4dn.xlarge for endpoint usage</code>，若此配额的第二列为 <code>0</code>，则需要提高配额：</p> 
<p><img src="https://images2.imgbox.com/f6/60/MeoqTiNu_o.png" alt="提升配额"></p> 
<p>提高限额，需首先选中 <code>ml.g4dn.xlarge for endpoint usage</code>，点击“请求增加配额”按钮：</p> 
<p><img src="https://images2.imgbox.com/01/1c/n8dXgvDH_o.png" alt="请求增加配额"></p> 
<p>在输入框中输入所需的限额，例如 “<code>1</code>”，填写完毕后，点击“请求”按钮提交请求：</p> 
<p><img src="https://images2.imgbox.com/fb/d5/irNZbb9K_o.png" alt="提交配额请求"><br> 等待配额请求通过后，就可以继续该实验过程。</p> 
<h5><a id="32__Amazon_SageMaker_Notebook__79"></a>3.2 创建 Amazon SageMaker Notebook 实例</h5> 
<p><code>Amazon SageMaker Notebook</code> 实例是运行 <code>Jupyter Notebook</code> 应用程序的机器学习计算实例。<code>Amazon SageMaker</code> 用于管理实例和相关资源的创建，我们可以在 <code>Notetbook</code> 实例中使用 <code>Jupyter Notebook</code> 准备和处理数据、编写代码来训练模型、或将模型部署到 <code>Amazon SageMaker</code> 中，并测试或验证模型。接下来，我们将创建 <code>Amazon SageMaker Notebook</code> 示例，用于运行相关 <code>Jupyter Notebook</code> 代码。</p> 
<p><strong>(1)</strong> 登录 <a href="https://bbs.csdn.net/topics/618545628">Amazon 云科技控制台</a>，并将当前区域修改为 <code>Tokyo</code> 区域：</p> 
<p><img src="https://images2.imgbox.com/f3/1e/pWVjYuXi_o.png" alt="修改当前区域"></p> 
<p><strong>(2)</strong> 在搜索框中搜索 <code>Amazon SageMaker</code>，并点击进入 <code>Amazon SageMaker</code> 服务：</p> 
<p><img src="https://images2.imgbox.com/f2/09/zRjap1oC_o.png" alt="进入Amazon SageMake服务"></p> 
<p><strong>(3)</strong> 在左侧菜单栏，首先点击“笔记本”按钮，然后点击“笔记本实例”，进入笔记本 (<code>Notebook</code>) 实例控制面板，并点击右上角”创建笔记本实例“按钮：</p> 
<p><img src="https://images2.imgbox.com/22/60/nQ6XgoMI_o.png" alt="创建笔记本实例"></p> 
<p><strong>(4)</strong> 配置笔记本实例设置，在创建笔记本实例详情页中，配置笔记本实例的基本信息，包括笔记本实例名称(例如 <code>stable-diffusion</code>)、笔记本实例类型(选择 <code>ml.g4dn.xlarge</code> 实例类型，该类型实例搭载 <code>NVIDIA T4 Tensor Core GPU</code> 显卡，提供了模型所需执行浮点数计算的能力)、平台标识符( <code>Amazon Linux 2, Jupyter Lab 3</code> )和在“其他配置”下的卷大小(推荐至少 <code>75GB</code> 磁盘大小，用于存储机器学习模型)：</p> 
<p><img src="https://images2.imgbox.com/1b/b8/usSMS7yh_o.png" alt="配置笔记本实例"></p> 
<p><strong>(5)</strong> 配置笔记本实例权限，为笔记本实例创建一个 <code>IAM</code> 角色，用于调用包括 <code>Amazon SageMaker</code> 和 <code>S3</code> 在内的服务，例如上传模型，部署模型等。在“权限和加密”下的 <code>IAM</code> 角色中，点击下拉列表，单击“创建新角色”：</p> 
<p><img src="https://images2.imgbox.com/4c/68/WvyFUbiW_o.png" alt="创建新角色"></p> 
<p>在配置页面中，保持默认配置，并点击“创建角色”按钮：</p> 
<p><img src="https://images2.imgbox.com/f8/65/el7x3Rbh_o.png" alt="创建角色"></p> 
<p>成功创建 <code>IAM</code> 角色后，可以得到类似下图的提示信息：</p> 
<p><img src="https://images2.imgbox.com/4b/f0/TgTJnkGL_o.png" alt="成功创建角色"></p> 
<p><strong>(6)</strong> 检查配置的信息，确认无误后点击“创建笔记本实例”按钮，等待实例创建完成。</p> 
<p><img src="https://images2.imgbox.com/21/e7/zegzh9L6_o.png" alt="创建笔记本实例"></p> 
<p><strong>(7)</strong> 当笔记本状态变为 <code>InService</code> 后，点击“打开Jupyter”进入 <code>Jupyter Notebook</code>：</p> 
<p><img src="https://images2.imgbox.com/e8/81/z4pUSKof_o.png" alt="进入Jupyter Notebook"></p> 
<h5><a id="33__AIGC_139"></a>3.3 端到端体验 AIGC</h5> 
<p>接下来，我们可以<a href="https://bbs.csdn.net/topics/618545628">下载</a>保存 <code>Notebook</code> 代码文件，并将其上传到 <code>Jupyter Notebook</code>，然后直接运行代码，但亲手编写代码的体验是无与伦比，我们将介绍代码文件的主要内容，从头开始端到端体验 <code>AIGC</code>！需要注意的是，需要确保 <code>Kernel</code> 以 <code>conda_pytorch</code> 开头。</p> 
<p><strong>(1)</strong> 安装相关库并进行环境配置工作：</p> 
<pre><code># 检查环境版本
!nvcc --version
!pip list | grep torch
# 安装Notebook运行模型所需的库文件
!sudo yum -y install pigz
!pip install -U pip
!pip install -U transformers==4.26.1 diffusers==0.13.1 ftfy accelerate
!pip install -U torch==1.13.1+cu117 -f https://download.pytorch.org/whl/torch_stable.html
!pip install -U sagemaker
!pip list | grep torch

</code></pre> 
<p><strong>(2)</strong> 下载模型文件，我们将使用 <code>Stable Diffusion V2</code> 版本，其包含一个具有鲁棒性的文本生成图像模型，能够极大的提高了图像生成质量，模型相关介绍参见 <a href="https://bbs.csdn.net/topics/618545628">Github</a>：</p> 
<pre><code># 安装git lfs以克隆模型仓库
!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash
!sudo yum install git-lfs -y
# 设定模型版本的环境变量，使用 Stable Diffusion V2
SD_SPACE="stabilityai/"
SD_MODEL = "stable-diffusion-2-1"
# 克隆代码仓库
# Estimated time to spend 3min(V1), 8min(V2)
%cd ~/SageMaker
!printf "=======Current Path========%s\n"
!rm -rf $SD_MODEL
# !git lfs clone https://huggingface.co/$SD\_SPACE$SD\_MODEL -X "\*.safetensors"
!mkdir $SD_MODEL
%cd $SD_MODEL
!git init
!git config core.sparseCheckout true
!echo "/\*" &gt;&gt; .git/info/sparse-checkout
!echo "!\*\*/\*.safetensors" &gt;&gt; .git/info/sparse-checkout
!git remote add -f master https://huggingface.co/$SD_SPACE$SD_MODEL
!git pull master main
%cd ~/SageMaker
!printf "=======Folder========%s\n$(ls)\n"

</code></pre> 
<p><strong>(3)</strong> 在 <code>Notebook</code> 中配置并使用模型，首先加载相关库与模型：</p> 
<pre><code>import torch
import datetime
from diffusers import StableDiffusionPipeline
# Load stable diffusion
pipe = StableDiffusionPipeline.from_pretrained(SD_MODEL, torch_dtype=torch.float16)

</code></pre> 
<p>使用 <code>GPU</code> 进行运算并设定超参数，部分超参数如下：</p> 
<ul><li>prompt (str 或 List[str])：引导图像生成的文本提示或文本列表</li><li>height (int, *optional, V2 默认模型可支持到 <code>768</code> 像素)：生成图像的高度(以像素为单位)</li><li>width (int, *optional, V2 默认模型可支持到 <code>768</code> 像素)：生成图像的宽度(以像素为单位)</li><li>num_inference_steps (int, *optional, 默认降噪步数为 <code>50</code>)：降噪步数，更多的降噪步数通常会以较慢的推理为代价获得更高质量的图像</li><li>guidance_scale (float, *optional, 默认指导比例为 <code>7.5</code>)：较高的指导比例会导致图像与提示密切相关，但会牺牲图像质量，当 <code>guidance_scale&lt;=1</code> 时会被忽略</li><li>negative_prompt (str or List[str], *optional)：不引导图像生成的文本或文本列表</li><li>num_images_per_prompt (int, *optional, 默认每个提示生成 <code>1</code> 张图像)：每个提示生成的图像数量</li></ul> 
<pre><code># move Model to the GPU
torch.cuda.empty_cache()
pipe = pipe.to("cuda")

print(datetime.datetime.now())
prompts =[
    "Eiffel tower landing on the Mars",
    "a photograph of an astronaut riding a horse,van Gogh style",
]
generated_images = pipe(
    prompt=prompts,
    height=512,
    width=512,
    num_images_per_prompt=1
).images  # image here is in [PIL format](https://bbs.csdn.net/topics/618545628)

print(f"Prompts: {prompts}\n")
print(datetime.datetime.now())

for image in generated_images:
    display(image)

</code></pre> 
<p><strong>(4)</strong> 将模型部署至 <code>Sagemaker Inference Endpoint</code>，构建和训练模型后，可以将模型部署至终端节点，以获取预测推理结果：</p> 
<pre><code>import sagemaker
import boto3
sess = sagemaker.Session()
# sagemaker session bucket -&gt; used for uploading data, models and logs
# sagemaker will automatically create this bucket if it not exists
sagemaker_session_bucket=None

if sagemaker_session_bucket is None and sess is not None:
    # set to default bucket if a bucket name is not given
    sagemaker_session_bucket = sess.default_bucket()

try:
    role = sagemaker.get_execution_role()
except ValueError:
    iam = boto3.client('iam')
    role = iam.get_role(RoleName='sagemaker\_execution\_role')['Role']['Arn']

sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)

print(f"sagemaker role arn: {role}")
print(f"sagemaker bucket: {sess.default\_bucket()}")
print(f"sagemaker session region: {sess.boto\_region\_name}")

</code></pre> 
<p>创建自定义推理脚本 <code>inference.py</code>：</p> 
<pre><code>!mkdir ./$SD_MODEL/code
# 为模型创建所需依赖声明的文件
%%writefile ./$SD_MODEL/code/requirements.txt
diffusers==0.13.1
transformers==4.26.1
# 编写 inference.py 脚本
%%writefile ./$SD_MODEL/code/inference.py
import base64
import torch
from io import BytesIO
from diffusers import StableDiffusionPipeline


def model\_fn(model_dir):
    # Load stable diffusion and move it to the GPU
    pipe = StableDiffusionPipeline.from_pretrained(model_dir, torch_dtype=torch.float16)
    pipe = pipe.to("cuda")

    return pipe


def predict\_fn(data, pipe):

    # get prompt &amp; parameters
    prompt = data.pop("prompt", "")
    # set valid HP for stable diffusion
    height = data.pop("height", 512)
    width = data.pop("width", 512)
    num_inference_steps = data.pop("num\_inference\_steps", 50)
    guidance_scale = data.pop("guidance\_scale", 7.5)
    num_images_per_prompt = data.pop("num\_images\_per\_prompt", 1)
    # run generation with parameters
    generated_images = pipe(
        prompt=prompt,
        height=height,
        width=width,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
        num_images_per_prompt=num_images_per_prompt,
    )["images"]

    # create response
    encoded_images = []
    for image in generated_images:
        buffered = BytesIO()
        image.save(buffered, format="JPEG")
        encoded_images.append(base64.b64encode(buffered.getvalue()).decode())

    # create response
    return {"generated\_images": encoded_images}

</code></pre> 
<p>打包模型并上传至 <code>S3</code> 桶：</p> 
<pre><code>#Package model, Estimated time to spend 2min(V1),5min(V2)
!echo $(date)
!tar --exclude .git --use-compress-program=pigz -pcvf ./$SD_MODEL'.tar.gz' -C ./$SD_MODEL/ .
!echo $(date)

from sagemaker.s3 import S3Uploader

print(datetime.datetime.now())
# upload model.tar.gz to s3, Estimated time to spend 30s(V1), 1min(V2)
sd_model_uri=S3Uploader.upload(local_path=f"{SD\_MODEL}.tar.gz", desired_s3_uri=f"s3://{sess.default\_bucket()}/stable-diffusion")
print(f"=======S3 File Location========\nmodel uploaded to:\n{sd\_model\_uri}")



![img](https://img-blog.csdnimg.cn/img_convert/baeae6f19b580cf99d9c295ca8b44015.png)
![img](https://img-blog.csdnimg.cn/img_convert/f0d1980dedb15a8b6be3fc4be1cc3904.png)

**网上学习资料一大堆，但如果学到的知识不成体系，遇到问题时只是浅尝辄止，不再深入研究，那么很难做到真正的技术提升。**

**[需要这份系统化资料的朋友，可以戳这里获取](https://bbs.csdn.net/topics/618545628)**


**一个人可以走的很快，但一群人才能走的更远！不论你是正从事IT行业的老鸟或是对IT行业感兴趣的新人，都欢迎加入我们的的圈子（技术交流、学习资源、职场吐槽、大厂内推、面试辅导），让我们一起学习成长！**

aded to:\n{sd\_model\_uri}")



[外链图片转存中...(img-JOyPsQ3W-1714465985099)]
[外链图片转存中...(img-yZ8ddUix-1714465985099)]

**网上学习资料一大堆，但如果学到的知识不成体系，遇到问题时只是浅尝辄止，不再深入研究，那么很难做到真正的技术提升。**

**[需要这份系统化资料的朋友，可以戳这里获取](https://bbs.csdn.net/topics/618545628)**


**一个人可以走的很快，但一群人才能走的更远！不论你是正从事IT行业的老鸟或是对IT行业感兴趣的新人，都欢迎加入我们的的圈子（技术交流、学习资源、职场吐槽、大厂内推、面试辅导），让我们一起学习成长！**

</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c795726e118e10d01e74a8d108fc23f1/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">AI大模型探索之路-训练篇5：大语言模型预训练数据准备-词元化</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/81d3353b046e221c7f6016c6dab4155b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">一文带你搞懂AI前沿技术AIGC</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>