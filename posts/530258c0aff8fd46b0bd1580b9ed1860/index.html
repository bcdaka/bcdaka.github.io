<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【李沐】动手学深度学习 学习笔记 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/530258c0aff8fd46b0bd1580b9ed1860/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【李沐】动手学深度学习 学习笔记">
  <meta property="og:description" content="目录 【内容介绍】动手学深度学习-基于pytorch版本【脉络梳理】预备知识数据操作数据预处理线性代数矩阵计算自动求导 线性神经网络线性回归深度学习的基础优化算法线性回归的从零开始实现线性回归的简洁实现Softmax回归损失函数图像分类数据集Softmax回归的从零开始实现Softmax回归的简洁实现 多层感知机感知机多层感知机多层感知机的从零开始实现多层感知机的简洁实现模型选择过拟合和欠拟合权重衰退暂退法（Dropout）数值稳定性模型初始化和激活函数 深度学习计算层和块参数管理自定义层读写文件 卷积神经网络从全连接层到卷积图像卷积填充和步幅多输入多输出通道池化层（汇聚层）LeNet 现代卷积神经网络深度卷积神经网络（AlexNet）使用块的网络（VGG）网络中的网络（NiN）含并行连结的网络（GoogLeNet）批量规范化（归一化）残差网络（ResNet） 计算性能深度学习硬件(CPU和GPU)深度学习硬件(TPU和其他)单机多卡并行分布式训练 计算机视觉图像增广微调物体检测和数据集锚框物体检测算法：R-CNN，SSD，YOLO单发多框检测（SSD）YOLO语义分割转置卷积全连接卷积神经网络 FCN样式迁移 循环神经网络序列模型语言模型循环神经网络 现代循环神经网络门控循环单元GRU长短期记忆网络（LSTM）深度循环神经网络双向循环神经网络编码器-解码器序列到序列学习束搜索注意力机制注意力分数使用注意力机制的seq2seq自注意力Transformer 自然语言处理：预训练BERT预训练 自然语言处理：应用BERT微调 优化算法优化算法 【内容介绍】动手学深度学习-基于pytorch版本 你好！ 这是【李沐】动手学深度学习v2-基于pytorch版本的学习笔记
教材
源代码
安装教程（安装pytorch不要用pip，改成conda，pip太慢了，下载不下来）
个人推荐学习学习笔记
【脉络梳理】 预备知识 数据操作 本节代码文件在源代码文件的chapter_preliminaries/ndarray.ipynb中
创建数组
创建数组需要：
①形状
②每个元素的数据类型
③每个元素的值访问元素
①一个元素：[1,2]
②一行：[1,:]
③一列：[:,1]
④子区域：[1:3,1:] #第1到2行，第1到最后1列
⑤子区域：[::3,::2] #从第0行开始，每3行一跳，从第0列开始，每2列一跳。 数据预处理 本节代码文件在源代码文件的chapter_preliminaries/pandas.ipynb中
reshape函数
使用reshape函数后不指向同一块内存，但数据改变是同步的import torch a=torch.arange(12) b=a.reshape((3,4)) b[:]=2 # 改变b，a也会随之改变 print(a) # tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) a[:]=1 # 改变a，b也会随之改变 print(b) # tensor([[1, 1, 1, 1],[1, 1, 1, 1],[1, 1, 1, 1]]) print(id(b)==id(a)) # False # 但a、b内存不同 print(id(a)) # 2157597781424 print(id(b)) # 2157597781424 线性代数 本节代码文件在源代码文件的chapter_preliminaries/linear-algebra.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-03-21T13:27:05+08:00">
    <meta property="article:modified_time" content="2023-03-21T13:27:05+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【李沐】动手学深度学习 学习笔记</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#pytorch_2" rel="nofollow">【内容介绍】动手学深度学习-基于pytorch版本</a></li><li><a href="#_11" rel="nofollow">【脉络梳理】</a></li><li><ul><li><a href="#_12" rel="nofollow">预备知识</a></li><li><ul><li><a href="#_13" rel="nofollow">数据操作</a></li><li><a href="#_26" rel="nofollow">数据预处理</a></li><li><a href="#_43" rel="nofollow">线性代数</a></li><li><a href="#_72" rel="nofollow">矩阵计算</a></li><li><a href="#_88" rel="nofollow">自动求导</a></li></ul> 
   </li><li><a href="#_104" rel="nofollow">线性神经网络</a></li><li><ul><li><a href="#_105" rel="nofollow">线性回归</a></li><li><a href="#_121" rel="nofollow">深度学习的基础优化算法</a></li><li><a href="#_133" rel="nofollow">线性回归的从零开始实现</a></li><li><a href="#_141" rel="nofollow">线性回归的简洁实现</a></li><li><a href="#Softmax_149" rel="nofollow">Softmax回归</a></li><li><a href="#_164" rel="nofollow">损失函数</a></li><li><a href="#_184" rel="nofollow">图像分类数据集</a></li><li><a href="#Softmax_193" rel="nofollow">Softmax回归的从零开始实现</a></li><li><a href="#Softmax_203" rel="nofollow">Softmax回归的简洁实现</a></li></ul> 
   </li><li><a href="#_212" rel="nofollow">多层感知机</a></li><li><ul><li><a href="#_213" rel="nofollow">感知机</a></li><li><a href="#_226" rel="nofollow">多层感知机</a></li><li><a href="#_253" rel="nofollow">多层感知机的从零开始实现</a></li><li><a href="#_261" rel="nofollow">多层感知机的简洁实现</a></li><li><a href="#_266" rel="nofollow">模型选择</a></li><li><a href="#_276" rel="nofollow">过拟合和欠拟合</a></li><li><a href="#_299" rel="nofollow">权重衰退</a></li><li><a href="#Dropout_312" rel="nofollow">暂退法（Dropout）</a></li><li><a href="#_324" rel="nofollow">数值稳定性</a></li><li><a href="#_344" rel="nofollow">模型初始化和激活函数</a></li></ul> 
   </li><li><a href="#_372" rel="nofollow">深度学习计算</a></li><li><ul><li><a href="#_373" rel="nofollow">层和块</a></li><li><a href="#_425" rel="nofollow">参数管理</a></li><li><a href="#_627" rel="nofollow">自定义层</a></li><li><a href="#_695" rel="nofollow">读写文件</a></li></ul> 
   </li><li><a href="#_777" rel="nofollow">卷积神经网络</a></li><li><ul><li><a href="#_778" rel="nofollow">从全连接层到卷积</a></li><li><a href="#_789" rel="nofollow">图像卷积</a></li><li><a href="#_803" rel="nofollow">填充和步幅</a></li><li><a href="#_821" rel="nofollow">多输入多输出通道</a></li><li><a href="#_838" rel="nofollow">池化层（汇聚层）</a></li><li><a href="#LeNet_852" rel="nofollow">LeNet</a></li></ul> 
   </li><li><a href="#_866" rel="nofollow">现代卷积神经网络</a></li><li><ul><li><a href="#AlexNet_867" rel="nofollow">深度卷积神经网络（AlexNet）</a></li><li><a href="#VGG_896" rel="nofollow">使用块的网络（VGG）</a></li><li><a href="#NiN_910" rel="nofollow">网络中的网络（NiN）</a></li><li><a href="#GoogLeNet_923" rel="nofollow">含并行连结的网络（GoogLeNet）</a></li><li><a href="#_950" rel="nofollow">批量规范化（归一化）</a></li><li><a href="#ResNet_962" rel="nofollow">残差网络（ResNet）</a></li></ul> 
   </li><li><a href="#_981" rel="nofollow">计算性能</a></li><li><ul><li><a href="#CPUGPU_982" rel="nofollow">深度学习硬件(CPU和GPU)</a></li><li><a href="#TPU_1006" rel="nofollow">深度学习硬件(TPU和其他)</a></li><li><a href="#_1030" rel="nofollow">单机多卡并行</a></li><li><a href="#_1041" rel="nofollow">分布式训练</a></li></ul> 
   </li><li><a href="#_1063" rel="nofollow">计算机视觉</a></li><li><ul><li><a href="#_1064" rel="nofollow">图像增广</a></li><li><a href="#_1083" rel="nofollow">微调</a></li><li><a href="#_1100" rel="nofollow">物体检测和数据集</a></li><li><a href="#_1112" rel="nofollow">锚框</a></li><li><a href="#RCNNSSDYOLO_1125" rel="nofollow">物体检测算法：R-CNN，SSD，YOLO</a></li><li><a href="#SSD_1145" rel="nofollow">单发多框检测（SSD）</a></li><li><a href="#YOLO_1155" rel="nofollow">YOLO</a></li><li><a href="#_1160" rel="nofollow">语义分割</a></li><li><a href="#_1170" rel="nofollow">转置卷积</a></li><li><a href="#_FCN_1188" rel="nofollow">全连接卷积神经网络 FCN</a></li><li><a href="#_1195" rel="nofollow">样式迁移</a></li></ul> 
   </li><li><a href="#_1202" rel="nofollow">循环神经网络</a></li><li><ul><li><a href="#_1203" rel="nofollow">序列模型</a></li><li><a href="#_1218" rel="nofollow">语言模型</a></li><li><a href="#_1229" rel="nofollow">循环神经网络</a></li></ul> 
   </li><li><a href="#_1245" rel="nofollow">现代循环神经网络</a></li><li><ul><li><a href="#GRU_1246" rel="nofollow">门控循环单元GRU</a></li><li><a href="#LSTM_1260" rel="nofollow">长短期记忆网络（LSTM）</a></li><li><a href="#_1274" rel="nofollow">深度循环神经网络</a></li><li><a href="#_1285" rel="nofollow">双向循环神经网络</a></li><li><a href="#_1297" rel="nofollow">编码器-解码器</a></li><li><a href="#_1307" rel="nofollow">序列到序列学习</a></li><li><a href="#_1323" rel="nofollow">束搜索</a></li><li><a href="#_1334" rel="nofollow">注意力机制</a></li><li><a href="#_1348" rel="nofollow">注意力分数</a></li><li><a href="#seq2seq_1360" rel="nofollow">使用注意力机制的seq2seq</a></li><li><a href="#_1368" rel="nofollow">自注意力</a></li><li><a href="#Transformer_1383" rel="nofollow">Transformer</a></li></ul> 
   </li><li><a href="#_1402" rel="nofollow">自然语言处理：预训练</a></li><li><ul><li><a href="#BERT_1403" rel="nofollow">BERT预训练</a></li></ul> 
   </li><li><a href="#_1419" rel="nofollow">自然语言处理：应用</a></li><li><ul><li><a href="#BERT_1420" rel="nofollow">BERT微调</a></li></ul> 
   </li><li><a href="#_1432" rel="nofollow">优化算法</a></li><li><ul><li><a href="#_1433" rel="nofollow">优化算法</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="pytorch_2"></a>【内容介绍】动手学深度学习-基于pytorch版本</h2> 
<p>你好！ 这是<a href="https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497" rel="nofollow">【李沐】动手学深度学习v2-基于pytorch版本</a>的学习笔记<br> <a href="https://zh.d2l.ai/" rel="nofollow">教材</a><br> <a href="https://github.com/d2l-ai/d2l-zh">源代码</a><br> <a href="https://www.zhihu.com/zvideo/1363284223420436480" rel="nofollow">安装教程</a>（安装pytorch不要用pip，改成conda，pip太慢了，下载不下来）<br> <a href="https://github.com/HIT-UG-Group/DeepLearning-MuLi-Notes">个人推荐学习学习笔记</a></p> 
<h2><a id="_11"></a>【脉络梳理】</h2> 
<h3><a id="_12"></a>预备知识</h3> 
<h4><a id="_13"></a>数据操作</h4> 
<p>  本节代码文件在源代码文件的chapter_preliminaries/ndarray.ipynb中</p> 
<ul><li><strong>创建数组</strong><br>   创建数组需要：<br>   ①形状<br>   ②每个元素的数据类型<br>   ③每个元素的值</li><li><strong>访问元素</strong><br>   ①一个元素：[1,2]<br>   ②一行：[1,:]<br>   ③一列：[:,1]<br>   ④子区域：[1:3,1:] #第1到2行，第1到最后1列<br>   ⑤子区域：[::3,::2] #从第0行开始，每3行一跳，从第0列开始，每2列一跳。</li></ul> 
<h4><a id="_26"></a>数据预处理</h4> 
<p>  本节代码文件在源代码文件的chapter_preliminaries/pandas.ipynb中</p> 
<ul><li><strong>reshape函数</strong><br>   使用reshape函数后不指向同一块内存，但数据改变是同步的<pre><code class="prism language-python"><span class="token keyword">import</span> torch
a<span class="token operator">=</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">)</span>
b<span class="token operator">=</span>a<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
b<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">2</span> <span class="token comment"># 改变b，a也会随之改变</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token comment"># tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) </span>
a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">1</span> <span class="token comment"># 改变a，b也会随之改变</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token comment"># tensor([[1, 1, 1, 1],[1, 1, 1, 1],[1, 1, 1, 1]])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">id</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token operator">==</span><span class="token builtin">id</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># False </span>
<span class="token comment"># 但a、b内存不同</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">id</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 2157597781424</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">id</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 2157597781424</span>
</code></pre> </li></ul> 
<h4><a id="_43"></a>线性代数</h4> 
<p>  本节代码文件在源代码文件的chapter_preliminaries/linear-algebra.ipynb中</p> 
<ul><li><strong>张量的按元素操作</strong><br>   标量、向量、矩阵和任意数量轴的张量（统称为<strong>张量</strong>）有一些实用的属性。 例如，你可能已经从按元素操作的定义中注意到，任何按元素的一元运算都不会改变其操作数的形状。 同样，给定<strong>具有相同形状</strong>的任意两个张量，任何<strong>按元素二元运算</strong>的结果都将是相同形状的张量。<br>   例如，将两个相同形状的矩阵相加，会在这两个矩阵上执行按元素加法；两个矩阵的按元素乘法称为Hadamard积（Hadamard product）（数学符号<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          ⨀ 
         
        
       
         \bigodot 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mop op-symbol small-op" style="position: relative; top: 0em;">⨀</span></span></span></span></span>）。 对于矩阵<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          A 
         
        
          ∈ 
         
         
         
           R 
          
          
          
            m 
           
          
            n 
           
          
         
        
          , 
         
         
         
           B 
          
         
           ∈ 
          
          
          
            R 
           
           
           
             m 
            
           
             n 
            
           
          
         
        
       
         \bf A\in\mathbb R^{mn},\bf B\in\mathbb R^{mn} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8833em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathbf">A</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6741em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathbf mtight">mn</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathbf">B</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6741em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathbf mtight">mn</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>， 其中第<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          i 
         
        
       
         \it i 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6554em;"></span><span class="mord"><span class="mord mathit">i</span></span></span></span></span></span>行和第<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          j 
         
        
       
         \it j 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8498em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathit">j</span></span></span></span></span></span>列的元素是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           b 
          
          
          
            i 
           
          
            j 
           
          
         
        
       
         b_{ij} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9805em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span>。 矩阵<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          A 
         
        
       
         \bf A 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6861em;"></span><span class="mord"><span class="mord mathbf">A</span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          B 
         
        
       
         \bf B 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6861em;"></span><span class="mord"><span class="mord mathbf">B</span></span></span></span></span></span>的Hadamard积为<br> <img src="https://images2.imgbox.com/45/38/qWL9StcU_o.png" alt="在这里插入图片描述" width="400"><pre><code class="prism language-python"><span class="token keyword">import</span> torch
A <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
B <span class="token operator">=</span> A<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 通过分配新内存，将A的一个副本分配给B</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">)</span>
<span class="token comment">#tensor([[ 0.,  1.,  2.,  3.],[ 4.,  5.,  6.,  7.],[ 8.,  9., 10., 11.],[12., 13., 14., 15.],[16., 17., 18., 19.]])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token operator">+</span>B<span class="token punctuation">)</span>
<span class="token comment">#tensor([[ 0.,  2.,  4.,  6.],[ 8., 10., 12., 14.],[ 16., 18., 20., 22.],[24., 26., 28., 30.],[32., 34., 36., 38.]])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token operator">*</span>B<span class="token punctuation">)</span>
<span class="token comment">#tensor([[0., 1., 4., 9.],[16., 25., 36., 49.],[64., 81., 100., 121.],[144., 169., 196., 225.],[256., 289., 324., 361.]])</span>
</code></pre> </li><li><strong>降维求和</strong><br>   ①原始shape：[5,4]<br>     · axis=0 sum:[ 4 ]<br>     · axis=1 sum:[ 5 ]<br>   ②原始shape：[2,5,4]<br>     · axis=1 sum:[2,4]<br>     · axis=2 sum:[2,5]<br>     · axis=[1,2] sum:[ 4 ]</li><li><strong>非降维求和</strong><br>   原始shape：[2,5,4] <mark>参数 keepdims=True</mark><br>     · axis=1 sum:[2,1,4]<br>     · axis=1 sum:[2,1,1]</li></ul> 
<h4><a id="_72"></a>矩阵计算</h4> 
<p>  本节代码文件在源代码文件的chapter_preliminaries/calculus.ipynb中</p> 
<ul><li> <p><strong>将导数拓展到向量</strong><br>   <img src="https://images2.imgbox.com/65/6b/AeLxYmf9_o.png" alt="将导数拓展到向量" width="400"></p> </li><li> <p><strong>标量对列向量求导</strong><br>     <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           其中， 
          
          
          
            a 
           
          
            不是关于 
           
           
           
             x 
            
           
             的函数， 
            
            
            
              0 
             
            
              和 
             
             
             
               1 
              
             
               是向量 
              
             
               ; 
              
             
            
           
          
         
        
          其中，\it a不是关于\bf x的函数，\color{black} 0和\bf 1是向量; 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8805em; vertical-align: -0.1944em;"></span><span class="mord cjk_fallback">其中，</span><span class="mord"><span class="mord mathit">a</span><span class="mord mathit cjk_fallback">不是关于</span><span class="mord"><span class="mord mathbf">x</span><span class="mord mathbf cjk_fallback">的函数，</span><span class="mord mathbf" style="color: black;">0</span><span class="mord mathbf cjk_fallback" style="color: black;">和</span><span class="mord" style="color: black;"><span class="mord mathbf" style="color: black;">1</span><span class="mord mathbf cjk_fallback" style="color: black;">是向量</span><span class="mpunct" style="color: black;">;</span></span></span></span></span></span></span></span><br>       <img src="https://images2.imgbox.com/2d/99/1YGhHWYt_o.png" alt="在这里插入图片描述" width="400"></p> </li><li> <p><strong>列向量对列向量求导</strong><br>   <mark><strong>结果是矩阵</strong></mark><br>     <img src="https://images2.imgbox.com/f9/0d/IGEspXdz_o.png" alt="列向量对列向量求导公式" width="400"><br>    <strong>样例：</strong><br>      <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           x 
          
         
           ∈ 
          
          
          
            R 
           
          
            n 
           
          
         
           , 
          
          
          
            y 
           
          
            ∈ 
           
           
           
             R 
            
           
             m 
            
           
          
            , 
           
           
            
            
              ∂ 
             
            
              y 
             
            
            
            
              ∂ 
             
            
              x 
             
            
           
          
            ∈ 
           
           
           
             R 
            
            
            
              m 
             
            
              n 
             
            
           
          
            ; 
           
           
           
             a 
            
           
             , 
            
            
            
              a 
             
            
              和 
             
             
             
               A 
              
             
               不是关于 
              
              
              
                x 
               
              
                的函数， 
               
               
               
                 0 
                
               
                 和 
                
                
                
                  I 
                 
                
                  是矩阵 
                 
                
                  ; 
                 
                
               
              
             
            
           
          
         
        
          \bf x\in\mathbb R^{n} ,\bf y\in\mathbb R^{m},\frac{\partial\bf y}{\partial\bf x}\in\mathbb R^{mn};\it a,\bf a和\bf A不是关于\bf x的函数，\color{black} 0和\bf I是矩阵; 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.2772em; vertical-align: -0.345em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6741em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathbf mtight">n</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathbf" style="margin-right: 0.016em;">y</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6741em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathbf mtight">m</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.9322em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathbf mtight" style="margin-right: 0.0639em;">∂</span><span class="mord mtight"><span class="mord mathbf mtight">x</span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.4461em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathbf mtight" style="margin-right: 0.0639em;">∂</span><span class="mord mtight"><span class="mord mathbf mtight" style="margin-right: 0.016em;">y</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6741em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathbf mtight">mn</span></span></span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathit">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathbf">a</span><span class="mord mathbf cjk_fallback">和</span><span class="mord"><span class="mord mathbf">A</span><span class="mord mathbf cjk_fallback">不是关于</span><span class="mord"><span class="mord mathbf">x</span><span class="mord mathbf cjk_fallback">的函数，</span><span class="mord mathbf" style="color: black;">0</span><span class="mord mathbf cjk_fallback" style="color: black;">和</span><span class="mord" style="color: black;"><span class="mord mathbf" style="color: black;">I</span><span class="mord mathbf cjk_fallback" style="color: black;">是矩阵</span><span class="mpunct" style="color: black;">;</span></span></span></span></span></span></span></span></span></span></span></span><br>           <img src="https://images2.imgbox.com/26/cf/05KXIWKg_o.png" alt="列向量对列向量求导样例" width="300"></p> </li><li> <p><strong>将导数拓展到矩阵</strong><br>     <img src="https://images2.imgbox.com/c7/79/umfCpuSA_o.png" alt="将导数拓展到矩阵" width="500"></p> </li></ul> 
<h4><a id="_88"></a>自动求导</h4> 
<p>  本节代码文件在源代码文件的chapter_preliminaries/autograd.ipynb中</p> 
<ul><li><strong>向量链式法则</strong><br>   <img src="https://images2.imgbox.com/b4/95/dG9gFZnc_o.png" alt="链式法则拓展到向量" width="600"><br>   <strong>样例：</strong><br>       <img src="https://images2.imgbox.com/f5/f8/gSReUBgA_o.png" alt="链式法则拓展到向量的样例" width="380"></li><li><strong>自动求导的两种模式</strong><br>     <img src="https://images2.imgbox.com/0c/35/JH57nIoV_o.png" alt="在这里插入图片描述" width="400"></li><li><strong>反向累积模式</strong><br>     <img src="https://images2.imgbox.com/1b/da/az5I5bFH_o.png" alt="在这里插入图片描述" width="350"></li></ul> 
<p>    <strong>样例：</strong><br>       <img src="https://images2.imgbox.com/61/66/WNbxUOJ7_o.png" alt="反向累积模式样例" width="400"></p> 
<ul><li><strong>正向累积与反向累积复杂度比较</strong><br>   正向累积内存复杂度为O(1),反向累积内存复杂度为O(n);但神经网络通常不会用正向累积，因为正向累积每计算一个变量的梯度都需要扫一遍，计算复杂度太高。<br>   <img src="https://images2.imgbox.com/b5/15/ie1LmSqM_o.png" alt="正向累积与反向累积复杂度比较" width="400"></li></ul> 
<h3><a id="_104"></a>线性神经网络</h3> 
<h4><a id="_105"></a>线性回归</h4> 
<p>  本节代码文件在源代码文件的chapter_linear-networks/linear-regression.ipynb中</p> 
<ul><li><strong>线性模型</strong><br>     <img src="https://images2.imgbox.com/00/1b/WKIYFXRJ_o.png" alt="线性模型" width="400"><br>   <strong>线性模型可以看做是单层神经网络：</strong><br>       <img src="https://images2.imgbox.com/4b/08/JQFIxIDv_o.png" alt="线性模型可以看做是单层神经网络" width="400"><br>    <strong>衡量预估质量（损失函数）：</strong><br>       <img src="https://images2.imgbox.com/f2/e4/iGHLdjSg_o.png" alt="衡量预估质量" width="350"></li><li><strong>训练数据</strong><br>     <img src="https://images2.imgbox.com/be/1a/nJTI0ul4_o.png" alt="训练数据" width="400"></li><li><strong>参数学习</strong><br>     <img src="https://images2.imgbox.com/fd/25/wOEKY4Jh_o.png" alt="参数学习" width="400"><br>   <strong>显示解：</strong><br>       <img src="https://images2.imgbox.com/6f/a1/crl2nIbW_o.png" alt="显示解" width="400"></li><li><strong>总结</strong><br>     <img src="https://images2.imgbox.com/23/32/EsTRJsnM_o.png" alt="总结" width="300"></li></ul> 
<h4><a id="_121"></a>深度学习的基础优化算法</h4> 
<ul><li><strong>梯度下降</strong><br>   通过不断地在损失函数递减的方向上更新参数来降低误差。<br>     <img src="https://images2.imgbox.com/33/52/68DrITYE_o.png" alt="梯度下降" width="400"><br>   <strong>学习率不能太大也不能太小：</strong><br>     <img src="https://images2.imgbox.com/4d/04/a9fyU8XU_o.png" alt="学习率不能太大也不能太小" width="400"></li><li><strong>小批量随机梯度下降</strong><br>     <img src="https://images2.imgbox.com/e7/7e/ekx4wGzQ_o.png" alt="小批量随机梯度下降" width="400"><br>   <strong>批量大小不能太大也不能太小：</strong><br>       <img src="https://images2.imgbox.com/93/df/JWcgRRO2_o.png" alt="批量大小不能太大也不能太小" width="300"></li><li><strong>总结</strong><br>     <img src="https://images2.imgbox.com/f8/64/DJrj7pkW_o.png" alt="总结" width="350"></li></ul> 
<h4><a id="_133"></a>线性回归的从零开始实现</h4> 
<p>  本节代码文件在源代码文件的chapter_linear-networks/linear-regression-scratch.ipynb中</p> 
<ul><li><strong>实现流程</strong></li></ul> 
<div class="mermaid sequence-diagram"> 
 <svg id="mermaid-svg-IW2xOck1CxgQL7os" width="100%" xmlns="http://www.w3.org/2000/svg" height="62" style="max-width: 684px;" viewbox="0 0 684 62"> 
  <style>#mermaid-svg-IW2xOck1CxgQL7os {font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;fill:#333;}#mermaid-svg-IW2xOck1CxgQL7os .error-icon{fill:#552222;}#mermaid-svg-IW2xOck1CxgQL7os .error-text{fill:#552222;stroke:#552222;}#mermaid-svg-IW2xOck1CxgQL7os .edge-thickness-normal{stroke-width:2px;}#mermaid-svg-IW2xOck1CxgQL7os .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg-IW2xOck1CxgQL7os .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg-IW2xOck1CxgQL7os .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg-IW2xOck1CxgQL7os .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg-IW2xOck1CxgQL7os .marker{fill:#333333;stroke:#333333;}#mermaid-svg-IW2xOck1CxgQL7os .marker.cross{stroke:#333333;}#mermaid-svg-IW2xOck1CxgQL7os svg{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg-IW2xOck1CxgQL7os .label{font-family:"trebuchet ms",verdana,arial,sans-serif;color:#333;}#mermaid-svg-IW2xOck1CxgQL7os .cluster-label text{fill:#333;}#mermaid-svg-IW2xOck1CxgQL7os .cluster-label span{color:#333;}#mermaid-svg-IW2xOck1CxgQL7os .label text,#mermaid-svg-IW2xOck1CxgQL7os span{fill:#333;color:#333;}#mermaid-svg-IW2xOck1CxgQL7os .node rect,#mermaid-svg-IW2xOck1CxgQL7os .node circle,#mermaid-svg-IW2xOck1CxgQL7os .node ellipse,#mermaid-svg-IW2xOck1CxgQL7os .node polygon,#mermaid-svg-IW2xOck1CxgQL7os .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#mermaid-svg-IW2xOck1CxgQL7os .node .label{text-align:center;}#mermaid-svg-IW2xOck1CxgQL7os .node.clickable{cursor:pointer;}#mermaid-svg-IW2xOck1CxgQL7os .arrowheadPath{fill:#333333;}#mermaid-svg-IW2xOck1CxgQL7os .edgePath .path{stroke:#333333;stroke-width:2.0px;}#mermaid-svg-IW2xOck1CxgQL7os .flowchart-link{stroke:#333333;fill:none;}#mermaid-svg-IW2xOck1CxgQL7os .edgeLabel{background-color:#e8e8e8;text-align:center;}#mermaid-svg-IW2xOck1CxgQL7os .edgeLabel rect{opacity:0.5;background-color:#e8e8e8;fill:#e8e8e8;}#mermaid-svg-IW2xOck1CxgQL7os .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#mermaid-svg-IW2xOck1CxgQL7os .cluster text{fill:#333;}#mermaid-svg-IW2xOck1CxgQL7os .cluster span{color:#333;}#mermaid-svg-IW2xOck1CxgQL7os div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-svg-IW2xOck1CxgQL7os :root{--mermaid-font-family:"trebuchet ms",verdana,arial,sans-serif;}</style> 
  <g> 
   <g class="output"> 
    <g class="clusters"></g> 
    <g class="edgePaths"> 
     <g class="edgePath LS-A LE-B" id="L-A-B" style="opacity: 1;"> 
      <path class="path" d="M108,31L112.16666666666667,31C116.33333333333333,31,124.66666666666667,31,133,31C141.33333333333334,31,149.66666666666666,31,153.83333333333334,31L158,31" marker-end="url(#arrowhead9715)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9715" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
     <g class="edgePath LS-B LE-C" style="opacity: 1;" id="L-B-C"> 
      <path class="path" d="M258,31L262.1666666666667,31C266.3333333333333,31,274.6666666666667,31,283,31C291.3333333333333,31,299.6666666666667,31,303.8333333333333,31L308,31" marker-end="url(#arrowhead9716)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9716" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
     <g class="edgePath LS-C LE-D" style="opacity: 1;" id="L-C-D"> 
      <path class="path" d="M440,31L444.1666666666667,31C448.3333333333333,31,456.6666666666667,31,465,31C473.3333333333333,31,481.6666666666667,31,485.8333333333333,31L490,31" marker-end="url(#arrowhead9717)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9717" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
     <g class="edgePath LS-D LE-E" style="opacity: 1;" id="L-D-E"> 
      <path class="path" d="M574,31L578.1666666666666,31C582.3333333333334,31,590.6666666666666,31,599,31C607.3333333333334,31,615.6666666666666,31,619.8333333333334,31L624,31" marker-end="url(#arrowhead9718)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9718" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
    </g> 
    <g class="edgeLabels"> 
     <g class="edgeLabel" transform="" style="opacity: 1;"> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-A-B" class="edgeLabel L-LS-A' L-LE-B"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
     <g class="edgeLabel" style="opacity: 1;" transform=""> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-B-C" class="edgeLabel L-LS-B' L-LE-C"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
     <g class="edgeLabel" style="opacity: 1;" transform=""> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-C-D" class="edgeLabel L-LS-C' L-LE-D"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
     <g class="edgeLabel" style="opacity: 1;" transform=""> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-D-E" class="edgeLabel L-LS-D' L-LE-E"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
    </g> 
    <g class="nodes"> 
     <g class="node default" id="flowchart-A-4351" transform="translate(58,31)" style="opacity: 1;"> 
      <rect rx="0" ry="0" x="-50" y="-23" width="100" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-40,-13)"> 
        <foreignobject width="80" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           生成数据集 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-B-4352" transform="translate(208,31)"> 
      <rect rx="0" ry="0" x="-50" y="-23" width="100" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-40,-13)"> 
        <foreignobject width="80" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           读取数据集 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-C-4353" transform="translate(374,31)"> 
      <rect rx="0" ry="0" x="-66" y="-23" width="132" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-56,-13)"> 
        <foreignobject width="112" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           初始化模型参数 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-D-4354" transform="translate(532,31)"> 
      <rect rx="0" ry="0" x="-42" y="-23" width="84" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-32,-13)"> 
        <foreignobject width="64" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           定义模型 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-E-4355" transform="translate(650,31)"> 
      <rect rx="0" ry="0" x="-26" y="-23" width="52" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-16,-13)"> 
        <foreignobject width="32" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           训练 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
    </g> 
   </g> 
  </g> 
 </svg> 
</div> 
<p>  <strong>其中，定义模型包括定义损失函数和定义优化算法</strong></p> 
<h4><a id="_141"></a>线性回归的简洁实现</h4> 
<p>  本节代码文件在源代码文件的chapter_linear-networks/linear-regression-concise.ipynb中<br>   <strong>简洁实现是指通过使用深度学习框架来实现线性回归模型，具体流程与从零开始实现大体相同，不过一些常用函数不需要我们自己写了（直接导库，用别人写好的）</strong></p> 
<ul><li><strong>实现流程</strong></li></ul> 
<div class="mermaid sequence-diagram"> 
 <svg id="mermaid-svg-h3lhWMbi3bwrdYqr" width="100%" xmlns="http://www.w3.org/2000/svg" height="62" style="max-width: 684px;" viewbox="0 0 684 62"> 
  <style>#mermaid-svg-h3lhWMbi3bwrdYqr {font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;fill:#333;}#mermaid-svg-h3lhWMbi3bwrdYqr .error-icon{fill:#552222;}#mermaid-svg-h3lhWMbi3bwrdYqr .error-text{fill:#552222;stroke:#552222;}#mermaid-svg-h3lhWMbi3bwrdYqr .edge-thickness-normal{stroke-width:2px;}#mermaid-svg-h3lhWMbi3bwrdYqr .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg-h3lhWMbi3bwrdYqr .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg-h3lhWMbi3bwrdYqr .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg-h3lhWMbi3bwrdYqr .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg-h3lhWMbi3bwrdYqr .marker{fill:#333333;stroke:#333333;}#mermaid-svg-h3lhWMbi3bwrdYqr .marker.cross{stroke:#333333;}#mermaid-svg-h3lhWMbi3bwrdYqr svg{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg-h3lhWMbi3bwrdYqr .label{font-family:"trebuchet ms",verdana,arial,sans-serif;color:#333;}#mermaid-svg-h3lhWMbi3bwrdYqr .cluster-label text{fill:#333;}#mermaid-svg-h3lhWMbi3bwrdYqr .cluster-label span{color:#333;}#mermaid-svg-h3lhWMbi3bwrdYqr .label text,#mermaid-svg-h3lhWMbi3bwrdYqr span{fill:#333;color:#333;}#mermaid-svg-h3lhWMbi3bwrdYqr .node rect,#mermaid-svg-h3lhWMbi3bwrdYqr .node circle,#mermaid-svg-h3lhWMbi3bwrdYqr .node ellipse,#mermaid-svg-h3lhWMbi3bwrdYqr .node polygon,#mermaid-svg-h3lhWMbi3bwrdYqr .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#mermaid-svg-h3lhWMbi3bwrdYqr .node .label{text-align:center;}#mermaid-svg-h3lhWMbi3bwrdYqr .node.clickable{cursor:pointer;}#mermaid-svg-h3lhWMbi3bwrdYqr .arrowheadPath{fill:#333333;}#mermaid-svg-h3lhWMbi3bwrdYqr .edgePath .path{stroke:#333333;stroke-width:2.0px;}#mermaid-svg-h3lhWMbi3bwrdYqr .flowchart-link{stroke:#333333;fill:none;}#mermaid-svg-h3lhWMbi3bwrdYqr .edgeLabel{background-color:#e8e8e8;text-align:center;}#mermaid-svg-h3lhWMbi3bwrdYqr .edgeLabel rect{opacity:0.5;background-color:#e8e8e8;fill:#e8e8e8;}#mermaid-svg-h3lhWMbi3bwrdYqr .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#mermaid-svg-h3lhWMbi3bwrdYqr .cluster text{fill:#333;}#mermaid-svg-h3lhWMbi3bwrdYqr .cluster span{color:#333;}#mermaid-svg-h3lhWMbi3bwrdYqr div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-svg-h3lhWMbi3bwrdYqr :root{--mermaid-font-family:"trebuchet ms",verdana,arial,sans-serif;}</style> 
  <g> 
   <g class="output"> 
    <g class="clusters"></g> 
    <g class="edgePaths"> 
     <g class="edgePath LS-A LE-B" id="L-A-B" style="opacity: 1;"> 
      <path class="path" d="M108,31L112.16666666666667,31C116.33333333333333,31,124.66666666666667,31,133,31C141.33333333333334,31,149.66666666666666,31,153.83333333333334,31L158,31" marker-end="url(#arrowhead9740)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9740" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
     <g class="edgePath LS-B LE-D" style="opacity: 1;" id="L-B-D"> 
      <path class="path" d="M258,31L262.1666666666667,31C266.3333333333333,31,274.6666666666667,31,283,31C291.3333333333333,31,299.6666666666667,31,303.8333333333333,31L308,31" marker-end="url(#arrowhead9741)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9741" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
     <g class="edgePath LS-D LE-C" style="opacity: 1;" id="L-D-C"> 
      <path class="path" d="M392,31L396.1666666666667,31C400.3333333333333,31,408.6666666666667,31,417,31C425.3333333333333,31,433.6666666666667,31,437.8333333333333,31L442,31" marker-end="url(#arrowhead9742)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9742" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
     <g class="edgePath LS-C LE-E" style="opacity: 1;" id="L-C-E"> 
      <path class="path" d="M574,31L578.1666666666666,31C582.3333333333334,31,590.6666666666666,31,599,31C607.3333333333334,31,615.6666666666666,31,619.8333333333334,31L624,31" marker-end="url(#arrowhead9743)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9743" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
    </g> 
    <g class="edgeLabels"> 
     <g class="edgeLabel" transform="" style="opacity: 1;"> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-A-B" class="edgeLabel L-LS-A' L-LE-B"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
     <g class="edgeLabel" style="opacity: 1;" transform=""> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-B-D" class="edgeLabel L-LS-B' L-LE-D"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
     <g class="edgeLabel" style="opacity: 1;" transform=""> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-D-C" class="edgeLabel L-LS-D' L-LE-C"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
     <g class="edgeLabel" style="opacity: 1;" transform=""> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-C-E" class="edgeLabel L-LS-C' L-LE-E"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
    </g> 
    <g class="nodes"> 
     <g class="node default" id="flowchart-A-4361" transform="translate(58,31)" style="opacity: 1;"> 
      <rect rx="0" ry="0" x="-50" y="-23" width="100" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-40,-13)"> 
        <foreignobject width="80" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           生成数据集 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-B-4362" transform="translate(208,31)"> 
      <rect rx="0" ry="0" x="-50" y="-23" width="100" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-40,-13)"> 
        <foreignobject width="80" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           读取数据集 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-D-4363" transform="translate(350,31)"> 
      <rect rx="0" ry="0" x="-42" y="-23" width="84" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-32,-13)"> 
        <foreignobject width="64" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           定义模型 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-C-4364" transform="translate(508,31)"> 
      <rect rx="0" ry="0" x="-66" y="-23" width="132" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-56,-13)"> 
        <foreignobject width="112" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           初始化模型参数 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-E-4365" transform="translate(650,31)"> 
      <rect rx="0" ry="0" x="-26" y="-23" width="52" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-16,-13)"> 
        <foreignobject width="32" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           训练 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
    </g> 
   </g> 
  </g> 
 </svg> 
</div> 
<h4><a id="Softmax_149"></a>Softmax回归</h4> 
<p>  本节代码文件在源代码文件的chapter_linear-networks/softmax-regression.ipynb中</p> 
<ul><li><strong>回归vs分类（从回归到多类分类）</strong><br>   回归估计一个连续值；分类预测一个离散类别。<br>      <img src="https://images2.imgbox.com/06/d4/8dnxF0uM_o.png" alt="从回归到多类分类" width="400"></li><li><strong>从回归到多类分类 — 均方损失</strong><br>      <img src="https://images2.imgbox.com/68/0b/mkpu1HQH_o.png" alt="均方损失" width="400"></li><li><strong>从回归到多类分类 — 无校验比例</strong><br>      <img src="https://images2.imgbox.com/21/47/JxGS6Bau_o.png" alt="无校验比例" width="400"></li><li><strong>从回归到多类分类 — 校验比例</strong><br>     <img src="https://images2.imgbox.com/80/e5/P8bQbAsA_o.png" alt="校验比例" width="400"></li><li><strong>Softmax和交叉熵损失</strong><br>     <img src="https://images2.imgbox.com/e3/8b/ywWGFL0u_o.png" alt="Softmax和交叉熵损失" width="400"></li><li><strong>总结</strong><br>     <img src="https://images2.imgbox.com/d0/6e/c6KeZfZ5_o.png" alt="总结" width="300"></li></ul> 
<h4><a id="_164"></a>损失函数</h4> 
<p>  本节代码文件在源代码文件的chapter_linear-networks/softmax-regression.ipynb中<br> 损失函数用来衡量预测值与真实值之间的区别，是机器学习里非常重要的概念。下面介绍三种常用的损失函数。</p> 
<ul><li><strong>①L2 Loss</strong><br>      <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          l 
         
        
          ( 
         
        
          y 
         
        
          , 
         
         
         
           y 
          
         
           ′ 
          
         
        
          ) 
         
        
          = 
         
        
          ∣ 
         
        
          y 
         
        
          − 
         
         
         
           y 
          
         
           ′ 
          
         
        
          ∣ 
         
        
       
         \it l(y,y') = \mid y-y' \mid 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0019em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathit">l</span><span class="mopen">(</span><span class="mord mathit">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathit">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7519em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=∣</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord mathit">y</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord"><span class="mord mathit">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7519em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">∣</span></span></span></span></span></span><br>         <img src="https://images2.imgbox.com/cd/87/3ypTdl3p_o.png" alt="L2 Loss" width="400"><br> 蓝色曲线：表示当y=0时，变换预测值y’。<br> 绿色曲线：表示似然函数。<br> 橙色曲线：表示损失函数的梯度，可以发现，当y与y’相差较大的时候，梯度的绝对值也较大。</li><li><strong>②L1 Loss</strong><br>      <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          l 
         
        
          ( 
         
        
          y 
         
        
          , 
         
         
         
           y 
          
         
           ′ 
          
         
        
          ) 
         
        
          = 
         
         
         
           1 
          
         
           2 
          
         
        
          ( 
         
        
          y 
         
        
          − 
         
         
         
           y 
          
         
           ′ 
          
         
         
         
           ) 
          
         
           2 
          
         
        
       
         \it l(y,y') = \frac{1}{2} ( y-y')^2 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.1901em; vertical-align: -0.345em;"></span><span class="mord"><span class="mord mathit">l</span><span class="mopen">(</span><span class="mord mathit">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathit">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7519em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8451em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">2</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mopen">(</span><span class="mord mathit">y</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord"><span class="mord mathit">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7519em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span><br>         <img src="https://images2.imgbox.com/0a/31/GwbYwxzT_o.png" alt="L1 Loss" width="400"><br> 蓝色曲线：表示当y=0时，变换预测值y’。<br> 绿色曲线：表示似然函数。<br> 橙色曲线：表示损失函数的梯度，可以发现，当y’&gt;0时，导数为1，当y’&lt;0时，导数为-1。</li><li><strong>③Huber’s Robust Loss（鲁棒损失）</strong><br>         <img src="https://images2.imgbox.com/28/8b/0HOvTWYW_o.png" alt="Huber's  Robust   Loss（鲁棒损失）" width="400"><br> 蓝色曲线：表示当y=0时，变换预测值y’。<br> 绿色曲线：表示似然函数。<br> 橙色曲线：表示损失函数的梯度。</li></ul> 
<h4><a id="_184"></a>图像分类数据集</h4> 
<p>  本节代码文件在源代码文件的chapter_linear-networks/image-classification-dataset.ipynb中</p> 
<ul><li><strong>MNIST数据集</strong><br>   是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。 我们将使用类似但更复杂的Fashion-MNIST数据集。</li><li><strong>Fashion-MNIST数据集</strong><br>   <strong>①</strong>Fashion-MNIST由10个类别的图像组成，分别为t-shirt（T恤）、trouser（裤子）、pullover（套衫）、dress（连衣裙）、coat（外套）、sandal（凉鞋）、shirt（衬衫）、sneaker（运动鞋）、bag（包）和ankle boot（短靴）。<br>   <strong>②</strong> 每个类别由训练数据集（train dataset）中的6000张图像 和测试数据集（test dataset）中的1000张图像组成。<br>   <strong>③</strong> 每个输入图像的高度和宽度均为28像素。 数据集由灰度图像组成，其通道数为1。</li></ul> 
<h4><a id="Softmax_193"></a>Softmax回归的从零开始实现</h4> 
<p>  本节代码文件在源代码文件的chapter_linear-networks/softmax-regression-scratch.ipynb中</p> 
<ul><li><strong>实现流程</strong><br>   我们使用Fashion-MNIST数据集， 并设置数据迭代器的批量大小为256。每个样本都将用固定长度的向量表示。 原始数据集中的每个样本都是 28×28 的图像。 在本节中，我们将展平每个图像，把它们看作<strong>长度为784的向量</strong>。 在后面的章节中，我们将讨论能够利用图像空间结构的特征， 但现在我们暂时只把每个像素位置看作一个特征。</li></ul> 
<div class="mermaid sequence-diagram"> 
 <svg id="mermaid-svg-NYZaiygjZL01dkUa" width="100%" xmlns="http://www.w3.org/2000/svg" height="62" style="max-width: 977.0277709960938px;" viewbox="0 0 977.0277709960938 62"> 
  <style>#mermaid-svg-NYZaiygjZL01dkUa {font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;fill:#333;}#mermaid-svg-NYZaiygjZL01dkUa .error-icon{fill:#552222;}#mermaid-svg-NYZaiygjZL01dkUa .error-text{fill:#552222;stroke:#552222;}#mermaid-svg-NYZaiygjZL01dkUa .edge-thickness-normal{stroke-width:2px;}#mermaid-svg-NYZaiygjZL01dkUa .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg-NYZaiygjZL01dkUa .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg-NYZaiygjZL01dkUa .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg-NYZaiygjZL01dkUa .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg-NYZaiygjZL01dkUa .marker{fill:#333333;stroke:#333333;}#mermaid-svg-NYZaiygjZL01dkUa .marker.cross{stroke:#333333;}#mermaid-svg-NYZaiygjZL01dkUa svg{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg-NYZaiygjZL01dkUa .label{font-family:"trebuchet ms",verdana,arial,sans-serif;color:#333;}#mermaid-svg-NYZaiygjZL01dkUa .cluster-label text{fill:#333;}#mermaid-svg-NYZaiygjZL01dkUa .cluster-label span{color:#333;}#mermaid-svg-NYZaiygjZL01dkUa .label text,#mermaid-svg-NYZaiygjZL01dkUa span{fill:#333;color:#333;}#mermaid-svg-NYZaiygjZL01dkUa .node rect,#mermaid-svg-NYZaiygjZL01dkUa .node circle,#mermaid-svg-NYZaiygjZL01dkUa .node ellipse,#mermaid-svg-NYZaiygjZL01dkUa .node polygon,#mermaid-svg-NYZaiygjZL01dkUa .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#mermaid-svg-NYZaiygjZL01dkUa .node .label{text-align:center;}#mermaid-svg-NYZaiygjZL01dkUa .node.clickable{cursor:pointer;}#mermaid-svg-NYZaiygjZL01dkUa .arrowheadPath{fill:#333333;}#mermaid-svg-NYZaiygjZL01dkUa .edgePath .path{stroke:#333333;stroke-width:2.0px;}#mermaid-svg-NYZaiygjZL01dkUa .flowchart-link{stroke:#333333;fill:none;}#mermaid-svg-NYZaiygjZL01dkUa .edgeLabel{background-color:#e8e8e8;text-align:center;}#mermaid-svg-NYZaiygjZL01dkUa .edgeLabel rect{opacity:0.5;background-color:#e8e8e8;fill:#e8e8e8;}#mermaid-svg-NYZaiygjZL01dkUa .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#mermaid-svg-NYZaiygjZL01dkUa .cluster text{fill:#333;}#mermaid-svg-NYZaiygjZL01dkUa .cluster span{color:#333;}#mermaid-svg-NYZaiygjZL01dkUa div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-svg-NYZaiygjZL01dkUa :root{--mermaid-font-family:"trebuchet ms",verdana,arial,sans-serif;}</style> 
  <g> 
   <g class="output"> 
    <g class="clusters"></g> 
    <g class="edgePaths"> 
     <g class="edgePath LS-A LE-B" id="L-A-B" style="opacity: 1;"> 
      <path class="path" d="M140,31L144.16666666666666,31C148.33333333333334,31,156.66666666666666,31,165,31C173.33333333333334,31,181.66666666666666,31,185.83333333333334,31L190,31" marker-end="url(#arrowhead9775)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9775" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
     <g class="edgePath LS-B LE-C" style="opacity: 1;" id="L-B-C"> 
      <path class="path" d="M331.02777099609375,31L335.19443766276044,31C339.36110432942706,31,347.69443766276044,31,356.02777099609375,31C364.36110432942706,31,372.69443766276044,31,376.86110432942706,31L381.02777099609375,31" marker-end="url(#arrowhead9776)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9776" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
     <g class="edgePath LS-C LE-D" style="opacity: 1;" id="L-C-D"> 
      <path class="path" d="M465.02777099609375,31L469.19443766276044,31C473.36110432942706,31,481.69443766276044,31,490.02777099609375,31C498.36110432942706,31,506.69443766276044,31,510.86110432942706,31L515.0277709960938,31" marker-end="url(#arrowhead9777)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9777" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
     <g class="edgePath LS-D LE-E" style="opacity: 1;" id="L-D-E"> 
      <path class="path" d="M631.0277709960938,31L635.1944376627604,31C639.3611043294271,31,647.6944376627604,31,656.0277709960938,31C664.3611043294271,31,672.6944376627604,31,676.8611043294271,31L681.0277709960938,31" marker-end="url(#arrowhead9778)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9778" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
     <g class="edgePath LS-E LE-F" style="opacity: 1;" id="L-E-F"> 
      <path class="path" d="M765.0277709960938,31L769.1944376627604,31C773.3611043294271,31,781.6944376627604,31,790.0277709960938,31C798.3611043294271,31,806.6944376627604,31,810.8611043294271,31L815.0277709960938,31" marker-end="url(#arrowhead9779)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9779" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
     <g class="edgePath LS-F LE-G" style="opacity: 1;" id="L-F-G"> 
      <path class="path" d="M867.0277709960938,31L871.1944376627604,31C875.3611043294271,31,883.6944376627604,31,892.0277709960938,31C900.3611043294271,31,908.6944376627604,31,912.8611043294271,31L917.0277709960938,31" marker-end="url(#arrowhead9780)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9780" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
    </g> 
    <g class="edgeLabels"> 
     <g class="edgeLabel" transform="" style="opacity: 1;"> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-A-B" class="edgeLabel L-LS-A' L-LE-B"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
     <g class="edgeLabel" style="opacity: 1;" transform=""> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-B-C" class="edgeLabel L-LS-B' L-LE-C"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
     <g class="edgeLabel" style="opacity: 1;" transform=""> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-C-D" class="edgeLabel L-LS-C' L-LE-D"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
     <g class="edgeLabel" style="opacity: 1;" transform=""> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-D-E" class="edgeLabel L-LS-D' L-LE-E"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
     <g class="edgeLabel" style="opacity: 1;" transform=""> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-E-F" class="edgeLabel L-LS-E' L-LE-F"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
     <g class="edgeLabel" style="opacity: 1;" transform=""> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-F-G" class="edgeLabel L-LS-F' L-LE-G"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
    </g> 
    <g class="nodes"> 
     <g class="node default" id="flowchart-A-4373" transform="translate(74,31)" style="opacity: 1;"> 
      <rect rx="0" ry="0" x="-66" y="-23" width="132" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-56,-13)"> 
        <foreignobject width="112" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           初始化模型参数 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-B-4374" transform="translate(260.5138854980469,31)"> 
      <rect rx="0" ry="0" x="-70.51388931274414" y="-23" width="141.02777862548828" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-60.51388931274414,-13)"> 
        <foreignobject width="121.02777862548828" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           定义softmax操作 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-C-4375" transform="translate(423.02777099609375,31)"> 
      <rect rx="0" ry="0" x="-42" y="-23" width="84" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-32,-13)"> 
        <foreignobject width="64" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           定义模型 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-D-4376" transform="translate(573.0277709960938,31)"> 
      <rect rx="0" ry="0" x="-58" y="-23" width="116" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-48,-13)"> 
        <foreignobject width="96" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           定义损失函数 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-E-4377" transform="translate(723.0277709960938,31)"> 
      <rect rx="0" ry="0" x="-42" y="-23" width="84" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-32,-13)"> 
        <foreignobject width="64" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           分类精度 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-F-4378" transform="translate(841.0277709960938,31)"> 
      <rect rx="0" ry="0" x="-26" y="-23" width="52" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-16,-13)"> 
        <foreignobject width="32" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           训练 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-G-4379" transform="translate(943.0277709960938,31)"> 
      <rect rx="0" ry="0" x="-26" y="-23" width="52" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-16,-13)"> 
        <foreignobject width="32" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           预测 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
    </g> 
   </g> 
  </g> 
 </svg> 
</div> 
<p>    <strong>分类精度即正确预测数量与总预测数量之比</strong></p> 
<h4><a id="Softmax_203"></a>Softmax回归的简洁实现</h4> 
<p>  本节代码文件在源代码文件的chapter_linear-networks/softmax-regression-concise.ipynb中<br>   <strong>通过深度学习框架的高级API也能更方便地实现softmax回归模型。</strong></p> 
<ul><li><strong>实现流程</strong><br>   本节 继续使用Fashion-MNIST数据集，并保持批量大小为256。</li></ul> 
<div class="mermaid sequence-diagram"> 
 <svg id="mermaid-svg-sWOeJBEf0xgofPrh" width="100%" xmlns="http://www.w3.org/2000/svg" height="62" style="max-width: 652px;" viewbox="0 0 652 62"> 
  <style>#mermaid-svg-sWOeJBEf0xgofPrh {font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;fill:#333;}#mermaid-svg-sWOeJBEf0xgofPrh .error-icon{fill:#552222;}#mermaid-svg-sWOeJBEf0xgofPrh .error-text{fill:#552222;stroke:#552222;}#mermaid-svg-sWOeJBEf0xgofPrh .edge-thickness-normal{stroke-width:2px;}#mermaid-svg-sWOeJBEf0xgofPrh .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg-sWOeJBEf0xgofPrh .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg-sWOeJBEf0xgofPrh .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg-sWOeJBEf0xgofPrh .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg-sWOeJBEf0xgofPrh .marker{fill:#333333;stroke:#333333;}#mermaid-svg-sWOeJBEf0xgofPrh .marker.cross{stroke:#333333;}#mermaid-svg-sWOeJBEf0xgofPrh svg{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg-sWOeJBEf0xgofPrh .label{font-family:"trebuchet ms",verdana,arial,sans-serif;color:#333;}#mermaid-svg-sWOeJBEf0xgofPrh .cluster-label text{fill:#333;}#mermaid-svg-sWOeJBEf0xgofPrh .cluster-label span{color:#333;}#mermaid-svg-sWOeJBEf0xgofPrh .label text,#mermaid-svg-sWOeJBEf0xgofPrh span{fill:#333;color:#333;}#mermaid-svg-sWOeJBEf0xgofPrh .node rect,#mermaid-svg-sWOeJBEf0xgofPrh .node circle,#mermaid-svg-sWOeJBEf0xgofPrh .node ellipse,#mermaid-svg-sWOeJBEf0xgofPrh .node polygon,#mermaid-svg-sWOeJBEf0xgofPrh .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#mermaid-svg-sWOeJBEf0xgofPrh .node .label{text-align:center;}#mermaid-svg-sWOeJBEf0xgofPrh .node.clickable{cursor:pointer;}#mermaid-svg-sWOeJBEf0xgofPrh .arrowheadPath{fill:#333333;}#mermaid-svg-sWOeJBEf0xgofPrh .edgePath .path{stroke:#333333;stroke-width:2.0px;}#mermaid-svg-sWOeJBEf0xgofPrh .flowchart-link{stroke:#333333;fill:none;}#mermaid-svg-sWOeJBEf0xgofPrh .edgeLabel{background-color:#e8e8e8;text-align:center;}#mermaid-svg-sWOeJBEf0xgofPrh .edgeLabel rect{opacity:0.5;background-color:#e8e8e8;fill:#e8e8e8;}#mermaid-svg-sWOeJBEf0xgofPrh .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#mermaid-svg-sWOeJBEf0xgofPrh .cluster text{fill:#333;}#mermaid-svg-sWOeJBEf0xgofPrh .cluster span{color:#333;}#mermaid-svg-sWOeJBEf0xgofPrh div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-svg-sWOeJBEf0xgofPrh :root{--mermaid-font-family:"trebuchet ms",verdana,arial,sans-serif;}</style> 
  <g> 
   <g class="output"> 
    <g class="clusters"></g> 
    <g class="edgePaths"> 
     <g class="edgePath LS-A LE-D" id="L-A-D" style="opacity: 1;"> 
      <path class="path" d="M140,31L144.16666666666666,31C148.33333333333334,31,156.66666666666666,31,165,31C173.33333333333334,31,181.66666666666666,31,185.83333333333334,31L190,31" marker-end="url(#arrowhead9802)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9802" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
     <g class="edgePath LS-D LE-E" style="opacity: 1;" id="L-D-E"> 
      <path class="path" d="M306,31L310.1666666666667,31C314.3333333333333,31,322.6666666666667,31,331,31C339.3333333333333,31,347.6666666666667,31,351.8333333333333,31L356,31" marker-end="url(#arrowhead9803)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9803" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
     <g class="edgePath LS-E LE-F" style="opacity: 1;" id="L-E-F"> 
      <path class="path" d="M440,31L444.1666666666667,31C448.3333333333333,31,456.6666666666667,31,465,31C473.3333333333333,31,481.6666666666667,31,485.8333333333333,31L490,31" marker-end="url(#arrowhead9804)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9804" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
     <g class="edgePath LS-F LE-G" style="opacity: 1;" id="L-F-G"> 
      <path class="path" d="M542,31L546.1666666666666,31C550.3333333333334,31,558.6666666666666,31,567,31C575.3333333333334,31,583.6666666666666,31,587.8333333333334,31L592,31" marker-end="url(#arrowhead9805)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9805" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
    </g> 
    <g class="edgeLabels"> 
     <g class="edgeLabel" transform="" style="opacity: 1;"> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-A-D" class="edgeLabel L-LS-A' L-LE-D"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
     <g class="edgeLabel" style="opacity: 1;" transform=""> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-D-E" class="edgeLabel L-LS-D' L-LE-E"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
     <g class="edgeLabel" style="opacity: 1;" transform=""> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-E-F" class="edgeLabel L-LS-E' L-LE-F"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
     <g class="edgeLabel" style="opacity: 1;" transform=""> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-F-G" class="edgeLabel L-LS-F' L-LE-G"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
    </g> 
    <g class="nodes"> 
     <g class="node default" id="flowchart-A-4385" transform="translate(74,31)" style="opacity: 1;"> 
      <rect rx="0" ry="0" x="-66" y="-23" width="132" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-56,-13)"> 
        <foreignobject width="112" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           初始化模型参数 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-D-4386" transform="translate(248,31)"> 
      <rect rx="0" ry="0" x="-58" y="-23" width="116" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-48,-13)"> 
        <foreignobject width="96" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           定义损失函数 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-E-4387" transform="translate(398,31)"> 
      <rect rx="0" ry="0" x="-42" y="-23" width="84" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-32,-13)"> 
        <foreignobject width="64" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           优化算法 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-F-4388" transform="translate(516,31)"> 
      <rect rx="0" ry="0" x="-26" y="-23" width="52" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-16,-13)"> 
        <foreignobject width="32" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           训练 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-G-4389" transform="translate(618,31)"> 
      <rect rx="0" ry="0" x="-26" y="-23" width="52" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-16,-13)"> 
        <foreignobject width="32" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           预测 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
    </g> 
   </g> 
  </g> 
 </svg> 
</div> 
<h3><a id="_212"></a>多层感知机</h3> 
<h4><a id="_213"></a>感知机</h4> 
<ul><li><strong>感知机</strong><br>   <img src="https://images2.imgbox.com/e9/14/mkLOigHo_o.png" alt="感知机" width="400"></li><li><strong>感知机与回归和Softmax回归的区别</strong><br>   感知机是二分类(1或-1)，而回归的输出是实数，Softmax回归的输出是概率。</li><li><strong>训练感知机</strong><br>     <img src="https://images2.imgbox.com/5c/3b/C6HPrrRf_o.png" alt="训练感知机" width="350"></li><li><strong>收敛定理</strong><br>     <img src="https://images2.imgbox.com/7c/a5/2iBzlMlH_o.png" alt="收敛定理" width="500"></li><li><strong>XOR问题</strong><br>    <img src="https://images2.imgbox.com/25/fb/c5GB5kgU_o.png" alt="XOR问题" width="400"></li><li><strong>总结</strong><br>   <img src="https://images2.imgbox.com/17/16/idKXZi7S_o.png" alt="在这里插入图片描述" width="400"></li></ul> 
<h4><a id="_226"></a>多层感知机</h4> 
<p>  本节代码文件在源代码文件的chapter_multilayer-perceptrons/mlp.ipynb中</p> 
<ul><li> <p><strong>学习XOR</strong><br>     <img src="https://images2.imgbox.com/72/bb/JyC322hv_o.png" alt="学习XOR" width="400"></p> </li><li> <p><strong>单隐藏层</strong><br>   <mark><strong>隐藏层大小是超参数</strong></mark>（输入和输出大小由数据决定，输出大小人为决定。）<br>     <img src="https://images2.imgbox.com/b9/e5/MASe59kI_o.png" alt="单隐藏层" width="400"></p> </li><li> <p><strong>单隐藏层 — 单分类</strong><br>   只有一个输出，即输出是标量。<br>     <img src="https://images2.imgbox.com/52/30/temkgblL_o.png" alt="单隐藏层 --- 单分类" width="200"><br> <mark><strong>Q：为什么需要非线性的激活函数？</strong></mark>（σ(x)不可以等于x，也不可以等于nx）<br> <strong>A:</strong> 如果激活函数是线性的，那么单隐藏层的多层感知机就变成了最简单的线性模型。<br>    <img src="https://images2.imgbox.com/be/ba/EAkJvx40_o.png" alt="在这里插入图片描述" width="300"></p> </li><li> <p><strong>激活函数</strong><br>   <strong>①Sigmoid 激活函数</strong><br>     <img src="https://images2.imgbox.com/4b/81/iZMwMPi5_o.png" alt="Sigmoid 激活函数" width="500"><br>   <strong>②Tanh 激活函数</strong><br>    <img src="https://images2.imgbox.com/85/9b/j7z9dMsU_o.png" alt="Tanh 激活函数" width="500"><br>   <strong>③ReLU 激活函数</strong><br>     <img src="https://images2.imgbox.com/3e/cc/5WsezPig_o.png" alt="ReLU 激活函数" width="500"></p> </li><li> <p><strong>单隐藏层 — 多类分类</strong><br>     <img src="https://images2.imgbox.com/fa/cf/pMCRBo4z_o.png" alt="单隐藏层 --- 多类分类"></p> </li><li> <p><strong>多隐藏层</strong><br>     <img src="https://images2.imgbox.com/4b/b2/UWl3hGsX_o.png" alt="多隐藏层"></p> </li><li> <p><strong>总结</strong><br>    <img src="https://images2.imgbox.com/d5/41/Arvu8JAN_o.png" alt="在这里插入图片描述" width="450"></p> </li></ul> 
<h4><a id="_253"></a>多层感知机的从零开始实现</h4> 
<p>  本节代码文件在源代码文件的chapter_multilayer-perceptrons/mlp-scratch.ipynb中</p> 
<ul><li><strong>实现流程</strong><br>   我们实现一个具有单隐藏层的多层感知机， 它包含256个隐藏单元。 注意，我们可以将这两个变量都视为超参数。 通常，我们选择2的若干次幂作为层的宽度。 因为内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。</li></ul> 
<div class="mermaid sequence-diagram"> 
 <svg id="mermaid-svg-ABtkPCP2RUM1MgnQ" width="100%" xmlns="http://www.w3.org/2000/svg" height="62" style="max-width: 620px;" viewbox="0 0 620 62"> 
  <style>#mermaid-svg-ABtkPCP2RUM1MgnQ {font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;fill:#333;}#mermaid-svg-ABtkPCP2RUM1MgnQ .error-icon{fill:#552222;}#mermaid-svg-ABtkPCP2RUM1MgnQ .error-text{fill:#552222;stroke:#552222;}#mermaid-svg-ABtkPCP2RUM1MgnQ .edge-thickness-normal{stroke-width:2px;}#mermaid-svg-ABtkPCP2RUM1MgnQ .edge-thickness-thick{stroke-width:3.5px;}#mermaid-svg-ABtkPCP2RUM1MgnQ .edge-pattern-solid{stroke-dasharray:0;}#mermaid-svg-ABtkPCP2RUM1MgnQ .edge-pattern-dashed{stroke-dasharray:3;}#mermaid-svg-ABtkPCP2RUM1MgnQ .edge-pattern-dotted{stroke-dasharray:2;}#mermaid-svg-ABtkPCP2RUM1MgnQ .marker{fill:#333333;stroke:#333333;}#mermaid-svg-ABtkPCP2RUM1MgnQ .marker.cross{stroke:#333333;}#mermaid-svg-ABtkPCP2RUM1MgnQ svg{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;}#mermaid-svg-ABtkPCP2RUM1MgnQ .label{font-family:"trebuchet ms",verdana,arial,sans-serif;color:#333;}#mermaid-svg-ABtkPCP2RUM1MgnQ .cluster-label text{fill:#333;}#mermaid-svg-ABtkPCP2RUM1MgnQ .cluster-label span{color:#333;}#mermaid-svg-ABtkPCP2RUM1MgnQ .label text,#mermaid-svg-ABtkPCP2RUM1MgnQ span{fill:#333;color:#333;}#mermaid-svg-ABtkPCP2RUM1MgnQ .node rect,#mermaid-svg-ABtkPCP2RUM1MgnQ .node circle,#mermaid-svg-ABtkPCP2RUM1MgnQ .node ellipse,#mermaid-svg-ABtkPCP2RUM1MgnQ .node polygon,#mermaid-svg-ABtkPCP2RUM1MgnQ .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#mermaid-svg-ABtkPCP2RUM1MgnQ .node .label{text-align:center;}#mermaid-svg-ABtkPCP2RUM1MgnQ .node.clickable{cursor:pointer;}#mermaid-svg-ABtkPCP2RUM1MgnQ .arrowheadPath{fill:#333333;}#mermaid-svg-ABtkPCP2RUM1MgnQ .edgePath .path{stroke:#333333;stroke-width:2.0px;}#mermaid-svg-ABtkPCP2RUM1MgnQ .flowchart-link{stroke:#333333;fill:none;}#mermaid-svg-ABtkPCP2RUM1MgnQ .edgeLabel{background-color:#e8e8e8;text-align:center;}#mermaid-svg-ABtkPCP2RUM1MgnQ .edgeLabel rect{opacity:0.5;background-color:#e8e8e8;fill:#e8e8e8;}#mermaid-svg-ABtkPCP2RUM1MgnQ .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#mermaid-svg-ABtkPCP2RUM1MgnQ .cluster text{fill:#333;}#mermaid-svg-ABtkPCP2RUM1MgnQ .cluster span{color:#333;}#mermaid-svg-ABtkPCP2RUM1MgnQ div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#mermaid-svg-ABtkPCP2RUM1MgnQ :root{--mermaid-font-family:"trebuchet ms",verdana,arial,sans-serif;}</style> 
  <g> 
   <g class="output"> 
    <g class="clusters"></g> 
    <g class="edgePaths"> 
     <g class="edgePath LS-A LE-B" id="L-A-B" style="opacity: 1;"> 
      <path class="path" d="M140,31L144.16666666666666,31C148.33333333333334,31,156.66666666666666,31,165,31C173.33333333333334,31,181.66666666666666,31,185.83333333333334,31L190,31" marker-end="url(#arrowhead9827)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9827" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
     <g class="edgePath LS-B LE-C" style="opacity: 1;" id="L-B-C"> 
      <path class="path" d="M274,31L278.1666666666667,31C282.3333333333333,31,290.6666666666667,31,299,31C307.3333333333333,31,315.6666666666667,31,319.8333333333333,31L324,31" marker-end="url(#arrowhead9828)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9828" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
     <g class="edgePath LS-C LE-D" style="opacity: 1;" id="L-C-D"> 
      <path class="path" d="M376,31L380.1666666666667,31C384.3333333333333,31,392.6666666666667,31,401,31C409.3333333333333,31,417.6666666666667,31,421.8333333333333,31L426,31" marker-end="url(#arrowhead9829)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9829" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
     <g class="edgePath LS-D LE-E" style="opacity: 1;" id="L-D-E"> 
      <path class="path" d="M510,31L514.1666666666666,31C518.3333333333334,31,526.6666666666666,31,535,31C543.3333333333334,31,551.6666666666666,31,555.8333333333334,31L560,31" marker-end="url(#arrowhead9830)" style="fill:none"></path> 
      <defs> 
       <marker id="arrowhead9830" viewbox="0 0 10 10" refx="9" refy="5" markerunits="strokeWidth" markerwidth="8" markerheight="6" orient="auto"> 
        <path d="M 0 0 L 10 5 L 0 10 z" class="arrowheadPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"></path> 
       </marker> 
      </defs> 
     </g> 
    </g> 
    <g class="edgeLabels"> 
     <g class="edgeLabel" transform="" style="opacity: 1;"> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-A-B" class="edgeLabel L-LS-A' L-LE-B"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
     <g class="edgeLabel" style="opacity: 1;" transform=""> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-B-C" class="edgeLabel L-LS-B' L-LE-C"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
     <g class="edgeLabel" style="opacity: 1;" transform=""> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-C-D" class="edgeLabel L-LS-C' L-LE-D"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
     <g class="edgeLabel" style="opacity: 1;" transform=""> 
      <g transform="translate(0,0)" class="label"> 
       <rect rx="0" ry="0" width="0" height="0"></rect> 
       <foreignobject width="0" height="0"> 
        <div style="display: inline-block; white-space: nowrap;"> 
         <span id="L-L-D-E" class="edgeLabel L-LS-D' L-LE-E"></span> 
        </div> 
       </foreignobject> 
      </g> 
     </g> 
    </g> 
    <g class="nodes"> 
     <g class="node default" id="flowchart-A-4395" transform="translate(74,31)" style="opacity: 1;"> 
      <rect rx="0" ry="0" x="-66" y="-23" width="132" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-56,-13)"> 
        <foreignobject width="112" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           初始化模型参数 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-B-4396" transform="translate(232,31)"> 
      <rect rx="0" ry="0" x="-42" y="-23" width="84" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-32,-13)"> 
        <foreignobject width="64" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           激活函数 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-C-4397" transform="translate(350,31)"> 
      <rect rx="0" ry="0" x="-26" y="-23" width="52" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-16,-13)"> 
        <foreignobject width="32" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           模型 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-D-4398" transform="translate(468,31)"> 
      <rect rx="0" ry="0" x="-42" y="-23" width="84" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-32,-13)"> 
        <foreignobject width="64" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           损失函数 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
     <g class="node default" style="opacity: 1;" id="flowchart-E-4399" transform="translate(586,31)"> 
      <rect rx="0" ry="0" x="-26" y="-23" width="52" height="46" class="label-container"></rect> 
      <g class="label" transform="translate(0,0)"> 
       <g transform="translate(-16,-13)"> 
        <foreignobject width="32" height="26"> 
         <div style="display: inline-block; white-space: nowrap;">
           训练 
         </div> 
        </foreignobject> 
       </g> 
      </g> 
     </g> 
    </g> 
   </g> 
  </g> 
 </svg> 
</div> 
<h4><a id="_261"></a>多层感知机的简洁实现</h4> 
<p>  本节代码文件在源代码文件的chapter_multilayer-perceptrons/mlp-concise.ipynb中</p> 
<ul><li><strong>与softmax回归的简洁实现相比</strong><br>   唯一的区别是我们添加了2个全连接层(之前我们只添加了1个全连接层)。第一层是隐藏层，它包含256个隐藏单元，并使用了ReLU激活函数。第二层是输出层。<br>   训练过程的实现与我们实现softmax回归时完全相同。</li></ul> 
<h4><a id="_266"></a>模型选择</h4> 
<p>  本节代码文件在源代码文件的chapter_multilayer-perceptrons/underfit-overfit.ipynb中</p> 
<ul><li><strong>训练误差和泛化误差</strong><br>    <img src="https://images2.imgbox.com/39/cc/Rufw7pLy_o.png" alt="训练误差和泛化误差" width="450"></li><li><strong>验证数据集和测试数据集</strong><br>    <img src="https://images2.imgbox.com/ca/49/HkvrDTWm_o.png" alt="验证数据集和测试数据集" width="350"></li><li><strong>K-折交叉验证</strong><br>    <img src="https://images2.imgbox.com/da/84/TQ2u0fPH_o.png" alt="K-折交叉验证" width="380"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/15/ff/fs9KwzsX_o.png" alt="总结" width="300"></li></ul> 
<h4><a id="_276"></a>过拟合和欠拟合</h4> 
<p>  本节代码文件在源代码文件的chapter_multilayer-perceptrons/underfit-overfit.ipynb中</p> 
<ul><li><strong>过拟合和欠拟合</strong><br>    <img src="https://images2.imgbox.com/fa/8d/QFllJtRi_o.png" alt="过拟合和欠拟合" width="500"><br>    <img src="https://images2.imgbox.com/0e/41/Yf35lCrb_o.png" alt="过拟合和欠拟合" width="400"></li><li><strong>数据复杂度</strong><br>    <img src="https://images2.imgbox.com/59/e3/StSnYh71_o.png" alt="数据复杂度" width="600"></li><li><strong>模型容量</strong><br>    <img src="https://images2.imgbox.com/f7/0e/LBU7Jzad_o.png" alt="模型容量" width="280"></li><li><strong>模型容量的影响</strong><br>    <img src="https://images2.imgbox.com/e6/95/q5hevwLK_o.png" alt="模型容量的影响" width="350"></li><li><strong>估计模型容量</strong><br>    <img src="https://images2.imgbox.com/f3/93/ROXVDbHY_o.png" alt="估计模型容量" width="350"><br>     <mark><strong>其中，d+1中的1是偏移，m是隐藏层大小，k是分类的类别数</strong></mark></li><li><strong>VC 维</strong></li></ul> 
<p>    <img src="https://images2.imgbox.com/37/cf/a5naXM7q_o.png" alt="VC 维" width="450"></p> 
<ul><li><strong>线性分类器的 VC 维</strong><br>    <img src="https://images2.imgbox.com/a9/db/Q1wtJrAp_o.png" alt="线性分类器的 VC 维" width="500"></li><li><strong>VC 维的用处</strong><br>    <img src="https://images2.imgbox.com/aa/94/1ozMDiMq_o.png" alt="VC 维的用处" width="350"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/00/51/ejei7yt5_o.png" alt="总结" width="400"></li></ul> 
<h4><a id="_299"></a>权重衰退</h4> 
<p>  本节代码文件在源代码文件的chapter_multilayer-perceptrons/weight-decay.ipynb中<br>   <mark><strong>权重衰退是一种常见的处理过拟合（模型复杂度过高）的方法</strong></mark>。</p> 
<ul><li><strong>使用均方范数作为硬性限制</strong><br>    <img src="https://images2.imgbox.com/7f/4c/baRBOeZZ_o.png" alt="使用均方范数作为硬性限制" width="450"></li><li><strong>使用均方范数作为柔性限制</strong><br>    <img src="https://images2.imgbox.com/6c/7b/hY60w5SS_o.png" alt="使用均方范数作为柔性限制" width="450"></li><li><strong>演示对最优解的影响</strong><br>    <img src="https://images2.imgbox.com/10/d7/Av6raQMS_o.png" alt="演示对最优解的影响" width="400"></li><li><strong>参数更新法则</strong><br>    <img src="https://images2.imgbox.com/43/65/wcQPLEGU_o.png" alt="参数更新法则" width="350"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/3f/11/bBDrfA0o_o.png" alt="总结" width="450"></li></ul> 
<h4><a id="Dropout_312"></a>暂退法（Dropout）</h4> 
<p>  本节代码文件在源代码文件的chapter_multilayer-perceptrons/dropout.ipynb中</p> 
<ul><li><strong>暂退法(丢弃法)的动机</strong><br>    <img src="https://images2.imgbox.com/aa/69/REODuTub_o.png" alt="暂退法的动机" width="250"></li><li><strong>无偏差的加入噪音</strong><br>    <img src="https://images2.imgbox.com/67/ba/8JSdtcaB_o.png" alt="无偏差的加入噪音" width="450"></li><li><strong>使用丢弃法</strong><br>    <img src="https://images2.imgbox.com/31/d3/OxZrQAmU_o.png" alt="使用丢弃法" width="600"></li><li><strong>推理中的丢弃法</strong><br>    <img src="https://images2.imgbox.com/26/21/2f8W5foE_o.png" alt="推理中的丢弃法" width="400"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/28/0c/DVWX1J5u_o.png" alt="总结" width="380"></li></ul> 
<h4><a id="_324"></a>数值稳定性</h4> 
<p>  本节代码文件在源代码文件的chapter_multilayer-perceptrons/numerical-stability-and-init.ipynb中</p> 
<ul><li><strong>神经网络的梯度</strong><br>    <img src="https://images2.imgbox.com/37/c3/p2F26ySq_o.png" alt="神经网络的梯度" width="350"><br>    <mark><strong>其中，矩阵乘法容易带来梯度爆炸和梯度消失的问题</strong></mark></li><li><strong>数值稳定性的常见两个问题</strong><br>    <img src="https://images2.imgbox.com/c5/60/DEox1rHX_o.png" alt="数值稳定性的常见两个问题" width="350"><br>  <strong>例子：MLP</strong><br>    <img src="https://images2.imgbox.com/93/81/lcqycNKD_o.png" alt="MLP" width="400"><br>    <img src="https://images2.imgbox.com/f9/6f/LaZ0E4gk_o.png" alt="梯度爆炸" width="500"><br>       <mark><strong>↑ ↑ ↑ 此时，就会造成梯度爆炸</strong></mark><br>    <img src="https://images2.imgbox.com/19/2a/Ufdnq1S8_o.png" alt="梯度消失" width="350"><br>    <img src="https://images2.imgbox.com/8a/82/MNMAh8mo_o.png" alt="梯度消失" width="480"><br>       <mark><strong>↑ ↑ ↑ 此时，就会造成梯度消失</strong></mark></li><li><strong>梯度爆炸的问题</strong><br>    <img src="https://images2.imgbox.com/bb/1f/NzxtHZ1L_o.png" alt="梯度爆炸的问题" width="380"></li><li><strong>梯度消失的问题</strong><br>    <img src="https://images2.imgbox.com/e7/dc/EuKzg5r3_o.png" alt="梯度消失的问题" width="220"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/da/e1/Xa6QkM08_o.png" alt="总结" width="350"></li></ul> 
<h4><a id="_344"></a>模型初始化和激活函数</h4> 
<p>  本节代码文件在源代码文件的chapter_multilayer-perceptrons/numerical-stability-and-init.ipynb中</p> 
<ul><li><strong>让训练更加稳定</strong><br>    <img src="https://images2.imgbox.com/c7/a0/wQROJE8o_o.png" alt="让训练更加稳定" width="250"></li><li><strong>让每层的方差是个常数</strong><br>    <img src="https://images2.imgbox.com/f5/52/k3AGwIc3_o.png" alt="让每层的方差是个常数" width="350"></li><li><strong>权重初始化</strong><br>    <img src="https://images2.imgbox.com/94/80/RZcgDQRU_o.png" alt="权重初始化" width="450"><br> <strong>例子：MLP</strong><br>     <img src="https://images2.imgbox.com/d5/96/OPbtUAVy_o.png" alt="MLP" width="350"><br>   <strong>①假设没有激活函数：</strong><br>     <img src="https://images2.imgbox.com/aa/ad/cHS7KALf_o.png" alt="假设没有激活函数" width="450"><br>    <strong>正向方差：</strong><br>      <img src="https://images2.imgbox.com/22/45/TdgTdvzB_o.png" alt="正向方差" width="450"><br>    <strong>反向均值和方差：</strong><br>       <img src="https://images2.imgbox.com/66/c3/s9wngHYw_o.png" alt="反向均值和方差" width="320"><br>   <strong>②假设线性的激活函数：</strong><br>    <strong>正向：</strong><br>      <img src="https://images2.imgbox.com/7b/0c/eso3AQSj_o.png" alt="假设线性的激活函数" width="380"><br>    <strong>反向：</strong><br>      <img src="https://images2.imgbox.com/3d/09/hQqDgAiU_o.png" alt="反向" width="310"></li><li><strong>Xavier 初始</strong><br>    <mark><strong>Xavier 是一种常见的权重初始化方法</strong></mark><br>    <img src="https://images2.imgbox.com/e2/ec/Xz0uEGLV_o.png" alt="Xavier 初始" width="400"></li><li><strong>检查常用激活函数</strong><br>    <img src="https://images2.imgbox.com/09/9a/oOmlBykF_o.png" alt="检查常用激活函数" width="500"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/eb/e2/WpNQ03mb_o.png" alt="总结" width="450"></li></ul> 
<h3><a id="_372"></a>深度学习计算</h3> 
<h4><a id="_373"></a>层和块</h4> 
<p>  本节代码文件在源代码文件的chapter_deep-learning-computation/model-construction.ipynb中</p> 
<ul><li> <p><strong>块的组成</strong><br>   <strong>块（block）可以描述单个层、由多个层组成的组件或整个模型本身</strong>。 使用块进行抽象的一个好处是可以将一些块组合成更大的组件， 这一过程通常是递归的。<br>     <img src="https://images2.imgbox.com/70/4c/1OxdcQKi_o.png" alt="块的组成" width="600"><br>   从编程的角度来看，块由类（class）表示。 它的任何子类都必须定义一个将其输入转换为输出的<strong>前向传播函数</strong>， 并且必须存储任何必需的<strong>参数</strong>。 注意，有些块不需要任何参数。 最后，为了计算梯度，块必须具有<strong>反向传播函数</strong>。 <mark><strong>在定义我们自己的块时，由于自动微分提供了一些后端实现，我们只需要考虑前向传播函数和必需的参数。</strong></mark></p> </li><li> <p><strong>块需要提供的基本功能</strong><br>   ①将输入数据作为其前向传播函数的参数。<br>   ②通过前向传播函数来生成输出。注：输出的形状可能与输入的形状不同。<br>   ③计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。<br>   ④存储和访问前向传播计算所需的参数。<br>   ⑤根据需要初始化模型参数。</p> </li><li> <p><strong>自定义块</strong><br>   在下面的代码片段中，我们从零开始编写一个块。 它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。 注意，下面的MLP类继承了表示块的类。<mark><strong>我们的实现只需要提供我们自己的构造函数（Python中的__init__函数）和前向传播函数。</strong></mark></p> <pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">MLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 用模型参数声明层。这里，我们声明两个全连接的层</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用MLP的父类Module的构造函数来执行必要的初始化。</span>
        <span class="token comment"># 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>  <span class="token comment"># 隐藏层</span>
        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>  <span class="token comment"># 输出层</span>

    <span class="token comment"># 定义模型的前向传播，即如何根据输入X返回所需的模型输出</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <p>注意一些关键细节： 首先，我们定制的__init__函数通过super().<strong>init</strong>() 调用父类的__init__函数， 省去了重复编写模版代码的痛苦。 然后，我们实例化两个全连接层， 分别为self.hidden和self.out。 注意，除非我们实现一个新的运算符， 否则我们不必担心反向传播函数或参数初始化， 系统将自动生成这些。</p> </li><li> <p><strong>顺序块</strong><br>   回想一下Sequential的设计是为了把其他模块串起来。 为了构建我们自己的简化的MySequential， 我们只需要定义两个关键函数：<br>    ①一种将块逐个追加到列表中的函数。<br>    ②一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。<br> 下面的MySequential类提供了与默认Sequential类相同的功能：</p> <pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">MySequential</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> idx<span class="token punctuation">,</span> module <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员</span>
            <span class="token comment"># 变量_modules中。_module的类型是OrderedDict</span>
            self<span class="token punctuation">.</span>_modules<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>idx<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> module

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span>
        <span class="token keyword">for</span> block <span class="token keyword">in</span> self<span class="token punctuation">.</span>_modules<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            X <span class="token operator">=</span> block<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        <span class="token keyword">return</span> X
</code></pre> <p><strong>_modules</strong>：__init__函数将每个模块逐个添加到有序字典_modules中，_modules的主要优点是： 在模块的参数初始化过程中， 系统知道在_modules字典中查找需要初始化参数的子块。</p> </li></ul> 
<h4><a id="_425"></a>参数管理</h4> 
<p>  本节代码文件在源代码文件的chapter_deep-learning-computation/parameters.ipynb中</p> 
<ul><li><strong>参数访问</strong><br>   为方便解释，先定义如下神经网络：<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn

net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
</code></pre>   我们从已有模型中访问参数。 当通过Sequential类定义模型时， 我们可以<strong>通过索引来访问模型的任意层</strong>。 这就像模型是一个列表一样，每层的参数都在其属性中。 如下所示，我们可以检查第二个全连接层的参数。<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#OrderedDict([('weight', tensor([[ 0.0251, -0.2952, -0.1204,  0.3436, -0.3450, -0.0372,  0.0462,  0.2307]])), ('bias', tensor([0.2871]))])</span>
</code></pre> </li><li><strong>提取目标参数</strong><br>   从第二个全连接层（即第三个神经网络层）提取偏置， 提取后返回的是一个参数类实例，并进一步访问该参数的值：<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>net<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#&lt;class 'torch.nn.parameter.Parameter'&gt;</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">)</span>
<span class="token comment">#Parameter containing:</span>
<span class="token comment">#tensor([0.2871], requires_grad=True)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
<span class="token comment">#tensor([0.2871])</span>
</code></pre> 参数是复合的对象，包含值、梯度和额外信息。 这就是我们需要显式参数值的原因。 除了值之外，我们还可以访问每个参数的梯度。 在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>grad <span class="token operator">==</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token comment">#True</span>
</code></pre> </li><li><strong>一次性访问所有参数</strong><br>   当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。 当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂， 因为我们需要递归整个树来提取每个子块的参数。 下面，我们将通过演示来比较<strong>访问第一个全连接层的参数</strong>和<strong>访问所有层</strong>:<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment">#('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment">#('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))</span>
</code></pre> 这为我们提供了<strong>另一种访问网络参数的方式</strong>，如下所示:<pre><code class="prism language-python">net<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'2.bias'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>data
<span class="token comment">#tensor([0.2871])</span>
</code></pre> </li><li><strong>从嵌套块收集参数</strong><br>   我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">block1</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                         nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">block2</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 在这里嵌套</span>
        net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'block </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>i<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">,</span> block1<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> net

rgnet <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>block2<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
rgnet<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
<span class="token comment">#tensor([[0.1713],</span>
<span class="token comment">#    [0.1713]], grad_fn=&lt;AddmmBackward0&gt;)</span>
</code></pre> 设计了网络后，我们看看它是如何工作的:<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>rgnet<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''
Sequential(
  (0): Sequential(
    (block 0): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 1): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 2): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 3): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
  )
  (1): Linear(in_features=4, out_features=1, bias=True)
)
'''</span>
</code></pre> 因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。 下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项:<pre><code class="prism language-python">rgnet<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data
<span class="token comment">#tensor([-0.0444, -0.4451, -0.4149,  0.0549, -0.0969,  0.2053, -0.2514,  0.0220])</span>
</code></pre> </li><li><strong>参数初始化</strong><br>   深度学习框架提供默认随机初始化， 也允许我们创建自定义初始化方法， 满足我们通过其他规则实现初始化权重。默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵， 这个范围是根据输入和输出维度计算出的。 PyTorch的nn.init模块提供了多种预置初始化方法。<br> <strong>①内置初始化</strong><br>   下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量， 且将偏置参数设置为0。<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">init_normal</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>zeros_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>
net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_normal<span class="token punctuation">)</span>
net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
<span class="token comment">#(tensor([-0.0017,  0.0232, -0.0026,  0.0026]), tensor(0.))</span>
</code></pre>   我们还可以将所有参数初始化为给定的常数，比如初始化为1：<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">init_constant</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>zeros_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>
net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_constant<span class="token punctuation">)</span>
net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
<span class="token comment">#(tensor([1., 1., 1., 1.]), tensor(0.))</span>
</code></pre>   我们还可以<strong>对某些块应用不同的初始化方法</strong>。 例如，下面我们使用Xavier初始化方法初始化第一个神经网络层， 然后将第三个神经网络层初始化为常量值42：<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">xavier</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">init_42</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token number">42</span><span class="token punctuation">)</span>

net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>xavier<span class="token punctuation">)</span>
net<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_42<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment">#tensor([-0.4645,  0.0062, -0.5186,  0.3513])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
<span class="token comment">#tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])</span>
</code></pre> <strong>②自定义初始化</strong><br>   有时，深度学习框架没有提供我们需要的初始化方法。 在下面的例子中，我们使用以下的分布为任意权重参数 𝑤 定义初始化方法：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           w 
          
         
           ∽ 
          
          
          
            { 
           
           
            
             
              
               
               
                 U 
                
               
                 ( 
                
               
                 5 
                
               
                 , 
                
               
                 10 
                
               
                 ) 
                
               
                 , 
                
               
              
             
             
              
              
                可能性1/4 
               
              
             
            
            
             
              
               
               
                 0 
                
               
                 , 
                
               
              
             
             
              
              
                可能性1/2 
               
              
             
            
            
             
              
               
               
                 U 
                
               
                 ( 
                
               
                 − 
                
               
                 10 
                
               
                 , 
                
               
                 − 
                
               
                 5 
                
               
                 ) 
                
               
                 , 
                
               
              
             
             
              
              
                可能性1/4 
               
              
             
            
           
          
         
        
          w \backsim \begin{cases} U(5,10), &amp; \text{可能性1/4} \\ 0, &amp; \text{可能性1/2} \\ U(-10,-5), &amp; \text{可能性1/4} \\ \end{cases} 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel amsrm">∽</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 4.32em; vertical-align: -1.91em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.35em;"><span class="" style="top: -2.2em;"><span class="pstrut" style="height: 3.15em;"></span><span class="delimsizinginner delim-size4"><span class="">⎩</span></span></span><span class="" style="top: -2.192em;"><span class="pstrut" style="height: 3.15em;"></span><span class="" style="height: 0.316em; width: 0.8889em;"> 
               <svg width="0.8889em" height="0.316em" style="width:0.8889em" viewbox="0 0 888.89 316" preserveaspectratio="xMinYMin"> 
                <path d="M384 0 H504 V316 H384z M384 0 H504 V316 H384z"></path> 
               </svg></span></span><span class="" style="top: -3.15em;"><span class="pstrut" style="height: 3.15em;"></span><span class="delimsizinginner delim-size4"><span class="">⎨</span></span></span><span class="" style="top: -4.292em;"><span class="pstrut" style="height: 3.15em;"></span><span class="" style="height: 0.316em; width: 0.8889em;"> 
               <svg width="0.8889em" height="0.316em" style="width:0.8889em" viewbox="0 0 888.89 316" preserveaspectratio="xMinYMin"> 
                <path d="M384 0 H504 V316 H384z M384 0 H504 V316 H384z"></path> 
               </svg></span></span><span class="" style="top: -4.6em;"><span class="pstrut" style="height: 3.15em;"></span><span class="delimsizinginner delim-size4"><span class="">⎧</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.85em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.41em;"><span class="" style="top: -4.41em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.109em;">U</span><span class="mopen">(</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">10</span><span class="mclose">)</span><span class="mpunct">,</span></span></span><span class="" style="top: -2.97em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord">0</span><span class="mpunct">,</span></span></span><span class="" style="top: -1.53em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.109em;">U</span><span class="mopen">(</span><span class="mord">−</span><span class="mord">10</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">−</span><span class="mord">5</span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.91em;"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.41em;"><span class="" style="top: -4.41em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord text"><span class="mord cjk_fallback">可能性</span><span class="mord">1/4</span></span></span></span><span class="" style="top: -2.97em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord text"><span class="mord cjk_fallback">可能性</span><span class="mord">1/2</span></span></span></span><span class="" style="top: -1.53em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord text"><span class="mord cjk_fallback">可能性</span><span class="mord">1/4</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.91em;"><span class=""></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span><pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">my_init</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Init"</span><span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">[</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
                        <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> m<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
        m<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data <span class="token operator">*=</span> m<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;=</span> <span class="token number">5</span>
<span class="token triple-quoted-string string">'''
Init weight torch.Size([8, 4])
Init weight torch.Size([1, 8])
'''</span>
net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>my_init<span class="token punctuation">)</span>
net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span>
<span class="token triple-quoted-string string">'''
tensor([[ 8.8025,  6.4078,  0.0000, -8.4598],
        [-0.0000,  9.0582,  8.8258,  7.4997]], grad_fn=&lt;SliceBackward0&gt;)
'''</span>
</code></pre> 注意，我们始终可以直接设置参数：<pre><code class="prism language-python">net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>
net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">42</span>
net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
<span class="token comment">#tensor([42.0000,  7.4078,  1.0000, -7.4598])</span>
</code></pre> </li><li><strong>参数绑定</strong><br>   有时我们希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。<pre><code class="prism language-python"><span class="token comment"># 我们需要给共享层一个名称，以便可以引用它的参数</span>
shared <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>
net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    shared<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    shared<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
<span class="token comment"># 检查参数是否相同</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> net<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''
tensor([True, True, True, True, True, True, True, True])
'''</span>
net<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">100</span>
<span class="token comment"># 确保它们实际上是同一个对象，而不只是有相同的值</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> net<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''
tensor([True, True, True, True, True, True, True, True])
'''</span>
</code></pre>   这个例子表明第三个和第五个神经网络层的参数是绑定的。 它们不仅值相等，而且由相同的张量表示。 因此，如果我们改变其中一个参数，另一个参数也会改变。 你可能会思考：当参数绑定时，梯度会发生什么情况？ 答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层 （即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。</li></ul> 
<h4><a id="_627"></a>自定义层</h4> 
<p>  本节代码文件在源代码文件的chapter_deep-learning-computation/custom-layer.ipynb中</p> 
<ul><li> <p><strong>不带参数的层</strong><br>   下面的CenteredLayer类要从其输入中减去均值。 要构建它，我们只需继承基础层类并实现前向传播功能。</p> <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn


<span class="token keyword">class</span> <span class="token class-name">CenteredLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> X <span class="token operator">-</span> X<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> <p>让我们向该层提供一些数据，验证它是否能按预期工作。</p> <pre><code class="prism language-python">layer <span class="token operator">=</span> CenteredLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>
layer<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#tensor([-2., -1.,  0.,  1.,  2.])</span>
</code></pre> <p>我们可以将层作为组件合并到更复杂的模型中，比如：</p> <pre><code class="prism language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span> CenteredLayer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> </li><li> <p><strong>带参数的层</strong><br>   下面我们定义具有参数的层， 这些参数可以通过训练进行调整。 我们可以使用内置函数来创建参数，这些函数提供一些基本的管理功能。 比如管理访问、初始化、共享、保存和加载模型参数。 这样做的好处之一是：我们不需要为每个自定义层编写自定义的序列化程序。<br>   现在，让我们实现自定义版本的全连接层。该层需要两个参数，一个用于表示权重，另一个用于表示偏置项。 在此实现中，我们使用修正线性单元作为激活函数。<mark>该层需要输入参数：in_units和units，分别表示输入数(输入维度)和输出数(输出维度)。</mark></p> <pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">MyLinear</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_units<span class="token punctuation">,</span> units<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>in_units<span class="token punctuation">,</span> units<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>units<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        linear <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data
        <span class="token keyword">return</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>linear<span class="token punctuation">)</span>
linear <span class="token operator">=</span> MyLinear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
linear<span class="token punctuation">.</span>weight
<span class="token triple-quoted-string string">'''
Parameter containing:
tensor([[ 1.9054, -3.4102, -0.9792],
        [ 1.5522,  0.8707,  0.6481],
        [ 1.0974,  0.2568,  0.4034],
        [ 0.1416, -1.1389,  0.5875],
        [-0.7209,  0.4432,  0.1222]], requires_grad=True)
'''</span>
</code></pre> <p>我们可以使用自定义层直接执行前向传播计算：</p> <pre><code class="prism language-python">linear<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''
tensor([[2.4784, 0.0000, 0.8991],
        [3.6132, 0.0000, 1.1160]])
'''</span>
</code></pre> <p>我们还可以使用自定义层构建模型，就像使用内置的全连接层一样使用自定义层：</p> <pre><code class="prism language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>MyLinear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span> MyLinear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
net<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''
tensor([[0.],
        [0.]])
'''</span>
</code></pre> </li></ul> 
<h4><a id="_695"></a>读写文件</h4> 
<p>  本节代码文件在源代码文件的chapter_deep-learning-computation/read-write.ipynb中</p> 
<p>  有时我们希望保存训练的模型， 以备将来在各种环境中使用（比如在部署中进行预测）。 此外，当运行一个耗时较长的训练过程时， 最佳的做法是定期保存中间结果， 以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果。</p> 
<ul><li> <p><strong>加载和保存张量</strong><br>   对于单个张量，我们可以直接调用load和save函数分别读写它们。 这两个函数都要求我们提供一个名称，save要求将要保存的变量作为输入。</p> <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F

x <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">'x-file'</span><span class="token punctuation">)</span>
</code></pre> <p>我们现在可以将存储在文件中的数据读回内存。</p> <pre><code class="prism language-python">x2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'x-file'</span><span class="token punctuation">)</span>
x2
<span class="token comment">#tensor([0, 1, 2, 3])</span>
</code></pre> <p>  我们可以<strong>存储一个张量列表，然后把它们读回内存。</strong></p> <pre><code class="prism language-python">y <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">,</span> y<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">'x-files'</span><span class="token punctuation">)</span>
x2<span class="token punctuation">,</span> y2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'x-files'</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span>x2<span class="token punctuation">,</span> y2<span class="token punctuation">)</span>
<span class="token comment">#(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))</span>
</code></pre> <p>  我们甚至可以<strong>写入或读取从字符串映射到张量的字典</strong>。 当我们要读取或写入模型中的所有权重时，这很方便。</p> <pre><code class="prism language-python">mydict <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">'x'</span><span class="token punctuation">:</span> x<span class="token punctuation">,</span> <span class="token string">'y'</span><span class="token punctuation">:</span> y<span class="token punctuation">}</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>mydict<span class="token punctuation">,</span> <span class="token string">'mydict'</span><span class="token punctuation">)</span>
mydict2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'mydict'</span><span class="token punctuation">)</span>
mydict2
<span class="token comment">#{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}</span>
</code></pre> </li><li> <p><strong>加载和保存模型参数</strong><br>   保存单个权重向量（或其他张量）确实有用， 但是如果我们想保存整个模型，并在以后加载它们， 单独保存每个向量则会变得很麻烦。 毕竟，我们可能有数百个参数散布在各处。 因此，<strong>深度学习框架提供了内置函数来保存和加载整个网络</strong>。 需要注意的一个重要细节是，<strong>这将保存模型的参数而不是保存整个模型</strong>。 例如，如果我们有一个3层多层感知机，我们需要单独指定架构。 因为模型本身可以包含任意代码，所以模型本身难以序列化。 因此，为了恢复模型，我们需要用代码生成架构， 然后从磁盘加载参数。 让我们从熟悉的多层感知机开始尝试一下。</p> <pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">MLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>output <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>output<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

net <span class="token operator">=</span> MLP<span class="token punctuation">(</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
Y <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
</code></pre> <p>接下来，我们将模型的参数存储在一个叫做“mlp.params”的文件中。</p> <pre><code class="prism language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'mlp.params'</span><span class="token punctuation">)</span>
</code></pre> <p>为了恢复模型，我们实例化了原始多层感知机模型的一个备份。 这里我们不需要随机初始化模型参数，而是直接读取文件中存储的参数。</p> <pre><code class="prism language-python">clone <span class="token operator">=</span> MLP<span class="token punctuation">(</span><span class="token punctuation">)</span>
clone<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'mlp.params'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
clone<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">'''
MLP(
  (hidden): Linear(in_features=20, out_features=256, bias=True)
  (output): Linear(in_features=256, out_features=10, bias=True)
)
'''</span>
</code></pre> <p>由于两个实例具有相同的模型参数，在输入相同的X时， 两个实例的计算结果应该相同。 让我们来验证一下。</p> <pre><code class="prism language-python">Y_clone <span class="token operator">=</span> clone<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
Y_clone <span class="token operator">==</span> Y
<span class="token triple-quoted-string string">'''
tensor([[True, True, True, True, True, True, True, True, True, True],
        [True, True, True, True, True, True, True, True, True, True]])
'''</span>
</code></pre> </li></ul> 
<h3><a id="_777"></a>卷积神经网络</h3> 
<h4><a id="_778"></a>从全连接层到卷积</h4> 
<p>  本节代码文件在源代码文件的chapter_convolutional-neural-networks/why-conv.ipynb中</p> 
<ul><li><strong>重新考察全连接层</strong><br>    <img src="https://images2.imgbox.com/20/49/1S6J8wlm_o.png" alt="重新考察全连接层" width="400"></li><li><strong>平移不变性</strong><br>   不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。<br>    <img src="https://images2.imgbox.com/a7/b9/qLTYKWfj_o.png" alt="平移不变性" width="350"></li><li><strong>局部性</strong><br>    <img src="https://images2.imgbox.com/9d/78/MvJYKUDT_o.png" alt="局部性" width="350"></li><li><strong>全连接层与卷积层的关系</strong><br>    <img src="https://images2.imgbox.com/05/90/AjK50NGw_o.png" alt="总结" width="400"></li></ul> 
<h4><a id="_789"></a>图像卷积</h4> 
<p>  本节代码文件在源代码文件的chapter_convolutional-neural-networks/conv-layer.ipynb中</p> 
<ul><li><strong>互相关运算</strong><br>   在卷积层中，输入张量和核张量通过<strong>互相关运算</strong>产生输出张量。<br>      <img src="https://images2.imgbox.com/e7/27/xAYdbwQU_o.png" alt="二维交叉相关" width="450"><br>      <img src="https://images2.imgbox.com/36/40/A1eWQXWf_o.png" alt="二维卷积层" width="280"><br>   <mark><strong>不同的卷积核可以带来不同的效果：</strong></mark><br>      <img src="https://images2.imgbox.com/80/a5/979MYarD_o.png" alt="不同的卷积核的举例" width="380"></li><li><strong>交叉相关vs卷积</strong><br>    <img src="https://images2.imgbox.com/e7/f8/5fER1BPy_o.png" alt="交叉相关vs卷积" width="380"></li><li><strong>一维和三维交叉相关</strong><br>    <img src="https://images2.imgbox.com/f0/8b/Vr20qGlo_o.png" alt="一维和三维交叉相关" width="500"></li><li><strong>一维和三维交叉相关</strong><br>    <img src="https://images2.imgbox.com/a4/68/UC2upwAP_o.png" alt="总结" width="500"></li></ul> 
<h4><a id="_803"></a>填充和步幅</h4> 
<p>  本节代码文件在源代码文件的chapter_convolutional-neural-networks/padding-and-strides.ipynb中</p> 
<p>  <mark><strong>填充和步幅是卷积层的两个控制输出大小的超参数</strong></mark></p> 
<ul><li><strong>填充</strong><br>    <img src="https://images2.imgbox.com/a9/74/TtGiz247_o.png" alt="填充" width="500"><br>   由于卷积核的宽度和高度通常大于1，在应用多层卷积时，我们常常<strong>丢失边缘像素</strong>。 由于我们通常使用小卷积核，因此对于任何单个卷积，我们可能只会丢失几个像素。 但随着我们应用许多连续卷积层，累积丢失的像素数就多了。 解决这个问题的简单方法即为填充：<br>    <img src="https://images2.imgbox.com/5b/50/QsM9FrFZ_o.png" alt="填充" width="500"><br>   <mark>填充原则：</mark><br>      <img src="https://images2.imgbox.com/65/13/OSsFG28y_o.png" alt="填充原则" width="300"></li><li><strong>步幅</strong><br>    <img src="https://images2.imgbox.com/05/20/Bn4YJBvq_o.png" alt="步幅" width="500"><br>   有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。<br>    <img src="https://images2.imgbox.com/03/96/km64XbgF_o.png" alt="步幅" width="500"><br>   <mark>步幅原则：</mark><br>      <img src="https://images2.imgbox.com/6d/2b/syVgYq9R_o.png" alt="步幅原则" width="320"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/70/cd/fTL6YXsE_o.png" alt="总结" width="450"></li></ul> 
<h4><a id="_821"></a>多输入多输出通道</h4> 
<p>  本节代码文件在源代码文件的chapter_convolutional-neural-networks/channels.ipynb中</p> 
<ul><li><strong>多个输入通道</strong><br>    <img src="https://images2.imgbox.com/e1/cd/lZCWyXIi_o.png" alt="多个输入通道" width="350"><br>   当输入包含多个通道时，需要构造一个与输入数据具有<strong>相同输入通道数</strong>的卷积核，以便与输入数据进行互相关运算。<br>    <img src="https://images2.imgbox.com/79/07/RWzs3Xee_o.png" alt="多个输入通道" width="500"><br>    <img src="https://images2.imgbox.com/90/a6/ET8ms9yg_o.png" alt="多个输入通道" width="250"></li><li><strong>多个输出通道</strong><br>    <img src="https://images2.imgbox.com/5b/42/AR2EpDNe_o.png" alt="多个输出通道" width="420"></li><li><strong>多个输入和输出通道</strong><br>    <img src="https://images2.imgbox.com/f2/02/Aplb0aH3_o.png" alt="多个输入和输出通道" width="400"></li><li><strong>1x1卷积层</strong><br>    <img src="https://images2.imgbox.com/84/ae/eeyo8LD5_o.png" alt="1x1卷积层" width="550"></li><li><strong>二维卷积层</strong><br>    <img src="https://images2.imgbox.com/8e/a5/0v59PFNy_o.png" alt="二维卷积层" width="450"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/6e/7d/FbQPo5Kh_o.png" alt="总结" width="450"></li></ul> 
<h4><a id="_838"></a>池化层（汇聚层）</h4> 
<p>  本节代码文件在源代码文件的chapter_convolutional-neural-networks/pooling.ipynb中</p> 
<ul><li><strong>卷积层的缺点</strong><br>    <img src="https://images2.imgbox.com/1b/58/By1NEMof_o.png" alt="卷积层的缺点" width="400"></li><li><strong>二维最大池化层</strong><br>    <img src="https://images2.imgbox.com/a6/d1/ByMZyYfu_o.png" alt="二维最大池化层" width="450"><br>    <img src="https://images2.imgbox.com/f0/32/GSG6fyKg_o.png" alt="二维最大池化层" width="450"></li><li><strong>池化层vs卷积层</strong><br>   <mark><strong>池化层没有可学习的参数，也不会融合多输入通道。</strong></mark><br>    <img src="https://images2.imgbox.com/34/5d/XNVOZ8J4_o.png" alt="池化层vs卷积层" width="450"></li><li><strong>平均池化层</strong><br>    <img src="https://images2.imgbox.com/be/67/kJ7Qc7rP_o.png" alt="平均池化层" width="450"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/9f/66/36b31lz3_o.png" alt="总结" width="350"></li></ul> 
<h4><a id="LeNet_852"></a>LeNet</h4> 
<p>  本节代码文件在源代码文件的chapter_convolutional-neural-networks/lenet.ipynb中</p> 
<ul><li><strong>手写数字识别</strong><br>    <img src="https://images2.imgbox.com/64/0f/IzoDztdb_o.png" alt="手写数字识别" width="500"></li><li><strong>MNIST数据集</strong><br>    <img src="https://images2.imgbox.com/59/5a/EA7viBxz_o.png" alt="MNIST数据集" width="500"></li><li><strong>LeNet</strong><br>   LeNet是最早发布的<strong>卷积神经网络</strong>之一，因其在计算机视觉任务中的高效性能而受到广泛关注。 这个模型是由AT&amp;T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），<strong>目的是识别图像中的手写数字</strong>。 当时，LeNet取得了与支持向量机性能相媲美的成果，成为监督学习的主流方法。 LeNet被广泛用于自动取款机（ATM）机中，帮助识别处理支票的数字。 时至今日，一些自动取款机仍在运行Yann LeCun和他的同事Leon Bottou在上世纪90年代写的代码呢！<br>   <strong>总体来看，LeNet（LeNet-5）由两个部分组成：</strong><br>     <strong>①卷积编码器：由两个卷积层组成。</strong><br>     <strong>②全连接层密集块：由三个全连接层组成。</strong><br>    <img src="https://images2.imgbox.com/2a/20/rFSQGLRD_o.png" alt="LeNet" width="900"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/73/ea/oHnBnoyo_o.png" alt="总结" width="300"></li></ul> 
<h3><a id="_866"></a>现代卷积神经网络</h3> 
<h4><a id="AlexNet_867"></a>深度卷积神经网络（AlexNet）</h4> 
<p>  本节代码文件在源代码文件的chapter_convolutional-modern/alexnet.ipynb中</p> 
<ul><li> <p><strong>机器学习VS神经网路</strong><br>   在上世纪90年代初到2012年之间的大部分时间里，神经网络往往被其他机器学习方法超越，如支持向量机（support vector machines）。<br>    <img src="https://images2.imgbox.com/3a/f8/M2eYx6F4_o.png" alt="机器学习" width="600"></p> </li><li> <p><strong>计算机视觉与几何学</strong><br>   卷积神经网络通常用在计算机视觉，在2000年时，计算机视觉的知识主要来源于几何学。<br>     <img src="https://images2.imgbox.com/28/20/8oT5gzwt_o.png" alt="几何学" width="600"></p> </li><li> <p><strong>计算机视觉与特征工程</strong><br>   15年前，计算机视觉中，最重要的是特征工程。<br>     <img src="https://images2.imgbox.com/d9/19/cYqogikt_o.png" alt="特征工程" width="600"></p> </li><li> <p><strong>深度学习的崛起原因</strong><br>   尽管一直有一群执着的研究者不断钻研，试图学习视觉数据的逐级表征，然而很长一段时间里这些尝试都未有突破。深度卷积神经网络的突破出现在2012年。突破可归因于两个关键因素。<br> <strong>①硬件</strong><br>   2012年，当Alex Krizhevsky和Ilya Sutskever实现了可以在GPU硬件上运行的深度卷积神经网络时，一个重大突破出现了。他们意识到卷积神经网络中的计算瓶颈：卷积和矩阵乘法，都是可以在硬件上并行化的操作。 于是，他们使用两个显存为3GB的NVIDIA GTX580 GPU实现了快速卷积运算。他们的创新cuda-convnet几年来它一直是行业标准，并推动了深度学习热潮。<br>    <img src="https://images2.imgbox.com/fa/bd/wGf02Rdj_o.png" alt="硬件" width="600"></p> <p><strong>②数据</strong><br>   2009年，ImageNet数据集发布，并发起ImageNet挑战赛：要求研究人员从100万个样本中训练模型，以区分1000个不同类别的对象。ImageNet数据集由斯坦福教授李飞飞小组的研究人员开发，利用谷歌图像搜索（Google Image Search）对每一类图像进行预筛选，并利用亚马逊众包（Amazon Mechanical Turk）来标注每张图片的相关类别。这种规模是前所未有的。这项被称为ImageNet的挑战赛推动了计算机视觉和机器学习研究的发展，挑战研究人员确定哪些模型能够在更大的数据规模下表现最好。<br>    <img src="https://images2.imgbox.com/87/31/v4OxrrrZ_o.png" alt="数据" width="600"></p> </li><li> <p><strong>AlexNet vs LeNet</strong><br>    <img src="https://images2.imgbox.com/44/87/taBx8lfj_o.png" alt="AlexNet vs LeNet" width="600"><br>    <img src="https://images2.imgbox.com/c2/d7/zTGK2c6Z_o.png" alt="AlexNet vs LeNet" width="650"></p> </li><li> <p><strong>AlexNet</strong><br>    <img src="https://images2.imgbox.com/0a/14/824UP9Uz_o.png" alt="AlexNet的更多细节" width="650"><br>    <img src="https://images2.imgbox.com/99/94/ZIHCh4fi_o.png" alt="AlexNet的复杂度" width="650"></p> </li><li> <p><strong>总结</strong><br>    <img src="https://images2.imgbox.com/e4/b8/EwEtn8w0_o.png" alt="总结" width="450"></p> </li></ul> 
<h4><a id="VGG_896"></a>使用块的网络（VGG）</h4> 
<p>  本节代码文件在源代码文件的chapter_convolutional-modern/vgg.ipynb中</p> 
<ul><li><strong>VGG 出现的背景</strong><br>    <img src="https://images2.imgbox.com/cc/ad/QzaJz803_o.png" alt="VGG出现的背景" width="500"></li><li><strong>VGG 块</strong><br>    <img src="https://images2.imgbox.com/fa/1a/aujIWheF_o.png" alt="VGG块" width="600"></li><li><strong>VGG 架构</strong><br>    <img src="https://images2.imgbox.com/9e/39/sRam5ayf_o.png" alt="VGG架构" width="500"></li><li><strong>进度（发展）</strong><br>    <img src="https://images2.imgbox.com/30/1f/WmT2v1OT_o.png" alt="进度" width="350"><br>   <strong>下图横坐标代表速率，纵坐标代表准确率，圆的大小代表内存占用的大小：</strong><br>    <img src="https://images2.imgbox.com/08/58/sDx8wMzO_o.png" alt="进度" width="650"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/ad/cc/19dn7PNz_o.png" alt="总结" width="450"></li></ul> 
<h4><a id="NiN_910"></a>网络中的网络（NiN）</h4> 
<p>  本节代码文件在源代码文件的chapter_convolutional-modern/nin.ipynb中</p> 
<ul><li><strong>全连接层的问题</strong><br>    <img src="https://images2.imgbox.com/3a/8f/6tmpyX0G_o.png" alt="全连接层的问题" width="550"></li><li><strong>NiN 块</strong><br>    <img src="https://images2.imgbox.com/08/e4/7nZgMTrt_o.png" alt="NiN块" width="600"></li><li><strong>NiN 架构</strong><br>    <img src="https://images2.imgbox.com/c6/b1/dDUGTAmZ_o.png" alt="NiN架构" width="350"></li><li><strong>NiN 网络</strong><br>    <img src="https://images2.imgbox.com/bb/51/zayjmmRT_o.png" alt="NiN网络" width="350"><br>    <img src="https://images2.imgbox.com/1f/25/9OkhKnxD_o.png" alt="NiN网络" width="550"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/09/8d/AE8at5oO_o.png" alt="总结" width="350"></li></ul> 
<h4><a id="GoogLeNet_923"></a>含并行连结的网络（GoogLeNet）</h4> 
<p>  本节代码文件在源代码文件的chapter_convolutional-modern/googlenet.ipynb中</p> 
<ul><li><strong>GoogLeNet 出现的背景</strong><br>   我们往往不确定到底选取什么样的层效果更好，到底是3X3卷积层还是5X5的卷积层，诸如此类的问题是GooLeNet选择了另一种思路“小学生才做选择，我全都要”，这也使得GooLeNet成为了第一个模型中超过1000个层的模型。<br>    <img src="https://images2.imgbox.com/0c/b6/cedRmLG5_o.png" alt="GoogLeNet出现的背景" width="550"><br>    <img src="https://images2.imgbox.com/f0/5c/Og3rVDWU_o.png" alt="GoogLeNet出现的背景" width="550"></li><li><strong>Inception 块</strong><br>   Inception块由四条并行路径组成。 前三条路径使用窗口大小为1x1、3x3和5x5的卷积层，从不同空间大小中提取信息。 中间的两条路径在输入上执行1x1卷积，以减少通道数，从而降低模型的复杂性。 第四条路径使用3x3最大汇聚层，然后使用1x1卷积层来改变通道数。 这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成Inception块的输出。在Inception块中，通常调整的超参数是每层输出通道数。<br>    <img src="https://images2.imgbox.com/98/22/4DxS1KJs_o.png" alt="Inception块" width="550"><br>    <img src="https://images2.imgbox.com/3d/4e/a1AhlDca_o.png" alt="Inception块" width="550"></li><li><strong>GoogLeNet</strong><br>   GoogLeNet一共使用9个Inception块和全局平均汇聚层(Global AvgPool)的堆叠来生成其估计值。Inception块之间的最大汇聚层可降低维度。 第一个模块类似于AlexNet和LeNet，Inception块的组合从VGG继承，全局平均汇聚层避免了在最后使用全连接层(FC)。<br>    <img src="https://images2.imgbox.com/2b/8f/sq2eNxe5_o.png" alt="GoogLeNet" width="550"><br>    <img src="https://images2.imgbox.com/e1/19/gIAggGVo_o.png" alt="段1&amp;2" width="550"><br>    <img src="https://images2.imgbox.com/98/db/DghGXCDp_o.png" alt="段3" width="550"><br>    <img src="https://images2.imgbox.com/dd/15/JHoSqA3M_o.png" alt="段4&amp;5" width="550"></li><li><strong>Inception 有各种后续变种</strong><br>   <mark>v3是在v2基础上变化的:</mark><br>    <img src="https://images2.imgbox.com/ec/ae/06txN9kt_o.png" alt="Inception有各种后续变种" width="500"><br>    <img src="https://images2.imgbox.com/59/36/FQ7oIO7S_o.png" alt="Inception V3块，段3" width="750"><br>    <img src="https://images2.imgbox.com/6d/eb/28Hx59fK_o.png" alt="Inception V3块，段4" width="750"><br>    <img src="https://images2.imgbox.com/e8/88/sZNMhvBG_o.png" alt="Inception V3块，段5" width="750"></li><li><strong>Inception与其他网络的比较</strong><br>   <strong>下图横坐标代表速率，纵坐标代表准确率，圆的大小代表内存占用的大小。</strong><mark>由图可见，Inception V3速率较低，占用内存较大，但准确率很高。</mark><br>    <img src="https://images2.imgbox.com/f2/aa/oJAuq9fS_o.png" alt="Inception与其他网络的比较" width="900"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/8c/9a/fDbTKPDH_o.png" alt="总结" width="400"></li></ul> 
<h4><a id="_950"></a>批量规范化（归一化）</h4> 
<p>  本节代码文件在源代码文件的chapter_convolutional-modern/batch-norm.ipynb中</p> 
<ul><li><strong>背景</strong><br>    <img src="https://images2.imgbox.com/c2/f4/nRPJCtKL_o.png" alt="背景" width="600"></li><li><strong>批量归一化</strong><br>    <img src="https://images2.imgbox.com/70/55/XIMAFzHo_o.png" alt="批量归一化" width="450"></li><li><strong>批量归一化层</strong><br>    <img src="https://images2.imgbox.com/6d/69/zeVmZP1P_o.png" alt="批量归一化层" width="350"></li><li><strong>批量归一化在做什么？</strong><br>    <img src="https://images2.imgbox.com/66/a3/zc0ps0s7_o.png" alt="批量归一化在做什么" width="500"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/c2/85/6HlWBIL1_o.png" alt="总结" width="400"></li></ul> 
<h4><a id="ResNet_962"></a>残差网络（ResNet）</h4> 
<p>  本节代码文件在源代码文件的chapter_convolutional-modern/resnet.ipynb中</p> 
<ul><li><strong>加更多的层总是改进精度吗？</strong><br>    <img src="https://images2.imgbox.com/2c/11/WbWqvmb6_o.png" alt="加更多的层总是改进精度吗" width="600"></li><li><strong>残差块</strong><br>    <img src="https://images2.imgbox.com/50/4e/IKmRdXde_o.png" alt="残差块" width="700"></li><li><strong>ResNet 块细节</strong><br>    <img src="https://images2.imgbox.com/02/42/JMLDzKr3_o.png" alt="ResNet 块细节" width="600"></li><li><strong>不同的残差块</strong><br>    <img src="https://images2.imgbox.com/ca/39/fmIY0cZM_o.png" alt="不同的残差块" width="1000"></li><li><strong>ResNet 块</strong><br>    <img src="https://images2.imgbox.com/20/4a/q2HlQhRp_o.png" alt="ResNet 块" width="800"></li><li><strong>ResNet 架构</strong><br>    <img src="https://images2.imgbox.com/1a/9e/45dx7zaW_o.png" alt="ResNet 架构" width="650"></li><li><strong>ResNet与其他网络的比较</strong><br>   <strong>下图横坐标代表速率，纵坐标代表准确率，圆的大小代表内存占用的大小。</strong><mark>由图可见，ResNet 152 速率较低，但占用内存较小，且准确率很高。</mark><br>    <img src="https://images2.imgbox.com/0f/d9/76zeSVgq_o.png" alt="ResNet与其他网络的比较" width="900"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/74/62/7hAeT0vg_o.png" alt="总结" width="500"></li></ul> 
<h3><a id="_981"></a>计算性能</h3> 
<h4><a id="CPUGPU_982"></a>深度学习硬件(CPU和GPU)</h4> 
<ul><li><strong>电脑CPU、GPU、内存之间的关系</strong><br>    <img src="https://images2.imgbox.com/e6/01/krlvFjmG_o.png" alt="电脑CPU、GPU、内存之间的关系" width="600"></li><li><strong>CPU芯片图</strong><br>    <img src="https://images2.imgbox.com/a5/5e/uva8ghSs_o.png" alt="CPU芯片图" width="500"></li><li><strong>提升CPU利用率</strong><br>    <img src="https://images2.imgbox.com/74/f9/TzN59Jjg_o.png" alt="提升CPU利用率" width="350"><br>    <img src="https://images2.imgbox.com/40/fd/UhEDbbI5_o.png" alt="样例分析" width="500"><br>    <img src="https://images2.imgbox.com/fc/62/lZvhZdyS_o.png" alt="提升CPU利用率" width="400"><br>    <img src="https://images2.imgbox.com/fc/c5/MQLfEZI8_o.png" alt="样例分析" width="400"></li><li><strong>GPU芯片图</strong><br>    <img src="https://images2.imgbox.com/83/99/zPQfSQDr_o.png" alt="GPU芯片图" width="400"></li><li><strong>GPU vs CPU</strong><br>    <img src="https://images2.imgbox.com/5e/52/HrCpFeM1_o.png" alt="GPU vs CPU" width="650"></li><li><strong>提升GPU利用率</strong><br>    <img src="https://images2.imgbox.com/3e/fa/8erzNZ9M_o.png" alt="提升GPU利用率" width="230"></li><li><strong>CPU/GPU 带宽</strong><br>    <img src="https://images2.imgbox.com/3f/16/2XJrTnpl_o.png" alt="CPU/GPU 带宽" width="650"></li><li><strong>更多的 CPUs 和 GPUs</strong><br>    <img src="https://images2.imgbox.com/41/d5/ScciXO2j_o.png" alt="更多的 CPUs 和 GPUs" width="400"></li><li><strong>CPU/GPU 高性能计算编程</strong><br>    <img src="https://images2.imgbox.com/4b/dd/iupnaRLZ_o.png" alt="CPU/GPU 高性能计算编程" width="280"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/89/66/hBY9qgRg_o.png" alt="总结" width="350"></li></ul> 
<h4><a id="TPU_1006"></a>深度学习硬件(TPU和其他)</h4> 
<ul><li><strong>手机内部的芯片</strong><br>    <img src="https://images2.imgbox.com/d6/44/RoPjPDdK_o.png" alt="手机内部的芯片" width="550"></li><li><strong>DSP：数字信号处理</strong><br>    <img src="https://images2.imgbox.com/5f/7d/ccWh7v9P_o.png" alt="DSP：数字信号处理" width="400"></li><li><strong>可编程阵列(FPGA)</strong><br>    <img src="https://images2.imgbox.com/98/e1/ddASLFRH_o.png" alt="可编程阵列(FPGA)" width="380"></li><li><strong>AI ASIC</strong><br>    <img src="https://images2.imgbox.com/25/4b/FoST6fna_o.png" alt="AI ASIC" width="480"></li><li><strong>Systolic Array</strong><br>    <img src="https://images2.imgbox.com/1d/91/TInv7Cg8_o.png" alt="Systolic Array" width="240"><br> <img src="https://images2.imgbox.com/c1/2f/3bTy5jps_o.png" alt="Systolic Array的矩阵乘法" width="800"><br> <img src="https://images2.imgbox.com/04/d4/cihvFmfH_o.png" alt="Systolic Array的矩阵乘法" width="800"><br> <img src="https://images2.imgbox.com/db/49/yKmh082a_o.png" alt="Systolic Array的矩阵乘法" width="800"><br> <img src="https://images2.imgbox.com/29/b4/rXxS2ZL8_o.png" alt="Systolic Array的矩阵乘法" width="800"><br> <img src="https://images2.imgbox.com/09/9d/WHI2lNO8_o.png" alt="Systolic Array的矩阵乘法" width="800"><br> <img src="https://images2.imgbox.com/3c/c9/83sE3yN0_o.png" alt="Systolic Array的矩阵乘法" width="800"><br> <img src="https://images2.imgbox.com/f0/fd/hiEm7O87_o.png" alt="Systolic Array的矩阵乘法" width="800"><br> <img src="https://images2.imgbox.com/13/ed/xkGsv9Dq_o.png" alt="Systolic Array的矩阵乘法" width="800"><br>    <img src="https://images2.imgbox.com/58/e9/bK5MP7Go_o.png" alt="Systolic Array" width="550"></li><li><strong>总结</strong><br>   <strong>· 灵活性、易用性：Intel(CPU) &gt; GPU &gt; DSP &gt; FPGA &gt; ASIC</strong><br>   <strong>· 性能功耗：Intel(CPU) &lt; GPU &lt; DSP &lt; FPGA &lt; ASIC</strong><br>    <img src="https://images2.imgbox.com/41/bd/syztPAeC_o.png" alt="总结" width="550"></li></ul> 
<h4><a id="_1030"></a>单机多卡并行</h4> 
<ul><li><strong>多GPU并行</strong><br>    <img src="https://images2.imgbox.com/ee/21/FRM9Xx6I_o.png" alt="多GPU并行" width="350"></li><li><strong>单机多卡并行</strong><br>    <img src="https://images2.imgbox.com/ba/6c/JT06tye4_o.png" alt="单机多卡并行" width="530"></li><li><strong>数据并行 vs 模型并行</strong><br>    <img src="https://images2.imgbox.com/71/21/EcP0srcm_o.png" alt="数据并行 vs 模型并行" width="530"></li><li><strong>数据并行</strong><br>    <img src="https://images2.imgbox.com/db/c1/KbdtliQG_o.png" alt="数据并行" width="530"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/9c/06/6IEANj5q_o.png" alt="总结" width="500"></li></ul> 
<h4><a id="_1041"></a>分布式训练</h4> 
<ul><li><strong>分布式计算</strong><br>   <strong>本质上来说和之前讲的单机多卡并行没有区别。二者之间的区别是分布式计算是通过网络把数据从一台机器搬到另一台机器。</strong><br>    <img src="https://images2.imgbox.com/16/6a/oRx393kF_o.png" alt="分布式计算" width="350"><img src="https://images2.imgbox.com/5e/d9/ss0Mnn08_o.png" alt="分布式计算" width="500"></li><li><strong>GPU机器架构</strong><br>   总的来说，gpu到gpu的通讯是很快的，gpu到cpu慢一点。机器到机器更慢。因而总体性能的关键就是尽量在本地做通讯而少在机器之间做通讯。<br>    <img src="https://images2.imgbox.com/53/58/bq01vIrf_o.png" alt="GPU机器架构" width="750"><br> <strong>举例：计算一个小批量：</strong><br>    <img src="https://images2.imgbox.com/12/21/hqFohh5K_o.png" alt="计算一个小批量" width="450"><img src="https://images2.imgbox.com/14/27/1HTqi5tt_o.png" alt="计算一个小批量" width="450"><br>    <img src="https://images2.imgbox.com/b6/21/0PvDqplr_o.png" alt="计算一个小批量" width="450"><img src="https://images2.imgbox.com/b4/69/5fOAUsba_o.png" alt="计算一个小批量" width="450"><br>    <img src="https://images2.imgbox.com/38/52/roZHKem4_o.png" alt="计算一个小批量" width="450"><img src="https://images2.imgbox.com/11/25/7JssRwlo_o.png" alt="计算一个小批量" width="450"><br>    <img src="https://images2.imgbox.com/74/8d/ML7DZU0y_o.png" alt="计算一个小批量" width="450"><img src="https://images2.imgbox.com/86/cd/CAK34abI_o.png" alt="计算一个小批量" width="450"></li><li><strong>同步 SGD</strong><br>    <img src="https://images2.imgbox.com/48/0c/L2VWj6Ew_o.png" alt="同步 SGD" width="400"></li><li><strong>性能</strong><br>    <img src="https://images2.imgbox.com/14/2b/bUQ1fpcz_o.png" alt="性能" width="450"></li><li><strong>性能的权衡</strong><br>    <img src="https://images2.imgbox.com/2b/ea/l4Jf9ka5_o.png" alt="性能的权衡" width="450"></li><li><strong>实践时的建议</strong><br>    <img src="https://images2.imgbox.com/00/47/ClsnAtiY_o.png" alt="实践时的建议" width="480"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/fe/3d/qstBohAI_o.png" alt="总结" width="530"></li></ul> 
<h3><a id="_1063"></a>计算机视觉</h3> 
<h4><a id="_1064"></a>图像增广</h4> 
<p>  本节代码文件在源代码文件的chapter_computer-vision/image-augmentation.ipynb中</p> 
<ul><li> <p><strong>数据增广</strong><br>    <img src="https://images2.imgbox.com/0e/11/IWiIeFdw_o.png" alt="数据增广" width="300"><img src="https://images2.imgbox.com/36/dc/OBokYwYm_o.png" alt="数据增广" width="600"></p> </li><li> <p><strong>数据增强</strong><br>    <img src="https://images2.imgbox.com/4b/44/A9Xf8enI_o.png" alt="数据增强" width="600"></p> </li><li> <p><strong>使用增强数据训练</strong><br>    <img src="https://images2.imgbox.com/eb/19/ANngXK0J_o.png" alt="使用增强数据训练" width="700"></p> </li><li> <p><strong>翻转</strong><br>    <img src="https://images2.imgbox.com/4b/87/f8yatmdk_o.png" alt="翻转" width="600"></p> </li><li> <p><strong>切割</strong><br>    <img src="https://images2.imgbox.com/fc/2b/L8KFP9hP_o.png" alt="切割" width="600"></p> </li><li> <p><strong>颜色</strong><br>    <img src="https://images2.imgbox.com/07/7e/nUzcOvMP_o.png" alt="颜色" width="600"></p> </li><li> <p><strong>几十种其他办法</strong><br>    <img src="https://images2.imgbox.com/c1/15/78dzBH1L_o.png" alt="几十种其他办法" width="600"></p> </li><li> <p><strong>总结</strong><br>    <img src="https://images2.imgbox.com/91/3e/QQja0Acr_o.png" alt="总结" width="300"></p> </li></ul> 
<h4><a id="_1083"></a>微调</h4> 
<p>  本节代码文件在源代码文件的chapter_computer-vision/fine-tuning.ipynb中</p> 
<ul><li><strong>微调的原因</strong><br>    <img src="https://images2.imgbox.com/95/72/98yEKdvA_o.png" alt="微调的原因" width="600"></li><li><strong>神经网络的网络架构</strong><br>    <img src="https://images2.imgbox.com/7f/a9/sftGhKom_o.png" alt="神经网络的网络架构" width="500"><br>     <img src="https://images2.imgbox.com/7a/30/iAlBnoIx_o.png" alt="神经网络的微调" width="450"></li><li><strong>微调中的权重初始化</strong><br>    <img src="https://images2.imgbox.com/88/89/Nz9CnSfB_o.png" alt="微调中的权重初始化" width="550"></li><li><strong>训练时的微调</strong><br>    <img src="https://images2.imgbox.com/94/eb/7GlLAunH_o.png" alt="训练时的微调" width="400"></li><li><strong>重用分类器权重</strong><br>    <img src="https://images2.imgbox.com/c8/43/gVGG17Nx_o.png" alt="重用分类器权重" width="500"></li><li><strong>固定一些层</strong><br>    <img src="https://images2.imgbox.com/4d/c4/SMNl2bET_o.png" alt="固定一些层" width="500"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/9d/6e/hz9iRGWe_o.png" alt="总结" width="350"></li></ul> 
<h4><a id="_1100"></a>物体检测和数据集</h4> 
<p>  本节代码文件在源代码文件的chapter_computer-vision/bounding-box.ipynb中</p> 
<ul><li> <p><strong>图片分类与目标检测的区别</strong><br>   目标检测更加复杂，需要进行多个物体的识别，还要找出每个物体的位置。目标检测的应用场景也更多。<br>    <img src="https://images2.imgbox.com/84/60/3bTRo27F_o.png" alt="在这里插入图片描述" width="400"></p> </li><li> <p><strong>边缘框</strong><br>    <img src="https://images2.imgbox.com/ba/fb/YkwVJhXM_o.png" alt="边缘框" width="400"></p> </li><li> <p><strong>目标检测数据集</strong><br>    <img src="https://images2.imgbox.com/d5/78/6bLz5Nzz_o.png" alt="目标检测数据集" width="450"></p> </li><li> <p><strong>总结</strong><br>    <img src="https://images2.imgbox.com/7a/2a/iXGW2Emk_o.png" alt="总结" width="400"></p> </li></ul> 
<h4><a id="_1112"></a>锚框</h4> 
<p>  本节代码文件在源代码文件的chapter_computer-vision/anchor.ipynb中</p> 
<ul><li><strong>锚框</strong><br>    <img src="https://images2.imgbox.com/68/ab/W0HGYIw5_o.png" alt="锚框" width="500"></li><li><strong>IoU - 交并比</strong><br>    <img src="https://images2.imgbox.com/49/43/Ehf0JoyP_o.png" alt="IoU - 交并比" width="450"></li><li><strong>赋予锚框标号</strong><br>    <img src="https://images2.imgbox.com/fa/5c/AHLaO2z0_o.png" alt="赋予锚框标号" width="300"><br>    <img src="https://images2.imgbox.com/5e/21/bIby66Uy_o.png" alt="赋予锚框标号" width="500"></li><li><strong>使用非极大值抑制（NMS）输出</strong><br>    <img src="https://images2.imgbox.com/51/69/DlaM3Xqh_o.png" alt="使用非极大值抑制（NMS）输出" width="400"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/fd/04/IBSrnDCy_o.png" alt="总结" width="300"></li></ul> 
<h4><a id="RCNNSSDYOLO_1125"></a>物体检测算法：R-CNN，SSD，YOLO</h4> 
<p>  本节代码文件在源代码文件的chapter_computer-vision/rcnn.ipynb中</p> 
<ul><li><strong>R-CNN</strong><br>    <br> <img src="https://images2.imgbox.com/83/42/DkmKYtD8_o.png" alt="R-CNN" width="400"></li><li><strong>兴趣区域（RoI）池化层</strong><br>   <mark>目的：每个锚框都可以变成想要的形状</mark><br>    <img src="https://images2.imgbox.com/03/45/pNDrs5jB_o.png" alt="兴趣区域（RoI）池化层" width="450"></li><li><strong>Fast RCNN</strong><br>   RCNN需要对每个锚框进行CNN运算，这些特征抽取计算有重复，并且锚框数量大，特征抽取的计算量也大。Fast RCNN改进了这种计算量大的问题，使用CNN对整张图片抽取特征（快的关键），再使用Rol池化层对每个锚框（将在原图片中搜索到的锚框，映射到CNN得到的结果上），生成固定长度的特征。<br>    <img src="https://images2.imgbox.com/6b/b6/U9AlbRly_o.png" alt="Fast RCNN" width="450"></li><li><strong>Faster RCNN</strong><br>   将CNN结果输入到卷积层，然后用锚框去圈区域，这些锚框很多有好有坏，然后进行预测，binary 预测是预测这个锚框的好坏，即有没有有效的圈住物体，bounding box prediction预测是对锚框进行一些改进，最后用NMS（非极大值抑制）对锚框进行合并。<br>    <img src="https://images2.imgbox.com/37/b2/8cuf57Ct_o.png" alt="Faster RCNN" width="600"></li><li><strong>Mask RCNN</strong><br>    <img src="https://images2.imgbox.com/76/37/T28QnU3S_o.png" alt="Mask RCNN" width="600"><br>   <strong>如图，Faster RCNN精度高但是速度慢（贵）：</strong><br>    <img src="https://images2.imgbox.com/b2/8e/gP6gzMo3_o.png" alt="精度比较" width="400"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/bb/50/Du7JyWuo_o.png" alt="总结" width="350"></li></ul> 
<h4><a id="SSD_1145"></a>单发多框检测（SSD）</h4> 
<p>  本节代码文件在源代码文件的chapter_computer-vision/ssd.ipynb中</p> 
<ul><li><strong>生成锚框</strong><br>    <img src="https://images2.imgbox.com/9e/91/Wx7qpvn3_o.png" alt="生成锚框" width="500"></li><li><strong>SSD模型</strong><br>    <img src="https://images2.imgbox.com/17/35/puZ6lzDw_o.png" alt="SSD模型" width="500"><br>   <strong>如图，SSD速度快但精度不高：</strong><br>    <img src="https://images2.imgbox.com/0e/64/16DdYUEM_o.png" alt="比较" width="400"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/05/b2/4BnwPp9m_o.png" alt="总结" width="350"></li></ul> 
<h4><a id="YOLO_1155"></a>YOLO</h4> 
<ul><li><strong>YOLO（你只看一次）</strong><br>    <img src="https://images2.imgbox.com/3d/43/GNKq9BQx_o.png" alt="在这里插入图片描述" width="450"><br>   <strong>如图，相同精度下YoLo比SSD速度快：</strong><br>    <img src="https://images2.imgbox.com/21/7f/L7c5LBmG_o.png" alt="比较" width="400"></li></ul> 
<h4><a id="_1160"></a>语义分割</h4> 
<p>  本节代码文件在源代码文件的chapter_computer-vision/semantic-segmentation-and-dataset.ipynb中</p> 
<ul><li><strong>语义分割</strong><br>    <img src="https://images2.imgbox.com/f6/0b/Fhl6EauZ_o.png" alt="语义分割" width="500"></li><li><strong>应用：背景虚化</strong><br>    <img src="https://images2.imgbox.com/3f/14/aAmAjFy9_o.png" alt="应用" width="500"></li><li><strong>应用：路面分割</strong><br>    <img src="https://images2.imgbox.com/96/0f/Zxd9DCl8_o.png" alt="应用" width="500"></li><li><strong>应用：实例分割</strong><br>    <img src="https://images2.imgbox.com/3e/78/tOwMOQ1a_o.png" alt="应用" width="400"></li></ul> 
<h4><a id="_1170"></a>转置卷积</h4> 
<p>  本节代码文件在源代码文件的chapter_computer-vision/transposed-conv.ipynb中</p> 
<ul><li><strong>转置卷积</strong><br>    <img src="https://images2.imgbox.com/5d/2d/s1uSUJF7_o.png" alt="转置卷积" width="500"></li><li><strong>为什么称之为“转置”</strong><br>    <img src="https://images2.imgbox.com/f4/10/WSwXLHNF_o.png" alt="为什么称之为“转置”" width="450"></li><li><strong>转置卷积是一种卷积</strong><br>    <img src="https://images2.imgbox.com/7a/37/UVSy2G5X_o.png" alt="转置卷积是一种卷积" width="400"></li><li><strong>重新排列输入和核</strong><br>    <img src="https://images2.imgbox.com/a6/80/nKPAKH7y_o.png" alt="重新排列输入和核" width="350"><br>    <img src="https://images2.imgbox.com/2a/c7/UVwdydLb_o.png" alt="重新排列输入和核" width="350"><br>    <img src="https://images2.imgbox.com/81/08/yDOyoXGK_o.png" alt="重新排列输入和核" width="350"></li><li><strong>形状换算</strong><br>    <img src="https://images2.imgbox.com/22/bc/f0xxq2i2_o.png" alt="形状换算" width="400"></li><li><strong>同反卷积的关系</strong><br>    <img src="https://images2.imgbox.com/d2/26/JNdxL0gr_o.png" alt="同反卷积的关系" width="400"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/92/1c/spP1fwpQ_o.png" alt="总结" width="300"></li></ul> 
<h4><a id="_FCN_1188"></a>全连接卷积神经网络 FCN</h4> 
<p>  本节代码文件在源代码文件的chapter_computer-vision/fcn.ipynb中</p> 
<ul><li><strong>全连接卷积神经网络（FCN）</strong><br>    <img src="https://images2.imgbox.com/33/5f/BJzhOzC3_o.png" alt="全连接卷积神经网络（FCN）" width="500"><br>    <img src="https://images2.imgbox.com/42/01/zJxuEv8E_o.png" alt="全连接卷积神经网络（FCN）" width="400"></li></ul> 
<h4><a id="_1195"></a>样式迁移</h4> 
<p>  本节代码文件在源代码文件的chapter_computer-vision/neural-style.ipynb中</p> 
<ul><li><strong>样式迁移</strong><br>    <img src="https://images2.imgbox.com/5b/b1/sOCnFHJl_o.png" alt="样式迁移" width="500"><br>    <img src="https://images2.imgbox.com/26/e9/4Jw3NKwL_o.png" alt="样式迁移" width="600"></li><li><strong>基于CNN的样式迁移</strong><br>    <img src="https://images2.imgbox.com/0d/fe/s4i5YXOQ_o.png" alt="基于CNN的样式迁移" width="500"></li></ul> 
<h3><a id="_1202"></a>循环神经网络</h3> 
<h4><a id="_1203"></a>序列模型</h4> 
<p>  本节代码文件在源代码文件的chapter_recurrent-neural-networks/sequence.ipynb中</p> 
<ul><li><strong>序列数据</strong><br>    <img src="https://images2.imgbox.com/a4/09/XNvo60of_o.png" alt="序列数据" width="350"></li><li><strong>序列数据 - 更多例子</strong><br>    <img src="https://images2.imgbox.com/9e/2e/pkR9TW6k_o.png" alt="序列数据 - 更多例子" width="400"></li><li><strong>统计工具</strong><br>    <img src="https://images2.imgbox.com/78/b6/ZismFCab_o.png" alt="统计工具" width="400"><br>    <img src="https://images2.imgbox.com/08/01/UyyM9vJ2_o.png" alt="统计工具" width="400"></li><li><strong>序列模型</strong><br>    <img src="https://images2.imgbox.com/e7/22/fyjYmIVM_o.png" alt="序列模型" width="500"><br>    <img src="https://images2.imgbox.com/79/a6/OVIq4pOO_o.png" alt="序列模型" width="500"><br>    <img src="https://images2.imgbox.com/50/8b/37wK6h4t_o.png" alt="序列模型" width="500"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/3f/49/GkXYo4RQ_o.png" alt="总结" width="400"></li></ul> 
<h4><a id="_1218"></a>语言模型</h4> 
<p>  本节代码文件在源代码文件的chapter_recurrent-neural-networks/language-models-and-dataset.ipynb中</p> 
<ul><li><strong>语言模型</strong><br>    <img src="https://images2.imgbox.com/cc/26/uyonlw0R_o.png" alt="语言模型" width="400"><br>    <img src="https://images2.imgbox.com/74/e7/DyA0rCqp_o.png" alt="语言模型" width="500"></li><li><strong>使用计数来建模</strong><br>    <img src="https://images2.imgbox.com/b5/7c/bIQfa1o5_o.png" alt="使用计数来建模" width="500"></li><li><strong>N元语法</strong><br>    <img src="https://images2.imgbox.com/84/34/SfP2Vwc3_o.png" alt="N元语法" width="500"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/db/79/irU7epDz_o.png" alt="总结" width="300"></li></ul> 
<h4><a id="_1229"></a>循环神经网络</h4> 
<p>  本节代码文件在源代码文件的chapter_recurrent-neural-networks/rnn.ipynb中</p> 
<ul><li><strong>潜变量自回归模型</strong><br>    <img src="https://images2.imgbox.com/83/32/SMYKov1R_o.png" alt="潜变量自回归模型" width="450"></li><li><strong>循环神经网络</strong><br>    <img src="https://images2.imgbox.com/09/9e/LFvrUfEo_o.png" alt="循环神经网络" width="450"></li><li><strong>使用循环神经网络的语言模型</strong><br>    <img src="https://images2.imgbox.com/26/e7/mlAplws0_o.png" alt="使用循环神经网络的语言模型" width="500"></li><li><strong>困惑度（perplexity）</strong><br>    <img src="https://images2.imgbox.com/b6/cd/csUZgaOB_o.png" alt="困惑度（perplexity）" width="350"></li><li><strong>梯度裁剪</strong><br>    <img src="https://images2.imgbox.com/1c/fa/JpCBgl1f_o.png" alt="梯度裁剪" width="450"></li><li><strong>更多的应用RNNs</strong><br>    <img src="https://images2.imgbox.com/73/63/HAnOz5qf_o.png" alt="更多的应用RNNs" width="500"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/4d/24/4yPW1pdA_o.png" alt="总结" width="350"></li></ul> 
<h3><a id="_1245"></a>现代循环神经网络</h3> 
<h4><a id="GRU_1246"></a>门控循环单元GRU</h4> 
<p>  本节代码文件在源代码文件的chapter_recurrent-modern/gru.ipynb中</p> 
<ul><li><strong>门控循环单元GRU</strong><br>    <img src="https://images2.imgbox.com/02/05/H0JRwaEE_o.png" alt="门控循环单元GRU" width="500"></li><li><strong>关注一个序列</strong><br>    <img src="https://images2.imgbox.com/b4/46/RLLg7HlO_o.png" alt="关注一个序列" width="500"></li><li><strong>门</strong><br>    <img src="https://images2.imgbox.com/a1/f0/0lrPFoo9_o.png" alt="门" width="400"></li><li><strong>候选隐状态</strong><br>    <img src="https://images2.imgbox.com/39/0c/5yA1TeEd_o.png" alt="候选隐状态" width="400"></li><li><strong>隐状态</strong><br>    <img src="https://images2.imgbox.com/7e/7e/3sp6Bjtm_o.png" alt="隐状态" width="500"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/d3/0b/UI1bSQ1q_o.png" alt="总结" width="500"></li></ul> 
<h4><a id="LSTM_1260"></a>长短期记忆网络（LSTM）</h4> 
<p>  本节代码文件在源代码文件的chapter_recurrent-modern/lstm.ipynb中</p> 
<ul><li><strong>长短期记忆网络</strong><br>    <img src="https://images2.imgbox.com/1e/cb/rHJbH2mH_o.png" alt="长短期记忆网络" width="280"></li><li><strong>门</strong><br>    <img src="https://images2.imgbox.com/f8/e3/TT6GATF7_o.png" alt="门" width="400"></li><li><strong>候选记忆单元</strong><br>    <img src="https://images2.imgbox.com/48/10/BMiduSle_o.png" alt="候选记忆单元" width="400"></li><li><strong>记忆单元</strong><br>    <img src="https://images2.imgbox.com/5a/ce/tmN5td8D_o.png" alt="记忆单元" width="400"></li><li><strong>隐状态</strong><br>    <img src="https://images2.imgbox.com/d8/41/ctUXpjKB_o.png" alt="隐状态" width="400"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/68/3a/aAiV1UTc_o.png" alt="总结" width="600"></li></ul> 
<h4><a id="_1274"></a>深度循环神经网络</h4> 
<p>  本节代码文件在源代码文件的chapter_recurrent-modern/deep-rnn.ipynb中</p> 
<ul><li><strong>回顾：循环神经网络</strong><br>    <img src="https://images2.imgbox.com/3c/43/vH4Fk4V1_o.png" alt="回顾：循环神经网络" width="450"></li><li><strong>Plan A - Nonlinearity in the units</strong><br>    <img src="https://images2.imgbox.com/75/ad/qmAvB90p_o.png" alt="Plan A - Nonlinearity in the units" width="500"></li><li><strong>更深</strong><br>    <img src="https://images2.imgbox.com/dd/52/K58pFVsd_o.png" alt="更深" width="500"><br>    <img src="https://images2.imgbox.com/75/8a/D9agyTaH_o.png" alt="更深" width="450"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/37/c2/6cywcEli_o.png" alt="总结" width="350"></li></ul> 
<h4><a id="_1285"></a>双向循环神经网络</h4> 
<p>  本节代码文件在源代码文件的chapter_recurrent-modern/bi-rnn.ipynb中</p> 
<ul><li><strong>未来很重要</strong><br>    <img src="https://images2.imgbox.com/18/ca/PODKYhtk_o.png" alt="未来很重要" width="450"><br>    <img src="https://images2.imgbox.com/7a/44/R0EhYtdX_o.png" alt="未来很重要" width="450"></li><li><strong>双向RNN</strong><br>    <img src="https://images2.imgbox.com/38/7a/8AANl1Cl_o.png" alt="双向RNN" width="550"><br>    <img src="https://images2.imgbox.com/e4/d4/z0I8zR3Q_o.png" alt="双向RNN" width="550"></li><li><strong>推理</strong><br>    <img src="https://images2.imgbox.com/55/01/CCoI5g5G_o.png" alt="推理" width="500"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/03/16/NNVVeSbf_o.png" alt="总结" width="350"></li></ul> 
<h4><a id="_1297"></a>编码器-解码器</h4> 
<p>  本节代码文件在源代码文件的chapter_recurrent-modern/encoder-decoder.ipynb中</p> 
<ul><li><strong>重新考察CNN</strong><br>    <img src="https://images2.imgbox.com/2e/7c/W60rKzFZ_o.png" alt="重新考察CNN" width="500"></li><li><strong>重新考察RNN</strong><br>    <img src="https://images2.imgbox.com/a3/d7/6QjaIWou_o.png" alt="重新考察CNN" width="400"></li><li><strong>编码器-解码器架构</strong><br>    <img src="https://images2.imgbox.com/4d/c5/cExVHSOK_o.png" alt="编码器-解码器架构" width="400"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/55/f6/PFjzknZU_o.png" alt="总结" width="500"></li></ul> 
<h4><a id="_1307"></a>序列到序列学习</h4> 
<p>  本节代码文件在源代码文件的chapter_recurrent-modern/seq2seq.ipynb中</p> 
<ul><li><strong>序列到序列学习（seq2seq）</strong><br>    <img src="https://images2.imgbox.com/5d/bc/M54dpK0m_o.png" alt="序列到序列学习（seq2seq）" width="450"></li><li><strong>机器翻译</strong><br>    <img src="https://images2.imgbox.com/68/a5/W1xhZBiQ_o.png" alt="机器翻译" width="500"></li><li><strong>Seq2seq</strong><br>    <img src="https://images2.imgbox.com/5c/90/cF8AhHS1_o.png" alt="Seq2seq" width="500"></li><li><strong>编码器-解码器细节</strong><br>    <img src="https://images2.imgbox.com/ac/05/bsONtA2Z_o.png" alt="编码器-解码器细节" width="500"></li><li><strong>训练</strong><br>    <img src="https://images2.imgbox.com/3d/80/D7Wo4mEe_o.png" alt="训练" width="500"></li><li><strong>衡量生成序列的好坏的BLEU</strong><br>    <img src="https://images2.imgbox.com/82/c1/B8aCTiEf_o.png" alt="衡量生成序列的好坏的BLEU" width="400"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/68/b4/IxQlYhNZ_o.png" alt="总结" width="300"></li></ul> 
<h4><a id="_1323"></a>束搜索</h4> 
<p>  本节代码文件在源代码文件的chapter_recurrent-modern/beam-search.ipynb中</p> 
<ul><li><strong>贪心搜索</strong><br>    <img src="https://images2.imgbox.com/9e/d7/pV04XVSw_o.png" alt="贪心搜索" width="500"></li><li><strong>穷举搜索</strong><br>    <img src="https://images2.imgbox.com/6f/5d/f4pFiVGt_o.png" alt="穷举搜索" width="500"></li><li><strong>束搜索</strong><br>    <img src="https://images2.imgbox.com/93/37/i4skAb6m_o.png" alt="束搜索" width="500"><br>    <img src="https://images2.imgbox.com/c8/17/YcPF9evr_o.png" alt="束搜索" width="450"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/9d/76/AYZ32PIv_o.png" alt="总结" width="350"></li></ul> 
<h4><a id="_1334"></a>注意力机制</h4> 
<p>  本节代码文件在源代码文件的chapter_attention-mechanisms/attention-cues.ipynb中</p> 
<ul><li><strong>心理学</strong><br>    <img src="https://images2.imgbox.com/2a/9e/8FXSMoHR_o.png" alt="心理学" width="450"></li><li><strong>注意力机制</strong><br>    <img src="https://images2.imgbox.com/e8/a6/AbjDk55r_o.png" alt="注意力机制" width="550"></li><li><strong>非参注意力池化层</strong><br>    <img src="https://images2.imgbox.com/d3/8f/xV0YgwEc_o.png" alt="非参注意力池化层" width="500"></li><li><strong>Nadaraya-Watson核回归</strong><br>    <img src="https://images2.imgbox.com/1d/e1/PNLDA8z4_o.png" alt="Nadaraya-Watson核回归" width="300"></li><li><strong>参数化的注意力机制</strong><br>    <img src="https://images2.imgbox.com/03/da/UqIXYadi_o.png" alt="参数化的注意力机制" width="350"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/3b/a0/P63LRYM5_o.png" alt="总结" width="450"></li></ul> 
<h4><a id="_1348"></a>注意力分数</h4> 
<p>  本节代码文件在源代码文件的chapter_attention-mechanisms/attention-scoring-functions.ipynb中</p> 
<ul><li><strong>注意力分数</strong><br>    <img src="https://images2.imgbox.com/d7/1a/Dam3ikM5_o.png" alt="注意力分数" width="450"></li><li><strong>拓展到高维度</strong><br>    <img src="https://images2.imgbox.com/06/a0/OiwSWvvj_o.png" alt="拓展到高维度" width="500"></li><li><strong>Additive Attention</strong><br>    <img src="https://images2.imgbox.com/c1/d0/KwZeu83w_o.png" alt="Additive Attention" width="500"></li><li><strong>Scaled Dot-Product Attention</strong><br>    <img src="https://images2.imgbox.com/27/e4/EVS7TmXl_o.png" alt="Scaled Dot-Product Attention" width="500"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/4e/09/gdPVx4wE_o.png" alt="总结" width="500"></li></ul> 
<h4><a id="seq2seq_1360"></a>使用注意力机制的seq2seq</h4> 
<p>  本节代码文件在源代码文件的chapter_attention-mechanisms/bahdanau-attention.ipynb中</p> 
<ul><li><strong>动机</strong><br>    <img src="https://images2.imgbox.com/87/8c/XM92Geyf_o.png" alt="动机" width="500"></li><li><strong>加入注意力</strong><br>    <img src="https://images2.imgbox.com/e4/38/KAx3K84P_o.png" alt="加入注意力" width="500"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/ce/bd/P3g7tAqk_o.png" alt="总结" width="500"></li></ul> 
<h4><a id="_1368"></a>自注意力</h4> 
<p>  本节代码文件在源代码文件的chapter_attention-mechanisms/self-attention-and-positional-encoding.ipynb中</p> 
<ul><li><strong>自注意力</strong><br>    <img src="https://images2.imgbox.com/62/a1/C6mwgkRt_o.png" alt="自注意力" width="500"></li><li><strong>跟CNN，RNN对比</strong><br>    <img src="https://images2.imgbox.com/0d/c5/dKVMbrMo_o.png" alt="跟CNN，RNN对比" width="600"></li><li><strong>位置编码</strong><br>    <img src="https://images2.imgbox.com/2d/50/51JAKvam_o.png" alt="位置编码" width="500"></li><li><strong>位置编码矩阵</strong><br>    <img src="https://images2.imgbox.com/15/7a/mg48FYQl_o.png" alt="位置编码矩阵" width="600"></li><li><strong>绝对位置信息</strong><br>    <img src="https://images2.imgbox.com/b4/e8/sjzqsl0H_o.png" alt="绝对位置信息" width="500"><br>    <img src="https://images2.imgbox.com/14/36/uhbFwwdo_o.png" alt="绝对位置信息" width="550"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/56/af/ZtLgpgQV_o.png" alt="总结" width="350"></li></ul> 
<h4><a id="Transformer_1383"></a>Transformer</h4> 
<p>  本节代码文件在源代码文件的chapter_attention-mechanisms/transformer.ipynb中</p> 
<ul><li><strong>Transformer架构</strong><br>    <img src="https://images2.imgbox.com/ca/1e/YtqYixyx_o.png" alt="Transformer架构" width="550"></li><li><strong>多头注意力</strong><br>    <img src="https://images2.imgbox.com/fd/8c/3MZeXnRc_o.png" alt="多头注意力" width="550"><br>    <img src="https://images2.imgbox.com/e3/86/qQcGuzpP_o.png" alt="多头注意力" width="550"></li><li><strong>有掩码的多头注意力</strong><br>    <img src="https://images2.imgbox.com/d2/eb/v6giUgLA_o.png" alt="有掩码的多头注意力" width="550"></li><li><strong>基于位置的前馈网络</strong><br>    <img src="https://images2.imgbox.com/1e/7e/1RcnSW7L_o.png" alt="基于位置的前馈网络" width="550"></li><li><strong>层归一化</strong><br>    <img src="https://images2.imgbox.com/4a/c5/ra0BrkMZ_o.png" alt="层归一化" width="550"></li><li><strong>信息传递</strong><br>    <img src="https://images2.imgbox.com/fe/e1/z9rKvaBp_o.png" alt="信息传递" width="650"></li><li><strong>预测</strong><br>    <img src="https://images2.imgbox.com/96/e3/P03wTqEC_o.png" alt="预测" width="500"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/60/d2/OXM4sGLx_o.png" alt="总结" width="500"></li></ul> 
<h3><a id="_1402"></a>自然语言处理：预训练</h3> 
<h4><a id="BERT_1403"></a>BERT预训练</h4> 
<p>  本节代码文件在源代码文件的chapter_natural-language-processing-pretraining/bert-pretraining.ipynb中</p> 
<ul><li><strong>NLP里的迁移学习</strong><br>    <img src="https://images2.imgbox.com/04/c3/xnesgqpG_o.png" alt="NLP里的迁移学习" width="400"></li><li><strong>BERT的动机</strong><br>    <img src="https://images2.imgbox.com/ae/41/UypcVny2_o.png" alt="BERT的动机" width="500"></li><li><strong>BERT架构</strong><br>    <img src="https://images2.imgbox.com/f9/bb/dTmf5deU_o.png" alt="BERT架构" width="450"></li><li><strong>对输入的修改</strong><br>    <img src="https://images2.imgbox.com/a5/26/C6gmq5dy_o.png" alt="对输入的修改" width="500"></li><li><strong>预训练任务1：带掩码的语言模型</strong><br>    <img src="https://images2.imgbox.com/00/ac/QmEVEhQR_o.png" alt="预训练任务1：带掩码的语言模型" width="500"></li><li><strong>预训练任务2：下一句子预测</strong><br>    <img src="https://images2.imgbox.com/b7/f6/PcGOn3Bb_o.png" alt="预训练任务2：下一句子预测" width="500"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/a2/9e/YvWebtJk_o.png" alt="总结" width="400"></li></ul> 
<h3><a id="_1419"></a>自然语言处理：应用</h3> 
<h4><a id="BERT_1420"></a>BERT微调</h4> 
<p>  本节代码文件在源代码文件的chapter_natural-language-processing-applications/finetuning-bert.ipynb中</p> 
<ul><li><strong>微调 Bert</strong><br>    <img src="https://images2.imgbox.com/56/f2/Hzhk4SXP_o.png" alt="微调bert" width="500"></li><li><strong>句子分类</strong><br>    <img src="https://images2.imgbox.com/a5/ed/UNC6FIRx_o.png" alt="句子分类" width="500"></li><li><strong>命名实体识别</strong><br>    <img src="https://images2.imgbox.com/95/cc/9JWoj9yk_o.png" alt="命名实体识别" width="500"></li><li><strong>问题回答</strong><br>    <img src="https://images2.imgbox.com/27/40/xPEgNTSu_o.png" alt="问题回答" width="500"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/6d/42/8gA3zKYP_o.png" alt="总结" width="500"></li></ul> 
<h3><a id="_1432"></a>优化算法</h3> 
<h4><a id="_1433"></a>优化算法</h4> 
<p>  本节代码文件在源代码文件的chapter_optimization/optimization-intro.ipynb中</p> 
<ul><li><strong>优化问题</strong><br>    <img src="https://images2.imgbox.com/ef/7d/a7xZOh1o_o.png" alt="优化问题" width="500"></li><li><strong>局部最小 vs 全局最小</strong><br>    <img src="https://images2.imgbox.com/53/23/2WMQVw4K_o.png" alt="局部最小 vs 全局最小" width="500"></li><li><strong>凸集</strong><br>    <img src="https://images2.imgbox.com/0f/4a/XsxYzD2j_o.png" alt="凸集" width="600"></li><li><strong>凸函数</strong><br>    <img src="https://images2.imgbox.com/52/18/wLSBOlZJ_o.png" alt="凸函数" width="650"></li><li><strong>凸函数优化</strong><br>    <img src="https://images2.imgbox.com/00/e2/M73k18sz_o.png" alt="凸函数优化" width="550"></li><li><strong>凸和非凸例子</strong><br>    <img src="https://images2.imgbox.com/27/a2/XVGusAPn_o.png" alt="凸和非凸例子" width="550"></li><li><strong>梯度下降</strong><br>    <img src="https://images2.imgbox.com/60/8b/vd8YQzzP_o.png" alt="梯度下降" width="550"></li><li><strong>随机梯度下降</strong><br>    <img src="https://images2.imgbox.com/64/10/aHxyZpHX_o.png" alt="随机梯度下降" width="550"></li><li><strong>小批量随机梯度下降</strong><br>    <img src="https://images2.imgbox.com/3c/39/Y0wqxXip_o.png" alt="小批量随机梯度下降" width="550"></li><li><strong>冲量法</strong><br>    <img src="https://images2.imgbox.com/2b/d3/SJPUc1Hf_o.png" alt="冲量法" width="550"></li><li><strong>Adam</strong><br>    <img src="https://images2.imgbox.com/df/63/1L0I0QBb_o.png" alt="Adam" width="450"><br>    <img src="https://images2.imgbox.com/13/f3/2s8ijkeW_o.png" alt="Adam" width="450"></li><li><strong>总结</strong><br>    <img src="https://images2.imgbox.com/5f/da/KsiJm9Lf_o.png" alt="总结" width="450"></li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/631cec76cde0c46406fd4fe66dc21a55/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">C&#43;&#43; Lambda表达式的常见用法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d256fe38a33570b45fee932ca998f3bf/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python爬虫完整代码拿走不谢</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>