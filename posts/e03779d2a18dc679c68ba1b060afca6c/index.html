<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LLAma-Factory框架详细使用方法-0.8.3版本 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/e03779d2a18dc679c68ba1b060afca6c/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="LLAma-Factory框架详细使用方法-0.8.3版本">
  <meta property="og:description" content="一、安装 LLaMA Factory git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git #拉取代码到本地
cd LLaMA-Factory #进入目录
conda create -n llamafactory python=3.11 #创建python环境
conda activate llamafactory #进入conda环境 pip install -e &#34;.[torch,metrics]&#34; #下载环境包
可选的额外依赖项：torch、torch-npu、metrics、deepspeed、bitsandbytes、hqq、eetq、gptq、awq、aqlm、vllm、galore、badam、qwen、modelscope、quality 因为我们后续单机多卡会用到deepspeed，所以我们可以提前下载
pip install deepspeed
当我们安装好环境后，可以看一下我们的llama-factory的版本，命令： llamafactory-cli env
二、LLaMA Board 他是一个可视化的web页面，可以手动去配置参数，可进行推理，微调，非常的方便，由gradio驱动。
启动命令：
llamafactory-cli webui 我们也可以在启动之前限制显卡等等数量，启动命令如下：
export CUDA_VISIBLE_DEVICES=1,2,3,4 llamafactory-cli webui
打开链接后的页面展示
每个参数的作用这里我就不具体解释了，网上一搜一大把，如果是新手，我建议去bilibili搜一个LLaMA Board参数讲解视频细心看完！！！
2.1 inference 下面我们来做一个简单的推理测试，我本次的测试模型是qwen2-7b-instruct
1.第一步先点击chat
2.第二部选择你的模型名称
3.粘贴模型路径
2.2 train 首先选择模型和数据还有微调方式
然后选择保存模型的位置
注：如果你的模型比较大，一台显卡无法加载你的模型，你可以选择使用deepspeed单机多卡，DeepSpeed stage选择zero3，如果你一张显卡能够微调，但你有多张显卡的话，这里建议你使用zero2，可以提高接近两倍的训练速度，也就是节省一半的时间
可以看到后台正在训练
前端也可以看到loss在下降，说明模型在正常训练
2.3 train_xinference 当我们训练好模型后，我们可以先测试一下模型的后坏，如果认为模型没问题的话我们再合并模型。
这是我们微调后模型的参数的保存位置
我们进入checkpoint-45，然后复制模型路径，粘贴到检查点路径位置上，然后点击加载模型
2.4 merge_model 我们只需要配置下面箭头指向的几个位置就可以了">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-10T08:47:44+08:00">
    <meta property="article:modified_time" content="2024-07-10T08:47:44+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">LLAma-Factory框架详细使用方法-0.8.3版本</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>一、安装 LLaMA Factory</h2> 
<blockquote> 
 <p>git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git   #拉取代码到本地<br> cd LLaMA-Factory          #进入目录</p> 
 <p>conda create -n llamafactory python=3.11   #创建python环境</p> 
 <p>conda activate llamafactory       #进入conda环境 <br> pip install -e ".[torch,metrics]"     #下载环境包</p> 
 <p>可选的额外依赖项：torch、torch-npu、metrics、deepspeed、bitsandbytes、hqq、eetq、gptq、awq、aqlm、vllm、galore、badam、qwen、modelscope、quality </p> 
 <p>因为我们后续单机多卡会用到deepspeed，所以我们可以提前下载</p> 
 <p>pip install deepspeed</p> 
</blockquote> 
<p>当我们安装好环境后，可以看一下我们的llama-factory的版本，命令： <span style="color:#1c7331;">llamafactory-cli env</span></p> 
<p><img alt="" height="147" src="https://images2.imgbox.com/4a/8f/M5iDH1KK_o.png" width="351"></p> 
<h2>二、LLaMA Board</h2> 
<p>他是一个可视化的web页面，可以手动去配置参数，可进行推理，微调，非常的方便，由gradio驱动。</p> 
<p>启动命令：</p> 
<blockquote> 
 <p>llamafactory-cli webui </p> 
 <p>我们也可以在启动之前限制显卡等等数量，启动命令如下：<br> export CUDA_VISIBLE_DEVICES=1,2,3,4 </p> 
 <p>llamafactory-cli webui</p> 
</blockquote> 
<p><img alt="" height="168" src="https://images2.imgbox.com/28/78/AxTwUaOC_o.png" width="400"></p> 
<p>打开链接后的页面展示<br><img alt="" height="890" src="https://images2.imgbox.com/79/21/NvAGUWAC_o.png" width="1200"></p> 
<p>每个参数的作用这里我就不具体解释了，网上一搜一大把，如果是新手，我建议去bilibili搜一个LLaMA Board参数讲解视频细心看完！！！</p> 
<h3>2.1 inference</h3> 
<p>下面我们来做一个简单的推理测试，我本次的测试模型是qwen2-7b-instruct</p> 
<p>1.第一步先点击chat</p> 
<p>2.第二部选择你的模型名称</p> 
<p>3.粘贴模型路径</p> 
<p><img alt="" height="755" src="https://images2.imgbox.com/39/c6/5P4Eo2Av_o.png" width="1200"></p> 
<h3>2.2 train</h3> 
<p>首先选择模型和数据还有微调方式</p> 
<p><img alt="" height="604" src="https://images2.imgbox.com/79/04/F36YMGyH_o.png" width="1200"></p> 
<p>然后选择保存模型的位置<br><span style="color:#fe2c24;">注：如果你的模型比较大，一台显卡无法加载你的模型，你可以选择使用deepspeed单机多卡，DeepSpeed stage选择zero3，如果你一张显卡能够微调，但你有多张显卡的话，这里建议你使用zero2，可以提高接近两倍的训练速度，也就是节省一半的时间</span></p> 
<p><img alt="" height="517" src="https://images2.imgbox.com/c9/00/bSKczw3d_o.png" width="1200"></p> 
<p>可以看到后台正在训练</p> 
<p><img alt="" height="630" src="https://images2.imgbox.com/bf/ba/2fPcTwhO_o.png" width="1200"></p> 
<p>前端也可以看到loss在下降，说明模型在正常训练</p> 
<p><img alt="" height="536" src="https://images2.imgbox.com/d3/1f/QlhOwVlk_o.png" width="1200"></p> 
<h3>2.3 train_xinference</h3> 
<p>当我们训练好模型后，我们可以先测试一下模型的后坏，如果认为模型没问题的话我们再合并模型。</p> 
<p>这是我们微调后模型的参数的保存位置</p> 
<p><img alt="" height="564" src="https://images2.imgbox.com/57/2a/8FvQGCdh_o.png" width="1078"></p> 
<p>我们进入checkpoint-45，然后复制模型路径，粘贴到检查点路径位置上，然后点击加载模型</p> 
<p><img alt="" height="762" src="https://images2.imgbox.com/56/30/THm5ZFCa_o.png" width="1200"></p> 
<h3>2.4 merge_model</h3> 
<p>我们只需要配置下面箭头指向的几个位置就可以了<br><span style="color:#ffd900;">注：模型分块不要太大，太大的话会按默认参数，不会按你设置的参数去分块</span></p> 
<p><img alt="" height="841" src="https://images2.imgbox.com/36/bb/Bfu6cpGl_o.png" width="1200"></p> 
<h2>三、命令形式</h2> 
<h3>3.1 inference</h3> 
<h4>3.1.1 cli-inference</h4> 
<blockquote> 
 <p>首先进入inference的参数文件配置路径<br> cd ~/LLaMA-Factory/examples/inference</p> 
 <p>然后查看llama.yaml文件的内容</p> 
 <p><img alt="" height="64" src="https://images2.imgbox.com/4b/d3/OLdEj3k1_o.png" width="1034"></p> 
 <p>然后复制内容，内容如下</p> 
 <p><span style="color:#a2e043;">model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct<br> template: llama3</span><br> 再新建一个yaml的文件，把内容换成你的模型路径和template<br> 格式如下<br><span style="color:#a2e043;">model_name_or_path: </span>模型路径<br><span style="color:#a2e043;">template:选择见下图</span></p> 
</blockquote> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/c9/48/A6TIhlZK_o.png" width="807"></p> 
<p>我还是那Qwen2举例子，下图是我的参数配置</p> 
<p><img alt="" height="61" src="https://images2.imgbox.com/1a/43/RTK3uONz_o.png" width="1194"></p> 
<p>然后运行推理命令</p> 
<blockquote> 
 <p>CUDA_VISIBLE_DEVICES=2 llamafactory-cli chat examples/inference/qwen2_7b_instruct.yaml<br>  </p> 
</blockquote> 
<p><span style="color:#fe2c24;">注：主要执行命令时自己的所在路径！！！！！！</span><br><img alt="" height="41" src="https://images2.imgbox.com/29/db/CPKbIuJi_o.png" width="1200"></p> 
<p><img alt="" height="310" src="https://images2.imgbox.com/f1/84/nsgUrU4M_o.png" width="1200"></p> 
<h4>3.1.2 webchat-inference</h4> 
<p>现在我们来演示一下web推理加载方式</p> 
<blockquote> 
 <p>llamafactory-cli webchat examples/inference/qwen2_7b_instruct.yaml</p> 
</blockquote> 
<p><img alt="" height="465" src="https://images2.imgbox.com/b7/5c/wqjLZ89b_o.png" width="1200"></p> 
<p><img alt="" height="988" src="https://images2.imgbox.com/cb/33/0gW249j4_o.png" width="1200"></p> 
<h3>3.2 train</h3> 
<h4>3.2.1 single_train</h4> 
<p>如果你只要一台显卡，那么就使用单卡训练</p> 
<p>首先进入路径，修改单卡训练的文件配置</p> 
<blockquote> 
 <p>cd ~/LLaMA-Factory/examples/train_lora</p> 
 <p>尽量不要在原文件上修改，我们自己创建一个yaml文件，然后按llama3_lora_sft.yaml文件的格式进行修改，这次我们拿glm4-9B-chat举例子<br> 具体参数配置如下，有些参数如果你理解的话也可以自行更改，我这里用的是官方的数据，如果想用自己构建的数据集，需要到data目录下修改dataset_info.json文件</p> 
 <p>然后启动训练命令</p> 
 <p>CUDA_VISIBLE_DEVICES=0 llamafactory-cli train examples/train_lora/glm4_lora_sft.yaml</p> 
</blockquote> 
<p><img alt="" height="832" src="https://images2.imgbox.com/a7/63/VrIT35qm_o.png" width="979"></p> 
<p>下面就开始训练了！</p> 
<p><img alt="" height="817" src="https://images2.imgbox.com/aa/5a/EeflerL1_o.png" width="1200"></p> 
<h4>3.2.2 multi_train</h4> 
<p>如果你的模型参数比较大，一张卡无法加载你的模型参数，那么你就需要使用单机多卡来训练模型，你可选择deepspeed的zero3，如果你的一张显卡能够加载模型参数，你又有多张卡，可以选择zero2，使数据并行，这样可以大大减少训练时间。</p> 
<p>加入我现在使用的是qwen2-72b-instruct模型，一张卡无法加载模型参数，这个时候我们必须使用deepspeed的zero3，我们就新建一个yaml文件，参考llama3_lora_sft_ds3.yaml文件内容。</p> 
<blockquote> 
 <p>vim glm4_lora_sft_ds3.yaml</p> 
 <p>然后启动训练命令</p> 
 <p>export CUDA_VISIBLE_DEVICES=2,3</p> 
 <p>llamafactory-cli train examples/train_lora/glm4_lora_sft_ds3.yaml</p> 
</blockquote> 
<p><img alt="" height="829" src="https://images2.imgbox.com/28/75/BeLp7OOv_o.png" width="744"></p> 
<p>模型开始训练了！</p> 
<p><img alt="" height="685" src="https://images2.imgbox.com/19/d4/xOPGtDSx_o.png" width="1200"></p> 
<h3>3.3 train_inference</h3> 
<p>当我们训练好模型参数后，可以先验证一下训练参数的好坏再进行模型合并</p> 
<p>进入~/LLaMA-Factory/examples/inference路径下，新建一个yaml文件，参照llama3_lora_sft.yaml修改文件配置<br><img alt="" height="107" src="https://images2.imgbox.com/9f/53/GNNHo8Dn_o.png" width="1050"></p> 
<blockquote> 
 <p>cd ~/LLaMA-Factory/examples/inference</p> 
 <p>export CUDA_VISIBLE_DEVICES=1,2</p> 
 <p>llamafactory-cli webchat examples/inference/glm4_lora_sft.yaml</p> 
</blockquote> 
<p><img alt="" height="229" src="https://images2.imgbox.com/bf/15/Nktyu4fP_o.png" width="1200"></p> 
<p>单机url，改成自己的IP就可以打开web页面进行模型测试了<br><img alt="" height="575" src="https://images2.imgbox.com/19/dd/x1Gjcy0T_o.png" width="1200"></p> 
<h3>3.4 merge_model</h3> 
<p>进入~/LLaMA-Factory/examples/merge_lora目录下，参考llama3_lora_sft.yaml文件创建并修改glm4_lora_sft.yaml文件参数，如下图</p> 
<p><img alt="" height="302" src="https://images2.imgbox.com/f9/a4/bcntVzW7_o.png" width="1089"></p> 
<p>然后运行合并模型的命令就可以了！！！</p> 
<blockquote> 
 <p>llamafactory-cli export examples/merge_lora/glm4_lora_sft.yaml</p> 
</blockquote> 
<h3>3.5 api_model</h3> 
<p>如果想封装model，然后使用模型的接口，可以使用启动 OpenAI 风格的API</p> 
<blockquote> 
 <p>llamafactory-cli api examples/inference/glm4_lora_sft.yaml</p> 
</blockquote> 
<h2>四、数据格式修改</h2> 
<p> 我们拿两种格式举例，首先我们需要找到dataset_info.json文件的位置，进入~/LLaMA-Factory/data，目录下有个dataset_info.json,我们需要修改他的参数文件。</p> 
<h4>4.1 Alpaca 格式</h4> 
<p>如果你的文件是如下图格式<br><img alt="" height="281" src="https://images2.imgbox.com/02/b5/pb5mHdFj_o.png" width="824"></p> 
<p>就按官方的例子修改</p> 
<p><img alt="" height="957" src="https://images2.imgbox.com/d7/cb/B0aaOW1j_o.png" width="1200"></p> 
<p>例子如下：<br><img alt="" height="209" src="https://images2.imgbox.com/e6/7d/UJ9gVSiB_o.png" width="591"></p> 
<h4>4.2 Sharegpt 格式 </h4> 
<p>如果你的文件格式如下：<br><img alt="" height="467" src="https://images2.imgbox.com/e0/09/aHUK8os1_o.png" width="765"></p> 
<p>官方的说明如下：<br>  <img alt="" height="1200" src="https://images2.imgbox.com/e4/e1/t01WeuWO_o.png" width="1200"></p> 
<p>示例：<br><img alt="" height="381" src="https://images2.imgbox.com/3b/b3/nkQzXpWQ_o.png" width="859"></p> 
<p>只需要修改文件名字和文件路径就可以了！</p> 
<p><span style="color:#a2e043;">完结！！！</span></p> 
<p><span style="color:#a2e043;">如果后期有什么疑问，欢迎大家评论区留言</span></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/53c7003c1a23e8fe9aee78c40bbc82bf/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">vue实现预览编辑ppt、word、pdf、excel、等功能的解决方案（内网-前端）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0419a5ee489abc1cedec8fd35f373fab/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">基于Python的哔哩哔哩数据分析系统设计实现过程，技术使用flask、MySQL、echarts，前端使用Layui</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>