<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【hive】transform脚本 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/1edd6f2a3d1ef83d6911dc633fdbafb6/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【hive】transform脚本">
  <meta property="og:description" content="文档地址：https://cwiki.apache.org/confluence/display/Hive/LanguageManual&#43;Transform
一、介绍二、实现1.脚本上传到本地2.脚本上传到hdfs 三、几个注意点1.脚本名不要写全路径2.using后面语句中，带不带&#34;python&#34;的问题3.py脚本Shebang：#!/usr/bin/env python4.通过约定增强脚本的通用性5.指定python执行包和第三方库路径6.python执行包tar包安装第三方库 一、介绍 和udf差不多的作用，支持用python实现。通过标准输入流从hive读取数据，内部处理完再通过标准输出流将处理结果返回给hive。实现流程上比udf要更简单灵活一些，只需要上传脚本=&gt;add file加载到分布式缓存=&gt;使用。
二、实现 先定义一个名为transform.py的脚本，将传入的两个字段值都&#43;1。
#!/usr/bin/env python import sys for line in sys.stdin: try: x, y = map(float, line.strip().split(&#39;\t&#39;)) x &#43;= 1 y &#43;= 1 print(&#39;\t&#39;.join(map(str, [x, y]))) except ValueError as e: print(&#39;\t&#39;.join([r&#39;\N&#39;] * 2)) 上面对输入流按照\t分隔是因为hive中的数据在传递到py脚本时，多个字段间默认会用\t分隔拼接为字符串，并且空值null会被转为字符串\N。同样将处理结果返回给hive时，如果多个字段，为了hive能够正确解析，也需要用\t拼接输出，单独的\N在hive中也会被重新解释为null。
除了单独的\N会被重新解释为null外，还有一种情况也会被hive解释为null，就是脚本里返回的字段个数小于hive中接收的字段个数时，hive中多余的字段会被赋值为null。
1.脚本上传到本地 这里的本地指的是hive主服务hive server2所在的节点，也就是我们客户端连接的那个机器。
先上传到主服务机器下的某个路径：
# 文件上传路径 [root@node1 HiveLib]# readlink -e transform.py /root/HiveLib/transform.py 上传后通过add file命令将脚本添加到分布式缓存，之后就可以直接使用了。
-- 添加到分布式缓存 add file /root/HiveLib/transform.py; -- 创建一个临时表测试执行 with `table` as ( select &#39;1&#39; as id, &#39;1.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-05T18:10:06+08:00">
    <meta property="article:modified_time" content="2024-06-05T18:10:06+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【hive】transform脚本</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>文档地址：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Transform" rel="nofollow">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Transform</a></p> 
</blockquote> 
<p></p> 
<div class="toc"> 
 <h4> </h4> 
 <ul><li><a href="#_3" rel="nofollow">一、介绍</a></li><li><a href="#_6" rel="nofollow">二、实现</a></li><li><ul><li><a href="#1_24" rel="nofollow">1.脚本上传到本地</a></li><li><a href="#2hdfs_51" rel="nofollow">2.脚本上传到hdfs</a></li></ul> 
  </li><li><a href="#_76" rel="nofollow">三、几个注意点</a></li><li><ul><li><a href="#1_77" rel="nofollow">1.脚本名不要写全路径</a></li><li><a href="#2usingpython_81" rel="nofollow">2.using后面语句中，带不带"python"的问题</a></li><li><a href="#3pyShebangusrbinenv_python_85" rel="nofollow">3.py脚本Shebang：#!/usr/bin/env python</a></li><li><a href="#4_98" rel="nofollow">4.通过约定增强脚本的通用性</a></li><li><a href="#5python_102" rel="nofollow">5.指定python执行包和第三方库路径</a></li><li><a href="#6pythontar_126" rel="nofollow">6.python执行包tar包安装第三方库</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_3"></a>一、介绍</h2> 
<p>和udf差不多的作用，支持用python实现。通过标准输入流从hive读取数据，内部处理完再通过标准输出流将处理结果返回给hive。实现流程上比udf要更简单灵活一些，只需要上传脚本=&gt;add file加载到分布式缓存=&gt;使用。</p> 
<h2><a id="_6"></a>二、实现</h2> 
<p>先定义一个名为<code>transform.py</code>的脚本，将传入的两个字段值都+1。</p> 
<pre><code class="prism language-py"><span class="token comment">#!/usr/bin/env python</span>
<span class="token keyword">import</span> sys

<span class="token keyword">for</span> line <span class="token keyword">in</span> sys<span class="token punctuation">.</span>stdin<span class="token punctuation">:</span>
    <span class="token keyword">try</span><span class="token punctuation">:</span>
        x<span class="token punctuation">,</span> y <span class="token operator">=</span> <span class="token builtin">map</span><span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">,</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">+=</span> <span class="token number">1</span>
        y <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>x<span class="token punctuation">,</span> y<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">except</span> ValueError <span class="token keyword">as</span> e<span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">r'\N'</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<blockquote> 
 <p>上面对输入流按照<code>\t</code>分隔是因为hive中的数据在传递到py脚本时，多个字段间默认会用<code>\t</code>分隔拼接为字符串，并且空值null会被转为字符串<code>\N</code>。同样将处理结果返回给hive时，如果多个字段，为了hive能够正确解析，也需要用<code>\t</code>拼接输出，单独的<code>\N</code>在hive中也会被重新解释为null。<br> <img src="https://images2.imgbox.com/14/ee/MkoCfO5s_o.png" alt="在这里插入图片描述"><br> 除了单独的<code>\N</code>会被重新解释为null外，还有一种情况也会被hive解释为null，就是脚本里返回的字段个数小于hive中接收的字段个数时，hive中<mark>多余的字段</mark>会被赋值为null。</p> 
</blockquote> 
<h3><a id="1_24"></a>1.脚本上传到本地</h3> 
<blockquote> 
 <p>这里的本地指的是hive主服务hive server2所在的节点，也就是我们客户端连接的那个机器。</p> 
</blockquote> 
<p>先上传到主服务机器下的某个路径：</p> 
<pre><code class="prism language-sh"><span class="token comment"># 文件上传路径</span>
<span class="token punctuation">[</span>root@node1 HiveLib<span class="token punctuation">]</span><span class="token comment"># readlink -e transform.py</span>
/root/HiveLib/transform.py
</code></pre> 
<p>上传后通过add file命令将脚本添加到分布式缓存，之后就可以直接使用了。</p> 
<pre><code class="prism language-sql"><span class="token comment">-- 添加到分布式缓存</span>
<span class="token keyword">add</span> <span class="token keyword">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>HiveLib<span class="token operator">/</span>transform<span class="token punctuation">.</span>py<span class="token punctuation">;</span>

<span class="token comment">-- 创建一个临时表测试执行</span>
<span class="token keyword">with</span> <span class="token identifier"><span class="token punctuation">`</span>table<span class="token punctuation">`</span></span> <span class="token keyword">as</span> <span class="token punctuation">(</span>
    <span class="token keyword">select</span> <span class="token string">'1'</span> <span class="token keyword">as</span> id<span class="token punctuation">,</span> <span class="token string">'1.6789'</span> <span class="token keyword">as</span> col1<span class="token punctuation">,</span> <span class="token string">'7.13'</span> <span class="token keyword">as</span> col2
    <span class="token keyword">union</span> <span class="token keyword">all</span>
    <span class="token keyword">select</span> <span class="token string">'2'</span> <span class="token keyword">as</span> id<span class="token punctuation">,</span> <span class="token string">'11.568'</span> <span class="token keyword">as</span> col1<span class="token punctuation">,</span> <span class="token boolean">null</span> <span class="token keyword">as</span> col2
    <span class="token keyword">union</span> <span class="token keyword">all</span>
    <span class="token keyword">select</span> <span class="token string">'3'</span> <span class="token keyword">as</span> id<span class="token punctuation">,</span> <span class="token string">'26.09761'</span> <span class="token keyword">as</span> col1<span class="token punctuation">,</span> <span class="token string">'71.89002'</span> <span class="token keyword">as</span> col2
<span class="token punctuation">)</span>
<span class="token comment">-- as后面接收脚本返回值的字段也可以指定字段类型, eg:(col1 double, col2 double), 省略时默认都是字符串string类型</span>
<span class="token keyword">select</span> transform <span class="token punctuation">(</span>col1<span class="token punctuation">,</span> col2<span class="token punctuation">)</span> <span class="token keyword">using</span> <span class="token string">'transform.py'</span> <span class="token keyword">as</span> <span class="token punctuation">(</span>col1<span class="token punctuation">,</span> col2<span class="token punctuation">)</span> <span class="token keyword">from</span> <span class="token identifier"><span class="token punctuation">`</span>table<span class="token punctuation">`</span></span><span class="token punctuation">;</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/9b/84/ojvH3qaO_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="2hdfs_51"></a>2.脚本上传到hdfs</h3> 
<p>这种方式和本地实现基本一致，只不过需要将脚本上传到hdfs中，add file时后面跟的是hdfs路径。</p> 
<pre><code class="prism language-sh"><span class="token punctuation">[</span>root@node1 HiveLib<span class="token punctuation">]</span><span class="token comment"># hadoop fs -put ./transform.py /user/hive/lib</span>
<span class="token punctuation">[</span>root@node1 HiveLib<span class="token punctuation">]</span><span class="token comment"># hadoop fs -ls /user/hive/lib</span>
Found <span class="token number">2</span> items
-rw-r--r--   <span class="token number">3</span> root supergroup       <span class="token number">4164</span> <span class="token number">2022</span>-12-18 00:48 /user/hive/lib/hive_udf-1.0-SNAPSHOT.jar
-rw-r--r--   <span class="token number">3</span> root supergroup        <span class="token number">257</span> <span class="token number">2024</span>-05-05 <span class="token number">19</span>:13 /user/hive/lib/transform.py
</code></pre> 
<p>sql客户端中执行：</p> 
<pre><code class="prism language-sql"><span class="token comment">-- 脚本路径换为hdfs路径</span>
<span class="token keyword">add</span> <span class="token keyword">file</span> hdfs:<span class="token comment">//node1:8020/user/hive/lib/transform.py;</span>

<span class="token keyword">with</span> <span class="token identifier"><span class="token punctuation">`</span>table<span class="token punctuation">`</span></span> <span class="token keyword">as</span> <span class="token punctuation">(</span>
    <span class="token keyword">select</span> <span class="token string">'1'</span> <span class="token keyword">as</span> id<span class="token punctuation">,</span> <span class="token string">'1.6789'</span> <span class="token keyword">as</span> col1<span class="token punctuation">,</span> <span class="token string">'7.13'</span> <span class="token keyword">as</span> col2
    <span class="token keyword">union</span> <span class="token keyword">all</span>
    <span class="token keyword">select</span> <span class="token string">'2'</span> <span class="token keyword">as</span> id<span class="token punctuation">,</span> <span class="token string">'11.568'</span> <span class="token keyword">as</span> col1<span class="token punctuation">,</span> <span class="token boolean">null</span> <span class="token keyword">as</span> col2
    <span class="token keyword">union</span> <span class="token keyword">all</span>
    <span class="token keyword">select</span> <span class="token string">'3'</span> <span class="token keyword">as</span> id<span class="token punctuation">,</span> <span class="token string">'26.09761'</span> <span class="token keyword">as</span> col1<span class="token punctuation">,</span> <span class="token string">'71.89002'</span> <span class="token keyword">as</span> col2
<span class="token punctuation">)</span>
<span class="token keyword">select</span> transform <span class="token punctuation">(</span>col1<span class="token punctuation">,</span> col2<span class="token punctuation">)</span> <span class="token keyword">using</span> <span class="token string">'transform.py'</span> <span class="token keyword">as</span> <span class="token punctuation">(</span>col1<span class="token punctuation">,</span> col2<span class="token punctuation">)</span> <span class="token keyword">from</span> <span class="token identifier"><span class="token punctuation">`</span>table<span class="token punctuation">`</span></span><span class="token punctuation">;</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/8c/68/1xTHeAT5_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_76"></a>三、几个注意点</h2> 
<h3><a id="1_77"></a>1.脚本名不要写全路径</h3> 
<p>using语句后面指定脚本只写脚本名即可，不要写全路径。全路径的话会报错<code>[08S01][20000] Error while processing statement: FAILED: Execution Error, return code 20000 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. Unable to initialize custom script.</code>，参考<br> <a href="https://stackoverflow.com/questions/15106127/i-met-an-error-when-i-using-hive-transform-features" rel="nofollow">https://stackoverflow.com/questions/15106127/i-met-an-error-when-i-using-hive-transform-features</a>，我也不太理解为什么有这要求，先照做就行。<br> <img src="https://images2.imgbox.com/77/a6/J6Qll2rn_o.png" alt="在这里插入图片描述" width="600"></p> 
<h3><a id="2usingpython_81"></a>2.using后面语句中，带不带"python"的问题</h3> 
<p>这里说的是sql语句中，是<code>using 'transform.py'</code>还是<code>using 'python transform.py'</code>的问题。可以不带python这个关键字，但是前提脚本中必须指定了Shebang，类似于<code>#!/usr/bin/env python</code>这样，指定脚本的解释器。如果指定Shebang，using后面带不带python都可以，如果脚本中没指定，using后面必须带python这个关键字，否则报错。</p> 
<blockquote> 
 <p>看到有人说需要给py脚本<code>chmod +x transform.py</code>赋予可执行权限，实际操作中经过验证本地和hdfs都不需要。</p> 
</blockquote> 
<h3><a id="3pyShebangusrbinenv_python_85"></a>3.py脚本Shebang：#!/usr/bin/env python</h3> 
<blockquote> 
 <p>Shebang（也称为Hashbang）是一个源于Unix系统中的概念，特别是在类Unix操作系统中广泛使用。它是指脚本文件第一行以#!开头的特殊注释行，用于指定该脚本应该由哪个解释器程序来执行。这个名称来源于这两个起始字符—井号（#）和叹号（!）。</p> 
</blockquote> 
<p>主要解释下<code>#!/usr/bin/env python</code>和<code>#!/usr/bin/python</code>的区别。两者都是用来指定该脚本的解释器，但是前者比后者有更好的兼容性，可以理解为：后者是指定了一个固定的解释器路径，虽然多数情况下遵循规范解释器路径会在该目录下，但是并不能保证一定存在。而前者逻辑上等价于<code>env | grep python</code>，它是从当前所有的环境变量中按照一定的优先级顺序去找python解释器，最先找到哪个就用哪个执行，所以可以有效避免路径指定错误的问题，推荐前面这种写法。</p> 
<pre><code class="prism language-sh"><span class="token punctuation">[</span>root@node1 HiveLib<span class="token punctuation">]</span><span class="token comment"># which python</span>
/root/anaconda3/bin/python
<span class="token punctuation">[</span>root@node1 HiveLib<span class="token punctuation">]</span><span class="token comment"># which env</span>
/usr/bin/env
<span class="token punctuation">[</span>root@node1 HiveLib<span class="token punctuation">]</span><span class="token comment"># env | grep python</span>
<span class="token assign-left variable">CONDA_PYTHON_EXE</span><span class="token operator">=</span>/root/anaconda3/bin/python
</code></pre> 
<h3><a id="4_98"></a>4.通过约定增强脚本的通用性</h3> 
<p>hive transform脚本相比udf有个缺点，就是select中如果使用了transform就不能再选择其它字段了，而实际业务需求每次处理的字段并不总是固定。为了避免每次需求变化而导致需要修改脚本，可以在调用时通过通配符<code>*</code>将外部所有参数传递给脚本，脚本处理完再将所有参数返回。这样即使字段发生变化，也只需要在外部sql中动态调整接收脚本返回值的列名即可。这个过程中需要处理的字段在输入输出时只需要按照约定放在一个指定的位置就行了。<br> <code>select transform (*) using 'transform.py' as (col1, col2, ...) from `table`;</code></p> 
<h3><a id="5python_102"></a>5.指定python执行包和第三方库路径</h3> 
<p>将python环境打tar包，上传hdfs：</p> 
<pre><code class="prism language-sh">map@gzdt-map-poi-yingxiang-offline04 venv$ gfs <span class="token parameter variable">-ls</span> /hdfs_path/pyenv
<span class="token number">24</span>/06/05 <span class="token number">16</span>:08:12 INFO fs.LibdfsLoader: Trying to load the libdfs library<span class="token punctuation">..</span>.
<span class="token number">24</span>/06/05 <span class="token number">16</span>:08:12 INFO fs.LibdfsLoader: Loaded the libdfs library
<span class="token number">24</span>/06/05 <span class="token number">16</span>:08:12 INFO fs.DFileSystem: Loaded the libdfs library
Found <span class="token number">2</span> items
<span class="token parameter variable">-rwxrwxrwx</span>   <span class="token number">3</span> poi-yingxiang poi-yingxiang   <span class="token number">87745890</span> <span class="token number">2024</span>-05-11 <span class="token number">18</span>:33 /hdfs_path/pyenv/python3.tar.gz
<span class="token parameter variable">-rwxrwxrwx</span>   <span class="token number">3</span> poi-yingxiang poi-yingxiang   <span class="token number">90376769</span> <span class="token number">2024</span>-06-05 <span class="token number">16</span>:08 /hdfs_path/pyenv/python3_shapely.tar.gz
</code></pre> 
<p>transform脚本中指定使用<code>python3.tar.gz</code>这个包及包内的第三方库作为环境执行：</p> 
<pre><code class="prism language-sql"><span class="token keyword">set</span> <span class="token identifier"><span class="token punctuation">`</span>spark.sql.she.replaceAddResources<span class="token punctuation">`</span></span><span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>		<span class="token comment"># 允许同名资源覆盖替换</span>
<span class="token keyword">add</span> <span class="token keyword">file</span> hdfs:<span class="token comment">//hdfs_path/hive_udf/xy_aoi_fix.py;</span>
<span class="token keyword">add</span> archive hdfs:<span class="token comment">//hdfs_path/pyenv/python3_shapely.tar.gz;</span>

<span class="token keyword">select</span>
    transform<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">using</span> <span class="token string">'PYTHONPATH=python3.tar.gz/lib/python3.9/site-packages python3.tar.gz/bin/python3 xy_aoi_fix.py'</span>
    <span class="token keyword">as</span><span class="token punctuation">(</span>col1<span class="token punctuation">,</span> col2<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
<span class="token keyword">from</span> <span class="token identifier"><span class="token punctuation">`</span>table<span class="token punctuation">`</span></span><span class="token punctuation">;</span>
</code></pre> 
<h3><a id="6pythontar_126"></a>6.python执行包tar包安装第三方库</h3> 
<p>脚本中用到了<code>shapely</code>这个库，sql执行失败，查看spark（hive on spark环境）日志提示该模块未安装。</p> 
<p>解压安装包，进入bin目录下，执行<code>python -m pip install package_name</code>，默认会安装到<code>lib/python/site-packages</code>目录下，安装完重新打包就行。</p> 
<p>解压缩相关命令：</p> 
<blockquote> 
 <p>tar -zcf xxx.tar.gz ./* （压缩当前文件夹下所有）<br> tar -zxf xxx.tar.gz -C ./ （解压到当前文件夹）<br> tar -tf xxx.tar.gz （不解压 只查看压缩包内容）</p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e83c77cdc8127e8479536d41ec8f66a9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">数据结构——(java版)包装类与泛型</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b2c02ef11ae22517ac65dc57475ea488/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">20240606 每日AI必读资讯</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>