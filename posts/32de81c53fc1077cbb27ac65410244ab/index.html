<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【大数据】Spark使用大全:下载安装、RDD操作、JAVA编程、SQL - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/32de81c53fc1077cbb27ac65410244ab/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【大数据】Spark使用大全:下载安装、RDD操作、JAVA编程、SQL">
  <meta property="og:description" content="目录
前言
1.下载安装
2.RDD操作
3.JAVA编程示例
4.Spark SQL
前言 本文是作者大数据系列中的一文，专栏地址：
https://blog.csdn.net/joker_zjn/category_12631789.html?spm=1001.2014.3001.5482
该系列会成体系的聊一聊整个大数据的技术栈，绝对干货，欢迎订阅。
1.下载安装 前置环境：
Hadoop 3.1.3Java JDK 1.8 下载地址：
Downloads | Apache Spark
往下拉找到Spark release archives.
由于前面我们已经搭建好了hadoop环境，所以这里选择with out hadoop的版本。
配置config目录下有一个配置模板spark-env.sh.template:
将这个模板修改或者复制为spark-env.sh然后在里面：
export SPARK_DIST_CLASSPATH=${Hadoop的安装路径/bin classpath}
因为Spark只是个计算引擎，具体要去操作对应的分部署文件系统的，所以将Spark的类路径指向了hadoop。也就是通过这个配置将Spark要操作的数据源设置为了HDFS。
启动：
bin目录下：
./run-exmaple SparkPi
这是一个Spark自带的demo，如果跑起来不报错，说明就没什么问题了。
2.RDD操作 可以用Spark自带的Spark shell来进行RDD操作：
./bin/spark-shell
RDD操作分为两类：
转换，就是只是返回中间数据集的操作。动作，就是有具体单个返回值的操作。 map - 应用于RDD的每个元素，产生一个新的RDD。
val numbersRdd = spark.sparkContext.parallelize(Array(1, 2, 3, 4))
val squaredRdd = numbersRdd.map(x =&gt; x * x) filter - 根据函数条件过滤RDD中的元素。
val evenNumbersRdd = numbersRdd.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-13T14:29:56+08:00">
    <meta property="article:modified_time" content="2024-06-13T14:29:56+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【大数据】Spark使用大全:下载安装、RDD操作、JAVA编程、SQL</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="%E2%80%8B%E7%BC%96%E8%BE%91" style="background-color:transparent;"><img alt="" height="100" src="https://images2.imgbox.com/ab/a0/imY7d0u6_o.png" width="607"></p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E5%89%8D%E8%A8%80-toc" style="margin-left:0px;"><a href="#%E5%89%8D%E8%A8%80" rel="nofollow">前言</a></p> 
<p id="1.%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85-toc" style="margin-left:0px;"><a href="#1.%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85" rel="nofollow">1.下载安装</a></p> 
<p id="2.RDD%E6%93%8D%E4%BD%9C-toc" style="margin-left:0px;"><a href="#2.RDD%E6%93%8D%E4%BD%9C" rel="nofollow">2.RDD操作</a></p> 
<p id="3.JAVA%E7%BC%96%E7%A8%8B%E7%A4%BA%E4%BE%8B-toc" style="margin-left:0px;"><a href="#3.JAVA%E7%BC%96%E7%A8%8B%E7%A4%BA%E4%BE%8B" rel="nofollow">3.JAVA编程示例</a></p> 
<p id="4.Spark%20SQL-toc" style="margin-left:0px;"><a href="#4.Spark%20SQL" rel="nofollow">4.Spark SQL</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E5%89%8D%E8%A8%80">前言</h2> 
<p>本文是作者大数据系列中的一文，专栏地址：</p> 
<p><a href="https://blog.csdn.net/joker_zjn/category_12631789.html?spm=1001.2014.3001.5482" title="https://blog.csdn.net/joker_zjn/category_12631789.html?spm=1001.2014.3001.5482">https://blog.csdn.net/joker_zjn/category_12631789.html?spm=1001.2014.3001.5482</a></p> 
<p>该系列会成体系的聊一聊整个大数据的技术栈，绝对干货，欢迎订阅。</p> 
<h2 id="1.%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85" style="background-color:transparent;">1.下载安装</h2> 
<p>前置环境：</p> 
<ul><li>Hadoop 3.1.3</li><li>Java JDK 1.8</li></ul> 
<p>下载地址：</p> 
<p><a href="https://spark.apache.org/downloads.html" rel="nofollow" title="Downloads | Apache Spark">Downloads | Apache Spark</a></p> 
<p>往下拉找到<a href="https://archive.apache.org/dist/spark/" rel="nofollow" title="Spark release archives">Spark release archives</a>.</p> 
<p>由于前面我们已经搭建好了hadoop环境，所以这里选择with out hadoop的版本。</p> 
<p><img alt="" height="383" src="https://images2.imgbox.com/5c/17/v7UfzQVb_o.png" width="1200"></p> 
<p>配置config目录下有一个配置模板spark-env.sh.template:</p> 
<p>将这个模板修改或者复制为spark-env.sh然后在里面：</p> 
<blockquote> 
 <p>export SPARK_DIST_CLASSPATH=${Hadoop的安装路径/bin classpath}</p> 
</blockquote> 
<p>因为Spark只是个计算引擎，具体要去操作对应的分部署文件系统的，所以将Spark的类路径指向了hadoop。也就是通过这个配置将Spark要操作的数据源设置为了HDFS。</p> 
<p>启动：</p> 
<p>bin目录下：</p> 
<p>./run-exmaple SparkPi</p> 
<p>这是一个Spark自带的demo，如果跑起来不报错，说明就没什么问题了。</p> 
<h2 id="2.RDD%E6%93%8D%E4%BD%9C">2.RDD操作</h2> 
<p>可以用Spark自带的Spark  shell来进行RDD操作：</p> 
<p>./bin/spark-shell</p> 
<p>RDD操作分为两类：</p> 
<ul><li>转换，就是只是返回中间数据集的操作。</li><li>动作，就是有具体单个返回值的操作。</li></ul> 
<p>map - 应用于RDD的每个元素，产生一个新的RDD。</p> 
<blockquote> 
 <p>val numbersRdd = spark.sparkContext.parallelize(Array(1, 2, 3, 4))<br> val squaredRdd = numbersRdd.map(x =&gt; x * x) </p> 
</blockquote> 
<p>filter - 根据函数条件过滤RDD中的元素。</p> 
<blockquote> 
 <p>val evenNumbersRdd = numbersRdd.filter(_ % 2 == 0)</p> 
</blockquote> 
<p>flatMap - 对RDD中的每个元素应用函数并展平结果。</p> 
<blockquote> 
 <p>val wordsRdd = spark.sparkContext.textFile("hdfs://path/to/textfile.txt")<br> val wordsFlatMapped = wordsRdd.flatMap(line =&gt; line.split(" "))</p> 
</blockquote> 
<p>mapPartitions - 对每个分区应用一个函数。</p> 
<blockquote> 
 <p>val incrementedRdd = numbersRdd.mapPartitions(iter =&gt; iter.map(x =&gt; x + 1))</p> 
</blockquote> 
<p>union - 合并两个RDD。</p> 
<blockquote> 
 <p>val rdd1 = spark.sparkContext.parallelize(Array(1, 2))<br> val rdd2 = spark.sparkContext.parallelize(Array(3, 4))<br> val combinedRdd = rdd1.union(rdd2)</p> 
</blockquote> 
<p>distinct - 返回RDD中不重复的元素。</p> 
<blockquote> 
 <p>val uniqueNumbers = numbersRdd.distinct()</p> 
</blockquote> 
<p>join - 对两个键值对RDD进行内连接。</p> 
<blockquote> 
 <p>val rddA = spark.sparkContext.parallelize(Array((1, "a"), (2, "b")))<br> val rddB = spark.sparkContext.parallelize(Array((1, "x"), (3, "y")))<br> val joinedRdd = rddA.join(rddB)</p> 
</blockquote> 
<p>reduce - 通过函数聚合RDD中的所有元素。</p> 
<blockquote> 
 <p>val sum = numbersRdd.reduce(_ + _)</p> 
</blockquote> 
<p>collect - 返回RDD的所有元素到Driver作为数组。</p> 
<blockquote> 
 <p>val allElements = numbersRdd.collect()</p> 
</blockquote> 
<p>count - 返回RDD中元素的数量。</p> 
<blockquote> 
 <p>val count = numbersRdd.count()</p> 
</blockquote> 
<p>first - 返回RDD的第一个元素。</p> 
<blockquote> 
 <p>val firstElement = numbersRdd.first()</p> 
</blockquote> 
<p>take(n) - 返回RDD的前n个元素。</p> 
<blockquote> 
 <p>val topThree = numbersRdd.take(3)</p> 
</blockquote> 
<p>saveAsTextFile - 将RDD的内容保存为文本文件。</p> 
<blockquote> 
 <p>wordsRdd.saveAsTextFile("hdfs://path/to/output")</p> 
</blockquote> 
<p>foreach - 对RDD的每个元素应用函数，常用于副作用操作。</p> 
<blockquote> 
 <p>numbersRdd.foreach(println)</p> 
</blockquote> 
<h2 id="3.JAVA%E7%BC%96%E7%A8%8B%E7%A4%BA%E4%BE%8B" style="background-color:transparent;">3.JAVA编程示例</h2> 
<p>依赖：</p> 
<pre><code>&lt;dependencies&gt;
        &lt;dependency&gt; &lt;!-- Spark dependency --&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.0&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;</code></pre> 
<p> 编码：</p> 
<pre><code class="language-java">import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import scala.Tuple2;

import java.util.Arrays;
import java.util.List;
import java.util.stream.Collectors;

public class WordCountFromHDFS {

    public static void main(String[] args) {
        if (args.length != 1) {
            System.err.println("Usage: WordCountFromHDFS &lt;input path&gt;");
            System.exit(1);
        }

        // 初始化Spark配置
        SparkConf conf = new SparkConf().setAppName("WordCountFromHDFS").setMaster("local"); // 本地模式运行，根据实际情况可改为yarn等

        // 创建SparkContext实例
        JavaSparkContext sc = new JavaSparkContext(conf);

        // HDFS文件路径，这里直接从命令行参数获取
        String inputPath = args[0];

        // 从HDFS读取文件内容
        JavaRDD&lt;String&gt; lines = sc.textFile(inputPath);

        // 每行分割成单词，然后扁平化，最后统计每个单词出现的次数
        JavaRDD&lt;String&gt; words = lines.flatMap(line -&gt; Arrays.asList(line.split("\\s+")).iterator());
        JavaPairRDD&lt;String, Integer&gt; wordCounts = words.mapToPair(word -&gt; new Tuple2&lt;&gt;(word, 1))
                                                     .reduceByKey((a, b) -&gt; a + b);

        // 收集结果并打印
        List&lt;Tuple2&lt;String, Integer&gt;&gt; results = wordCounts.collect();
        for (Tuple2&lt;String, Integer&gt; result : results) {
            System.out.println(result._1() + ": " + result._2());
        }

        // 停止SparkContext
        sc.stop();
    }
}
</code></pre> 
<h2 id="4.Spark%20SQL" style="background-color:transparent;">4.Spark SQL</h2> 
<p>park SQL是Spark的一个组件，它从Spark 1.3.0版本开始被引入，并在后续版本中不断得到增强和发展。Spark SQL允许用户使用SQL或者DataFrame API来处理结构化和半结构化的数据。下面做个小小的演示。</p> 
<p>假设我们有一个CSV文件位于HDFS上，我们可以用以下命令加载它：</p> 
<blockquote> 
 <p>   val df = spark.read<br>      .option("header", "true")<br>      .csv("hdfs://localhost:9000/path/to/yourfile.csv")</p> 
</blockquote> 
<p>创建临时视图：</p> 
<blockquote> 
 <p>   df.createOrReplaceTempView("my_table")</p> 
</blockquote> 
<p>执行sql：</p> 
<blockquote> 
 <p>val result = spark.sql("SELECT column_name FROM my_table WHERE condition")</p> 
 <p>joinResult.show()</p> 
</blockquote> 
<p>连表查询：</p> 
<blockquote> 
 <p>// 假设dfOrders和dfCustomers分别是orders和customers的DataFrame<br> dfOrders.createOrReplaceTempView("orders")<br> dfCustomers.createOrReplaceTempView("customers")</p> 
 <p>val joinResult = spark.sql(<br>   """<br>     SELECT orders.order_id, customers.customer_name<br>     FROM orders<br>     INNER JOIN customers<br>     ON orders.customer_id = customers.customer_id<br>   """<br> )</p> 
 <p>joinResult.show()</p> 
</blockquote> 
<p>当然Spark SQL也有对应的JAVA API，支持编程的方式来操作，用到的时候查一下就是，此处就不展开了。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/865e7f790cb38e76bfe2b6da067194af/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">了解AIGC：让AI创造内容，改变未来</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/108d9078e1571fda80b633da2382dc4f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【递归、搜索与回溯】综合练习一</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>