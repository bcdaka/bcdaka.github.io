<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>万字长文，详细解读AI大模型技术原理！！ - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/eea6996233e983d2e51b3144636002ec/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="万字长文，详细解读AI大模型技术原理！！">
  <meta property="og:description" content="大模型是指具有大规模参数和复杂计算结构的机器学习模型。
本文从大模型的发展历程出发，对大模型领域的各个技术细节进行详细解读，供大家在了解大模型基本知识的过程中起到一定参考作用。
一、大模型的定义 大语言模型作为一个被验证可行的方向，其“大”体现在训练数据集广，模型参数和层数大，计算量大，其价值体现在通用性上，并且有更好的泛化能力。
这些模型通常由深度神经网络构建而成，拥有数十亿甚至数千亿个参数。
大模型的设计目的是为了提高模型的表达能力和预测性能，能够处理更加复杂的任务和数据。
大模型在各种领域都有广泛的应用，包括自然语言处理、计算机视觉、语音识别和推荐系统等。
大模型通过训练海量数据来学习复杂的模式和特征，具有更强大的泛化能力，可以对未见过的数据做出准确的预测。
ChatGPT 对大模型的解释更为通俗易懂，也更体现出类似人类的归纳和思考能力：大模型本质上是一个使用海量数据训练而成的深度神经网络模型，其巨大的数据和参数规模，实现了智能的涌现，展现出类似人类的智能 。
那么，大模型和小模型有什么区别？ 小模型通常指参数较少、层数较浅的模型，它们具有轻量级、高效率、易于部署等优点，适用于数据量较小、计算资源有限的场景，例如移动端应用、嵌入式设备、物联网等。
而当模型的训练数据和参数不断扩大，直到达到一定的临界规模后，其表现出了一些未能预测的、更复杂的能力和特性，模型能够从原始训练数据中自动学习并发现新的、更高层次的特征和模式，这种能力被称为“涌现能力”。
而具备涌现能力的机器学习模型就被认为是独立意义上的大模型，这也是其和小模型最大意义上的区别。 相比小模型，大模型通常参数较多、层数较深，具有更强的表达能力和更高的准确度，但也需要更多的计算资源和时间来训练和推理，适用于数据量较大、计算资源充足的场景，例如云端计算、高性能计算、人工智能等。
二、大模型相关概念区分
大模型（Large Model,也称基础模型，即 Foundation Model），是指具有大量参数和复杂结构的机器学习模型，能够处理海量数据、完成各种复杂的任务，如自然语言处理、计算机视觉、语音识别等。
超大模型：超大模型是大模型的一个子集，它们的参数量远超过大模型。
大语言模型（Large Language Model）：通常是具有大规模参数和计算能力的自然语言处理模型，例如 OpenAI 的 GPT-3 模型。这些模型可以通过大量的数据和参数进行训练，以生成人类类似的文本或回答自然语言的问题。大型语言模型在自然语言处理、文本生成和智能对话等领域有广泛应用。
GPT（Generative Pre-trained Transformer）：GPT 和 ChatGPT 都是基于 Transformer 架构的语言模型，但它们在设计和应用上存在区别：GPT 模型旨在生成自然语言文本并处理各种自然语言处理任务，如文本生成、翻译、摘要等。它通常在单向生成的情况下使用，即根据给定的文本生成连贯的输出。
ChatGPT：ChatGPT 则专注于对话和交互式对话。它经过特定的训练，以更好地处理多轮对话和上下文理解。ChatGPT 设计用于提供流畅、连贯和有趣的对话体验，以响应用户的输入并生成合适的回复。
三、大语言模型的发展 大模型相较于传统特定领域训练出来的语言模型，有更广泛的应用场景。
3.1 Transformer模型的提出 在Transformer提出之前，自然语言处理领域的主流模型是循环神经网络RNN，使用递归和卷积神经网络进行语言序列转换。
2017年，谷歌大脑团队在人工智能领域的顶会NeurIPS发表了一篇名为“Attention is all you need”的论文，首次提出了一种新的简单网络架构，即 Transformer，它完全基于注意力机制(attention)，完全摒弃了循环递归和卷积。
递归模型通常沿输入和输出序列的符号位置进行计算，来预测后面的值。
但这种固有的顺序性质阻碍了训练样例内的并行化，因为内存约束限制了样例之间的批处理。
而注意力机制允许对依赖项进行建模，而无需考虑它们在输入或输出序列中的距离。
Transformer避开了递归网络的模型体系结构，并且完全依赖于注意力机制来绘制输入和输出之间的全局依存关系。
在八个P100 GPU上进行了仅仅12个小时的训练之后，Transformer就可以在翻译质量方面达到新的最先进水平，体现了很好的并行能力。
总结两个核心突破：
1）突破了远距离文本依赖的学习限制，避开了递归网络的模型体系结构，并且完全依赖于注意力机制来绘制输入和输出之间的全局依赖关系。
2）可高度并行进行训练，这对发挥硬件红利以及快速迭代模型非常重要。
下图是论文提到的Transformer模型，对编码器和解码器使用堆叠式的自注意力和逐点式、全连接层，分别如图1的左半部分（编码器）和右半部分（解码器）所示，相关技术细节后面会重点讲到。
OpenAI基于该工作基础上发展了GPT（Generative Pre-training）生成式预训练模型，这里借用网上一张图简单改过，相关细节将在后面展开。
3.2 生成式预训练初现潜力：GPT-1 2018年，OpenAI公司发表了论文“Improving Language Understanding by Generative Pre-training”，">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-17T09:41:15+08:00">
    <meta property="article:modified_time" content="2024-05-17T09:41:15+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">万字长文，详细解读AI大模型技术原理！！</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>大模型是指具有<strong>大规模参数</strong>和<strong>复杂计算结构</strong>的机器学习模型。</p> 
<p>本文从大模型的发展历程出发，对大模型领域的各个技术细节进行详细解读，供大家在了解大模型基本知识的过程中起到一定参考作用。</p> 
<p><img src="https://images2.imgbox.com/b4/d5/T1OuT8gV_o.jpg" alt="图片"></p> 
<h3><a id="_10"></a><strong>一、大模型的定义</strong></h3> 
<p>大语言模型作为一个被验证可行的方向，其“大”体现在训练数据集广，模型参数和层数大，计算量大，其价值体现在通用性上，并且有更好的泛化能力。</p> 
<p>这些模型通常由深度神经网络构建而成，拥有数十亿甚至数千亿个参数。</p> 
<p>大模型的设计目的是为了提高模型的表达能力和预测性能，能够处理更加复杂的任务和数据。</p> 
<p>大模型在各种领域都有广泛的应用，包括自然语言处理、计算机视觉、语音识别和推荐系统等。</p> 
<p>大模型通过训练海量数据来学习复杂的模式和特征，具有更强大的泛化能力，可以对未见过的数据做出准确的预测。</p> 
<p>ChatGPT 对大模型的解释更为通俗易懂，也更体现出类似人类的归纳和思考能力：<strong>大模型本质上是一个使用海量数据训练而成的深度神经网络模型，其巨大的数据和参数规模，实现了智能的涌现，展现出类似人类的智能</strong> 。</p> 
<p>那么，大模型和小模型有什么区别？ 小模型通常指参数较少、层数较浅的模型，它们具有轻量级、高效率、易于部署等优点，适用于数据量较小、计算资源有限的场景，例如移动端应用、嵌入式设备、物联网等。</p> 
<p>而当模型的训练数据和参数不断扩大，直到达到一定的临界规模后，其表现出了一些未能预测的、更复杂的能力和特性，模型能够从原始训练数据中自动学习并发现新的、更高层次的特征和模式，这种能力被称为“涌现能力”。</p> 
<p>而具备涌现能力的机器学习模型就被认为是独立意义上的大模型，这也是其和小模型最大意义上的区别。 相比小模型，大模型通常参数较多、层数较深，具有更强的表达能力和更高的准确度，但也需要更多的计算资源和时间来训练和推理，适用于数据量较大、计算资源充足的场景，例如云端计算、高性能计算、人工智能等。</p> 
<p><strong>二、大模型相关概念区分</strong></p> 
<p><strong>大模型（Large Model,也称基础模型，即 Foundation Model）</strong>，是指具有大量参数和复杂结构的机器学习模型，能够处理海量数据、完成各种复杂的任务，如自然语言处理、计算机视觉、语音识别等。</p> 
<p><strong>超大模型</strong>：超大模型是大模型的一个子集，它们的参数量远超过大模型。</p> 
<p><strong>大语言模型（Large Language Model）</strong>：通常是具有大规模参数和计算能力的自然语言处理模型，例如 OpenAI 的 GPT-3 模型。这些模型可以通过大量的数据和参数进行训练，以生成人类类似的文本或回答自然语言的问题。大型语言模型在自然语言处理、文本生成和智能对话等领域有广泛应用。</p> 
<p><strong>GPT（Generative Pre-trained Transformer）</strong>：GPT 和 ChatGPT 都是基于 Transformer 架构的语言模型，但它们在设计和应用上存在区别：GPT 模型旨在生成自然语言文本并处理各种自然语言处理任务，如文本生成、翻译、摘要等。它通常在单向生成的情况下使用，即根据给定的文本生成连贯的输出。</p> 
<p><strong>ChatGPT</strong>：ChatGPT 则专注于对话和交互式对话。它经过特定的训练，以更好地处理多轮对话和上下文理解。ChatGPT 设计用于提供流畅、连贯和有趣的对话体验，以响应用户的输入并生成合适的回复。</p> 
<h3><a id="_45"></a><strong>三、大语言模型的发展</strong></h3> 
<p>大模型相较于传统特定领域训练出来的语言模型，有更广泛的应用场景。</p> 
<h3><a id="31_Transformer_49"></a><strong>3.1 Transformer模型的提出</strong></h3> 
<p>在Transformer提出之前，自然语言处理领域的主流模型是循环神经网络RNN，使用递归和卷积神经网络进行语言序列转换。</p> 
<p>2017年，谷歌大脑团队在人工智能领域的顶会NeurIPS发表了一篇名为“Attention is all you need”的论文，首次提出了一种新的简单网络架构，即 Transformer，它完全基于注意力机制(attention)，完全摒弃了循环递归和卷积。</p> 
<p>递归模型通常沿输入和输出序列的符号位置进行计算，来预测后面的值。</p> 
<p>但这种固有的顺序性质阻碍了训练样例内的并行化，因为内存约束限制了样例之间的批处理。</p> 
<p>而注意力机制允许对依赖项进行建模，而无需考虑它们在输入或输出序列中的距离。</p> 
<p><strong>Transformer避开了递归网络的模型体系结构，并且完全依赖于注意力机制来绘制输入和输出之间的全局依存关系。</strong></p> 
<p>在八个P100 GPU上进行了仅仅12个小时的训练之后，Transformer就可以在翻译质量方面达到新的最先进水平，体现了很好的并行能力。</p> 
<p><strong>总结两个核心突破：</strong></p> 
<p>1）突破了远距离文本依赖的学习限制，避开了递归网络的模型体系结构，并且完全依赖于注意力机制来绘制输入和输出之间的全局依赖关系。</p> 
<p>2）可高度并行进行训练，这对发挥硬件红利以及快速迭代模型非常重要。</p> 
<p>下图是论文提到的Transformer模型，对编码器和解码器使用堆叠式的自注意力和逐点式、全连接层，分别如图1的左半部分（编码器）和右半部分（解码器）所示，相关技术细节后面会重点讲到。</p> 
<p><img src="https://images2.imgbox.com/c4/8b/SUSaOw3y_o.png" alt="在这里插入图片描述"></p> 
<p><strong>OpenAI基于该工作基础上发展了GPT</strong>（Generative Pre-training）生成式预训练模型，这里借用网上一张图简单改过，相关细节将在后面展开。</p> 
<p><img src="https://images2.imgbox.com/9d/4b/7XSsPdUB_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="32_GPT1_81"></a><strong>3.2 生成式预训练初现潜力：GPT-1</strong></h3> 
<p>2018年，OpenAI公司发表了论文“Improving Language Understanding by Generative Pre-training”，</p> 
<p>使用的模型有两个阶段，第一阶段是无监督预训练，基于海量的文本集通过Transformer学习一个大容量的语言模型，第二阶段基于标注数据进行参数微调。</p> 
<p>得到的一般任务不可知模型（或称为通用模型）优于经过判别训练的模型，在论文选定的12种数据集中有9个取得更好效果。</p> 
<p>在 GPT-1 中，采用了 <strong>12 层</strong>Transformer 的结构作为解码器，每个 Transformer 层是一个多头的自注意力机制，然后通过全连接得到输出的概率分布。</p> 
<p>这次实践对OpenAI来讲，我觉得是奠定了他们往这个路线发展的核心因素，主要有几个重点突破：</p> 
<p><strong>1）证明了通用模型训练具有很大的价值潜力</strong>。之前用于学习特定任务的标注数据难以获得，导致模型效果不能持续提升，而通过Transformer无监督训练+少量标注数据的Finetune就取得了更优的效果。</p> 
<p><strong>2、论文尝试增加Transformer中间层</strong>， 在从2层到12层的数量增加中，平均每增加1层能够提升9%的准确性。加上Transformer本身具备并行能力，这在GPU上无疑潜力巨大。</p> 
<p><strong>3、论文发现在第二步的Finetune中添加语言建模作为辅助学习目标，能够提高监督模型的泛化能力，并加速收敛</strong>。说明在更海量的数据集时，模型会更收益于辅助学习目标。</p> 
<p><img src="https://images2.imgbox.com/19/02/gHKS2yXS_o.png" alt="在这里插入图片描述"></p> 
<p>但GPT-1在生成长文本时，仍然会出现信息遗忘和重复等问题，和特定领域的模型对比还有很多不足。</p> 
<h3><a id="33_GPT2_104"></a><strong>3.3 泛化能力突破：GPT-2</strong></h3> 
<p>2019年，OpenAI发表了最新进展，一篇“Language Models are Unsupervised Multitask Learners”的论文。</p> 
<p>重点实践了<strong>更大的模型更广的数据集具有更好的泛化能力</strong>。</p> 
<p>GPT-1是12层的transformer，BERT最深是24层的transformer，GPT-2则是48层，共有15亿个参数的transformer，训练集叫WebText，是从4500万个链接提取文本去重后，得到800万文档共40GB文本。</p> 
<p>论文认为现有系统用单个任务来训练的单个领域数据集，是缺乏模型泛化能力的主要原因，因此在更广的数据集上，GPT-2采用了多任务（multitask)的方式，每一个任务都要保证其损失函数能收敛，不同的任务共享主体transformer参数。</p> 
<p>最终训练出来的模型在不需要任何参数和模型改动下，在zero-shot（零样本）任务中，在8个数据集中有7个表现为业界最优，<strong>这个泛化能力可以说已经很强大了</strong>，并且在机器翻译场景取得亮眼结果，GPT也是在2.0出来后，开始备受关注。</p> 
<h3><a id="34_GPT3_116"></a><strong>3.4 更大参数更大数据集：GPT3</strong></h3> 
<p>之前的模型要在特定领域有更好表现，依然需要上千条标注样本数据来进行finetune，很大程度影响了模型的通用性，而人类能够根据前面一句话知道语境（in-context)，从而正确回答问题。</p> 
<p>GPT3就通过调大参数(1750亿）来测试in-context 学习能力，并在没有finetune情况下得到以下数据。</p> 
<p>在参数不断增加的同时，分为三种场景看回答准确率表现：Zero-shot（0样本），One-shot（只给一个标准样本），Few-shot（少量标准样本，1000条左右）。</p> 
<p>下图可以看到模型参数和样本集对正确性的影响，随着参数增多，Few-shot相比Zero-shot的提升效果在拉大，说明越大的参数对样本具有更强的泛化能力。</p> 
<p><img src="https://images2.imgbox.com/d4/d4/Oq1Cp5WS_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="35_ChatGPTGPT_35_131"></a><strong>3.5 火爆的ChatGPT：GPT 3.5</strong></h3> 
<p>2022年3月，OpenAI再次发表论文“Training language models to follow instructions with human feedback”，通过人工反馈和微调，使语言模型与用户对各种任务的意图保持一致。</p> 
<p>并推出了InstructGPT模型，InstructGPT 是基于 GPT-3 的一轮增强优化，所以也被称为 GPT-3.5。</p> 
<p>尽管GPT3.5还会犯一些简单的错误，但论文工作表明<strong>利用人类反馈进行微调是一个很有前景的方向</strong>。</p> 
<p>InstructGPT模型的参数为1.3B，而GPT-3模型的参数为175B，约为InstructGPT模型的130倍，但InstructGPT模型的输出却优于GPT-3模型的输出。</p> 
<p>训练过程首先聘请了40个承包商来标注数据，收集提交给OpenAI的prompts的人工答案样本集，以及一些人工写的prompts作为训练监督学习的基线。</p> 
<p>然后，在更大的prompts集上对比OpenAI的输出，并人工标记差距，据此训练出一个奖励模型(Reward Model)来预测人类喜好的输出。</p> 
<p>最后用PPO来最大化这个奖励模型和fine-tune对监督模型的效果。这部分具体技术细节将在后面展开。</p> 
<p><strong>论文认为模型如果有价值观的话，体现更多的是标注者的价值观念而不是更广泛人的价值观</strong>。</p> 
<p>对人类任务意图的识别，是一个非常重要的能力。ChatGPT 采用 InstructGPT 相同结构的模型，针对 Chat 进行了专门的优化， 同时开放到公众测试训练，以便产生更多有效标注数据。</p> 
<p>基于**人类反馈的强化学习（RLHF）**是 ChatGPT 区别于其他生成类模型的最主要特点，该法帮助模型尽量减少有害的、不真实的及有偏见的输出，提升自然沟通效果。</p> 
<p>同时，为了更好地支持多轮对话，ChatGPT 引入了一种基于堆栈的上下文管理的机制，帮助 ChatGPT 跟踪和管理多轮对话中的上下文信息，从而在多轮对话中生成连贯自然的回复。</p> 
<p><img src="https://images2.imgbox.com/7b/01/9LBO0S1H_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_161"></a><strong>四、主要技术细节</strong></h3> 
<p>从数学或机器学习的角度来看，语言模型都是对词语序列的概率相关性分布的建模，即利用已经说过的语句作为输入条件，预测下一个时刻不同语句甚至语言集合出现的概率分布。</p> 
<p>GPT生成式预训练模型也是根据语料概率来自动生成回答的每一个字，ChatGPT在此基础上通过使用基于人类反馈的强化学习（RLHF）来干预增强学习以取得更好效果。</p> 
<h3><a id="41_Transformer_167"></a><strong>4.1 什么是Transformer？</strong></h3> 
<p>本文重点介绍Transformer核心结构和技术点。</p> 
<h3><a id="1_171"></a><strong>1)编解码组件结构</strong></h3> 
<p>Transformer 本质上是一个 Encoder-Decoder 架构，包括编码组件和解码组件。</p> 
<p>编码组件和解码组件可以有很多层，比如Google刚提出时的论文用的是6层，后面GPT-1是12层，然后到GPT-3是96层。</p> 
<p><img src="https://images2.imgbox.com/22/3d/VDHiQkrH_o.png" alt="在这里插入图片描述"></p> 
<p>每个编码器由两个子层组成：Self-Attention 层（自注意力层）和 Position-wise Feed Forward Network（前馈网络，缩写为 FFN），每个编码器的结构都是相同的，但是它们使用不同的权重参数。编码器的输入会先流入 Self-Attention 层。</p> 
<p>解码器也有编码器中这两层，但是它们之间还有一个编解码注意力层（即 Encoder-Decoder Attention），其用来帮助解码器关注输入句子中需要关注的相关部分。</p> 
<p><img src="https://images2.imgbox.com/51/d6/HDUnekyG_o.png" alt="在这里插入图片描述"></p> 
<p><strong>2)编码器对文本的处理</strong></p> 
<p>对文本处理和通常的 NLP 任务一样，首先使用词嵌入算法（Embedding）将每个词转换为一个词向量(vector)。</p> 
<p>嵌入仅发生在最底层的编码器中，其他编码器接收的是上一个编码器的输出。</p> 
<p>这个列表大小是我们可以设置的参数——基本上这个参数就是训练数据集中最长句子的长度。</p> 
<p>对输入序列完成嵌入操作后，每个词都会流经编码器内的两层，然后逐个编码器向上传递。</p> 
<p><img src="https://images2.imgbox.com/16/4b/uddy44fv_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="3SelfAttention__198"></a><strong>3)Self-Attention 原理</strong></h3> 
<p>之前说Transformer的自注意机制突破了文本关注距离的限制，因此非常关键。先看这样一个句子:</p> 
<pre><code>The animal didn't cross the street becauseit was too tired
</code></pre> 
<p>这个句子中的"it"代表什么意思，是animal，还是street还是其他？这个对人来说很容易，但对模型来说不简单。</p> 
<p>self-Attention就是用来解决这个问题，让it指向animal。通过加权之后可以得到类似图8的加权情况，The animal获得最大关注。</p> 
<p><img src="https://images2.imgbox.com/62/de/EtVDvSHN_o.png" alt="在这里插入图片描述"></p> 
<p>在self-attention中，每个单词有3个不同的向量，它们分别是Query向量（ Q ），Key向量（ K ）和Value向量（ V ），长度均是64。</p> 
<p>它们是通过3个不同的权值矩阵由嵌入向量 X 乘以三个不同的权值矩阵 W^Q ， W^K ，W^V 得到，其中三个矩阵的尺寸也是相同的。均是 512×64 。</p> 
<p>Query，Key，Value的概念取自于信息检索系统，举个简单的搜索的例子来说。</p> 
<p>当你在某电商平台搜索某件商品（年轻女士冬季穿的红色薄款羽绒服）时，你在搜索引擎上输入的内容便是Query，然后搜索引擎根据Query为你匹配Key（例如商品的种类，颜色，描述等），然后根据Query和Key的相似度得到匹配的内容（Value)。</p> 
<p>self-attention中的Q，K，V也是起着类似的作用，在矩阵计算中，点积是计算两个矩阵相似度的方法之一，因此式1中使用了QK^T进行相似度的计算。</p> 
<p>接着便是根据相似度进行输出的匹配，这里使用了加权匹配的方式，而权值就是query与key的相似度。</p> 
<h3><a id="4_225"></a><strong>4)多注意头机制</strong></h3> 
<p>Multi-headed attention增强了自注意能力，其一是扩展了关注的位置，使之同时关注多个不同位置，其二是它为注意力层提供了多个“表示子空间”，如论文用了8个注意头，那就有8组不同的Q/K/V矩阵，每个输入的词向量都被投影到8个表示子空间中进行计算。</p> 
<p>具体流程如下图：</p> 
<p><img src="https://images2.imgbox.com/11/d8/Wvym5zx5_o.png" alt="在这里插入图片描述"></p> 
<p>因此<strong>多注意头本质上是用更多个角度进行注意力计算再统一起来，能够增强对句子上下文的完整理解</strong>。</p> 
<h3><a id="42_ChatGPT_235"></a><strong>4.2 ChatGPT是如何提升训练效果的？</strong></h3> 
<p>ChatGPT的背后是大型语言模型 (LLM) 生成领域的新训练范式：RLHF ，即基于来自人类反馈的强化学习来优化语言模型。</p> 
<p>关于RLHF训练有个TAMER框架值得参考。</p> 
<p>RLHF 是一项涉及多个模型和不同训练阶段的复杂概念，这里我们按三个步骤分解：</p> 
<ul><li> <p>预训练一个语言模型 (LM) ；</p> </li><li> <p>聚合问答数据并训练一个奖励模型 (Reward Model，RM) ；</p> </li><li> <p>用强化学习 (RL) 方式微调 LM。</p> </li></ul> 
<p>GPT3训练后的大语言模型是根据概率分布，计算出下一个最大可能的词，他不管事实逻辑上的准确性，也没有所谓的意识，所以有时会一本正经地胡说八道。</p> 
<p>RLHF是用生成文本的人工反馈作为性能衡量标准，或更进一步用该反馈作为奖励来优化模型，使得在一般文本数据语料库上训练的语言模型能和复杂的人类价值观对齐。</p> 
<p>首先，我们使用经典的预训练目标训练一个语言模型。对这一步的模型，OpenAI 在其第一个流行的 RLHF 模型 InstructGPT 中使用了较小版本的 GPT-3。然后进行以下步骤：</p> 
<p><strong>第一步：训练监督策略语言模型</strong></p> 
<p>GPT-3本身无法识别人类指令蕴含的不同意图，也很难判断生成内容是否高质量。</p> 
<p>为了解决这一问题，训练过程是从数据集中随机抽取问题，由标注人员给出高质量答案，相当于提供了一系列人工编写的prompts和对应的答案数据集。</p> 
<p>然后用这些人工标注好的数据集微调GPT3.5模型，获得SFT模型(Supervised Fine-Tune)。</p> 
<p><strong>第二步：训练奖励模型</strong></p> 
<p>训练方法：根据第一阶段的模型，随机抽取问题，给出多个不同的回答，人工选出最优答案进行标注，有点类似教学辅导。</p> 
<p>将高质量答案的奖励值进入下一轮强化学习RL，训练一个奖励模型来预测人类偏好的输出。</p> 
<p>RM 的训练是 RLHF 区别于旧范式的开端。</p> 
<p>这一模型接收一系列文本并返回一个标量奖励，数值上对应人的偏好。</p> 
<p>我们可以用端到端的方式用 LM 建模，或者用模块化的系统建模 (比如对输出进行排名，再将排名转换为奖励) 。</p> 
<p>这一奖励数值将对后续无缝接入现有的强化学习 RL 算法至关重要。</p> 
<p>关于模型选择方面，RM 可以是另一个经过微调的 LM，也可以是根据偏好数据从头开始训练的 LM。</p> 
<p>微调LM被认为对样本数据的利用率更高，但对于哪种 RM 更好尚无定论。</p> 
<p><strong>第三步：近端策略优化 (Proximal Policy Optimization，PPO)</strong></p> 
<p>使用PPO优化奖励模型的策略。使用奖励模型的输出作为标量奖励，并使用PPO算法对监督策略进行微调，以优化该奖励。</p> 
<p>**训练方法：**PPO的核心目的是将在线的人工学习转为离线学习，机器自己给自己打分。</p> 
<p>利用第二阶段训练好的奖励模型，在数据集中随机抽取问题，使用PPO模型生成多个回答，并用上一阶段训练好的RM模型分别给出质量分数。</p> 
<p>把回报分数按排序依次传递，产生策略梯度，通过强化学习的方式更新PPO模型参数。</p> 
<p>最后步骤二和步骤三可以循环迭代，可以不断完善模型。</p> 
<p><img src="https://images2.imgbox.com/ce/c9/a7RMZsAD_o.png" alt="在这里插入图片描述"></p> 
<p>总体来说，ChatGPT 在人工标注的prompts和回答里训练出SFT监督策略模型，再通过随机问题由模型给出多个答案，然后人工排序，生成奖励模型，再通过PPO强化训练增强奖励效果。</p> 
<p>最终ChatGPT能够更好理解指令的意图，并且按指令完成符合训练者价值观的输出。</p> 
<p>最后，大语言模型作为一个被验证可行的方向，其“大”体现在数据集广泛，参数和层数大，计算量大，其价值体现在通用性上，有广泛的应用场景。</p> 
<p>大语言模型能够发展，主要还是模型具备很好的<strong>并行扩展性</strong>，随着数据量和计算量的增加，主要挑战在工程和调优上。</p> 
<h3><a id="_AI__311"></a>如何学习大模型 AI ？</h3> 
<p>由于新岗位的生产效率，要优于被取代岗位的生产效率，所以实际上整个社会的生产效率是提升的。</p> 
<p>但是具体到个人，只能说是：</p> 
<p><strong>“最先掌握AI的人，将会比较晚掌握AI的人有竞争优势”。</strong></p> 
<p>这句话，放在计算机、互联网、移动互联网的开局时期，都是一样的道理。</p> 
<p>我在一线互联网企业工作十余年里，指导过不少同行后辈。帮助很多人得到了学习和成长。</p> 
<p>我意识到有很多经验和知识值得分享给大家，也可以通过我们的能力和经验解答大家在人工智能学习中的很多困惑，所以在工作繁忙的情况下还是坚持各种整理和分享。但苦于知识传播途径有限，很多互联网行业朋友无法获得正确的资料得到学习提升，故此将并将重要的AI大模型资料包括AI大模型入门学习思维导图、精品AI大模型学习书籍手册、视频教程、实战学习等录播视频免费分享出来。</p> 
<p><img src="https://images2.imgbox.com/89/62/fIZVEshi_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="10_329"></a>第一阶段（10天）：初阶应用</h3> 
<p>该阶段让大家对大模型 AI有一个最前沿的认识，对大模型 AI 的理解超过 95% 的人，可以在相关讨论时发表高级、不跟风、又接地气的见解，别人只会和 AI 聊天，而你能调教 AI，并能用代码将大模型和业务衔接。</p> 
<ul><li>大模型 AI 能干什么？</li><li>大模型是怎样获得「智能」的？</li><li>用好 AI 的核心心法</li><li>大模型应用业务架构</li><li>大模型应用技术架构</li><li>代码示例：向 GPT-3.5 灌入新知识</li><li>提示工程的意义和核心思想</li><li>Prompt 典型构成</li><li>指令调优方法论</li><li>思维链和思维树</li><li>Prompt 攻击和防范</li><li>…</li></ul> 
<h3><a id="30_350"></a>第二阶段（30天）：高阶应用</h3> 
<p>该阶段我们正式进入大模型 AI 进阶实战学习，学会构造私有知识库，扩展 AI 的能力。快速开发一个完整的基于 agent 对话机器人。掌握功能最强的大模型开发框架，抓住最新的技术进展，适合 Python 和 JavaScript 程序员。</p> 
<ul><li>为什么要做 RAG</li><li>搭建一个简单的 ChatPDF</li><li>检索的基础概念</li><li>什么是向量表示（Embeddings）</li><li>向量数据库与向量检索</li><li>基于向量检索的 RAG</li><li>搭建 RAG 系统的扩展知识</li><li>混合检索与 RAG-Fusion 简介</li><li>向量模型本地部署</li><li>…</li></ul> 
<h3><a id="30_366"></a>第三阶段（30天）：模型训练</h3> 
<p>恭喜你，如果学到这里，你基本可以找到一份大模型 AI相关的工作，自己也能训练 GPT 了！通过微调，训练自己的垂直大模型，能独立训练开源多模态大模型，掌握更多技术方案。</p> 
<p>到此为止，大概2个月的时间。你已经成为了一名“AI小子”。那么你还想往下探索吗？</p> 
<ul><li>为什么要做 RAG</li><li>什么是模型</li><li>什么是模型训练</li><li>求解器 &amp; 损失函数简介</li><li>小实验2：手写一个简单的神经网络并训练它</li><li>什么是训练/预训练/微调/轻量化微调</li><li>Transformer结构简介</li><li>轻量化微调</li><li>实验数据集的构建</li><li>…</li></ul> 
<h3><a id="20_385"></a>第四阶段（20天）：商业闭环</h3> 
<p>对全球大模型从性能、吞吐量、成本等方面有一定的认知，可以在云端和本地等多种环境下部署大模型，找到适合自己的项目/创业方向，做一名被 AI 武装的产品经理。</p> 
<ul><li>硬件选型</li><li>带你了解全球大模型</li><li>使用国产大模型服务</li><li>搭建 OpenAI 代理</li><li>热身：基于阿里云 PAI 部署 Stable Diffusion</li><li>在本地计算机运行大模型</li><li>大模型的私有化部署</li><li>基于 vLLM 部署大模型</li><li>案例：如何优雅地在阿里云私有部署开源大模型</li><li>部署一套开源 LLM 项目</li><li>内容安全</li><li>互联网信息服务算法备案</li><li>…</li></ul> 
<p>学习是一个过程，只要学习就会有挑战。天道酬勤，你越努力，就会成为越优秀的自己。</p> 
<p>如果你能在15天内完成所有的任务，那你堪称天才。然而，如果你能完成 60-70% 的内容，你就已经开始具备成为一名大模型 AI 的正确特征了。</p> 
<h6><a id="httpsblogcsdnnetJavachichiarticledetails122513096spm1001201430015501httpsblogcsdnnetm0_57081622articledetails122378123spm1001201430015501_AI_CSDNCSDN100_410"></a><a href="https://blog.csdn.net/Javachichi/article/details/122513096?spm=1001.2014.3001.5501"></a><a href="https://blog.csdn.net/m0_57081622/article/details/122378123?spm=1001.2014.3001.5501"></a>这份完整版的大模型 AI 学习资料已经上传CSDN，朋友们如果需要可以微信扫描下方CSDN官方认证二维码免费领取【<code>保证100%免费</code>】</h6> 
<p><img src="https://images2.imgbox.com/3d/b0/WAr1q2cJ_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c498c5e4ec153d7873708de00af95610/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【TPC-C】TPC-C标准化基准测试设计RDBMS的相关表结构</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4c47cb9edd1ee685569f206327e901eb/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">系列学习前端之第 11 章：将前端项目部署到本机运行起来 2 种方式（使用 Express 和 Nginx），使用花生壳做内网穿透，让外网可以访问网站</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>