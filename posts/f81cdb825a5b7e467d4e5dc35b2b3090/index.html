<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>2024年6月后2周重要的大语言模型论文总结：LLM进展、微调、推理和对齐 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/f81cdb825a5b7e467d4e5dc35b2b3090/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="2024年6月后2周重要的大语言模型论文总结：LLM进展、微调、推理和对齐">
  <meta property="og:description" content="本文总结了2024年6月后两周发表的一些最重要的大语言模型论文。这些论文涵盖了塑造下一代语言模型的各种主题，从模型优化和缩放到推理、基准测试和增强性能。
LLM进展与基准 1、 BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions
自动化软件工程近期受益于大型语言模型（LLMs）在编程领域的进展。尽管现有基准测试表明LLMs能够执行各种软件工程任务，但它们的评估主要限于短小且自成一体的算法任务。
解决具有挑战性和实用性的编程任务，需要利用各种函数调用作为工具，如数据分析和网页开发。使用多个工具解决任务需要通过准确理解复杂指令来进行组合推理。
满足这两种特性对LLMs来说是一个巨大的挑战。为了评估LLMs在解决具有挑战性和实用性的编程任务方面的表现，论文引入了一个基准测试Bench，挑战LLMs从139个库和7个领域调用多个函数作为工具，用于1,140个细粒度的编程任务。
为了严格评估LLMs，每个编程任务包含5.6个测试用例，平均分支覆盖率为99%。提出了一个以自然语言为导向的Bench变体，Benchi，它自动将原始文档字符串转换为仅包含关键信息的简短指令。
我们对60个LLMs的广泛评估显示，LLMs尚未能够按照复杂指令精确使用函数调用，得分最高为60%，远低于人类的97%表现。这些结果强调了该领域需要进一步发展的必要性。
https://arxiv.org/abs/2406.15877
2、Unlocking Continual Learning Abilities in Language Models
语言模型（LMs）表现出令人印象深刻的性能和泛化能力。但是LMs面临着持续学习（CL）中长期可持续性受损的持久挑战——灾难性遗忘。
现有方法通常通过将旧任务数据或任务相关的归纳偏置加入到LMs中来解决这一问题。但是旧数据和准确的任务信息往往难以获得或成本高昂，这限制了当前LMs持续学习方法的可用性。
为了解决这一限制，论文引入了MIGU（基于幅度的梯度更新，用于持续学习），这是一种无需复习和无需任务标签的方法，仅在LMs线性层中更新具有大幅度输出的模型参数。
MIGU基于以下观察：当LMs处理不同任务数据时，其线性层输出的L1标准化幅度分布有所不同。通过在梯度更新过程中施加这一简单约束，可以利用LMs的固有行为，从而释放其固有的持续学习能力。
实验表明，MIGU普遍适用于所有三种LM架构（T5、RoBERTa和Llama2），在四个持续学习基准测试中，无论是持续微调还是持续预训练设置，均展现出最先进或相当的性能。
例如在一个包含15个任务的持续学习基准测试中，MIGU比传统的高效参数微调基线平均提高了15.2%的准确率。MIGU还可以与所有三种现有的CL类型无缝集成，进一步提升性能。
https://arxiv.org/abs/2406.17245
3、Large Language Models Assume People Are More Rational than We Really Are
为了使人工智能系统能够有效地与人类交流，它们必须了解人类是如何做决策的。但是人类的决策并不总是理性的，因此大型语言模型（LLMs）中关于人类决策制定的隐含内部模型必须考虑到这一点。
以前的经验证据似乎表明这些隐含模型是准确的——LLMs提供了可信的人类行为代理，表现出我们期望人类在日常互动中的行为。
但是通过将LLM的行为和预测与大型人类决策数据集进行比较，论文发现实际情况并非如此：在模拟和预测人们的选择时，一系列尖端的LLMs（包括GPT-4o &amp; 4-Turbo，Llama-3–8B &amp; 70B，Claude 3 Opus）假设人们比实际上更理性。
这些模型偏离了人类行为，更接近于一个经典的理性选择模型——期望值理论。人们在解释他人行为时，也倾向于假设他人是理性的。
当论文作者使用另一个心理学数据集比较LLMs和人类从他人决策中得出的推断时，发现这些推断高度相关。因此，LLMs的隐含决策模型似乎与人们期望他人将理性行事的预期相一致，而不是与人们的实际行为相一致。
https://arxiv.org/abs/2406.17055
4、MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-06T11:35:26+08:00">
    <meta property="article:modified_time" content="2024-07-06T11:35:26+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">2024年6月后2周重要的大语言模型论文总结：LLM进展、微调、推理和对齐</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>本文总结了2024年6月后两周发表的一些最重要的大语言模型论文。这些论文涵盖了塑造下一代语言模型的各种主题，从模型优化和缩放到推理、基准测试和增强性能。</p> 
<h3><a id="LLM_2"></a>LLM进展与基准</h3> 
<p><strong>1、 BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions</strong></p> 
<p><img src="https://images2.imgbox.com/d7/be/mwMSJ0ZR_o.jpg" alt=""></p> 
<p>自动化软件工程近期受益于大型语言模型（LLMs）在编程领域的进展。尽管现有基准测试表明LLMs能够执行各种软件工程任务，但它们的评估主要限于短小且自成一体的算法任务。</p> 
<p>解决具有挑战性和实用性的编程任务，需要利用各种函数调用作为工具，如数据分析和网页开发。使用多个工具解决任务需要通过准确理解复杂指令来进行组合推理。</p> 
<p>满足这两种特性对LLMs来说是一个巨大的挑战。为了评估LLMs在解决具有挑战性和实用性的编程任务方面的表现，论文引入了一个基准测试Bench，挑战LLMs从139个库和7个领域调用多个函数作为工具，用于1,140个细粒度的编程任务。</p> 
<p>为了严格评估LLMs，每个编程任务包含5.6个测试用例，平均分支覆盖率为99%。提出了一个以自然语言为导向的Bench变体，Benchi，它自动将原始文档字符串转换为仅包含关键信息的简短指令。</p> 
<p>我们对60个LLMs的广泛评估显示，LLMs尚未能够按照复杂指令精确使用函数调用，得分最高为60%，远低于人类的97%表现。这些结果强调了该领域需要进一步发展的必要性。</p> 
<p>https://arxiv.org/abs/2406.15877</p> 
<p><strong>2、Unlocking Continual Learning Abilities in Language Models</strong></p> 
<p><img src="https://images2.imgbox.com/a4/b2/TqMHsZ7z_o.jpg" alt=""></p> 
<p>语言模型（LMs）表现出令人印象深刻的性能和泛化能力。但是LMs面临着持续学习（CL）中长期可持续性受损的持久挑战——灾难性遗忘。</p> 
<p>现有方法通常通过将旧任务数据或任务相关的归纳偏置加入到LMs中来解决这一问题。但是旧数据和准确的任务信息往往难以获得或成本高昂，这限制了当前LMs持续学习方法的可用性。</p> 
<p>为了解决这一限制，论文引入了MIGU（基于幅度的梯度更新，用于持续学习），这是一种无需复习和无需任务标签的方法，仅在LMs线性层中更新具有大幅度输出的模型参数。</p> 
<p>MIGU基于以下观察：当LMs处理不同任务数据时，其线性层输出的L1标准化幅度分布有所不同。通过在梯度更新过程中施加这一简单约束，可以利用LMs的固有行为，从而释放其固有的持续学习能力。</p> 
<p>实验表明，MIGU普遍适用于所有三种LM架构（T5、RoBERTa和Llama2），在四个持续学习基准测试中，无论是持续微调还是持续预训练设置，均展现出最先进或相当的性能。</p> 
<p>例如在一个包含15个任务的持续学习基准测试中，MIGU比传统的高效参数微调基线平均提高了15.2%的准确率。MIGU还可以与所有三种现有的CL类型无缝集成，进一步提升性能。</p> 
<p>https://arxiv.org/abs/2406.17245</p> 
<p><strong>3、Large Language Models Assume People Are More Rational than We Really Are</strong></p> 
<p><img src="https://images2.imgbox.com/04/18/wH0TGIhs_o.jpg" alt=""></p> 
<p>为了使人工智能系统能够有效地与人类交流，它们必须了解人类是如何做决策的。但是人类的决策并不总是理性的，因此大型语言模型（LLMs）中关于人类决策制定的隐含内部模型必须考虑到这一点。</p> 
<p>以前的经验证据似乎表明这些隐含模型是准确的——LLMs提供了可信的人类行为代理，表现出我们期望人类在日常互动中的行为。</p> 
<p>但是通过将LLM的行为和预测与大型人类决策数据集进行比较，论文发现实际情况并非如此：在模拟和预测人们的选择时，一系列尖端的LLMs（包括GPT-4o &amp; 4-Turbo，Llama-3–8B &amp; 70B，Claude 3 Opus）假设人们比实际上更理性。</p> 
<p>这些模型偏离了人类行为，更接近于一个经典的理性选择模型——期望值理论。人们在解释他人行为时，也倾向于假设他人是理性的。</p> 
<p>当论文作者使用另一个心理学数据集比较LLMs和人类从他人决策中得出的推断时，发现这些推断高度相关。因此，LLMs的隐含决策模型似乎与人们期望他人将理性行事的预期相一致，而不是与人们的实际行为相一致。</p> 
<p>https://arxiv.org/abs/2406.17055</p> 
<p><strong>4、MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression</strong></p> 
<p><img src="https://images2.imgbox.com/91/99/uNJ6hCPc_o.jpg" alt=""></p> 
<p>稀疏注意力可以有效减轻大型语言模型（LLMs）在长文本上的显著内存和吞吐量需求。现有方法通常采用统一的稀疏注意力掩码，在不同的注意力头和输入长度上应用相同的稀疏模式。</p> 
<p>然而这种统一方法未能捕捉LLMs固有的多样化注意力模式，忽略了它们不同的准确性-延迟权衡。为了解决这一挑战，论文提出了混合注意力（MoA），它能自动为不同的头和层定制不同的稀疏注意力配置。</p> 
<p>MoA构建并导航各种注意力模式及其相对于输入序列长度的缩放规则的搜索空间。它对模型进行分析，评估潜在配置，并确定最优的稀疏注意力压缩计划。</p> 
<p>MoA能适应不同的输入大小，显示出一些注意力头为适应更长的序列而扩展其焦点，而其他头则始终集中在固定长度的局部上下文上。</p> 
<p>实验表明，MoA将有效上下文长度增加了3.9倍，同时保持相同的平均注意力跨度，使得在Vicuna-7B、Vicuna-13B和Llama3–8B模型上的检索准确性提高了1.5-7.1倍，超过了统一注意力基线。</p> 
<p>此外MoA缩小了稀疏和密集模型之间的能力差距，将最大相对性能下降从9%-36%减少到两个长文本理解基准中的5%以内。</p> 
<p>MoA在单GPU上实现了1.2-1.4倍的GPU内存减少，并将7B和13B密集模型的解码吞吐量提高了5.5-6.7倍，对性能的影响最小。</p> 
<p>https://arxiv.org/abs/2406.14909</p> 
<p><strong>5、Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs</strong></p> 
<p><img src="https://images2.imgbox.com/b1/e3/GmvOKug4_o.jpg" alt=""></p> 
<p>论文提出了语义熵探针（SEPs），这是一种在大型语言模型（LLMs）中进行不确定性量化的低成本且可靠的方法。幻觉——听起来合理但事实上不正确且随意的模型生成，是LLMs实际应用中的一个主要挑战。</p> 
<p>Farquhar等人（2024年）的最近研究提出了语义熵（SE），通过估计一组模型生成中的语义意义空间的不确定性来检测幻觉。但是与SE计算相关的5到10倍的计算成本增加阻碍了其实际应用。</p> 
<p>为了解决这个问题，作者提出了SEPs，它可以直接从单一生成的隐藏状态近似SE。SEPs易于训练，在测试时不需要采样多个模型生成，将语义不确定性量化的开销几乎降为零。</p> 
<p>论文展示了SEPs在幻觉检测方面保持高性能，并且在泛化到分布外数据方面比直接预测模型准确性的以前的探针方法更好。</p> 
<p>在多个模型和任务上的结果表明，模型隐藏状态捕获了SE，我们的消融研究进一步洞察了哪些标记位置和模型层是这种情况。</p> 
<p>https://arxiv.org/abs/2406.15927</p> 
<p><strong>6、A Closer Look into Mixture-of-Experts in Large Language Models</strong></p> 
<p><img src="https://images2.imgbox.com/a2/19/B39sYQbr_o.jpg" alt=""></p> 
<p>专家混合模型（MoE）由于其独特的属性和卓越的性能，尤其是在语言任务中，越来越受到关注。通过稀疏激活每个标记的一部分参数，MoE架构可以在不牺牲计算效率的情况下增加模型大小，实现性能和训练成本之间更好的权衡。</p> 
<p>然而MoE的底层机制仍需进一步探索，其模块化程度仍有待商榷。论文首次尝试理解基于MoE的大型语言模型的内部工作原理。</p> 
<p>具体来说，全面研究了三个最近的基于MoE的模型的参数和行为特征，并揭示了一些有趣的观察结果，包括：</p> 
<ul><li>神经元表现得像细粒度专家。</li><li>MoE的路由器通常选择具有较大输出范数的专家。</li><li>专家多样性随层的增加而增加，而最后一层是一个异常值。</li></ul> 
<p>基于这些观察结果，作者还为广泛的MoE实践者提供了一些建议，例如路由器设计和专家分配。希望这项工作能为未来对MoE框架和其他模块化架构的研究提供启示。</p> 
<p>https://arxiv.org/abs/2406.18219</p> 
<p><strong>7、Leave No Document Behind Benchmarking Long-Context LLMs with Extended Multi-Doc QA</strong></p> 
<p><img src="https://images2.imgbox.com/8e/ca/UNEShQ6t_o.jpg" alt=""></p> 
<p>长文本建模能力已经引起广泛关注，出现了具有超长上下文窗口的大型语言模型（LLMs）。与此同时，评估长文本LLMs的基准测试也在逐渐跟进。</p> 
<p>现有的基准测试使用不相关的噪音文本人为延长测试用例的长度，与长文本应用的现实场景背道而驰。为了弥合这一差距，作者提出了一个新的长文本基准测试，Loong，通过扩展的多文档问答（QA）与现实场景保持一致。</p> 
<p>与典型的文档QA不同，在Loong的测试用例中，每个文档都与最终答案相关，忽略任何一个文档都会导致答案失败。此外，Loong引入了四种类型的任务，包括不同范围的上下文长度：聚光定位、比较、聚类和推理链，以便更真实、全面地评估长文本理解能力。</p> 
<p>广泛的实验表明，现有的长文本语言模型仍然显示出相当大的增强潜力。检索增强生成（RAG）表现不佳，证明Loong能可靠地评估模型的长文本建模能力。</p> 
<p>https://arxiv.org/abs/2406.17419</p> 
<p><strong>8、LongIns: A Challenging Long-context Instruction-based Exam for LLMs</strong></p> 
<p><img src="https://images2.imgbox.com/e4/d3/mQ6frN3k_o.jpg" alt=""></p> 
<p>近年来，大型语言模型（LLMs）的长文本能力成为热门话题。为了评估LLMs在不同场景下的表现，各种评估基准测试已经出现。</p> 
<p>由于这些基准测试大多专注于识别关键信息以回答问题，主要需要LLMs的检索能力，这些基准测试只能部分代表LLMs从大量信息中的推理性能。</p> 
<p>尽管LLMs经常声称具有32k、128k、200k甚至更长的上下文窗口，但这些基准测试未能揭示这些LLMs实际支持的长度。为了解决这些问题，作者提出了LongIns基准数据集，这是一个基于现有指令数据集建立的挑战性长文本指令型考试，用于评估LLMs。</p> 
<p>具体来说，在LongIns中，引入了三种评估设置：全局指令与单任务（GIST）、局部指令与单任务（LIST）和局部指令与多任务（LIMT）。基于LongIns，我们对现有的LLMs进行了全面评估，并得出以下重要发现：</p> 
<ul><li>在LongIns中，表现最佳的GPT-4具有128k的上下文长度，在16k的评估上下文窗口中表现不佳。</li><li>对于许多现有LLMs的多跳推理能力，在短上下文窗口（小于4k）下仍需要显著努力。</li></ul> 
<p>https://arxiv.org/abs/2406.17588</p> 
<h3><a id="RAG_148"></a>RAG</h3> 
<p><strong>1、 LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs</strong></p> 
<p><img src="https://images2.imgbox.com/b3/b2/fYSCb48y_o.jpg" alt=""></p> 
<p>在传统的RAG（检索增强生成）框架中，基本的检索单元通常较短。像DPR这样的常见检索器通常使用100字的维基百科段落。这种设计迫使检索器在大型语料库中搜索小型单元。</p> 
<p>相比之下，读取器只需要从检索到的小型单元中提取答案。这种不平衡的重检索器和轻读取器设计可能导致性能不佳。为了缓解这种不平衡，作者提出了一个新框架LongRAG，包括一个长检索器和一个长读取器。</p> 
<p>LongRAG将整个维基百科处理成4K令牌单元，这比以前长30倍。通过增加单元大小，显著减少了总单元数，从2200万减少到70万。这显著降低了检索器的负担，从而导致了显著的检索得分：在NQ上的答案召回率@1=71%（之前为52%）和在HotpotQA（全维基）上的答案召回率@2=72%（之前为47%）。</p> 
<p>然后将前k个检索到的单元（大约30K令牌）提供给现有的长上下文LLM进行零样本答案提取。无需任何训练，LongRAG在NQ上实现了62.7%的精确匹配（EM），这是已知的最佳结果。</p> 
<p>LongRAG在HotpotQA（全维基）上也达到了64.3%，与当前最好的模型持平。论文的研究为将RAG与长上下文LLMs结合提供了未来的发展路线图。</p> 
<p>https://arxiv.org/abs/2406.15319</p> 
<p><strong>2、Towards Retrieval Augmented Generation over Large Video Libraries</strong></p> 
<p><img src="https://images2.imgbox.com/87/aa/5tXZE3ur_o.jpg" alt=""></p> 
<p>在这篇论文中引入了视频库问答（VLQA）任务，通过一个可互操作的架构，将检索增强生成（RAG）应用于视频库。</p> 
<p>作者提出了一个系统，该系统使用大型语言模型（LLMs）生成搜索查询，检索通过语音和视觉元数据索引的相关视频时刻。</p> 
<p>一个答案生成模块将用户查询与此元数据整合，以生成带有具体视频时间戳的响应。这种方法在多媒体内容检索和AI辅助视频内容创作中显示出潜力。</p> 
<p>https://arxiv.org/abs/2406.14938</p> 
<p><strong>3、A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems</strong></p> 
<p><img src="https://images2.imgbox.com/4a/cd/VA1Yspcp_o.jpg" alt=""></p> 
<p>检索增强生成（RAG）代表了人工智能的重大进展，它结合了检索阶段和生成阶段，后者通常由大型语言模型（LLMs）驱动。</p> 
<p>当前RAG的常见做法包括使用“受指导”的LLMs，这些模型经过监督训练进行微调，以增强其遵循指令的能力，并使用最先进的技术与人类偏好对齐。</p> 
<p>与流行观念相反，论文的研究表明，在作者的实验设置下，基础模型在RAG任务中的表现平均比受指导的对应模型高出20%。这一发现挑战了有关受指导LLMs在RAG应用中优越性的普遍假设。</p> 
<p>进一步的调查揭示了更为细致的情况，质疑RAG的基本方面，并建议就该话题进行更广泛的讨论；正如弗罗姆（Fromm）所说，“很少有一瞥统计数据就足以理解数字的含义”。</p> 
<p>https://arxiv.org/abs/2406.14972</p> 
<p><strong>4、Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation</strong></p> 
<p><img src="https://images2.imgbox.com/f3/ed/0MD2CrV5_o.jpg" alt=""></p> 
<p>检索增强生成（RAG）已证明在减轻大型语言模型（LLMs）的幻觉问题方面的有效性。但是使检索器与LLMs多样的知识偏好相匹配的困难不可避免地给开发可靠的RAG系统带来了挑战。</p> 
<p>为了解决这个问题，作者提出了DPA-RAG，使RAG系统内部多样的知识偏好对齐的通用框架。首先引入了一个偏好知识构建管道，并整合了五种新颖的查询增强策略，以缓解偏好数据稀缺的问题。</p> 
<p>基于偏好数据，DPA-RAG完成了外部和内部偏好的对齐：</p> 
<ul><li>它将成对的、点对点的和对比性的偏好对齐能力集成到重排器中，实现了RAG组件间的外部偏好对齐。</li><li>它进一步在传统的监督式微调（SFT）之前引入了一个预对齐阶段，使LLMs能够隐式地捕捉与其推理偏好一致的知识，实现了LLMs的内部对齐。</li></ul> 
<p>在四个知识密集型问答数据集上的实验结果表明，DPA-RAG超越了所有基线，并且无缝整合了黑盒和开源的LLM阅读器。进一步的定性分析和讨论也为实现可靠的RAG系统提供了经验性指导。</p> 
<p>https://arxiv.org/abs/2406.18676</p> 
<h3><a id="LLM_214"></a>LLM微调</h3> 
<p><strong>1、Dataset Size Recovery from LoRA Weights</strong></p> 
<p><img src="https://images2.imgbox.com/97/1f/o0BTBS1t_o.jpg" alt=""></p> 
<p>作者引入了一项新任务：数据集大小恢复，旨在直接从模型的权重确定用于训练模型的样本数量。提出了一种名为DSiRe的方法，用于恢复在常见的LoRA微调情况下用于微调模型的图像数量。</p> 
<p>作者发现，LoRA矩阵的范数和谱与微调数据集的大小密切相关；利用这一发现提出了一个简单而有效的预测算法。</p> 
<p>为了评估LoRA权重的数据集大小恢复，作者开发并发布了一个新的基准测试LoRA-WiSE，包含来自2000多个多样化LoRA微调模型的超过25000个权重快照。最好的分类器能够预测微调图像的数量，平均绝对误差为0.36图像，从而证明了这种方案的可行性。</p> 
<p>https://arxiv.org/abs/2406.19395</p> 
<p><strong>2、Can Few Shots Work in a Long Context? Recycling the Context to Generate Demonstrations</strong></p> 
<p><img src="https://images2.imgbox.com/ee/e8/aZFJVx7i_o.jpg" alt=""></p> 
<p>尽管大型语言模型（LLMs）近年来取得了显著进展，但在处理长上下文任务时，它们的性能仍然不尽如人意。在此场景中，使用少量示例进行上下文内学习（ICL）可能是一种提高LLM性能的有吸引力的解决方案。</p> 
<p>但是简单地添加带有长上下文的ICL示例会引入挑战，包括为每个少量示例增加大量的令牌开销，以及示范与目标查询之间的上下文不匹配。在这项工作中，论文提出通过回收上下文自动为长上下文问答（QA）任务生成少量示例。</p> 
<p>具体来说，给定一个长输入上下文（1-3k令牌）和一个查询，从给定上下文中生成额外的查询-输出对作为少量示例，同时只引入一次上下文。这确保了演示利用的是与目标查询相同的上下文，同时只向提示中添加少量令牌。</p> 
<p>作者进一步通过指示模型在回答之前明确识别相关段落来增强每个演示，这在提高性能的同时，为答案来源提供了细粒度归因。</p> 
<p>在多个LLMs上应用论文的方法，并在多个带有长上下文的QA数据集上获得了显著的改进（平均跨模型提高23%），特别是当答案位于上下文中部时。令人惊讶的是，尽管只引入了单跳ICL示例，LLMs也成功地使用论文的方法推广到多跳长上下文QA。</p> 
<p>https://arxiv.org/abs/2406.13632</p> 
<h3><a id="LLM_246"></a>LLM推理</h3> 
<p><strong>1、Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models</strong></p> 
<p><img src="https://images2.imgbox.com/51/72/hkm21V5C_o.jpg" alt=""></p> 
<p>大型语言模型（LLMs）在文本数学问题解决方面表现出令人印象深刻的推理能力。</p> 
<p>但是现有的开源图像指令微调数据集，每张图像包含的问题-答案对数量有限，未能充分利用视觉信息来增强多模态LLMs（MLLMs）的多模态数学推理能力。</p> 
<p>为了弥合这一差距，作者通过从24个现有数据集中收集40,000张高质量图像及其问题-答案对，并合成320,000个新对，创建了MathV360K数据集，这一数据集增强了多模态数学问题的广度和深度。</p> 
<p>论文还引入了Math-LLaVA，一种基于LLaVA-1.5并用MathV360K微调的模型。这种新方法显著提高了LLaVA-1.5的多模态数学推理能力，在MathVista的最小分割上实现了19点的增长，并与GPT-4V的表现相当。</p> 
<p>并且Math-LLaVA展示了更强的泛化能力，在MMMU基准测试上显示出显著改进。这个研究强调了数据集多样性和合成在推进MLLMs数学推理能力方面的重要性。</p> 
<p>https://arxiv.org/abs/2406.17294</p> 
<h3><a id="LLM_265"></a>LLM安全与对齐</h3> 
<p><strong>1、Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges</strong></p> 
<p><img src="https://images2.imgbox.com/1d/f9/lJyHVaDu_o.jpg" alt=""></p> 
<p>论文提供了一种解决人类评估可扩展性挑战的有希望的解决方案，LLM作为评判的模式正在迅速成为评估大型语言模型（LLMs）的一种方法。</p> 
<p>但是关于这一模式的优势和劣势，以及它可能持有的潜在偏见，仍有许多未解决的问题。作者展示了对各种作为评判的LLMs的性能的全面研究。</p> 
<p>利用TriviaQA作为评估LLMs客观知识推理的基准，并与作者发现具有高度一致性的人类注释一起评估它们。</p> 
<p>论文的研究包括9个评判模型和9个考生模型——基础型和指令调优型。评估评判模型在不同模型大小、家族和评判提示之间的一致性。</p> 
<p>在其他结果中，了坤问的研究重新发现了使用科恩的卡帕系数作为一致性度量而非简单的百分比一致性的重要性，显示出具有高百分比一致性的评判仍然可以分配极其不同的分数。</p> 
<p>作者发现Llama-3 70B和GPT-4 Turbo在与人类的一致性上表现出色</p> 
<p>https://arxiv.org/abs/2406.12624</p> 
<p><strong>2、Cross-Modality Safety Alignment</strong></p> 
<p><img src="https://images2.imgbox.com/65/9b/fcwCUq7c_o.jpg" alt=""></p> 
<p>随着人工通用智能（AGI）越来越多地融入人类生活的各个方面，确保这些系统的安全性和道德一致性至关重要。</p> 
<p>以往的研究主要关注单一模态的威胁，鉴于跨模态交互的综合性和复杂性，这可能不足以应对挑战。论文引入了一个名为“安全输入但不安全输出”（SIUO）的新型安全一致性挑战，以评估跨模态安全一致性。</p> 
<p>它考虑了单一模态独立安全但结合时可能导致不安全或不道德输出的情况。为了实证研究这个问题，作者开发了SIUO，一个包括9个关键安全领域（如自我伤害、非法活动和隐私侵犯）的跨模态基准测试。</p> 
<p>研究发现，在封闭和开源的大型变量语言模型（LVLMs），如GPT-4V和LLaVA中发现了重大的安全漏洞，这突显了当前模型在可靠解释和响应复杂的现实世界场景中的不足。</p> 
<p>https://arxiv.org/abs/2406.15279</p> 
<p><strong>3、Ruby Teaming: Improving Quality Diversity Search with Memory for Automated Red Teaming</strong></p> 
<p><img src="https://images2.imgbox.com/3a/91/vqxCkCTr_o.jpg" alt=""></p> 
<p>作者的Ruby Teaming方法是在Rainbow Teaming的基础上改进的，它新增了一个内存缓存作为第三维度。这一内存维度为变异器提供线索，以生成质量更高的提示，无论是在攻击成功率（ASR）还是质量多样性方面。</p> 
<p>由Ruby Teaming生成的提示档案的ASR为74%，比基线高出20%。在质量多样性方面，根据香农均匀指数（SEI）和辛普森多样性指数（SDI），Ruby Teaming分别比Rainbow Teaming高出6%和3%。这表明Ruby Teaming在提升生成提示的效率和多样性方面具有显著优势。</p> 
<p>https://arxiv.org/abs/2406.11654</p> 
<p><strong>4、Aligning Teacher with Student Preferences for Tailored Training Data Generation</strong></p> 
<p><img src="https://images2.imgbox.com/46/dd/yHdO4Gpc_o.jpg" alt=""></p> 
<p>大型语言模型（LLMs）在多种任务中作为协作工具显示出显著的潜力。在处理隐私敏感数据或对延迟敏感的任务时，LLMs在边缘设备上的本地部署是必要的。</p> 
<p>这种设备的计算限制使得直接部署强大的大规模LLMs变得不切实际，这就需要从大规模模型到轻量级模型的知识蒸馏。</p> 
<p>已经有很多工作致力于从LLMs中引出多样化和高质量的训练示例，但对于基于学生偏好对教师指导内容进行对齐的关注却很少，这类似于教育学中的“响应性教学”。</p> 
<p>所以作者提出了一种名为ARTE（Aligning TeacheR with StudenT PreferencEs）的框架，该框架将教师模型与学生偏好对齐，以生成针对知识蒸馏的定制化训练示例。</p> 
<p>从教师模型中引出草稿问题和论证，然后使用学生在上下文学习中的表现作为代理，收集学生对这些问题和论证的偏好，并最终将教师模型与学生偏好对齐。</p> 
<p>最后重复第一步，使用对齐后的教师模型为学生模型在目标任务上引出定制化的训练示例。在学术基准上进行的广泛实验表明，ARTE优于现有从强大的LLMs蒸馏出的指导调整数据集。</p> 
<p>作者还深入研究了ARTE的泛化能力，包括经过微调的学生模型在推理能力上的泛化以及对齐的教师模型在跨任务和学生上生成定制训练数据的泛化。</p> 
<p>这篇论文贡献在于提出了一个生成定制训练示例的新框架，证明了它在实验中的有效性，并调查了ARTE中学生模型和对齐的教师模型的泛化能力。</p> 
<p>https://arxiv.org/abs/2406.19227</p> 
<p><strong>5、WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models</strong></p> 
<p><img src="https://images2.imgbox.com/27/6d/msmPJ3gr_o.jpg" alt=""></p> 
<p>论文介绍了WildTeaming，这是一个自动化的LLM安全红队框架，它挖掘用户与聊天机器人的互动，发现5700个独特的新型越狱策略群集，然后组合多种策略系统地探索新型越狱。</p> 
<p>与之前通过招募人工工作人员、基于梯度的优化或与LLMs迭代修订进行红队操作的工作相比，论文的工作调查了那些没有特别指示去破坏系统的聊天机器人用户的越狱行为。</p> 
<p>WildTeaming揭示了前沿LLMs以前未识别的漏洞，与最先进的越狱方法相比，导致了多达4.6倍更多样化和成功的对抗性攻击。虽然有许多数据集用于越狱评估，但很少有开源数据集用于越狱训练，因为即使模型权重是开放的，安全训练数据也是封闭的。</p> 
<p>通过WildTeaming，作者创建了WildJailbreak，一个大规模的开源合成安全数据集，包含262K个普通（直接请求）和对抗性（复杂越狱）的提示-响应对。</p> 
<p>为了缓解夸大的安全行为，WildJailbreak提供了两种对比类型的查询：</p> 
<ul><li>有害查询（普通和对抗性）</li><li>形式上类似有害查询但不含有害内容的良性查询。</li></ul> 
<p>由于WildJailbreak大幅提升了现有安全资源的质量和规模，它独特地使我们能够研究数据的规模效应以及在安全训练期间数据属性与模型能力的相互作用。</p> 
<p>通过广泛的实验作者确定了使模型能够理想平衡安全行为的训练属性：适当的保护而不过度拒绝，有效处理普通和对抗性查询，以及尽可能少地降低总体能力。WildJailbeak的所有组成部分都有助于实现模型的平衡安全行为。</p> 
<p>https://arxiv.org/abs/2406.18510</p> 
<p><a href="https://avoid.overfit.cn/post/42caafd81dfb40f387c59747c6a96417" rel="nofollow">https://avoid.overfit.cn/post/42caafd81dfb40f387c59747c6a96417</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9afff48cde21691ff7f2d033fc401766/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python面试题：请解释 `lambda` 函数是什么，并举一个例子</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/88388e23231bfb40647d487d2eb9bd96/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Maven基本使用</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>