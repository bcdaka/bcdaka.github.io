<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【AI】ubuntu 22.04 本地搭建Qwen-VL 支持图片识别的大语言模型 AI视觉 【3】Qwen-VL-Chat-Int4版本 &#43; 4060ti 16G - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/868d2d80e33d813ef4756795466e25f1/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【AI】ubuntu 22.04 本地搭建Qwen-VL 支持图片识别的大语言模型 AI视觉 【3】Qwen-VL-Chat-Int4版本 &#43; 4060ti 16G">
  <meta property="og:description" content="接上篇
【AI】ubuntu 22.04 本地搭建Qwen-VL 支持图片识别的大语言模型 AI视觉-CSDN博客
【AI】ubuntu 22.04 本地搭建Qwen-VL 支持图片识别的大语言模型 AI视觉 【2】 4060ti 16G 也顶不住-CSDN博客
下载Qwen-VL-Chat-Int4版本模型
cd ~/Downloads/ai git lfs install git clone https://www.modelscope.cn/qwen/Qwen-VL-Chat-Int4.git 这个版本模型体积小不少
2060 6G 仍然不能启动web
尝试参考模型中的README.md编写使用量化的代码
test.py
import os os.environ[&#39;CUDA_VISIBLE_DEVICES&#39;] = &#39;0&#39; from modelscope import ( AutoModelForCausalLM, AutoTokenizer, GenerationConfig, ) from transformers import BitsAndBytesConfig import torch model_dir = &#34;/home/yeqiang/Downloads/ai/Qwen-VL-Chat-Int4&#34; torch.manual_seed(1234) quantization_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_quant_type=&#39;nf4&#39;, bnb_4bit_use_double_quant=True, llm_int8_skip_modules=[&#39;lm_head&#39;, &#39;attn_pool.attn&#39;]) tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=&#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-09T10:11:55+08:00">
    <meta property="article:modified_time" content="2024-04-09T10:11:55+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【AI】ubuntu 22.04 本地搭建Qwen-VL 支持图片识别的大语言模型 AI视觉 【3】Qwen-VL-Chat-Int4版本 &#43; 4060ti 16G</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>接上篇</p> 
<p><a href="https://blog.csdn.net/hknaruto/article/details/137461855?spm=1001.2014.3001.5501" title="【AI】ubuntu 22.04 本地搭建Qwen-VL 支持图片识别的大语言模型 AI视觉-CSDN博客">【AI】ubuntu 22.04 本地搭建Qwen-VL 支持图片识别的大语言模型 AI视觉-CSDN博客</a></p> 
<p><a href="https://blog.csdn.net/hknaruto/article/details/137491071?spm=1001.2014.3001.5501" title="【AI】ubuntu 22.04 本地搭建Qwen-VL 支持图片识别的大语言模型 AI视觉 【2】 4060ti 16G 也顶不住-CSDN博客">【AI】ubuntu 22.04 本地搭建Qwen-VL 支持图片识别的大语言模型 AI视觉 【2】 4060ti 16G 也顶不住-CSDN博客</a></p> 
<p></p> 
<p>下载Qwen-VL-Chat-Int4版本模型</p> 
<pre><code>cd ~/Downloads/ai
git lfs install
git clone https://www.modelscope.cn/qwen/Qwen-VL-Chat-Int4.git</code></pre> 
<p>这个版本模型体积小不少</p> 
<p><img alt="" height="748" src="https://images2.imgbox.com/6a/78/oIZTUgN1_o.png" width="1028"></p> 
<p>2060 6G 仍然不能启动web</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/e2/c7/4yGGmz0g_o.png" width="1200"></p> 
<p>尝试参考模型中的README.md编写使用量化的代码</p> 
<p>test.py</p> 
<pre><code>import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
from modelscope import (
    AutoModelForCausalLM, AutoTokenizer, GenerationConfig,
)
from transformers import BitsAndBytesConfig
import torch

model_dir = "/home/yeqiang/Downloads/ai/Qwen-VL-Chat-Int4"
torch.manual_seed(1234)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    llm_int8_skip_modules=['lm_head', 'attn_pool.attn'])

tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto",
                                             trust_remote_code=True, fp16=True,
                                             quantization_config=quantization_config).eval()
model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)

query = tokenizer.from_list_format([
    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'},
    {'text': '这是什么'},
])
response, history = model.chat(tokenizer, query=query, history=None)
print(response)

response, history = model.chat(tokenizer, '输出狗的检测框', history=history)
print(response)
image = tokenizer.draw_bbox_on_latest_picture(response, history)
image.save('output_chat2.jpg')
</code></pre> 
<p>启动报错</p> 
<p>2024-04-08 13:40:52,816 - modelscope - INFO - PyTorch version 2.2.2 Found.<br> 2024-04-08 13:40:52,816 - modelscope - INFO - Loading ast index from /home/yeqiang/.cache/modelscope/ast_indexer<br> 2024-04-08 13:40:52,840 - modelscope - INFO - Loading done! Current index file version is 1.13.3, with md5 1c4da6103bff77f1a134ac63a6cb75b9 and a total number of 972 components indexed<br> /home/yeqiang/Downloads/src/Qwen-VL/venv/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.<br>   torch.utils._pytree._register_pytree_node(<br> /home/yeqiang/Downloads/src/Qwen-VL/venv/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.<br>   torch.utils._pytree._register_pytree_node(<br> Traceback (most recent call last):<br>   File "/home/yeqiang/Downloads/src/Qwen-VL/test.py", line 11, in &lt;module&gt;<br>     quantization_config = BitsAndBytesConfig(<br>   File "/home/yeqiang/Downloads/src/Qwen-VL/venv/lib/python3.10/site-packages/transformers/utils/quantization_config.py", line 212, in __init__<br>     self.post_init()<br>   File "/home/yeqiang/Downloads/src/Qwen-VL/venv/lib/python3.10/site-packages/transformers/utils/quantization_config.py", line 238, in post_init<br>     if self.load_in_4bit and not version.parse(importlib.metadata.version("bitsandbytes")) &gt;= version.parse(<br>   File "/usr/lib/python3.10/importlib/metadata/__init__.py", line 996, in version<br>     return distribution(distribution_name).version<br>   File "/usr/lib/python3.10/importlib/metadata/__init__.py", line 969, in distribution<br>     return Distribution.from_name(distribution_name)<br>   File "/usr/lib/python3.10/importlib/metadata/__init__.py", line 548, in from_name<br>     raise PackageNotFoundError(name)<br><span style="color:#fe2c24;"><strong>importlib.metadata.PackageNotFoundError: No package metadata was found for bitsandbytes</strong></span></p> 
<p></p> 
<p>这个脚本的依赖与requirements.txt有一定差异</p> 
<pre><code>pip install modelscope -U
pip install transformers accelerate tiktoken -U
pip install einops transformers_stream_generator -U
pip install "pillow==9.*" -U
pip install torchvision
pip install matplotlib -U</code></pre> 
<p><img alt="" height="480" src="https://images2.imgbox.com/4e/82/2mQjNNsV_o.png" width="1200"></p> 
<p>参考文档手动安装</p> 
<p>故障依旧</p> 
<p></p> 
<p>AI分析</p> 
<p>根据您给出的终端命令输出，您当前虚拟环境中安装的库版本如下：</p> 
<ul><li><code>transformers</code> 版本为 4.39.3</li><li><code>bitsandbytes</code> 版本为 0.43.0</li></ul> 
<p>然而，根据之前的错误信息，<code>BitsAndBytesConfig</code> 对象似乎缺乏 <code>get_loading_attributes</code> 方法，导致在尝试从预训练模型加载时抛出了异常。这可能意味着在您当前安装的 <code>transformers</code> 版本中，对于如何与 <code>bitsandbytes</code> 配合进行模型量化加载有所变更，或者 <code>bitsandbytes</code> 的某个版本引入了不兼容的改变。</p> 
<p>为了修复这个问题，您可以尝试以下步骤：</p> 
<ol><li>查阅 <code>transformers</code> 和 <code>bitsandbytes</code> 的官方文档，查找有关二者协同工作以及量化配置的最佳实践指导。</li><li>检查 <code>transformers</code> 与 <code>bitsandbytes</code> 的兼容性列表，确保您使用的版本搭配是经过测试和验证的。</li><li>尝试升级或降级 <code>bitsandbytes</code> 或者 <code>transformers</code> 至已知兼容的版本。</li><li>若上述方式无效，可能需要查看 <code>transformers</code> 项目的 GitHub 页面或其他社区论坛，查找是否存在类似问题的解决方案，或者向项目作者报告此问题以求得帮助。</li></ol> 
<p>总之，您需要确保 <code>transformers</code> 和 <code>bitsandbytes</code> 之间的配合无缝无误，以避免类似 <code>get_loading_attributes</code> 方法缺失引发的加载失败问题。</p> 
<p></p> 
<p>这python项目的依赖管理似乎不如主流maven项目来的稳定，这种版本依赖关系应该是requirements.txt约定好。</p> 
<p></p> 
<p>不使用Int4版本</p> 
<p>model_dir = "/home/yeqiang/Downloads/ai/Qwen-VL-Chat"</p> 
<p>报错</p> 
<p>Traceback (most recent call last):<br>   File "/home/yeqiang/Downloads/src/Qwen-VL/test.py", line 19, in &lt;module&gt;<br>     model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto",<br>   File "/home/yeqiang/Downloads/src/Qwen-VL/venv/lib/python3.10/site-packages/modelscope/utils/hf_util.py", line 113, in from_pretrained<br>     module_obj = module_class.from_pretrained(model_dir, *model_args,<br>   File "/home/yeqiang/Downloads/src/Qwen-VL/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 558, in from_pretrained<br>     return model_class.from_pretrained(<br>   File "/home/yeqiang/Downloads/src/Qwen-VL/venv/lib/python3.10/site-packages/modelscope/utils/hf_util.py", line 76, in from_pretrained<br>     return ori_from_pretrained(cls, model_dir, *model_args, **kwargs)<br>   File "/home/yeqiang/Downloads/src/Qwen-VL/venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3481, in from_pretrained<br>     hf_quantizer.validate_environment(device_map=device_map)<br>   File "/home/yeqiang/Downloads/src/Qwen-VL/venv/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py", line 86, in validate_environment<br>     raise ValueError(<br> ValueError: <br>                   <span style="color:#fe2c24;">  Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the<br>                     quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules<br>                     in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to<br>                     `from_pretrained`. Check<br>                     https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu<br>                     for more details.</span></p> 
<p></p> 
<p></p> 
<p></p> 
<h2>4060ti 16G</h2> 
<p><img alt="" height="754" src="https://images2.imgbox.com/fa/0f/hyCxEdcO_o.png" width="1200"></p> 
<p>根据提示，安装python包</p> 
<pre><code class="hljs">pip install optimum
pip install auto-gptq</code></pre> 
<p>大约10秒启动成功</p> 
<p><img alt="" height="425" src="https://images2.imgbox.com/39/ee/HWLCZIG8_o.png" width="1200"></p> 
<p>显卡状态</p> 
<p><img alt="" height="597" src="https://images2.imgbox.com/a3/23/bQJ73TX3_o.png" width="819"></p> 
<p>访问web</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/23/8b/GZ90Szxo_o.png" width="1108"></p> 
<p>测试对话，速度飞快，没有等待（但是回答的内容与问题对不上！）</p> 
<p><img alt="" height="275" src="https://images2.imgbox.com/2d/da/8x0Xm5sa_o.png" width="988"></p> 
<p>对比在线版本（这才是正确的内容，最近在线版本速度明显变慢了）</p> 
<p><img alt="" height="948" src="https://images2.imgbox.com/74/e7/plnDSb6f_o.png" width="1018"></p> 
<p>测试图片内容的识别</p> 
<p><img alt="" height="642" src="https://images2.imgbox.com/fb/a9/xiAHCDpE_o.png" width="1200"></p> 
<p>这种复杂的验证码不能识别</p> 
<p><img alt="" height="458" src="https://images2.imgbox.com/d3/1d/LUfUA78V_o.png" width="938"></p> 
<p></p> 
<p>写作文（太弱了）</p> 
<p><img alt="" height="232" src="https://images2.imgbox.com/d1/cf/lOVj0OX2_o.png" width="1200"></p> 
<p>对比在线版本</p> 
<p><img alt="" height="560" src="https://images2.imgbox.com/63/8e/QnbqvH6p_o.png" width="1033"></p> 
<p></p> 
<p>总体评价：</p> 
<p>Int4这个版本模型能够给出的内容不多，实际意义不大。需要更大的显存跑更大的模型来提供优质的内容输出。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/959268803f1f32a7b510b1aa54a01374/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Kafka高可用之ISR机制：揭秘消息一致性背后的守护者</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5df2e2cc85e924a3c6ed8264b2f60f28/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Kafka 实战 - Kafka Consumer 重置 Offset</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>