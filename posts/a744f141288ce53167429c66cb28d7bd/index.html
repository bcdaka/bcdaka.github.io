<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【Python学习】网络爬虫-爬取豆瓣电影评论 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/a744f141288ce53167429c66cb28d7bd/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【Python学习】网络爬虫-爬取豆瓣电影评论">
  <meta property="og:description" content="一、实现目标 编写一个爬虫，获取豆瓣网站上“庆余年 第二季”这部电视剧的短评，网站如下：
# https://movie.douban.com/subject/34937650/comments?sort=new_score&amp;status=P #
二、实现步骤 我们在 Google Chrome浏览器中复制粘贴下面的链接，先看看网页内容，打开网页后可以看到，《庆余年 第二季》这部电视剧的相关短评，就在标注的红色方框内。这就是我们今天要获取的内容。
想要获取网页中的短评，首先要获取网页 HTML 代码，再把短评从中提取出来。
2.1 获取网页源码 获取网页中的 HTML 代码，我们可以使用 requests 模块的 get 方法来实现。
# 使用import导入requests模块 import requests # 将豆瓣电影评论URL地址，赋值给变量url url = &#34;https://movie.douban.com/subject/34937650/comments?sort=new_score&amp;status=P&#34; # 使用requests发起GET请求，赋值给response response = requests.get(url) # 使用print输出response.status_code print(response) 这里执行后返回的状态码是418，这意味我们的爬虫被服务端发现了，因为我们用的是 requests 发起的请求，而不是浏览器。
我们需要加上User-Agent参数，伪装成一个浏览器。User-Agent参数值的获取方法也很简单，在chome浏览器中右键检查，在网络功能中可以看到客户端访问服务的请求，在请求头中包括本机的User-Agent参数，直接拿来用即可，如下图。
将User-Agent参数的值以字典的形式的存储在headers参数中，然后在请求时带上headers参数。
# 使用import导入requests模块 import requests # 将豆瓣电影评论URL地址，赋值给变量url url = &#34;https://movie.douban.com/subject/34937650/comments?sort=new_score&amp;status=P&#34; # 将User-Agent以字典键对形式赋值给headers headers = {&#34;User-Agent&#34;: &#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-09T23:44:23+08:00">
    <meta property="article:modified_time" content="2024-06-09T23:44:23+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【Python学习】网络爬虫-爬取豆瓣电影评论</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>一、实现目标</h2> 
<p>        编写一个爬虫，获取豆瓣网站上“庆余年 第二季”这部电视剧的短评，网站如下：</p> 
<p># https://movie.douban.com/subject/34937650/comments?sort=new_score&amp;status=P #</p> 
<h2>二、实现步骤</h2> 
<p>        我们在 Google Chrome浏览器中复制粘贴下面的链接，先看看网页内容，打开网页后可以看到，《庆余年 第二季》这部电视剧的相关短评，就在标注的红色方框内。这就是我们今天要获取的内容。</p> 
<p>        想要获取网页中的短评，首先要获取网页 HTML 代码，再把短评从中提取出来。</p> 
<p class="img-center"><img alt="" height="594" src="https://images2.imgbox.com/7e/e4/XGLrVBt6_o.png" width="1092"></p> 
<h3>2.1 获取网页源码</h3> 
<p>        获取网页中的 HTML 代码，我们可以使用 requests 模块的 get 方法来实现。</p> 
<pre><code class="language-python"># 使用import导入requests模块
import requests

# 将豆瓣电影评论URL地址，赋值给变量url
url = "https://movie.douban.com/subject/34937650/comments?sort=new_score&amp;status=P"

# 使用requests发起GET请求，赋值给response
response = requests.get(url)

# 使用print输出response.status_code
print(response)</code></pre> 
<p>        这里执行后返回的状态码是418，这意味我们的爬虫被服务端发现了，因为我们用的是 requests 发起的请求，而不是浏览器。</p> 
<p class="img-center"><img alt="" height="40" src="https://images2.imgbox.com/05/a0/2TccRV7m_o.png" width="398"></p> 
<p>        我们需要加上User-Agent参数，伪装成一个浏览器。User-Agent参数值的获取方法也很简单，在chome浏览器中右键检查，在网络功能中可以看到客户端访问服务的请求，在请求头中包括本机的User-Agent参数，直接拿来用即可，如下图。</p> 
<p class="img-center"><img alt="" height="691" src="https://images2.imgbox.com/45/e2/pDZDXw3a_o.png" width="1200"></p> 
<p>        将User-Agent参数的值以字典的形式的存储在headers参数中，然后在请求时带上headers参数。</p> 
<pre><code class="language-python"># 使用import导入requests模块
import requests

# 将豆瓣电影评论URL地址，赋值给变量url
url = "https://movie.douban.com/subject/34937650/comments?sort=new_score&amp;status=P"
# 将User-Agent以字典键对形式赋值给headers
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"}

# 使用requests发起GET请求，赋值给response
response = requests.get(url, headers=headers)

# 使用print输出response.status_code
print(response)</code></pre> 
<p>        然后响应状态码就变成了 200 ，这说明我们请求成功了。</p> 
<p class="img-center"><img alt="" height="420" src="https://images2.imgbox.com/36/7d/G2967j5v_o.png" width="700"></p> 
<p>        那么得到的网页内容在那能看到呢？网页的内容我们可以使用.text属性可以获取。但这个内容很长，不好分析，我们可以用切片的方法，先输出前1000个字符。</p> 
<p class="img-center"><img alt="" height="646" src="https://images2.imgbox.com/02/64/aXlSvC8w_o.png" width="929"></p> 
<h3>2.2 解析源码获取数据</h3> 
<p>        上一步获取到网页 HTML 代码。接下来，就需要解析网页，从网页中提取想要的数据。解析网页我们使用 BeautifulSoup 解析，这里就不需要做切片了，我们要解析整个网页内容。我们先导入 BeautifulSoup，并创建一个 BeautifulSoup 对象：</p> 
<pre><code class="language-python"># 从bs4中导入BeautifulSoup
from bs4 import BeautifulSoup

# 使用BeautifulSoup()传入变量html和解析器lxml，赋值给soup
soup = BeautifulSoup(html, "lxml")

# 使用print输出soup
print(soup)</code></pre> 
<p>        接下来就要需要从 soup 中提取出短评所在的标签节点，我们可以使用 BeautifulSoup 中的 find_all() 函数，它可以查询所有符合条件的元素，并组成一个列表。</p> 
<p class="img-center"><img alt="" height="362" src="https://images2.imgbox.com/2b/ef/xOfPTobD_o.png" width="1200"></p> 
<p>        在这里，find_all() 函数中传入的参数内容是什么呢？我们需要去网页中找到短评所在的节点。定位方法如下图：</p> 
<p class="img-center"><img alt="" height="616" src="https://images2.imgbox.com/e9/90/3fJJ5vYZ_o.png" width="1200"></p> 
<p>        仔细观察发现，所有的评论都在 &lt;span&gt; 标签中，查找span标签在网页中出现了100多次，而显然，该页只有二十条评论。也可以试试以 &lt;p&gt; 节点来定位，网页中p标签出现的次数也大于20。那么我们该定位哪个节点，才能提取评论呢？</p> 
<p class="img-center"><img alt="" height="613" src="https://images2.imgbox.com/8c/82/lWkh66Nz_o.png" width="1200"></p> 
<p>        多对照几条评论我们发现，短评所在的节点都是类似的它们都有一样的标签和属性，如下图所示。</p> 
<p class="img-center"><img alt="" height="64" src="https://images2.imgbox.com/b7/da/vgyh71PH_o.png" width="789"></p> 
<p><br>         在 HTML 中，<a href="https://baike.baidu.com/item/span%E6%A0%87%E7%AD%BE/2421838" rel="nofollow" title="span 标签">span 标签</a>的作用是用来组合文档中的行内元素。class 属性的作用是用来给标签分类。class="short"也就代表着，这里标签的内容是短评。</p> 
<p>        在网页中属性需要使用 (点).属性值 来定位。例如，图中我们在检索框中输入 .short 进行搜索，就能看到该页中 class="short" 属性出现的次数为20次，这些就是我们想要的内容。</p> 
<p class="img-center"><img alt="" height="608" src="https://images2.imgbox.com/1a/bb/8XQg6wSt_o.png" width="1200"></p> 
<p>        那么，就定位到了提取评论的节点 class="short" 。以属性为查询条件就需要在 find_all() 中，传入 class="short"。由于 class 在 Python 里是一个关键字 ，所以后面需要加一个下划线，即 class_="short"。</p> 
<pre><code class="language-python"># 使用find_all()查询soup中class="short"的节点，赋值给content_all
content_all = soup.find_all(class_="short")</code></pre> 
<p>        如下图，我们获得了一个带标签的短语列表。</p> 
<p class="img-center"><img alt="" height="663" src="https://images2.imgbox.com/69/fd/8JERX6Rz_o.png" width="1046"></p> 
<p>        标签对我们来说没有用，我们需要去掉它，拿到标签里的内容， 访问 .string 获取每个节点中标签的内容。由于 find_all() 返回的是一个列表，我们不能直接访问 .string 属性。需要使用 for 循环遍历列表 content_all。</p> 
<pre><code class="language-python">for content in content_all:
    # 使用.string获取标签里的内容
    contentString = content.string
     
    # 使用print打印评论    
    print(contentString)</code></pre> 
<p>        如下图，我们就获得了第一页的20条评论</p> 
<p class="img-center"><img alt="" height="661" src="https://images2.imgbox.com/18/4c/1Pf5nOAs_o.png" width="1057"></p> 
<h3>2.3 保存评论</h3> 
<p>        为保存评论，我们在前面定义一个 comments 列表，用于临时存储评论内容，在最后使用with open() 将 comments 列表的中的评论写入本地的.txt文件中。</p> 
<pre><code class="language-python"># 定义一个列表，保存评论
comments = []

......

    # 保存评论到新的列表中
    comments.append(contentString)


# 保存评论到txt文件中
with open("D:/学习资料/Python/VSCode/yequbiancheng/网络爬虫/豆瓣评论_庆余庆.txt", "a", encoding='utf-8') as fp:
    # 将评论逐个写入txt文件中
    for comment in comments:
        fp.write(comment + "\n")
print("保存完毕")</code></pre> 
<p>        结果如下图</p> 
<p class="img-center"><img alt="" height="656" src="https://images2.imgbox.com/ed/34/oHL3ZhnW_o.png" width="1012"></p> 
<h3>2.4 优化代码获取更多评论 </h3> 
<p>        现在我们获得了第一页的20条评论，可我们需要获得更多评论。我们点击后页，把每一页的URL地址拿出来对比一下。</p> 
<p class="img-center"><img alt="" height="677" src="https://images2.imgbox.com/a8/cb/oUmLWf9B_o.png" width="875"></p> 
<p>        通过对比第一页的URL，我们发现除了第一页，其它都有两个共同的参数start和limit，很明显start表示的该页第一条评论的编号，limit是该页的评论条数。</p> 
<p class="img-center"><img alt="" height="82" src="https://images2.imgbox.com/19/4a/lNrNNoBL_o.png" width="1016"></p> 
<p>       我们对URL进行一下改造，将limit设置为100，表示获取100条，使用 for 循环连续获取 5 页的评论。</p> 
<pre><code class="language-python">for page in range(5):
    # 将豆瓣电影评论URL地址，赋值给变量url
    url = f"https://movie.douban.com/subject/34937650/comments?start={page*100}&amp;limit=100&amp;status=P&amp;sort=new_score"</code></pre> 
<p>        完整的代码如下：</p> 
<pre><code class="language-python"># 使用import导入requests模块
import requests
# 从bs4中导入BeautifulSoup
from bs4 import BeautifulSoup


# 使用for循环，获取前10页的评论
for page in range(5):

    # 定义一个列表，保存评论
    comments = []
    # 将豆瓣电影评论URL地址，赋值给变量url
    url = f"https://movie.douban.com/subject/34937650/comments?start={page*100}&amp;limit=100&amp;status=P&amp;sort=new_score"
    # 将User-Agent以字典键对形式赋值给headers
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"}

    # 使用requests发起GET请求，赋值给response
    response = requests.get(url, headers=headers)

    # 使用.text属性获取网页的内容，并赋值给html
    html = response.text

    # 使用BeautifulSoup()传入变量html和解析器lxml，赋值给soup
    soup = BeautifulSoup(html, "lxml")

    # 使用find_all()查询soup中class="short"的节点，赋值给content_all
    content_all = soup.find_all(class_="short")

    for content in content_all:
        # 使用.string获取标签里的内容
        contentString = content.string
        
        # 保存评论到新的列表中
        comments.append(contentString)

    # 保存评论到txt文件中
    with open("D:/学习资料/Python/VSCode/yequbiancheng/网络爬虫/豆瓣评论_庆余庆.txt", "a", encoding='utf-8') as fp:
        # 将评论逐个写入txt文件中
        for comment in comments:
            # 写入评论并回车
            fp.write(comment + "\n")
    print(f"第{page}页保存完毕")
</code></pre> 
<p>        执行结果如下</p> 
<p><img alt="" height="655" src="https://images2.imgbox.com/d6/9e/cqrzW9y3_o.png" width="1009"></p> 
<h2>三、总结</h2> 
<p>        本实例实现了庆余年的豆瓣电影评论爬取，使用了requests、bs4模块。</p> 
<p>        <strong>requests</strong>：是一个常用的 HTTP 请求库，可以方便地向网站发送 HTTP 请求，并获取响应结果。requests 模块比 <a href="https://www.runoob.com/python3/python-urllib.html" rel="nofollow" title="urllib">urllib</a> 模块更简洁。</p> 
<p>        <strong>bs4</strong>：全名 <a href="https://so.csdn.net/so/search?q=BeautifulSoup&amp;spm=1001.2101.3001.7020" title="BeautifulSoup">BeautifulSoup</a>，是编写 python 爬虫常用库之一，主要用来解析 html 标签。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/87b574cd2a689adbdb010aeaaf8db924/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">C&#43;&#43; 高性能爬虫</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2624315c1d890ccc2e100c6aee11e015/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">图像处理方向信息</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>