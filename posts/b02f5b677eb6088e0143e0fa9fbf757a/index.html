<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>本地离线模型搭建指南-LLaMA-Factory训练框架及工具 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/b02f5b677eb6088e0143e0fa9fbf757a/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="本地离线模型搭建指南-LLaMA-Factory训练框架及工具">
  <meta property="og:description" content="搭建一个本地中文大语言模型（LLM）涉及多个关键步骤，从选择模型底座，到运行机器和框架，再到具体的架构实现和训练方式。以下是一个详细的指南，帮助你从零开始构建和运行一个中文大语言模型。
本地离线模型搭建指南将按照以下四个部分展开
中文大语言模型底座选择依据本地运行显卡选择RAG架构实现LLaMA-Factory训练框架及工具 4 训练架构及工具 4.1 为什么要使用LLaMA-Factory进行训练 LLaMA-Factory是一个专为大模型训练设计的开源平台，具有以下几个优势：
快速学习和应用： 对于没有微调大模型经验的用户，通过学习LLaMA-Factory后，可以快速地训练出自己需要的模型。理解微调技术： 对于技术人员，LLaMA-Factory提供了一个很好的学习平台，通过阅读源码，可以深入了解大模型的微调技术。捷径： LLaMA-Factory为用户提供了一条走向大模型微调的捷径，使用户能够快速掌握相关概念和技术。 4.2 LLaMA-Factory训练所能解决的问题 大模型的实际应用： 企业想要利用大模型进行实际应用时，必须懂得微调的过程，而LLaMA-Factory提供了一个实用的平台来实现这一点。个性化模型训练： 用户可以通过LLaMA-Factory快速训练出适合自己需求的模型，满足不同领域的具体要求。技术理解和提升： 技术人员可以通过LLaMA-Factory的源码学习，进一步理解大模型的微调技术，并应用于实际项目中。 4.3 LLaMA-Factory的训练步骤和方法 4.3.1 模型训练阶段 预训练阶段（Pre-Training）：
预训练是大模型训练的初始阶段，主要目的是通过大规模数据集训练基础模型。这一步是最消耗计算资源的，通常需要使用大量的计算集群。监督微调阶段（Supervised Finetuning, SFT）：
这个阶段的训练数据质量较高，通常由人工筛选或生成。经过这个阶段的模型已经具备上线的能力。基于人类反馈的强化学习（RLHF）： 奖励建模阶段（Reward Modeling）： 在这个阶段，模型不仅输出预测的内容，还输出一个奖励值（评分值），用于后续的强化学习。强化学习阶段（Reinforcement Learning）： 通过奖励模型对多个输出进行评分，并基于评分进行加权，反向传播调整模型参数。 4.3.2 模型训练模式 根据具体需求，可以选择不同的训练模式：
模式一： 基于base模型 &#43; 领域任务的SFT模式二： 基于base模型 &#43; 领域数据 continue pre-train &#43; 领域任务SFT模式三： 基于base模型 &#43; 领域数据 continue pre-train &#43; 通用任务SFT &#43; 领域任务SFT模式四： 基于base模型 &#43; 领域数据 continue pre-train &#43; 通用任务与领域任务混合SFT模式五： 基于base模型 &#43; 领域数据 continue pre-train（混入SFT数据 &#43; 通用任务与领域任务混合SFT）模式六： 基于chat模型 &#43; 领域任务SFT模式七： 基于chat模型 &#43; 领域数据 continue pre-train &#43; 领域任务SFT 4.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-23T17:38:43+08:00">
    <meta property="article:modified_time" content="2024-06-23T17:38:43+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">本地离线模型搭建指南-LLaMA-Factory训练框架及工具</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>搭建一个本地中文大语言模型（LLM）涉及多个关键步骤，从选择模型底座，到运行机器和框架，再到具体的架构实现和训练方式。以下是一个详细的指南，帮助你从零开始构建和运行一个中文大语言模型。</p> 
<p>本地离线模型搭建指南将按照以下四个部分展开</p> 
<ol><li><a href="https://blog.csdn.net/Master_Shifu_/article/details/139903072">中文大语言模型底座选择依据</a></li><li><a href="https://blog.csdn.net/Master_Shifu_/article/details/139902029">本地运行显卡选择</a></li><li><a href="https://blog.csdn.net/Master_Shifu_/article/details/139902910">RAG架构实现</a></li><li><a href="https://blog.csdn.net/Master_Shifu_/article/details/139903210">LLaMA-Factory训练框架及工具</a></li></ol> 
<h2><a id="4__11"></a>4 训练架构及工具</h2> 
<h3><a id="41_LLaMAFactory_13"></a>4.1 为什么要使用LLaMA-Factory进行训练</h3> 
<p>LLaMA-Factory是一个专为大模型训练设计的开源平台，具有以下几个优势：</p> 
<ol><li>快速学习和应用： 对于没有微调大模型经验的用户，通过学习LLaMA-Factory后，可以快速地训练出自己需要的模型。</li><li>理解微调技术： 对于技术人员，LLaMA-Factory提供了一个很好的学习平台，通过阅读源码，可以深入了解大模型的微调技术。</li><li>捷径： LLaMA-Factory为用户提供了一条走向大模型微调的捷径，使用户能够快速掌握相关概念和技术。</li></ol> 
<h3><a id="42_LLaMAFactory_21"></a>4.2 LLaMA-Factory训练所能解决的问题</h3> 
<ol><li>大模型的实际应用： 企业想要利用大模型进行实际应用时，必须懂得微调的过程，而LLaMA-Factory提供了一个实用的平台来实现这一点。</li><li>个性化模型训练： 用户可以通过LLaMA-Factory快速训练出适合自己需求的模型，满足不同领域的具体要求。</li><li>技术理解和提升： 技术人员可以通过LLaMA-Factory的源码学习，进一步理解大模型的微调技术，并应用于实际项目中。</li></ol> 
<h3><a id="43_LLaMAFactory_27"></a>4.3 LLaMA-Factory的训练步骤和方法</h3> 
<h5><a id="431__29"></a>4.3.1 模型训练阶段</h5> 
<ol><li>预训练阶段（Pre-Training）：<br> 预训练是大模型训练的初始阶段，主要目的是通过大规模数据集训练基础模型。这一步是最消耗计算资源的，通常需要使用大量的计算集群。</li><li>监督微调阶段（Supervised Finetuning, SFT）：<br> 这个阶段的训练数据质量较高，通常由人工筛选或生成。经过这个阶段的模型已经具备上线的能力。</li><li>基于人类反馈的强化学习（RLHF）： 
  <ol><li>奖励建模阶段（Reward Modeling）： 在这个阶段，模型不仅输出预测的内容，还输出一个奖励值（评分值），用于后续的强化学习。</li><li>强化学习阶段（Reinforcement Learning）： 通过奖励模型对多个输出进行评分，并基于评分进行加权，反向传播调整模型参数。</li></ol> </li></ol> 
<h5><a id="432__39"></a>4.3.2 模型训练模式</h5> 
<p>根据具体需求，可以选择不同的训练模式：</p> 
<ol><li>模式一： 基于base模型 + 领域任务的SFT</li><li>模式二： 基于base模型 + 领域数据 continue pre-train + 领域任务SFT</li><li>模式三： 基于base模型 + 领域数据 continue pre-train + 通用任务SFT + 领域任务SFT</li><li>模式四： 基于base模型 + 领域数据 continue pre-train + 通用任务与领域任务混合SFT</li><li>模式五： 基于base模型 + 领域数据 continue pre-train（混入SFT数据 + 通用任务与领域任务混合SFT）</li><li>模式六： 基于chat模型 + 领域任务SFT</li><li>模式七： 基于chat模型 + 领域数据 continue pre-train + 领域任务SFT</li></ol> 
<h5><a id="433_continue_pretrain_51"></a>4.3.3 是否需要继续预训练（continue pre-train）</h5> 
<ol><li>数据集差异大： 如果领域任务的数据集与预训练的数据集差异较大，建议进行continue pre-train。</li><li>数据量较大： 如果领域任务的数据量较大（token在1B以上），并且只追求领域任务的效果，也建议进行continue pre-train。</li></ol> 
<h5><a id="434_chatbase_56"></a>4.3.4 选择chat模型还是base模型</h5> 
<ol><li>base模型： 如果希望模型的通用能力不下降，建议选择base模型进行多任务混合训练。</li><li>chat模型： 如果只追求领域任务的效果，可以选择chat模型进行领域任务的SFT。</li></ol> 
<h3><a id="44__61"></a>4.4 实践建议</h3> 
<ol><li>资源充足： 如只考虑领域任务效果，建议选择模式二；如考虑模型综合能力，建议选择模式五。</li><li>资源有限： 建议选择模式六。</li><li>一般情况下： 不进行RLHF微调。</li></ol> 
<h3><a id="45__67"></a>4.5 开发工具库</h3> 
<p>LLaMA-Factory使用了一些关键的开发工具库，包括Transformers和PEFT库，后者提供了多种高效的微调方法，如LoRA、AdaLoRA、P-tuning等。</p> 
<p>通过以上步骤和方法，用户可以有效地使用LLaMA-Factory进行大模型的微调训练，满足不同领域和任务的需求。</p> 
<h3><a id="46_LLaMAFactory_73"></a>4.6 本地LLaMA-Factory训练模型实践运行配置</h3> 
<table><thead><tr><th>底座</th><th>包含模型</th><th>模型参数大小</th><th>机器配置</th><th>显存大小</th><th>是否可运行</th></tr></thead><tbody><tr><td>ChatGLM</td><td><a href="https://github.com/THUDM/ChatGLM3">ChatGLM3-6B</a></td><td>6B</td><td>CPU 16核心 32G内存</td><td>无</td><td>未测试</td></tr><tr><td>ChatGLM</td><td><a href="https://github.com/THUDM/ChatGLM3">ChatGLM3-6B</a></td><td>6B</td><td>显卡4070Ti</td><td>16G</td><td>可以</td></tr><tr><td>ChatGLM</td><td><a href="https://modelscope.cn/models/ZhipuAI/glm-4-9b" rel="nofollow">ChatGLM4-9B</a></td><td>9B</td><td>显卡4070Ti</td><td>16G</td><td>未测试</td></tr><tr><td>LLaMA</td><td><a href="https://github.com/michael-wzhu/Chinese-LlaMA2">Chinese-LlaMA2</a></td><td>7B</td><td>显卡3060Ti</td><td>8G</td><td>未测试</td></tr><tr><td>Qwen</td><td><a href="https://github.com/QwenLM/Qwen">Qwen1.5</a></td><td>7B</td><td>显卡4070Ti</td><td>16G</td><td>可以</td></tr></tbody></table> 
<p>更多详细信息，请访问</p> 
<p><a href="https://www.cnblogs.com/lm970585581/p/18140564" rel="nofollow">从LLaMA-Factory项目认识微调</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/695287607" rel="nofollow">LLaMA-Factory QuickStart</a></p> 
<h2><a id="5__89"></a>5 结论</h2> 
<p>搭建本地中文大语言模型是一个复杂而系统的工程，涉及模型选择、硬件配置、框架选择、架构实现、训练工具和方法等多个方面。通过合理选择和配置，可以在本地成功搭建并运行一个高性能的中文大语言模型。</p> 
<p>更多详细信息，请访问</p> 
<p><a href="https://www.cnblogs.com/lm970585581/p/18140564" rel="nofollow">从LLaMA-Factory项目认识微调</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/695287607" rel="nofollow">LLaMA-Factory QuickStart</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6ea17d3c3c3dd6e69d69faa124214e33/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Spark web UI 介绍</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/245857a79458d9b7d51f2f1f080516a3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">数据结构（链表的增删改查）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>