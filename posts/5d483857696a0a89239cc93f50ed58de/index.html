<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>手把手教你微调Stable Diffusion - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/5d483857696a0a89239cc93f50ed58de/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="手把手教你微调Stable Diffusion">
  <meta property="og:description" content="温馨提示 关于本文：
本文你可以学习到完整的不使用webui借助lora和dreambooth微调Stable Diffusion的全过程。
手把手教你微调Stable Diffusion生成优弧，但是半失败版😂
关于训练：
单卡32GV100进行的微调，因为一些训练策略显存只需要16G就够了。
训练用时一个半小时多一点点。根据自己显卡量力而行。
搞环境 先搞个虚拟环境：
conda create -n youhu
conda activate youhu scipy
进入虚拟环境了。开搞。
因为我们是使用DreamBooth对Stable Diffusion进行微调，所以先把Diffusion Model的库搞下来。
git clone https://github.com/huggingface/diffusers.git
下载成功之后你现在会看到多出来一个diffuser文件。
然后进入到这个文件夹了里。开始安排环境
pip install -e .
进入examples/dreambooth目录，继续安排环境的依赖包：
pip install -r requirements_sdxl.txt
pip install bitsandbytes xformers
配置一下accelerate的环境
accelerate config default
数据集 接下来就是准备几个你小子的图。放到examples/dreambooth目录下。
准备脚本 打开vim写个脚本，代码下拉可以直接复制。
这个脚本是使用你刚才的图片通过Dreambooth微调Stable Diffusion模型。
export MODEL_NAME=&#34;./stable-diffusion-xl-base-1.0&#34; export INSTANCE_DIR=&#34;yh&#34; export OUTPUT_DIR=&#34;lora-trained-xl&#34; # export VAE_PATH=&#34;madebyollin/sdxl-vae-fp16-fix&#34; python train_dreambooth_lora_sdxl.py \ --pretrained_model_name_or_path=$MODEL_NAME \ --instance_data_dir=$INSTANCE_DIR \ --output_dir=$OUTPUT_DIR \ --instance_prompt=&#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-10T10:40:23+08:00">
    <meta property="article:modified_time" content="2024-05-10T10:40:23+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">手把手教你微调Stable Diffusion</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atelier-sulphurpool-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_2"></a>温馨提示</h2> 
<p><strong>关于本文：</strong></p> 
<p>本文你可以学习到完整的<strong>不使用webui</strong>借助lora和dreambooth微调Stable Diffusion的全过程。</p> 
<p>手把手教你微调Stable Diffusion生成优弧，但是半失败版😂</p> 
<p><strong>关于训练：</strong></p> 
<blockquote> 
 <p>单卡32GV100进行的微调，因为一些训练策略显存只需要16G就够了。</p> 
 <p><img src="https://images2.imgbox.com/55/87/5E8kfGzq_o.png" alt="image.png"></p> 
 <p>训练用时一个半小时多一点点。根据自己显卡量力而行。</p> 
 <p><img src="https://images2.imgbox.com/7b/f9/ICAr2hWk_o.png" alt="image.png"></p> 
</blockquote> 
<hr> 
<h2><a id="_23"></a>搞环境</h2> 
<p>先搞个虚拟环境：</p> 
<p><code>conda create -n youhu</code></p> 
<p><code>conda activate youhu scipy</code></p> 
<p>进入虚拟环境了。开搞。</p> 
<p><img src="https://images2.imgbox.com/a7/4d/3ctEOK2Y_o.png" alt="image.png"></p> 
<p>因为我们是使用DreamBooth对Stable Diffusion进行微调，所以先把Diffusion Model的库搞下来。</p> 
<p><code>git clone https://github.com/huggingface/diffusers.git</code></p> 
<p>下载成功之后你现在会看到多出来一个diffuser文件。</p> 
<p><img src="https://images2.imgbox.com/2b/73/ipEoLCTM_o.png" alt="image.png"></p> 
<p>然后进入到这个文件夹了里。开始安排环境</p> 
<p><code>pip install -e .</code></p> 
<p><img src="https://images2.imgbox.com/b9/7c/2A7OGCKB_o.png" alt="image.png"></p> 
<p>进入<code>examples/dreambooth</code>目录，继续安排环境的依赖包：</p> 
<p><code>pip install -r requirements_sdxl.txt</code></p> 
<p><code>pip install bitsandbytes xformers</code></p> 
<p>配置一下accelerate的环境</p> 
<p><code>accelerate config default</code></p> 
<p><img src="https://images2.imgbox.com/30/42/sKY24yoK_o.png" alt="image.png"></p> 
<h2><a id="_62"></a>数据集</h2> 
<p>接下来就是准备几个你小子的图。放到<code>examples/dreambooth</code>目录下。</p> 
<p><img src="https://images2.imgbox.com/d8/3e/e7R4OefX_o.png" alt="image.png"></p> 
<p><img src="https://images2.imgbox.com/99/44/qcrUWMcg_o.png" alt="image.png"></p> 
<h2><a id="_71"></a>准备脚本</h2> 
<p>打开vim写个脚本，代码下拉可以直接复制。</p> 
<p>这个脚本是使用你刚才的图片通过Dreambooth微调Stable Diffusion模型。</p> 
<p><img src="https://images2.imgbox.com/2c/28/mH1oVdV1_o.png" alt="image.png"></p> 
<pre><code>export MODEL_NAME="./stable-diffusion-xl-base-1.0"
export INSTANCE_DIR="yh"
export OUTPUT_DIR="lora-trained-xl"
# export VAE_PATH="madebyollin/sdxl-vae-fp16-fix"

python train_dreambooth_lora_sdxl.py \
  --pretrained_model_name_or_path=$MODEL_NAME  \
  --instance_data_dir=$INSTANCE_DIR \
  --output_dir=$OUTPUT_DIR \
  --instance_prompt="upper_body, 1 boy, glasses, youhu, nixiaozi" \
  --resolution=1024 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --learning_rate=1e-5 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --max_train_steps=500 \
  --validation_prompt="youhu, nixiaozi" \
  --validation_epochs=25 \
  --seed="0" \
  --enable_xformers_memory_efficient_attention \
  --gradient_checkpointing \
  --use_8bit_adam \
  # --mixed_precision="fp16" \
  # --pretrained_vae_model_name_or_path=$VAE_PATH \

</code></pre> 
<p>这里解释一下。这个脚本的主要目的是配置和运行一个Stable Diffusion模型的训练过程，包括模型的参数设置、数据路径、学习率、批处理大小等。你可以根据自己的需求修改这些参数，然后运行脚本来训练模型。</p> 
<ol><li> <p><code>export MODEL_NAME="stabilityai/stable-diffusion-xl-base-1.0"</code>：设置了一个名为 <code>MODEL_NAME</code> 的环境变量，该变量指定了预训练模型的名称或路径。</p> </li><li> <p><code>export INSTANCE_DIR="yh"</code>：设置了一个名为 <code>INSTANCE_DIR</code> 的环境变量，该变量指定了实例数据的目录或路径。</p> </li><li> <p><code>export OUTPUT_DIR="lora-trained-xl"</code>：设置了一个名为 <code>OUTPUT_DIR</code> 的环境变量，该变量指定了模型训练结果的输出目录或路径。</p> </li><li> <p><code>python train_dreambooth_lora_sdxl.py</code> 命令，它实际执行了模型训练的操作。以下是命令中的参数和选项的解释：</p> 
  <ul><li> <p><code>--pretrained_model_name_or_path=$MODEL_NAME</code>：指定了预训练模型的名称或路径，使用了之前设置的 <code>MODEL_NAME</code> 环境变量。</p> </li><li> <p><code>--instance_data_dir=$INSTANCE_DIR</code>：指定了实例数据的目录或路径，使用了之前设置的 <code>INSTANCE_DIR</code> 环境变量。</p> </li><li> <p><code>--output_dir=$OUTPUT_DIR</code>：指定了模型训练结果的输出目录或路径，使用了之前设置的 <code>OUTPUT_DIR</code> 环境变量。</p> </li><li> <p><code>--instance_prompt="a photo of youhu"</code>：设置了实例提示，描述了输入数据的内容。</p> </li><li> <p><code>--resolution=1024</code>：指定了训练过程中使用的分辨率。</p> </li><li> <p><code>--train_batch_size=1</code>：指定了训练时的批量大小。</p> </li><li> <p><code>--gradient_accumulation_steps=4</code>：指定了梯度累积的步数。</p> </li><li> <p><code>--learning_rate=1e-5</code>：指定了学习率的初始值。</p> </li><li> <p><code>--lr_scheduler="constant"</code>：选择了学习率调度器的类型，这里是常数学习率。</p> </li><li> <p><code>--lr_warmup_steps=0</code>：指定了学习率预热的步数。</p> </li><li> <p><code>--max_train_steps=500</code>：指定了最大训练步数。</p> </li><li> <p><code>--validation_prompt="youhu, ni xiao zi"</code>：设置了验证提示，描述了用于验证的输入数据的内容。</p> </li><li> <p><code>--seed="0"</code>：设置了随机种子，以确保训练的可重复性。</p> </li><li> <p><code>--enable_xformers_memory_efficient_attention</code>：启用了XFormers模块的内存效率注意力机制。</p> </li><li> <p><code>--gradient_checkpointing</code>：启用了梯度检查点，以减少内存使用。</p> </li><li> <p><code>--use_8bit_adam</code>：使用了8位Adam优化器，这可以提高训练速度。</p> </li><li> <p><strong>最后两行的注释部分是选项的注释，</strong> Stable Diffusion原装VAE存在数值不稳定的问题，因此hugging face官方一般都推荐换一个VAE模型。但是推荐的那个VAE模型实际使用起来有冲突，会一直报错。所以这里还是使用原装VAE吧。</p> </li></ul> </li></ol> 
<h2><a id="_154"></a>缓存模型</h2> 
<p>接下来还不能直接运行。因为墙的原因，你的服务器没办法直接从抱抱脸上直接下载模型。所以我的建议是：</p> 
<p><strong>在<code>examples/dreambooth</code>目录下建一个文件夹<code>stable-diffusion-xl-base-1.0</code></strong></p> 
<p><img src="https://images2.imgbox.com/8a/e3/NGruDlAy_o.png" alt="image.png"></p> 
<p>其他解决方法：</p> 
<ol><li> <p>给你服务器上搞个梯子，开全局</p> </li><li> <p>本地缓存模型，上传到服务器缓存</p> </li></ol> 
<h2><a id="_171"></a>开始训练</h2> 
<p><code>bash train.sh</code></p> 
<p><img src="https://images2.imgbox.com/ba/79/6cbW9tLJ_o.png" alt="image.png"></p> 
<p>然后就等着他进度条就行了：</p> 
<p><img src="https://images2.imgbox.com/70/ee/fSOdbDmC_o.png" alt="image.png"></p> 
<p>到这样就是训练完了：</p> 
<p><img src="https://images2.imgbox.com/0a/35/gkkYBeRD_o.png" alt="image.png"></p> 
<h2><a id="_186"></a>开始生成</h2> 
<p>还是在<code>examples/dreambooth</code>目录下，搞一个python文件，我这里叫<code>generate.py</code></p> 
<p>直接复制下边的代码即可。</p> 
<pre><code>from diffusers import DiffusionPipeline
import torch

lora_model_id = './lora-trained-xl/pytorch_lora_weights.safetensors'
base_model_id = "./stable-diffusion-xl-base-1.0"

pipe = DiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16)
pipe = pipe.to("cuda")
pipe.load_lora_weights(lora_model_id)

prompt = ["youhu, ni xiao zi","youhu","ni xiao zi","ni xiao zi, youhu","a photo of youhu", "a photo of ni xiao zi"]

for p in prompt:
    image = pipe(p, num_inference_steps=50).images[0]
    image.save(f"{p}.png")

</code></pre> 
<ol><li> <p><code>from diffusers import DiffusionPipeline</code>：从<code>diffusers</code>模块导入<code>DiffusionPipeline</code>类。这个类提供了一个管道，用于执行扩散过程，这是生成图片的关键步骤。</p> </li><li> <p><code>import torch</code>：导入PyTorch库，这是一个用于深度学习的开源库。</p> </li><li> <p><code>lora_model_id = './lora-trained-xl/pytorch_lora_weights.safetensors'</code>：定义一个变量来存储预训练模型的权重文件路径。</p> </li><li> <p><code>base_model_id = "./stable-diffusion-xl-base-1.0"</code>：定义一个变量来存储基础模型的路径。</p> </li><li> <p><code>pipe = DiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16)</code>：使用基础模型的路径加载预训练的扩散管道，并设置数据类型为float16。</p> </li><li> <p><code>pipe = pipe.to("cuda")</code>：将管道移动到GPU设备上进行计算。</p> </li><li> <p><code>pipe.load_lora_weights(lora_model_id)</code>：加载预训练模型的权重到管道中。</p> </li><li> <p><code>prompt = ["youhu, ni xiao zi","youhu","ni xiao zi","ni xiao zi, youhu","a photo of youhu", "a photo of ni xiao zi"]</code>：定义一个列表，包含了多个生成图片的提示。</p> </li><li> <p><code>for p in prompt:</code>：对提示列表进行遍历，每次循环处理一个提示。</p> 
  <ul><li><code>image = pipe(p, num_inference_steps=50).images[0]</code>：通过管道生成一张图片。这里的字符串p是生成图片的提示，而<code>num_inference_steps=25</code>表示推理步骤的数量。</li><li><code>image.save(f"优弧{i}.png")</code>：将生成的图片保存为<code>优弧{i}.png</code>文件，其中<code>i</code>是当前循环的索引号。</li></ul> </li></ol> 
<p>然后就是生成过程：</p> 
<p><img src="https://images2.imgbox.com/0b/e8/s1eEQpdW_o.png" alt="image.png"></p> 
<h2><a id="_237"></a>结果图</h2> 
<p><img src="https://images2.imgbox.com/cb/fe/OMUoDjNc_o.png" alt="image.png"></p> 
<p>发个勉强能看的。😂</p> 
<p><img src="https://images2.imgbox.com/da/a1/aBwwP9my_o.png" alt="image.png"></p> 
<p>失败原因分析：</p> 
<ol><li> <p>训练集不足，且图片差异较大，分辨率过低（<strong>这个是主要原因</strong>）。</p> </li><li> <p>以及训练过程和验证过程prompt差异较大</p> </li><li> <p>没换VAE（因为<code>madebyollin/sdxl-vae-fp16-fix</code>）实在是用不了。</p> </li><li> <p>这个模型本身效果就一般。</p> </li></ol> 
<p>以上种种原因导致最后效果不尽人意😂。</p> 
<p><strong>毕竟别人用五张高清柯基，训练出了比较好的柯基图。</strong></p> 
<p>训练集：</p> 
<p><img src="https://images2.imgbox.com/10/0d/ziIOHTBa_o.png" alt="image.png"></p> 
<p>结果：</p> 
<p><img src="https://images2.imgbox.com/db/40/Ny93yTDN_o.png" alt="image.png"></p> 
<h3><a id="_271"></a>这里直接将该软件分享出来给大家吧~</h3> 
<h4><a id="1stable_diffusion_273"></a>1.stable diffusion安装包</h4> 
<p>随着技术的迭代，目前 Stable Diffusion 已经能够生成非常艺术化的图片了，完全有赶超人类的架势，已经有不少工作被这类服务替代，比如制作一个 logo 图片，画一张虚拟老婆照片，画质堪比相机。</p> 
<p>最新 Stable Diffusion 除了有win多个版本，就算说底端的显卡也能玩了哦！此外还带来了Mac版本，<strong>仅支持macOS 12.3或更高版本</strong>。</p> 
<p><img src="https://images2.imgbox.com/98/17/QaxWfPdh_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="2stable_diffusion_281"></a>2.stable diffusion视频合集</h4> 
<p>我们在学习的时候，往往书籍源码难以理解，阅读困难，这时候视频教程教程是就很适合了，生动形象加上案例实战，一步步带你入坑stable diffusion，科学有趣才能更方便的学习下去。</p> 
<p><img src="https://images2.imgbox.com/f1/29/nVEENoFP_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="3stable_diffusion_287"></a>3.stable diffusion模型下载</h4> 
<p>stable diffusion往往一开始使用时图片等无法达到理想的生成效果，这时则需要通过使用大量训练数据，调整模型的超参数（如学习率、训练轮数、模型大小等），可以使得模型更好地适应数据集，并生成更加真实、准确、高质量的图像。</p> 
<p><img src="https://images2.imgbox.com/e7/00/fscJ8sUv_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="4stable_diffusion_293"></a>4.stable diffusion提示词</h4> 
<p>提示词是构建由文本到图像模型解释和理解的单词的过程。可以把它理解为你告诉 AI 模型要画什么而需要说的语言，整个SD学习过程中都离不开这本提示词手册。</p> 
<p><img src="https://images2.imgbox.com/98/5a/4wSlYeg0_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="5SD0_299"></a>5.SD从0到落地实战演练</h4> 
<p><img src="https://images2.imgbox.com/68/61/ZY4Rg3T2_o.png" alt="在这里插入图片描述"></p> 
<p>如果你能在15天内完成所有的任务，那你堪称天才。然而，如果你能完成 60-70% 的内容，你就已经开始具备成为一名SD大神的正确特征了。</p> 
<p>这份完整版的stable diffusion资料我已经打包好，需要的点击下方添加，即可前往<strong>免费领取！</strong></p> 
<p>​<img src="https://images2.imgbox.com/0a/0e/w0j68qMM_o.jpg"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/20f97fefc6923caaef9e75b7177c48ee/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">GitHub Copilot 支持 IntelliJ IDEA啦，插件装起来！</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1f51cd0507b3f6f2ddc276fa1871b8cc/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">助力工业物联网，工业大数据之服务域：Shell调度测试【三十三】_shell调度系统</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>