<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Mac本地部署大模型-单机运行 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/06d81afcc42f653b6b9e910ec20928d3/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Mac本地部署大模型-单机运行">
  <meta property="og:description" content="前些天在一台linux服务器（8核，32G内存，无显卡）使用ollama运行阿里通义千问Qwen1.5和Qwen2.0低参数版本大模型，Qwen2-1.5B可以运行，但是推理速度有些慢。
一直还没有尝试在macbook上运行测试大模型，不知道单机部署是否会有压力？
恰好家里有一台平时用的较少的macbook，因此，简单做了一些测试，分享给大家，供参考和讨论。
电脑配置 所使用的Macbook Pro稍微有些老，配置如下：
机型，MacBook Pro（13 英寸，2019 年）；芯片，四核Intel Core i5 2.4GHz；图形卡：Intel Iris Plus Graphics 655 1536 MB =》1.5G显存，是否能有用？内存，16G；系统，macOS Sonoma 14.5 （原来系统Catalina 10.15，发现没有brew命令，安装提示系统版本太低，apple已不再支持；同时系统提示，可以升级到最新mac OS，于是安装升级到最新的系统Sonoma，点击这里查询macOS Sonoma 与哪些电脑兼容）
Ollama下载安装 我们简单实用Ollama方式运行大模型。
首先从Ollama官网下载安装包。
Download Ollama on macOS
直接点击maxOS下载即可，下载的文件是Ollama.zip压缩文件，解压后是应用程序（ollama.apk），直接双击运行安装就可以。
安装完后，在启动台，就可以看到这个Ollama的图标。
大模型下载和运行 接下来就可以从ollama模型库中，选择需要的模型，做实验测试了。
例如：我们选择阿里通义千问qwen2-1.5B参数的模型：qwen2:1.5b-instruct-q5_K_M
qwen2:1.5b-instruct-q5_K_M
命令行直接运行： ollama run qwen2:1.5b-instruct-q5_K_M
如果下载失败，再次尝试即可。
(modelscope) MacBook-Pro ~ % ollama run qwen2:1.5b-instruct-q5_K_M pulling manifest Error: pull model manifest: Get &#34;https://registry.ollama.ai/v2/library/qwen2/manifests/1.5b-instruct-q5_K_M&#34;: dial tcp: lookup registry.ollama.ai: i/o timeout (modelscope) deMacBook-Pro ~ % ollama run qwen2:1.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-04T13:45:05+08:00">
    <meta property="article:modified_time" content="2024-07-04T13:45:05+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Mac本地部署大模型-单机运行</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="udc56f0fb">前些天在一台linux服务器（8核，32G内存，无显卡）使用ollama运行阿里通义千问Qwen1.5和Qwen2.0低参数版本大模型，Qwen2-1.5B可以运行，但是推理速度有些慢。</p> 
<p id="u690fad5c">一直还没有尝试在macbook上运行测试大模型，不知道单机部署是否会有压力？</p> 
<p id="u9a26c61a">恰好家里有一台平时用的较少的macbook，因此，简单做了一些测试，分享给大家，供参考和讨论。</p> 
<h3 id="224e2ccd">电脑配置</h3> 
<p id="ua47de397">所使用的Macbook Pro稍微有些老，配置如下：</p> 
<ul><li id="u75d6b702">机型，<strong>MacBook Pro（13 英寸，2019 年）</strong>；</li><li id="u3785e4ee">芯片，四核Intel Core i5 2.4GHz；</li><li id="uafeeee8b">图形卡：Intel Iris Plus Graphics 655 1536 MB =》1.5G显存，是否能有用？</li><li id="u2860c674">内存，16G；</li><li id="u789792e7">系统，macOS Sonoma 14.5</li></ul> 
<p id="uf1dcb180">（原来系统Catalina 10.15，发现没有brew命令，安装提示系统版本太低，apple已不再支持；同时系统提示，可以升级到最新mac OS，于是安装升级到最新的系统Sonoma，点击<a href="https://support.apple.com/zh-cn/108052" rel="nofollow" title="这里">这里</a>查询<a href="https://support.apple.com/zh-cn/108052" rel="nofollow" title="macOS Sonoma 与哪些电脑兼容">macOS Sonoma 与哪些电脑兼容</a>）</p> 
<p id="udf4bd1a6"></p> 
<h3 id="t4zXd">Ollama下载安装</h3> 
<p id="u1b5ca5a8">我们简单实用Ollama方式运行大模型。</p> 
<p id="u8944382d">首先从Ollama官网下载安装包。</p> 
<p id="u84997dd8"><a href="https://ollama.com/download/mac" rel="nofollow" title="Download Ollama on macOS">Download Ollama on macOS</a></p> 
<p id="ud5b58a20">直接点击maxOS下载即可，下载的文件是Ollama.zip压缩文件，解压后是应用程序（ollama.apk），直接双击运行安装就可以。</p> 
<p id="u5c3c2f12"></p> 
<p class="img-center"><img alt="" height="1048" id="u9da74971" src="https://images2.imgbox.com/6e/50/WcDsr3UK_o.png" width="1200"></p> 
<p id="u1f11df8a">安装完后，在启动台，就可以看到这个Ollama的图标。</p> 
<p id="u752da112"></p> 
<p class="img-center"><img alt="" height="294" id="uefbc1564" src="https://images2.imgbox.com/45/b7/WqiQxHHs_o.png" width="376"></p> 
<p id="uf5e72fdf"></p> 
<h3 id="fAN17">大模型下载和运行</h3> 
<p id="u428a6f05">接下来就可以从<a href="https://ollama.com/library" rel="nofollow" title="ollama模型库">ollama模型库</a>中，选择需要的模型，做实验测试了。</p> 
<p id="u260a9445">例如：我们选择阿里通义千问qwen2-1.5B参数的模型：qwen2:1.5b-instruct-q5_K_M</p> 
<p id="u7a8474f8"><a href="https://ollama.com/library/qwen2:1.5b-instruct-q5_K_M" rel="nofollow" title="qwen2:1.5b-instruct-q5_K_M">qwen2:1.5b-instruct-q5_K_M</a></p> 
<p id="ue834edc7">命令行直接运行： ollama run qwen2:1.5b-instruct-q5_K_M</p> 
<p id="u495b9662">如果下载失败，再次尝试即可。</p> 
<pre id="ZpgjO"><code class="language-bash">(modelscope) MacBook-Pro ~ % ollama run qwen2:1.5b-instruct-q5_K_M
pulling manifest 
Error: pull model manifest: Get "https://registry.ollama.ai/v2/library/qwen2/manifests/1.5b-instruct-q5_K_M": dial tcp: lookup registry.ollama.ai: i/o timeout

(modelscope) deMacBook-Pro ~ % ollama run qwen2:1.5b-instruct-q5_K_M
pulling manifest 
pulling 9d9344b43f5a... 100% ▕████████████████████████████████████████████████████████▏ 1.1 GB                         
pulling 62fbfd9ed093... 100% ▕████████████████████████████████████████████████████████▏  182 B                         
pulling c156170b718e... 100% ▕████████████████████████████████████████████████████████▏  11 KB                         
pulling f02dd72bb242... 100% ▕████████████████████████████████████████████████████████▏   59 B                         
pulling 624b547e1c39... 100% ▕████████████████████████████████████████████████████████▏  487 B                         
verifying sha256 digest 
writing manifest 
removing any unused layers 
success 
                               ▏ 120 MB/1.1 GB  930 KB/s  </code></pre> 
<p id="u4d58fa1b">简单测试运行，<strong>发现执行速度比预期快（几秒内出结果）</strong>，实验情况如下：</p> 
<pre id="G1tKt"><code class="language-bash">&gt;&gt;&gt; 今天是周三，7月4号。7月11号是周几？
2023年7月11日是星期三。

&gt;&gt;&gt; 从下面相应的文本中提取关键词。
... ###
... 文本1：{PicTech提供图片翻译API，开发者可以集成图片翻译到他们的跨境电商系统中。}
... 关键词1：PicTech, 图片翻译API, 开发者, 跨境电商系统
... ##
... 文本2：{Meta公司训练了非常擅长理解和生成文本的前沿语言模型，并且开源给全世界使用，这种开源精神加速了人类科技的发展，
... 非常值得尊敬。}
... 关键词2：Meta公司, 前沿语言模型, 开源, 人类科技的发展, 尊敬
... ##
... 文本3：{Neil长期从事互联网行业，对人工智能也非常感兴趣，致力于科技改变生活。}
... 关键词3
... ###


此题答案为：

1、PicTech, 图片翻译API, 开发者, 跨境电商系统

2、Meta公司, 前沿语言模型, 开源, 人类科技的发展, 尊敬

3、Neil, 互联网行业, 人工智能, 科技改变生活

&gt;&gt;&gt; Send a message (/? for help)
</code></pre> 
<p id="u8236b3e1"><strong>结论：</strong> 从运行速度来看，运行相同模型（Qwen2-1.5B），在这台macbook（4核，16G内存）运行速度，比之前在linux单机（8核，32G内存）运行速度要快了不少！ 虽然linux机器，从cpu和内存上配置高于我这台macbook，或许两个系统在底层加速优化方面，macbook做的较好？！</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4af8f88fdaef9abd15f3e4fb5abcc165/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Mysql慢日志、慢SQL</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ecf5256a441fad3c783547f9bb487a40/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">前端Vue3开发工具对比：VSCode，IntelliJ IDEA，WebStorm</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>