<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>10 分钟，教你如何用 LLama-Factory 训练和微调 LLama3 模型 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/01f2192d0bc124a8dfca5b4b85093df5/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="10 分钟，教你如何用 LLama-Factory 训练和微调 LLama3 模型">
  <meta property="og:description" content="本文将探讨于2024年3月21日发布的LLama-Factory，并学习如何使用 DigitalOcean 旗下的 Paperspace平台对 LLama 3 进行微调。为了完成我们的任务，我们将使用 NVIDIA A4000 GPU，它被认为是功能最强大的单插槽 GPU 之一，能够无缝集成到各种工作站中。
RTX A4000 采用了 NVIDIA Ampere 架构，集成了 48 个第二代 RT Core、192 个第三代 Tensor Core 和 6144 个CUDA 核心，以及带有错误校正码（ECC）的 16GB 图形内存。这些配置确保了它能为创新项目提供精确可靠的计算能力。
一直以来，微调大语言模型一直是一项复杂的任务，主要由深谙机器学习和人工智能的工程师完成。然而，随着人工智能领域的不断发展，这一观念正在迅速改变。类似 LLama-Factory 等新工具的出现，使得微调过程更加便捷和高效。此外，现在还可以使用 DPO、ORPO、PPO 和 SFT 等技术进行微调和模型优化。更进一步说，大家现在可以有效地训练和微调如 LLama、Mistral、Falcon 等模型。
什么是模型的微调？ 微调模型涉及调整预训练模型或基础模型的参数，这些参数可用于特定任务或数据集，可以提高模型的性能和准确性。这个过程涉及向模型提供新数据并修改其权重、偏差和某些参数。通过这种方式，可以让这个新模型可以在新任务或数据集上获得更好的表现，而无需为了新任务或数据从头开始开发一个模型，从而节省时间和资源。
通常，当创建新的大语言模型（LLM）时，它会在大量文本数据上进行训练，这些数据可能包含潜在的有害的数据。在预训练或初始训练阶段之后，该模型会被进行微调，并采取一些安全措施，以确保其避免生成有害或有毒的响应。然而，这种方法仍有改进的空间。不过，微调解决了需要让模型适应特定需求的问题。
为什么要用 LLama-Factory？ LLama Factory，这个工具能够高效且低成本地支持对 100 多个模型进行微调。LLama Factory 简化了模型微调的过程，并且易于访问，使用体验友好。此外，它还提供了由 Hiyouga 提供的 Hugging Face 空间，可用于对模型进行微调。
图：LLama Board(Huggingface 空间)
这个空间还支持 Lora 和 GaLore 配置，以减少 GPU 的使用。用户可以通过简单的滑块轻松更改参数，如 dropout、epochs、批次大小等。同时，也有多个数据集选项可供选择以微调你的模型。正如本文所述，LLama Factory支持许多模型，包括不同版本的 LLama、mistral 和 Falcon。它还支持像 galore、badm 和 Lora 这样的高级算法，提供诸如flash attention、位置编码和缩放等各种功能。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-21T17:02:28+08:00">
    <meta property="article:modified_time" content="2024-05-21T17:02:28+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">10 分钟，教你如何用 LLama-Factory 训练和微调 LLama3 模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>本文将探讨于2024年3月21日发布的LLama-Factory，并学习如何使用 <a href="https://www.aidroplet.cn/product/gpu/" rel="nofollow" title="DigitalOcean 旗下的 Paperspace平台">DigitalOcean 旗下的 Paperspace平台</a>对 LLama 3 进行微调。为了完成我们的任务，我们将使用 NVIDIA A4000 GPU，它被认为是功能最强大的单插槽 GPU 之一，能够无缝集成到各种工作站中。</p> 
<p>RTX A4000 采用了 NVIDIA Ampere 架构，集成了 48 个第二代 RT Core、192 个第三代 Tensor Core 和 6144 个CUDA 核心，以及带有错误校正码（ECC）的 16GB 图形内存。这些配置确保了它能为创新项目提供精确可靠的计算能力。</p> 
<p></p> 
<p>一直以来，微调大语言模型一直是一项复杂的任务，主要由深谙机器学习和人工智能的工程师完成。然而，随着人工智能领域的不断发展，这一观念正在迅速改变。类似 LLama-Factory 等新工具的出现，使得微调过程更加便捷和高效。此外，现在还可以使用 DPO、ORPO、PPO 和 SFT 等技术进行微调和模型优化。更进一步说，大家现在可以有效地训练和微调如 LLama、Mistral、Falcon 等模型。</p> 
<h3>什么是模型的微调？</h3> 
<p></p> 
<p>微调模型涉及调整预训练模型或基础模型的参数，这些参数可用于特定任务或数据集，可以提高模型的性能和准确性。这个过程涉及向模型提供新数据并修改其权重、偏差和某些参数。通过这种方式，可以让这个新模型可以在新任务或数据集上获得更好的表现，而无需为了新任务或数据从头开始开发一个模型，从而节省时间和资源。</p> 
<p>通常，当创建新的大语言模型（LLM）时，它会在大量文本数据上进行训练，这些数据可能包含潜在的有害的数据。在预训练或初始训练阶段之后，该模型会被进行微调，并采取一些安全措施，以确保其避免生成有害或有毒的响应。然而，这种方法仍有改进的空间。不过，微调解决了需要让模型适应特定需求的问题。</p> 
<p></p> 
<h3>为什么要用 LLama-Factory？</h3> 
<p></p> 
<p>LLama Factory，这个工具能够高效且低成本地支持对 100 多个模型进行微调。LLama Factory 简化了模型微调的过程，并且易于访问，使用体验友好。此外，它还提供了由 Hiyouga 提供的 Hugging Face 空间，可用于对模型进行微调。</p> 
<p style="text-align:center;"><br><img alt="" src="https://images2.imgbox.com/5a/a4/FNq4NTTZ_o.png"></p> 
<p>图：LLama Board(Huggingface 空间)</p> 
<p></p> 
<p>这个空间还支持 Lora 和 GaLore 配置，以减少 GPU 的使用。用户可以通过简单的滑块轻松更改参数，如 dropout、epochs、批次大小等。同时，也有多个数据集选项可供选择以微调你的模型。正如本文所述，LLama Factory支持许多模型，包括不同版本的 LLama、mistral 和 Falcon。它还支持像 galore、badm 和 Lora 这样的高级算法，提供诸如flash attention、位置编码和缩放等各种功能。</p> 
<p>此外，你还可以集成像 TensorBoard、VanDB 和 MLflow 这样的监控工具。为了更快地进行推理，你还可以使用Gradio 和 CLI。本质上，LLama Factory 提供了一系列多样化的选项，以增强模型性能并简化微调过程。</p> 
<h3><strong>LLama Board：LLama Factory 的统一</strong><strong>用户界面</strong></h3> 
<p>LLama Board 可以帮助人们调整和改进语言模型（LLM）的性能，而无需了解如何编写代码。它就像一个仪表板，让你可以轻松地自定义语言模型如何学习和处理信息。</p> 
<p>以下是它的一些关键特性：</p> 
<ul><li> <p><strong>易于定制</strong>：你可以通过在网页上调整设置来改变模型的学习方式。默认设置适用于大多数情况。你还可以在开始之前查看模型将如何查看你的数据。</p> </li><li> <p><strong>进度监控</strong>：随着模型的学习，你可以看到更新和图表，显示它的表现如何。这有助于你了解它是否正在改进。</p> </li><li> <p><strong>灵活的测试</strong>：你可以通过将模型与已知答案进行比较，或自己与其交流，来检查模型理解文本的程度。这有助于你观察模型是否在理解语言方面变得越来越好。</p> </li><li> <p><strong>支持多种语言</strong>：LLama Board 支持英语、俄语和中文，对于说不同语言的人来说很有用。它也被设置为在未来添加更多语言。</p> </li></ul> 
<h3>使用 Paperspac 微调 LLama3</h3> 
<p></p> 
<p>我们此前也介绍过<a href="https://www.aidroplet.cn/tutorial/2183/" rel="nofollow" title="如何在 Paperspace 上跑 LLama3 Demo">如何在 Paperspace 上跑 LLama3 Demo</a>。我们这次来对 LLama3 进行微调。如果你还对目前<a href="https://www.aidroplet.cn/product/gpu/" rel="nofollow" title="DigitalOcean 的 Paperspace GPU 云服务">DigitalOcean 的 Paperspace GPU 云服务</a>不熟悉，可以参考<a href="https://www.digitaloceans.cn/tutorial/402/" rel="nofollow" title="Paperspace 的账户设置教程">Paperspace 的账户设置教程</a>，或 <a href="https://docs.digitalocean.com/products/paperspace/accounts-and-teams/manage-accounts/" rel="nofollow" title="DigitalOcean 英文官网的更多文档">DigitalOcean 英文官网的更多文档</a>。另外，DigitalOcean 提供多种 GPU 可供选择，包括 H100、A100 等，按秒计费，能合理控制成本。</p> 
<p>你也可以点击这个链接，<a href="https://console.paperspace.com/github/gradient-ai/LLaMA-Factory?machine=Free-GPU&amp;ref=blog.paperspace.com&amp;_gl=1*m5p0cw*_gcl_au*MTAyMzg1OTY5NC4xNzA4NTA2NjE1" rel="nofollow" title="直接在 Paperspace 上开始运行">直接在 Paperspace 上开始运行</a>这个项目，如下图所示。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/14/23/banIBWdY_o.png"></p> 
<p>让我们登录到 Paperspace 平台，选择你希望使用的 GPU，并开始使用 <a href="https://www.paperspace.com/notebooks" rel="nofollow" title="notebook（Paperspace 上的 IDE）">notebook（Paperspace 上的 IDE）</a>。你还可以点击上面的链接来帮助你启动 notebook。</p> 
<p>1、首先，我们将克隆仓库并安装必要的库</p> 
<pre><code>!git clone https://github.com/hiyouga/LLaMA-Factory.git
%cd LLaMA-Factory
%ls
</code></pre> 
<p>2、接下来，我们将安装 <a href="https://github.com/unslothai/unsloth?ref=blog.paperspace.com" title="unsloth">unsloth</a>，它允许我们有效地对模型进行微调。此外，我们还会安装 xformers 和 bitsandbytes。</p> 
<pre><code># install necessary packages
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps xformers==0.0.25
!pip install .[bitsandbytes]
!pip install 'urllib3&lt;2'</code></pre> 
<p>3、一旦所有东西都安装好了，我们将检查 GPU 规格</p> 
<pre><code>!nvidia-smi</code></pre> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/f8/2b/IOJmIJtr_o.png"></p> 
<p>4、接下来，我们将导入torch并检查我们 的CUDA，因为我们正在使用 GPU，</p> 
<pre><code>import torch
try:
    assert torch.cuda.is_available() is True
except AssertionError:
    print("Your GPU is not setup!")</code></pre> 
<p>5、我们现在将导入我们克隆的 GitHub仓库附带的数据集。我们也可以创建一个自定义数据集并使用它。</p> 
<pre><code>import json

%cd /notebooks/LLaMA-Factory
MODEL_NAME = "Llama-3"


with open("/notebooks/LLaMA-Factory/data/identity.json", "r", encoding="utf-8") as f:
    dataset = json.load(f)

for sample in dataset:
    sample["output"] = sample["output"].replace("MODEL_NAME", MODEL_NAME).replace("AUTHOR", "LLaMA Factory")

with open("/notebooks/LLaMA-Factory/data/identity.json", "w", encoding="utf-8") as f:
    json.dump(dataset, f, indent=2, ensure_ascii=False)</code></pre> 
<p>6、完成上一步后，我们将执行下面的代码，为 Llama Factory 生成 Gradio 网络应用链接。</p> 
<pre><code>#generates the web app link 
%cd /notebooks/LLaMA-Factory
!GRADIO_SHARE=1 llamafactory-cli webui</code></pre> 
<p>你可以单击生成的链接并按照说明进行操作，也可以使用你的方法。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/71/db/Ob6QlP7T_o.png"></p> 
<p>型号选择：</p> 
<ul><li> <p>你可以选择任何模型；在这里，我们选择具有80亿个参数的Llama 3。</p> </li></ul> 
<p>适配器配置：</p> 
<ul><li> <p>你可以选择指定适配器路径。</p> </li><li> <p>可用的适配器包括LoRa、QLoRa、freeze或full。</p> </li><li> <p>如果需要，你可以刷新适配器列表。</p> </li></ul> 
<p>培训选项：</p> 
<ul><li> <p>你可以使用监督微调来训练模型。</p> </li><li> <p>或者，你可以选择DPU（数据处理单元）或PPU（并行处理单元）。</p> </li></ul> 
<p>数据集选择：</p> 
<ul><li> <p>所选数据集用于监督微调（SFT）。</p> </li><li> <p>你也可以选择自己的数据集。</p> </li></ul> 
<p>超参数配置：</p> 
<ul><li> <p>你可以调整超参数，例如迭代次数、最大梯度范数和最大样本大小。</p> </li></ul> 
<p>Laura配置：</p> 
<ul><li> <p>LoRa型号提供详细的配置选项。</p> </li></ul> 
<p>开始培训：</p> 
<ul><li> <p>设置完所有配置后，你可以通过单击“开始”按钮启动训练过程。</p> </li></ul> 
<p></p> 
<p>下面将开始训练。我们还将使用 CLI 命令开始训练和微调。你可以使用以下代码指定参数。</p> 
<pre><code>args = dict(
  stage="sft",                        # Specifies the stage of training. Here, it's set to "sft" for supervised fine-tuning
  do_train=True,
  model_name_or_path="unsloth/llama-3-8b-Instruct-bnb-4bit", # use bnb-4bit-quantized Llama-3-8B-Instruct model
  dataset="identity,alpaca_gpt4_en",             # use the alpaca and identity datasets
  template="llama3",                     # use llama3 for prompt template
  finetuning_type="lora",                   # use the LoRA adapters which saves up memory
  lora_target="all",                     # attach LoRA adapters to all linear layers
  output_dir="llama3_lora",                  # path to save LoRA adapters
  per_device_train_batch_size=2,               # specify the batch size
  gradient_accumulation_steps=4,               # the gradient accumulation steps
  lr_scheduler_type="cosine",                 # use the learning rate as cosine learning rate scheduler
  logging_steps=10,                      # log every 10 steps
  warmup_ratio=0.1,                      # use warmup scheduler
  save_steps=1000,                      # save checkpoint every 1000 steps
  learning_rate=5e-5,                     # the learning rate
  num_train_epochs=3.0,                    # the epochs of training
  max_samples=500,                      # use 500 examples in each dataset
  max_grad_norm=1.0,                     # clip gradient norm to 1.0
  quantization_bit=4,                     # use 4-bit QLoRA
  loraplus_lr_ratio=16.0,                   # use LoRA+ with lambda=16.0
  use_unsloth=True,                      # use UnslothAI's LoRA optimization for 2x faster training
  fp16=True,                         # use float16 mixed precision training
)

json.dump(args, open("train_llama3.json", "w", encoding="utf-8"), indent=2)
</code></pre> 
<p>接下来，打开一个终端并运行以下命令</p> 
<pre><code>!llamafactory-cli train train_llama3.json</code></pre> 
<p>下面的代码会开启训练</p> 
<p>7、一旦模型训练完成，我们就可以使用该模型进行推断。让我们试着这样做并检查模型是如何工作的。</p> 
<pre><code>args = dict(
  model_name_or_path="unsloth/llama-3-8b-Instruct-bnb-4bit", # Specifies the name or path of the pre-trained model to be used for inference. In this case, it's set to "unsloth/llama-3-8b-Instruct-bnb-4bit".
  #adapter_name_or_path="llama3_lora",            # load the saved LoRA adapters
  finetuning_type="lora",                  # Specifies the type of fine-tuning. Here, it's set to "lora" for LoRA adapters.
  template="llama3",                     # Specifies the prompt template to be used for inference. Here, it's set to "llama3"
  quantization_bit=4,                    # Specifies the number of bits for quantization. In this case, it's set to 4
  use_unsloth=True,                     # use UnslothAI's LoRA optimization for 2x faster generation
)

json.dump(args, open("infer_llama3.json", "w", encoding="utf-8"), indent=2)
</code></pre> 
<p>在这里，我们使用保存的适配器定义我们的模型，选择聊天模板，并指定用户与助手的交互。</p> 
<p>接下来，使用你的终端运行以下代码，</p> 
<pre><code>!llamafactory-cli chat infer_llama3.json</code></pre> 
<p></p> 
<p>建议你可以使用 Llama-Factory 与其它模型做做尝试，调整参数。</p> 
<h3>小结</h3> 
<p>有效的微调已成为大型语言模型（LLMs）适应特定任务的必要条件之一。然而，这需要一定的努力，有时也相当具有挑战性。随着 Llama-Factory 的引入，这一全面的框架让训练更加高效，用户无需编写代码即可轻松为超过 100 个 LLMs 定制微调。</p> 
<p>现在，很多人对大型语言模型（LLMs）更加好奇，有这样想法的开发者可以试试 Llama-Factory 是否可以调整自己的模型。这有助于开源社区的成长和活跃。Llama-Factory 正变得广为人知，甚至已被列入 Awesome Transformers3 中，作为高效微调 LLMs 的工具。</p> 
<p>我们希望本文能鼓励更多开发者使用这一框架来创建有价值的 LLMs。不过请记得，在使用Llama-Factory微调LLMs时，遵守模型的许可规则很重要。</p> 
<p>至此，本文结束。我们看到了如今在几分钟内微调任何模型是多么容易。我们还可以使用 Hugging Face CLI 将这个模型推送到 Hugging Face Hub 上。</p> 
<p>最后，如果你想在 H100 或其他型号的 GPU 上跑自己的大语言模型，可以<a href="https://www.aidroplet.cn/product/gpu/" rel="nofollow" title="在 DigitalOcean 的 GPU 云服务">在 DigitalOcean 的 GPU 云服务</a>上直接开通使用。你也可以<a href="https://www.aidroplet.cn/" rel="nofollow" title="访问 DigitalOcean 中国区独家战略合作伙伴卓普云科技咨询更多云服务相关方案">访问 DigitalOcean 中国区独家战略合作伙伴卓普云科技咨询更多云服务相关方案</a>。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a9ae3eb316d81e3d1540882e35078937/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python 可视化 web 神器：streamlit、Gradio、dash、nicegui；低代码 Python Web 框架：PyWebIO</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/29afee1745cd46dcf5438971c7ece567/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">[Collection与数据结构] Map与Set(一):二叉搜索树与Map,Set的使用</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>