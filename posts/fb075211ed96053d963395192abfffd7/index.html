<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【03】LLaMA-Factory微调大模型——多模型部署 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/fb075211ed96053d963395192abfffd7/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【03】LLaMA-Factory微调大模型——多模型部署">
  <meta property="og:description" content="为了构建法律领域的垂直应用大模型，记录使用LLaMA-Factory微调大模型的过程，以期UU们可以复刻与应用。上文【02】LLaMA-Factory微调大模型——LLaMA-Factory搭建已在本机成功部署模型微调框架，本文则在本机部署多种基础模型，为后续微调提供准备。
一、基础模型选择 LLaMA-Factory框架所支持的模型如下表所示，覆盖主流的开源大模型，可按照自身需求进行应用。本文主要部署Qwen2、GLM-4、LLaMA3-8B-Chinese-Chat三个模型进行实验。
模型名模型大小TemplateBaichuan 27B/13Bbaichuan2BLOOM/BLOOMZ560M/1.1B/1.7B/3B/7.1B/176B-ChatGLM36Bchatglm3Command R35B/104BcohereDeepSeek (Code/MoE)7B/16B/67B/236BdeepseekFalcon7B/11B/40B/180BfalconGemma/Gemma 2/CodeGemma2B/7B/9B/27BgemmaGLM-49Bglm4InternLM27B/20Bintern2Llama7B/13B/33B/65B-Llama 27B/13B/70Bllama2Llama 38B/70Bllama3LLaVA-1.57B/13BvicunaMistral/Mixtral7B/8x7B/8x22BmistralOLMo1B/7B-PaliGemma3BgemmaPhi-1.5/Phi-21.3B/2.7B-Phi-34B/7B/14BphiQwen/Qwen1.5/Qwen2 (Code/MoE)0.5B/1.5B/4B/7B/14B/32B/72B/110BqwenStarCoder 23B/7B/15B-XVERSE7B/13B/65BxverseYi/Yi-1.56B/9B/34ByiYi-VL6B/34Byi_vlYuan 22B/51B/102Byuan 二、准备阶段 在LLaMA-Factory文件下新建文件夹存放模型文件
更换pypi源为清华镜像进行加速
# 更换 pypi 源加速库的安装【建议】 pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple 换源成功如下图所示 【提示】除了清华源以外还可以选择其他多个国内镜像
阿里云： http://mirrors.aliyun.com/pypi/simple/ 中国科技大学： https://pypi.mirrors.ustc.edu.cn/simple/ 豆瓣： http://pypi.douban.com/simple/ 清华大学： https://pypi.tuna.tsinghua.edu.cn/simple/ 中国科学技术大学： http://pypi.mirrors.ustc.edu.cn/simple/ 安装下列代码库（可选）
pip install deepspeed pip install flash-attn --no-build-isolation 安装flash-attention时会出现报错或者超时情况，如下图所示 该环节出现问题可参考以下这篇博文进行解决
关于flash-attention安装踩过的坑_flash-attn踩坑-CSDN博客https://blog.csdn.net/weixin_44044132/article/details/137055014?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Ctr-1-137055014-blog-136054392.235%5Ev43%5Epc_blog_bottom_relevance_base5&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Ctr-1-137055014-blog-136054392.235%5Ev43%5Epc_blog_bottom_relevance_base5安装git-lfs，Git Large File Storage（简称Git LFS）是一个用于存储大文件的Git扩展。在传统的Git中，大文件的版本控制会导致存储库变得庞大且访问速度变慢。Git LFS通过将大文件存储在Git存储库之外的特殊服务器上，并在Git存储库中只保留指向这些大文件的指针来解决这个问题。
# 先安装git（如已安装可忽略） sudo apt-get install git #安装curl # 安装apt-get源 curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash # 安装git-lfs sudo apt-get install git-lfs # 初始化git-lfs git lfs install 安装过程如下图所示 安装完成后，对其初始化">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-17T11:09:10+08:00">
    <meta property="article:modified_time" content="2024-07-17T11:09:10+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【03】LLaMA-Factory微调大模型——多模型部署</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>为了构建法律领域的垂直应用大模型，记录使用LLaMA-Factory微调大模型的过程，以期UU们可以复刻与应用。上文【02】LLaMA-Factory微调大模型——LLaMA-Factory搭建已在本机成功部署模型微调框架，本文则在本机部署多种基础模型，为后续微调提供准备。</p> 
<h2>一、基础模型选择</h2> 
<p> LLaMA-Factory框架所支持的模型如下表所示，覆盖主流的开源大模型，可按照自身需求进行应用。本文主要部署Qwen2、GLM-4、LLaMA3-8B-Chinese-Chat三个模型进行实验。</p> 
<table border="1" cellpadding="1" cellspacing="1" style="width:500px;"><tbody><tr><th>模型名</th><th>模型大小</th><th>Template</th></tr><tr><td><a href="https://huggingface.co/baichuan-inc" rel="nofollow" title="Baichuan 2">Baichuan 2</a></td><td>7B/13B</td><td>baichuan2</td></tr><tr><td><a href="https://huggingface.co/bigscience" rel="nofollow" title="BLOOM/BLOOMZ">BLOOM/BLOOMZ</a></td><td>560M/1.1B/1.7B/3B/7.1B/176B</td><td>-</td></tr><tr><td><a href="https://huggingface.co/THUDM" rel="nofollow" title="ChatGLM3">ChatGLM3</a></td><td>6B</td><td>chatglm3</td></tr><tr><td><a href="https://huggingface.co/CohereForAI" rel="nofollow" title="Command R">Command R</a></td><td>35B/104B</td><td>cohere</td></tr><tr><td><a href="https://huggingface.co/deepseek-ai" rel="nofollow" title="DeepSeek (Code/MoE)">DeepSeek (Code/MoE)</a></td><td>7B/16B/67B/236B</td><td>deepseek</td></tr><tr><td><a href="https://huggingface.co/tiiuae" rel="nofollow" title="Falcon">Falcon</a></td><td>7B/11B/40B/180B</td><td>falcon</td></tr><tr><td><a href="https://huggingface.co/google" rel="nofollow" title="Gemma/Gemma 2/CodeGemma">Gemma/Gemma 2/CodeGemma</a></td><td>2B/7B/9B/27B</td><td>gemma</td></tr><tr><td><a href="https://huggingface.co/THUDM" rel="nofollow" title="GLM-4">GLM-4</a></td><td>9B</td><td>glm4</td></tr><tr><td><a href="https://huggingface.co/internlm" rel="nofollow" title="InternLM2">InternLM2</a></td><td>7B/20B</td><td>intern2</td></tr><tr><td><a href="https://github.com/facebookresearch/llama" title="Llama">Llama</a></td><td>7B/13B/33B/65B</td><td>-</td></tr><tr><td><a href="https://huggingface.co/meta-llama" rel="nofollow" title="Llama 2">Llama 2</a></td><td>7B/13B/70B</td><td>llama2</td></tr><tr><td><a href="https://huggingface.co/meta-llama" rel="nofollow" title="Llama 3">Llama 3</a></td><td>8B/70B</td><td>llama3</td></tr><tr><td><a href="https://huggingface.co/llava-hf" rel="nofollow" title="LLaVA-1.5">LLaVA-1.5</a></td><td>7B/13B</td><td>vicuna</td></tr><tr><td><a href="https://huggingface.co/mistralai" rel="nofollow" title="Mistral/Mixtral">Mistral/Mixtral</a></td><td>7B/8x7B/8x22B</td><td>mistral</td></tr><tr><td><a href="https://huggingface.co/allenai" rel="nofollow" title="OLMo">OLMo</a></td><td>1B/7B</td><td>-</td></tr><tr><td><a href="https://huggingface.co/google" rel="nofollow" title="PaliGemma">PaliGemma</a></td><td>3B</td><td>gemma</td></tr><tr><td><a href="https://huggingface.co/microsoft" rel="nofollow" title="Phi-1.5/Phi-2">Phi-1.5/Phi-2</a></td><td>1.3B/2.7B</td><td>-</td></tr><tr><td><a href="https://huggingface.co/microsoft" rel="nofollow" title="Phi-3">Phi-3</a></td><td>4B/7B/14B</td><td>phi</td></tr><tr><td><a href="https://huggingface.co/Qwen" rel="nofollow" title="Qwen/Qwen1.5/Qwen2 (Code/MoE)">Qwen/Qwen1.5/Qwen2 (Code/MoE)</a></td><td>0.5B/1.5B/4B/7B/14B/32B/72B/110B</td><td>qwen</td></tr><tr><td><a href="https://huggingface.co/bigcode" rel="nofollow" title="StarCoder 2">StarCoder 2</a></td><td>3B/7B/15B</td><td>-</td></tr><tr><td><a href="https://huggingface.co/xverse" rel="nofollow" title="XVERSE">XVERSE</a></td><td>7B/13B/65B</td><td>xverse</td></tr><tr><td><a href="https://huggingface.co/01-ai" rel="nofollow" title="Yi/Yi-1.5">Yi/Yi-1.5</a></td><td>6B/9B/34B</td><td>yi</td></tr><tr><td><a href="https://huggingface.co/01-ai" rel="nofollow" title="Yi-VL">Yi-VL</a></td><td>6B/34B</td><td>yi_vl</td></tr><tr><td><a href="https://huggingface.co/IEITYuan" rel="nofollow" title="Yuan 2">Yuan 2</a></td><td>2B/51B/102B</td><td>yuan</td></tr></tbody></table> 
<p></p> 
<h2>二、准备阶段</h2> 
<p>在LLaMA-Factory文件下新建文件夹存放模型文件</p> 
<p><img alt="" height="262" src="https://images2.imgbox.com/fa/d6/6Q1vuUx1_o.png" width="1168"></p> 
<p>更换pypi源为清华镜像进行加速</p> 
<pre><code class="language-bash"># 更换 pypi 源加速库的安装【建议】
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</code></pre> 
<p>换源成功如下图所示 </p> 
<p><img alt="" height="86" src="https://images2.imgbox.com/b3/ef/FFZqLTjU_o.png" width="1200"></p> 
<p>【提示】除了清华源以外还可以选择其他多个国内镜像</p> 
<pre><code class="hljs">阿里云： http://mirrors.aliyun.com/pypi/simple/
中国科技大学： https://pypi.mirrors.ustc.edu.cn/simple/
豆瓣： http://pypi.douban.com/simple/
清华大学： https://pypi.tuna.tsinghua.edu.cn/simple/
中国科学技术大学： http://pypi.mirrors.ustc.edu.cn/simple/</code></pre> 
<p>安装下列代码库（可选）</p> 
<pre><code class="language-bash">pip install deepspeed
pip install flash-attn --no-build-isolation</code></pre> 
<p>安装flash-attention时会出现报错或者超时情况，如下图所示 </p> 
<p><img alt="" height="424" src="https://images2.imgbox.com/4b/8a/OPivdXw3_o.png" width="1200"></p> 
<p>该环节出现问题可参考以下这篇博文进行解决</p> 
<p><a class="has-card" href="https://blog.csdn.net/weixin_44044132/article/details/137055014?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Ctr-1-137055014-blog-136054392.235%5Ev43%5Epc_blog_bottom_relevance_base5&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Ctr-1-137055014-blog-136054392.235%5Ev43%5Epc_blog_bottom_relevance_base5" title="关于flash-attention安装踩过的坑_flash-attn踩坑-CSDN博客"><span class="link-card-box"><span class="link-title">关于flash-attention安装踩过的坑_flash-attn踩坑-CSDN博客</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/f2/c7/grPnBjXK_o.png" alt="icon-default.png?t=N7T8">https://blog.csdn.net/weixin_44044132/article/details/137055014?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Ctr-1-137055014-blog-136054392.235%5Ev43%5Epc_blog_bottom_relevance_base5&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Ctr-1-137055014-blog-136054392.235%5Ev43%5Epc_blog_bottom_relevance_base5</span></span></a>安装git-lfs，Git Large File Storage（简称Git LFS）是一个用于存储大文件的Git扩展。在传统的Git中，大文件的版本控制会导致存储库变得庞大且访问速度变慢。Git LFS通过将大文件存储在Git存储库之外的特殊服务器上，并在Git存储库中只保留指向这些大文件的指针来解决这个问题。</p> 
<pre><code class="language-bash"># 先安装git（如已安装可忽略）
sudo apt-get install git
#安装curl

# 安装apt-get源
curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash
# 安装git-lfs
sudo apt-get install git-lfs
# 初始化git-lfs
git lfs install
</code></pre> 
<p>安装过程如下图所示 </p> 
<p><img alt="" height="482" src="https://images2.imgbox.com/6f/f2/GdRnA7Yp_o.png" width="1200"></p> 
<p>安装完成后，对其初始化</p> 
<p><img alt="" height="98" src="https://images2.imgbox.com/0b/e0/P5x8jnK4_o.png" width="1182"></p> 
<h2>三、Qwen-2模型</h2> 
<p> Qwen是阿里巴巴集团Qwen团队研发的大语言模型和大型多模态模型系列。目前，大语言模型已升级至Qwen2版本。无论是语言模型还是多模态模型，均在大规模多语言和多模态数据上进行预训练，并通过高质量数据进行后期微调以贴近人类偏好。Qwen具备自然语言理解、文本生成、视觉理解、音频理解、工具使用、角色扮演、作为AI Agent进行互动等多种能力。详情可参考该链接。</p> 
<p><a class="has-card" href="https://www.modelscope.cn/models/qwen/Qwen2-7B" rel="nofollow" title="Qwen2-7B · 模型库 (modelscope.cn)"><span class="link-card-box"><span class="link-title">Qwen2-7B · 模型库 (modelscope.cn)</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/c2/34/saket1Up_o.png" alt="icon-default.png?t=N7T8">https://www.modelscope.cn/models/qwen/Qwen2-7B</span></span></a>从魔塔社区中克隆模型代码</p> 
<pre><code class="language-bash">git clone https://www.modelscope.cn/qwen/Qwen2-7B.git</code></pre> 
<p><img alt="" height="262" src="https://images2.imgbox.com/71/77/dj01RdcF_o.png" width="1200"></p> 
<p>克隆完成后如下图所示</p> 
<p><img alt="" height="292" src="https://images2.imgbox.com/b8/ce/VVgIvyej_o.png" width="1200"></p> 
<p>使用LLaMA-Factory框架微调Qwen可参考以下官方教程</p> 
<p><a class="has-card" href="https://qwen.readthedocs.io/zh-cn/latest/training/SFT/llama_factory.html" rel="nofollow" title="LLaMA-Factory - Qwen"><span class="link-card-box"><span class="link-title">LLaMA-Factory - Qwen</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/77/56/HW5nU5W9_o.png" alt="icon-default.png?t=N7T8">https://qwen.readthedocs.io/zh-cn/latest/training/SFT/llama_factory.html</span></span></a></p> 
<p>查看Qwen2模型本地的绝对路径</p> 
<pre><code class="language-bash">pwd</code></pre> 
<p><img alt="" height="316" src="https://images2.imgbox.com/cb/63/fRogSJ7j_o.png" width="1064"></p> 
<p>此后，启动LLaMA-Factory的web端，在浏览器中进行访问</p> 
<pre><code class="language-bash">llamafactory-cli webui</code></pre> 
<p>模型选择Qwen2-7B，路径替换为本地的绝对路径，后在Chat标签下加载模型 </p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/7c/fe/5Z9Ww6oT_o.png" width="1200"></p> 
<p>模型加载成功后的web页面及命令行如下图所示</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/d8/8a/glmrwk1u_o.png" width="1200"></p> 
<p>此后便可在页面中进行问答，模型成功加载，可以开始进行聊天</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/0f/52/UmCslYVr_o.png" width="1200"></p> 
<h2> 四、GLM-4模型</h2> 
<p>GLM-4-9B 是智谱 AI 推出的最新一代预训练模型 GLM-4 系列中的开源版本。 在语义、数学、推理、代码和知识等多方面的数据集测评中，GLM-4-9B 及其人类偏好对齐的版本 GLM-4-9B-Chat 均表现出较高的性能。 除了能进行多轮对话，GLM-4-9B-Chat 还具备网页浏览、代码执行、自定义工具调用（Function Call）和长文本推理（支持最大 128K 上下文）等高级功能。 本代模型增加了多语言支持，支持包括日语，韩语，德语在内的 26 种语言。我们还推出了支持 1M 上下文长度（约 200 万中文字符）的模型。详情可访问模型库。</p> 
<p><a class="has-card" href="https://www.modelscope.cn/models/ZhipuAI/glm-4-9b" rel="nofollow" title="glm-4-9b · 模型库 (modelscope.cn)"><span class="link-card-box"><span class="link-title">glm-4-9b · 模型库 (modelscope.cn)</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/b8/c9/cNGvPNUL_o.png" alt="icon-default.png?t=N7T8">https://www.modelscope.cn/models/ZhipuAI/glm-4-9b</span></span></a>从魔塔社区中克隆模型代码</p> 
<p><img alt="" height="450" src="https://images2.imgbox.com/e1/3e/mHMOvnb3_o.png" width="1200"></p> 
<p>完成后如下图所示</p> 
<p><img alt="" height="356" src="https://images2.imgbox.com/09/91/Cbtj8uAf_o.png" width="1200"></p> 
<p>在原LLM虚拟环境中直接启动会依赖冲突</p> 
<p><img alt="" height="476" src="https://images2.imgbox.com/4a/54/lQ7oU9sW_o.png" width="1200"></p> 
<p>因此，创建新的虚拟环境GLM</p> 
<pre><code class="language-bash">conda create -n GLM python=3.11</code></pre> 
<p><img alt="" height="468" src="https://images2.imgbox.com/b7/9c/q3zJEo2Q_o.png" width="1200"></p> 
<p>创建好之后，克隆GLM-4的源代码，获取requirements.txt文件</p> 
<pre><code class="language-bash">git clone https://github.com/THUDM/GLM-4.git</code></pre> 
<p><img alt="" height="258" src="https://images2.imgbox.com/e3/fa/fgUCQGEm_o.png" width="1200"></p> 
<p>之后，通过requirements.txt文件来更新依赖</p> 
<pre><code class="language-bash">pip install -r requirements.txt</code></pre> 
<p><img alt="" height="906" src="https://images2.imgbox.com/de/83/S556HK2K_o.png" width="1200"></p> 
<p>将tramsformers的版本更新至4.41.2</p> 
<pre><code class="language-bash">pip install --upgrade transformers=4.41.2</code></pre> 
<p><img alt="" height="762" src="https://images2.imgbox.com/da/78/1wfq8YBU_o.png" width="1200"></p> 
<p>此后，在GLM虚拟环境中启动LLaMA-Factory的web端，在浏览器中进行访问</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/ce/2b/NgrHitvV_o.png" width="1200"></p> 
<p>模型加载成功如下图所示 </p> 
<p><img alt="" height="796" src="https://images2.imgbox.com/6b/ea/OdRVzKz1_o.png" width="1200"></p> 
<h2>五、Llama-3模型</h2> 
<p> Llama3-8B-Chinese-Chat是一个针对中文和英文用户的指令调整语言模型，具有各种能力，如角色扮演和工具使用，建立在Meta-Llama-3-8B-Instruct模型之上。详情可见该链接。</p> 
<p><a class="has-card" href="https://github.com/Shenzhi-Wang/Llama3-Chinese-Chat" title="GitHub - Shenzhi-Wang/Llama3-Chinese-Chat：这是第一个基于 Meta-Llama-3-8B-Instruct 模型，通过 ORPO 专门针对中文进行微调的中文聊天模型。"><span class="link-card-box"><span class="link-title">GitHub - Shenzhi-Wang/Llama3-Chinese-Chat：这是第一个基于 Meta-Llama-3-8B-Instruct 模型，通过 ORPO 专门针对中文进行微调的中文聊天模型。</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/06/61/Xac1NJz3_o.png" alt="icon-default.png?t=N7T8">https://github.com/Shenzhi-Wang/Llama3-Chinese-Chat</span></span></a>从魔塔社区中克隆模型代码</p> 
<pre><code class="hljs">git clone https://www.modelscope.cn/LLM-Research/Llama3-8B-Chinese-Chat.git</code></pre> 
<p><img alt="" height="340" src="https://images2.imgbox.com/7e/55/aR4IPJ9L_o.png" width="1200"></p> 
<p>获取模型的绝对路径 </p> 
<p><img alt="" height="64" src="https://images2.imgbox.com/a0/13/MWMczhNl_o.png" width="1200"></p> 
<p> 启动LLaMA-Factory的web端，在浏览器中进行访问</p> 
<p><img alt="" height="178" src="https://images2.imgbox.com/e8/7e/H8WKbI2J_o.png" width="1172"> 路径替换为本地的绝对路径，后在Chat标签下加载模型 </p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/d1/ac/GtZTvBYF_o.png" width="1200"></p> 
<p> 加载成功如下图所示</p> 
<p><img alt="" height="622" src="https://images2.imgbox.com/0c/84/OW0AvGuo_o.png" width="1200"></p> 
<p> 可通过网页端与模型进行简单交互，测试其部署效果</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/7c/4b/kUblAVdz_o.png" width="1200"></p> 
<p>同时，可在后台实时查看显卡开销情况</p> 
<pre><code class="language-bash">nvidia-smi</code></pre> 
<p><img alt="" height="652" src="https://images2.imgbox.com/dd/27/rwk6W0gz_o.png" width="1200"></p> 
<h2>小结</h2> 
<p>至此Qwen2、GLM-4、LLaMA3-8B-Chinese-Chat三个模型成功在本机完成了搭建，下文【04】LLaMA-Factory微调大模型——数据准备，将分析整理微调所用的数据，为后续的微调模型提供高质量、格式规范的数据支撑。欢迎您持续关注，如果本文对您有所帮助，感谢您一键三连，多多支持！</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/477b9513833e1c5266cf849a52d9eda5/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">精通JVM监控与调优：工具使用与命令指南</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/13ab26fa31a292076c314dc8e6ca3da9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">前端GIS开发详细指南</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>