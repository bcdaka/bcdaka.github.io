<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AIGC专栏2——Stable Diffusion结构解析-以文本生成图像（文生图，txt2img）为例 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/23c84878af9247ef3bdf02c3c8dd6280/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="AIGC专栏2——Stable Diffusion结构解析-以文本生成图像（文生图，txt2img）为例">
  <meta property="og:description" content="AIGC专栏2——Stable Diffusion结构解析-以文本生成图像（文生图，txt2img）为例 学习前言源码下载地址网络构建一、什么是Stable Diffusion（SD）二、Stable Diffusion的组成三、生成流程1、文本编码2、采样流程a、生成初始噪声b、对噪声进行N次采样c、单次采样解析I、预测噪声II、施加噪声 d、预测噪声过程中的网络结构解析I、apply_model方法解析II、UNetModel模型解析 3、隐空间解码生成图片 文本到图像预测过程代码 学习前言 用了很久的Stable Diffusion，但从来没有好好解析过它内部的结构，写个博客记录一下，嘿嘿。
源码下载地址 https://github.com/bubbliiiing/stable-diffusion
喜欢的可以点个star噢。
网络构建 一、什么是Stable Diffusion（SD） Stable Diffusion是比较新的一个扩散模型，翻译过来是稳定扩散，虽然名字叫稳定扩散，但实际上换个seed生成的结果就完全不一样，非常不稳定哈。
Stable Diffusion最开始的应用应该是文本生成图像，即文生图，随着技术的发展Stable Diffusion不仅支持image2image图生图的生成，还支持ControlNet等各种控制方法来定制生成的图像。
Stable Diffusion基于扩散模型，所以不免包含不断去噪的过程，如果是图生图的话，还有不断加噪的过程，此时离不开DDPM那张老图，如下：
Stable Diffusion相比于DDPM，使用了DDIM采样器，使用了隐空间的扩散，另外使用了非常大的LAION-5B数据集进行预训练。
直接Finetune Stable Diffusion大多数同学应该是无法cover住成本的，不过Stable Diffusion有很多轻量Finetune的方案，比如Lora、Textual Inversion等，但这是后话。
本文主要是解析一下整个SD模型的结构组成，一次扩散，多次扩散的流程。
大模型、AIGC是当前行业的趋势，不会的话容易被淘汰，hh。
二、Stable Diffusion的组成 Stable Diffusion由四大部分组成。
1、Sampler采样器。
2、Variational Autoencoder (VAE) 变分自编码器。
3、UNet 主网络，噪声预测器。
4、CLIPEmbedder文本编码器。
每一部分都很重要，我们首先以文本生成图像为例进行解析。既然是文本生成图像，那么我们的输入也只剩下文本了，这时候没有输入图片。
三、生成流程 生成流程分为三个部分：
1、prompt文本编码。
2、进行若干次采样。
3、进行解码。
with torch.no_grad(): if seed == -1: seed = random.randint(0, 65535) seed_everything(seed) # ----------------------- # # 获得编码后的prompt # ----------------------- # cond = {&#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-09-03T11:30:45+08:00">
    <meta property="article:modified_time" content="2023-09-03T11:30:45+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AIGC专栏2——Stable Diffusion结构解析-以文本生成图像（文生图，txt2img）为例</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night-eighties">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>AIGC专栏2——Stable Diffusion结构解析-以文本生成图像（文生图，txt2img）为例</h4> 
 <ul><li><a href="#_2" rel="nofollow">学习前言</a></li><li><a href="#_5" rel="nofollow">源码下载地址</a></li><li><a href="#_10" rel="nofollow">网络构建</a></li><li><ul><li><a href="#Stable_DiffusionSD_11" rel="nofollow">一、什么是Stable Diffusion（SD）</a></li><li><a href="#Stable_Diffusion_25" rel="nofollow">二、Stable Diffusion的组成</a></li><li><a href="#_33" rel="nofollow">三、生成流程</a></li><li><ul><li><a href="#1_68" rel="nofollow">1、文本编码</a></li><li><a href="#2_125" rel="nofollow">2、采样流程</a></li><li><ul><li><a href="#a_127" rel="nofollow">a、生成初始噪声</a></li><li><a href="#bN_144" rel="nofollow">b、对噪声进行N次采样</a></li><li><a href="#c_182" rel="nofollow">c、单次采样解析</a></li><li><ul><li><a href="#I_183" rel="nofollow">I、预测噪声</a></li><li><a href="#II_223" rel="nofollow">II、施加噪声</a></li></ul> 
     </li><li><a href="#d_264" rel="nofollow">d、预测噪声过程中的网络结构解析</a></li><li><ul><li><a href="#Iapply_model_265" rel="nofollow">I、apply_model方法解析</a></li><li><a href="#IIUNetModel_321" rel="nofollow">II、UNetModel模型解析</a></li></ul> 
    </li></ul> 
    </li><li><a href="#3_698" rel="nofollow">3、隐空间解码生成图片</a></li></ul> 
  </li></ul> 
  </li><li><a href="#_724" rel="nofollow">文本到图像预测过程代码</a></li></ul> 
</div> 
<p></p> 
<h2><a id="_2"></a>学习前言</h2> 
<p>用了很久的Stable Diffusion，但从来没有好好解析过它内部的结构，写个博客记录一下，嘿嘿。<br> <img src="https://images2.imgbox.com/21/3b/LOXp2eSa_o.jpg" alt="在这里插入图片描述"></p> 
<h2><a id="_5"></a>源码下载地址</h2> 
<p><a href="https://github.com/bubbliiiing/stable-diffusion">https://github.com/bubbliiiing/stable-diffusion</a></p> 
<p>喜欢的可以点个star噢。</p> 
<h2><a id="_10"></a>网络构建</h2> 
<h3><a id="Stable_DiffusionSD_11"></a>一、什么是Stable Diffusion（SD）</h3> 
<p>Stable Diffusion是比较新的一个扩散模型，翻译过来是稳定扩散，虽然名字叫稳定扩散，但实际上换个seed生成的结果就完全不一样，非常不稳定哈。</p> 
<p>Stable Diffusion最开始的应用应该是文本生成图像，即文生图，随着技术的发展Stable Diffusion不仅支持image2image图生图的生成，还支持ControlNet等各种控制方法来定制生成的图像。</p> 
<p>Stable Diffusion基于扩散模型，所以不免包含不断去噪的过程，如果是图生图的话，还有不断加噪的过程，此时离不开DDPM那张老图，如下：<br> <img src="https://images2.imgbox.com/d3/49/NlRljBJp_o.png" alt="在这里插入图片描述"><br> Stable Diffusion相比于DDPM，使用了DDIM采样器，使用了隐空间的扩散，另外使用了非常大的LAION-5B数据集进行预训练。</p> 
<p>直接Finetune Stable Diffusion大多数同学应该是无法cover住成本的，不过Stable Diffusion有很多轻量Finetune的方案，比如Lora、Textual Inversion等，但这是后话。</p> 
<p>本文主要是解析一下整个SD模型的结构组成，一次扩散，多次扩散的流程。</p> 
<p>大模型、AIGC是当前行业的趋势，不会的话容易被淘汰，hh。</p> 
<h3><a id="Stable_Diffusion_25"></a>二、Stable Diffusion的组成</h3> 
<p>Stable Diffusion由四大部分组成。<br> 1、Sampler采样器。<br> 2、Variational Autoencoder (VAE) 变分自编码器。<br> 3、UNet 主网络，噪声预测器。<br> 4、CLIPEmbedder文本编码器。</p> 
<p>每一部分都很重要，我们首先以文本生成图像为例进行解析。既然是文本生成图像，那么我们的输入也只剩下文本了，这时候没有输入图片。</p> 
<h3><a id="_33"></a>三、生成流程</h3> 
<p><img src="https://images2.imgbox.com/c1/6d/wihY7p5F_o.png" alt="在这里插入图片描述" width="500"><br> 生成流程分为三个部分：<br> 1、prompt文本编码。<br> 2、进行若干次采样。<br> 3、进行解码。</p> 
<pre><code class="prism language-python"><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> seed <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>
        seed <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">65535</span><span class="token punctuation">)</span>
    seed_everything<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>

    <span class="token comment"># ----------------------- #</span>
    <span class="token comment">#   获得编码后的prompt</span>
    <span class="token comment"># ----------------------- #</span>
    cond    <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">"c_crossattn"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>model<span class="token punctuation">.</span>get_learned_conditioning<span class="token punctuation">(</span><span class="token punctuation">[</span>prompt <span class="token operator">+</span> <span class="token string">', '</span> <span class="token operator">+</span> a_prompt<span class="token punctuation">]</span> <span class="token operator">*</span> num_samples<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
    un_cond <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">"c_crossattn"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>model<span class="token punctuation">.</span>get_learned_conditioning<span class="token punctuation">(</span><span class="token punctuation">[</span>n_prompt<span class="token punctuation">]</span> <span class="token operator">*</span> num_samples<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
    H<span class="token punctuation">,</span> W    <span class="token operator">=</span> input_shape
    shape   <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> H <span class="token operator">//</span> <span class="token number">8</span><span class="token punctuation">,</span> W <span class="token operator">//</span> <span class="token number">8</span><span class="token punctuation">)</span>

    <span class="token comment"># ----------------------- #</span>
    <span class="token comment">#   进行采样</span>
    <span class="token comment"># ----------------------- #</span>
    samples<span class="token punctuation">,</span> intermediates <span class="token operator">=</span> ddim_sampler<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>ddim_steps<span class="token punctuation">,</span> num_samples<span class="token punctuation">,</span>
                                                    shape<span class="token punctuation">,</span> cond<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> eta<span class="token operator">=</span>eta<span class="token punctuation">,</span>
                                                    unconditional_guidance_scale<span class="token operator">=</span>scale<span class="token punctuation">,</span>
                                                    unconditional_conditioning<span class="token operator">=</span>un_cond<span class="token punctuation">)</span>

    <span class="token comment"># ----------------------- #</span>
    <span class="token comment">#   进行解码</span>
    <span class="token comment"># ----------------------- #</span>
    x_samples <span class="token operator">=</span> model<span class="token punctuation">.</span>decode_first_stage<span class="token punctuation">(</span>samples<span class="token punctuation">)</span>
    x_samples <span class="token operator">=</span> <span class="token punctuation">(</span>einops<span class="token punctuation">.</span>rearrange<span class="token punctuation">(</span>x_samples<span class="token punctuation">,</span> <span class="token string">'b c h w -&gt; b h w c'</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">127.5</span> <span class="token operator">+</span> <span class="token number">127.5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>clip<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="1_68"></a>1、文本编码</h4> 
<p><img src="https://images2.imgbox.com/1c/27/Y2aVJr4q_o.png" alt="在这里插入图片描述" width="500"><br> 文本编码的思路比较简单，直接使用CLIP的文本编码器进行编码就可以了，在代码中定义了一个FrozenCLIPEmbedder类别，使用了transformers库的CLIPTokenizer和CLIPTextModel。</p> 
<p>在前传过程中，我们对输入进来的文本首先利用CLIPTokenizer进行编码，然后使用CLIPTextModel进行特征提取，通过FrozenCLIPEmbedder，我们可以获得一个[batch_size, 77, 768]的特征向量。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">FrozenCLIPEmbedder</span><span class="token punctuation">(</span>AbstractEncoder<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Uses the CLIP transformer encoder for text (from huggingface)"""</span>
    LAYERS <span class="token operator">=</span> <span class="token punctuation">[</span>
        <span class="token string">"last"</span><span class="token punctuation">,</span>
        <span class="token string">"pooled"</span><span class="token punctuation">,</span>
        <span class="token string">"hidden"</span>
    <span class="token punctuation">]</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> version<span class="token operator">=</span><span class="token string">"openai/clip-vit-large-patch14"</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">77</span><span class="token punctuation">,</span>
                 freeze<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> layer<span class="token operator">=</span><span class="token string">"last"</span><span class="token punctuation">,</span> layer_idx<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># clip-vit-base-patch32</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">assert</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>LAYERS
        <span class="token comment"># 定义文本的tokenizer和transformer</span>
        self<span class="token punctuation">.</span>tokenizer      <span class="token operator">=</span> CLIPTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>version<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>transformer    <span class="token operator">=</span> CLIPTextModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>version<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>device         <span class="token operator">=</span> device
        self<span class="token punctuation">.</span>max_length     <span class="token operator">=</span> max_length
        <span class="token comment"># 冻结模型参数</span>
        <span class="token keyword">if</span> freeze<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>freeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer <span class="token operator">=</span> layer
        self<span class="token punctuation">.</span>layer_idx <span class="token operator">=</span> layer_idx
        <span class="token keyword">if</span> layer <span class="token operator">==</span> <span class="token string">"hidden"</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> layer_idx <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
            <span class="token keyword">assert</span> <span class="token number">0</span> <span class="token operator">&lt;=</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>layer_idx<span class="token punctuation">)</span> <span class="token operator">&lt;=</span> <span class="token number">12</span>

    <span class="token keyword">def</span> <span class="token function">freeze</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>transformer <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># self.train = disabled_train</span>
        <span class="token keyword">for</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 对输入的图片进行分词并编码，padding直接padding到77的长度。</span>
        batch_encoding  <span class="token operator">=</span> self<span class="token punctuation">.</span>tokenizer<span class="token punctuation">(</span>text<span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span>self<span class="token punctuation">.</span>max_length<span class="token punctuation">,</span> return_length<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                        return_overflowing_tokens<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"max_length"</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span>
        <span class="token comment"># 拿出input_ids然后传入transformer进行特征提取。</span>
        tokens          <span class="token operator">=</span> batch_encoding<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        outputs         <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">(</span>input_ids<span class="token operator">=</span>tokens<span class="token punctuation">,</span> output_hidden_states<span class="token operator">=</span>self<span class="token punctuation">.</span>layer<span class="token operator">==</span><span class="token string">"hidden"</span><span class="token punctuation">)</span>
        <span class="token comment"># 取出所有的token</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>layer <span class="token operator">==</span> <span class="token string">"last"</span><span class="token punctuation">:</span>
            z <span class="token operator">=</span> outputs<span class="token punctuation">.</span>last_hidden_state
        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>layer <span class="token operator">==</span> <span class="token string">"pooled"</span><span class="token punctuation">:</span>
            z <span class="token operator">=</span> outputs<span class="token punctuation">.</span>pooler_output<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            z <span class="token operator">=</span> outputs<span class="token punctuation">.</span>hidden_states<span class="token punctuation">[</span>self<span class="token punctuation">.</span>layer_idx<span class="token punctuation">]</span>
        <span class="token keyword">return</span> z

    <span class="token keyword">def</span> <span class="token function">encode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="2_125"></a>2、采样流程</h4> 
<p><img src="https://images2.imgbox.com/92/55/88bErR2J_o.png" alt="在这里插入图片描述" width="500"></p> 
<h5><a id="a_127"></a>a、生成初始噪声</h5> 
<p>既然输入里面只有文本，没有输入图片，那么最初始的噪声哪里来？</p> 
<p>在这里直接搞个正态分布的噪声就可以了，简单理解就是：<strong>既然在训练的时候就是不断的给 原图 加 正态分布噪声 得到最终的噪声矩阵，那么我直接初始化一个 正态分布的噪声 作为 初始噪声 生成图片很合理吧</strong>。</p> 
<p>在代码里面其实也是这么做的，不过因为我们是在<strong>隐空间</strong>去进行扩散的，所以我们生成的噪声也是相对于<strong>隐空间</strong>的。</p> 
<p>在这里简单介绍一下VAE，VAE是变分自编码器，可以将输入图片进行编码，<strong>一个高宽原本为512x512x3的图片在使用VAE编码后会变成64x64x4</strong>，<strong>这个4是人为设定的，不必纠结为什么不是3</strong>。这个时候我们就使用一个简单的矩阵代替原有的512x512x3的图片了，传输与存储成本就很低。<strong>在实际要去看的时候，可以对64x64x4的矩阵进行解码，获得512x512x3的图片。</strong></p> 
<p>因此，如果 我们生成的噪声是相对于<strong>隐空间</strong>的，同时我们要生成一个512x512x3的图片，那么我们就要初始化一个64x64x4的隐向量，我们在隐空间扩散好后，<strong>再使用解码器就可以生成512x512x3的图像。</strong></p> 
<p>在代码中，我们确实是这么做的，初始噪声的生成代码为：</p> 
<pre><code class="prism language-python">img <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>shape<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
</code></pre> 
<p>代码位于ldm.models.diffusion.ddim.py中的ddim_sampling方法中。shape是外面传进来的，大小为<code>[4, 64, 64]</code>。<br> <img src="https://images2.imgbox.com/c8/91/kZKcT5GC_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="bN_144"></a>b、对噪声进行N次采样</h5> 
<p>既然Stable Diffusion是一个不断扩散的过程，那么少不了不断的去噪声，那么怎么去噪声便是一个问题。</p> 
<p>在上一步中，我们已经获得了一个img，它是一个符合正态分布的向量，我们便从它开始去噪声。</p> 
<p>我们会对ddim_timesteps的时间步取反，因为我们现在是去噪声而非加噪声，然后对其进行一个循环，循环的代码如下：</p> 
<p><strong>循环中有一个mask，它的作用是用于进行局部的重建，对部分区域的隐向量进行mask，此处没用到</strong>。其它东西都是个方法或者函数，也看不出东西来。<strong>在这里面看起来最像采样过程的就是p_sample_ddim方法，我们需要进入p_sample_ddim这个方法看看。</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">for</span> i<span class="token punctuation">,</span> step <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>iterator<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># index是用来取得对应的调节参数的</span>
    index   <span class="token operator">=</span> total_steps <span class="token operator">-</span> i <span class="token operator">-</span> <span class="token number">1</span>
    <span class="token comment"># 将步数拓展到bs维度</span>
    ts      <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> step<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>

    <span class="token comment"># 用于进行局部的重建，对部分区域的隐向量进行mask。</span>
    <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token keyword">assert</span> x0 <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
        img_orig <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>q_sample<span class="token punctuation">(</span>x0<span class="token punctuation">,</span> ts<span class="token punctuation">)</span>  <span class="token comment"># TODO: deterministic forward pass?</span>
        img <span class="token operator">=</span> img_orig <span class="token operator">*</span> mask <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1.</span> <span class="token operator">-</span> mask<span class="token punctuation">)</span> <span class="token operator">*</span> img

    <span class="token comment"># 进行采样</span>
    outs <span class="token operator">=</span> self<span class="token punctuation">.</span>p_sample_ddim<span class="token punctuation">(</span>img<span class="token punctuation">,</span> cond<span class="token punctuation">,</span> ts<span class="token punctuation">,</span> index<span class="token operator">=</span>index<span class="token punctuation">,</span> use_original_steps<span class="token operator">=</span>ddim_use_original_steps<span class="token punctuation">,</span>
                                quantize_denoised<span class="token operator">=</span>quantize_denoised<span class="token punctuation">,</span> temperature<span class="token operator">=</span>temperature<span class="token punctuation">,</span>
                                noise_dropout<span class="token operator">=</span>noise_dropout<span class="token punctuation">,</span> score_corrector<span class="token operator">=</span>score_corrector<span class="token punctuation">,</span>
                                corrector_kwargs<span class="token operator">=</span>corrector_kwargs<span class="token punctuation">,</span>
                                unconditional_guidance_scale<span class="token operator">=</span>unconditional_guidance_scale<span class="token punctuation">,</span>
                                unconditional_conditioning<span class="token operator">=</span>unconditional_conditioning<span class="token punctuation">)</span>
    img<span class="token punctuation">,</span> pred_x0 <span class="token operator">=</span> outs
    <span class="token comment"># 回调函数</span>
    <span class="token keyword">if</span> callback<span class="token punctuation">:</span> callback<span class="token punctuation">(</span>i<span class="token punctuation">)</span>
    <span class="token keyword">if</span> img_callback<span class="token punctuation">:</span> img_callback<span class="token punctuation">(</span>pred_x0<span class="token punctuation">,</span> i<span class="token punctuation">)</span>

    <span class="token keyword">if</span> index <span class="token operator">%</span> log_every_t <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> index <span class="token operator">==</span> total_steps <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>
        intermediates<span class="token punctuation">[</span><span class="token string">'x_inter'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>img<span class="token punctuation">)</span>
        intermediates<span class="token punctuation">[</span><span class="token string">'pred_x0'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>pred_x0<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/fa/a3/UPkr1Md0_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="c_182"></a>c、单次采样解析</h5> 
<h6><a id="I_183"></a>I、预测噪声</h6> 
<p>在进行单词采样前，需要首先判断是否有neg prompt，如果有，我们需要同时处理neg prompt，否则仅仅需要处理pos prompt。实际使用的时候一般都有neg prompt（效果会好一些），所以默认进入对应的处理过程。</p> 
<p>在处理neg prompt时，我们对输入进来的隐向量和步数进行复制，一个属于pos prompt，一个属于neg prompt。torch.cat默认堆叠维度为0，所以是在batch_size维度进行堆叠，二者不会互相影响。然后我们将pos prompt和neg prompt堆叠到一个batch中，也是在batch_size维度堆叠。</p> 
<pre><code class="prism language-python"><span class="token comment"># 首先判断是否由neg prompt，unconditional_conditioning是由neg prompt获得的</span>
<span class="token keyword">if</span> unconditional_conditioning <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">or</span> unconditional_guidance_scale <span class="token operator">==</span> <span class="token number">1.</span><span class="token punctuation">:</span>
    e_t <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>apply_model<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">,</span> c<span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token comment"># 一般都是有neg prompt的，所以进入到这里</span>
    <span class="token comment"># 在这里我们对隐向量和步数进行复制，一个属于pos prompt，一个属于neg prompt</span>
    <span class="token comment"># torch.cat默认堆叠维度为0，所以是在bs维度进行堆叠，二者不会互相影响</span>
    x_in <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span>
    t_in <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>t<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span>
    <span class="token comment"># 然后我们将pos prompt和neg prompt堆叠到一个batch中</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>c<span class="token punctuation">,</span> <span class="token builtin">dict</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">assert</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>unconditional_conditioning<span class="token punctuation">,</span> <span class="token builtin">dict</span><span class="token punctuation">)</span>
        c_in <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> k <span class="token keyword">in</span> c<span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>c<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                c_in<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span>
                    torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>unconditional_conditioning<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> c<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
                    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>c<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                <span class="token punctuation">]</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                c_in<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>unconditional_conditioning<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">,</span> c<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        c_in <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>unconditional_conditioning<span class="token punctuation">,</span> c<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/39/90/eUiIIyaN_o.png" alt="在这里插入图片描述"><br> 堆叠完后，我们将隐向量、步数和prompt条件一起传入网络中，将结果在bs维度进行使用chunk进行分割。</p> 
<p>因为我们在堆叠时，neg prompt放在了前面。因此分割好后，前半部分<code>e_t_uncond</code>属于利用neg prompt得到的，后半部分<code>e_t</code>属于利用pos prompt得到的，我们本质上应该<strong>扩大pos prompt的影响，远离neg prompt的影响</strong>。因此，我们使用<code>e_t-e_t_uncond</code>计算二者的距离，使用scale扩大二者的距离。在e_t_uncond基础上，得到最后的隐向量。</p> 
<pre><code class="prism language-python"><span class="token comment"># 堆叠完后，隐向量、步数和prompt条件一起传入网络中，将结果在bs维度进行使用chunk进行分割</span>
e_t_uncond<span class="token punctuation">,</span> e_t <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>apply_model<span class="token punctuation">(</span>x_in<span class="token punctuation">,</span> t_in<span class="token punctuation">,</span> c_in<span class="token punctuation">)</span><span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
e_t <span class="token operator">=</span> e_t_uncond <span class="token operator">+</span> unconditional_guidance_scale <span class="token operator">*</span> <span class="token punctuation">(</span>e_t <span class="token operator">-</span> e_t_uncond<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/05/8a/MjsgCzje_o.png" alt="在这里插入图片描述"><br> 此时获得的e_t就是通过隐向量和prompt共同获得的预测噪声啦。</p> 
<h6><a id="II_223"></a>II、施加噪声</h6> 
<p>获得噪声就OK了吗？显然不是的，我们还要将获得的新噪声，按照一定的比例添加到原来的原始噪声上。</p> 
<p>这个地方我们最好结合ddim中的公式来看，我们需要获得<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
         
         
           α 
          
         
           ˉ 
          
         
        
          t 
         
        
       
      
        \bar{\alpha}_t 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7178em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.5678em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.2222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: -0.0037em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>、<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
         
         
           α 
          
         
           ˉ 
          
         
         
         
           t 
          
         
           − 
          
         
           1 
          
         
        
       
      
        \bar{\alpha}_{t-1} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7761em; vertical-align: -0.2083em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.5678em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.2222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0037em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span></span></span></span></span>、<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          σ 
         
        
          t 
         
        
       
      
        \sigma_t 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>、<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
         
         
           1 
          
         
           − 
          
          
           
           
             α 
            
           
             ˉ 
            
           
          
            t 
           
          
         
        
       
      
        \sqrt{1-\bar{\alpha}_t} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.04em; vertical-align: -0.2078em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8322em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord" style="padding-left: 0.833em;"><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.5678em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.2222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: -0.0037em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.7922em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail" style="min-width: 0.853em; height: 1.08em;"> 
           <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
            <path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path> 
           </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2078em;"><span class=""></span></span></span></span></span></span></span></span></span>。<br> <img src="https://images2.imgbox.com/79/4a/P0fY6kDO_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/31/e8/MzwIl68y_o.png" alt="在这里插入图片描述"><br> 代码中，我们其实已经预先计算好了这些参数。我们只需要直接取出即可，下方的a_t也就是公式中括号外的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
         
         
           α 
          
         
           ˉ 
          
         
        
          t 
         
        
       
      
        \bar{\alpha}_t 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7178em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.5678em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.2222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: -0.0037em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，a_prev 就是公式中的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
         
         
           α 
          
         
           ˉ 
          
         
         
         
           t 
          
         
           − 
          
         
           1 
          
         
        
       
      
        \bar{\alpha}_{t-1} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7761em; vertical-align: -0.2083em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.5678em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.2222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0037em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，sigma_t就是公式中的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          σ 
         
        
          t 
         
        
       
      
        \sigma_t 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，sqrt_one_minus_at就是公式中的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
         
         
           1 
          
         
           − 
          
          
           
           
             α 
            
           
             ˉ 
            
           
          
            t 
           
          
         
        
       
      
        \sqrt{1-\bar{\alpha}_t} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.04em; vertical-align: -0.2078em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8322em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord" style="padding-left: 0.833em;"><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.5678em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.2222em;"><span class="mord">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: -0.0037em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.7922em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail" style="min-width: 0.853em; height: 1.08em;"> 
           <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
            <path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path> 
           </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2078em;"><span class=""></span></span></span></span></span></span></span></span></span>。</p> 
<pre><code class="prism language-python"><span class="token comment"># 根据采样器选择参数</span>
alphas      <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>alphas_cumprod <span class="token keyword">if</span> use_original_steps <span class="token keyword">else</span> self<span class="token punctuation">.</span>ddim_alphas
alphas_prev <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>alphas_cumprod_prev <span class="token keyword">if</span> use_original_steps <span class="token keyword">else</span> self<span class="token punctuation">.</span>ddim_alphas_prev
sqrt_one_minus_alphas <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>sqrt_one_minus_alphas_cumprod <span class="token keyword">if</span> use_original_steps <span class="token keyword">else</span> self<span class="token punctuation">.</span>ddim_sqrt_one_minus_alphas
sigmas      <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>ddim_sigmas_for_original_num_steps <span class="token keyword">if</span> use_original_steps <span class="token keyword">else</span> self<span class="token punctuation">.</span>ddim_sigmas

<span class="token comment"># 根据步数选择参数，</span>
<span class="token comment"># 这里的index就是上面循环中的total_steps - i - 1</span>
a_t         <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> alphas<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
a_prev      <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> alphas_prev<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
sigma_t     <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> sigmas<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
sqrt_one_minus_at <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> sqrt_one_minus_alphas<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span>device<span class="token operator">=</span>device<span class="token punctuation">)</span>
</code></pre> 
<p>其实这一步我们只是把公式需要用到的系数全都拿了出来，方便后面的加减乘除。然后我们便在代码中实现上述的公式。</p> 
<pre><code class="prism language-python"><span class="token comment"># current prediction for x_0</span>
<span class="token comment"># 公式中的最左边</span>
pred_x0             <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">-</span> sqrt_one_minus_at <span class="token operator">*</span> e_t<span class="token punctuation">)</span> <span class="token operator">/</span> a_t<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">if</span> quantize_denoised<span class="token punctuation">:</span>
    pred_x0<span class="token punctuation">,</span> _<span class="token punctuation">,</span> <span class="token operator">*</span>_  <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>first_stage_model<span class="token punctuation">.</span>quantize<span class="token punctuation">(</span>pred_x0<span class="token punctuation">)</span>
<span class="token comment"># direction pointing to x_t</span>
<span class="token comment"># 公式的中间</span>
dir_xt              <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1.</span> <span class="token operator">-</span> a_prev <span class="token operator">-</span> sigma_t<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> e_t
<span class="token comment"># 公式最右边</span>
noise               <span class="token operator">=</span> sigma_t <span class="token operator">*</span> noise_like<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> device<span class="token punctuation">,</span> repeat_noise<span class="token punctuation">)</span> <span class="token operator">*</span> temperature
<span class="token keyword">if</span> noise_dropout <span class="token operator">&gt;</span> <span class="token number">0.</span><span class="token punctuation">:</span>
    noise           <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>noise<span class="token punctuation">,</span> p<span class="token operator">=</span>noise_dropout<span class="token punctuation">)</span>
x_prev              <span class="token operator">=</span> a_prev<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> pred_x0 <span class="token operator">+</span> dir_xt <span class="token operator">+</span> noise
<span class="token comment"># 输出添加完公式的结果</span>
<span class="token keyword">return</span> x_prev<span class="token punctuation">,</span> pred_x0
</code></pre> 
<p><img src="https://images2.imgbox.com/09/9a/VmPEd7Y4_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="d_264"></a>d、预测噪声过程中的网络结构解析</h5> 
<h6><a id="Iapply_model_265"></a>I、apply_model方法解析</h6> 
<p>在3.a的预测噪声过程中，我们使用了model.apply_model方法进行噪声的预测，这个方法具体做了什么被隐掉了，我们看看具体做的工作。</p> 
<p>apply_model方法在ldm.models.diffusion.ddpm.py文件中。在apply_model中，我们将x_noisy传入self.model中预测噪声。</p> 
<pre><code class="prism language-python">x_recon <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x_noisy<span class="token punctuation">,</span> t<span class="token punctuation">,</span> <span class="token operator">**</span>cond<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/a2/45/Sxvx6GDX_o.png" alt="在这里插入图片描述"><br> self.model是一个预先构建好的类，定义在ldm.models.diffusion.ddpm.py文件的1416行，内部包含Stable Diffusion的Unet网络，self.model的功能有点类似于包装器，根据模型选择的特征融合方式，进行文本与上文生成的噪声的融合。</p> 
<p>c_concat代表使用堆叠的方式进行融合，c_crossattn代表使用attention的方式融合。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">DiffusionWrapper</span><span class="token punctuation">(</span>pl<span class="token punctuation">.</span>LightningModule<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> diff_model_config<span class="token punctuation">,</span> conditioning_key<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>sequential_cross_attn <span class="token operator">=</span> diff_model_config<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token string">"sequential_crossattn"</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token comment"># stable diffusion的unet网络</span>
        self<span class="token punctuation">.</span>diffusion_model <span class="token operator">=</span> instantiate_from_config<span class="token punctuation">(</span>diff_model_config<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conditioning_key <span class="token operator">=</span> conditioning_key
        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>conditioning_key <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token string">'concat'</span><span class="token punctuation">,</span> <span class="token string">'crossattn'</span><span class="token punctuation">,</span> <span class="token string">'hybrid'</span><span class="token punctuation">,</span> <span class="token string">'adm'</span><span class="token punctuation">,</span> <span class="token string">'hybrid-adm'</span><span class="token punctuation">,</span> <span class="token string">'crossattn-adm'</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">,</span> c_concat<span class="token punctuation">:</span> <span class="token builtin">list</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span> c_crossattn<span class="token punctuation">:</span> <span class="token builtin">list</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span> c_adm<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>conditioning_key <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            out <span class="token operator">=</span> self<span class="token punctuation">.</span>diffusion_model<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>conditioning_key <span class="token operator">==</span> <span class="token string">'concat'</span><span class="token punctuation">:</span>
            xc <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">]</span> <span class="token operator">+</span> c_concat<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
            out <span class="token operator">=</span> self<span class="token punctuation">.</span>diffusion_model<span class="token punctuation">(</span>xc<span class="token punctuation">,</span> t<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>conditioning_key <span class="token operator">==</span> <span class="token string">'crossattn'</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>sequential_cross_attn<span class="token punctuation">:</span>
                cc <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>c_crossattn<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                cc <span class="token operator">=</span> c_crossattn
            out <span class="token operator">=</span> self<span class="token punctuation">.</span>diffusion_model<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">,</span> context<span class="token operator">=</span>cc<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>conditioning_key <span class="token operator">==</span> <span class="token string">'hybrid'</span><span class="token punctuation">:</span>
            xc <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">]</span> <span class="token operator">+</span> c_concat<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
            cc <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>c_crossattn<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            out <span class="token operator">=</span> self<span class="token punctuation">.</span>diffusion_model<span class="token punctuation">(</span>xc<span class="token punctuation">,</span> t<span class="token punctuation">,</span> context<span class="token operator">=</span>cc<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>conditioning_key <span class="token operator">==</span> <span class="token string">'hybrid-adm'</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> c_adm <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
            xc <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">]</span> <span class="token operator">+</span> c_concat<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
            cc <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>c_crossattn<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            out <span class="token operator">=</span> self<span class="token punctuation">.</span>diffusion_model<span class="token punctuation">(</span>xc<span class="token punctuation">,</span> t<span class="token punctuation">,</span> context<span class="token operator">=</span>cc<span class="token punctuation">,</span> y<span class="token operator">=</span>c_adm<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>conditioning_key <span class="token operator">==</span> <span class="token string">'crossattn-adm'</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> c_adm <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
            cc <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>c_crossattn<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            out <span class="token operator">=</span> self<span class="token punctuation">.</span>diffusion_model<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">,</span> context<span class="token operator">=</span>cc<span class="token punctuation">,</span> y<span class="token operator">=</span>c_adm<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>conditioning_key <span class="token operator">==</span> <span class="token string">'adm'</span><span class="token punctuation">:</span>
            cc <span class="token operator">=</span> c_crossattn<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            out <span class="token operator">=</span> self<span class="token punctuation">.</span>diffusion_model<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">,</span> y<span class="token operator">=</span>cc<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> NotImplementedError<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> out
</code></pre> 
<p><img src="https://images2.imgbox.com/9f/23/4oeGkyqK_o.png" alt="在这里插入图片描述"><br> 代码中的self.diffusion_model便是Stable Diffusion的Unet网络，网络结构位于ldm.modules.diffusionmodules.openaimodel.py文件中的UNetModel类。</p> 
<h6><a id="IIUNetModel_321"></a>II、UNetModel模型解析</h6> 
<p><strong>UNetModel主要做的工作是结合时间步t和文本Embedding计算这一时刻的噪声</strong>。尽管UNet的思路非常简单，但是在StableDiffusion中，UNetModel由ResBlock和Transformer模块组成，整体来讲相比于普通的UNet复杂一些。</p> 
<p>Prompt通过Frozen CLIP Text Encoder获得Text Embedding，Timesteps通过全连接（MLP）获得Timesteps Embedding；</p> 
<p>ResBlock用于结合时间步Timesteps Embedding，Transformer模块用于结合文本Text Embedding。</p> 
<p>我在这里放一张大图，同学们可以看到内部shape的变化。<br> <img src="https://images2.imgbox.com/a2/2b/jd5jyMxS_o.jpg" alt="请添加图片描述"></p> 
<p>Unet代码如下所示：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">UNetModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    The full UNet model with attention and timestep embedding.
    :param in_channels: channels in the input Tensor.
    :param model_channels: base channel count for the model.
    :param out_channels: channels in the output Tensor.
    :param num_res_blocks: number of residual blocks per downsample.
    :param attention_resolutions: a collection of downsample rates at which
        attention will take place. May be a set, list, or tuple.
        For example, if this contains 4, then at 4x downsampling, attention
        will be used.
    :param dropout: the dropout probability.
    :param channel_mult: channel multiplier for each level of the UNet.
    :param conv_resample: if True, use learned convolutions for upsampling and
        downsampling.
    :param dims: determines if the signal is 1D, 2D, or 3D.
    :param num_classes: if specified (as an int), then this model will be
        class-conditional with `num_classes` classes.
    :param use_checkpoint: use gradient checkpointing to reduce memory usage.
    :param num_heads: the number of attention heads in each attention layer.
    :param num_heads_channels: if specified, ignore num_heads and instead use
                               a fixed channel width per attention head.
    :param num_heads_upsample: works with num_heads to set a different number
                               of heads for upsampling. Deprecated.
    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.
    :param resblock_updown: use residual blocks for up/downsampling.
    :param use_new_attention_order: use a different attention pattern for potentially
                                    increased efficiency.
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        image_size<span class="token punctuation">,</span>
        in_channels<span class="token punctuation">,</span>
        model_channels<span class="token punctuation">,</span>
        out_channels<span class="token punctuation">,</span>
        num_res_blocks<span class="token punctuation">,</span>
        attention_resolutions<span class="token punctuation">,</span>
        dropout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
        channel_mult<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        conv_resample<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
        dims<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
        num_classes<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        use_checkpoint<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        use_fp16<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        num_heads<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>
        num_head_channels<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>
        num_heads_upsample<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>
        use_scale_shift_norm<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        resblock_updown<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        use_new_attention_order<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        use_spatial_transformer<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    <span class="token comment"># custom transformer support</span>
        transformer_depth<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>              <span class="token comment"># custom transformer support</span>
        context_dim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                 <span class="token comment"># custom transformer support</span>
        n_embed<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>                     <span class="token comment"># custom support for prediction of discrete ids into codebook of first stage vq model</span>
        legacy<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> use_spatial_transformer<span class="token punctuation">:</span>
            <span class="token keyword">assert</span> context_dim <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token string">'Fool!! You forgot to include the dimension of your cross-attention conditioning...'</span>

        <span class="token keyword">if</span> context_dim <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> use_spatial_transformer<span class="token punctuation">,</span> <span class="token string">'Fool!! You forgot to use the spatial transformer for your cross-attention conditioning...'</span>
            <span class="token keyword">from</span> omegaconf<span class="token punctuation">.</span>listconfig <span class="token keyword">import</span> ListConfig
            <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>context_dim<span class="token punctuation">)</span> <span class="token operator">==</span> ListConfig<span class="token punctuation">:</span>
                context_dim <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>context_dim<span class="token punctuation">)</span>

        <span class="token keyword">if</span> num_heads_upsample <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>
            num_heads_upsample <span class="token operator">=</span> num_heads

        <span class="token keyword">if</span> num_heads <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> num_head_channels <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'Either num_heads or num_head_channels has to be set'</span>

        <span class="token keyword">if</span> num_head_channels <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> num_heads <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'Either num_heads or num_head_channels has to be set'</span>

        self<span class="token punctuation">.</span>image_size <span class="token operator">=</span> image_size
        self<span class="token punctuation">.</span>in_channels <span class="token operator">=</span> in_channels
        self<span class="token punctuation">.</span>model_channels <span class="token operator">=</span> model_channels
        self<span class="token punctuation">.</span>out_channels <span class="token operator">=</span> out_channels
        self<span class="token punctuation">.</span>num_res_blocks <span class="token operator">=</span> num_res_blocks
        self<span class="token punctuation">.</span>attention_resolutions <span class="token operator">=</span> attention_resolutions
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> dropout
        self<span class="token punctuation">.</span>channel_mult <span class="token operator">=</span> channel_mult
        self<span class="token punctuation">.</span>conv_resample <span class="token operator">=</span> conv_resample
        self<span class="token punctuation">.</span>num_classes <span class="token operator">=</span> num_classes
        self<span class="token punctuation">.</span>use_checkpoint <span class="token operator">=</span> use_checkpoint
        self<span class="token punctuation">.</span>dtype <span class="token operator">=</span> th<span class="token punctuation">.</span>float16 <span class="token keyword">if</span> use_fp16 <span class="token keyword">else</span> th<span class="token punctuation">.</span>float32
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads
        self<span class="token punctuation">.</span>num_head_channels <span class="token operator">=</span> num_head_channels
        self<span class="token punctuation">.</span>num_heads_upsample <span class="token operator">=</span> num_heads_upsample
        self<span class="token punctuation">.</span>predict_codebook_ids <span class="token operator">=</span> n_embed <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>

        <span class="token comment"># 用于计算当前采样时间t的embedding</span>
        time_embed_dim  <span class="token operator">=</span> model_channels <span class="token operator">*</span> <span class="token number">4</span>
        self<span class="token punctuation">.</span>time_embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            linear<span class="token punctuation">(</span>model_channels<span class="token punctuation">,</span> time_embed_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>SiLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            linear<span class="token punctuation">(</span>time_embed_dim<span class="token punctuation">,</span> time_embed_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>num_classes <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>label_emb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>num_classes<span class="token punctuation">,</span> time_embed_dim<span class="token punctuation">)</span>
        
        <span class="token comment"># 定义输入模块的第一个卷积</span>
        <span class="token comment"># TimestepEmbedSequential也可以看作一个包装器，根据层的种类进行时间或者文本的融合。</span>
        self<span class="token punctuation">.</span>input_blocks <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>
            <span class="token punctuation">[</span>
                TimestepEmbedSequential<span class="token punctuation">(</span>
                    conv_nd<span class="token punctuation">(</span>dims<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> model_channels<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
                <span class="token punctuation">)</span>
            <span class="token punctuation">]</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_feature_size  <span class="token operator">=</span> model_channels
        input_block_chans   <span class="token operator">=</span> <span class="token punctuation">[</span>model_channels<span class="token punctuation">]</span>
        ch                  <span class="token operator">=</span> model_channels
        ds                  <span class="token operator">=</span> <span class="token number">1</span>
        <span class="token comment"># 对channel_mult进行循环，channel_mult一共有四个值，代表unet四个部分通道的扩张比例</span>
        <span class="token comment"># [1, 2, 4, 4]</span>
        <span class="token keyword">for</span> level<span class="token punctuation">,</span> mult <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>channel_mult<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 每个部分循环两次</span>
            <span class="token comment"># 添加一个ResBlock和一个AttentionBlock</span>
            <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_res_blocks<span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token comment"># 先添加一个ResBlock</span>
                <span class="token comment"># 用于对输入的噪声进行通道数的调整，并且融合t的特征</span>
                layers <span class="token operator">=</span> <span class="token punctuation">[</span>
                    ResBlock<span class="token punctuation">(</span>
                        ch<span class="token punctuation">,</span>
                        time_embed_dim<span class="token punctuation">,</span>
                        dropout<span class="token punctuation">,</span>
                        out_channels<span class="token operator">=</span>mult <span class="token operator">*</span> model_channels<span class="token punctuation">,</span>
                        dims<span class="token operator">=</span>dims<span class="token punctuation">,</span>
                        use_checkpoint<span class="token operator">=</span>use_checkpoint<span class="token punctuation">,</span>
                        use_scale_shift_norm<span class="token operator">=</span>use_scale_shift_norm<span class="token punctuation">,</span>
                    <span class="token punctuation">)</span>
                <span class="token punctuation">]</span>
                <span class="token comment"># ch便是上述ResBlock的输出通道数</span>
                ch <span class="token operator">=</span> mult <span class="token operator">*</span> model_channels
                <span class="token keyword">if</span> ds <span class="token keyword">in</span> attention_resolutions<span class="token punctuation">:</span>
                    <span class="token comment"># num_heads=8</span>
                    <span class="token keyword">if</span> num_head_channels <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>
                        dim_head <span class="token operator">=</span> ch <span class="token operator">//</span> num_heads
                    <span class="token keyword">else</span><span class="token punctuation">:</span>
                        num_heads <span class="token operator">=</span> ch <span class="token operator">//</span> num_head_channels
                        dim_head <span class="token operator">=</span> num_head_channels
                    <span class="token keyword">if</span> legacy<span class="token punctuation">:</span>
                        <span class="token comment">#num_heads = 1</span>
                        dim_head <span class="token operator">=</span> ch <span class="token operator">//</span> num_heads <span class="token keyword">if</span> use_spatial_transformer <span class="token keyword">else</span> num_head_channels
                    <span class="token comment"># 使用了SpatialTransformer自注意力，加强全局特征，融合文本的特征</span>
                    layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                        AttentionBlock<span class="token punctuation">(</span>
                            ch<span class="token punctuation">,</span>
                            use_checkpoint<span class="token operator">=</span>use_checkpoint<span class="token punctuation">,</span>
                            num_heads<span class="token operator">=</span>num_heads<span class="token punctuation">,</span>
                            num_head_channels<span class="token operator">=</span>dim_head<span class="token punctuation">,</span>
                            use_new_attention_order<span class="token operator">=</span>use_new_attention_order<span class="token punctuation">,</span>
                        <span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token keyword">not</span> use_spatial_transformer <span class="token keyword">else</span> SpatialTransformer<span class="token punctuation">(</span>
                            ch<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dim_head<span class="token punctuation">,</span> depth<span class="token operator">=</span>transformer_depth<span class="token punctuation">,</span> context_dim<span class="token operator">=</span>context_dim
                        <span class="token punctuation">)</span>
                    <span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>input_blocks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>TimestepEmbedSequential<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span><span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>_feature_size <span class="token operator">+=</span> ch
                input_block_chans<span class="token punctuation">.</span>append<span class="token punctuation">(</span>ch<span class="token punctuation">)</span>
            <span class="token comment"># 如果不是四个部分中的最后一个部分，那么都要进行下采样。</span>
            <span class="token keyword">if</span> level <span class="token operator">!=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>channel_mult<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>
                out_ch <span class="token operator">=</span> ch
                <span class="token comment"># 在此处进行下采样</span>
                <span class="token comment"># 一般直接使用Downsample模块</span>
                self<span class="token punctuation">.</span>input_blocks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                    TimestepEmbedSequential<span class="token punctuation">(</span>
                        ResBlock<span class="token punctuation">(</span>
                            ch<span class="token punctuation">,</span>
                            time_embed_dim<span class="token punctuation">,</span>
                            dropout<span class="token punctuation">,</span>
                            out_channels<span class="token operator">=</span>out_ch<span class="token punctuation">,</span>
                            dims<span class="token operator">=</span>dims<span class="token punctuation">,</span>
                            use_checkpoint<span class="token operator">=</span>use_checkpoint<span class="token punctuation">,</span>
                            use_scale_shift_norm<span class="token operator">=</span>use_scale_shift_norm<span class="token punctuation">,</span>
                            down<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                        <span class="token punctuation">)</span>
                        <span class="token keyword">if</span> resblock_updown
                        <span class="token keyword">else</span> Downsample<span class="token punctuation">(</span>
                            ch<span class="token punctuation">,</span> conv_resample<span class="token punctuation">,</span> dims<span class="token operator">=</span>dims<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_ch
                        <span class="token punctuation">)</span>
                    <span class="token punctuation">)</span>
                <span class="token punctuation">)</span>
                <span class="token comment"># 为下一阶段定义参数。</span>
                ch <span class="token operator">=</span> out_ch
                input_block_chans<span class="token punctuation">.</span>append<span class="token punctuation">(</span>ch<span class="token punctuation">)</span>
                ds <span class="token operator">*=</span> <span class="token number">2</span>
                self<span class="token punctuation">.</span>_feature_size <span class="token operator">+=</span> ch

        <span class="token keyword">if</span> num_head_channels <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>
            dim_head <span class="token operator">=</span> ch <span class="token operator">//</span> num_heads
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            num_heads <span class="token operator">=</span> ch <span class="token operator">//</span> num_head_channels
            dim_head <span class="token operator">=</span> num_head_channels
        <span class="token keyword">if</span> legacy<span class="token punctuation">:</span>
            <span class="token comment">#num_heads = 1</span>
            dim_head <span class="token operator">=</span> ch <span class="token operator">//</span> num_heads <span class="token keyword">if</span> use_spatial_transformer <span class="token keyword">else</span> num_head_channels
        <span class="token comment"># 定义中间层</span>
        <span class="token comment"># ResBlock + SpatialTransformer + ResBlock</span>
        self<span class="token punctuation">.</span>middle_block <span class="token operator">=</span> TimestepEmbedSequential<span class="token punctuation">(</span>
            ResBlock<span class="token punctuation">(</span>
                ch<span class="token punctuation">,</span>
                time_embed_dim<span class="token punctuation">,</span>
                dropout<span class="token punctuation">,</span>
                dims<span class="token operator">=</span>dims<span class="token punctuation">,</span>
                use_checkpoint<span class="token operator">=</span>use_checkpoint<span class="token punctuation">,</span>
                use_scale_shift_norm<span class="token operator">=</span>use_scale_shift_norm<span class="token punctuation">,</span>
            <span class="token punctuation">)</span><span class="token punctuation">,</span>
            AttentionBlock<span class="token punctuation">(</span>
                ch<span class="token punctuation">,</span>
                use_checkpoint<span class="token operator">=</span>use_checkpoint<span class="token punctuation">,</span>
                num_heads<span class="token operator">=</span>num_heads<span class="token punctuation">,</span>
                num_head_channels<span class="token operator">=</span>dim_head<span class="token punctuation">,</span>
                use_new_attention_order<span class="token operator">=</span>use_new_attention_order<span class="token punctuation">,</span>
            <span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token keyword">not</span> use_spatial_transformer <span class="token keyword">else</span> SpatialTransformer<span class="token punctuation">(</span>
                            ch<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dim_head<span class="token punctuation">,</span> depth<span class="token operator">=</span>transformer_depth<span class="token punctuation">,</span> context_dim<span class="token operator">=</span>context_dim
                        <span class="token punctuation">)</span><span class="token punctuation">,</span>
            ResBlock<span class="token punctuation">(</span>
                ch<span class="token punctuation">,</span>
                time_embed_dim<span class="token punctuation">,</span>
                dropout<span class="token punctuation">,</span>
                dims<span class="token operator">=</span>dims<span class="token punctuation">,</span>
                use_checkpoint<span class="token operator">=</span>use_checkpoint<span class="token punctuation">,</span>
                use_scale_shift_norm<span class="token operator">=</span>use_scale_shift_norm<span class="token punctuation">,</span>
            <span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_feature_size <span class="token operator">+=</span> ch

        <span class="token comment"># 定义Unet上采样过程</span>
        self<span class="token punctuation">.</span>output_blocks <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token comment"># 循环把channel_mult反了过来</span>
        <span class="token keyword">for</span> level<span class="token punctuation">,</span> mult <span class="token keyword">in</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">enumerate</span><span class="token punctuation">(</span>channel_mult<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
            <span class="token comment"># 上采样时每个部分循环三次</span>
            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_res_blocks <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                ich <span class="token operator">=</span> input_block_chans<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token comment"># 首先添加ResBlock层</span>
                layers <span class="token operator">=</span> <span class="token punctuation">[</span>
                    ResBlock<span class="token punctuation">(</span>
                        ch <span class="token operator">+</span> ich<span class="token punctuation">,</span>
                        time_embed_dim<span class="token punctuation">,</span>
                        dropout<span class="token punctuation">,</span>
                        out_channels<span class="token operator">=</span>model_channels <span class="token operator">*</span> mult<span class="token punctuation">,</span>
                        dims<span class="token operator">=</span>dims<span class="token punctuation">,</span>
                        use_checkpoint<span class="token operator">=</span>use_checkpoint<span class="token punctuation">,</span>
                        use_scale_shift_norm<span class="token operator">=</span>use_scale_shift_norm<span class="token punctuation">,</span>
                    <span class="token punctuation">)</span>
                <span class="token punctuation">]</span>
                ch <span class="token operator">=</span> model_channels <span class="token operator">*</span> mult
                <span class="token comment"># 然后进行SpatialTransformer自注意力</span>
                <span class="token keyword">if</span> ds <span class="token keyword">in</span> attention_resolutions<span class="token punctuation">:</span>
                    <span class="token keyword">if</span> num_head_channels <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>
                        dim_head <span class="token operator">=</span> ch <span class="token operator">//</span> num_heads
                    <span class="token keyword">else</span><span class="token punctuation">:</span>
                        num_heads <span class="token operator">=</span> ch <span class="token operator">//</span> num_head_channels
                        dim_head <span class="token operator">=</span> num_head_channels
                    <span class="token keyword">if</span> legacy<span class="token punctuation">:</span>
                        <span class="token comment">#num_heads = 1</span>
                        dim_head <span class="token operator">=</span> ch <span class="token operator">//</span> num_heads <span class="token keyword">if</span> use_spatial_transformer <span class="token keyword">else</span> num_head_channels
                    layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                        AttentionBlock<span class="token punctuation">(</span>
                            ch<span class="token punctuation">,</span>
                            use_checkpoint<span class="token operator">=</span>use_checkpoint<span class="token punctuation">,</span>
                            num_heads<span class="token operator">=</span>num_heads_upsample<span class="token punctuation">,</span>
                            num_head_channels<span class="token operator">=</span>dim_head<span class="token punctuation">,</span>
                            use_new_attention_order<span class="token operator">=</span>use_new_attention_order<span class="token punctuation">,</span>
                        <span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token keyword">not</span> use_spatial_transformer <span class="token keyword">else</span> SpatialTransformer<span class="token punctuation">(</span>
                            ch<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dim_head<span class="token punctuation">,</span> depth<span class="token operator">=</span>transformer_depth<span class="token punctuation">,</span> context_dim<span class="token operator">=</span>context_dim
                        <span class="token punctuation">)</span>
                    <span class="token punctuation">)</span>
                <span class="token comment"># 如果不是channel_mult循环的第一个</span>
                <span class="token comment"># 且</span>
                <span class="token comment"># 是num_res_blocks循环的最后一次，则进行上采样</span>
                <span class="token keyword">if</span> level <span class="token keyword">and</span> i <span class="token operator">==</span> num_res_blocks<span class="token punctuation">:</span>
                    out_ch <span class="token operator">=</span> ch
                    layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                        ResBlock<span class="token punctuation">(</span>
                            ch<span class="token punctuation">,</span>
                            time_embed_dim<span class="token punctuation">,</span>
                            dropout<span class="token punctuation">,</span>
                            out_channels<span class="token operator">=</span>out_ch<span class="token punctuation">,</span>
                            dims<span class="token operator">=</span>dims<span class="token punctuation">,</span>
                            use_checkpoint<span class="token operator">=</span>use_checkpoint<span class="token punctuation">,</span>
                            use_scale_shift_norm<span class="token operator">=</span>use_scale_shift_norm<span class="token punctuation">,</span>
                            up<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                        <span class="token punctuation">)</span>
                        <span class="token keyword">if</span> resblock_updown
                        <span class="token keyword">else</span> Upsample<span class="token punctuation">(</span>ch<span class="token punctuation">,</span> conv_resample<span class="token punctuation">,</span> dims<span class="token operator">=</span>dims<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_ch<span class="token punctuation">)</span>
                    <span class="token punctuation">)</span>
                    ds <span class="token operator">//=</span> <span class="token number">2</span>
                self<span class="token punctuation">.</span>output_blocks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>TimestepEmbedSequential<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span><span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>_feature_size <span class="token operator">+=</span> ch

        <span class="token comment"># 最后在输出部分进行一次卷积</span>
        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            normalization<span class="token punctuation">(</span>ch<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>SiLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            zero_module<span class="token punctuation">(</span>conv_nd<span class="token punctuation">(</span>dims<span class="token punctuation">,</span> model_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>predict_codebook_ids<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>id_predictor <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            normalization<span class="token punctuation">(</span>ch<span class="token punctuation">)</span><span class="token punctuation">,</span>
            conv_nd<span class="token punctuation">(</span>dims<span class="token punctuation">,</span> model_channels<span class="token punctuation">,</span> n_embed<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token comment">#nn.LogSoftmax(dim=1)  # change to cross_entropy and produce non-normalized logits</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">convert_to_fp16</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Convert the torso of the model to float16.
        """</span>
        self<span class="token punctuation">.</span>input_blocks<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>convert_module_to_f16<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>middle_block<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>convert_module_to_f16<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>output_blocks<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>convert_module_to_f16<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">convert_to_fp32</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Convert the torso of the model to float32.
        """</span>
        self<span class="token punctuation">.</span>input_blocks<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>convert_module_to_f32<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>middle_block<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>convert_module_to_f32<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>output_blocks<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>convert_module_to_f32<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> timesteps<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> context<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Apply the model to an input batch.
        :param x: an [N x C x ...] Tensor of inputs.
        :param timesteps: a 1-D batch of timesteps.
        :param context: conditioning plugged in via crossattn
        :param y: an [N] Tensor of labels, if class-conditional.
        :return: an [N x C x ...] Tensor of outputs.
        """</span>
        <span class="token keyword">assert</span> <span class="token punctuation">(</span>y <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>num_classes <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
        <span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"must specify y if and only if the model is class-conditional"</span>
        hs      <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token comment"># 用于计算当前采样时间t的embedding</span>
        t_emb   <span class="token operator">=</span> timestep_embedding<span class="token punctuation">(</span>timesteps<span class="token punctuation">,</span> self<span class="token punctuation">.</span>model_channels<span class="token punctuation">,</span> repeat_only<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        emb     <span class="token operator">=</span> self<span class="token punctuation">.</span>time_embed<span class="token punctuation">(</span>t_emb<span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>num_classes <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> y<span class="token punctuation">.</span>shape <span class="token operator">==</span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
            emb <span class="token operator">=</span> emb <span class="token operator">+</span> self<span class="token punctuation">.</span>label_emb<span class="token punctuation">(</span>y<span class="token punctuation">)</span>

        <span class="token comment"># 对输入模块进行循环，进行下采样并且融合时间特征与文本特征。</span>
        h <span class="token operator">=</span> x<span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        <span class="token keyword">for</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>input_blocks<span class="token punctuation">:</span>
            h <span class="token operator">=</span> module<span class="token punctuation">(</span>h<span class="token punctuation">,</span> emb<span class="token punctuation">,</span> context<span class="token punctuation">)</span>
            hs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>h<span class="token punctuation">)</span>

        <span class="token comment"># 中间模块的特征提取</span>
        h <span class="token operator">=</span> self<span class="token punctuation">.</span>middle_block<span class="token punctuation">(</span>h<span class="token punctuation">,</span> emb<span class="token punctuation">,</span> context<span class="token punctuation">)</span>

        <span class="token comment"># 上采样模块的特征提取</span>
        <span class="token keyword">for</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>output_blocks<span class="token punctuation">:</span>
            h <span class="token operator">=</span> th<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>h<span class="token punctuation">,</span> hs<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
            h <span class="token operator">=</span> module<span class="token punctuation">(</span>h<span class="token punctuation">,</span> emb<span class="token punctuation">,</span> context<span class="token punctuation">)</span>
        h <span class="token operator">=</span> h<span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        <span class="token comment"># 输出模块</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>predict_codebook_ids<span class="token punctuation">:</span>
            <span class="token keyword">return</span> self<span class="token punctuation">.</span>id_predictor<span class="token punctuation">(</span>h<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>h<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="3_698"></a>3、隐空间解码生成图片</h4> 
<p><img src="https://images2.imgbox.com/3c/90/IgQxxYgO_o.png" alt="在这里插入图片描述" width="500"><br> 通过上述步骤，已经可以多次采样获得结果，然后我们便可以通过隐空间解码生成图片。</p> 
<p>隐空间解码生成图片的过程非常简单，将上文多次采样后的结果，使用decode_first_stage方法即可生成图片。</p> 
<p>在decode_first_stage方法中，网络调用VAE对获取到的64x64x3的隐向量进行解码，获得512x512x3的图片。</p> 
<pre><code class="prism language-python"><span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>no_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">decode_first_stage</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> z<span class="token punctuation">,</span> predict_cids<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> force_not_quantize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> predict_cids<span class="token punctuation">:</span>
        <span class="token keyword">if</span> z<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">4</span><span class="token punctuation">:</span>
            z <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>z<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        z <span class="token operator">=</span> self<span class="token punctuation">.</span>first_stage_model<span class="token punctuation">.</span>quantize<span class="token punctuation">.</span>get_codebook_entry<span class="token punctuation">(</span>z<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
        z <span class="token operator">=</span> rearrange<span class="token punctuation">(</span>z<span class="token punctuation">,</span> <span class="token string">'b h w c -&gt; b c h w'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>

    z <span class="token operator">=</span> <span class="token number">1.</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>scale_factor <span class="token operator">*</span> z
	<span class="token comment"># 一般无需分割输入，所以直接将x_noisy传入self.model中，在下面else进行</span>
    <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token string">"split_input_params"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    	<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>first_stage_model<span class="token punctuation">,</span> VQModelInterface<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> self<span class="token punctuation">.</span>first_stage_model<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>z<span class="token punctuation">,</span> force_not_quantize<span class="token operator">=</span>predict_cids <span class="token keyword">or</span> force_not_quantize<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> self<span class="token punctuation">.</span>first_stage_model<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>z<span class="token punctuation">)</span>
</code></pre> 
<h2><a id="_724"></a>文本到图像预测过程代码</h2> 
<p>整体预测代码如下：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> random

<span class="token keyword">import</span> einops
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> cv2
<span class="token keyword">import</span> os
<span class="token keyword">from</span> ldm_hacked <span class="token keyword">import</span> DDIMSampler
<span class="token keyword">from</span> ldm_hacked <span class="token keyword">import</span> create_model<span class="token punctuation">,</span> load_state_dict<span class="token punctuation">,</span> DDIMSampler
<span class="token keyword">from</span> pytorch_lightning <span class="token keyword">import</span> seed_everything

<span class="token comment"># ----------------------- #</span>
<span class="token comment">#   使用的参数</span>
<span class="token comment"># ----------------------- #</span>
<span class="token comment"># config的地址</span>
config_path <span class="token operator">=</span> <span class="token string">"model_data/sd_v15.yaml"</span>
<span class="token comment"># 模型的地址</span>
model_path  <span class="token operator">=</span> <span class="token string">"model_data/v1-5-pruned-emaonly.safetensors"</span>

<span class="token comment"># 生成的图像大小为input_shape</span>
input_shape <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">]</span>
<span class="token comment"># 一次生成几张图像</span>
num_samples <span class="token operator">=</span> <span class="token number">2</span>
<span class="token comment"># 采样的步数</span>
ddim_steps  <span class="token operator">=</span> <span class="token number">20</span>
<span class="token comment"># 采样的种子，为-1的话则随机。</span>
seed        <span class="token operator">=</span> <span class="token number">12345</span>
<span class="token comment"># eta</span>
eta         <span class="token operator">=</span> <span class="token number">0</span>

<span class="token comment"># 提示词</span>
prompt      <span class="token operator">=</span> <span class="token string">"a cat"</span>
<span class="token comment"># 正面提示词</span>
a_prompt    <span class="token operator">=</span> <span class="token string">"best quality, extremely detailed"</span>
<span class="token comment"># 负面提示词</span>
n_prompt    <span class="token operator">=</span> <span class="token string">"longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality"</span>
<span class="token comment"># 正负扩大倍数</span>
scale       <span class="token operator">=</span> <span class="token number">9</span>

<span class="token comment"># save_path</span>
save_path   <span class="token operator">=</span> <span class="token string">"imgs/outputs_imgs"</span>

<span class="token comment"># ----------------------- #</span>
<span class="token comment">#   创建模型</span>
<span class="token comment"># ----------------------- #</span>
model   <span class="token operator">=</span> create_model<span class="token punctuation">(</span>config_path<span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>load_state_dict<span class="token punctuation">(</span>model_path<span class="token punctuation">,</span> location<span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> strict<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
model   <span class="token operator">=</span> model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
ddim_sampler <span class="token operator">=</span> DDIMSampler<span class="token punctuation">(</span>model<span class="token punctuation">)</span>

<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> seed <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>
        seed <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">65535</span><span class="token punctuation">)</span>
    seed_everything<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>

    <span class="token comment"># ----------------------- #</span>
    <span class="token comment">#   获得编码后的prompt</span>
    <span class="token comment"># ----------------------- #</span>
    cond    <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">"c_crossattn"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>model<span class="token punctuation">.</span>get_learned_conditioning<span class="token punctuation">(</span><span class="token punctuation">[</span>prompt <span class="token operator">+</span> <span class="token string">', '</span> <span class="token operator">+</span> a_prompt<span class="token punctuation">]</span> <span class="token operator">*</span> num_samples<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
    un_cond <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">"c_crossattn"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>model<span class="token punctuation">.</span>get_learned_conditioning<span class="token punctuation">(</span><span class="token punctuation">[</span>n_prompt<span class="token punctuation">]</span> <span class="token operator">*</span> num_samples<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
    H<span class="token punctuation">,</span> W    <span class="token operator">=</span> input_shape
    shape   <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> H <span class="token operator">//</span> <span class="token number">8</span><span class="token punctuation">,</span> W <span class="token operator">//</span> <span class="token number">8</span><span class="token punctuation">)</span>

    <span class="token comment"># ----------------------- #</span>
    <span class="token comment">#   进行采样</span>
    <span class="token comment"># ----------------------- #</span>
    samples<span class="token punctuation">,</span> intermediates <span class="token operator">=</span> ddim_sampler<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>ddim_steps<span class="token punctuation">,</span> num_samples<span class="token punctuation">,</span>
                                                    shape<span class="token punctuation">,</span> cond<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> eta<span class="token operator">=</span>eta<span class="token punctuation">,</span>
                                                    unconditional_guidance_scale<span class="token operator">=</span>scale<span class="token punctuation">,</span>
                                                    unconditional_conditioning<span class="token operator">=</span>un_cond<span class="token punctuation">)</span>

    <span class="token comment"># ----------------------- #</span>
    <span class="token comment">#   进行解码</span>
    <span class="token comment"># ----------------------- #</span>
    x_samples <span class="token operator">=</span> model<span class="token punctuation">.</span>decode_first_stage<span class="token punctuation">(</span>samples<span class="token punctuation">)</span>
    x_samples <span class="token operator">=</span> <span class="token punctuation">(</span>einops<span class="token punctuation">.</span>rearrange<span class="token punctuation">(</span>x_samples<span class="token punctuation">,</span> <span class="token string">'b c h w -&gt; b h w c'</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">127.5</span> <span class="token operator">+</span> <span class="token number">127.5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>clip<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span>

<span class="token comment"># ----------------------- #</span>
<span class="token comment">#   保存图片</span>
<span class="token comment"># ----------------------- #</span>
<span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>save_path<span class="token punctuation">)</span><span class="token punctuation">:</span>
    os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span>save_path<span class="token punctuation">)</span>
<span class="token keyword">for</span> index<span class="token punctuation">,</span> image <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>x_samples<span class="token punctuation">)</span><span class="token punctuation">:</span>
    cv2<span class="token punctuation">.</span>imwrite<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>save_path<span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">(</span>index<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">".jpg"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>image<span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>COLOR_BGR2RGB<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f950adfca2a170068d3abef0c43db27a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">解读《生成式人工智能服务管理暂行办法》</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0ef2c59da7b4ecc4c8a7f74d96bccff4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【算法系列篇】模拟算法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>