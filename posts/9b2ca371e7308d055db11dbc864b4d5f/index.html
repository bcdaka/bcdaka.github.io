<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>GPT-SoVITS-WebUI可以中文声音克隆开源AI工具简介 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/9b2ca371e7308d055db11dbc864b4d5f/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="GPT-SoVITS-WebUI可以中文声音克隆开源AI工具简介">
  <meta property="og:description" content="引言： 近年来，随着人工智能技术的不断发展，语音技术已经成为了热门领域。尽管已有不少技术实现了一定程度的成功，但传统语音合成技术的问题依然存在。传统方法对于训练数据的需求量大、效果受到声音质量和口音差异的影响。因此，在语音技术领域中，一种受到广泛关注的新技术GPT-SoVITS-WebUI应运而生。GPT-SoVITS是花儿不哭大佬研发的低成本AI音色克隆软件，作为一种新兴的网络界面工具，具有自主学习、高效合成、支持多语言等特点，在语音领域中受到极高的关注和重视。它不仅能够迅速训练高质量的文本到语音模型，更能够克服声音质量和口音差异的影响，在语音合成中发挥既有的功效。同时，GPT-SoVITS-WebUI具有零次学习和少量次学习的TTS功能，可以实现自然语言转换，进一步提高了语音合成的性能。这一特性可为语音技术的开发者和爱好者带来更多的机会，实现多样化的语音应用，创造更加便捷的用户体验。综上所述，GPT-SoVITS-WebUI是一种非常先进的语音技术，具有广泛的应用前景和潜力。创作不易，要是对您有用请加个关注或点个赞，非常感谢了！
开源地址： https://github.com/RVC-Boss/GPT-SoVITS
以下依据官方文档整理而得：
特点： 零样本 TTS：输入 5 秒的人声样本，体验即时文本到语音转换。
小镜头 TTS：只需 1 分钟的训练数据即可微调模型，以提高语音相似度和真实感。
跨语言支持：使用与训练数据集不同的语言进行推理，目前支持英语、日语和中文。
WebUI工具：集成工具包括语音伴奏分离、自动训练集分割、中文ASR和文本标注，帮助初学者创建训练数据集和GPT/SoVITS模型。
详细说明：教程中文版 User guide （EN）
安装 对于中国地区的用户，您可以点击此处使用AutoDL Cloud Docker在线体验全部功能。
经测试的环境 Python 3.9、PyTorch 2.0.1、CUDA 11Python 3.10.13、PyTorch 2.1.2、CUDA 12.3Python 3.9、PyTorch 2.3.0.dev20240122、macOS 14.3（Apple 芯片） 注意：numba==0.56.4 需要 py&lt;3.11
window系统 如果你是 Windows 用户（使用 win&gt;=10 测试），你可以直接下载预打包的发行版，双击 go-webui.bat 启动 GPT-SoVITS-WebUI。
Linux操作系统 conda create -n GPTSoVits python=3.9 conda activate GPTSoVits bash install.sh macOS操作系统 只有满足以下条件的 Mac 才能训练模型：
搭载 Apple 芯片的 Mac 电脑macOS 12.3 或更高版本通过运行xcode-select --install 所有 Mac 都可以使用 CPU 进行推理，这已被证明优于 GPU 推理。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-03-10T08:02:07+08:00">
    <meta property="article:modified_time" content="2024-03-10T08:02:07+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">GPT-SoVITS-WebUI可以中文声音克隆开源AI工具简介</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h4 style="text-align:center;"><img alt="" src="https://images2.imgbox.com/ca/e5/sYk5Rs86_o.png"></h4> 
<h4>引言：</h4> 
<p>        近年来，随着人工智能技术的不断发展，语音技术已经成为了热门领域。尽管已有不少技术实现了一定程度的成功，但传统语音合成技术的问题依然存在。传统方法对于训练数据的需求量大、效果受到声音质量和口音差异的影响。因此，在语音技术领域中，一种受到广泛关注的新技术GPT-SoVITS-WebUI应运而生。GPT-SoVITS是<a href="https://space.bilibili.com/5760446/" rel="nofollow" title="花儿不哭">花儿不哭</a>大佬研发的低成本AI音色克隆软件，作为一种新兴的网络界面工具，具有自主学习、高效合成、支持多语言等特点，在语音领域中受到极高的关注和重视。它不仅能够迅速训练高质量的文本到语音模型，更能够克服声音质量和口音差异的影响，在语音合成中发挥既有的功效。同时，GPT-SoVITS-WebUI具有零次学习和少量次学习的TTS功能，可以实现自然语言转换，进一步提高了语音合成的性能。这一特性可为语音技术的开发者和爱好者带来更多的机会，实现多样化的语音应用，创造更加便捷的用户体验。综上所述，GPT-SoVITS-WebUI是一种非常先进的语音技术，具有广泛的应用前景和潜力。<strong><span style="color:#511b78;"><span style="background-color:#efedf6;">创作不易，要是对您有用请加个关注或点个赞，非常感谢了！</span></span></strong></p> 
<h4>开源地址：</h4> 
<p>https://github.com/RVC-Boss/GPT-SoVITS</p> 
<p>以下依据官方文档整理而得：</p> 
<h3>特点：</h3> 
<ol><li> <p><strong>零样本 TTS：</strong>输入 5 秒的人声样本，体验即时文本到语音转换。</p> </li><li> <p><strong>小镜头 TTS：</strong>只需 1 分钟的训练数据即可微调模型，以提高语音相似度和真实感。</p> </li><li> <p><strong>跨语言支持：</strong>使用与训练数据集不同的语言进行推理，目前支持英语、日语和中文。</p> </li><li> <p><strong>WebUI工具：</strong>集成工具包括语音伴奏分离、自动训练集分割、中文ASR和文本标注，帮助初学者创建训练数据集和GPT/SoVITS模型。</p> </li><li> <p><strong>详细说明</strong>：<a href="https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e" rel="nofollow" title="教程中文版">教程中文版</a> <a href="https://rentry.co/GPT-SoVITS-guide#/" rel="nofollow" title="User guide （EN）">User guide （EN）</a></p> </li></ol> 
<h3>安装</h3> 
<p></p> 
<p>对于中国地区的用户，您可以<a href="https://www.codewithgpu.com/i/RVC-Boss/GPT-SoVITS/GPT-SoVITS-Official" rel="nofollow" title="点击此处">点击此处</a>使用AutoDL Cloud Docker在线体验全部功能。</p> 
<h4>经测试的环境</h4> 
<p></p> 
<ul><li>Python 3.9、PyTorch 2.0.1、CUDA 11</li><li>Python 3.10.13、PyTorch 2.1.2、CUDA 12.3</li><li>Python 3.9、PyTorch 2.3.0.dev20240122、macOS 14.3（Apple 芯片）</li></ul> 
<p><em>注意：numba==0.56.4 需要 py&lt;3.11</em></p> 
<h4>window系统</h4> 
<p>如果你是 Windows 用户（使用 win&gt;=10 测试），你可以直接下载<a href="https://huggingface.co/lj1995/GPT-SoVITS-windows-package/resolve/main/GPT-SoVITS-beta.7z?download=true" rel="nofollow" title="预打包的发行版">预打包的发行版</a>，双击 <em>go-webui.bat</em> 启动 GPT-SoVITS-WebUI。</p> 
<h4>Linux操作系统</h4> 
<div> 
 <pre><span style="color:#1f2328;">conda create -n GPTSoVits python=3.9
conda activate GPTSoVits
bash install.sh</span></pre> 
</div> 
<div> 
 <h4><span style="color:#1f2328;"><span style="background-color:#ffffff;">macOS操作系统</span></span></h4> 
</div> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">只有满足以下条件的 Mac 才能训练模型：</span></span></p> 
<ul><li>搭载 Apple 芯片的 Mac 电脑</li><li>macOS 12.3 或更高版本</li><li>通过运行<code>xcode-select --install</code></li></ul> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;"><strong>所有 Mac 都可以使用 CPU 进行推理，这已被证明优于 GPU 推理。</strong></span></span></p> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">首先，通过运行 或 确保已安装 FFmpeg，然后使用以下命令进行安装：<code>brew install ffmpeg</code><code>conda install ffmpeg</code></span></span></p> 
<div> 
 <pre><span style="color:#1f2328;">conda create -n GPTSoVits python=3.9
conda activate GPTSoVits

pip3 install --pre torch torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu
pip install -r requirements.txt</span></pre> 
</div> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;"><em>注意：仅当安装了 PyTorch Nightly 时，训练模型才有效。</em></span></span></p> 
<p></p> 
<h4>手动安装</h4> 
<h5>安装依赖项</h5> 
<pre>pip install -r requirements.txt</pre> 
<h5>安装 FFmpeg</h5> 
<h6>Conda 用户</h6> 
<pre>conda install ffmpeg</pre> 
<h6>Ubuntu/Debian 用户</h6> 
<pre>sudo apt install ffmpeg
sudo apt install libsox-dev
conda install -c conda-forge 'ffmpeg&lt;7'</pre> 
<h6>Windows 用户</h6> 
<p>下载并放置<a href="https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffmpeg.exe" rel="nofollow" title="ffmpeg.exe">ffmpeg.exe</a>和<a href="https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffprobe.exe" rel="nofollow" title="ffprobe.exe">ffprobe.exe</a> GPT-SoVITS 根目录。</p> 
<h4>使用 Docker</h4> 
<h5>docker-compose.yaml 配置</h5> 
<ol><li>关于镜像标签：由于代码库更新速度快，镜像打包测试过程较慢，请在 <a href="https://hub.docker.com/r/breakstring/gpt-sovits" rel="nofollow" title="Docker Hub">Docker Hub</a> 上查看当前打包的最新镜像，并根据自己的情况进行选择，或者根据自己的需求使用 Dockerfile 在本地构建。</li><li>环境变量：</li></ol> 
<ul><li>is_half：控制半精度/双精度。如果在“SSL 提取”步骤中未正确生成目录 4-cnhubert/5-wav32k 下的内容，则通常是原因。根据您的实际情况调整为“真”或“假”。</li></ul> 
<ol><li>卷配置，容器内应用的根目录设置为 /workspace。默认的 docker-compose.yaml 列出了一些上传/下载内容的实际示例。</li><li>shm_size：Windows 上 Docker Desktop 默认可用内存太小，可能导致操作异常。根据自己的情况进行调整。</li><li>在部署部分，GPU相关设置应根据您的系统和实际情况谨慎调整。</li></ol> 
<h5>使用 docker compose 运行</h5> 
<pre><code>docker compose -f "docker-compose.yaml" up -d
</code></pre> 
<h5>使用 docker 命令运行</h5> 
<p>如上所述，根据您的实际情况修改相应的参数，然后运行以下命令：</p> 
<pre><code>docker run --rm -it --gpus=all --env=is_half=False --volume=G:\GPT-SoVITS-DockerTest\output:/workspace/output --v</code></pre> 
<div> 
 <h3><span style="color:#1f2328;"><span style="background-color:#ffffff;">预训练模型</span></span></h3> 
</div> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">从 <a href="https://huggingface.co/lj1995/GPT-SoVITS" rel="nofollow" title="GPT-SoVITS 模型">GPT-SoVITS 模型</a>下载预训练模型并将它们放在 中。<code>GPT_SoVITS/pretrained_models</code></span></span></p> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">对于UVR5（人声/伴奏分离和混响去除），请从<a href="https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/uvr5_weights" rel="nofollow" title="UVR5 Weights">UVR5 Weights</a>下载模型并将其放置在 .<code>tools/uvr5/uvr5_weights</code></span></span></p> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">中国地区的用户可以通过输入下面的链接并点击“下载副本”来下载这两个模型</span></span></p> 
<ul><li> <p><a href="https://www.icloud.com.cn/iclouddrive/056y_Xog_HXpALuVUjscIwTtg#GPT-SoVITS_Models" rel="nofollow" title="GPT-SoVITS 模型">GPT-SoVITS 模型</a></p> </li><li> <p><a href="https://www.icloud.com.cn/iclouddrive/0bekRKDiJXboFhbfm3lM2fVbA#UVR5_Weights" rel="nofollow" title="UVR5砝码">UVR5砝码</a></p> </li></ul> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">对于中文 ASR（另外），请从 <a href="https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/files" rel="nofollow" title="Damo ASR 模型">Damo ASR 模型</a>、<a href="https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/files" rel="nofollow" title="Damo VAD 模型">Damo VAD 模型</a>和 <a href="https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/files" rel="nofollow" title="Damo Punc 模型">Damo Punc 模型</a>下载模型，并将其放在 中。<code>tools/damo_asr/models</code></span></span></p> 
<div> 
 <h3><span style="color:#1f2328;"><span style="background-color:#ffffff;">数据集格式</span></span></h3> 
</div> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">TTS 注解 .list 文件格式：</span></span></p> 
<div> 
 <pre class="has"><code class="language-notranslate">&lt;span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"&gt;&lt;span style="color:#1f2328"&gt;&lt;span style="color:var(--fgColor-default, var(--color-fg-default))"&gt;&lt;span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"&gt;&lt;code&gt;vocal_path|speaker_name|language|text
&lt;/code&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;</code></pre> 
</div> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">语言词典：</span></span></p> 
<ul><li>'zh'： 中文</li><li>'ja'： 日语</li><li>'en'： English</li></ul> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">例：</span></span></p> 
<div> 
 <pre class="has"><code class="language-notranslate">&lt;span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"&gt;&lt;span style="color:#1f2328"&gt;&lt;span style="color:var(--fgColor-default, var(--color-fg-default))"&gt;&lt;span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"&gt;&lt;code&gt;D:\GPT-SoVITS\xxx/xxx.wav|xxx|en|I like playing Genshin.
&lt;/code&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;</code></pre> 
</div> 
<div> 
 <h3><span style="color:#1f2328;"><span style="background-color:#ffffff;">待办事项列表</span></span></h3> 
</div> 
<ul><li> <p> <strong>高优先级：</strong></p> 
  <ul><li>日语和英语本地化。</li><li>用户指南。</li><li>日语和英语数据集微调训练。</li></ul></li><li> <p> <strong>特征：</strong></p> 
  <ul><li>零样本语音转换（5s）/小样本语音转换（1min）。</li><li>TTS语速控制。</li><li>增强的 TTS 情绪控制。</li><li>尝试将 SoVITS 令牌输入更改为词汇的概率分布。</li><li>改进英文和日文文本前端。</li><li>开发小型和大型 TTS 模型。</li><li>Colab 脚本。</li><li>尝试扩展训练数据集（2k 小时 -&gt; 10k 小时）。</li><li>更好的 Sovits基本型号（增强的音频质量）</li><li>模型组合</li></ul></li></ul> 
<div> 
 <h3><span style="color:#1f2328;"><span style="background-color:#ffffff;">（可选）如果需要，这里将提供命令行操作模式</span></span></h3> 
</div> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">使用命令行打开 UVR5 的 WebUI</span></span></p> 
<div> 
 <pre class="has"><code class="language-notranslate">&lt;span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"&gt;&lt;span style="color:#1f2328"&gt;&lt;span style="color:var(--fgColor-default, var(--color-fg-default))"&gt;&lt;span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"&gt;&lt;code&gt;python tools/uvr5/webui.py "&lt;infer_device&gt;" &lt;is_half&gt; &lt;webui_port_uvr5&gt;
&lt;/code&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;</code></pre> 
</div> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">如果打不开浏览器，按照下面的格式进行UVR处理，这是使用mdxnet进行音频处理</span></span></p> 
<div> 
 <pre class="has"><code class="language-notranslate">&lt;span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"&gt;&lt;span style="color:#1f2328"&gt;&lt;span style="color:var(--fgColor-default, var(--color-fg-default))"&gt;&lt;span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"&gt;&lt;code&gt;python mdxnet.py --model --input_root --output_vocal --output_ins --agg_level --format --device --is_half_precision 
&lt;/code&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;</code></pre> 
</div> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">这是使用命令行完成数据集的音频分割的方式</span></span></p> 
<div> 
 <pre class="has"><code class="language-notranslate">&lt;span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"&gt;&lt;span style="color:#1f2328"&gt;&lt;span style="color:var(--fgColor-default, var(--color-fg-default))"&gt;&lt;span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"&gt;&lt;code&gt;python audio_slicer.py \
    --input_path "&lt;path_to_original_audio_file_or_directory&gt;" \
    --output_root "&lt;directory_where_subdivided_audio_clips_will_be_saved&gt;" \
    --threshold &lt;volume_threshold&gt; \
    --min_length &lt;minimum_duration_of_each_subclip&gt; \
    --min_interval &lt;shortest_time_gap_between_adjacent_subclips&gt; 
    --hop_size &lt;step_size_for_computing_volume_curve&gt;
&lt;/code&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;</code></pre> 
</div> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">这是使用命令行完成数据集 ASR 处理的方式（仅限中文）</span></span></p> 
<div> 
 <pre class="has"><code class="language-notranslate">&lt;span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"&gt;&lt;span style="color:#1f2328"&gt;&lt;span style="color:var(--fgColor-default, var(--color-fg-default))"&gt;&lt;span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"&gt;&lt;code&gt;python tools/damo_asr/cmd-asr.py "&lt;Path to the directory containing input audio files&gt;"
&lt;/code&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;</code></pre> 
</div> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">ASR处理通过Faster_Whisper进行（ASR标记，中文除外）</span></span></p> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">（没有进度条，GPU性能可能会导致时间延迟）</span></span></p> 
<div> 
 <pre class="has"><code class="language-notranslate">&lt;span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"&gt;&lt;span style="color:#1f2328"&gt;&lt;span style="color:var(--fgColor-default, var(--color-fg-default))"&gt;&lt;span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"&gt;&lt;code&gt;python ./tools/damo_asr/WhisperASR.py -i &lt;input&gt; -o &lt;output&gt; -f &lt;file_name.list&gt; -l &lt;language&gt;
&lt;/code&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;</code></pre> 
</div> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">已启用自定义列表保存路径</span></span></p> 
<div> 
 <h3></h3> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/fd781bd936d47103502a039b7cc6dd04/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">下载pycharm后还要python吗,用python必须下载pycharm</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e787c017df9022322c418474ae5d70f4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【人工智能】本地搭建AI模型Gemma</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>