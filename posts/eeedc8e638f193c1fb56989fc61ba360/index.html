<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>笔记本电脑安装属于自己的Llama 3 8B大模型和对话客户端 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/eeedc8e638f193c1fb56989fc61ba360/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="笔记本电脑安装属于自己的Llama 3 8B大模型和对话客户端">
  <meta property="og:description" content="选择 Llama 3 模型版本（8B，80 亿参数） 特别注意： Meta 虽然开源了 Llama 3 大模型，但是每个版本都有 Meta 的许可协议，建议大家在接受使用这些模型所需的条款之前仔细阅读。
Llama 3 模型版本有几个，我们主要关注 80 亿参数（Llama 3 8B）和 700 亿参数（Llama 3 70B）这两个版本。它们对电脑系统配置有不同的要求，主要计算资源（即：CPU/GPU）和内存来存储和处理模型权重：
Llama 3 8B 版本：对于 80 亿参数的模型，建议至少 4 核 CPU，至少 16GB 内存（推荐 32GB 或更高），以确保模型加载和运行过程中的流畅性；模型文件大小 5 GB 左右，磁盘空间有 10GB 足够了；GPU 是可选的，它可以显著提高推理速度
Llama 3 70B 版本：对于 700 亿参数的模型，CPU 要求显著提高（建议 16 核以上），至少需要 64GB 内存（推荐 128GB 或更高），模型在推理时会占用大量的内存资源；模型文件超过 20GB，远超 8B 版本；强烈推荐使用高端 GPU，以实现有效加速
综上所述，8B 版本比较适合我们个人电脑，硬件配置基本能符合，同时模型又不失推理效果：
下载 Llama 3 8B 模型文件 我们第一步是想自己部署尝鲜，因此直接下载压缩后的模型权重，文件为GGUF格式，GGUF格式是为了快速推理和优化内存使用而设计的，相比以前的GGML格式，GGUF支持更复杂的令牌化过程和特殊令牌处理，能更好地应对多样化的语言模型需求。就是因为有GGUF格式，Llama 3大语言模型才可以在笔记本电脑上运行，同时GGUF就一个文件，也简化了模型交换和部署的过程，它对促进模型的普及和应用有着积极作用。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-15T11:15:26+08:00">
    <meta property="article:modified_time" content="2024-06-15T11:15:26+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">笔记本电脑安装属于自己的Llama 3 8B大模型和对话客户端</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_Llama_3_8B80__2"></a>选择 Llama 3 模型版本（8B，80 亿参数）</h3> 
<p><strong>特别注意：</strong> Meta 虽然开源了 Llama 3 大模型，但是每个版本都有 Meta 的许可协议，建议大家在接受使用这些模型所需的条款之前仔细阅读。</p> 
<p>Llama 3 模型版本有几个，我们主要关注 80 亿参数（<strong>Llama 3 8B</strong>）和 700 亿参数（Llama 3 70B）这两个版本。它们对电脑系统配置有不同的要求，主要计算资源（即：CPU/GPU）和内存来存储和处理模型权重：</p> 
<ul><li> <p>Llama 3 8B 版本：对于 80 亿参数的模型，建议至少 4 核 CPU，至少 16GB 内存（推荐 32GB 或更高），以确保模型加载和运行过程中的流畅性；模型文件大小 5 GB 左右，磁盘空间有 10GB 足够了；GPU 是可选的，它可以显著提高推理速度</p> </li><li> <p>Llama 3 70B 版本：对于 700 亿参数的模型，CPU 要求显著提高（建议 16 核以上），至少需要 64GB 内存（推荐 128GB 或更高），模型在推理时会占用大量的内存资源；模型文件超过 20GB，远超 8B 版本；强烈推荐使用高端 GPU，以实现有效加速</p> </li></ul> 
<p>综上所述，8B 版本比较适合我们个人电脑，硬件配置基本能符合，同时模型又不失推理效果：</p> 
<p><img src="https://images2.imgbox.com/a7/f3/B6mWokco_o.jpg" alt="笔记本电脑配置"></p> 
<h3><a id="_Llama_3_8B__17"></a>下载 Llama 3 8B 模型文件</h3> 
<p>我们第一步是想自己部署尝鲜，因此直接下载压缩后的模型权重，文件为<strong>GGUF</strong>格式，<strong>GGUF</strong>格式是为了快速推理和优化内存使用而设计的，相比以前的<strong>GGML</strong>格式，<strong>GGUF</strong>支持更复杂的令牌化过程和特殊令牌处理，能更好地应对多样化的语言模型需求。就是因为有<strong>GGUF</strong>格式，<strong>Llama 3</strong>大语言模型才可以在笔记本电脑上运行，同时<strong>GGUF</strong>就一个文件，也简化了模型交换和部署的过程，它对促进模型的普及和应用有着积极作用。</p> 
<p>因为<strong>Hugging Face</strong>官网正常无法访问，因此推荐<strong>国内镜像</strong>进行下载：</p> 
<p>官网地址：[https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/tree/main]</p> 
<p>国内镜像：[https://hf-mirror.com/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/tree/main]</p> 
<p>GGUF 模型文件名称接受，如上述列表中，有<code>Meta-Llama-3-8B-Instruct.Q4_K_M.gguf</code>和<code>Meta-Llama-3-8B-Instruct.Q5_K_M.gguf</code>等：</p> 
<ul><li> <p><strong>Instruct</strong>代表本模型是对基线模型进行了微调，用于更好地理解和生成遵循指令（instruction-following）的文本，以提供符合要求的响应</p> </li><li> <p><strong>Q4/Q5 等</strong>代表模型权重的量化位数（其中<strong>Q</strong>是<strong>Quantization</strong>的缩小，即量化），是一种模型压缩技术，用于减少模型大小，同时降低对计算资源的需求（特别是内存），但又尽量保持模型的性能；数字<strong>4</strong>或<strong>5</strong>则代表量化精度的位数（Q4 是 4 位，Q5 是 5 位等），精度越高模型体积和内存使用也会越大，但仍然远小于未量化的基线模型</p> </li><li> <p><strong>K_M/K_S</strong>代表含义笔者还未明确，<strong>K</strong>可能是<strong>Knowledge</strong>的缩写；<strong>M</strong>应该是<strong>Medium</strong>缩写（即中等模型），<strong>S</strong>应该是<strong>Small</strong>缩小（即小模型）；若有明确的朋友，还望不吝告知，共同进步！</p> </li></ul> 
<p>若个人电脑配置不是特别好，我们可以选择<strong>Q2_K</strong>版本（大小 3.2GB），它相较于<strong>Q4_K_M</strong>版本（大小 4.9GB），<strong>Q2</strong>版本的推理精度较低，但速度较快，而<strong>Q4</strong>版本在速度和精度之间均取得了很好的平衡，因此首选推荐<strong>Q4_K_M</strong>版本。</p> 
<p>点击<strong>下载</strong>图标即可下载，由于文件较大，浏览器的下载容易过程容易终端，重试可继续下载（笔者浏览器中断了好几次，总共耗时 4 个多小时）</p> 
<h3><a id="_40"></a>启动大模型服务端</h3> 
<p><strong>GGUF</strong>模型量化文件下载完成后，我们就可以来运行<strong>Llama 3</strong>大模型了。首先打开一个 Terminal 终端窗口，切换到<strong>GGUF</strong>文件目录，设置 Python<strong>虚拟环境</strong>：</p> 
<pre><code># 切换到存放GGUF文件目录
cd ~/PythonSpace/Llama3-8B/

# 切换Python 3.12.2版本
conda activate PY3.12.2

# 创建并激活虚拟环境
python -m venv venv
source ./venv/bin/activate

# 安装依赖包
pip install llama-cpp-python
pip install openai
pip install uvicorn
pip install starlette
pip install fastapi
pip install sse_starlette
pip install starlette_context
pip install pydantic_settings

# 启动Llama大模型
python -m llama_cpp.server --host 0.0.0.0 --model \
   ./Meta-Llama-3-8B-Instruct.Q4_K_M.gguf \
   --n_ctx 2048

</code></pre> 
<p>最后启动 Llama 模型命令中，<code>n_ctx 2048</code>代表单次回话最大 Token 数量。启动成功，我们应该看到类似如下的信息：</p> 
<p>恭喜你，你已经迈入 Llama 大模型大厦的大门了，后面存在无限可能，就看我们的创意了！<br> <img src="https://images2.imgbox.com/5c/ce/xziZb8CY_o.jpg" alt="在这里插入图片描述"></p> 
<h3><a id="_Llama__80"></a>编写 Llama 模型对话客户端</h3> 
<p>接下来，我们将使用<strong>llama-cpp</strong>库和<strong>openai</strong>库在个人电脑上快速搭建<strong>Llama 模型</strong>的<strong>客户端</strong>，开始尝鲜大模型（它<strong>目前</strong>只是个控制台客户端，还不能如 ChatGPT 那样有可视化的界面，但它的功能一样完备，所以请各位不用着急，我们先来体验一下 Llama 大模型，可视化的界面下文我在和大家分享）。</p> 
<p>Python 客户端代码如下，为了后续方便演示，这个 <strong>Client.py</strong> 文件也放到<strong>GGUF</strong>模型文件一起：</p> 
<ol><li>我们使用<strong>OpenAI</strong>接口来与 Llama 交互，上面启动模型的最后，我们看到服务端 IP 是本地，端口是<strong>8000</strong></li><li>接着，我们使用 2 条信息对历史记录进行初始化：第一个条是<strong>系统信息</strong>，第二个条是要求模型自我介绍的<strong>用户提示</strong>，为了避免长篇大论，我这里限制了回答的长度和字数</li><li>接下来，通过<code>&gt;</code>提示符等待用户（即我们）输入，输入<code>bye</code>、<code>quit</code>和<code>exit</code>任意一个即代表退出客户端</li></ol> 
<pre><code>from openai import OpenAI

# 注意服务端端口，因为是本地，所以不需要api_key
client = OpenAI(base_url="http://localhost:8000/v1",
         api_key="not-needed")

# 对话历史：设定系统角色是一个只能助理，同时提交“自我介绍”问题
history = [
    {"role": "system", "content": "你是一个智能助理，你的回答总是正确的、有用的和内容非常精简."},
    {"role": "user", "content": "请用中文进行自我介绍，要求不能超过5句话，总字数不超过100个字。"},
]
print("\033[92;1m")

# 首次自我介绍完毕，接下来是等代码我们的提示
while True:
    completion = client.chat.completions.create(
        model="local-model",
        messages=history,
        temperature=0.7,
        stream=True,
    )

    new_message = {"role": "assistant", "content": ""}

    for chunk in completion:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="", flush=True)
            new_message["content"] += chunk.choices[0].delta.content

    history.append(new_message)
    print("\033[91;1m")

    userinput = input("&gt; ")
    if userinput.lower() in ["bye", "quit", "exit"]: # 我们输入bye/quit/exit等均退出客户端
        print("\033[0mBYE BYE!")
        break

    history.append({"role": "user", "content": userinput})
    print("\033[92;1m")

</code></pre> 
<p>我们新打开一个 Terminal 终端窗口，同样切换目标到 GGUF 文件目录，并且激活 Python 虚拟环境：</p> 
<pre><code># 切换到存放GGUF文件目录
cd ~/PythonSpace/Llama3-8B/

# 切换Python 3.12.2版本
conda activate PY3.12.2

# 激活虚拟环境（之前已经创建）
source ./venv/bin/activate

# 启动客户端
python client.py

</code></pre> 
<p>首次打开客户端，因为有第一个默认的<strong>自我介绍</strong>问题，稍微有点忙，但是可以看到，<strong>Llama 模型</strong>按照我们的要求完成了自我介绍，总体还不赖：</p> 
<p><img src="https://images2.imgbox.com/0b/54/A1HX5hvZ_o.jpg" alt="Llama模型自我介绍"></p> 
<p>接着，我给<strong>Llama 模型</strong>来了一个类<strong>哲学</strong>的问题：<code>请你用中文问答：人为什么要不断追求卓越？</code></p> 
<p><strong>Llama 模型</strong>的回答非常精简，且只有 5 句话，所谓言简意赅：</p> 
<p><img src="https://images2.imgbox.com/05/47/BpM86Ner_o.jpg" alt="Llama回答：人为什么要不断追求卓越？"></p> 
<p>上图中，红色为我的输入，绿色为模型的答复，超级赞！</p> 
<h3><a id="_163"></a>禅定：总结</h3> 
<p>现在我们的<strong>Llama 模型</strong>聊天机器人已准备就绪，我们想问什么就可以问什么，尽情享受吧。</p> 
<p>当然，我们废了大半天劲，如果只是和模型简单的聊聊天，那就有点可惜了，或者说如果要人工输入，那我们本地部署的意义就不大。</p> 
<p>假设能够通过程序的方式，自动调用本地部署的<strong>Llama 模型</strong>是不是可以提供我们工作效率；<strong>Llama 模型</strong>的能力非常广泛，可用于多种场景和任务：</p> 
<ol><li><strong>自然语言生成</strong>：Llama 3 能够生成连贯、高质量的文本，包括文章、故事、诗歌等创意写作，以及邮件、报告等实用文体。</li><li><strong>对话系统</strong>：模型可以用于构建聊天机器人或 AI 助手，进行自然、流畅的对话交流，提供信息查询、娱乐互动等功能。</li><li><strong>代码生成</strong>：它在代码生成任务上表现优异，能够根据描述自动生成或补全代码片段，辅助程序员提高开发效率。</li><li><strong>翻译</strong>：Llama 3 支持跨语言应用，可以实现文本的自动翻译，覆盖多种语言对。</li><li><strong>文本摘要</strong>：能够自动生成文章、报告的摘要，提取关键信息，帮助用户快速浏览大量内容。</li><li><strong>情感分析和文本分类</strong>：可以识别文本中的情绪倾向、主题分类，为企业提供市场洞察、客户服务优化等。</li><li><strong>问答系统</strong>：高效准确地回答用户提出的问题，无论是常识性问题还是专业领域的复杂询问。</li><li><strong>个性化推荐</strong>：基于用户的历史交互和偏好，生成个性化的推荐内容，如新闻、商品、音乐等。</li><li><strong>文本生成图像描述</strong>：结合多模态技术，Llama 3 可以根据文本描述生成图像内容的描述，助力图像生成或图像检索。</li><li><strong>法律文档处理</strong>：微调后的模型可以用于法律文档的理解、分析，比如合同审查、案例研究等。</li></ol> 
<p>可能大家都想学习AI大模型技术，也想通过这项技能真正达到升职加薪，就业或是副业的目的，但是不知道该如何开始学习，因为网上的资料太多太杂乱了，如果不能系统的学习就相当于是白学。为了让大家少走弯路，少碰壁，这里我直接把全套AI技术和大模型入门资料、操作变现玩法都打包整理好，希望能够真正帮助到大家。</p> 
<p>👉AI大模型学习路线汇总👈<br> 大模型学习路线图，整体分为7个大的阶段：（全套教程文末领取哈）<br> <img src="https://images2.imgbox.com/01/fd/fqlywr59_o.png" alt="在这里插入图片描述"></p> 
<p>第一阶段： 从大模型系统设计入手，讲解大模型的主要方法；</p> 
<p>第二阶段： 在通过大模型提示词工程从Prompts角度入手更好发挥模型的作用；</p> 
<p>第三阶段： 大模型平台应用开发借助阿里云PAI平台构建电商领域虚拟试衣系统；</p> 
<p>第四阶段： 大模型知识库应用开发以LangChain框架为例，构建物流行业咨询智能问答系统；</p> 
<p>第五阶段： 大模型微调开发借助以大健康、新零售、新媒体领域构建适合当前领域大模型；</p> 
<p>第六阶段： 以SD多模态大模型为主，搭建了文生图小程序案例；</p> 
<p>第七阶段： 以大模型平台应用与开发为主，通过星火大模型，文心大模型等成熟大模型构建大模型行业应用。</p> 
<p>👉大模型实战案例👈<br> 光学理论是没用的，要学会跟着一起做，要动手实操，才能将自己的所学运用到实际当中去，这时候可以搞点实战案例来学习。<br> <img src="https://images2.imgbox.com/74/d9/Hx80lMS1_o.jpg" alt="在这里插入图片描述"></p> 
<p>👉大模型视频和PDF合集👈<br> 观看零基础学习书籍和视频，看书籍和视频学习是最快捷也是最有效果的方式，跟着视频中老师的思路，从基础到深入，还是很容易入门的。<br> <img src="https://images2.imgbox.com/83/e7/PAurf2Lb_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/2f/94/EX2Sr9wt_o.png" alt="在这里插入图片描述"></p> 
<p>👉学会后的收获：👈<br> • 基于大模型全栈工程实现（前端、后端、产品经理、设计、数据分析等），通过这门课可获得不同能力；</p> 
<p>• 能够利用大模型解决相关实际项目需求： 大数据时代，越来越多的企业和机构需要处理海量数据，利用大模型技术可以更好地处理这些数据，提高数据分析和决策的准确性。因此，掌握大模型应用开发技能，可以让程序员更好地应对实际项目需求；</p> 
<p>• 基于大模型和企业数据AI应用开发，实现大模型理论、掌握GPU算力、硬件、LangChain开发框架和项目实战技能， 学会Fine-tuning垂直训练大模型（数据准备、数据蒸馏、大模型部署）一站式掌握；</p> 
<p>• 能够完成时下热门大模型垂直领域模型训练能力，提高程序员的编码能力： 大模型应用开发需要掌握机器学习算法、深度学习框架等技术，这些技术的掌握可以提高程序员的编码能力和分析能力，让程序员更加熟练地编写高质量的代码。</p> 
<p>👉获取方式：<br> 😝有需要的小伙伴，可以保存图片到wx扫描二v码免费领取【保证100%免费】🆓</p> 
<p><img src="https://images2.imgbox.com/73/1d/WzXemAkq_o.jpg" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c193db648f05cf870e5954ad599695b9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【C&#43;&#43;】类和对象（下）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9c7f388025994309b74aa96c00a12214/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【MySQL】事务一</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>