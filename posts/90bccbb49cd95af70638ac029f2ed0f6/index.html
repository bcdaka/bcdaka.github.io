<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【大模型】Ollama&#43;open-webui/Anything LLM部署本地大模型构建RAG个人知识库教程（Mac） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/90bccbb49cd95af70638ac029f2ed0f6/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【大模型】Ollama&#43;open-webui/Anything LLM部署本地大模型构建RAG个人知识库教程（Mac）">
  <meta property="og:description" content="目录
一、Ollama是什么？
二、如何在Mac上安装Ollama
1. 准备工作
2. 下载并安装Ollama
3. 运行Ollama
4. 安装和配置大型语言模型
5. 使用Ollama
三、安装open-webui
1. 准备工作
2. Open WebUI ⭐的主要特点
3. Docker安装OpenWebUI，拉去太慢可以使用手动安装
4. 配置本地大模型LLaMA2-7B
5. 验证配置
四、使用Ollama &#43; AnythingLLM构建类ChatGPT本地问答机器人系
学习目标
1. 下载AnythingLLM
2. 安装AnythingLLM
3. 配置AnythingLLM
3.1 选择LLM、嵌入模型和向量数据库
3.2 设置环境变量（如果需要）
3.3 权限管理（如果需要）
4. 构建知识库
5. 开始使用
6. 自定义集成（如果需要）
7. 监控和反馈
8. 注意事项
9. 额外信息
参考文章
一、Ollama是什么？ Ollama是一个功能强大的开源框架，旨在简化在Docker容器中部署和管理大型语言模型（LLM）的过程。以下是关于Ollama的详细介绍：
定义与功能： Ollama是一个开源的大型语言模型服务工具，它帮助用户快速在本地运行大模型。通过简单的安装指令，用户可以执行一条命令就在本地运行开源大型语言模型，如Llama 2。Ollama极大地简化了在Docker容器内部署和管理LLM的过程，使得用户能够快速地在本地运行大型语言模型。特点与优势： 功能齐全：Ollama将模型权重、配置和数据捆绑到一个包中，定义成Modelfile。优化设置与配置：它优化了设置和配置细节，包括GPU使用情况。易用性：用户无需深入了解复杂的部署和管理流程，只需简单的安装和配置即可使用。支持热加载：用户无需重新启动即可切换不同的模型。支持的平台与模型： Ollama支持在Mac和Linux平台上运行。它支持运行多种开源大型语言模型，如Llama 2。API与界面： Ollama提供了类似OpenAI的API接口和聊天界面，方便用户部署和使用最新版本的GPT模型。安装与部署： Ollama的安装过程被极大地简化，并提供了多种选择，包括Docker镜像。 综上所述，Ollama是一个为在本地运行大型语言模型而设计的强大、易用、功能齐全的开源框架。它通过优化设置和配置，简化了在Docker容器中部署和管理LLM的过程，使得用户能够快速、方便地在本地运行大型语言模型。
二、如何在Mac上安装Ollama 在Mac上安装Ollama的步骤如下，结合了参考文章中的信息，并进行了适当的总结和归纳：
1. 准备工作 确认系统兼容性：Ollama支持在Mac上运行，但请确保您的Mac满足运行大型语言模型所需的最低系统要求。检查存储空间：安装和运行Ollama以及大型语言模型可能需要较大的磁盘空间。请确保您的Mac有足够的存储空间。 2.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-07T08:35:59+08:00">
    <meta property="article:modified_time" content="2024-06-07T08:35:59+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【大模型】Ollama&#43;open-webui/Anything LLM部署本地大模型构建RAG个人知识库教程（Mac）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81Ollama%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81Ollama%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F" rel="nofollow">一、Ollama是什么？</a></p> 
<p id="%E4%BA%8C%E3%80%81%E5%A6%82%E4%BD%95%E5%9C%A8Mac%E4%B8%8A%E5%AE%89%E8%A3%85Ollama-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E5%A6%82%E4%BD%95%E5%9C%A8Mac%E4%B8%8A%E5%AE%89%E8%A3%85Ollama" rel="nofollow">二、如何在Mac上安装Ollama</a></p> 
<p id="1.%20%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C-toc" style="margin-left:80px;"><a href="#1.%20%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C" rel="nofollow">1. 准备工作</a></p> 
<p id="2.%20%E4%B8%8B%E8%BD%BD%E5%B9%B6%E5%AE%89%E8%A3%85Ollama-toc" style="margin-left:80px;"><a href="#2.%20%E4%B8%8B%E8%BD%BD%E5%B9%B6%E5%AE%89%E8%A3%85Ollama" rel="nofollow">2. 下载并安装Ollama</a></p> 
<p id="3.%20%E8%BF%90%E8%A1%8COllama-toc" style="margin-left:80px;"><a href="#3.%20%E8%BF%90%E8%A1%8COllama" rel="nofollow">3. 运行Ollama</a></p> 
<p id="4.%20%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-toc" style="margin-left:80px;"><a href="#4.%20%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B" rel="nofollow">4. 安装和配置大型语言模型</a></p> 
<p id="5.%20%E4%BD%BF%E7%94%A8Ollama-toc" style="margin-left:80px;"><a href="#5.%20%E4%BD%BF%E7%94%A8Ollama" rel="nofollow">5. 使用Ollama</a></p> 
<p id="%E4%B8%89%E3%80%81%E5%AE%89%E8%A3%85open-webui-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81%E5%AE%89%E8%A3%85open-webui" rel="nofollow">三、安装open-webui</a></p> 
<p id="1.%20%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C-toc" style="margin-left:80px;"><a href="#1.%20%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C" rel="nofollow">1. 准备工作</a></p> 
<p id="2.%20Open%20WebUI%20%E2%AD%90%E7%9A%84%E4%B8%BB%E8%A6%81%E7%89%B9%E7%82%B9-toc" style="margin-left:80px;"><a href="#2.%20Open%20WebUI%20%E2%AD%90%E7%9A%84%E4%B8%BB%E8%A6%81%E7%89%B9%E7%82%B9" rel="nofollow">2. Open WebUI ⭐的主要特点</a></p> 
<p id="3.%20Docker%E5%AE%89%E8%A3%85OpenWebUI%EF%BC%8C%E6%8B%89%E5%8E%BB%E5%A4%AA%E6%85%A2%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85-toc" style="margin-left:80px;"><a href="#3.%20Docker%E5%AE%89%E8%A3%85OpenWebUI%EF%BC%8C%E6%8B%89%E5%8E%BB%E5%A4%AA%E6%85%A2%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85" rel="nofollow">3. Docker安装OpenWebUI，拉去太慢可以使用手动安装</a></p> 
<p id="4.%20%E9%85%8D%E7%BD%AE%E6%9C%AC%E5%9C%B0%E5%A4%A7%E6%A8%A1%E5%9E%8BLLaMA2-7B-toc" style="margin-left:80px;"><a href="#4.%20%E9%85%8D%E7%BD%AE%E6%9C%AC%E5%9C%B0%E5%A4%A7%E6%A8%A1%E5%9E%8BLLaMA2-7B" rel="nofollow">4. 配置本地大模型LLaMA2-7B</a></p> 
<p id="5.%20%E9%AA%8C%E8%AF%81%E9%85%8D%E7%BD%AE-toc" style="margin-left:80px;"><a href="#5.%20%E9%AA%8C%E8%AF%81%E9%85%8D%E7%BD%AE" rel="nofollow">5. 验证配置</a></p> 
<p id="%E5%9B%9B%E3%80%81%E4%BD%BF%E7%94%A8Ollama%20%2B%20AnythingLLM%E6%9E%84%E5%BB%BA%E7%B1%BBChatGPT%E6%9C%AC%E5%9C%B0%E9%97%AE%E7%AD%94%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%B3%BB-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81%E4%BD%BF%E7%94%A8Ollama%20%2B%20AnythingLLM%E6%9E%84%E5%BB%BA%E7%B1%BBChatGPT%E6%9C%AC%E5%9C%B0%E9%97%AE%E7%AD%94%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%B3%BB" rel="nofollow">四、使用Ollama + AnythingLLM构建类ChatGPT本地问答机器人系</a></p> 
<p id="%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87-toc" style="margin-left:80px;"><a href="#%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87" rel="nofollow">学习目标</a></p> 
<p id="1.%20%E4%B8%8B%E8%BD%BDAnythingLLM-toc" style="margin-left:80px;"><a href="#1.%20%E4%B8%8B%E8%BD%BDAnythingLLM" rel="nofollow">1. 下载AnythingLLM</a></p> 
<p id="2.%20%E5%AE%89%E8%A3%85AnythingLLM-toc" style="margin-left:80px;"><a href="#2.%20%E5%AE%89%E8%A3%85AnythingLLM" rel="nofollow">2. 安装AnythingLLM</a></p> 
<p id="3.%20%E9%85%8D%E7%BD%AEAnythingLLM-toc" style="margin-left:80px;"><a href="#3.%20%E9%85%8D%E7%BD%AEAnythingLLM" rel="nofollow">3. 配置AnythingLLM</a></p> 
<p id="3.1%20%E9%80%89%E6%8B%A9LLM%E3%80%81%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93-toc" style="margin-left:120px;"><a href="#3.1%20%E9%80%89%E6%8B%A9LLM%E3%80%81%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93" rel="nofollow">3.1 选择LLM、嵌入模型和向量数据库</a></p> 
<p id="3.2%20%E8%AE%BE%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%EF%BC%88%E5%A6%82%E6%9E%9C%E9%9C%80%E8%A6%81%EF%BC%89-toc" style="margin-left:120px;"><a href="#3.2%20%E8%AE%BE%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%EF%BC%88%E5%A6%82%E6%9E%9C%E9%9C%80%E8%A6%81%EF%BC%89" rel="nofollow">3.2 设置环境变量（如果需要）</a></p> 
<p id="3.3%20%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86%EF%BC%88%E5%A6%82%E6%9E%9C%E9%9C%80%E8%A6%81%EF%BC%89-toc" style="margin-left:120px;"><a href="#3.3%20%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86%EF%BC%88%E5%A6%82%E6%9E%9C%E9%9C%80%E8%A6%81%EF%BC%89" rel="nofollow">3.3 权限管理（如果需要）</a></p> 
<p id="4.%20%E6%9E%84%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93-toc" style="margin-left:80px;"><a href="#4.%20%E6%9E%84%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93" rel="nofollow">4. 构建知识库</a></p> 
<p id="5.%20%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8-toc" style="margin-left:80px;"><a href="#5.%20%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8" rel="nofollow">5. 开始使用</a></p> 
<p id="6.%20%E8%87%AA%E5%AE%9A%E4%B9%89%E9%9B%86%E6%88%90%EF%BC%88%E5%A6%82%E6%9E%9C%E9%9C%80%E8%A6%81%EF%BC%89-toc" style="margin-left:80px;"><a href="#6.%20%E8%87%AA%E5%AE%9A%E4%B9%89%E9%9B%86%E6%88%90%EF%BC%88%E5%A6%82%E6%9E%9C%E9%9C%80%E8%A6%81%EF%BC%89" rel="nofollow">6. 自定义集成（如果需要）</a></p> 
<p id="7.%20%E7%9B%91%E6%8E%A7%E5%92%8C%E5%8F%8D%E9%A6%88-toc" style="margin-left:80px;"><a href="#7.%20%E7%9B%91%E6%8E%A7%E5%92%8C%E5%8F%8D%E9%A6%88" rel="nofollow">7. 监控和反馈</a></p> 
<p id="8.%20%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-toc" style="margin-left:80px;"><a href="#8.%20%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9" rel="nofollow">8. 注意事项</a></p> 
<p id="9.%20%E9%A2%9D%E5%A4%96%E4%BF%A1%E6%81%AF-toc" style="margin-left:80px;"><a href="#9.%20%E9%A2%9D%E5%A4%96%E4%BF%A1%E6%81%AF" rel="nofollow">9. 额外信息</a></p> 
<p id="%E5%8F%82%E8%80%83%E6%96%87%E7%AB%A0-toc" style="margin-left:0px;"><a href="#%E5%8F%82%E8%80%83%E6%96%87%E7%AB%A0" rel="nofollow">参考文章</a></p> 
<hr id="hr-toc"> 
<h2>一、Ollama是什么？</h2> 
<p>Ollama是一个功能强大的开源框架，旨在简化在Docker容器中部署和管理大型语言模型（LLM）的过程。以下是关于Ollama的详细介绍：</p> 
<ol><li><strong>定义与功能</strong>： 
  <ul><li>Ollama是一个开源的大型语言模型服务工具，它帮助用户快速在本地运行大模型。</li><li>通过简单的安装指令，用户可以执行一条命令就在本地运行开源大型语言模型，如Llama 2。</li><li>Ollama极大地简化了在Docker容器内部署和管理LLM的过程，使得用户能够快速地在本地运行大型语言模型。</li></ul></li><li><strong>特点与优势</strong>： 
  <ul><li><strong>功能齐全</strong>：Ollama将模型权重、配置和数据捆绑到一个包中，定义成Modelfile。</li><li><strong>优化设置与配置</strong>：它优化了设置和配置细节，包括GPU使用情况。</li><li><strong>易用性</strong>：用户无需深入了解复杂的部署和管理流程，只需简单的安装和配置即可使用。</li><li><strong>支持热加载</strong>：用户无需重新启动即可切换不同的模型。</li></ul></li><li><strong>支持的平台与模型</strong>： 
  <ul><li>Ollama支持在Mac和Linux平台上运行。</li><li>它支持运行多种开源大型语言模型，如Llama 2。</li></ul></li><li><strong>API与界面</strong>： 
  <ul><li>Ollama提供了类似OpenAI的API接口和聊天界面，方便用户部署和使用最新版本的GPT模型。</li></ul></li><li><strong>安装与部署</strong>： 
  <ul><li>Ollama的安装过程被极大地简化，并提供了多种选择，包括Docker镜像。</li></ul></li></ol> 
<p>综上所述，Ollama是一个为在本地运行大型语言模型而设计的强大、易用、功能齐全的开源框架。它通过优化设置和配置，简化了在Docker容器中部署和管理LLM的过程，使得用户能够快速、方便地在本地运行大型语言模型。</p> 
<p><img alt="" height="374" src="https://images2.imgbox.com/c8/06/aO6KdE3q_o.png" width="299"><img alt="" height="384" src="https://images2.imgbox.com/2c/bc/329dAekO_o.png" width="307"></p> 
<h2 id="%E4%BA%8C%E3%80%81%E5%A6%82%E4%BD%95%E5%9C%A8Mac%E4%B8%8A%E5%AE%89%E8%A3%85Ollama">二、如何在Mac上安装Ollama</h2> 
<p>在Mac上安装Ollama的步骤如下，结合了参考文章中的信息，并进行了适当的总结和归纳：</p> 
<h4 id="1.%20%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C">1. 准备工作</h4> 
<ul><li><strong>确认系统兼容性</strong>：Ollama支持在Mac上运行，但请确保您的Mac满足运行大型语言模型所需的最低系统要求。</li><li><strong>检查存储空间</strong>：安装和运行Ollama以及大型语言模型可能需要较大的磁盘空间。请确保您的Mac有足够的存储空间。</li></ul> 
<h4 id="2.%20%E4%B8%8B%E8%BD%BD%E5%B9%B6%E5%AE%89%E8%A3%85Ollama">2. 下载并安装Ollama</h4> 
<ul><li><strong>访问Ollama官网</strong>：前往Ollama的官方网站（如：<a href="https://ollama.com/download" rel="nofollow" title="https://ollama.com/）">https://ollama.com/）</a>下载适用于Mac的安装包。</li><li><strong>下载安装包</strong>：在官网找到适用于Mac的下载链接，下载Ollama的安装包。</li><li><strong>安装Ollama</strong>：双击下载的安装包，按照提示完成安装过程。</li></ul> 
<h4 id="3.%20%E8%BF%90%E8%A1%8COllama">3. 运行Ollama</h4> 
<ul><li><strong>打开终端</strong>：在Mac上打开终端（Terminal）。</li><li><strong>运行命令</strong>：在终端中，输入相应的命令来启动和运行Ollama。例如，使用<code>ollama pull llama3</code>命令来拉取并安装Llama 3模型（请注意，这只是一个示例命令，具体命令可能因Ollama的版本和您的需求而有所不同）。</li><li>下载llama2地址： <a href="https://ollama.com/library/llama2" rel="nofollow" title="llama2">llama2</a></li><li>下载命令：ollama run llama2:7b 
  <ul><li><img alt="" height="570" src="https://images2.imgbox.com/d9/cf/LdtDliLT_o.png" width="1200"></li></ul></li></ul> 
<h4 id="4.%20%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">4. 安装和配置大型语言模型</h4> 
<ul><li><strong>选择模型</strong>：根据您的需求选择合适的大型语言模型。Ollama支持多种开源大型语言模型，如Llama 2、Llama 3等。</li><li><strong>安装模型</strong>：使用Ollama提供的命令来安装所选的模型。例如，使用<code>ollama pull llama3</code>命令来安装Llama 3模型。</li><li><strong>配置模型</strong>：根据模型的文档和说明，进行必要的配置和设置。这可能包括设置模型的参数、配置GPU使用情况等。</li></ul> 
<h4 id="5.%20%E4%BD%BF%E7%94%A8Ollama">5. 使用Ollama</h4> 
<ul><li><strong>启动服务</strong>：在成功安装和配置模型后，您可以使用Ollama提供的命令或API来启动和运行模型服务。</li><li><strong>访问和使用</strong>：通过Ollama提供的Web界面或API接口，您可以访问和使用已部署的大型语言模型进行各种任务，如文本生成、问答等。</li><li>API调用 
  <blockquote> 
   <p>curl http://localhost:11434/api/generate -d '{<!-- --></p> 
   <p>"model": "llama2:7B",</p> 
   <p>"prompt":"Why is the sky blue?"</p> 
   <p>}'</p> 
  </blockquote> 
  <ul><li><img alt="" height="548" src="https://images2.imgbox.com/2d/7d/hHuA2wje_o.png" width="1200"></li></ul></li><li>回答问题时CPU100%，MAC M1 8G内存 
  <ul><li><img alt="" height="196" src="https://images2.imgbox.com/10/f2/zjQ3uhw6_o.png" width="279"></li></ul></li></ul> 
<p>请注意，以上步骤和命令可能因Ollama的版本和您的具体需求而有所不同。建议您参考Ollama的官方文档和社区资源，以获取最准确和最新的安装和使用指南。</p> 
<h2 id="%E4%B8%89%E3%80%81%E5%AE%89%E8%A3%85open-webui">三、安装open-webui</h2> 
<h4>1. 准备工作</h4> 
<ol><li><strong>安装Docker环境</strong>：确保你的系统上已经安装了Docker Desktop。你可以从Docker官网下载并安装适合你操作系统的Docker Desktop版本。</li><li><strong>配置Docker以支持GPU（可选）</strong>：如果你的本地有GPU，并且希望利用GPU加速大模型效果，你需要在Docker Desktop中配置GPU支持。这通常涉及到在Docker Desktop的设置中启用GPU支持，并安装相应的驱动程序和软件。</li><li>可参考文章：<a href="https://blog.csdn.net/zqd_java/article/details/136879029" title="Macbook m1安装docker详细教程_mac m1安装docker-CSDN博客">Macbook m1安装docker详细教程_mac m1安装docker-CSDN博客</a></li></ol> 
<h4 id="2.%20Open%20WebUI%20%E2%AD%90%E7%9A%84%E4%B8%BB%E8%A6%81%E7%89%B9%E7%82%B9">2. Open WebUI ⭐的主要特点</h4> 
<p><a href="https://github.com/open-webui/open-webui/tree/main" title="GitHub - open-webui/open-webui: User-friendly WebUI for LLMs (Formerly Ollama WebUI)">GitHub - open-webui/open-webui: User-friendly WebUI for LLMs (Formerly Ollama WebUI)</a></p> 
<p><img alt="" height="1120" src="https://images2.imgbox.com/77/82/nkGXCZoT_o.png" width="1200"></p> 
<ul><li> <p>🚀 <strong>轻松设置</strong>：使用 Docker 或 Kubernetes（kubectl、kustomize 或 helm）无缝安装，提供无忧体验，同时支持标记映像和标记映像。<code>:ollama</code><code>:cuda</code></p> </li><li> <p>🤝 <strong>Ollama/OpenAI API 集成</strong>：轻松集成兼容 OpenAI 的 API，与 Ollama 模型进行多功能对话。自定义 OpenAI API URL 以链接 <strong>LMStudio、GroqCloud、Mistral、OpenRouter 等</strong>。</p> </li><li> <p>🧩 <strong>Pipelines、Open WebUI 插件支持</strong>：使用 <a href="https://github.com/open-webui/pipelines" title="Pipelines 插件框架">Pipelines 插件框架</a>将自定义逻辑和 Python 库无缝集成到 Open WebUI 中。启动您的 Pipelines 实例，将 OpenAI URL 设置为 Pipelines URL，探索无限可能。<a href="https://github.com/open-webui/pipelines/tree/main/examples" title="示例">示例</a>包括<strong>函数调用</strong>、控制访问的用户<strong>速率限制</strong>、使用 Langfuse 等工具<strong>进行使用情况监控</strong>、使用 <strong>LibreTranslate</strong> 提供多语言支持的实时翻译、<strong>有害消息过滤</strong>等等。</p> </li><li> <p>📱 <strong>响应式设计</strong>：在台式电脑、笔记本电脑和移动设备上享受无缝体验。</p> </li><li> <p>📱 <strong>适用于移动设备的渐进式 Web 应用程序 （PWA）：</strong>使用我们的 PWA 在您的移动设备上享受类似本机应用程序的体验，提供对 localhost 的离线访问和无缝的用户界面。</p> </li><li> <p>✒️🔢 <strong>完整的 Markdown 和 LaTeX 支持</strong>：通过全面的 Markdown 和 LaTeX 功能提升您的 LLM 体验，以实现丰富的交互。</p> </li><li> <p>🛠️ <strong>模型生成器</strong>：通过 Web UI 轻松创建 Ollama 模型。通过 <a href="https://openwebui.com/" rel="nofollow" title="Open WebUI 社区">Open WebUI 社区</a>集成，轻松创建和添加自定义角色/代理、自定义聊天元素和导入模型。</p> </li><li> <p>📚 <strong>本地 RAG 集成</strong>：通过突破性的检索增强生成 （RAG） 支持，深入了解聊天交互的未来。此功能将文档交互无缝集成到您的聊天体验中。您可以将文档直接加载到聊天中或将文件添加到文档库中，在查询之前使用命令轻松访问它们。<code>#</code></p> </li><li> <p>🔍 <strong>RAG 的 Web 搜索</strong>：使用 、 、 、 和 等提供程序执行 Web 搜索，并将结果直接注入到聊天体验中。<code>SearXNG</code><code>Google PSE</code><code>Brave Search</code><code>serpstack</code><code>serper</code></p> </li><li> <p>🌐 <strong>Web 浏览功能</strong>：使用后跟 URL 的命令将网站无缝集成到您的聊天体验中。此功能允许您将 Web 内容直接合并到您的对话中，从而增强交互的丰富性和深度。<code>#</code></p> </li><li> <p>🎨 <strong>图像生成集成</strong>：使用 AUTOMATIC1111 API 或 ComfyUI（本地）和 OpenAI 的 DALL-E（外部）等选项无缝整合图像生成功能，通过动态视觉内容丰富您的聊天体验。</p> </li><li> <p>⚙️ <strong>许多模型对话</strong>：毫不费力地同时与各种模型互动，利用它们的独特优势来获得最佳响应。通过并行利用各种模型来增强您的体验。</p> </li><li> <p>🔐 <strong>基于角色的访问控制 （RBAC）：</strong>确保使用受限权限进行安全访问;只有经过授权的个人才能访问您的 Ollama，并且为管理员保留独家模型创建/拉取权限。</p> </li><li> <p>🌐🌍 <strong>多语言支持</strong>：通过我们的国际化 （i18n） 支持，以您的首选语言体验 Open WebUI。加入我们，扩展我们支持的语言！我们正在积极寻找贡献者！</p> </li><li> <p>🌟 <strong>持续更新</strong>：我们致力于通过定期更新、修复和新功能来改进 Open WebUI。</p> </li></ul> 
<p><span style="color:#1f2328;"><span style="background-color:#ffffff;">想了解更多关于Open WebUI的功能吗？查看我们的 <a href="https://docs.openwebui.com/features" rel="nofollow" title="Open WebUI 文档">Open WebUI 文档</a>，了解全面概述！</span></span></p> 
<h4 id="3.%20Docker%E5%AE%89%E8%A3%85OpenWebUI%EF%BC%8C%E6%8B%89%E5%8E%BB%E5%A4%AA%E6%85%A2%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85">3. Docker安装OpenWebUI</h4> 
<p><strong>拉取Open-WebUI镜像</strong>：使用Docker命令从GitHub Container Registry拉取Open-WebUI的镜像。例如，你可以运行以下命令来拉取最新的Open-WebUI镜像：</p> 
<blockquote> 
 <p>docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</p> 
</blockquote> 
<p><img alt="" height="810" src="https://images2.imgbox.com/e6/ac/il4kmPjP_o.png" width="1200"></p> 
<p> 下载太慢，需要配置docker国内镜像仓库，参考这篇文章：</p> 
<p><a href="https://blog.csdn.net/u011308433/article/details/132058397" title="MacOS上配置docker国内镜像仓库地址_mac docker配置镜像源-CSDN博客">MacOS上配置docker国内镜像仓库地址_mac docker配置镜像源-CSDN博客</a></p> 
<p><img alt="" height="772" src="https://images2.imgbox.com/06/02/jgiZFmCu_o.png" width="1200"></p> 
<p><a href="https://cr.console.aliyun.com/undefined/instances/mirrors" rel="nofollow" title="如果163也很慢，建议配置阿里云镜像地址，需要登陆阿里云">如果163也很慢，建议配置阿里云镜像地址，需要登陆阿里云</a> 不过配置发现更慢！手动下载了</p> 
<p>最后只能用魔法解决了</p> 
<p><img alt="" height="838" src="https://images2.imgbox.com/7e/aa/5NmYu1eu_o.png" width="1200"></p> 
<h4>4. 安装完成</h4> 
<h5>4.1 本地登陆</h5> 
<p>登陆地址 <a href="http://localhost:3000/" rel="nofollow" title="http://localhost:3000/">http://localhost:3000/</a>  注册账号登陆</p> 
<p><img alt="" height="1128" src="https://images2.imgbox.com/53/d0/8cPG4N8f_o.png" width="1200"></p> 
<h5>4.2 选择模型</h5> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/91/8d/QhQkV66t_o.png" width="1200"></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/da/fb/xPzLVICa_o.png" width="1200"></p> 
<h5>4.3 聊天</h5> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/59/62/mNmGGopi_o.png" width="1200"></p> 
<h5>4.4 配置文本嵌入模型</h5> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/ee/b1/kwvyDNXx_o.png" width="1200"></p> 
<h5>4.5 上传PDF文档</h5> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/08/46/GpRWJBtb_o.png" width="1200"></p> 
<h5>4.6 关联文档，回答问题</h5> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/3a/ae/Lmzgp9gY_o.png" width="1200"></p> 
<h4>4. 配置本地大模型LLaMA2-7B</h4> 
<ol><li><strong>下载LLaMA2-7B模型</strong>：你需要从适当的来源（如Hugging Face的模型仓库）下载LLaMA2-7B模型的文件。由于模型文件可能非常大，下载可能需要一些时间。确保你有足够的存储空间来存储这些文件。</li><li><strong>配置Open-WebUI以使用LLaMA2-7B模型</strong>：Open-WebUI允许你通过配置文件或环境变量来指定要使用的模型。你需要根据你的Open-WebUI版本和配置方式，将LLaMA2-7B模型的路径或位置配置到Open-WebUI中。具体的配置方法可能因Open-WebUI版本而异，请参考Open-WebUI的官方文档或GitHub仓库中的说明进行配置。</li><li><strong>重启Open-WebUI容器</strong>：在配置完Open-WebUI以使用LLaMA2-7B模型后，你需要重启Open-WebUI容器以使配置生效。你可以使用Docker命令来停止并重新启动容器，或者如果Open-WebUI支持热重载配置，你也可以尝试重新加载配置而不必重启容器。</li></ol> 
<h4 id="5.%20%E9%AA%8C%E8%AF%81%E9%85%8D%E7%BD%AE">5. 验证配置</h4> 
<ol><li><strong>访问Open-WebUI界面</strong>：在配置完成后，你可以通过浏览器访问本地的3000端口来访问Open-WebUI的界面。在界面上，你应该能够看到已经配置好的LLaMA2-7B模型，并可以开始使用它进行对话或其他任务。 
  <ol><li><img alt="" height="1200" src="https://images2.imgbox.com/5f/92/4v2XnGx4_o.png" width="1200"></li></ol></li><li><strong>测试LLaMA2-7B模型</strong>：在Open-WebUI界面中，你可以尝试与LLaMA2-7B模型进行对话或执行其他任务来验证配置是否正确。如果一切正常，你应该能够看到LLaMA2-7B模型对你的输入做出合理的响应。</li></ol> 
<h2 id="%E5%9B%9B%E3%80%81%E4%BD%BF%E7%94%A8Ollama%20%2B%20AnythingLLM%E6%9E%84%E5%BB%BA%E7%B1%BBChatGPT%E6%9C%AC%E5%9C%B0%E9%97%AE%E7%AD%94%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%B3%BB">四、使用Ollama + AnythingLLM构建类ChatGPT本地问答机器人系</h2> 
<h4 id="%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87">学习目标</h4> 
<ul><li>使用开源软件Ollama+AnythingLLM构建本地类ChatGPT问答机器人系统</li><li>熟悉和了解基于LLM的本地RAG知识库搭建原理和逻辑，替换符合国内的LLM工具</li><li>学会安装、配置、使用问答系统，找出符合企业私有化客服（对内）的产品规划逻辑</li><li>对比和发现问题，寻找优劣点</li></ul> 
<p>当在MAC上安装AnythingLLM时，以下是更详细的步骤，结合了参考文章中的信息：</p> 
<h4 id="1.%20%E4%B8%8B%E8%BD%BDAnythingLLM">1. 下载AnythingLLM</h4> 
<ul><li>访问AnythingLLM的官方网站：<a href="https://useanything.com/download" rel="nofollow" title="Download AnythingLLM for Desktop">Download AnythingLLM for Desktop</a>（注意：链接可能随时间而变化，请以最新信息为准）。</li><li>在下载页面选择适用于MacOS的桌面版dmg文件，点击下载。</li><li><img alt="" height="268" src="https://images2.imgbox.com/57/2c/8NdB4LtN_o.png" width="328"></li></ul> 
<h4 id="2.%20%E5%AE%89%E8%A3%85AnythingLLM">2. 安装AnythingLLM</h4> 
<ul><li>下载完成后，找到下载的dmg文件，双击打开。</li><li>跟随安装向导的指示，完成AnythingLLM的安装过程。</li><li>安装完成后，打开AnythingLLM应用。初次启动可能需要一些时间进行初始化操作。</li><li><img alt="" height="1200" src="https://images2.imgbox.com/d9/86/LbrEPJka_o.png" width="1200"></li><li><img alt="" height="1200" src="https://images2.imgbox.com/6a/ce/ISsqppE1_o.png" width="1200"></li><li><img alt="" height="1200" src="https://images2.imgbox.com/42/8f/x0O61AJQ_o.png" width="1200"></li><li><img alt="" height="1200" src="https://images2.imgbox.com/2a/df/S9uE42tg_o.png" width="1200"></li><li><img alt="" height="1200" src="https://images2.imgbox.com/29/63/CMlcKoNA_o.png" width="1200"></li><li><img alt="" height="968" src="https://images2.imgbox.com/78/1f/eddXajZX_o.png" width="1200"></li></ul> 
<h4 id="3.%20%E9%85%8D%E7%BD%AEAnythingLLM">3. 配置AnythingLLM</h4> 
<h5>3.1 步骤说明</h5> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/d8/82/UJpRdQB6_o.png" width="1200"></p> 
<p><img alt="" height="1132" src="https://images2.imgbox.com/9f/a4/EwTfvuKm_o.png" width="1200"></p> 
<h5 id="3.1%20%E9%80%89%E6%8B%A9LLM%E3%80%81%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93">3.2 选择LLM、嵌入模型和向量数据库</h5> 
<ul><li>在AnythingLLM应用中，根据需求选择或下载适当的大语言模型（LLM）、嵌入模型和向量数据库。 
  <ul><li>LLM：AnythingLLM支持多种LLM，包括但不限于OpenAI的GPT系列、Gemini、Mistral等。 
    <ul><li><img alt="" height="1200" src="https://images2.imgbox.com/25/f8/SEgpcyll_o.png" width="1200"></li></ul></li><li>嵌入模型：可以选择内置的嵌入模型或下载其他模型，如OpenAI、LocalAi、Ollama等提供的嵌入模型。 
    <ul><li><a href="https://ollama.com/library/nomic-embed-text" rel="nofollow" title="nomic-embed-text">nomic-embed-text</a></li><li><img alt="" height="1200" src="https://images2.imgbox.com/0a/bb/0wGadV99_o.png" width="1200"></li><li><img alt="" height="1200" src="https://images2.imgbox.com/c4/d5/CfpxX0Bl_o.png" width="1200"></li></ul></li><li>向量数据库：默认使用内置的LanceDB，但也可以选择其他如Chroma、Milvus、Pinecone等。</li></ul></li></ul> 
<h5 id="3.2%20%E8%AE%BE%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%EF%BC%88%E5%A6%82%E6%9E%9C%E9%9C%80%E8%A6%81%EF%BC%89">3.3 设置环境变量（如果需要）</h5> 
<ul><li>根据AnythingLLM的文档说明，如有需要，设置所需的环境变量，例如<code>OLLAMA_MODELS</code>。</li></ul> 
<h5 id="3.3%20%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86%EF%BC%88%E5%A6%82%E6%9E%9C%E9%9C%80%E8%A6%81%EF%BC%89">3.4 权限管理（如果需要）</h5> 
<ul><li>如果是企业级应用，可以设置多用户并进行权限管理，确保数据的安全性。</li></ul> 
<h4 id="4.%20%E6%9E%84%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93">4. 构建知识库</h4> 
<ul><li>在AnythingLLM中，通过“选择知识”按钮上传文档或给定知识文件链接（支持PDF、TXT、DOCX等文档格式）。</li><li>将文档通过嵌入模型转化为向量，并保存到向量数据库中。这个过程可能需要一些时间，具体取决于文档的大小和系统的性能。</li><li> <img alt="" height="1200" src="https://images2.imgbox.com/19/54/IaG6gr9l_o.png" width="1200"></li><li><img alt="" height="1200" src="https://images2.imgbox.com/69/21/E3Nc74Ys_o.png" width="1200"></li><li><img alt="" height="1200" src="https://images2.imgbox.com/ff/79/rbWa1oIY_o.png" width="1200"></li><li><img alt="" height="1200" src="https://images2.imgbox.com/f1/a3/4iIi3DLd_o.png" width="1200"></li></ul> 
<h4 id="5.%20%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8">5. 开始使用</h4> 
<ul><li>配置完成后，就可以在AnythingLLM中进行基于检索增强生成（RAG）的聊天或问答了。</li><li>可以创建自己的工作区（workspace），设置不同的配置，并开始与LLM进行交互。</li><li><li><img alt="" height="1200" src="https://images2.imgbox.com/04/ce/m7Z36eI0_o.png" width="1200"></li><li><img alt="" height="1200" src="https://images2.imgbox.com/46/90/8lWhYRAH_o.png" width="1200"></li></ul> 
<h4 id="6.%20%E8%87%AA%E5%AE%9A%E4%B9%89%E9%9B%86%E6%88%90%EF%BC%88%E5%A6%82%E6%9E%9C%E9%9C%80%E8%A6%81%EF%BC%89">6. 自定义集成（如果需要）</h4> 
<ul><li>如果需要，可以使用AnythingLLM的开发者API进行自定义集成，以满足特定的业务需求。</li></ul> 
<h4 id="7.%20%E7%9B%91%E6%8E%A7%E5%92%8C%E5%8F%8D%E9%A6%88">7. 监控和反馈</h4> 
<ul><li>利用AnythingLLM的遥测功能来监控应用的使用情况。</li><li>如果遇到问题或需要改进，可以通过创建issue或PR来提供反馈。</li></ul> 
<h4 id="8.%20%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9">8. 注意事项</h4> 
<ul><li>定期检查并更新AnythingLLM和相关的模型、嵌入模型、向量数据库，以获取最佳的性能和安全性。</li><li>注意保护个人隐私和知识产权，确保上传的文档内容合法合规。</li></ul> 
<h4 id="9.%20%E9%A2%9D%E5%A4%96%E4%BF%A1%E6%81%AF">9. 额外信息</h4> 
<ul><li>AnythingLLM是一个全栈应用程序，允许用户将任何文档、资源或内容转化为任何LLM在聊天过程中可以用作参考的上下文。</li><li>该应用程序支持多种LLM、嵌入器和向量数据库，并提供了多用户支持和权限管理功能。</li><li>通过AnythingLLM，用户可以在本地或云端搭建个性化的聊天机器人系统，无需复杂设置。</li></ul> 
<h2 id="%E5%8F%82%E8%80%83%E6%96%87%E7%AB%A0">参考文章</h2> 
<p><a href="https://zhuanlan.zhihu.com/p/693666827" rel="nofollow" title="Ollama-0001-安装">Ollama-0001-安装</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/695040359" rel="nofollow" title="Ollama：本地大模型运行指南">Ollama：本地大模型运行指南</a></p> 
<p><a href="https://blog.csdn.net/spiderwower/article/details/138463635" title="ollama+open-webui，本地部署自己的大模型_ollama的webui如何部署-CSDN博客">ollama+open-webui，本地部署自己的大模型_ollama的webui如何部署-CSDN博客</a></p> 
<p><a href="https://blog.csdn.net/zqd_java/article/details/136879029" title="Macbook m1安装docker详细教程_mac m1安装docker-CSDN博客">Macbook m1安装docker详细教程_mac m1安装docker-CSDN博客</a></p> 
<p><a href="https://blog.csdn.net/u011308433/article/details/132058397" title="MacOS上配置docker国内镜像仓库地址_mac docker配置镜像源-CSDN博客">MacOS上配置docker国内镜像仓库地址_mac docker配置镜像源-CSDN博客</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/697317544" rel="nofollow" title="第九期： 使用Ollama + AnythingLLM构建类ChatGPT本地问答机器人系统 - 知乎 (zhihu.com)">第九期： 使用Ollama + AnythingLLM构建类ChatGPT本地问答机器人系统 - 知乎 (zhihu.com)</a></p> 
<p><a href="https://blog.csdn.net/yinmo_sc/article/details/138330350" title="AI小白使用Macbook Pro安装llama3与langchain初体验_mac安装llama3-CSDN博客">AI小白使用Macbook Pro安装llama3与langchain初体验_mac安装llama3-CSDN博客</a></p> 
<p><a href="https://www.bilibili.com/video/BV1Uf421o7wa" rel="nofollow" title="EP4 Ollama + AnythingLLM 解读本地文档 构建私有知识库_哔哩哔哩_bilibili">EP4 Ollama + AnythingLLM 解读本地文档 构建私有知识库_哔哩哔哩_bilibili</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/aeb72529209174e5e3fec4f04dddc52b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Flink Sql：四种Join方式详解（基于flink1.15官方文档）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8d8d709cbf30439ba274c97c314a2cc1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">告别冗长代码：Java Lambda 表达式如何简化你的编程</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>