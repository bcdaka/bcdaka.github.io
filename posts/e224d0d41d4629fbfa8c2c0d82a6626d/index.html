<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>llama factory学习笔记 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/e224d0d41d4629fbfa8c2c0d82a6626d/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="llama factory学习笔记">
  <meta property="og:description" content="模型 模型名模型大小默认模块TemplateBaichuan27B/13BW_packbaichuan2BLOOM560M/1.1B/1.7B/3B/7.1B/176Bquery_key_value-BLOOMZ560M/1.1B/1.7B/3B/7.1B/176Bquery_key_value-ChatGLM36Bquery_key_valuechatglm3DeepSeek (MoE)7B/16B/67Bq_proj,v_projdeepseekFalcon7B/40B/180Bquery_key_valuefalconGemma2B/7Bq_proj,v_projgemmaInternLM27B/20Bwqkvintern2LLaMA7B/13B/33B/65Bq_proj,v_proj-LLaMA-27B/13B/70Bq_proj,v_projllama2Mistral7Bq_proj,v_projmistralMixtral8x7Bq_proj,v_projmistralPhi-1.5/21.3B/2.7Bq_proj,v_proj-Qwen1.8B/7B/14B/72Bc_attnqwenQwen1.50.5B/1.8B/4B/7B/14B/72Bq_proj,v_projqwenXVERSE7B/13B/65Bq_proj,v_projxverseYi6B/34Bq_proj,v_projyiYuan2B/51B/102Bq_proj,v_projyuan 单 GPU 训练 [!IMPORTANT]
如果您使用多张 GPU 训练模型，请移步多 GPU 分布式训练部分。
预训练 CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \ --stage pt \ --do_train \ --model_name_or_path path_to_llama_model \ --dataset wiki_demo \ --finetuning_type lora \ --lora_target q_proj,v_proj \ --output_dir path_to_pt_checkpoint \ --overwrite_cache \ --per_device_train_batch_size 4 \ --gradient_accumulation_steps 4 \ --lr_scheduler_type cosine \ --logging_steps 10 \ --save_steps 1000 \ --learning_rate 5e-5 \ --num_train_epochs 3.0 \ --plot_loss \ --fp16 指令监督微调 CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \ --stage sft \ --do_train \ --model_name_or_path path_to_llama_model \ --dataset alpaca_gpt4_zh \ --template default \ --finetuning_type lora \ --lora_target q_proj,v_proj \ --output_dir path_to_sft_checkpoint \ --overwrite_cache \ --per_device_train_batch_size 4 \ --gradient_accumulation_steps 4 \ --lr_scheduler_type cosine \ --logging_steps 10 \ --save_steps 1000 \ --learning_rate 5e-5 \ --num_train_epochs 3.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-03-08T19:01:40+08:00">
    <meta property="article:modified_time" content="2024-03-08T19:01:40+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">llama factory学习笔记</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_0"></a>模型</h3> 
<table><thead><tr><th>模型名</th><th>模型大小</th><th>默认模块</th><th>Template</th></tr></thead><tbody><tr><td><a href="https://huggingface.co/baichuan-inc" rel="nofollow">Baichuan2</a></td><td>7B/13B</td><td>W_pack</td><td>baichuan2</td></tr><tr><td><a href="https://huggingface.co/bigscience/bloom" rel="nofollow">BLOOM</a></td><td>560M/1.1B/1.7B/3B/7.1B/176B</td><td>query_key_value</td><td>-</td></tr><tr><td><a href="https://huggingface.co/bigscience/bloomz" rel="nofollow">BLOOMZ</a></td><td>560M/1.1B/1.7B/3B/7.1B/176B</td><td>query_key_value</td><td>-</td></tr><tr><td><a href="https://huggingface.co/THUDM/chatglm3-6b" rel="nofollow">ChatGLM3</a></td><td>6B</td><td>query_key_value</td><td>chatglm3</td></tr><tr><td><a href="https://huggingface.co/deepseek-ai" rel="nofollow">DeepSeek (MoE)</a></td><td>7B/16B/67B</td><td>q_proj,v_proj</td><td>deepseek</td></tr><tr><td><a href="https://huggingface.co/tiiuae" rel="nofollow">Falcon</a></td><td>7B/40B/180B</td><td>query_key_value</td><td>falcon</td></tr><tr><td><a href="https://huggingface.co/google" rel="nofollow">Gemma</a></td><td>2B/7B</td><td>q_proj,v_proj</td><td>gemma</td></tr><tr><td><a href="https://huggingface.co/internlm" rel="nofollow">InternLM2</a></td><td>7B/20B</td><td>wqkv</td><td>intern2</td></tr><tr><td><a href="https://github.com/facebookresearch/llama">LLaMA</a></td><td>7B/13B/33B/65B</td><td>q_proj,v_proj</td><td>-</td></tr><tr><td><a href="https://huggingface.co/meta-llama" rel="nofollow">LLaMA-2</a></td><td>7B/13B/70B</td><td>q_proj,v_proj</td><td>llama2</td></tr><tr><td><a href="https://huggingface.co/mistralai" rel="nofollow">Mistral</a></td><td>7B</td><td>q_proj,v_proj</td><td>mistral</td></tr><tr><td><a href="https://huggingface.co/mistralai" rel="nofollow">Mixtral</a></td><td>8x7B</td><td>q_proj,v_proj</td><td>mistral</td></tr><tr><td><a href="https://huggingface.co/microsoft" rel="nofollow">Phi-1.5/2</a></td><td>1.3B/2.7B</td><td>q_proj,v_proj</td><td>-</td></tr><tr><td><a href="https://huggingface.co/Qwen" rel="nofollow">Qwen</a></td><td>1.8B/7B/14B/72B</td><td>c_attn</td><td>qwen</td></tr><tr><td><a href="https://huggingface.co/Qwen" rel="nofollow">Qwen1.5</a></td><td>0.5B/1.8B/4B/7B/14B/72B</td><td>q_proj,v_proj</td><td>qwen</td></tr><tr><td><a href="https://huggingface.co/xverse" rel="nofollow">XVERSE</a></td><td>7B/13B/65B</td><td>q_proj,v_proj</td><td>xverse</td></tr><tr><td><a href="https://huggingface.co/01-ai" rel="nofollow">Yi</a></td><td>6B/34B</td><td>q_proj,v_proj</td><td>yi</td></tr><tr><td><a href="https://huggingface.co/IEITYuan" rel="nofollow">Yuan</a></td><td>2B/51B/102B</td><td>q_proj,v_proj</td><td>yuan</td></tr></tbody></table> 
<h4><a id="_GPU__24"></a>单 GPU 训练</h4> 
<blockquote> 
 <p>[!IMPORTANT]<br> 如果您使用多张 GPU 训练模型，请移步<a href="#%E5%A4%9A-gpu-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83" rel="nofollow">多 GPU 分布式训练</a>部分。</p> 
</blockquote> 
<h5><a id="_29"></a>预训练</h5> 
<pre><code class="prism language-bash"><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> python src/train_bash.py <span class="token punctuation">\</span>
    <span class="token parameter variable">--stage</span> pt <span class="token punctuation">\</span>
    <span class="token parameter variable">--do_train</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--model_name_or_path</span> path_to_llama_model <span class="token punctuation">\</span>
    <span class="token parameter variable">--dataset</span> wiki_demo <span class="token punctuation">\</span>
    <span class="token parameter variable">--finetuning_type</span> lora <span class="token punctuation">\</span>
    <span class="token parameter variable">--lora_target</span> q_proj,v_proj <span class="token punctuation">\</span>
    <span class="token parameter variable">--output_dir</span> path_to_pt_checkpoint <span class="token punctuation">\</span>
    <span class="token parameter variable">--overwrite_cache</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--per_device_train_batch_size</span> <span class="token number">4</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--gradient_accumulation_steps</span> <span class="token number">4</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--lr_scheduler_type</span> cosine <span class="token punctuation">\</span>
    <span class="token parameter variable">--logging_steps</span> <span class="token number">10</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--save_steps</span> <span class="token number">1000</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--learning_rate</span> 5e-5 <span class="token punctuation">\</span>
    <span class="token parameter variable">--num_train_epochs</span> <span class="token number">3.0</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--plot_loss</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--fp16</span>
</code></pre> 
<h5><a id="_52"></a>指令监督微调</h5> 
<pre><code class="prism language-bash"><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> python src/train_bash.py <span class="token punctuation">\</span>
    <span class="token parameter variable">--stage</span> sft <span class="token punctuation">\</span>
    <span class="token parameter variable">--do_train</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--model_name_or_path</span> path_to_llama_model <span class="token punctuation">\</span>
    <span class="token parameter variable">--dataset</span> alpaca_gpt4_zh <span class="token punctuation">\</span>
    <span class="token parameter variable">--template</span> default <span class="token punctuation">\</span>
    <span class="token parameter variable">--finetuning_type</span> lora <span class="token punctuation">\</span>
    <span class="token parameter variable">--lora_target</span> q_proj,v_proj <span class="token punctuation">\</span>
    <span class="token parameter variable">--output_dir</span> path_to_sft_checkpoint <span class="token punctuation">\</span>
    <span class="token parameter variable">--overwrite_cache</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--per_device_train_batch_size</span> <span class="token number">4</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--gradient_accumulation_steps</span> <span class="token number">4</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--lr_scheduler_type</span> cosine <span class="token punctuation">\</span>
    <span class="token parameter variable">--logging_steps</span> <span class="token number">10</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--save_steps</span> <span class="token number">1000</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--learning_rate</span> 5e-5 <span class="token punctuation">\</span>
    <span class="token parameter variable">--num_train_epochs</span> <span class="token number">3.0</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--plot_loss</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--fp16</span>
</code></pre> 
<h5><a id="_76"></a>奖励模型训练</h5> 
<pre><code class="prism language-bash"><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> python src/train_bash.py <span class="token punctuation">\</span>
    <span class="token parameter variable">--stage</span> <span class="token function">rm</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--do_train</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--model_name_or_path</span> path_to_llama_model <span class="token punctuation">\</span>
    <span class="token parameter variable">--adapter_name_or_path</span> path_to_sft_checkpoint <span class="token punctuation">\</span>
    <span class="token parameter variable">--create_new_adapter</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--dataset</span> comparison_gpt4_zh <span class="token punctuation">\</span>
    <span class="token parameter variable">--template</span> default <span class="token punctuation">\</span>
    <span class="token parameter variable">--finetuning_type</span> lora <span class="token punctuation">\</span>
    <span class="token parameter variable">--lora_target</span> q_proj,v_proj <span class="token punctuation">\</span>
    <span class="token parameter variable">--output_dir</span> path_to_rm_checkpoint <span class="token punctuation">\</span>
    <span class="token parameter variable">--per_device_train_batch_size</span> <span class="token number">2</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--gradient_accumulation_steps</span> <span class="token number">4</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--lr_scheduler_type</span> cosine <span class="token punctuation">\</span>
    <span class="token parameter variable">--logging_steps</span> <span class="token number">10</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--save_steps</span> <span class="token number">1000</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--learning_rate</span> 1e-6 <span class="token punctuation">\</span>
    <span class="token parameter variable">--num_train_epochs</span> <span class="token number">1.0</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--plot_loss</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--fp16</span>
</code></pre> 
<h5><a id="PPO__101"></a>PPO 训练</h5> 
<pre><code class="prism language-bash"><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> python src/train_bash.py <span class="token punctuation">\</span>
    <span class="token parameter variable">--stage</span> ppo <span class="token punctuation">\</span>
    <span class="token parameter variable">--do_train</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--model_name_or_path</span> path_to_llama_model <span class="token punctuation">\</span>
    <span class="token parameter variable">--adapter_name_or_path</span> path_to_sft_checkpoint <span class="token punctuation">\</span>
    <span class="token parameter variable">--create_new_adapter</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--dataset</span> alpaca_gpt4_zh <span class="token punctuation">\</span>
    <span class="token parameter variable">--template</span> default <span class="token punctuation">\</span>
    <span class="token parameter variable">--finetuning_type</span> lora <span class="token punctuation">\</span>
    <span class="token parameter variable">--lora_target</span> q_proj,v_proj <span class="token punctuation">\</span>
    <span class="token parameter variable">--reward_model</span> path_to_rm_checkpoint <span class="token punctuation">\</span>
    <span class="token parameter variable">--output_dir</span> path_to_ppo_checkpoint <span class="token punctuation">\</span>
    <span class="token parameter variable">--per_device_train_batch_size</span> <span class="token number">2</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--gradient_accumulation_steps</span> <span class="token number">4</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--lr_scheduler_type</span> cosine <span class="token punctuation">\</span>
    <span class="token parameter variable">--top_k</span> <span class="token number">0</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--top_p</span> <span class="token number">0.9</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--logging_steps</span> <span class="token number">10</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--save_steps</span> <span class="token number">1000</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--learning_rate</span> 1e-5 <span class="token punctuation">\</span>
    <span class="token parameter variable">--num_train_epochs</span> <span class="token number">1.0</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--plot_loss</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--fp16</span>
</code></pre> 
<p>这些命令行参数用于在单GPU上进行不同类型的模型训练，包括预训练、指令监督微调、奖励模型训练和PPO训练。下面是对每个参数的详细解释：</p> 
<ol><li><strong>CUDA_VISIBLE_DEVICES</strong>：指定使用哪张GPU进行训练。在这里，它被设置为0，意味着将使用第一张GPU。</li><li><strong>python src/train_bash.py</strong>：这是训练脚本的路径，它包含执行训练的代码。</li><li><strong>–stage pt/sft/rm/ppo</strong>：指定训练的阶段。<code>pt</code>代表预训练，<code>sft</code>代表指令监督微调，<code>rm</code>代表奖励模型训练，<code>ppo</code>代表PPO训练。</li><li><strong>–do_train</strong>：指示脚本执行训练步骤。</li><li><strong>–model_name_or_path</strong>：指定要训练的模型的名称或路径。</li><li><strong>–dataset</strong>：指定用于训练的数据集。</li><li><strong>–finetuning_type lora</strong>：指定微调类型为LoRA，这是一种用于放大模型容量的技术。</li><li><strong>–lora_target</strong>：指定LoRA适配器的目标模块，这里是指定模型的特定层。</li><li><strong>–output_dir</strong>：指定训练输出的目录，用于保存检查点和其他相关文件。</li><li><strong>–overwrite_cache</strong>：如果缓存已存在，此选项将覆盖它。</li><li><strong>–per_device_train_batch_size</strong>：指定每个设备的训练批次大小。</li><li><strong>–gradient_accumulation_steps</strong>：指定梯度累积的步数，这可以增加批次大小而不增加内存消耗。</li><li><strong>–lr_scheduler_type cosine</strong>：指定学习率调度器的类型，这里使用余弦调度器。</li><li><strong>–logging_steps</strong>：指定记录日志的步数。</li><li><strong>–save_steps</strong>：指定保存检查点的步数。</li><li><strong>–learning_rate</strong>：指定学习率。</li><li><strong>–num_train_epochs</strong>：指定训练的epoch数量。</li><li><strong>–plot_loss</strong>：在训练过程中绘制损失图。</li><li><strong>–fp16</strong>：指示使用16位浮点数进行训练，这可以提高训练效率。</li><li><strong>–adapter_name_or_path</strong>：如果需要，指定适配器的名称或路径，用于迁移学习。</li><li><strong>–create_new_adapter</strong>：如果需要，创建一个新的适配器。</li><li><strong>–reward_model</strong>：如果正在进行PPO训练，指定奖励模型的路径。</li><li><strong>–top_k</strong>和**–top_p**：这些参数用于控制随机抽样的方式，用于生成文本。<br> 这些参数可以根据不同的模型和任务进行调整。在实际使用中，可能还需要根据具体情况添加或修改其他参数。<br> 以qwen 14B 举例子</li></ol> 
<pre><code class="prism language-bash"><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> python src/train_bash.py     
	<span class="token parameter variable">--stage</span> pt     
	<span class="token parameter variable">--do_train</span>     
	<span class="token parameter variable">--model_name_or_path</span> qwen/Qwen-14B     
	<span class="token parameter variable">--dataset</span> wiki_demo     
	<span class="token parameter variable">--finetuning_type</span> lora     
	<span class="token parameter variable">--lora_target</span> c_attn     
	<span class="token parameter variable">--output_dir</span> path_to_pt_checkpoint     
	<span class="token parameter variable">--overwrite_cache</span>     <span class="token parameter variable">--per_device_train_batch_size</span> <span class="token number">4</span>     
	<span class="token parameter variable">--gradient_accumulation_steps</span> <span class="token number">4</span>     <span class="token parameter variable">--lr_scheduler_type</span> cosine     
	<span class="token parameter variable">--logging_steps</span> <span class="token number">10</span>     <span class="token parameter variable">--save_steps</span> <span class="token number">1000</span>     <span class="token parameter variable">--learning_rate</span> 5e-5   
	<span class="token parameter variable">--num_train_epochs</span> <span class="token number">3.0</span>     <span class="token parameter variable">--plot_loss</span>     <span class="token parameter variable">--fp16</span>
</code></pre> 
<p>这里我们看到llama factory的预训练也是基于lora进行预训练的。<br> 显存占用38GB</p> 
<p>那么 接下来我们尝试多卡进行 qwen/Qwen-14B lora 预训练</p> 
<p>首先配置accelerate,输入只有accelerate config，剩下的内容都是选项。</p> 
<pre><code class="prism language-bash">accelerate config
In <span class="token function">which</span> compute environment are you running?
This machine                                                                                                                                                                                                                         
Which <span class="token builtin class-name">type</span> of machine are you using?                                                                                                                                                                                                 
multi-GPU                                                                                                                                                                                                                            
How many different machines will you use <span class="token punctuation">(</span>use <span class="token function">more</span> than <span class="token number">1</span> <span class="token keyword">for</span> multi-node training<span class="token punctuation">)</span>? <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>: <span class="token number">1</span>                                                                                                                                           
Should distributed operations be checked <span class="token keyword">while</span> running <span class="token keyword">for</span> errors? This can avoid <span class="token function">timeout</span> issues but will be slower. <span class="token punctuation">[</span>yes/NO<span class="token punctuation">]</span>: <span class="token function">yes</span>                                                                                                   
Do you wish to optimize your script with torch dynamo?<span class="token punctuation">[</span>yes/NO<span class="token punctuation">]</span>:yes                                                                                                                                                                   
Which dynamo backend would you like to use?                                                                                                                                                                                          
tensorrt                                                                                                                                                                                                                             
Do you want to customize the defaults sent to torch.compile? <span class="token punctuation">[</span>yes/NO<span class="token punctuation">]</span>:                                                                                                                                                               
Do you want to use DeepSpeed? <span class="token punctuation">[</span>yes/NO<span class="token punctuation">]</span>: NO                                                                                                                                                                                           
Do you want to use FullyShardedDataParallel? <span class="token punctuation">[</span>yes/NO<span class="token punctuation">]</span>: M^HNPO^H^H                                                                                                                                                                    
Please enter <span class="token function">yes</span> or no.                                                                                                                                                                                                              
Do you want to use FullyShardedDataParallel? <span class="token punctuation">[</span>yes/NO<span class="token punctuation">]</span>: NO                                                                                                                                                                            
Do you want to use Megatron-LM ? <span class="token punctuation">[</span>yes/NO<span class="token punctuation">]</span>: <span class="token function">yes</span>                                                                                                                                                                                       
What is the Tensor Parallelism degree/size? <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>:1                                                                                                                                                                                    
What is the Pipeline Parallelism degree/size? <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>:1                                                                                                                                                                                  
Do you want to <span class="token builtin class-name">enable</span> selective activation recomputation? <span class="token punctuation">[</span>YES/no<span class="token punctuation">]</span>: <span class="token number">1</span>                                                                                                                                                                
Please enter <span class="token function">yes</span> or no.                                                                                                                                                                                                              
Do you want to <span class="token builtin class-name">enable</span> selective activation recomputation? <span class="token punctuation">[</span>YES/no<span class="token punctuation">]</span>: YES                                                                                                                                                              
Do you want to use distributed optimizer <span class="token function">which</span> shards optimizer state and gradients across data parallel ranks? <span class="token punctuation">[</span>YES/no<span class="token punctuation">]</span>: YES                                                                                                        
What is the gradient clipping value based on global L2 Norm <span class="token punctuation">(</span><span class="token number">0</span> to disable<span class="token punctuation">)</span>? <span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">]</span>: <span class="token number">1</span>

How many GPU<span class="token punctuation">(</span>s<span class="token punctuation">)</span> should be used <span class="token keyword">for</span> distributed training? <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>:3
Do you wish to use FP16 or BF16 <span class="token punctuation">(</span>mixed precision<span class="token punctuation">)</span>?
bf16                                                                                                                                                                                                                                 
accelerate configuration saved at /home/ca2/.cache/huggingface/accelerate/default_config.yaml 
</code></pre> 
<p>您已经成功地为多GPU训练环境配置了<code>accelerate</code>。以下是您提供的配置的简要概述以及每个选项的含义：</p> 
<ol><li><strong>计算环境</strong>：您正在使用本地机器，这可能意味着您将在单台物理服务器或工作站上使用多个GPU。</li><li><strong>机器类型</strong>：您正在使用多GPU机器。</li><li><strong>多机器训练</strong>：您只计划使用一台机器进行训练，这意味着您将在单节点上进行训练。</li><li><strong>分布式操作检查</strong>：您希望在运行时检查分布式操作是否有错误，这样可以避免超时问题，但可能会使训练变慢。</li><li><strong>使用torch dynamo优化</strong>：您希望使用<code>torch dynamo</code>来优化您的PyTorch代码，这可以提高性能。</li><li><strong>dynamo后端</strong>：您选择使用<code>tensorrt</code>作为后端，这通常用于生产环境，可以提供优化的代码。</li><li><strong>DeepSpeed</strong>：您不打算使用DeepSpeed，这是一个用于深度学习训练的优化库。</li><li><strong>FullyShardedDataParallel</strong>：您不打算使用<code>FullyShardedDataParallel</code>，这是一个用于数据并行的PyTorch分布式训练的库。</li><li><strong>Megatron-LM</strong>：您打算使用<code>Megatron-LM</code>，这是一个用于大规模语言模型训练的PyTorch扩展。</li><li><strong>Tensor并行度</strong>：您设置为1，这意味着您不会使用Tensor并行。</li><li><strong>流水线并行度</strong>：您设置为1，这意味着您不会使用流水线并行。</li><li><strong>选择性激活重计算</strong>：您启用了选择性激活重计算，这可以提高效率。</li><li><strong>分布式优化器</strong>：您启用了分布式优化器，这意味着优化器状态和梯度将在数据并行等级上分片。</li><li><strong>梯度裁剪</strong>：您设置了一个基于全局L2范数的梯度裁剪值。</li><li><strong>用于分布式训练的GPU数量</strong>：您指定了使用3个GPU进行分布式训练。</li><li><strong>FP16或BF16（混合精度）</strong>：您选择了BF16，这是英伟达的混合精度之一，可以提高训练性能。<br> 这些配置为您的训练环境提供了一个良好的起点，但您可能需要根据您的具体需求和硬件配置进行调整。在开始训练之前，请确保您的环境变量（如<code>CUDA_VISIBLE_DEVICES</code>）设置正确，以便<code>accelerate</code>可以识别和使用您指定的GPU。<br> 如果您遇到任何问题或需要进一步的帮助，请随时提问。祝您训练顺利！</li></ol> 
<pre><code class="prism language-bash">accelerate launch src/train_bash.py <span class="token parameter variable">--stage</span> pt     <span class="token parameter variable">--do_train</span>     <span class="token parameter variable">--model_name_or_path</span> qwen/Qwen-14B     <span class="token parameter variable">--dataset</span> wiki_demo     <span class="token parameter variable">--finetuning_type</span> lora     <span class="token parameter variable">--lora_target</span> c_attn     <span class="token parameter variable">--output_dir</span> path_to_pt_checkpoint     <span class="token parameter variable">--overwrite_cache</span>     <span class="token parameter variable">--per_device_train_batch_size</span> <span class="token number">4</span>     <span class="token parameter variable">--gradient_accumulation_steps</span> <span class="token number">4</span>     <span class="token parameter variable">--lr_scheduler_type</span> cosine     <span class="token parameter variable">--logging_steps</span> <span class="token number">10</span>     <span class="token parameter variable">--save_steps</span> <span class="token number">1000</span>     <span class="token parameter variable">--learning_rate</span> 5e-5     <span class="token parameter variable">--num_train_epochs</span> <span class="token number">3.0</span>     <span class="token parameter variable">--plot_loss</span>     <span class="token parameter variable">--fp16</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/53/12/TbC3sQwt_o.png" alt="在这里插入图片描述"><br> 成功训练</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b6911d6d1598935b02222dec5c594b28/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">阿里云DSW做AI绘画时的显卡选择A10?V100?</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f24bc49d6dc28699c5f44fce151198c3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C语言—求最大公约数（4种算法思路）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>