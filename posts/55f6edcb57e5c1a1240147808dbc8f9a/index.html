<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【NLP】Jieba中文分词 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/55f6edcb57e5c1a1240147808dbc8f9a/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【NLP】Jieba中文分词">
  <meta property="og:description" content="Jieba分词是一个用于中文文本分词的开源工具。它可以将一段连续的中文文本切分成一个一个的词语，这对于中文自然语言处理（NLP）任务如文本分类、情感分析、机器翻译等非常重要。Jieba分词具有以下特点：
支持三种分词模式： 精确模式：试图将句子最精确地切开，适合文本分析。全模式：把句子中所有的可以成词的词语都扫描出来，速度快，但不能解决歧义。搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。 支持自定义词典：用户可以通过添加自定义词典来提高分词的准确性，尤其是对一些专有名词和新词的识别。内置多种分词词典：Jieba自带了多种分词词典，能够识别大量常用词汇。 Jieba基础使用 安装 可以使用pip安装Jieba：
pip install jieba 使用示例 import jieba # 精确模式 text = &#34;我来到北京清华大学&#34; seg_list = jieba.cut(text, cut_all=False) print(&#34;精确模式: &#34; &#43; &#34;/ &#34;.join(seg_list)) # 全模式 seg_list = jieba.cut(text, cut_all=True) print(&#34;全模式: &#34; &#43; &#34;/ &#34;.join(seg_list)) # 搜索引擎模式 seg_list = jieba.cut_for_search(text) print(&#34;搜索引擎模式: &#34; &#43; &#34;/ &#34;.join(seg_list)) 输出：
精确模式: 我/ 来到/ 北京/ 清华大学 全模式: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学 搜索引擎模式: 我/ 来到/ 北京/ 清华/ 华大/ 大学/ 清华大学 通过这种方式，Jieba可以高效地将中文文本分割成词语，为后续的自然语言处理任务提供基础支持。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-20T15:55:48+08:00">
    <meta property="article:modified_time" content="2024-07-20T15:55:48+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【NLP】Jieba中文分词</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>Jieba分词是一个用于中文文本分词的开源工具。它可以将一段连续的中文文本切分成一个一个的词语，这对于中文自然语言处理（NLP）任务如文本分类、情感分析、机器翻译等非常重要。Jieba分词具有以下特点：</p> 
 <ol><li><strong>支持三种分词模式</strong>： 
   <ul><li><strong>精确模式</strong>：试图将句子最精确地切开，适合文本分析。</li><li><strong>全模式</strong>：把句子中所有的可以成词的词语都扫描出来，速度快，但不能解决歧义。</li><li><strong>搜索引擎模式</strong>：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。</li></ul> </li><li><strong>支持自定义词典</strong>：用户可以通过添加自定义词典来提高分词的准确性，尤其是对一些专有名词和新词的识别。</li><li><strong>内置多种分词词典</strong>：Jieba自带了多种分词词典，能够识别大量常用词汇。</li></ol> 
</blockquote> 
<h3><a id="Jieba_9"></a>Jieba基础使用</h3> 
<h4><a id="_11"></a>安装</h4> 
<p>可以使用<code>pip</code>安装Jieba：</p> 
<pre><code>pip install jieba
</code></pre> 
<h4><a id="_19"></a>使用示例</h4> 
<pre><code class="prism language-python"><span class="token keyword">import</span> jieba

<span class="token comment"># 精确模式</span>
text <span class="token operator">=</span> <span class="token string">"我来到北京清华大学"</span>
seg_list <span class="token operator">=</span> jieba<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>text<span class="token punctuation">,</span> cut_all<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"精确模式: "</span> <span class="token operator">+</span> <span class="token string">"/ "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>seg_list<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 全模式</span>
seg_list <span class="token operator">=</span> jieba<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>text<span class="token punctuation">,</span> cut_all<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"全模式: "</span> <span class="token operator">+</span> <span class="token string">"/ "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>seg_list<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 搜索引擎模式</span>
seg_list <span class="token operator">=</span> jieba<span class="token punctuation">.</span>cut_for_search<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"搜索引擎模式: "</span> <span class="token operator">+</span> <span class="token string">"/ "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>seg_list<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-makefile">精确模式: 我/ 来到/ 北京/ 清华大学
全模式: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学
搜索引擎模式: 我/ 来到/ 北京/ 清华/ 华大/ 大学/ 清华大学
</code></pre> 
<p>通过这种方式，Jieba可以高效地将中文文本分割成词语，为后续的自然语言处理任务提供基础支持。</p> 
<h3><a id="_48"></a>自定义词典</h3> 
<p>Jieba分词支持自定义词典功能，允许用户添加特定领域或特定应用场景的词汇，以提高分词的准确性和识别率。以下是关于自定义词典功能的详细介绍：</p> 
<h4><a id="_52"></a>自定义词典的使用方法</h4> 
<ol><li> <p><strong>创建自定义词典文件</strong>： 自定义词典是一个纯文本文件，每行一个词汇，格式如下：</p> <pre><code>词语 词频 词性
</code></pre> 
  <ul><li>词语：需要添加的词汇。</li><li>词频：可选，词频越高，分词时越倾向于将此词作为一个整体分出。默认值是1。</li><li>词性：可选，词语的词性标注。</li></ul> <p>例如：</p> <pre><code>自然语言处理 3 n
机器学习 5 n
深度学习 2 n
</code></pre> </li><li> <p><strong>加载自定义词典</strong>： 使用<code>jieba.load_userdict</code>方法加载自定义词典：</p> <pre><code class="prism language-python"><span class="token keyword">import</span> jieba

jieba<span class="token punctuation">.</span>load_userdict<span class="token punctuation">(</span><span class="token string">'user_dict.txt'</span><span class="token punctuation">)</span>
</code></pre> </li><li> <p><strong>添加单个词汇</strong>： 除了加载整个词典文件，还可以动态添加单个词汇：</p> <pre><code class="prism language-python">jieba<span class="token punctuation">.</span>add_word<span class="token punctuation">(</span><span class="token string">'自定义词'</span><span class="token punctuation">)</span>
jieba<span class="token punctuation">.</span>add_word<span class="token punctuation">(</span><span class="token string">'深度学习'</span><span class="token punctuation">,</span> freq<span class="token operator">=</span><span class="token number">20000</span><span class="token punctuation">,</span> tag<span class="token operator">=</span><span class="token string">'n'</span><span class="token punctuation">)</span>
</code></pre> </li><li> <p><strong>删除单个词汇</strong>： 如果需要移除某个词汇，可以使用<code>jieba.del_word</code>方法：</p> <pre><code class="prism language-python">jieba<span class="token punctuation">.</span>del_word<span class="token punctuation">(</span><span class="token string">'不需要的词'</span><span class="token punctuation">)</span>
</code></pre> </li></ol> 
<h4><a id="_93"></a>自定义词典示例</h4> 
<p>假设有一个文本需要处理，并且有一些特定领域的词汇需要添加：</p> 
<h5><a id="_user_dicttxt_97"></a>自定义词典文件 (<code>user_dict.txt</code>):</h5> 
<pre><code class="prism language-python">人工智能 <span class="token number">5</span> n
区块链 <span class="token number">3</span> n
数据挖掘 <span class="token number">4</span> n
</code></pre> 
<h5><a id="_105"></a>加载自定义词典并进行分词</h5> 
<pre><code class="prism language-python"><span class="token keyword">import</span> jieba

<span class="token comment"># 加载自定义词典</span>
jieba<span class="token punctuation">.</span>load_userdict<span class="token punctuation">(</span><span class="token string">'user_dict.txt'</span><span class="token punctuation">)</span>

<span class="token comment"># 测试文本</span>
text <span class="token operator">=</span> <span class="token string">"人工智能和区块链是当今的热门技术，数据挖掘也是非常重要的技能。"</span>

<span class="token comment"># 分词</span>
seg_list <span class="token operator">=</span> jieba<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>text<span class="token punctuation">,</span> cut_all<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"精确模式: "</span> <span class="token operator">+</span> <span class="token string">"/ "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>seg_list<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="_121"></a>输出结果</h5> 
<pre><code class="prism language-makefile">精确模式: 人工智能/ 和/ 区块链/ 是/ 当今/ 的/ 热门/ 技术/ ，/ 数据挖掘/ 也是/ 非常/ 重要/ 的/ 技能/ 。
</code></pre> 
<h3><a id="_127"></a>词性划分</h3> 
<p>要查看每个词的词性，你可以使用 Jieba 的 <code>jieba.posseg</code> 模块，它提供了词性标注的功能。<code>jieba.posseg</code> 模块可以分词并同时返回词性标注。</p> 
<h4><a id="_131"></a>示例代码</h4> 
<p>以下是使用 <code>jieba.posseg</code> 模块进行分词和词性标注的示例代码：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> jieba<span class="token punctuation">.</span>posseg <span class="token keyword">as</span> pseg

<span class="token comment"># 测试文本</span>
text <span class="token operator">=</span> <span class="token string">"人工智能和区块链是当今的热门技术，数据挖掘也是非常重要的技能。"</span>

<span class="token comment"># 分词并标注词性</span>
words <span class="token operator">=</span> pseg<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>text<span class="token punctuation">)</span>

<span class="token comment"># 输出分词结果和词性</span>
<span class="token keyword">for</span> word<span class="token punctuation">,</span> flag <span class="token keyword">in</span> words<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>word<span class="token punctuation">}</span></span><span class="token string"> (</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>flag<span class="token punctuation">}</span></span><span class="token string">)'</span></span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="_149"></a>解释</h4> 
<ul><li><code>jieba.posseg.cut(text)</code>：对文本进行分词并返回一个生成器，每个生成器项是一个包含词语和词性标记的元组。</li><li><code>word</code>：表示分词结果中的词语。</li><li><code>flag</code>：表示词语的词性标记（例如，<code>n</code> 表示名词，<code>v</code> 表示动词等）。</li></ul> 
<h4><a id="_155"></a>词性标记</h4> 
<p>常见的词性标记包括：</p> 
<ul><li><code>n</code>：名词</li><li><code>v</code>：动词</li><li><code>a</code>：形容词</li><li><code>d</code>：副词</li><li><code>p</code>：介词</li><li><code>m</code>：数词</li><li><code>q</code>：量词</li></ul> 
<p>这将帮助你在分词结果中看到每个词汇的词性，从而进行更深入的文本分析。</p> 
<h3><a id="_169"></a>关键词提取</h3> 
<p>Jieba 提供了 <code>jieba.analyse</code> 模块来进行关键词提取，常用于从文本中提取出重要的词汇。</p> 
<h4><a id="_173"></a>示例代码</h4> 
<pre><code class="prism language-python"><span class="token keyword">import</span> jieba<span class="token punctuation">.</span>analyse

text <span class="token operator">=</span> <span class="token string">"人工智能是计算机科学的一个重要领域，近年来得到了广泛的应用。"</span>

<span class="token comment"># 提取关键词</span>
keywords <span class="token operator">=</span> jieba<span class="token punctuation">.</span>analyse<span class="token punctuation">.</span>extract_tags<span class="token punctuation">(</span>text<span class="token punctuation">,</span> topK<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> withWeight<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"关键词: "</span> <span class="token operator">+</span> <span class="token string">"/ "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>keywords<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li><code>topK</code>：返回前 K 个关键词。</li><li><code>withWeight</code>：是否返回词语的权重，默认为 False。</li></ul> 
<h4><a id="TFIDF__189"></a>TF-IDF 算法</h4> 
<p><code>jieba.analyse</code> 还支持 TF-IDF 算法来提取关键词。</p> 
<h5><a id="_193"></a>示例代码</h5> 
<pre><code class="prism language-python"><span class="token keyword">import</span> jieba<span class="token punctuation">.</span>analyse

text <span class="token operator">=</span> <span class="token string">"人工智能是计算机科学的一个重要领域，近年来得到了广泛的应用。"</span>

<span class="token comment"># 提取关键词及其权重</span>
keywords_with_weight <span class="token operator">=</span> jieba<span class="token punctuation">.</span>analyse<span class="token punctuation">.</span>extract_tags<span class="token punctuation">(</span>text<span class="token punctuation">,</span> topK<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> withWeight<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> word<span class="token punctuation">,</span> weight <span class="token keyword">in</span> keywords_with_weight<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>word<span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>weight<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="TextRank__207"></a>TextRank 算法</h4> 
<p><code>jieba.analyse</code> 模块也支持 TextRank 算法，这是一种图排序算法，用于提取关键词。</p> 
<h5><a id="_211"></a>示例代码</h5> 
<pre><code class="prism language-python"><span class="token keyword">import</span> jieba<span class="token punctuation">.</span>analyse

text <span class="token operator">=</span> <span class="token string">"人工智能是计算机科学的一个重要领域，近年来得到了广泛的应用。"</span>

<span class="token comment"># 使用 TextRank 算法提取关键词</span>
keywords <span class="token operator">=</span> jieba<span class="token punctuation">.</span>analyse<span class="token punctuation">.</span>textrank<span class="token punctuation">(</span>text<span class="token punctuation">,</span> topK<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> withWeight<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"关键词: "</span> <span class="token operator">+</span> <span class="token string">"/ "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>keywords<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_224"></a>工程技巧</h3> 
<h4><a id="_226"></a>位置获取</h4> 
<h5><a id="_228"></a>地名提取</h5> 
<p>要从文本中分出地名，可以使用<code>jieba</code>结合词性标注来识别地名。地名通常标记为<code>ns</code>（名词性地名）。你可以使用<code>jieba.posseg</code>模块进行词性标注，然后筛选出标记为<code>ns</code>的词汇。</p> 
<p>以下是从文本中提取地名的示例代码：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> jieba<span class="token punctuation">.</span>posseg <span class="token keyword">as</span> pseg

<span class="token comment"># 测试文本</span>
text <span class="token operator">=</span> <span class="token string">"我去了北京和上海，最近还计划去东京和纽约旅游。"</span>

<span class="token comment"># 分词并标注词性</span>
words <span class="token operator">=</span> pseg<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>text<span class="token punctuation">)</span>

<span class="token comment"># 提取地名</span>
places <span class="token operator">=</span> <span class="token punctuation">[</span>word <span class="token keyword">for</span> word<span class="token punctuation">,</span> flag <span class="token keyword">in</span> words <span class="token keyword">if</span> flag <span class="token operator">==</span> <span class="token string">'ns'</span><span class="token punctuation">]</span>
</code></pre> 
<h5><a id="_247"></a>经纬查询</h5> 
<p>尝试通过两个不同的地图服务API（腾讯地图和高德地图）来获取指定地点的经纬度坐标。</p> 
<blockquote> 
 <p><code>requests</code> 是一个用 Python 编写的第三方 HTTP 库，它使得发送 HTTP/1.1 请求变得非常简单。使用 <code>requests</code>，你可以很容易地发送各种 HTTP 请求（如 GET、POST、PUT、DELETE 等），并获取服务器响应的内容。<code>requests</code> 库的设计哲学是简洁易用，同时又不失强大和灵活性</p> 
</blockquote> 
<pre><code class="prism language-python"><span class="token keyword">import</span> requests

<span class="token keyword">def</span> <span class="token function">tencentMap</span><span class="token punctuation">(</span>location<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment">#调用腾讯api查找城市经纬度</span>
    url <span class="token operator">=</span> <span class="token string">"https://apis.map.qq.com/jsapi?"</span>  <span class="token comment"># 腾讯地图API接口</span>
    para <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"qt"</span><span class="token punctuation">:</span> <span class="token string">"geoc"</span><span class="token punctuation">,</span>
        <span class="token string">"addr"</span><span class="token punctuation">:</span> location<span class="token punctuation">,</span>  <span class="token comment"># 传入地址参数</span>
        <span class="token string">"output"</span><span class="token punctuation">:</span> <span class="token string">"jsonp"</span><span class="token punctuation">,</span>
        <span class="token string">"key"</span><span class="token punctuation">:</span> <span class="token string">"xxxxxxxxxx"</span><span class="token punctuation">,</span>  <span class="token comment"># 即腾讯地图API的key</span>
        <span class="token string">"pf"</span><span class="token punctuation">:</span> <span class="token string">"jsapi"</span><span class="token punctuation">,</span>
        <span class="token string">"ref"</span><span class="token punctuation">:</span> <span class="token string">"jsapi"</span>
    <span class="token punctuation">}</span>
    req <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span> para<span class="token punctuation">)</span><span class="token punctuation">.</span>json<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 请求数据并转为json格式</span>
    geted <span class="token operator">=</span> req<span class="token punctuation">[</span><span class="token string">"detail"</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> <span class="token builtin">float</span><span class="token punctuation">(</span>geted<span class="token punctuation">[</span><span class="token string">'pointx'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token builtin">float</span><span class="token punctuation">(</span>geted<span class="token punctuation">[</span><span class="token string">'pointy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">aliMap</span><span class="token punctuation">(</span>location<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment">#调用阿里api查找城市经纬度</span>
    para <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">'key'</span><span class="token punctuation">:</span><span class="token string">'xxxxxxxxxx'</span><span class="token punctuation">,</span>   <span class="token comment"># 高德Key</span>
        <span class="token string">'address'</span><span class="token punctuation">:</span>location<span class="token punctuation">}</span> <span class="token comment"># 地址参数</span>
    url <span class="token operator">=</span> <span class="token string">'https://restapi.amap.com/v3/geocode/geo?'</span>    <span class="token comment"># 高德地图地理编码API服务地址</span>
    result <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span>para<span class="token punctuation">)</span><span class="token punctuation">.</span>json<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># GET方式请求</span>
    lon_lat <span class="token operator">=</span> result<span class="token punctuation">[</span><span class="token string">'geocodes'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'location'</span><span class="token punctuation">]</span>
    lon<span class="token punctuation">,</span>lat <span class="token operator">=</span> <span class="token builtin">map</span><span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">,</span> lon_lat<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">','</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> lon<span class="token punctuation">,</span>lat

<span class="token keyword">def</span> <span class="token function">getLonLat</span><span class="token punctuation">(</span>location<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">try</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> tencentMap<span class="token punctuation">(</span>location<span class="token punctuation">)</span>
    <span class="token keyword">except</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> aliMap<span class="token punctuation">(</span>location<span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token boolean">None</span><span class="token punctuation">,</span><span class="token boolean">None</span>
</code></pre> 
<h4><a id="_287"></a>自定义缺省词典词性</h4> 
<p>可以通过在加载词典文件后遍历词汇并动态添加这些词汇到 Jieba 的分词库中，同时指定词性。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">add_dict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dict_path<span class="token punctuation">:</span><span class="token builtin">str</span><span class="token punctuation">,</span>tag<span class="token punctuation">:</span><span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>dict_path<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token builtin">file</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> line <span class="token keyword">in</span> <span class="token builtin">file</span><span class="token punctuation">:</span>
            word <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>
            jieba<span class="token punctuation">.</span>add_word<span class="token punctuation">(</span>word<span class="token punctuation">,</span> tag<span class="token operator">=</span>tag<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="_299"></a>找到第一个该词性的词</h4> 
<p>从一个句子中返回第一个指定词性的词，可以使用 jieba.posseg 模块进行词性标注，并在遍历分词结果时找到第一个匹配的词性。</p> 
<pre><code>import jieba.posseg as pseg

def find_first_word_of_type(text, word_type):
    # 分词并标注词性
    words = pseg.cut(text)
    
    # 遍历分词结果，寻找第一个指定词性的词
    for word, flag in words:
        if flag == word_type:
            return word
    return None
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d05a3948d4ae931624ae58d2a7066478/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">《无所不能的JavaScript · 异步编程》</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/015b874ad4d440e99e105c851b5b4683/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python实现发送邮件</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>