<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>在国产系统上部署开源大模型 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/08a5c4ec4f4a822fecacf67c6f743552/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="在国产系统上部署开源大模型">
  <meta property="og:description" content="在上一篇文章《国产系统上的 Copilot 初体验》中，我写到了 UOS AI。UOS 本身并没有提供大模型接入，目前市面上的开源大模型很多，我也具备本地部署大模型的条件，何不在 UOS 系统上部署一下大模型呢？
本地部署大模型的方法很多，一般选择 docker 容器部署，或者使用本地服务框架。这里介绍使用本地服务框架 Ollama 部署。
Ollama 大模型框架 Ollama 是一个新兴的大模型框架，旨在为机器学习和人工智能研究提供高效、灵活和可扩展的解决方案。随着深度学习模型的复杂性和规模不断增加，开发者和研究人员需要更强大的工具来处理大规模数据和复杂的模型架构。Ollama 正是在这种需求下应运而生的。
Ollama 的核心特点 高效计算：Ollama 采用先进的分布式计算技术，可以在多 GPU 、多节点环境中高效运行。这使得它能够处理大规模数据集和复杂的模型训练任务，大大缩短了训练时间。
灵活性：Ollama 支持多种深度学习框架，如 TensorFlow、PyTorch 等，开发者可以根据项目需要选择最合适的工具。同时，Ollama 还提供了丰富的 API 和库，方便用户进行自定义开发和扩展。
可扩展性：Ollama 具有强大的扩展能力，可以轻松应对模型和数据规模的增长。无论是初创公司的小型项目，还是大企业的大型应用，Ollama 都能提供稳定和高效的支持。
易用性：Ollama 注重用户体验，提供了简洁明了的用户界面和详细的文档说明。即使是没有深厚技术背景的用户，也可以快速上手，利用 Ollama 进行模型训练和部署。
Ollama 安装与运行 在 Deepin 系统下，安装 Ollama 非常简单，只需要如下命令：
$ curl -fsSL https://ollama.com/install.sh | sh &gt;&gt;&gt; Downloading ollama... ######################################################################## 100.0%-=O=# # # # ######################################################################## 100.0% &gt;&gt;&gt; Installing ollama to /usr/local/bin... 请输入密码 Ollama 默认会安装在 /usr/local/bin 目录下，安装完毕之后，可以在命令行运行 ollama，如果不知道有哪些命令，可以从 ollama help 开始：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-18T09:05:20+08:00">
    <meta property="article:modified_time" content="2024-07-18T09:05:20+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">在国产系统上部署开源大模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p>在上一篇文章《<a href="" rel="nofollow">国产系统上的 Copilot 初体验</a>》中，我写到了 UOS AI。UOS 本身并没有提供大模型接入，目前市面上的开源大模型很多，我也具备本地部署大模型的条件，何不在 UOS 系统上部署一下大模型呢？</p> 
 <p>本地部署大模型的方法很多，一般选择 docker 容器部署，或者使用本地服务框架。这里介绍使用本地服务框架 Ollama 部署。</p> 
 <h3>Ollama 大模型框架</h3> 
 <p>Ollama 是一个新兴的大模型框架，旨在为机器学习和人工智能研究提供高效、灵活和可扩展的解决方案。随着深度学习模型的复杂性和规模不断增加，开发者和研究人员需要更强大的工具来处理大规模数据和复杂的模型架构。Ollama 正是在这种需求下应运而生的。</p> 
 <h4>Ollama 的核心特点</h4> 
 <ol><li><p><strong>高效计算</strong>：Ollama 采用先进的分布式计算技术，可以在多 GPU 、多节点环境中高效运行。这使得它能够处理大规模数据集和复杂的模型训练任务，大大缩短了训练时间。</p></li><li><p><strong>灵活性</strong>：Ollama 支持多种深度学习框架，如 TensorFlow、PyTorch 等，开发者可以根据项目需要选择最合适的工具。同时，Ollama 还提供了丰富的 API 和库，方便用户进行自定义开发和扩展。</p></li><li><p><strong>可扩展性</strong>：Ollama 具有强大的扩展能力，可以轻松应对模型和数据规模的增长。无论是初创公司的小型项目，还是大企业的大型应用，Ollama 都能提供稳定和高效的支持。</p></li><li><p><strong>易用性</strong>：Ollama 注重用户体验，提供了简洁明了的用户界面和详细的文档说明。即使是没有深厚技术背景的用户，也可以快速上手，利用 Ollama 进行模型训练和部署。</p></li></ol> 
 <h3>Ollama 安装与运行</h3> 
 <p>在 Deepin 系统下，安装 Ollama 非常简单，只需要如下命令：</p> 
 <pre class="has"><code class="language-go">$ curl -fsSL https://ollama.com/install.sh | sh
&gt;&gt;&gt; Downloading ollama...
######################################################################## 100.0%-=O=#  #   #   #               ######################################################################## 100.0%
&gt;&gt;&gt; Installing ollama to /usr/local/bin...
请输入密码</code></pre> 
 <p>Ollama 默认会安装在 /usr/local/bin 目录下，安装完毕之后，可以在命令行运行 ollama，如果不知道有哪些命令，可以从 ollama help 开始：</p> 
 <pre class="has"><code class="language-go">(base) alex@alex-deepin-os:~$ ollama help
Large language model runner

Usage:
  ollama [flags]
  ollama [command]

Available Commands:
  serve       Start ollama
  create      Create a model from a Modelfile
  show        Show information for a model
  run         Run a model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models
  ps          List running models
  cp          Copy a model
  rm          Remove a model
  help        Help about any command

Flags:
  -h, --help      help for ollama
  -v, --version   Show version information

Use "ollama [command] --help" for more information about a command.</code></pre> 
 <p>可以看到，ollama 的命令行参数和 docker 有些相似。启动一个大模型非常简单，比如我想运行 gemma2 27b 参数的大模型：</p> 
 <pre class="has"><code class="language-go">(base) alex@alex-deepin-os:~$ ollama run gemma2:27b
pulling manifest 
pulling d7e4b00a7d7a...   4% ▕█                                             ▏ 655 MB/ 15 GB</code></pre> 
 <p>ollama 会自动完成模型文件的下载，容器的创建，并运行起来。ollama 本身提供了命令行交互接口。</p> 
 <pre class="has"><code class="language-go">(base) alex@alex-deepin-os:~$ ollama run gemma2
&gt;&gt;&gt; Send a message (/? for help)</code></pre> 
 <p>此外，Ollama 还提供了和 OpenAI API 兼容的接口服务，本地服务的地址为：</p> 
 <blockquote> 
   
  <p>http://127.0.0.1:11434</p> 
 </blockquote> 
 <h3>配置 UOS AI</h3> 
 <p>添加 UOS AI 账号，模型类型还是选择自定义，API Key 不用填，模型名就填写 ollama 运行的大模型名，比如 gemma2，如果运行的是 gemma2 27b 版本，就填写 gemma2:27b，API 地址填写 http://127.0.0.1:11434/v1</p> 
 <img src="https://images2.imgbox.com/9f/87/arRI36Mk_o.png" alt="97704685609ec11514fe9e9483874c1f.png"> 
 <p>配置完成后，在下拉框中选择刚配置的账号。</p> 
 <img src="https://images2.imgbox.com/ff/58/IVQIdo7P_o.png" alt="a45a2df016422e3b032e9a46418c504a.png"> 
 <p>接下来就可以愉快的和 AI 对话了。</p> 
 <img src="https://images2.imgbox.com/fb/66/BhXHGCs6_o.png" alt="a48a0a2fbcffabd85bbe01fbefea2f82.png"> 
 <p>我使用的是 Google 的 Gemma2 9b 大模型，速度还挺快。</p> 
 <p>如果你想尝试其它的大模型，可以去 ollama 的模型仓库看看。</p> 
 <blockquote> 
   
  <p>https://ollama.com/library</p> 
 </blockquote> 
 <p>里面收录了很多大模型，比如 llama3、qwen2、deepseek-coder-v2 等。</p> 
 <img src="https://images2.imgbox.com/5d/8b/yubylH5d_o.png" alt="523943598be142e21c15c742e7f7a0b4.png"> 
 <h3>小结</h3> 
 <p>写到这里，是不是感觉到在 Deepin 系统上部署大模型太简单了？是的，各种服务框架的出现，让我们不用手写代码就能部署大模型，其实本地服务框架远不止 ollama，还有 FastAPI、Streamlit 等等众多框架，甚至还有更多的高级框架，如 Dify，提供的功能更多更强。让我们慢慢探索吧！</p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/68261dd54a76426c7c61ea4ac23cd34f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Chromium CI/CD 之Jenkins实用指南2024 - 常见的构建错误（六）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8b445da4abcba3c298a2db5668579f41/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【基础算法总结】字符串</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>