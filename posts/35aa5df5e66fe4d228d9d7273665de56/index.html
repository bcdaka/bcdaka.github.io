<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Ollama管理本地开源大模型，用Open WebUI访问Ollama接口 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/35aa5df5e66fe4d228d9d7273665de56/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Ollama管理本地开源大模型，用Open WebUI访问Ollama接口">
  <meta property="og:description" content="现在开源大模型一个接一个的，而且各个都说自己的性能非常厉害，但是对于我们这些使用者，用起来就比较尴尬了。因为一个模型一个调用的方式，先得下载模型，下完模型，写加载代码，麻烦得很。
对于程序的规范来说，只要东西一多，我们就需要一个集中管理的平台，如管理python 的pip，管理js库的npm等等，而这种平台是大家争着抢着想实现的，这就有了Ollama。
Ollama Ollama 对于管理开源大模型是认真的，使用起来非常的简单，先看下如何使用：
github地址
linux 下的安装：
curl -fsSL https://ollama.com/install.sh | sh 等进度条跑完，如果不出意外的话，Ollama就已经安装成功了。
用命令试一试，比如我们想用下Dolphin Phi：
ollama run dolphin-phi 提示如下：
pulling manifest pulling 4eca7304a07a... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████▏ 1.6 GB pulling 876a8d805b60... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████▏ 10 KB pulling a47b02e00552... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████▏ 106 B pulling 8b586b146d99... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████▏ 40 B pulling f02dd72bb242... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████▏ 59 B pulling c87a43ded80f... 100% ▕███████████████████████████████████████████████████████████████████████████████████████████████▏ 555 B verifying sha256 digest writing manifest removing any unused layers success &gt;&gt;&gt; 有没有一种使用docker的感觉，Ollama 帮我们把大模型下了，还帮我们做了适配，在提示符下就可以使用了：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-03-14T11:10:50+08:00">
    <meta property="article:modified_time" content="2024-03-14T11:10:50+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Ollama管理本地开源大模型，用Open WebUI访问Ollama接口</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>现在开源大模型一个接一个的，而且各个都说自己的性能非常厉害，但是对于我们这些使用者，用起来就比较尴尬了。因为一个模型一个调用的方式，先得下载模型，下完模型，写加载代码，麻烦得很。</p> 
<p>对于程序的规范来说，只要东西一多，我们就需要一个集中管理的平台，如管理<code>python</code> 的<code>pip</code>，管理js库的<code>npm</code>等等，而这种平台是大家争着抢着想实现的，这就有了<code>Ollama</code>。</p> 
<h4><a id="Ollama_4"></a>Ollama</h4> 
<p>Ollama 对于管理开源大模型是认真的，使用起来非常的简单，先看下如何使用：</p> 
<p><a href="https://github.com/ollama/ollama">github地址</a><br> <img src="https://images2.imgbox.com/f6/bf/f29pbbDQ_o.png" alt="在这里插入图片描述"></p> 
<p><strong>linux 下的安装：</strong></p> 
<pre><code class="prism language-python">curl <span class="token operator">-</span>fsSL https<span class="token punctuation">:</span><span class="token operator">//</span>ollama<span class="token punctuation">.</span>com<span class="token operator">/</span>install<span class="token punctuation">.</span>sh <span class="token operator">|</span> sh
</code></pre> 
<p>等进度条跑完，如果不出意外的话，<code>Ollama</code>就已经安装成功了。</p> 
<p>用命令试一试，比如我们想用下Dolphin Phi：</p> 
<pre><code class="prism language-bash">ollama run dolphin-phi
</code></pre> 
<p>提示如下：</p> 
<pre><code class="prism language-bash">pulling manifest 
pulling 4eca7304a07a<span class="token punctuation">..</span>. <span class="token number">100</span>% ▕███████████████████████████████████████████████████████████████████████████████████████████████▏ <span class="token number">1.6</span> GB                         
pulling 876a8d805b60<span class="token punctuation">..</span>. <span class="token number">100</span>% ▕███████████████████████████████████████████████████████████████████████████████████████████████▏  <span class="token number">10</span> KB                         
pulling a47b02e00552<span class="token punctuation">..</span>. <span class="token number">100</span>% ▕███████████████████████████████████████████████████████████████████████████████████████████████▏  <span class="token number">106</span> B                         
pulling 8b586b146d99<span class="token punctuation">..</span>. <span class="token number">100</span>% ▕███████████████████████████████████████████████████████████████████████████████████████████████▏   <span class="token number">40</span> B                         
pulling f02dd72bb242<span class="token punctuation">..</span>. <span class="token number">100</span>% ▕███████████████████████████████████████████████████████████████████████████████████████████████▏   <span class="token number">59</span> B                         
pulling c87a43ded80f<span class="token punctuation">..</span>. <span class="token number">100</span>% ▕███████████████████████████████████████████████████████████████████████████████████████████████▏  <span class="token number">555</span> B                         
verifying sha256 digest 
writing manifest 
removing any unused layers 
success 
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>
</code></pre> 
<p>有没有一种使用<code>docker</code>的感觉，<code>Ollama</code> 帮我们把大模型下了，还帮我们做了适配，在提示符下就可以使用了：</p> 
<pre><code class="prism language-bash"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> hi
Hello<span class="token operator">!</span> How may I assist you today? Do you have any questions or need <span class="token builtin class-name">help</span> with anything specific? I'm here to provide information and support <span class="token keyword">for</span> any 
inquiries you might have.
</code></pre> 
<p>速度相当的快，但是这是控制台上测试使用的，还是不方便，<code>Ollama</code>还提供了<code>api</code>的方式，<code>Ollama</code> <strong>在安装成功以后，就 已经建立了</strong><code>11434</code>端口：</p> 
<pre><code class="prism language-bash"><span class="token function">curl</span> http://localhost:11434/api/generate <span class="token parameter variable">-d</span> <span class="token string">'{
  "model": "dolphin-phi",
  "prompt":"Why is the sky blue?"
}'</span>
</code></pre> 
<p>或者 对话模式：</p> 
<pre><code class="prism language-bash"><span class="token function">curl</span> http://localhost:11434/api/chat <span class="token parameter variable">-d</span> <span class="token string">'{
  "model": "mistral",
  "messages": [
    { "role": "user", "content": "why is the sky blue?" }
  ]
}'</span>
</code></pre> 
<p>有了<code>api</code>的方式，那想象空间就更大了，让他也想chatgpt 一样，用网页进行访问，还能选择已经安装的模型。</p> 
<p>但稍等一下，<code>Ollama</code>的默认配置是只有本地才可以访问，需要配置一下：</p> 
<p>找到配置文件：<code>/etc/systemd/system/ollama.servic</code></p> 
<p>添加 <code>Environment="OLLAMA_HOST=0.0.0.0:11434"</code></p> 
<p>结果文件如下：</p> 
<pre><code class="prism language-bash"><span class="token punctuation">..</span>.
<span class="token punctuation">[</span>Service<span class="token punctuation">]</span>
<span class="token assign-left variable">Environment</span><span class="token operator">=</span><span class="token string">"OLLAMA_HOST=0.0.0.0:11434"</span>
<span class="token assign-left variable">ExecStart</span><span class="token operator">=</span>/usr/bin/ollama serve
<span class="token punctuation">..</span>.
</code></pre> 
<p>重启：</p> 
<pre><code class="prism language-bash">systemctl daemon-reload
systemctl restart ollama

</code></pre> 
<p>这样，在外网就能够使用ip的方式进行访问。</p> 
<h4><a id="Open_WebUI_99"></a>Open WebUI</h4> 
<p>Open WebUI 适配了<code>Ollama</code>接口，提供了web的方式来访问<code>Ollama</code>的API，用法也非常的简单，用<code>docker</code>的安装方式：</p> 
<pre><code class="prism language-bash"><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token parameter variable">-p</span> <span class="token number">3000</span>:8080 --add-host<span class="token operator">=</span>host.docker.internal:host-gateway <span class="token parameter variable">-v</span> open-webui:/app/backend/data <span class="token parameter variable">--name</span> open-webui <span class="token parameter variable">--restart</span> always ghcr.io/open-webui/open-webui:main
</code></pre> 
<p>如果不是和<code>Ollama</code> 不是安装在同一台下，需要参考<a href="https://github.com/open-webui/open-webui">github</a></p> 
<p><img src="https://images2.imgbox.com/24/3e/ujpRic9p_o.png" alt="在这里插入图片描述"></p> 
<p>安装完成后，用浏览器 http://localhost:3000或者 http://ip:3000 打开Open WebUI。就可以看到Open WebUI的界面了。</p> 
<p><img src="https://images2.imgbox.com/a4/21/vaGo2eXM_o.png" alt="在这里插入图片描述"><br> 体验了下<code>Open WebUI</code>，功能还是很完善的，如果安装有问题，可以评论区见。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1ca5d92600f14246fb1ceb2006eebc85/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">mac下VSCode配置 SSH连接远程服务器&#43;免密连接</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8f288215970cdd09b178253a10e02e14/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Hive中的NVL函数与COALESCE函数</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>