<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>使用 ollama 部署最新的Llama 3 70B本地模型 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/a92c638ed9f1f8df383ed8139d1ab655/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="使用 ollama 部署最新的Llama 3 70B本地模型">
  <meta property="og:description" content="一、ollama是什么? 在本地启动并运行大型语言模型。运行Llama 3，Mistral, Gemma, Code Llama和其他模型。自定义并创建您自己的。 综合优点： 快速下载&#43;容器自动运行大模型，现在下载，马上上手。本地利用 cpu 运行大模型，本地安全可靠。ollama 命令，管理大模型相对方便，也可以多个大模型中切换。终端直接开始聊天。社区提供了支持 web api 方式访问 WebUI。 官方网站： Ollama github ： ollama/ollama: Get up and running with Llama 3, Mistral, Gemma, and other large language models. (github.com) 二、准备和安装工作 设备需求和辅助软件：
Ollama自带docker ，为了方便部署软件。良好的网络环境，大模型还是要先下载下来的。配置要求：一般来说参数越大，需要配置越好。存储空间：确保硬盘空间充足，并且学会设置环境变量来调整保存model的地址，别再下载到C盘了！ model地址和环境变量设置 win10输入path或者环境变量
增加系统环境变量
变量名（不能变）：OLLAMA_MODELS
（盘符和地址自主选择，建议先创建好文件夹）D:\Ollama
下载ollama 进入 ollama 下载页面，选择自己的系统版本的下载并安装即可。
检验是否安装成功 输入 ollama 命令，正常的得出命令行输出，表示已经安装成功，下面有 ollama 的常用命令：
ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model from a Modelfile show Show information for a model run Run a model pull Pull a model from a registry push Push a model to a registry list List models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use &#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-22T16:44:15+08:00">
    <meta property="article:modified_time" content="2024-04-22T16:44:15+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">使用 ollama 部署最新的Llama 3 70B本地模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p></p> 
<h2>一、ollama是什么?</h2> 
<p></p> 
<blockquote>
  在本地启动并运行大型语言模型。运行Llama 3，Mistral, Gemma, Code Llama和其他模型。自定义并创建您自己的。 
</blockquote> 
<blockquote>
  综合优点： 
 <ul><li>快速下载+容器自动运行大模型，现在下载，马上上手。</li><li>本地利用 cpu 运行大模型，本地安全可靠。</li><li>ollama 命令，管理大模型相对方便，也可以多个大模型中切换。</li><li>终端直接开始聊天。</li><li>社区提供了支持 web api 方式访问 WebUI。</li></ul> 
</blockquote> 
<blockquote>
  官方网站： 
 <a href="https://ollama.com/" rel="nofollow" title="Ollama">Ollama</a> 
</blockquote> 
<blockquote>
  github ：  
 <a href="https://github.com/ollama/ollama" title="ollama/ollama: Get up and running with Llama 3, Mistral, Gemma, and other large language models. (github.com)">ollama/ollama: Get up and running with Llama 3, Mistral, Gemma, and other large language models. (github.com)</a> 
</blockquote> 
<h2>二、准备和安装工作</h2> 
<p>设备需求和辅助软件：</p> 
<ul><li>Ollama自带docker ，为了方便部署软件。</li><li>良好的网络环境，大模型还是要先下载下来的。</li><li>配置要求：一般来说参数越大，需要配置越好。</li><li>存储空间：确保硬盘空间充足，并且学会设置环境变量来调整保存model的地址，别再下载到C盘了！</li></ul> 
<h3>model地址和环境变量设置</h3> 
<p>win10输入path或者环境变量</p> 
<p><img alt="" height="651" src="https://images2.imgbox.com/23/d1/2PHlcd12_o.png" width="831"></p> 
<p><img alt="" height="596" src="https://images2.imgbox.com/f2/c7/Oeq5UwaJ_o.png" width="478"></p> 
<p>增加系统环境变量</p> 
<p><img alt="" height="119" src="https://images2.imgbox.com/45/bf/8hkHZf4b_o.png" width="590"></p> 
<p><img alt="" height="183" src="https://images2.imgbox.com/5c/0e/ivKyz3V8_o.png" width="651"></p> 
<p>变量名（不能变）：OLLAMA_MODELS</p> 
<p>（盘符和地址自主选择，建议先创建好文件夹）D:\Ollama</p> 
<p></p> 
<h3>下载ollama</h3> 
<p><img alt="" height="966" src="https://images2.imgbox.com/60/02/atuRx57e_o.png" width="1200"></p> 
<p>进入 ollama 下载页面，选择自己的系统版本的下载并安装即可。</p> 
<h3>检验是否安装成功</h3> 
<p>输入 ollama 命令，正常的得出命令行输出，表示已经安装成功，下面有 ollama 的常用命令：</p> 
<pre><code class="language-bash"> ollama

Usage:
  ollama [flags]
  ollama [command]

Available Commands:
  serve       Start ollama
  create      Create a model from a Modelfile
  show        Show information for a model
  run         Run a model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models
  cp          Copy a model
  rm          Remove a model
  help        Help about any command

Flags:
  -h, --help      help for ollama
  -v, --version   Show version information

Use "ollama [command] --help" for more information about a command.
</code></pre> 
<h2>三、ollama 模型库</h2> 
<p>详见<a href="https://ollama.com/library" rel="nofollow" title="library (ollama.com)">library (ollama.com)</a>，用于搜索已收录的模型库。以下是一些流行的模型：</p> 
<p></p> 
<table><tbody><tr><th> <p>模型</p> </th><th>参数</th><th>尺寸</th><th>执行下载</th></tr><tr><td>Llama3 8B</td><td>8B</td><td>4.7GB</td><td>ollama run llama3:8b</td></tr><tr><td>Llama3 70B</td><td>70B</td><td>40GB</td><td>ollama run llama3:70b</td></tr><tr><td>Mistral</td><td>7B</td><td>26GB</td><td>ollama run mistral</td></tr><tr><td>Code Llama 7b</td><td>7B</td><td>3.8GB</td><td>ollama run codellama:7b</td></tr><tr><td>Code Llama 13b</td><td>13B</td><td>7.4GB</td><td>ollama run codellama:13b</td></tr><tr><td>Code Llama 34b</td><td>34B</td><td>19GB</td><td>ollama run codellama:34b</td></tr><tr><td>Code Llama 70b</td><td>70B</td><td>39GB</td><td>ollama run codellama:70b</td></tr><tr><td>Gemma</td><td>2B</td><td>1.7GB</td><td>ollama run gemma:2b</td></tr><tr><td>Gemma</td><td>7B</td><td>5GB</td><td>ollama run gemma:7b</td></tr></tbody></table> 
<p></p> 
<blockquote> 
 <p><span style="color:#cccccc;">Llama 3的亮点和特性如下：</span></p> 
 <ul><li><span style="color:#cccccc;">基于超过15T token训练，大小相当于Llama 2数据集的7倍还多；</span></li><li><span style="color:#cccccc;">训练效率比Llama 2高3倍；</span></li><li><span style="color:#cccccc;">支持8K长文本，改进的tokenizer具有128K token的词汇量，可实现更好的性能；</span></li><li><span style="color:#cccccc;">在大量重要基准测试中均具有最先进性能；</span></li><li><span style="color:#cccccc;">增强推理和代码能力；</span></li><li><span style="color:#cccccc;">安全性有重大突破，带有Llama Guard 2、Code Shield 和 CyberSec Eval 2的新版信任和安全工具，还能比Llama 2有更好“错误拒绝表现”。</span></li></ul> 
</blockquote> 
<p><img alt="" height="608" src="https://images2.imgbox.com/5d/a5/K3pptnDJ_o.png" width="1080"></p> 
<p><img alt="" height="549" src="https://images2.imgbox.com/e2/70/7MzQVchy_o.png" width="1080"></p> 
<h2>四、下载运行模型</h2> 
<pre><code class="language-bash">ollama run llama3:8b # 运行llama3 8b大模型
</code></pre> 
<p>直接使用 run 命令 + 模型名字就可以运行模型。</p> 
<p>如果之前没有下载过，那么会自动下载。（由于网络环境的原因，一开始可能比较慢，但会随着时间进展有所优化，如果模型比较大，要耐心等待一段时间）</p> 
<p>下载完毕之后可以在终端中直接进行对话了。</p> 
<p><img alt="" height="465" src="https://images2.imgbox.com/e7/83/xRC4duir_o.png" width="860"></p> 
<p>直接在终端中对话：用 llama3 模型写一个 nextjs组件或者 回答你的一些知识类问题</p> 
<p><u><span style="color:#333333;"><strong>根据实际测试，llama3  70B在windows10环境下，基本占用32GB内存，llama3 8B基本占用16GB内存。</strong></span></u></p> 
<p><u><span style="color:#333333;"><strong>建议至少不低于48GB或64GB内存，否则没有足够内存支持运行其他程序。</strong></span></u></p> 
<p></p> 
<p></p> 
<h3 id="第三方api调用">第三方API调用</h3> 
<p>API调用默认端口11434</p> 
<p>本地地址参考：127.0.0.1:11434</p> 
<p></p> 
<h2>五、可视化UI界面可以试试 Open WebUI</h2> 
<blockquote>
  LLMs用户友好的WebUI（以前的Ollama WebUI）: 
 <a href="https://github.com/open-webui/open-webui" title="open-webui/open-webui: User-friendly WebUI for LLMs (Formerly Ollama WebUI) (github.com)">open-webui/open-webui: User-friendly WebUI for LLMs (Formerly Ollama WebUI) (github.com)</a> 
</blockquote> 
<h4>docker 部署</h4> 
<p>如果您的计算机上有 Ollama，请使用以下命令：</p> 
<pre><code class="language-bash">docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</code></pre> 
<p>如果 Ollama 位于不同的服务器上，要连接到另一台服务器上的 Ollama，请将 <code>OLLAMA_BASE_URL</code> 更改为服务器的 URL：</p> 
<div> 
 <pre><code class="language-bash">docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</code></pre> 
 <p>安装完成后，可以通过访问Open WebUI。😄</p> 
</div> 
<pre><code class="language-html">http://localhost:3000</code></pre> 
<p>注意：端口<code>3000常</code>被其他的开发程序占用，可以根据自己的实际情况调整端口</p> 
<p><img alt="" height="966" src="https://images2.imgbox.com/7c/3c/spr8lbA7_o.png" width="1200"></p> 
<p>进入聊天界面</p> 
<p><img alt="" height="1080" src="https://images2.imgbox.com/80/77/wSuwdglT_o.png" width="1200"></p> 
<p>如果您想将本地 Docker 安装更新到最新版本，可以使用 <a href="https://containrrr.dev/watchtower/" rel="nofollow" title="Watchtower ">Watchtower </a>来完成：</p> 
<p>在命令的最后部分，如果不同，请将 <code>open-webui</code> 替换为您的容器名称。</p> 
<pre><code>docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui</code></pre> 
<div> 
 <div></div> 
 <div></div> 
 <div></div> 
</div> 
<p>以上，只是一个基本的流程，实际使用还有很多好的功能与用法。ollama 大部分代码基于 go 开发，大家可以多多探索。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/fa77503b4cd4575567054660965b0c6f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【数据结构】时间复杂度的例题</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7077a8ab97cb65fc08cefebab5766b72/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">微信小程序：基于MySQL&#43;Nodejs的汽车品牌管理系统</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>