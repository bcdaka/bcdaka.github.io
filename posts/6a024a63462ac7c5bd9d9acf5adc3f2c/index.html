<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ollama&#43;open-webui，本地部署自己的大模型 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/6a024a63462ac7c5bd9d9acf5adc3f2c/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="ollama&#43;open-webui，本地部署自己的大模型">
  <meta property="og:description" content="目录
一、效果预览
二、部署ollama
1.ollama说明
2.安装流程
2.1 windows系统
2.1.1下载安装包
2.1.2验证安装结果
2.1.3设置模型文件保存地址
2.1.4拉取大模型镜像
2.2linux系统
2.2.1下载并安装ollama
2.2.2设置环境变量
2.2.3拉取模型文件
三、部署open-webui
1.open-webui介绍
2.安装流程
2.1windows
2.1.1下载源码
2.1.2用pycharm新建一个虚拟环境​编辑​
2.1.3安装nodejs依赖
2.1.4安装python依赖
2.1.5启动webui
四、注意事项
一、效果预览 基于ollama和open-webui，本地部署类似chatgpt的网页版大模型。先看看部署效果
如果不习惯英文界面，可以设置成中文界面
二、部署ollama 1.ollama说明 ollama的官网https://www.ollama.com/
ollama的github项目地址https://github.com/ollama/ollama
ollama，是一个大模型容器管理框架，它可以帮助用户快速在本地运行大模型，类似于docker。ollama可以运行在GPU或CPU，它会自动根据你电脑硬件，选择GPU或CPU运行。
windows只支持win10或更高的版本，官方说明如下。
​
ollama 支持具有 5.0&#43; 计算能力的 Nvidia GPU，显卡计算能力如下图所示
​
ollama也支持AMD，详见官方文档说明ollama/docs/gpu.md at main · ollama/ollama (github.com)https://github.com/ollama/ollama/blob/main/docs/gpu.md
ollama管理的大模型都是经过量化后的大模型，所以它的模型文件会小很多。以ollama中llama3-8b为例，llama3-8b原始模型大小接近15G，量化模型的只有4.7G，需要的硬件条件大大降低（老破小的福音）。
​
2.安装流程 2.1 windows系统 windows只支持win10或更高的版本。
2.1.1下载安装包 windows系统，直接下载安装程序（默认是最新的版本），下载完成后，直接点击安装即可，默认安装在C:\Users\Administrator\AppData\Local\Programs\Ollama。
Download Ollama on Windowshttps://ollama.com/download​
2.1.2验证安装结果 安装完成后，电脑右下角任务栏处显示ollama图标。安装后，打开cmd命令窗口，输入ollama -v查看安装版本，如果不小心关闭了ollama程序，输入ollama serve即可重新启动ollama服务，ollama自带运行日志管理。
ollama serve默认端口为127.0.0.1:11434，这个端口在部署open-webui时需要用到，建议默认即可。如果需要修改默认端口，则需要添加一个环境变量OLLAMA_HOST=0.0.0.0:11434。
​
​
2.1.3设置模型文件保存地址 设置模型文件保存位置，打开系统环境变量配置，添加一个环境变量OLLAMA_MODELS=D:\huggingface\ollama（自己指定任意一个文件夹路径），然后点确定。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-28T10:00:16+08:00">
    <meta property="article:modified_time" content="2024-05-28T10:00:16+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ollama&#43;open-webui，本地部署自己的大模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="-toc" style="margin-left:0px;"></p> 
<p id="%E4%B8%80%E3%80%81%E6%95%88%E6%9E%9C%E9%A2%84%E8%A7%88-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E6%95%88%E6%9E%9C%E9%A2%84%E8%A7%88" rel="nofollow">一、效果预览</a></p> 
<p id="%E4%BA%8C%E3%80%81%E9%83%A8%E7%BD%B2ollama-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E9%83%A8%E7%BD%B2ollama" rel="nofollow">二、部署ollama</a></p> 
<p id="1.ollama%E8%AF%B4%E6%98%8E-toc" style="margin-left:40px;"><a href="#1.ollama%E8%AF%B4%E6%98%8E" rel="nofollow">1.ollama说明</a></p> 
<p id="2.%E5%AE%89%E8%A3%85%E6%B5%81%E7%A8%8B-toc" style="margin-left:40px;"><a href="#2.%E5%AE%89%E8%A3%85%E6%B5%81%E7%A8%8B" rel="nofollow">2.安装流程</a></p> 
<p id="2.1%20windows%E7%B3%BB%E7%BB%9F-toc" style="margin-left:80px;"><a href="#2.1%20windows%E7%B3%BB%E7%BB%9F" rel="nofollow">2.1 windows系统</a></p> 
<p id="2.1.1%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85%E5%8C%85-toc" style="margin-left:120px;"><a href="#2.1.1%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85%E5%8C%85" rel="nofollow">2.1.1下载安装包</a></p> 
<p id="2.1.2%E9%AA%8C%E8%AF%81%E5%AE%89%E8%A3%85%E7%BB%93%E6%9E%9C-toc" style="margin-left:120px;"><a href="#2.1.2%E9%AA%8C%E8%AF%81%E5%AE%89%E8%A3%85%E7%BB%93%E6%9E%9C" rel="nofollow">2.1.2验证安装结果</a></p> 
<p id="2.1.3%E8%AE%BE%E7%BD%AE%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E4%BF%9D%E5%AD%98%E5%9C%B0%E5%9D%80-toc" style="margin-left:120px;"><a href="#2.1.3%E8%AE%BE%E7%BD%AE%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E4%BF%9D%E5%AD%98%E5%9C%B0%E5%9D%80" rel="nofollow">2.1.3设置模型文件保存地址</a></p> 
<p id="2.1.4%E6%8B%89%E5%8F%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%95%9C%E5%83%8F-toc" style="margin-left:120px;"><a href="#2.1.4%E6%8B%89%E5%8F%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%95%9C%E5%83%8F" rel="nofollow">2.1.4拉取大模型镜像</a></p> 
<p id="2.2linux%E7%B3%BB%E7%BB%9F-toc" style="margin-left:80px;"><a href="#2.2linux%E7%B3%BB%E7%BB%9F" rel="nofollow">2.2linux系统</a></p> 
<p id="2.2.1%E4%B8%8B%E8%BD%BD%E5%B9%B6%E5%AE%89%E8%A3%85ollama-toc" style="margin-left:120px;"><a href="#2.2.1%E4%B8%8B%E8%BD%BD%E5%B9%B6%E5%AE%89%E8%A3%85ollama" rel="nofollow">2.2.1下载并安装ollama</a></p> 
<p id="2.2.2%E8%AE%BE%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F-toc" style="margin-left:120px;"><a href="#2.2.2%E8%AE%BE%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F" rel="nofollow">2.2.2设置环境变量</a></p> 
<p id="2.2.3%E6%8B%89%E5%8F%96%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6-toc" style="margin-left:120px;"><a href="#2.2.3%E6%8B%89%E5%8F%96%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6" rel="nofollow">2.2.3拉取模型文件</a></p> 
<p id="%E4%B8%89%E3%80%81%E9%83%A8%E7%BD%B2open-webui-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81%E9%83%A8%E7%BD%B2open-webui" rel="nofollow">三、部署open-webui</a></p> 
<p id="1.open-webui%E4%BB%8B%E7%BB%8D-toc" style="margin-left:40px;"><a href="#1.open-webui%E4%BB%8B%E7%BB%8D" rel="nofollow">1.open-webui介绍</a></p> 
<p id="2.%E5%AE%89%E8%A3%85%E6%B5%81%E7%A8%8B-toc" style="margin-left:40px;"><a href="#2.%E5%AE%89%E8%A3%85%E6%B5%81%E7%A8%8B" rel="nofollow">2.安装流程</a></p> 
<p id="2.1windows-toc" style="margin-left:80px;"><a href="#2.1windows" rel="nofollow">2.1windows</a></p> 
<p id="2.1.1%E4%B8%8B%E8%BD%BD%E6%BA%90%E7%A0%81-toc" style="margin-left:120px;"><a href="#2.1.1%E4%B8%8B%E8%BD%BD%E6%BA%90%E7%A0%81" rel="nofollow">2.1.1下载源码</a></p> 
<p id="2.1.2%E7%94%A8pycharm%E6%96%B0%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E2%80%8B%E7%BC%96%E8%BE%91%E2%80%8B-toc" style="margin-left:120px;"><a href="#2.1.2%E7%94%A8pycharm%E6%96%B0%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E2%80%8B%E7%BC%96%E8%BE%91%E2%80%8B" rel="nofollow">2.1.2用pycharm新建一个虚拟环境​编辑​</a></p> 
<p id="2.1.3%E5%AE%89%E8%A3%85nodejs%E4%BE%9D%E8%B5%96-toc" style="margin-left:120px;"><a href="#2.1.3%E5%AE%89%E8%A3%85nodejs%E4%BE%9D%E8%B5%96" rel="nofollow">2.1.3安装nodejs依赖</a></p> 
<p id="2.1.4%E5%AE%89%E8%A3%85python%E4%BE%9D%E8%B5%96-toc" style="margin-left:120px;"><a href="#2.1.4%E5%AE%89%E8%A3%85python%E4%BE%9D%E8%B5%96" rel="nofollow">2.1.4安装python依赖</a></p> 
<p id="2.1.5%E5%90%AF%E5%8A%A8webui-toc" style="margin-left:120px;"><a href="#2.1.5%E5%90%AF%E5%8A%A8webui" rel="nofollow">2.1.5启动webui</a></p> 
<p id="%E5%9B%9B%E3%80%81%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9" rel="nofollow">四、注意事项</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E4%B8%80%E3%80%81%E6%95%88%E6%9E%9C%E9%A2%84%E8%A7%88">一、效果预览</h2> 
<p>基于ollama和open-webui，本地部署类似chatgpt的网页版大模型。先看看部署效果</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/70/35/FBNTmjlJ_o.png"></p> 
<p>如果不习惯英文界面，可以设置成中文界面</p> 
<p><img alt="" height="959" src="https://images2.imgbox.com/74/76/2LTgdBVN_o.png" width="1200"></p> 
<p></p> 
<h2 id="%E4%BA%8C%E3%80%81%E9%83%A8%E7%BD%B2ollama">二、部署ollama</h2> 
<h3 id="1.ollama%E8%AF%B4%E6%98%8E">1.ollama说明</h3> 
<p>ollama的官网<a href="https://www.ollama.com/" rel="nofollow" title="https://www.ollama.com/">https://www.ollama.com/</a></p> 
<p>ollama的github项目地址<a href="https://github.com/ollama/ollama" title="https://github.com/ollama/ollama">https://github.com/ollama/ollama</a></p> 
<p>ollama，是一个大模型容器管理框架，它可以帮助用户快速在本地运行大模型，类似于docker。ollama可以运行在GPU或CPU，它会自动根据你电脑硬件，选择GPU或CPU运行。</p> 
<p>windows只支持win10或更高的版本，官方说明如下。</p> 
<p><img alt="" height="124" src="https://images2.imgbox.com/52/92/JlRvc41k_o.png" width="526">​</p> 
<p>ollama 支持具有 5.0+ 计算能力的 Nvidia GPU，显卡计算能力如下图所示</p> 
<p><img alt="" height="403" src="https://images2.imgbox.com/df/f1/mIog1add_o.png" width="406">​</p> 
<p>ollama也支持AMD，详见官方文档说明<a class="has-card" href="https://github.com/ollama/ollama/blob/main/docs/gpu.md" title="ollama/docs/gpu.md at main · ollama/ollama (github.com)"><span class="link-card-box"><span class="link-title">ollama/docs/gpu.md at main · ollama/ollama (github.com)</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/a0/08/XULPuufQ_o.png" alt="icon-default.png?t=N7T8">https://github.com/ollama/ollama/blob/main/docs/gpu.md</span></span></a></p> 
<p>ollama管理的大模型都是经过量化后的大模型，所以它的模型文件会小很多。以ollama中llama3-8b为例，llama3-8b原始模型大小接近15G，量化模型的只有4.7G，需要的硬件条件大大降低（老破小的福音）。</p> 
<p><img alt="" height="289" src="https://images2.imgbox.com/9f/bd/HyYCgfP2_o.png" width="428">​</p> 
<h3 id="2.%E5%AE%89%E8%A3%85%E6%B5%81%E7%A8%8B">2.安装流程</h3> 
<h4 id="2.1%20windows%E7%B3%BB%E7%BB%9F">2.1 windows系统</h4> 
<p>windows只支持win10或更高的版本。</p> 
<h5 id="2.1.1%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85%E5%8C%85">2.1.1下载安装包</h5> 
<p>windows系统，直接下载安装程序（默认是最新的版本），下载完成后，直接点击安装即可，默认安装在C:\Users\Administrator\AppData\Local\Programs\Ollama。<br><a class="has-card" href="https://ollama.com/download" rel="nofollow" title="Download Ollama on Windows"><span class="link-card-box"><span class="link-title">Download Ollama on Windows</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/2d/86/MxYJS5Fk_o.png" alt="icon-default.png?t=N7T8">https://ollama.com/download</span></span></a><img alt="" height="263" src="https://images2.imgbox.com/04/41/27luM1DL_o.png" width="362">​</p> 
<h5 id="2.1.2%E9%AA%8C%E8%AF%81%E5%AE%89%E8%A3%85%E7%BB%93%E6%9E%9C">2.1.2验证安装结果</h5> 
<p>安装完成后，电脑右下角任务栏处显示ollama图标。安装后，打开cmd命令窗口，输入ollama -v查看安装版本，如果不小心关闭了ollama程序，输入ollama serve即可重新启动ollama服务，ollama自带运行日志管理。</p> 
<p>ollama serve默认端口为127.0.0.1:11434，这个端口在部署open-webui时需要用到，建议默认即可。如果需要修改默认端口，则需要添加一个环境变量OLLAMA_HOST=0.0.0.0:11434。</p> 
<p><img alt="" height="50" src="https://images2.imgbox.com/bf/e9/eeN5cyNa_o.png" width="287">​</p> 
<p><img alt="" height="139" src="https://images2.imgbox.com/df/ac/pxTqrHHa_o.png" width="961">​</p> 
<h5 id="2.1.3%E8%AE%BE%E7%BD%AE%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E4%BF%9D%E5%AD%98%E5%9C%B0%E5%9D%80">2.1.3设置模型文件保存地址</h5> 
<p>设置模型文件保存位置，打开系统环境变量配置，添加一个环境变量OLLAMA_MODELS=D:\huggingface\ollama（自己指定任意一个文件夹路径），然后点确定。</p> 
<p>如果不设置环境变量，模型文件会自动保存在C盘。时间久了，C盘很容易存满。</p> 
<p><img alt="" height="181" src="https://images2.imgbox.com/4c/a4/ozRTfVy9_o.png" width="655">​</p> 
<h5 id="2.1.4%E6%8B%89%E5%8F%96%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%95%9C%E5%83%8F">2.1.4拉取大模型镜像</h5> 
<p>以下载llama3-8b为例子，打开ollama官网<a href="https://ollama.com/" rel="nofollow" title="Ollama">Ollama</a>，点击右上角Models型。</p> 
<p><img alt="" height="70" src="https://images2.imgbox.com/6b/53/5up5ubsw_o.png" width="340">​<img alt="" height="129" src="https://images2.imgbox.com/94/7e/teWHRHSx_o.png" width="319">​</p> 
<p><img alt="" height="537" src="https://images2.imgbox.com/aa/c8/5gdduDCy_o.png" width="818">​</p> 
<p>重新打开一个cmd窗口，输入ollama run llama3:8b。模型会拉取文件，然后运行模型。如果只想下载模型，则输入ollama pull llama3:8b。需要运行的时候再输入ollama run llama3:8b即可。</p> 
<p>模型下载速度很快（不需要魔法），基本上可以达到满速，如果自己电脑下载很慢，ctrl+c中止下载，再重新下载镜像。</p> 
<p><img alt="" height="188" src="https://images2.imgbox.com/f9/8a/IX8AM9GJ_o.png" width="961">​</p> 
<p>不知道为啥，文件快要下载完成的时候会突然变慢。此时ctrl+c中止后，再重新下载镜像，下载速度又快了。</p> 
<p><img alt="" height="93" src="https://images2.imgbox.com/ec/9f/G7NgukMt_o.png" width="963">​</p> 
<p>下载完成后，如果界面如下，则表示llama3:8b可以正常对话了。</p> 
<p><img alt="" height="136" src="https://images2.imgbox.com/2a/bc/PYMSdlEy_o.png" width="492">​</p> 
<p>如果出现Error: llama runner process no longer running: 3221225785。很有可能是ollama版本与操作系统不和谐的关系，默认下载的是最新版，此时需要降低ollama版本至0.1.31。博客首页有ollama0.1.31版本下载链接。<a class="link-info" href="https://pan.baidu.com/s/15iO-XhcY_AegAvG1Vd0xrA?pwd=1122" rel="nofollow" title="网盘下载链接">网盘下载链接</a></p> 
<h4 id="2.2linux%E7%B3%BB%E7%BB%9F">2.2linux系统</h4> 
<h5 id="2.2.1%E4%B8%8B%E8%BD%BD%E5%B9%B6%E5%AE%89%E8%A3%85ollama">2.2.1下载并安装ollama</h5> 
<div> 
 <pre><code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh</code></pre> 
</div> 
<h5 id="2.2.2%E8%AE%BE%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F">2.2.2设置环境变量</h5> 
<p>设置默认端口</p> 
<div> 
 <pre><code class="language-bash">echo export OLLAMA_HOST="0.0.0.0:11434"&gt;&gt;~/.bashrc</code></pre> 
</div> 
<p>设置模型默认下载地址</p> 
<div> 
 <pre><code class="language-bash">echo export OLLAMA_MODELS=/root/ollama/models&gt;&gt;~/.bashrc</code></pre> 
</div> 
<p>查看设置情况</p> 
<pre><code class="language-bash">cat ~/.bashrc</code></pre> 
<p>激活配置</p> 
<div> 
 <pre><code class="language-bash">source ~/.bashrc</code></pre> 
</div> 
<p>启动ollama 服务</p> 
<div> 
 <pre><code class="language-bash">ollama serve</code></pre> 
</div> 
<h5 id="2.2.3%E6%8B%89%E5%8F%96%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6">2.2.3拉取模型文件</h5> 
<p>重新打开一个shell终端</p> 
<div> 
 <pre><code class="language-bash">ollama run llama3:8b</code></pre> 
</div> 
<h2 id="%E4%B8%89%E3%80%81%E9%83%A8%E7%BD%B2open-webui">三、部署open-webui</h2> 
<h3 id="1.open-webui%E4%BB%8B%E7%BB%8D">1.open-webui介绍</h3> 
<p>Open WebUI 是一种可扩展、功能丰富且用户友好的自托管 WebUI，旨在完全离线运行。它支持各种LLM运行器，包括 Ollama 和 OpenAI 兼容的 API。</p> 
<p>open-webui项目地址<a href="https://github.com/open-webui/open-webui/tree/main" title="https://github.com/open-webui/open-webui/tree/main">https://github.com/open-webui/open-webui/tree/main</a></p> 
<p></p> 
<h3>2.安装流程</h3> 
<p>项目运行需要python和nodejs环境</p> 
<p><a href="https://ymjin.blog.csdn.net/article/details/121788104" rel="nofollow" title="node.js安装及环境配置超详细教程【Windows系统安装包方式】_node 0.12.6 安装 node-echarts 包-CSDN博客">node.js安装及环境配置超详细教程【Windows系统安装包方式】_node 0.12.6 安装 node-echarts 包-CSDN博客</a></p> 
<h4 id="2.1windows">2.1windows</h4> 
<h5 id="2.1.1%E4%B8%8B%E8%BD%BD%E6%BA%90%E7%A0%81">2.1.1下载源码</h5> 
<p>从github上下载open-webui源码，无法打开github网站的也可以从gitee等国内镜像上搜索open-webui。我用的是pycharm运行项目，然后复制.env.example文件为.env。</p> 
<p><img alt="" height="221" src="https://images2.imgbox.com/70/bf/8NSzjLRZ_o.png" width="339">​​</p> 
<h5 id="2.1.2%E7%94%A8pycharm%E6%96%B0%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E2%80%8B%E7%BC%96%E8%BE%91%E2%80%8B">2.1.2用pycharm新建一个虚拟环境<img alt="" height="621" src="https://images2.imgbox.com/86/f2/K4QDKk7i_o.png" width="1019">​​</h5> 
<h5 id="2.1.3%E5%AE%89%E8%A3%85nodejs%E4%BE%9D%E8%B5%96">2.1.3安装nodejs依赖</h5> 
<p>设置npm下载镜像源，提高下载速度</p> 
<pre><code class="language-bash">npm config set registry https://mirrors.huaweicloud.com/repository/npm/</code></pre> 
<div> 
 <div> 
  <pre><code class="language-bash">npm i</code></pre> 
 </div> 
</div> 
<div> 
 <div> 
  <pre><code class="language-bash">npm run build</code></pre> 
 </div> 
</div> 
<h5 id="2.1.4%E5%AE%89%E8%A3%85python%E4%BE%9D%E8%B5%96">2.1.4安装python依赖</h5> 
<div> 
 <pre><code class="language-bash">cd backend
pip install -r requirements.txt</code></pre> 
</div> 
<h5 id="2.1.5%E5%90%AF%E5%8A%A8webui">2.1.5启动webui</h5> 
<p>2.1.5.1windows系统，运行start_windows.bat。linux系统运行start.sh文件。</p> 
<p>2.1.5.2执行脚本后，会自动从<a href="https://huggingface.co/" rel="nofollow" title="huggingface.co">huggingface.co</a>下载sentence-transformers模型文件all-MiniLM-L6-v2，服务器在国外，无法下载，所以先从国内镜像网站上将模型和配置文件下载到本地。<a href="https://hf-mirror.com/" rel="nofollow" title="HF-Mirror - Huggingface 镜像站">HF-Mirror - Huggingface 镜像站</a></p> 
<p><img alt="" height="892" src="https://images2.imgbox.com/34/db/LrRQ0LBz_o.png" width="1200">​​</p> 
<p>2.1.5.3打开backend/apps/rag/utils.py文件，修改第318行。重新执行start_windows.bat或start.sh</p> 
<div> 
 <pre><code class="language-bash"># 原代码为
embedding_model_repo_path = snapshot_download(**snapshot_kwargs)

# 替换为本地文件路径
embedding_model_repo_path = r'D:\huggingface\all-MiniLM-L6-v2' 

#embedding_model_repo_path = snapshot_download(**snapshot_kwargs)</code></pre> 
</div> 
<p>2.1.6启动后的界面，网页地址在最下面，点击即可打开webui</p> 
<p><img alt="" height="411" src="https://images2.imgbox.com/d2/eb/RLNe1Ojh_o.png" width="361">​​</p> 
<h4>2.2linux</h4> 
<h5>2.2.1安装miniconda3</h5> 
<pre><code class="language-bash">
mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh
~/miniconda3/bin/conda init bash
# ~/miniconda3/bin/conda init zsh
#设置conda镜像源
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --set show_channel_urls yes
#设置pip镜像源
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</code></pre> 
<h5>2.2.2创建虚拟环境</h5> 
<pre><code class="language-bash">#1.构建虚拟环境
conda create -n open-webui python=3.8
#2.更新bashrc中的环境变量
conda init bash &amp;&amp; source /root/.bashrc
#3.切换到创建的虚拟环境
conda activate open-webui
</code></pre> 
<h5>2.2.3安装open-webui并启动</h5> 
<pre><code class="language-bash">#1.拉取源码
cd ~
git clone https://gitee.com/iafoot/open-webui.git
cd open-webui
cp -RPp .env.example .env
#2.安装前端依赖
npm i
npm run build
#3.安装后端依赖
cd backend
pip install -r requirements.txt
#4.启动后端
bash start.sh</code></pre> 
<h2 id="%E5%9B%9B%E3%80%81%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9">四、注意事项</h2> 
<p>1.打开网页后，注册的第一个用户默认为管理员账号，以后注册的账号角色均为pending（待分配权限）。</p> 
<p>2.修改权限的方法。方法一：数据存放在backend/data/webui.db中，可以用数据库可视化软件打开数据库，将账号由pending修改为user或admin。方法二：修改backend/config.py文件中第358行，DEFAULT_USER_ROLE = os.getenv("DEFAULT_USER_ROLE", "pending")，将pending修改为user（普通用户）或admin（管理员），以后每次注册的账号就不需要管理员分配权限。</p> 
<p>3.ollama run llama3无法运行，提示Error: llama runner process no longer running: 3221225785。很有可能是ollama版本与操作系统不和谐的关系，默认下载的是最新版，此时需要降低ollama版本至0.1.31。博客首页有ollama0.1.31版本下载链接。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/78228fa68256a20543844c12a11d965d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【基础算法总结】前缀和二</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/542e19d92bd73a8a7f416a78b9083337/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【AIGC调研系列】LlamaFS-使用llama3操作文件夹</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>