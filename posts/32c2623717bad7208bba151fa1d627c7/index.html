<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ELK（Elasticsearch&#43;Logstash&#43;Kibana）日志分析系统 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/32c2623717bad7208bba151fa1d627c7/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="ELK（Elasticsearch&#43;Logstash&#43;Kibana）日志分析系统">
  <meta property="og:description" content="目录
前言
一、ELK日志分析系统概述
1、三大组件工具介绍
1.1 Elasticsearch
1.1.1 Elasticsearch概念
1.1.2 关系型数据库和ElasticSearch中的对应关系
1.1.3 Elasticsearch提供的操作命令
1.2 Logstash
1.2.1 Logstash概念
1.2.2 Logstash的主要组件
1.2.3 Logstash主机分类
1.2.4 Logstash工作过程
1.3 Kiabana
2、其他可替代Logstash组件介绍
2.1 Filebeat
2.2 缓存/消息队列（redis、kafka、RabbitMQ等）
2.3 Fluentd
3、为什么要使用ELK
4、完整日志系统的基本特征
5、ELK 的工作原理
二、搭建ELK日志分析系统
1、环境部署
2、ElasticSearch 集群部署
2.1 安装 elasticsearch-rpm 包并加载系统服务
2.2 修改 elasticsearch 主配置文件
2.3 创建数据存放路径并授权
2.4 启动elasticsearch是否成功开启
2.5 查看节点信息
3、安装 Elasticsearch-head 插件
3.1 编译安装 node
3.2 安装 phantomjs（前端的框架）
3.3 安装 Elasticsearch-head 数据可视化工具
3.4 修改 Elasticsearch 主配置文件">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-18T22:42:55+08:00">
    <meta property="article:modified_time" content="2024-04-18T22:42:55+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ELK（Elasticsearch&#43;Logstash&#43;Kibana）日志分析系统</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E5%89%8D%E8%A8%80-toc" style="margin-left:0px;"><a href="#%E5%89%8D%E8%A8%80" rel="nofollow">前言</a></p> 
<p id="%E4%B8%80%E3%80%81ELK%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81ELK%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0" rel="nofollow">一、ELK日志分析系统概述</a></p> 
<p id="1%E3%80%81%E4%B8%89%E5%A4%A7%E7%BB%84%E4%BB%B6%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D-toc" style="margin-left:40px;"><a href="#1%E3%80%81%E4%B8%89%E5%A4%A7%E7%BB%84%E4%BB%B6%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D" rel="nofollow">1、三大组件工具介绍</a></p> 
<p id="1.1%C2%A0Elasticsearch-toc" style="margin-left:80px;"><a href="#1.1%C2%A0Elasticsearch" rel="nofollow">1.1 Elasticsearch</a></p> 
<p id="1.1.1%C2%A0Elasticsearch%E6%A6%82%E5%BF%B5-toc" style="margin-left:120px;"><a href="#1.1.1%C2%A0Elasticsearch%E6%A6%82%E5%BF%B5" rel="nofollow">1.1.1 Elasticsearch概念</a></p> 
<p id="1.1.2%C2%A0%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8CElasticSearch%E4%B8%AD%E7%9A%84%E5%AF%B9%E5%BA%94%E5%85%B3%E7%B3%BB-toc" style="margin-left:120px;"><a href="#1.1.2%C2%A0%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8CElasticSearch%E4%B8%AD%E7%9A%84%E5%AF%B9%E5%BA%94%E5%85%B3%E7%B3%BB" rel="nofollow">1.1.2 关系型数据库和ElasticSearch中的对应关系</a></p> 
<p id="1.1.3%C2%A0Elasticsearch%E6%8F%90%E4%BE%9B%E7%9A%84%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4-toc" style="margin-left:120px;"><a href="#1.1.3%C2%A0Elasticsearch%E6%8F%90%E4%BE%9B%E7%9A%84%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4" rel="nofollow">1.1.3 Elasticsearch提供的操作命令</a></p> 
<p id="1.2%C2%A0Logstash-toc" style="margin-left:80px;"><a href="#1.2%C2%A0Logstash" rel="nofollow">1.2 Logstash</a></p> 
<p id="1.2.1%20Logstash%E6%A6%82%E5%BF%B5-toc" style="margin-left:120px;"><a href="#1.2.1%20Logstash%E6%A6%82%E5%BF%B5" rel="nofollow">1.2.1 Logstash概念</a></p> 
<p id="1.2.2%20Logstash%E7%9A%84%E4%B8%BB%E8%A6%81%E7%BB%84%E4%BB%B6-toc" style="margin-left:120px;"><a href="#1.2.2%20Logstash%E7%9A%84%E4%B8%BB%E8%A6%81%E7%BB%84%E4%BB%B6" rel="nofollow">1.2.2 Logstash的主要组件</a></p> 
<p id="1.2.3%C2%A0Logstash%E4%B8%BB%E6%9C%BA%E5%88%86%E7%B1%BB-toc" style="margin-left:120px;"><a href="#1.2.3%C2%A0Logstash%E4%B8%BB%E6%9C%BA%E5%88%86%E7%B1%BB" rel="nofollow">1.2.3 Logstash主机分类</a></p> 
<p id="1.2.4%C2%A0Logstash%E5%B7%A5%E4%BD%9C%E8%BF%87%E7%A8%8B-toc" style="margin-left:120px;"><a href="#1.2.4%C2%A0Logstash%E5%B7%A5%E4%BD%9C%E8%BF%87%E7%A8%8B" rel="nofollow">1.2.4 Logstash工作过程</a></p> 
<p id="1.3%C2%A0Kiabana-toc" style="margin-left:80px;"><a href="#1.3%C2%A0Kiabana" rel="nofollow">1.3 Kiabana</a></p> 
<p id="2%E3%80%81%E5%85%B6%E4%BB%96%E5%8F%AF%E6%9B%BF%E4%BB%A3Logstash%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D-toc" style="margin-left:40px;"><a href="#2%E3%80%81%E5%85%B6%E4%BB%96%E5%8F%AF%E6%9B%BF%E4%BB%A3Logstash%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D" rel="nofollow">2、其他可替代Logstash组件介绍</a></p> 
<p id="2.1%C2%A0Filebeat-toc" style="margin-left:80px;"><a href="#2.1%C2%A0Filebeat" rel="nofollow">2.1 Filebeat</a></p> 
<p id="2.2%C2%A0%E7%BC%93%E5%AD%98%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%EF%BC%88redis%E3%80%81kafka%E3%80%81RabbitMQ%E7%AD%89%EF%BC%89-toc" style="margin-left:80px;"><a href="#2.2%C2%A0%E7%BC%93%E5%AD%98%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%EF%BC%88redis%E3%80%81kafka%E3%80%81RabbitMQ%E7%AD%89%EF%BC%89" rel="nofollow">2.2 缓存/消息队列（redis、kafka、RabbitMQ等）</a></p> 
<p id="2.3%C2%A0Fluentd-toc" style="margin-left:80px;"><a href="#2.3%C2%A0Fluentd" rel="nofollow">2.3 Fluentd</a></p> 
<p id="%C2%A03%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8ELK-toc" style="margin-left:40px;"><a href="#%C2%A03%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8ELK" rel="nofollow"> 3、为什么要使用ELK</a></p> 
<p id="4%E3%80%81%E5%AE%8C%E6%95%B4%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%89%B9%E5%BE%81-toc" style="margin-left:40px;"><a href="#4%E3%80%81%E5%AE%8C%E6%95%B4%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%89%B9%E5%BE%81" rel="nofollow">4、完整日志系统的基本特征</a></p> 
<p id="5%E3%80%81ELK%20%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86-toc" style="margin-left:40px;"><a href="#5%E3%80%81ELK%20%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86" rel="nofollow">5、ELK 的工作原理</a></p> 
<p id="%E4%BA%8C%E3%80%81%E6%90%AD%E5%BB%BAELK%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E6%90%AD%E5%BB%BAELK%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F" rel="nofollow">二、搭建ELK日志分析系统</a></p> 
<p id="1%E3%80%81%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2-toc" style="margin-left:40px;"><a href="#1%E3%80%81%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2" rel="nofollow">1、环境部署</a></p> 
<p id="2%E3%80%81ElasticSearch%20%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2-toc" style="margin-left:40px;"><a href="#2%E3%80%81ElasticSearch%20%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2" rel="nofollow">2、ElasticSearch 集群部署</a></p> 
<p id="2.1%C2%A0%E5%AE%89%E8%A3%85%20elasticsearch-rpm%20%E5%8C%85%E5%B9%B6%E5%8A%A0%E8%BD%BD%E7%B3%BB%E7%BB%9F%E6%9C%8D%E5%8A%A1-toc" style="margin-left:80px;"><a href="#2.1%C2%A0%E5%AE%89%E8%A3%85%20elasticsearch-rpm%20%E5%8C%85%E5%B9%B6%E5%8A%A0%E8%BD%BD%E7%B3%BB%E7%BB%9F%E6%9C%8D%E5%8A%A1" rel="nofollow">2.1 安装 elasticsearch-rpm 包并加载系统服务</a></p> 
<p id="%C2%A02.2%C2%A0%E4%BF%AE%E6%94%B9%20elasticsearch%20%E4%B8%BB%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-toc" style="margin-left:80px;"><a href="#%C2%A02.2%C2%A0%E4%BF%AE%E6%94%B9%20elasticsearch%20%E4%B8%BB%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6" rel="nofollow">2.2 修改 elasticsearch 主配置文件</a></p> 
<p id="2.3%C2%A0%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%AD%98%E6%94%BE%E8%B7%AF%E5%BE%84%E5%B9%B6%E6%8E%88%E6%9D%83-toc" style="margin-left:80px;"><a href="#2.3%C2%A0%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%AD%98%E6%94%BE%E8%B7%AF%E5%BE%84%E5%B9%B6%E6%8E%88%E6%9D%83" rel="nofollow">2.3 创建数据存放路径并授权</a></p> 
<p id="2.4%C2%A0%E5%90%AF%E5%8A%A8elasticsearch%E6%98%AF%E5%90%A6%E6%88%90%E5%8A%9F%E5%BC%80%E5%90%AF-toc" style="margin-left:80px;"><a href="#2.4%C2%A0%E5%90%AF%E5%8A%A8elasticsearch%E6%98%AF%E5%90%A6%E6%88%90%E5%8A%9F%E5%BC%80%E5%90%AF" rel="nofollow">2.4 启动elasticsearch是否成功开启</a></p> 
<p id="2.5%C2%A0%E6%9F%A5%E7%9C%8B%E8%8A%82%E7%82%B9%E4%BF%A1%E6%81%AF-toc" style="margin-left:80px;"><a href="#2.5%C2%A0%E6%9F%A5%E7%9C%8B%E8%8A%82%E7%82%B9%E4%BF%A1%E6%81%AF" rel="nofollow">2.5 查看节点信息</a></p> 
<p id="3%E3%80%81%E5%AE%89%E8%A3%85%20Elasticsearch-head%20%E6%8F%92%E4%BB%B6-toc" style="margin-left:40px;"><a href="#3%E3%80%81%E5%AE%89%E8%A3%85%20Elasticsearch-head%20%E6%8F%92%E4%BB%B6" rel="nofollow">3、安装 Elasticsearch-head 插件</a></p> 
<p id="3.1%C2%A0%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85%20node-toc" style="margin-left:80px;"><a href="#3.1%C2%A0%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85%20node" rel="nofollow">3.1 编译安装 node</a></p> 
<p id="3.2%C2%A0%E5%AE%89%E8%A3%85%20phantomjs%EF%BC%88%E5%89%8D%E7%AB%AF%E7%9A%84%E6%A1%86%E6%9E%B6%EF%BC%89-toc" style="margin-left:80px;"><a href="#3.2%C2%A0%E5%AE%89%E8%A3%85%20phantomjs%EF%BC%88%E5%89%8D%E7%AB%AF%E7%9A%84%E6%A1%86%E6%9E%B6%EF%BC%89" rel="nofollow">3.2 安装 phantomjs（前端的框架）</a></p> 
<p id="3.3%C2%A0%E5%AE%89%E8%A3%85%20Elasticsearch-head%20%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7-toc" style="margin-left:80px;"><a href="#3.3%C2%A0%E5%AE%89%E8%A3%85%20Elasticsearch-head%20%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7" rel="nofollow">3.3 安装 Elasticsearch-head 数据可视化工具</a></p> 
<p id="3.4%C2%A0%E4%BF%AE%E6%94%B9%20Elasticsearch%20%E4%B8%BB%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-toc" style="margin-left:80px;"><a href="#3.4%C2%A0%E4%BF%AE%E6%94%B9%20Elasticsearch%20%E4%B8%BB%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6" rel="nofollow">3.4 修改 Elasticsearch 主配置文件</a></p> 
<p id="3.5%C2%A0%E5%90%AF%E5%8A%A8%20elasticsearch-head%20%E6%9C%8D%E5%8A%A1-toc" style="margin-left:80px;"><a href="#3.5%C2%A0%E5%90%AF%E5%8A%A8%20elasticsearch-head%20%E6%9C%8D%E5%8A%A1" rel="nofollow">3.5 启动 elasticsearch-head 服务</a></p> 
<p id="3.6%C2%A0%E9%80%9A%E8%BF%87%20Elasticsearch-head%20%E6%9F%A5%E7%9C%8B%20Elasticsearch%20%E4%BF%A1%E6%81%AF-toc" style="margin-left:80px;"><a href="#3.6%C2%A0%E9%80%9A%E8%BF%87%20Elasticsearch-head%20%E6%9F%A5%E7%9C%8B%20Elasticsearch%20%E4%BF%A1%E6%81%AF" rel="nofollow">3.6 通过 Elasticsearch-head 查看 Elasticsearch 信息</a></p> 
<p id="3.7%C2%A0%E6%8F%92%E5%85%A5%E7%B4%A2%E5%BC%95-toc" style="margin-left:80px;"><a href="#3.7%C2%A0%E6%8F%92%E5%85%A5%E7%B4%A2%E5%BC%95" rel="nofollow">3.7 插入索引</a></p> 
<p id="4%E3%80%81ELK%20Logstash%20%E9%83%A8%E7%BD%B2-toc" style="margin-left:40px;"><a href="#4%E3%80%81ELK%20Logstash%20%E9%83%A8%E7%BD%B2" rel="nofollow">4、ELK Logstash 部署</a></p> 
<p id="4.1%C2%A0%E5%AE%89%E8%A3%85Apahce%E6%9C%8D%E5%8A%A1%EF%BC%88httpd%EF%BC%89-toc" style="margin-left:80px;"><a href="#4.1%C2%A0%E5%AE%89%E8%A3%85Apahce%E6%9C%8D%E5%8A%A1%EF%BC%88httpd%EF%BC%89" rel="nofollow">4.1 安装Apahce服务（httpd）</a></p> 
<p id="4.2%C2%A0%E5%AE%89%E8%A3%85logstash-toc" style="margin-left:80px;"><a href="#4.2%C2%A0%E5%AE%89%E8%A3%85logstash" rel="nofollow">4.2 安装logstash</a></p> 
<p id="4.3%20%E6%B5%8B%E8%AF%95Logstash-toc" style="margin-left:80px;"><a href="#4.3%20%E6%B5%8B%E8%AF%95Logstash" rel="nofollow">4.3 测试Logstash</a></p> 
<p id="4.4%C2%A0%E5%AE%9A%E4%B9%89%20logstash%20%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-toc" style="margin-left:80px;"><a href="#4.4%C2%A0%E5%AE%9A%E4%B9%89%20logstash%20%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6" rel="nofollow">4.4 定义 logstash 配置文件</a></p> 
<p id="5%E3%80%81ELK%20Kiabana%20%E9%83%A8%E7%BD%B2-toc" style="margin-left:40px;"><a href="#5%E3%80%81ELK%20Kiabana%20%E9%83%A8%E7%BD%B2" rel="nofollow">5、ELK Kiabana 部署</a></p> 
<p id="1.1%20%E5%AE%89%E8%A3%85%20Kiabana-toc" style="margin-left:80px;"><a href="#1.1%20%E5%AE%89%E8%A3%85%20Kiabana" rel="nofollow">1.1 安装 Kiabana</a></p> 
<p id="1.2%20%E8%AE%BE%E7%BD%AE%20Kibana%20%E7%9A%84%E4%B8%BB%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%B9%B6%E5%90%AF%E5%8A%A8%E6%9C%8D%E5%8A%A1-toc" style="margin-left:80px;"><a href="#1.2%20%E8%AE%BE%E7%BD%AE%20Kibana%20%E7%9A%84%E4%B8%BB%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%B9%B6%E5%90%AF%E5%8A%A8%E6%9C%8D%E5%8A%A1" rel="nofollow">1.2 设置 Kibana 的主配置文件并启动服务</a></p> 
<p id="1.3%20%E9%AA%8C%E8%AF%81%20Kibana-toc" style="margin-left:80px;"><a href="#1.3%20%E9%AA%8C%E8%AF%81%20Kibana" rel="nofollow">1.3 验证 Kibana</a></p> 
<p id="1.4%C2%A0%E5%B0%86%20Apache%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E6%97%A5%E5%BF%97%EF%BC%88%E8%AE%BF%E9%97%AE%E7%9A%84%E3%80%81%E9%94%99%E8%AF%AF%E7%9A%84%EF%BC%89%E6%B7%BB%E5%8A%A0%E5%88%B0%20Elasticsearch%20%E5%B9%B6%E9%80%9A%E8%BF%87%20Kibana%20%E6%98%BE%E7%A4%BA-toc" style="margin-left:80px;"><a href="#1.4%C2%A0%E5%B0%86%20Apache%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E6%97%A5%E5%BF%97%EF%BC%88%E8%AE%BF%E9%97%AE%E7%9A%84%E3%80%81%E9%94%99%E8%AF%AF%E7%9A%84%EF%BC%89%E6%B7%BB%E5%8A%A0%E5%88%B0%20Elasticsearch%20%E5%B9%B6%E9%80%9A%E8%BF%87%20Kibana%20%E6%98%BE%E7%A4%BA" rel="nofollow">1.4 将 Apache 服务器的日志（访问的、错误的）添加到 Elasticsearch 并通过 Kibana 显示</a></p> 
<p id="%E4%B8%89%E3%80%81%E6%90%AD%E5%BB%BAELFK%EF%BC%88Filebeat%2BELK%C2%A0%EF%BC%89-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81%E6%90%AD%E5%BB%BAELFK%EF%BC%88Filebeat%2BELK%C2%A0%EF%BC%89" rel="nofollow">三、搭建ELFK（Filebeat+ELK ）</a></p> 
<p id="1%E3%80%81%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2-toc" style="margin-left:40px;"><a href="#1%E3%80%81%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2" rel="nofollow">1、环境部署</a></p> 
<p id="2%E3%80%81ELFK%20Filebeat%20%E9%83%A8%E7%BD%B2-toc" style="margin-left:40px;"><a href="#2%E3%80%81ELFK%20Filebeat%20%E9%83%A8%E7%BD%B2" rel="nofollow">2、ELFK Filebeat 部署</a></p> 
<p id="1.1%C2%A0%E5%AE%89%E8%A3%85%20Filebeat-toc" style="margin-left:80px;"><a href="#1.1%C2%A0%E5%AE%89%E8%A3%85%20Filebeat" rel="nofollow">1.1 安装 Filebeat</a></p> 
<p id="1.2%20%E8%AE%BE%E7%BD%AE%20filebeat%20%E7%9A%84%E4%B8%BB%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-toc" style="margin-left:80px;"><a href="#1.2%20%E8%AE%BE%E7%BD%AE%20filebeat%20%E7%9A%84%E4%B8%BB%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6" rel="nofollow">1.2 设置 filebeat 的主配置文件</a></p> 
<p id="1.3%20%E5%9C%A8%20Logstash%20%E7%BB%84%E4%BB%B6%E6%89%80%E5%9C%A8%E8%8A%82%E7%82%B9%E4%B8%8A%E6%96%B0%E5%BB%BA%E4%B8%80%E4%B8%AA%20Logstash%20%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-toc" style="margin-left:80px;"><a href="#1.3%20%E5%9C%A8%20Logstash%20%E7%BB%84%E4%BB%B6%E6%89%80%E5%9C%A8%E8%8A%82%E7%82%B9%E4%B8%8A%E6%96%B0%E5%BB%BA%E4%B8%80%E4%B8%AA%20Logstash%20%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6" rel="nofollow">1.3 在 Logstash 组件所在节点上新建一个 Logstash 配置文件</a></p> 
<p id="1.4%20%E7%99%BB%E5%BD%95%E5%88%B0Kibana%E8%BF%9B%E8%A1%8C%E6%B5%8B%E8%AF%95-toc" style="margin-left:80px;"><a href="#1.4%20%E7%99%BB%E5%BD%95%E5%88%B0Kibana%E8%BF%9B%E8%A1%8C%E6%B5%8B%E8%AF%95" rel="nofollow">1.4 登录到Kibana进行测试</a></p> 
<p id="%E5%9B%9B%E3%80%81%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81%E6%80%BB%E7%BB%93" rel="nofollow">四、总结</a></p> 
<p id="1%E3%80%81ELK%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F-toc" style="margin-left:40px;"><a href="#1%E3%80%81ELK%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F" rel="nofollow">1、ELK日志分析系统</a></p> 
<p id="2%E3%80%81feilbeat%20%2B%20Logstash%20%2B%20Elasticsearch%20%2B%20Kibana%E6%A8%A1%E5%BC%8F-toc" style="margin-left:40px;"><a href="#2%E3%80%81feilbeat%20%2B%20Logstash%20%2B%20Elasticsearch%20%2B%20Kibana%E6%A8%A1%E5%BC%8F" rel="nofollow">2、feilbeat + Logstash + Elasticsearch + Kibana模式</a></p> 
<p id="3%E3%80%81feilbeat%20%2B%20%E7%BC%93%E5%AD%98%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%2B%20Logstash%20%2B%20Elasticsearch%20%2B%20Kibana%20%E6%A8%A1%E5%BC%8F-toc" style="margin-left:40px;"><a href="#3%E3%80%81feilbeat%20%2B%20%E7%BC%93%E5%AD%98%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%2B%20Logstash%20%2B%20Elasticsearch%20%2B%20Kibana%20%E6%A8%A1%E5%BC%8F" rel="nofollow">3、feilbeat + 缓存/消息队列+ Logstash + Elasticsearch + Kibana 模式</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E5%89%8D%E8%A8%80">前言</h2> 
<p>ELK 是一个流行的日志管理和分析平台，由三个开源工具组成：Elasticsearch、Logstash 和 Kibana。这些工具结合在一起，为用户提供了强大的日志收集、存储、搜索和可视化功能</p> 
<h2 id="%E4%B8%80%E3%80%81ELK%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0">一、ELK日志分析系统概述</h2> 
<p>ELK平台是一套完整的日志集中处理解决方案，将 ElasticSearch、Logstash 和 Kiabana 三个开源工具配合使用， 完成更强大的用户对日志的查询、排序、统计需求</p> 
<h3 id="1%E3%80%81%E4%B8%89%E5%A4%A7%E7%BB%84%E4%BB%B6%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D">1、三大组件工具介绍</h3> 
<h4 id="1.1%C2%A0Elasticsearch">1.1 Elasticsearch</h4> 
<pre><code class="language-bash">Elastic中文官网：https://www.elastic.co/cn/</code></pre> 
<h5 id="1.1.1%C2%A0Elasticsearch%E6%A6%82%E5%BF%B5">1.1.1 Elasticsearch概念</h5> 
<ul><li>Elasticsearch 是 ELK 系统的核心组件之一</li><li>它是一个基于 Lucene（一个全文检索引擎的架构）开发的分布式搜索和分析引擎，能够快速地存储、搜索和分析大量数据</li><li>Elasticsearch是一个实时的、分布式的可扩展的搜索引擎，允许进行全文、结构化搜索，它通常用于索引和搜索大容量的日志数据，也可用于搜索许多不同类型的文档</li><li>Elasticsearch 是用 Java 开发的，可通过 RESTful Web 接口，让用户可以通过浏览器与 Elasticsearch 通信</li></ul> 
<p><strong>Elasticsearch 的核心概念包括以下几个关键要素：</strong></p> 
<ul><li> <p><strong>索引（Index）</strong></p> </li></ul> 
<p>一个索引就是一个拥有几分相似特征的文档的集合</p> 
<p>一个索引由一个名字来标识（必须全部是小写字母），并且当我们要对对应于这个索引中的文档进行索引、搜索、更新和删除的时候。都要使用到这个名字。在一个集群中，可以定义任意多的索引</p> 
<blockquote> 
 <p>索引（库）-----&gt; 类型（表）-----&gt; 文档（记录）</p> 
</blockquote> 
<ul><li> <p><strong>类型（Type）</strong></p> </li></ul> 
<p>在一个索引中，你可以定义一种或多种类型。一个类型是你的索引的一个逻辑上的分类/分区，其语义完全由你来定</p> 
<p>通常会为具有一组共同字段的文档定义一个类型</p> 
<ul><li> <p><strong>文档（Document）</strong></p> </li></ul> 
<p>一个文档是一个可被索引的基础信息单元</p> 
<p>在一个index/type里面，只要你想，你可以存储任意多的文档。注意，虽然一个文档在物理上位于一个索引中，实际上一个文档必须在一个索引内被索引和分配一个类型</p> 
<ul><li> <p><strong>集群（Cluster）</strong></p> </li></ul> 
<p>一个集群就是由一个或者多个节点组织在一起，它们共同持有你整个的数据，并一起提供索引和搜索功能。其中一个为主节点，这个主节点是可以通过选举产生的，并提供跨节点的联合索引和搜索功能</p> 
<p>集群有一个唯一性标示的名字，默认是Elasticsearch，集群的名字很重要，每个节点是基于集群名字加入到集群中的。因此，确保在不同的环境中使用不同的集群名字</p> 
<p>一个集群可以只有一个节点，建议在配置Elasticsearch时，配置成集群模式</p> 
<p>Elasticsearch具有集群机制，节点通过集群名称加入到集群中，同时在集群中的节点会有一个自己唯一的身份标识（自己的名称）</p> 
<ul><li> <p><strong>接近实时（Near Real-Time）</strong></p> </li></ul> 
<p>Elasticsearch 被称为 "接近实时" 是因为它能够在文档被索引后几乎立即变得可搜索。当文档被索引到 Elasticsearch 中时，它们会在不久之后就变得可搜索，通常只有一小段延迟。这意味着对于用户来说，他们可以几乎实时地获取到最新的数据，而无需等待长时间的索引过程完成</p> 
<ul><li> <p><strong>节点（Node）</strong></p> </li></ul> 
<p>节点就是一台单一的服务器，是集群的一部分，存储数据并参与集群的索引和搜索功能。像集群一样，节点也是通过名字来标识，默认是在节点启动时随机分配的字符名。也可自己定义，名字很重要，在集群中用于识别服务器对应的节点</p> 
<p>节点可以通过指定集群名字来加入到集群中。默认情况下，每个节点被设置成加入到Elasticsearch集群。如果启动了多个节点，假设能自动发现对方，他们将会自动组建一个名为Elasticsearch的集群</p> 
<ul><li> <p><strong>分片和副本（Shards and Replicas）</strong></p> </li></ul> 
<p>Elasticsearch 使用分片将索引分成多个部分，每个部分称为一个分片。这允许索引水平扩展到多个节点上。此外，每个分片还可以具有零个或多个副本，用于提高数据的冗余性和可用性</p> 
<p><strong>分片的主要原因：</strong></p> 
<p>①水平分割扩展，增大存储量</p> 
<p>②分布式并跨越分片操作，提高性能和吞吐量</p> 
<p>③分布式分片机制和搜索请求的文档如何火鬃完全是由Elasticsearch控制的，这些对用户是完全透明的</p> 
<p>④为了健壮性，建议有一个故障切换机制，为此，Elasticsearch让我们将索引分片复制一份或多份，称之为分片副本</p> 
<p><strong>分片副本的主要原因：</strong></p> 
<p>①高可用性，以应对分片或者节点故障。处于这个原因，分片副本要在不同的节点上</p> 
<p>②增大吞吐量，搜索可以并行在所有副本上执行</p> 
<p>总之，每个索引可以被分成多个分片。一个索引可以被复制0次或者多次。一旦复制了，每个索引就有了主分片 （作为复制源的原来的分片）和复制分片（主分片的拷贝）之别。分片和副本的数量可以在索引创建的时候指定。在索引创建之后，你可以在指定任何时候动态的改变副本的数量，但是你事后不能改变分片的数量</p> 
<p> 默认情况下，Elasticsearch中的每个索引被分片5个主分片和1个副本，这意味着，如果你的集群中至少有两个节点，你的索引将会有5个主分片和另外的5个副本分片（一个完全拷贝），这样的话每个索引总共有10个分片</p> 
<h5 id="1.1.2%C2%A0%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8CElasticSearch%E4%B8%AD%E7%9A%84%E5%AF%B9%E5%BA%94%E5%85%B3%E7%B3%BB">1.1.2 <strong>关系型数据库和ElasticSearch中的对应关系</strong></h5> 
<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td><strong>关系型数据库</strong></td><td><strong>Elasticserch</strong></td></tr><tr><td>数据库database</td><td>索引index，支持全文索引</td></tr><tr><td>表table</td><td>类型type</td></tr><tr><td>数据行row</td><td>文档document。但不需要固定结构，不同文档可以具有不同字段集合</td></tr><tr><td>数据列cloumn</td><td>字段field</td></tr><tr><td>模式schema</td><td>映像mapping</td></tr></tbody></table> 
<h5 id="1.1.3%C2%A0Elasticsearch%E6%8F%90%E4%BE%9B%E7%9A%84%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4">1.1.3 <strong>Elasticsearch提供的操作命令</strong></h5> 
<p><strong>Elasticsearch 通过 RESTful Web 接口提供了各种操作指令，主要包括以下几种：</strong> </p> 
<table align="center" border="1" cellpadding="1" cellspacing="1" style="width:500px;"><thead><tr><th>操作指令</th><th>说明</th></tr></thead><tbody><tr><th>GET</th><td>用于从 Elasticsearch 中检索数据。例如，可以使用 GET 请求来获取文档、执行搜索或获取特定的统计信息</td></tr><tr><th>POST</th><td>用于执行各种操作，如索引文档、执行搜索、更新文档或执行批量操作。POST 请求是一个通用的操作请求方式</td></tr><tr><th>PUT</th><td>用于在 Elasticsearch 中创建或更新文档。通过 PUT 请求，您可以将新文档添加到索引中，或者更新现有文档的内容</td></tr><tr><th>DELTET</th><td>用于从 Elasticsearch 中删除文档、索引或其他资源。DELETE 请求允许您删除不再需要的数据</td></tr><tr><th>HEAD</th><td>用于获取关于索引或文档状态的元数据信息，而不返回实际数据内容。这对于检查索引或文档是否存在非常有用</td></tr></tbody></table> 
<h4 id="1.2%C2%A0Logstash">1.2 Logstash</h4> 
<pre><code class="language-bash">Logstash中文官网：https://www.elastic.co/cn/logstash</code></pre> 
<h5 id="1.2.1%20Logstash%E6%A6%82%E5%BF%B5">1.2.1 Logstash概念</h5> 
<ul><li>作为数据收集引擎。它支持动态的从各种数据源搜集数据，并对数据进行过滤、分析、丰富、统一格式等操作，然后存储到用户指定的位置,一般会发送给 Elasticsearch</li><li>Logstash 由 Ruby 语言编写，运行在 Java 虚拟机（JVM）上，是一款强大的数据处理工具， 可以实现数据传输、格式处理、格式化输出。Logstash 具有强大的插件功能，常用于日志处理</li></ul> 
<h5 id="1.2.2%20Logstash%E7%9A%84%E4%B8%BB%E8%A6%81%E7%BB%84%E4%BB%B6"><strong>1.2.2 Logstash的主要组件</strong></h5> 
<ul><li> <p><strong>Shipper</strong>：Shipper 是指负责从不同来源（如服务器、应用程序、网络设备等）收集日志数据的组件。它可以是 Logstash 的 Input 插件，负责实时收集和捕获各种日志信息</p> </li><li> <p><strong>Indexer</strong>：Indexer 负责接收 Shipper 收集到的原始日志数据，并将其标准化、转换为适当的格式，并将其发送到存储系统（比如 Elasticsearch）。在 Logstash 中，这一部分通常由 Filter 插件完成</p> </li><li> <p><strong>Broker</strong>：Broker 通常用于将数据发送到消息队列，以提供异步处理和缓冲功能。这使得系统能够更好地处理突发的大量数据，同时降低了对于 Indexer 的直接压力</p> </li><li> <p><strong>Search and Storage</strong>：这部分主要涉及将经过处理的日志数据存储到适当的存储系统中，通常是 Elasticsearch。Elasticsearch 提供了强大的搜索和分析功能，使用户能够快速查询和分析大量的日志数据</p> </li><li> <p><strong>Web Interface</strong>：Web Interface 提供了一个用户友好的界面，用于监控日志数据的收集、处理和存储情况。Kibana 是一个常用的 Web 接口，它与 Elasticsearch 集成，提供了丰富的数据可视化和查询功能，使用户能够直观地理解日志数据。</p> </li></ul> 
<p>这些组件共同构成了一个典型的日志处理流水线，使得 Logstash 能够有效地从多个来源收集日志数据，进行处理和存储，并提供用户友好的界面来查询和分析这些数据。</p> 
<h5 id="1.2.3%C2%A0Logstash%E4%B8%BB%E6%9C%BA%E5%88%86%E7%B1%BB">1.2.3 Logstash主机分类</h5> 
<ul><li>代理主机（agent host）：作为事件的传递者（Shipper），将各种日志数据发送至中心主机，只需运行Logstash代理程序</li><li>中心主机（central host）：可运行包括中间转发器（Broker）、索引器（Indexer）、搜索和存储器（Search and Storage）、Web界面端（Web Interface）在内的各个组件，以实现对日志数据的接收、处理和存储</li></ul> 
<h5 id="1.2.4%C2%A0Logstash%E5%B7%A5%E4%BD%9C%E8%BF%87%E7%A8%8B"><strong>1.2.4 Logstash工作过程</strong></h5> 
<p class="img-center"><img alt="" height="295" src="https://images2.imgbox.com/89/5d/FWU47LnV_o.png" width="700"></p> 
<ul><li><strong>Input（数据采集）：</strong>通过 Input 插件从不同的来源收集数据，这些来源可以包括日志文件、消息队列、数据库、网络流量等。Input 插件负责将原始数据获取并发送到 Logstash 进行处理</li><li><strong>Filter（数据过滤）：</strong>Filter 插件可以对数据进行加工处理过滤，可以做复杂的处理逻辑。这个步骤不是必须的</li><li><strong>Output（数据输出）：</strong>Output 插件负责将数据发送到指定的目的地，比如 Elasticsearch、Kafka、Redis 等</li></ul> 
<h4 id="1.3%C2%A0Kiabana">1.3 Kiabana</h4> 
<ul><li>Kibana 通常与 Elasticsearch 一起部署，Kibana 是 Elasticsearch 的一个功能强大的数据可视化 Dashboard，Kibana 提供图形化的 web 界面来浏览 Elasticsearch 日志数据，可以用来汇总、分析和搜索重要数据</li><li>Kibana 提供了丰富的图表、仪表盘和报表功能，使用户能够创建定制的数据可视化界面</li><li>这些可视化工具有助于用户理解数据趋势、发现异常情况，并监控系统运行状况</li><li>Kibana 还提供了强大的搜索功能，使用户能够快速查询和分析日志数据</li></ul> 
<p><strong>Kibana主要功能：</strong></p> 
<ul><li><strong>Elasticsearch无缝之集成</strong></li></ul> 
<p>Kibana架构为Elasticsearch定制，可以将任何结构化和非结构化数据加入Elasticsearch索引。Kibana还充分利用了Elasticsearch强大的搜索和分析功能</p> 
<ul><li><strong>整合数据 </strong></li></ul> 
<p>Kibana能够更好地处理海量数据，并据此创建柱形图、折线图、散点图、直方图、饼图和地图</p> 
<ul><li><strong>复杂数据分析</strong></li></ul> 
<p>Kibana提升了Elasticsearch分析能力，能够更加智能地分析数据，执行数学转换并且根据要求对数据切割分块</p> 
<ul><li><strong>让更多团队成员收益</strong></li></ul> 
<p>强大的数据库可视化接口让各业务岗位都能够从数据集合受益</p> 
<ul><li><strong>接口灵活，分享更容易</strong></li></ul> 
<p>使用Kibana可以更加方便地创建、保存、分享数据，并将可视化数据快速交流</p> 
<ul><li><strong>配置简单</strong></li></ul> 
<p>Kibana的配置和启用非常简单，用户体验非常友好。Kibana自带Web服务器，可以快速启动运行</p> 
<ul><li><strong>可视化多数据源</strong></li></ul> 
<p>Kibana可以非常方便地把来自Logstash、ES-Hadoop、Beats或第三方技术的数据整合到Elasticsearch，支持的第三方技术包括Apache flume、 Fluentd 等</p> 
<ul><li><strong>简单数据导出</strong></li></ul> 
<p>Kibana可以方便地导出感兴趣的数据，与其它数据集合并融合后快速建模分析，发现新结果</p> 
<h3 id="2%E3%80%81%E5%85%B6%E4%BB%96%E5%8F%AF%E6%9B%BF%E4%BB%A3Logstash%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D">2、其他可替代Logstash组件介绍</h3> 
<p><strong>资源消耗高</strong>: Logstash 在处理大量数据时会占用大量内存和CPU资源。特别是在进行复杂的数据转换和过滤时，其资源消耗会更加明显。这可能导致在资源受限的环境中出现性能下降或系统负载增加的问题</p> 
<p><strong>Java 虚拟机开销</strong>: Logstash 是基于 Java 编写的，因此在启动时需要加载 Java 虚拟机（JVM），这会消耗一定的内存和启动时间</p> 
<p><strong>单线程处理</strong>: 在默认配置下，Logstash 是单线程处理数据的，这意味着它可能无法充分利用多核处理器，尤其是在处理大量数据时可能会成为瓶颈</p> 
<p>综上由于 logstash 太重量级的缺点，Logstash 性能低、资源消耗比较多等问题，就出现了以下三种可替代Logstash收集数据的三个组件工具</p> 
<h4 id="2.1%C2%A0Filebeat">2.1 Filebeat</h4> 
<p>Filebeat：轻量级的开源日志文件数据搜集器。通常在需要采集数据的客户端安装 Filebeat，并指定目录与日志格式，Filebeat 就能快速收集数据，并发送给 logstash 进或是直接发给Elasticsearch 存储，性能上相比运行于 JVM 上的 logstash 优势明显，是对它的替代。常应用于 EFLK 架构当中。</p> 
<p><strong>filebeat 结合 logstash 带来好处：</strong></p> 
<ul><li>通过 Logstash 具有基于磁盘的自适应缓冲系统，该系统将吸收传入的吞吐量，从而减轻 Elasticsearch 持续写入数据的压力</li><li>从其他数据源（例如数据库，S3对象存储或消息传递队列）中提取</li><li>将数据发送到多个目的地，例如S3，HDFS（Hadoop分布式文件系统）或写入文件</li><li>使用条件数据流逻辑组成更复杂的处理管道</li></ul> 
<h4 id="2.2%C2%A0%E7%BC%93%E5%AD%98%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%EF%BC%88redis%E3%80%81kafka%E3%80%81RabbitMQ%E7%AD%89%EF%BC%89">2.2 缓存/消息队列（redis、kafka、RabbitMQ等）</h4> 
<p>可以对高并发日志数据进行流量削峰和缓冲，这样的缓冲可以一定程度的保护数据不丢失，还可以对整个架构进行应用解耦</p> 
<h4 id="2.3%C2%A0Fluentd">2.3 Fluentd</h4> 
<ul><li>Fluentd：是一个流行的开源数据收集器。相比较 logstash，Fluentd 更易用、资源消耗更少、性能更高，在数据处理上更高效可靠，受到企业欢迎，成为 logstash 的一种替代方案，常应用于 EFK 架构当中。在 Kubernetes 集群中也常使用 EFK 作为日志数据收集的方案</li><li>在 Kubernetes 集群中一般是通过 DaemonSet 来运行 Fluentd，以便它在每个 Kubernetes 工作节点上都可以运行一个 Pod</li><li>它通过获取容器日志文件、过滤和转换日志数据，然后将数据传递到 Elasticsearch 集群，在该集群中对其进行索引和存储</li></ul> 
<h3 id="%C2%A03%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8ELK"> 3、为什么要使用ELK</h3> 
<ul><li>日志主要包括系统日志、应用程序日志和安全日志。系统运维和开发人员可以通过日志了解服务器软硬件信息、检查配置过程中的错误及错误发生的原因。经常分析日志可以了解服务器的负荷，性能安全性，从而及时采取措施纠正错误</li><li>往往单台机器的日志我们使用grep、awk等工具就能基本实现简单分析，但是当日志被分散的储存不同的设备上。如果你管理数十上百台服务器，你还在使用依次登录每台机器的传统方法查阅日志。这样是不是感觉很繁琐和效率低下。当务之急我们使用集中化的日志管理。</li></ul> 
<blockquote> 
 <p>例如：开源的syslog，将所有服务器上的日志收集汇总。集中化管理日志后，日志的统计和检索又成为一件比较麻烦的事情，一般我们使用 grep、awk和wc等Linux命令能实现检索和统计，但是对于要求更高的查询、排序和统计等要求和庞大的机器数量依然使用这样的方法难免有点力不从心</p> 
</blockquote> 
<ul><li>一般大型系统是一个分布式部署的架构，不同的服务模块部署在不同的服务器上，问题出现时，大部分情况需要根据问题暴露的关键信息，定位到具体的服务器和服务模块，构建一套集中式日志系统，可以提高定位问题的效率</li></ul> 
<h3 id="4%E3%80%81%E5%AE%8C%E6%95%B4%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%89%B9%E5%BE%81">4、完整日志系统的基本特征</h3> 
<ul><li><strong>收集</strong>：能够采集多种来源的日志数据</li><li><strong>传输</strong>：能够稳定的把日志数据解析过滤并传输到存储系统</li><li><strong>存储</strong>：存储日志数据</li><li><strong>分析</strong>：支持 UI 分析</li><li><strong>警告</strong>：能够提供错误报告，监控机制</li></ul> 
<h3 id="5%E3%80%81ELK%20%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86">5、ELK 的工作原理</h3> 
<p class="img-center"><img alt="" height="361" src="https://images2.imgbox.com/28/a3/M0c0jQhC_o.png" width="700"></p> 
<ul><li>在所有需要收集日志的服务器上部署Logstash；或者先将日志进行集中化管理在日志服务器上，在日志服务器上部署 Logstash</li><li>Logstash 收集日志，将日志格式化并输出到 Elasticsearch 群集中</li><li>Elasticsearch 对格式化后的数据进行索引和存储</li><li>Kibana 从 ES 群集中查询数据生成图表，并进行前端数据的展示</li></ul> 
<blockquote> 
 <p><strong>总结：logstash作为日志搜集器，从数据源采集数据，并对数据进行过滤，格式化处理，然后交由Elasticsearch存储，kibana对日志进行可视化处理</strong> </p> 
</blockquote> 
<h2 id="%E4%BA%8C%E3%80%81%E6%90%AD%E5%BB%BAELK%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F">二、搭建ELK日志分析系统</h2> 
<p class="img-center"><img alt="" height="484" src="https://images2.imgbox.com/7c/da/dgl6Bnaq_o.png" width="700"></p> 
<blockquote> 
 <p>数据流向： apache下的/etc/httpd/logs/* ----&gt;  logstash ----&gt; ElasticSearch ----&gt; Kibana</p> 
</blockquote> 
<h3 id="1%E3%80%81%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2">1、环境部署</h3> 
<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td><strong>服务器</strong></td><td><strong>配置（越高性能越好）</strong></td><td><strong>主机名</strong></td><td><strong>ip地址</strong></td><td><strong>主要软件</strong></td></tr><tr><td>es_node1节点</td><td>2C/4G</td><td>es_node1</td><td>172.16.12.12</td><td>ElasticSearch、Kibana</td></tr><tr><td>es_node2节点</td><td>2C/4G</td><td>es_node2</td><td>172.16.12.13</td><td>ElasticSearch</td></tr><tr><td>logstash节点</td><td>/</td><td>logstash</td><td>172.16.12.10</td><td> <p>Logstash、Apache</p> </td></tr></tbody></table> 
<p><strong>（1）关闭所有设备的防火墙和核心防护</strong></p> 
<pre><code class="language-bash">[root@localhost ~]#systemctl stop firewalld
[root@localhost ~]#setenforce 0</code></pre> 
<p><strong>（2）修改三台设备的主机名</strong></p> 
<pre><code class="language-bash">[root@localhost ~]#hostnamectl set-hostname es_node1
[root@localhost ~]#bash
 
[root@localhost ~]#hostnamectl set-hostname es_node2
[root@localhost ~]#bash
 
[root@localhost ~]#hostnamectl set-hostname logstash
[root@localhost ~]#bash</code></pre> 
<p><strong>（3）es_node节点1和es_node节点2都要配置本地的/etc/hosts文件</strong></p> 
<pre><code class="language-bash">echo "172.16.12.12 es_node1" &gt;&gt; /etc/hosts
echo "172.16.12.13 es_node2" &gt;&gt; /etc/hosts</code></pre> 
<p class="img-center"><img alt="" height="110" src="https://images2.imgbox.com/33/dd/2B3PGmJj_o.png" width="700"></p> 
<p><strong>（3）三台设备都需部署java环境，安装oraclejdk</strong></p> 
<pre><code class="language-bash">java -version    #不建议使用openjdk，所以三台设备都需安装oraclejdk</code></pre> 
<p class="img-center"><img alt="" height="129" src="https://images2.imgbox.com/0f/89/BlEAa78k_o.png" width="700"></p> 
<pre><code class="language-bash"># rpm安装oraclejdk

#yum install或rpm -ivh安装oraclejdk
cd /opt    #将rpm软件包传至/opt目录下
rpm -ivh jdk-8u291-linux-x64.rpm

#将openjdk更换至oraclejdk
vim /etc/profile.d/jdk.sh
export JAVA_HOME=/usr/java/jdk1.8.0_201-amd64    #输出定义java的工作目录
export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar   #输出指定java所需的类文件
export PATH=$JAVA_HOME/bin:$PATH   #输出重新定义环境变量，$PATH一定要放在$JAVA_HOME的后面，让系统先读取到工作目录中的版本信息

source /etc/profile.d/jdk.sh  #执行配置文件
java -version
----------------------------------------------------------------------------------------
# 二进制包安装oraclejdk

cd /opt   #将二进制包传至/opt目录下
tar zxvf jdk-8u291-linux-x64.tar.gz -C /usr/local
ln -s /usr/local/jdk1.8.0_291/ /usr/local/jdk
 
vim /etc/profile.d/jdk.sh
export JAVA_HOME=/usr/local/jdk
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH
 
source /etc/profile.d/jdk.sh
java -version</code></pre> 
<p class="img-center"><img alt="" height="371" src="https://images2.imgbox.com/4d/55/Kg5Ajuol_o.png" width="700"></p> 
<h3 id="2%E3%80%81ElasticSearch%20%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2">2、ELK ElasticSearch 集群部署</h3> 
<p><span style="color:#fe2c24;"><strong>需要安装部署在es_node1节点和es_node2节点上</strong></span></p> 
<h4 id="2.1%C2%A0%E5%AE%89%E8%A3%85%20elasticsearch-rpm%20%E5%8C%85%E5%B9%B6%E5%8A%A0%E8%BD%BD%E7%B3%BB%E7%BB%9F%E6%9C%8D%E5%8A%A1">2.1 <strong>安装 elasticsearch-rpm 包并加载系统服务</strong></h4> 
<pre><code class="language-bash">#安装elasticsearch-rpm 包
cd /opt    #上传elasticsearch-5.5.0.rpm到/opt目录下
rpm -ivh elasticsearch-5.5.0.rpm

#加载系统服务
systemctl daemon-reload    
systemctl enable elasticsearch.service  </code></pre> 
<p><img alt="" height="527" src="https://images2.imgbox.com/b4/22/Zm3PDdtc_o.png" width="1200"></p> 
<h4 id="%C2%A02.2%C2%A0%E4%BF%AE%E6%94%B9%20elasticsearch%20%E4%B8%BB%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"> 2.2 <strong>修改 elasticsearch 主配置文件</strong></h4> 
<pre><code class="language-bash">cp /etc/elasticsearch/elasticsearch.yml /etc/elasticsearch/elasticsearch.yml.bak
vim /etc/elasticsearch/elasticsearch.yml
--17--取消注释，指定集群名字
cluster.name: my-elk-cluster
--23--取消注释，指定节点名字：Node1节点为node1，Node2节点为node2
node.name: node1
--33--取消注释，指定数据存放路径
path.data: /data/elk_data
--37--取消注释，指定日志存放路径
path.logs: /var/log/elasticsearch/
--43--取消注释，改为在启动的时候不锁定内存
bootstrap.memory_lock: false
--55--取消注释，设置监听地址，0.0.0.0代表所有地址
network.host: 0.0.0.0
--59--取消注释，ES 服务的默认监听端口为9200
http.port: 9200
--68--取消注释，集群发现通过单播实现，指定要发现的节点 es_node1、es_node2
discovery.zen.ping.unicast.hosts: ["es_node1", "es_node2"]</code></pre> 
<p class="img-center"><img alt="" height="321" src="https://images2.imgbox.com/68/f8/rcH18lKm_o.png" width="700"></p> 
<pre><code class="language-bash">grep -v "^#" /etc/elasticsearch/elasticsearch.yml
#过滤到能生效的语句，检查修改的是否正确</code></pre> 
<p class="img-center"><img alt="" height="193" src="https://images2.imgbox.com/4b/8d/jqf5arS7_o.png" width="700"></p> 
<h4 id="2.3%C2%A0%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%AD%98%E6%94%BE%E8%B7%AF%E5%BE%84%E5%B9%B6%E6%8E%88%E6%9D%83">2.3 创建数据存放路径并授权</h4> 
<pre><code class="language-bash">mkdir -p /data/elk_data
chown elasticsearch:elasticsearch /data/elk_data/</code></pre> 
<p class="img-center"><img alt="" height="71" src="https://images2.imgbox.com/8a/92/H4LCtGln_o.png" width="700"></p> 
<h4 id="2.4%C2%A0%E5%90%AF%E5%8A%A8elasticsearch%E6%98%AF%E5%90%A6%E6%88%90%E5%8A%9F%E5%BC%80%E5%90%AF">2.4 启动elasticsearch是否成功开启</h4> 
<pre><code class="language-bash">systemctl start elasticsearch.service
ss -antp | grep 9200</code></pre> 
<p class="img-center"><img alt="" height="48" src="https://images2.imgbox.com/e3/01/Mf4FXDXi_o.png" width="700"></p> 
<p class="img-center"><img alt="" height="95" src="https://images2.imgbox.com/9a/da/g9NX24FE_o.png" width="700"></p> 
<p>综上，两个es_node节点都部署安装完elasticsearch</p> 
<h4 id="2.5%C2%A0%E6%9F%A5%E7%9C%8B%E8%8A%82%E7%82%B9%E4%BF%A1%E6%81%AF">2.5 查看节点信息</h4> 
<p>浏览器访问：http://172.16.12.12:9200  、 http://172.16.12.13:9200 来查看节点 es_node1、es_node2 的信息</p> 
<p class="img-center"><img alt="" height="327" src="https://images2.imgbox.com/c0/b3/F0GTYxwO_o.png" width="700"></p> 
<p>浏览器访问：http://172.16.12.12:9200/_cluster/health?pretty  、 http://172.16.12.12:9200/_cluster/health?pretty 来查看群集的健康情况，可以看到 status 值为 green（绿色）， 表示节点健康运行</p> 
<p class="img-center"><img alt="" height="190" src="https://images2.imgbox.com/0c/4e/yB4drX5Q_o.png" width="700"></p> 
<blockquote> 
 <p>绿色：健康，数据和副本，全都没有问题<br> 红色：数据都不完整<br> 黄色：数据完整，但副本有问题</p> 
</blockquote> 
<p>使用上述方式查看群集的状态对用户并不友好，下面可以通过安装 Elasticsearch-head 插件，可以更方便地管理群集</p> 
<h3 id="3%E3%80%81%E5%AE%89%E8%A3%85%20Elasticsearch-head%20%E6%8F%92%E4%BB%B6">3、安装 Elasticsearch-head 插件</h3> 
<p><span style="color:#fe2c24;"><strong>需要安装部署在es_node1节点上</strong></span></p> 
<p><span style="color:#fe2c24;"><strong>（可根据需要部署在es_node2节点上，我这就不部署在es_node2节点上了）</strong></span></p> 
<p>Elasticsearch 在 5.0 版本后，Elasticsearch-head 插件需要作为独立服务进行安装，需要使用npm工具（NodeJS的包管理工具）安装</p> 
<p>安装 Elasticsearch-head 需要提前安装好依赖软件 node 和 phantomjs</p> 
<ul><li>node：是一个基于 Chrome V8 引擎的 JavaScript 运行环境</li><li>phantomjs：是一个基于 webkit 的JavaScriptAPI，可以理解为一个隐形的浏览器，任何基于 webkit 浏览器做的事情，它都可以做到</li></ul> 
<h4 id="3.1%C2%A0%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85%20node">3.1 编译安装 node</h4> 
<pre><code class="language-bash">yum install gcc gcc-c++ make -y   #安装依赖包

cd /opt       #上传软件包 node-v8.2.1.tar.gz 到/opt
tar zxvf node-v8.2.1.tar.gz

#到安装包目录进行编译安装
cd node-v8.2.1/
./configure
make &amp;&amp; make install</code></pre> 
<p class="img-center"><img alt="" height="164" src="https://images2.imgbox.com/38/a5/jsg7fSVA_o.png" width="700"></p> 
<p class="img-center"><img alt="" height="136" src="https://images2.imgbox.com/76/56/zJNpSIyJ_o.png" width="700"></p> 
<h4 id="3.2%C2%A0%E5%AE%89%E8%A3%85%20phantomjs%EF%BC%88%E5%89%8D%E7%AB%AF%E7%9A%84%E6%A1%86%E6%9E%B6%EF%BC%89">3.2 安装 phantomjs（前端的框架）</h4> 
<pre><code class="language-bash">cd /opt  #上传软件包 phantomjs-2.1.1-linux-x86_64.tar.bz2 到/opt

#无需安装，解压使用即可
tar jxvf phantomjs-2.1.1-linux-x86_64.tar.bz2 -C /usr/local/src/
cd /usr/local/src/phantomjs-2.1.1-linux-x86_64/bin
cp phantomjs /usr/local/bin</code></pre> 
<p class="img-center"><img alt="" height="144" src="https://images2.imgbox.com/3e/92/SRuoE7dr_o.png" width="700"></p> 
<h4 id="3.3%C2%A0%E5%AE%89%E8%A3%85%20Elasticsearch-head%20%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7">3.3 安装 Elasticsearch-head 数据可视化工具</h4> 
<pre><code class="language-bash">cd /opt     #上传软件包 elasticsearch-head.tar.gz 到/opt

#安装elasticsearch-head
tar zxvf elasticsearch-head.tar.gz -C /usr/local/src/
cd /usr/local/src/elasticsearch-head/
npm install</code></pre> 
<p class="img-center"><img alt="" height="189" src="https://images2.imgbox.com/0f/9c/Zp7iSwLd_o.png" width="700"></p> 
<h4 id="3.4%C2%A0%E4%BF%AE%E6%94%B9%20Elasticsearch%20%E4%B8%BB%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">3.4 修改 Elasticsearch 主配置文件</h4> 
<pre><code class="language-bash">vim /etc/elasticsearch/elasticsearch.yml
......
--末尾添加以下内容--
http.cors.enabled: true                #开启跨域访问支持，默认为 false
http.cors.allow-origin: "*"            #指定跨域访问允许的域名地址为所有

systemctl restart elasticsearch   #重启elasticsearch服务</code></pre> 
<p class="img-center"><img alt="" height="96" src="https://images2.imgbox.com/0b/17/PsMJmZKL_o.png" width="700"></p> 
<h4 id="3.5%C2%A0%E5%90%AF%E5%8A%A8%20elasticsearch-head%20%E6%9C%8D%E5%8A%A1">3.5 启动 elasticsearch-head 服务</h4> 
<pre><code class="language-bash">#必须在解压后的 elasticsearch-head 目录下启动服务，进程会读取该目录下的 gruntfile.js 文件，否则可能启动失败
cd /usr/local/src/elasticsearch-head/
npm run start &amp;</code></pre> 
<p class="img-center"><img alt="" height="173" src="https://images2.imgbox.com/6a/77/EkInNVGY_o.png" width="700"></p> 
<pre><code class="language-bash">ss -natp |grep 9100       #elasticsearch-head 监听的端口是 9100</code></pre> 
<p class="img-center"><img alt="" height="39" src="https://images2.imgbox.com/5e/aa/MVS3BbxN_o.png" width="700"></p> 
<h4 id="3.6%C2%A0%E9%80%9A%E8%BF%87%20Elasticsearch-head%20%E6%9F%A5%E7%9C%8B%20Elasticsearch%20%E4%BF%A1%E6%81%AF">3.6 通过 Elasticsearch-head 查看 Elasticsearch 信息</h4> 
<p>通过浏览器访问 http://172.16.12.12:9100/ 地址并连接群集。如果看到群集健康值为 green 绿色，代表群集很健康</p> 
<p class="img-center"><img alt="" height="191" src="https://images2.imgbox.com/ce/e5/iBLGmwST_o.png" width="700"></p> 
<h4 id="3.7%C2%A0%E6%8F%92%E5%85%A5%E7%B4%A2%E5%BC%95">3.7 插入索引</h4> 
<pre><code class="language-bash">#通过命令插入一个测试索引，索引为 index-demo，类型为 test
curl -X PUT 'localhost:9200/index-demo1/test/1?pretty&amp;pretty' -H 'content-Type: application/json' -d '{"user":"lisi","mesg":"hello world"}'</code></pre> 
<p class="img-center"><img alt="" height="168" src="https://images2.imgbox.com/1c/00/O44jesFH_o.png" width="700"></p> 
<p>浏览器再次访问 http://172.16.12.12:9100/ 查看索引信息，可以看见索引默认被分片5个，并且有一个副本 </p> 
<p class="img-center"><img alt="" height="250" src="https://images2.imgbox.com/e8/ff/Maj2mXfE_o.png" width="700"></p> 
<p>点击“数据浏览”，会发现在es_node1上创建的索引为 index-demo，类型为 test 的相关信息</p> 
<p class="img-center"><img alt="" height="255" src="https://images2.imgbox.com/f8/07/IGs8A0fb_o.png" width="700"></p> 
<h3 id="4%E3%80%81ELK%20Logstash%20%E9%83%A8%E7%BD%B2">4、ELK Logstash 部署</h3> 
<p><span style="color:#fe2c24;"><strong>需要安装部署在logstash节点服务器上（本身也是apache服务器）</strong></span></p> 
<p><span style="color:#0d0016;">Logstash 一般部署在需要监控其日志的服务器。在本案例中，Logstash 部署在 Apache 服务器上，用于收集 Apache 服务器的日志信息并发送到 Elasticsearch</span></p> 
<h4 id="4.1%C2%A0%E5%AE%89%E8%A3%85Apahce%E6%9C%8D%E5%8A%A1%EF%BC%88httpd%EF%BC%89">4.1 安装Apahce服务（httpd）</h4> 
<pre><code class="language-bash">#安装apache服务，等会会收集apache服务的日志文件
[root@logstash ~]#yum -y install httpd
[root@logstash ~]#systemctl start httpd</code></pre> 
<p class="img-center"><img alt="" height="120" src="https://images2.imgbox.com/8f/dc/Qy6PNXyc_o.png" width="700"></p> 
<h4 id="4.2%C2%A0%E5%AE%89%E8%A3%85logstash" style="background-color:transparent;">4.2 安装logstash</h4> 
<pre><code class="language-bash">[root@logstash ~]#cd /opt   #上传软件包 logstash-5.5.1.rpm 到/opt目录下
[root@logstash opt]#rpm -ivh logstash-5.5.1.rpm           
[root@logstash opt]#systemctl enable --now logstash.service
[root@logstash opt]#ln -s /usr/share/logstash/bin/logstash /usr/local/bin/</code></pre> 
<p class="img-center"><img alt="" height="181" src="https://images2.imgbox.com/3a/ae/XvnRi2GN_o.png" width="700"></p> 
<h4 id="4.3%20%E6%B5%8B%E8%AF%95Logstash">4.3 测试Logstash</h4> 
<p>Logstash 命令常用选项：</p> 
<table align="center" border="1" cellpadding="1" cellspacing="1" style="width:500px;"><thead><tr><th>常用选项</th><th>说明</th></tr></thead><tbody><tr><th>-f</th><td>通过这个选项可以指定 Logstash 的配置文件，根据配置文件配置 Logstash 的输入和输出流</td></tr><tr><th>-e</th><td>从命令行中获取，输入、输出后面跟着字符串，该字符串可以被当作 Logstash 的配置（如果是空，则默认使用 stdin 作为输入，stdout 作为输出）</td></tr><tr><th>-t</th><td>测试配置文件是否正确，然后退出</td></tr></tbody></table> 
<p><strong>定义输入和输出流：</strong></p> 
<p><strong>（1）输入采用标准输入，输出采用标准输出（类似管道）</strong></p> 
<pre><code class="language-bash">#输入采用标准输入，输出采用标准输出（类似管道）
[root@logstash opt]#logstash -e 'input { stdin{} } output { stdout{} }'

......
www.baidu.com										#键入内容（标准输入）
2024-04-11T05:50:42.684Z logstash www.baidu.com		#输出结果（标准输出）
www.google.com										#键入内容（标准输入）
2024-04-11T05:51:07.295Z logstash www.google.com	#输出结果（标准输出）

//执行 ctrl+c 退出</code></pre> 
<p class="img-center"><img alt="" height="259" src="https://images2.imgbox.com/c6/08/6j6ldDOP_o.png" width="700"></p> 
<p><strong>（2） 使用 rubydebug 输出详细格式显示，codec 为一种编解码器</strong></p> 
<pre><code class="language-bash">#使用 rubydebug 输出详细格式显示，codec 为一种编解码器
[root@logstash opt]#logstash -e 'input { stdin{} } output { stdout{ codec=&gt;rubydebug } }'

......
www.baidu.com										#键入内容（标准输入）
{
    "@timestamp" =&gt; 2024-04-11T05:54:11.432Z,       #输出结果（处理后的结果）
      "@version" =&gt; "1",
          "host" =&gt; "logstash",
       "message" =&gt; "www.baidu.com"
}

//执行 ctrl+c 退出</code></pre> 
<p class="img-center"><img alt="" height="204" src="https://images2.imgbox.com/b3/dd/PDYHJegy_o.png" width="700"></p> 
<p><strong>（3） 使用 Logstash 将信息写入 Elasticsearch 中</strong></p> 
<pre><code class="language-bash">#使用 Logstash 将信息写入 Elasticsearch 中
[root@logstash opt]#logstash -e 'input { stdin{} } output { elasticsearch { hosts=&gt;["172.16.12.13:9200"] } }'
			  输入				输出	    对接

......
www.baidu.com										#键入内容（标准输入）
www.sina.com.cn										#键入内容（标准输入）
www.google.com										#键入内容（标准输入）

//执行 ctrl+c 退出</code></pre> 
<p class="img-center"><img alt="" height="371" src="https://images2.imgbox.com/08/c6/Mar220NX_o.png" width="700"></p> 
<p>结果不在标准输出显示，而是发送至 Elasticsearch 中，可浏览器访问 http://172.16.12.12:9100/ 查看索引信息和数据浏览</p> 
<p class="img-center"><img alt="" height="243" src="https://images2.imgbox.com/cf/7d/ifuprFMo_o.png" width="700"></p> 
<p class="img-center"><img alt="" height="133" src="https://images2.imgbox.com/76/6b/SiIwP1XW_o.png" width="700"></p> 
<h4 id="4.4%C2%A0%E5%AE%9A%E4%B9%89%20logstash%20%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">4.4 定义 logstash 配置文件</h4> 
<p>Logstash 配置文件基本由三部分组成：<strong>input、output </strong>以及 <strong>filter</strong>（可选，根据需要选择使用）</p> 
<ul><li>input：表示从数据源采集数据，常见的数据源如Kafka、日志文件等</li><li>filter：表示数据处理层，包括对数据进行格式化处理、数据类型转换、数据过滤等，支持正则表达式</li><li>output：表示将Logstash收集的数据经由过滤器处理之后输出到Elasticsearch。</li></ul> 
<pre><code class="language-bash">#格式：
input {...}
filter {...}
output {...}</code></pre> 
<pre><code class="language-bash">#在每个部分中，也可以指定多个访问方式。例如，若要指定两个日志来源文件，则格式如下：
input {
    file { path =&gt;"/var/log/messages" type =&gt;"syslog"}
    file { path =&gt;"/var/log/httpd/access.log" type =&gt;"apache"}
}</code></pre> 
<p>修改 Logstash 配置文件，让其收集系统日志/var/log/messages，并将其输出到 elasticsearch 中</p> 
<pre><code class="language-bash">[root@logstash opt]#chmod +r /var/log/messages      #让其他用户Logstash可以读取日志

[root@logstash opt]#vim /etc/logstash/conf.d/system.conf
input {
    file{
        path =&gt;"/var/log/messages"        #指定要收集的日志的位置
        type =&gt;"system"                   #自定义日志类型标识
        start_position =&gt;"beginning"      #表示从开始处收集
    }
}
output {
    elasticsearch {                       #输出到 elasticsearch
        hosts =&gt; ["172.16.12.12:9200"]    #指定 elasticsearch 服务器的地址和端口
        index =&gt;"system-%{+YYYY.MM.dd}"   #指定输出到 elasticsearch 的索引格式
    }
}
[root@logstash opt]#systemctl restart logstash.service</code></pre> 
<p class="img-center"><img alt="" height="377" src="https://images2.imgbox.com/5a/d0/dwf6XZSQ_o.png" width="700"></p> 
<p>浏览器访问 http://172.16.12.12:9100/ 查看索引信息</p> 
<p class="img-center"><img alt="" height="232" src="https://images2.imgbox.com/c6/4d/yj2fQhfx_o.png" width="700"></p> 
<p class="img-center"><img alt="" height="234" src="https://images2.imgbox.com/25/1c/FrfZhZP7_o.png" width="700"></p> 
<h3 id="5%E3%80%81ELK%20Kiabana%20%E9%83%A8%E7%BD%B2">5、ELK Kiabana 部署</h3> 
<p>安装部署在es_node1节点上</p> 
<h4 id="1.1%20%E5%AE%89%E8%A3%85%20Kiabana">1.1 安装 Kiabana</h4> 
<pre><code class="language-bash">[root@es_node1 ~]#cd /opt      #上传软件包 kibana-5.5.1-x86_64.rpm 到/opt目录
[root@es_node1 opt]#rpm -ivh kibana-5.5.1-x86_64.rpm</code></pre> 
<p class="img-center"><img alt="" height="168" src="https://images2.imgbox.com/f2/66/JiDoT529_o.png" width="700"></p> 
<h4 id="1.2%20%E8%AE%BE%E7%BD%AE%20Kibana%20%E7%9A%84%E4%B8%BB%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%B9%B6%E5%90%AF%E5%8A%A8%E6%9C%8D%E5%8A%A1" style="background-color:transparent;">1.2 设置 Kibana 的主配置文件并启动服务</h4> 
<pre><code class="language-bash">[root@es_node1 opt]#vim /etc/kibana/kibana.yml
--2--取消注释，Kiabana 服务的默认监听端口为5601
server.port: 5601
--7--取消注释，设置 Kiabana 的监听地址，0.0.0.0代表所有地址
server.host: "0.0.0.0"
--21--取消注释，设置和 Elasticsearch 建立连接的地址和端口
elasticsearch.url: "http://172.16.12.12:9200" 
--30--取消注释，设置在 elasticsearch 中添加.kibana索引
kibana.index: ".kibana"

[root@es_node1 opt]#systemctl enable --now kibana.service
[root@es_node1 opt]#netstat -natp | grep 5601      #查看5601端口进程</code></pre> 
<p class="img-center"><img alt="" height="278" src="https://images2.imgbox.com/fa/0d/sk94XrRB_o.png" width="700"></p> 
<h4 id="1.3%20%E9%AA%8C%E8%AF%81%20Kibana">1.3 验证 Kibana</h4> 
<p>浏览器访问 http://172.16.12.12:5601<br> 第一次登录需要添加一个 Elasticsearch 索引：<br> Index name or pattern<br> //输入：system-*            #在索引名中输入之前配置的 Output 前缀“system”</p> 
<p>单击 “create” 按钮创建，单击 “Discover” 按钮可查看图表信息及日志信息。</p> 
<p class="img-center"><img alt="" height="348" src="https://images2.imgbox.com/41/14/bEjUNObL_o.png" width="700"></p> 
<p class="img-center"><img alt="" height="330" src="https://images2.imgbox.com/91/77/F0UK3H9g_o.png" width="700"></p> 
<p>数据展示可以分类显示，在“Available Fields”中的“选项”，然后单击 “add”按钮，可以看到按照“选项”筛选后的结果</p> 
<p class="img-center"><img alt="" height="332" src="https://images2.imgbox.com/7e/55/vMNuUuhT_o.png" width="700"></p> 
<h4 id="1.4%C2%A0%E5%B0%86%20Apache%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E6%97%A5%E5%BF%97%EF%BC%88%E8%AE%BF%E9%97%AE%E7%9A%84%E3%80%81%E9%94%99%E8%AF%AF%E7%9A%84%EF%BC%89%E6%B7%BB%E5%8A%A0%E5%88%B0%20Elasticsearch%20%E5%B9%B6%E9%80%9A%E8%BF%87%20Kibana%20%E6%98%BE%E7%A4%BA">1.4 将 Apache 服务器的日志（访问的、错误的）添加到 Elasticsearch 并通过 Kibana 显示</h4> 
<p><strong>在logstash节点服务器上操作：</strong></p> 
<pre><code class="language-bash">[root@logstash ~]#vim /etc/logstash/conf.d/apache_log.conf
input {
    file{
        path =&gt; "/etc/httpd/logs/access_log"
        type =&gt; "access"
        start_position =&gt; "beginning"
    }
    file{
        path =&gt; "/etc/httpd/logs/error_log"
        type =&gt; "error"
        start_position =&gt; "beginning"
    }
}
output {
    if [type] == "access" {
        elasticsearch {
            hosts =&gt; ["172.16.12.12:9200"]
            index =&gt; "apache_access-%{+YYYY.MM.dd}"
        }
    }
        if [type] == "error" {
        elasticsearch {
            hosts =&gt; ["172.16.12.12:9200"]
            index =&gt; "apache_error-%{+YYYY.MM.dd}"
        }
    }
}
</code></pre> 
<p class="img-center"><img alt="" height="432" src="https://images2.imgbox.com/f8/ba/TP2fWuTY_o.png" width="700"></p> 
<pre><code class="language-bash">[root@logstash ~]#cd /etc/logstash/conf.d/
[root@logstash conf.d]#logstash -f apache_log.conf</code></pre> 
<p class="img-center"><img alt="" height="36" src="https://images2.imgbox.com/28/ea/O8iKpJt5_o.png" width="700"></p> 
<p><strong>客户端测试： </strong></p> 
<p>浏览器先访问apache服务器，否则访问 http://172.16.12.12:9100无法有apache_access-2024.04.11索引</p> 
<p class="img-center"><img alt="" height="223" src="https://images2.imgbox.com/87/62/zPQ0alHl_o.png" width="700"></p> 
<p>浏览器访问 http://172.16.12.12:9100 查看索引是否创建</p> 
<p class="img-center"><img alt="" height="521" src="https://images2.imgbox.com/01/14/i4fkcXJC_o.png" width="700"></p> 
<p>浏览器访问 http://192.168.10.13:5601 登录 Kibana，单击“Create Index Pattern”按钮添加索引， 在索引名中输入之前配置的 Output 前缀 apache_access-*，并单击“Create”按钮。在用相同的方法添加 apache_error-*索引</p> 
<p class="img-center"><img alt="" height="245" src="https://images2.imgbox.com/af/11/H6na19IY_o.png" width="700"></p> 
<p class="img-center"><img alt="" height="234" src="https://images2.imgbox.com/ec/98/p4dg9yHT_o.png" width="700"></p> 
<p>选择“Discover”选项卡，在中间下拉列表中选择刚添加的 apache_access-* 、apache_error-* 索引， 可以查看相应的图表及日志信息</p> 
<p><img alt="" height="311" src="https://images2.imgbox.com/ae/2a/XULZt3Wh_o.png" width="869"></p> 
<p class="img-center"><img alt="" height="288" src="https://images2.imgbox.com/a2/19/YzeYuCzI_o.png" width="700"></p> 
<p class="img-center"><img alt="" height="262" src="https://images2.imgbox.com/d3/35/SXpVOhxI_o.png" width="700"></p> 
<h2 id="%E4%B8%89%E3%80%81%E6%90%AD%E5%BB%BAELFK%EF%BC%88Filebeat%2BELK%C2%A0%EF%BC%89">三、搭建ELFK（Filebeat+ELK ）</h2> 
<p><span style="color:#fe2c24;"><strong>filebeat + Logstash + Elasticsearch + Kibana模式</strong></span></p> 
<ul><li>这是一种更加完善和灵活的架构，适合处理复杂的日志数据</li><li>在这种模式下，filebeat(beats)负责收日志文件，并将其发送到Logstash进行处理</li><li>logstash可以对日志数据进行更多的过滤、转换和增强的操作并将其发送到Elasticsearch进行索引</li><li>kibana则可以用来查看和分析日志数据</li></ul> 
<h3>1、环境部署</h3> 
<p> <span style="color:#0d0016;"><strong>在上面ELK搭建成功的环境下，只需要部署安装filebeat</strong></span> </p> 
<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td><strong>服务器</strong></td><td><strong>配置（越高性能越好）</strong></td><td><strong>主机名</strong></td><td><strong>ip地址</strong></td><td><strong>主要软件</strong></td></tr><tr><td>es_node1节点</td><td>2C/4G</td><td>es_node1</td><td>172.16.12.12</td><td>ElasticSearch、Kibana</td></tr><tr><td>es_node2节点</td><td>2C/4G</td><td>es_node2</td><td>172.16.12.13</td><td>ElasticSearch</td></tr><tr><td>logstash节点</td><td>/</td><td>logstash</td><td>172.16.12.10</td><td> <p>Logstash、Apache</p> </td></tr><tr><td>filebeat</td><td>/</td><td>filebeat</td><td>172.16.12.15</td><td>filebeat</td></tr></tbody></table> 
<p><strong> （1）关闭</strong><span style="color:#0d0016;"><strong>filebeat</strong></span> <strong>服务器的防火墙和核心防护</strong></p> 
<pre><code class="language-bash">[root@localhost ~]#systemctl stop firewalld
[root@localhost ~]#setenforce 0</code></pre> 
<p><strong>（2）修改</strong><span style="color:#0d0016;"><strong>filebeat</strong></span><strong>的主机名</strong></p> 
<pre><code class="language-bash">[root@localhost ~]#hostnamectl set-hostname filebeat
[root@localhost ~]#bash</code></pre> 
<blockquote> 
 <p>数据流向： filebeat ----&gt;  logstash ----&gt; ElasticSearch ----&gt; Kibana </p> 
</blockquote> 
<h3 id="2%E3%80%81ELFK%20Filebeat%20%E9%83%A8%E7%BD%B2" style="background-color:transparent;">2、ELFK File<span style="color:#0d0016;"><strong>beat 部署</strong></span></h3> 
<h4 id="1.1%C2%A0%E5%AE%89%E8%A3%85%20Filebeat"><span style="color:#0d0016;"><strong>1.1 </strong>安装 Filebeat</span></h4> 
<pre><code class="language-bash">[root@filebeat ~]#cd /opt #上传软件包 filebeat-6.6.1-x86_64.rpm 到/opt目录
[root@filebeat opt]#rpm -ivh filebeat-6.6.1-x86_64.rpm</code></pre> 
<h4 id="1.2%20%E8%AE%BE%E7%BD%AE%20filebeat%20%E7%9A%84%E4%B8%BB%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6" style="background-color:transparent;"><span style="color:#0d0016;">1.2 设置 filebeat 的主配置文件</span></h4> 
<pre><code class="language-bash">[root@filebeat opt]#vim /etc/filebeat/filebeat.yml
filebeat.prospectors:
- type: log          #指定 log 类型，从日志文件中读取消息
  enabled: true
  paths:
    - /var/log/messages       #指定监控的日志文件
    - /var/log/*.log
  fields:            #可以使用 fields 配置选项设置一些参数字段添加到 output 中
    service_name: filebeat
    log_type: log
    service_id: 172.16.12.15

--------------Elasticsearch output-------------------
(全部注释掉)

----------------Logstash output---------------------
output.logstash:
  hosts: ["172.16.12.10:5044"]      #指定 logstash 的 IP 和端口

#启动filebeat
[root@filebeat opt]#systemctl start filebeat.service 
[root@filebeat opt]#filebeat -e -c  /etc/filebeat/filebeat.yml

#-e: 这是一个参数，表示以交互式模式（interactive mode）启动 Filebeat。在交互式模式下，Filebeat 会将日志输出打印到控制台，方便用户查看实时日志信息
#-c filebeat.yml: 这也是一个参数，指定了 Filebeat 的配置文件。通过指定这个配置文件，Filebeat 将会加载其中定义的配置选项和设置</code></pre> 
<p><img alt="" height="774" src="https://images2.imgbox.com/41/5f/c4PWFgo3_o.png" width="748"></p> 
<h4 id="1.3%20%E5%9C%A8%20Logstash%20%E7%BB%84%E4%BB%B6%E6%89%80%E5%9C%A8%E8%8A%82%E7%82%B9%E4%B8%8A%E6%96%B0%E5%BB%BA%E4%B8%80%E4%B8%AA%20Logstash%20%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span style="color:#0d0016;">1.3 在 Logstash 组件所在节点上新建一个 Logstash 配置文件</span></h4> 
<pre><code class="language-bash">[root@logstash ~]#cd /etc/logstash/conf.d
[root@logstash conf.d]#vim logstash.conf
input {
    beats {
        port =&gt; "5044"
    }
}
output {
    elasticsearch {
        hosts =&gt; ["172.16.12.12:9200"]
        index =&gt; "%{[fields][service_name]}-%{+YYYY.MM.dd}"
    }
    stdout {
        codec =&gt; rubydebug
    }
}

[root@logstash conf.d]#logstash -f logstash.conf   #启动logstash
</code></pre> 
<p class="img-center"><img alt="" height="487" src="https://images2.imgbox.com/16/5e/Hlecq0W2_o.png" width="700"></p> 
<h4 id="1.4%20%E7%99%BB%E5%BD%95%E5%88%B0Kibana%E8%BF%9B%E8%A1%8C%E6%B5%8B%E8%AF%95"><span style="color:#0d0016;">1.4 登录到Kibana进行测试</span></h4> 
<p>浏览器访问 http://172.16.12.12:9100 查看索引是否创建</p> 
<p class="img-center"><img alt="" height="183" src="https://images2.imgbox.com/ad/70/uolYDoI4_o.png" width="700"></p> 
<p><span style="color:#0d0016;">浏览器访问 http://172.16.12.12:5601 登录 Kibana，单击“Create Index Pattern”按钮添加索引“filebeat-*”，单击 “create” 按钮创建，单击 “Discover” 按钮可查看图表信息及日志信息</span></p> 
<p class="img-center"><img alt="" height="240" src="https://images2.imgbox.com/c5/a1/xveTSN9w_o.png" width="700"></p> 
<p class="img-center"><img alt="" height="861" src="https://images2.imgbox.com/9f/ef/kexJGW87_o.png" width="700"></p> 
<h2 id="%E5%9B%9B%E3%80%81%E6%80%BB%E7%BB%93">四、总结</h2> 
<h3 id="1%E3%80%81ELK%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F">1、ELK日志分析系统</h3> 
<p>一套基于Elasticsearch、Logstash、Kibana三个开源的日志收集、存储、检索和可视化的解决方案</p> 
<p>ELK可以帮助用户快速定位和分析应用程序的故障，监控应用程序的性能和安全性，以及提供丰富的数据分析和展示功能</p> 
<ul><li>Elasticsearch：是一个分布式和搜索和分析引擎，它可以对各种类型的数据进行近实时的索引和査询，支持高可用和水平扩展性</li><li>logstash：是一个数据处理管道，它可以从多个来源采集数据，对数据进行过滤、转换和增强，然后将数据发送到Elasticsearch或者其他的目的地</li><li>Kibana:是一个针对glasticsearch的数据可视化平台，它可以通过各种图表、仪表盘和地图来展示和探索Elasticsearch中的数据</li></ul> 
<h3 id="2%E3%80%81feilbeat%20%2B%20Logstash%20%2B%20Elasticsearch%20%2B%20Kibana%E6%A8%A1%E5%BC%8F">2、feilbeat + Logstash + Elasticsearch + Kibana模式</h3> 
<p>这是一种更加完善和灵活的架构，适合处理复杂的日志数据。</p> 
<p>在这种模式下，filebeat(beats)负责收日志文件，并将其发送到Logstash进行处理。</p> 
<p>logstash可以对日志数据进行更多的过滤、转换和增强的操作并将其发送到Elasticsearch进行索引。</p> 
<p>kibana则可以用来查看和分析日志数据</p> 
<h3 id="3%E3%80%81feilbeat%20%2B%20%E7%BC%93%E5%AD%98%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%2B%20Logstash%20%2B%20Elasticsearch%20%2B%20Kibana%20%E6%A8%A1%E5%BC%8F">3、feilbeat + 缓存/消息队列+ Logstash + Elasticsearch + Kibana 模式</h3> 
<p>这是一种加健壮高效的架构，适合处理海量复杂的日志数据。</p> 
<p>在这种模式下，filebeat和iogstach之间加入缓存或消息队列组件，如redis、kafka或RabbitMQ等。</p> 
<p>这样可以降低对目志源主机的影响 ，提高目志传输的稳定性和可靠性，以及实现负载均衡和高可用</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/eabc7fea8b776dd701053d7faafcf29e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">“全网最全”LLM推理框架集结营 | 看似微不足道，却决定着AIGC项目的成本、效率与性能!</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/289c518714dea89d49128ae28cca9a94/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">《大众点评爬虫程序实战：爬取店铺评论信息》</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>