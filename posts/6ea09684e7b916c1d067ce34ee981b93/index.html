<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ChatGLM3-6B使用lora微调实体抽取，工具LLaMA-Factory，医学数据集CMeEE - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/6ea09684e7b916c1d067ce34ee981b93/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="ChatGLM3-6B使用lora微调实体抽取，工具LLaMA-Factory，医学数据集CMeEE">
  <meta property="og:description" content="一、下载ChatGLM3-6B 下载地址，需要魔法
测试模型： 新建文件predict.py。运行下面测试代码。建议这里的transformers包最好和LLaMA-Factory环境的transformers包版本保持一致或者直接用LLaMA-Factory的环境。
from transformers import AutoTokenizer, AutoModelForCausalLM import torch device = &#34;cuda&#34; # the device to load the model onto model = AutoModelForCausalLM.from_pretrained(&#34;./models/chatglm3-6b&#34;,torch_dtype=torch.float16,trust_remote_code=True).half().cuda() tokenizer = AutoTokenizer.from_pretrained(&#34;./models/chatglm3-6b&#34;,trust_remote_code=True) model.to(device) model = model.eval() # response, history = model.chat(tokenizer, &#34;&#34;&#34; 你是谁&#34;&#34;&#34;, history=[]) response, history = model.chat(tokenizer, &#34;&#34;&#34; 从给定文本中提取医疗程序、疾病、症状、肌体、药品、体检项目、微生物、医疗器械、科室这几类的实体名，用字典格式返回。文本:房内折返性心动过速，每年发作1至3次，需药物干预。&#34;&#34;&#34;, history=[]) print(response) 问题：你是谁
答：我是一个名为 ChatGLM3-6B 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。
问题：从给定文本中提取医疗程序、疾病、症状、肌体、药品、体检项目、微生物、医疗器械、科室这几类的实体名，用字典格式返回。文本:房内折返性心动过速，每年发作1至3次，需药物干预。
答：
{
&#34;医疗程序&#34;: [],
&#34;疾病&#34;: [],
&#34;症状&#34;: [],
&#34;肌体&#34;: [],">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-18T15:13:03+08:00">
    <meta property="article:modified_time" content="2024-06-18T15:13:03+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ChatGLM3-6B使用lora微调实体抽取，工具LLaMA-Factory，医学数据集CMeEE</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>一、下载ChatGLM3-6B</h3> 
<p><a class="link-info" href="https://huggingface.co/THUDM/chatglm3-6b/tree/main" rel="nofollow" title="下载地址，需要魔法">下载地址，需要魔法</a></p> 
<h4>测试模型：</h4> 
<p>新建文件predict.py。运行下面测试代码。建议这里的transformers包最好和LLaMA-Factory环境的transformers包版本保持一致或者直接用LLaMA-Factory的环境。</p> 
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

device = "cuda" # the device to load the model onto

model = AutoModelForCausalLM.from_pretrained("./models/chatglm3-6b",torch_dtype=torch.float16,trust_remote_code=True).half().cuda()
tokenizer = AutoTokenizer.from_pretrained("./models/chatglm3-6b",trust_remote_code=True)

model.to(device)
model = model.eval()
# response, history = model.chat(tokenizer, """ 你是谁""", history=[])
response, history = model.chat(tokenizer, """ 从给定文本中提取医疗程序、疾病、症状、肌体、药品、体检项目、微生物、医疗器械、科室这几类的实体名，用字典格式返回。文本:房内折返性心动过速，每年发作1至3次，需药物干预。""", history=[])

print(response)</code></pre> 
<blockquote> 
 <p>问题：你是谁</p> 
 <p>答：我是一个名为 ChatGLM3-6B 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。</p> 
 <p>问题：从给定文本中提取医疗程序、疾病、症状、肌体、药品、体检项目、微生物、医疗器械、科室这几类的实体名，用字典格式返回。文本:房内折返性心动过速，每年发作1至3次，需药物干预。</p> 
 <p>答：</p> 
 <p>{<!-- --><br>   "医疗程序": [],<br>   "疾病": [],<br>   "症状": [],<br>   "肌体": [],<br>   "药品": [],<br>   "体检项目": [],<br>   "微生物": [],<br>   "医疗器械": [],<br>   "科室": []<br> }</p> 
</blockquote> 
<p><strong>模型在微调前几乎提取不了医学中的专业名词 。</strong></p> 
<p>问题中的文本来自CMeEE-V2_dev.json中最后一句。</p> 
<p><img alt="" height="260" src="https://images2.imgbox.com/b7/76/cv64pM2G_o.png" width="378"></p> 
<h3>二、部署LLaMA-Factory</h3> 
<p>按照<a class="link-info" href="https://github.com/hiyouga/LLaMA-Factory" title="LLaMA-Factory">LLaMA-Factory</a>项目中的要求创建好LLaMA-Factory的环境。有问题请放评论。</p> 
<blockquote> 
 <p>启动命令：llamafactory-cli webui</p> 
 <p>打开：本地启动打开： http://0.0.0.0:7860</p> 
 <p>或 服务器启动 服务器地址:7860</p> 
</blockquote> 
<blockquote> 
 <p>如果出现 ：尚不支持多 GPU 训练。</p> 
 <p>切换为单卡启动：CUDA_VISIBLE_DEVICES=0  llamafactory-cli webui</p> 
</blockquote> 
<h3>三、数据集格式转换</h3> 
<p> <a class="link-info" href="https://tianchi.aliyun.com/dataset/144495" rel="nofollow" title="CMeEE下载链接">CMeEE下载链接</a></p> 
<p>将医学数据集CMeEE制作为ChatGLM3-6B的训练数据集。转换代码如下：修改file_path为要转换的医学数据集CMeEE的json文件地址，修改倒数第二行的保存位置。本次实验只选择前200个进行操作，if index==200: break  。 想全部进行转换请注释掉这句。instruction中可自行修改指令。</p> 
<pre><code>import json

# 指定JSON文件的路径
file_path = r'CMeEE-V2_dev.json'

# 读取JSON文件
with open(file_path, 'r', encoding='utf-8') as file:
    datasets = json.load(file)

# print(datasets)

# 定义类型映射
type_mapping = {
    "pro": "医疗程序",
    "dis": "疾病",
    "sym": "症状",
    "bod": "肌体",
    "dru": "药品",
    "ite": "体检项目",
    "mic": "微生物",
    "equ": "医疗器械",
    "dep": "科室"
}

llf_data = []

for index,data in enumerate(datasets):
    # 创建一个包含所有类型的字典，初始值为空列表
    entities_by_type = {value: [] for key, value in type_mapping.items()}

    # 遍历所有实体，将它们按类型分类
    for entity in data["entities"]:
        entity_type = entity["type"]
        if entity_type in type_mapping:
            entities_by_type[type_mapping[entity_type]].append(entity["entity"])

    # 创建输出数据
    txt = data["text"]
    output_data = {
        "instruction": f"从给定文本中提取医疗程序、疾病、症状、肌体、药品、体检项目、微生物、医疗器械、科室这几类的实体名，用字典格式返回。文本:{txt}",
        "input": "",
        "output": f"{entities_by_type}",
        "history": []
    }
    llf_data.append(output_data)

    if index==200:
        break
llf_data = json.dumps(llf_data, ensure_ascii=False, indent=2)
print(llf_data)

with open(r'chatglm3_finetune_data.json', 'w', encoding='utf-8') as f:
    f.write(llf_data)</code></pre> 
<blockquote> 
 <p> 请注意instruction、input、output对应的值全部都要是字符串形式。history的值为列表。否则后续微调时可能会报错：</p> 
</blockquote> 
<pre><code>raise RuntimeError("Cannot find valid samples, check `data/README.md` for the data format.")
RuntimeError: Cannot find valid samples, check `data/README.md` for the data format.
</code></pre> 
<p></p> 
<p>转换完毕后如下形式：</p> 
<p><img alt="" height="544" src="https://images2.imgbox.com/cc/06/FzNrBjee_o.png" width="1200"></p> 
<p>制作好的训练集放在目录LLaMA-Factory/data下，也可以在其他目录下 。然后修改dataset_info.json文件。如果在其他目录，chatglm3_finetune_data.json需改为对应的绝对地址</p> 
<pre>{"chatglm3_finetune_data":{"file_name":"chatglm3_finetune_data.json"}}
</pre> 
<p><img alt="" height="62" src="https://images2.imgbox.com/bf/db/tp4bEmkb_o.png" width="535"></p> 
<h3>四、Lora训练</h3> 
<p>打开llamafactory的webui。</p> 
<p><img alt="" height="851" src="https://images2.imgbox.com/c5/61/bMt1JM6y_o.png" width="1200"></p> 
<p>设置好基本参数。训练30个epoch。如果你的显存较小batchsize无法调大,可以调大梯度累计的值来模拟大的batchsize。</p> 
<p>设置好lora参数为32，秩设置越大、参数更新的数量越多、训练越慢。</p> 
<p><img alt="" height="222" src="https://images2.imgbox.com/95/d8/O1DgAy7n_o.png" width="1200"></p> 
<p>设置保存名称</p> 
<p><img alt="" height="151" src="https://images2.imgbox.com/84/c6/6v2ePBdT_o.png" width="1165"></p> 
<p>点击<strong>开始训练，</strong>等待训练结束。</p> 
<p><img alt="" height="335" src="https://images2.imgbox.com/fb/38/su69rsgf_o.jpg" width="429"></p> 
<p>结果位于saves/ChatGLM3-6B-Chat/lora下。</p> 
<h3>五、微调模型测试</h3> 
<p>新建文件predict_lora.py。运行下面测试代码。</p> 
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from peft import PeftModel


device = "cuda" # the device to load the model onto

model = AutoModelForCausalLM.from_pretrained("./models/chatglm3-6b",torch_dtype=torch.float16,trust_remote_code=True).half().cuda()
tokenizer = AutoTokenizer.from_pretrained("./models/chatglm3-6b",trust_remote_code=True)
p_model = PeftModel.from_pretrained(model, model_id="/LLaMA-Factory/saves/ChatGLM3-6B-Chat/lora/train_2024-06-17-16-55-15")  # 将训练所得的LoRa权重加载起来

#
p_model.to(device)
p_model = p_model.eval()
response, history = p_model.chat(tokenizer, """ 从给定文本中提取医疗程序、疾病、症状、肌体、药品、体检项目、微生物、医疗器械、科室这几类的实体名，用字典格式返回。文本:房内折返性心动过速，每年发作1至3次，需药物干预。""", history=[])
print(response)</code></pre> 
<blockquote> 
 <pre>PeftModel.from_pretrained(model, model_id)中的model_id填入训练好的LORA模型绝对地址

运行结果：{'医疗程序': [], '疾病': ['房内折返性心动过速'], '症状': [], '肌体': [], '药品': [], '体检项目': [], '微生物': [], '医疗器械': [], '科室': []} </pre> 
</blockquote> 
<p>微调后，已经可以提取出疾病的实体名称了。</p> 
<p>合并使用模型时可能发生如下报错：</p> 
<pre><code>config = config_cls(**kwargs)
TypeError: LoraConfig.__init__() got an unexpected keyword argument 'layer_replication'
</code></pre> 
<p> 解决方法：将测试文件中的transformers包和peft包的版本与训练时LLaMA-Factory环境中的transformers包和peft包版本保持一致。</p> 
<p></p> 
<p></p> 
<p><strong>有问题放评论，有用请点赞！</strong> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/526b20da88867d539f2592877db4ee50/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">python字典转json</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/acc7df5cc32ab803b2bbd9329a1bc295/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">论文研读｜针对文生图模型的AIGC检测</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>