<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Hadoop 3.4.0&#43;HBase2.5.8&#43;ZooKeeper3.8.4&#43;Hive4.0&#43;Sqoop 分布式高可用集群部署安装 大数据系列二 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/7ec188da73f3ecbd998b1361d63d0cc3/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Hadoop 3.4.0&#43;HBase2.5.8&#43;ZooKeeper3.8.4&#43;Hive4.0&#43;Sqoop 分布式高可用集群部署安装 大数据系列二">
  <meta property="og:description" content="创建服务器,参考
虚拟机创建服务器
节点名字节点IP系统版本master11192.168.50.11centos 8.5slave12192.168.50.12centos 8.5slave13192.168.50.13centos 8.5 1 下载组件
Hadoop:官网地址
Hbase:官网地址
ZooKeeper:官网下载
Hive:官网下载
Sqoop:官网下载
为方便同学们下载，特整理到网盘
链接地址 提取码：i2ev
2 通过xftp 上传软件到服务器,统一放到/data/soft/
3 配置ZooKeeper
tar zxvf apache-zookeeper-3.8.4-bin.tar.gz mv apache-zookeeper-3.8.4-bin/ /data/zookeeper #修改配置文件 cd /data/zookeeper/conf cp zoo_sample.cfg zoo.cfg #创建数据保存目录 mkdir -p /data/zookeeper/zkdata mkdir -p /data/zookeeper/logs vim zoo.cfg dataDir=/tmp/zookeeper--&gt;dataDir=/data/zookeeper/zkdata dataLogDir=/data/zookeeper/logs server.1=master11:2888:3888 server.2=slave12:2888:3888 server.3=slave13:2888:3888 #配置环境变量 vim /etc/profile export ZooKeeper_HOME=/data/zookeeper export PATH=$PATH:$ZooKeeper_HOME/bin source /etc/profile #新建myid并且写入对应的myid
[root@master11 zkdata]# cat myid 1 #对应修改 slave12 myid--2 slave13 myid--3 4 配置HBase">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-08T19:44:00+08:00">
    <meta property="article:modified_time" content="2024-06-08T19:44:00+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Hadoop 3.4.0&#43;HBase2.5.8&#43;ZooKeeper3.8.4&#43;Hive4.0&#43;Sqoop 分布式高可用集群部署安装 大数据系列二</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>创建服务器,参考</p> 
<p><a class="link-info" href="https://blog.csdn.net/tonyhi6/article/details/138752683?spm=1001.2014.3001.5502" title="虚拟机创建服务器">虚拟机创建服务器</a></p> 
<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td>节点名字</td><td>节点IP</td><td style="width:136px;">系统版本</td><td style="width:92px;"></td><td></td></tr><tr><td>master11</td><td>192.168.50.11</td><td style="width:136px;">centos 8.5</td><td style="width:92px;"></td><td></td></tr><tr><td>slave12</td><td>192.168.50.12</td><td style="width:136px;">centos 8.5</td><td style="width:92px;"></td><td></td></tr><tr><td>slave13</td><td>192.168.50.13</td><td style="width:136px;">centos 8.5</td><td style="width:92px;"></td></tr></tbody></table> 
<p>1 下载组件</p> 
<p>Hadoop:<a class="link-info" href="https://hadoop.apache.org/" rel="nofollow" title="官网地址">官网地址</a></p> 
<p>Hbase:<a class="link-info" href="https://hbase.apache.org/" rel="nofollow" title="官网地址">官网地址</a></p> 
<p>ZooKeeper:<a class="link-info" href="https://zookeeper.apache.org/releases.html" rel="nofollow" title="官网下载">官网下载</a></p> 
<p>Hive:<a class="link-info" href="https://dlcdn.apache.org/hive/" rel="nofollow" title="官网下载">官网下载</a></p> 
<p>Sqoop:<a class="link-info" href="http://archive.apache.org/dist/sqoop/" rel="nofollow" title="官网下载">官网下载</a></p> 
<p>为方便同学们下载，特整理到网盘</p> 
<p><a class="link-info" href="https://pan.baidu.com/s/1r1CjH0S-39Yg16DvAv95rw" rel="nofollow" title="链接地址">链接地址</a> <br> 提取码：i2ev</p> 
<p>2 通过xftp 上传软件到服务器,统一放到/data/soft/</p> 
<p><img alt="" height="125" src="https://images2.imgbox.com/e5/89/79tPTEyA_o.png" width="586"></p> 
<p>3 配置ZooKeeper</p> 
<p></p> 
<pre><code>tar zxvf apache-zookeeper-3.8.4-bin.tar.gz
mv apache-zookeeper-3.8.4-bin/ /data/zookeeper
#修改配置文件
cd /data/zookeeper/conf
cp zoo_sample.cfg zoo.cfg
#创建数据保存目录
mkdir  -p /data/zookeeper/zkdata
mkdir -p /data/zookeeper/logs
vim zoo.cfg
dataDir=/tmp/zookeeper--&gt;dataDir=/data/zookeeper/zkdata
dataLogDir=/data/zookeeper/logs
server.1=master11:2888:3888
server.2=slave12:2888:3888
server.3=slave13:2888:3888

#配置环境变量
vim /etc/profile
export ZooKeeper_HOME=/data/zookeeper
export PATH=$PATH:$ZooKeeper_HOME/bin
source  /etc/profile</code></pre> 
<p>#新建myid并且写入对应的myid</p> 
<pre><code>[root@master11 zkdata]# cat myid 
1
#对应修改
slave12
myid--2
slave13
myid--3</code></pre> 
<p>4  配置HBase</p> 
<pre><code>tar  zxvf  hbase-2.5.8-bin.tar.gz
mv  hbase-2.5.8/ /data/hbase
mkdir -p /data/hbase/logs
#vim /etc/profile
export HBASE_LOG_DIR=/data/hbase/logs
export HBASE_MANAGES_ZK=false
export HBASE_HOME=/data/hbase
export PATH=$PATH:$ZooKeeper_HOME/bin
#vim  /data/hbase/conf/regionservers
slave12
slave13
#新建backup-masters
vim  /data/hbase/conf/backup-masters
slave12
#vim  /data/hbase/conf/hbase-site.xml
 &lt;property&gt;
    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
&lt;!--HBase端口--&gt; 
&lt;property&gt;
 &lt;name&gt;hbase.master.info.port&lt;/name&gt;
 &lt;value&gt;16010&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
    &lt;value&gt;master11,slave12,slave13&lt;/value&gt;
  &lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;hdfs://master11:9000/hbase&lt;/value&gt;
  &lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;hbase.wal.provider&lt;/name&gt;
  &lt;value&gt;filesystem&lt;/value&gt;
&lt;/property&gt;</code></pre> 
<p> 5 配置hadoop</p> 
<pre><code>tar zxvf hadoop-3.4.0.tar.gz
mv  hadoop-3.4.0/ /data/hadoop
#配置环境变量
vim /etc/profile
export HADOOP_HOME=/data/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$PATH:$HADOOP_HOME/sbin
source /etc/profile
#查看版本
[root@master11 soft]# hadoop version
Hadoop 3.4.0
Source code repository git@github.com:apache/hadoop.git -r bd8b77f398f626bb7791783192ee7a5dfaeec760
Compiled by root on 2024-03-04T06:35Z
Compiled on platform linux-x86_64
Compiled with protoc 3.21.12
From source with checksum f7fe694a3613358b38812ae9c31114e
This command was run using /data/hadoop/share/hadoop/common/hadoop-common-3.4.0.jar</code></pre> 
<p>6 修改hadoop配置文件</p> 
<p>#core-site.xml</p> 
<pre><code>vim /data/hadoop/etc/hadoop/core-site.xml
#增加如下
&lt;configuration&gt;
&lt;property&gt;
    &lt;name&gt;fs.defaultFS&lt;/name&gt;
    &lt;value&gt;hdfs://master11&lt;/value&gt;
&lt;/property&gt;
&lt;!-- hadoop 本地数据存储目录 format 时自动生成 --&gt;
&lt;property&gt;
    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
    &lt;value&gt;/data/hadoop/data/tmp&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 在 WebUI访问 HDFS 使用的用户名。--&gt;
&lt;property&gt;
    &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;
    &lt;value&gt;root&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;
  &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;
  &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;
    &lt;value&gt;master11:2181,slave12:2181,slave13:2181&lt;/value&gt;
 &lt;/property&gt;
 &lt;property&gt;
    &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt;
    &lt;value&gt;10000&lt;/value&gt;
 &lt;/property&gt;
&lt;/configuration&gt;
</code></pre> 
<p>#hdfs-site.xml</p> 
<pre><code>vim  /data/hadoop/etc/hadoop/hdfs-site.xml
</code></pre> 
<pre><code>&lt;configuration&gt;

    &lt;!-- 副本数dfs.replication默认值3，可不配置 --&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;3&lt;/value&gt;
    &lt;/property&gt;

    &lt;!-- 节点数据存储地址 --&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
        &lt;value&gt;/data/hadoop/data/dfs/name&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
        &lt;value&gt;/data/hadoop/data/dfs/data&lt;/value&gt;
    &lt;/property&gt;

    &lt;!-- 主备配置 --&gt;
    &lt;!-- 为namenode集群定义一个services name --&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.nameservices&lt;/name&gt;
        &lt;value&gt;mycluster&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 声明集群有几个namenode节点 --&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;
        &lt;value&gt;nn1,nn2&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 指定 RPC通信地址 的地址 --&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;
        &lt;value&gt;master11:8020&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 指定 RPC通信地址 的地址 --&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;
        &lt;value&gt;slave12:8020&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- http通信地址 web端访问地址 --&gt;
    &lt;property&gt;
            &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;
            &lt;value&gt;master11:50070&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- http通信地址 web 端访问地址 --&gt;
    &lt;property&gt;
            &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;
            &lt;value&gt;slave12:50070&lt;/value&gt;
     &lt;/property&gt;

     &lt;!-- 声明journalnode集群服务器 --&gt;
     &lt;property&gt;
            &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;
            &lt;value&gt;qjournal://master11:8485;slave12:8485;slave13:8485/mycluster&lt;/value&gt;
         &lt;/property&gt;
     &lt;!-- 声明journalnode服务器数据存储目录 --&gt;
     &lt;property&gt;
            &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;
            &lt;value&gt;/data/hadoop/data/dfs/jn&lt;/value&gt;
     &lt;/property&gt;
     &lt;!-- 开启NameNode失败自动切换 --&gt;
     &lt;property&gt;
            &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;
            &lt;value&gt;true&lt;/value&gt;
     &lt;/property&gt;
     &lt;!-- 隔离：同一时刻只能有一台服务器对外响应 --&gt;
        &lt;property&gt;
        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;
        &lt;value&gt;
            sshfence
            shell(/bin/true)
        &lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 配置失败自动切换实现方式,通过ConfiguredFailoverProxyProvider这个类实现自动切换 --&gt;
     &lt;property&gt;
            &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;
            &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;
     &lt;/property&gt;

     &lt;!-- 指定上述选项ssh通讯使用的密钥文件在系统中的位置。 --&gt;
     &lt;property&gt;
            &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;
            &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;
      &lt;/property&gt;
      &lt;!-- 配置sshfence隔离机制超时时间（active异常，standby如果没有在30秒之内未连接上，那么standby将变成active） --&gt;
      &lt;property&gt;
            &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;
            &lt;value&gt;30000&lt;/value&gt;
     &lt;/property&gt;
     &lt;property&gt;
       &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;
       &lt;value&gt;sshfence&lt;/value&gt;
     &lt;/property&gt;
&lt;!-- 开启hdfs允许创建目录的权限，配置hdfs-site.xml --&gt;
     &lt;property&gt;
                &lt;name&gt;dfs.permissions.enabled&lt;/name&gt;
                &lt;value&gt;false&lt;/value&gt;
        &lt;/property&gt;
    &lt;!-- 使用host+hostName的配置方式 --&gt;
        &lt;property&gt;
                &lt;name&gt;dfs.namenode.datanode.registration.ip-hostname-check&lt;/name&gt;
                &lt;value&gt;false&lt;/value&gt;
        &lt;/property&gt;
&lt;property&gt;
   &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;!-- 开启自动化： 启动zkfc --&gt;
&lt;property&gt;
   &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;
   &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;ipc.client.connect.max.retries&lt;/name&gt;
    &lt;value&gt;100&lt;/value&gt;
    &lt;description&gt;Indicates the number of retries a client will make to establish a server connection.&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;ipc.client.connect.retry.interval&lt;/name&gt;
    &lt;value&gt;10000&lt;/value&gt;
    &lt;description&gt;Indicates the number of milliseconds a client will wait for before retrying to establish a server connection.&lt;/description&gt;
&lt;/property&gt;

&lt;/configuration&gt;
</code></pre> 
<p> #yarn-site.xml</p> 
<pre><code>vi /data/hadoop/etc/hadoop/yarn-site.xml
&lt;configuration&gt;

 &lt;!-- 指定yarn占电脑资源，默认8核8g --&gt;
 &lt;property&gt;
  &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;
  &lt;value&gt;2&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
  &lt;value&gt;4096&lt;/value&gt;
&lt;/property&gt;

 &lt;property&gt;
    &lt;name&gt;yarn.log.server.url&lt;/name&gt;
    &lt;value&gt;http://node10:19888/jobhistory/logs&lt;/value&gt;
&lt;/property&gt;
    &lt;!-- 指定 MR 走 shuffle --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
        &lt;/property&gt;
    &lt;!-- 开启日志聚集功能 --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 设置日志保留时间为 7 天 --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;
        &lt;value&gt;86400&lt;/value&gt;
    &lt;/property&gt;

    &lt;!-- 主备配置 --&gt;
    &lt;!-- 启用resourcemanager ha --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;
        &lt;value&gt;my-yarn-cluster&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 声明两台resourcemanager的地址 --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;
        &lt;value&gt;rm1,rm2&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;
        &lt;value&gt;slave12&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;
        &lt;value&gt;slave13&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;
        &lt;value&gt;slave12:8088&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;
        &lt;value&gt;slave13:8088&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 指定zookeeper集群的地址 --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;
        &lt;value&gt;master11:2181,slave12:2181,slave13:2181&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 启用自动恢复 --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
   &lt;!-- 指定resourcemanager的状态信息存储在zookeeper集群 --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;
    &lt;/property&gt;


    &lt;property&gt;
        &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;
        &lt;value&gt;2048&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;
        &lt;value&gt;2048&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;
        &lt;value&gt;2.1&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapred.child.java.opts&lt;/name&gt;
        &lt;value&gt;-Xmx1024m&lt;/value&gt;
    &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt;
    &lt;value&gt;slave12:8032&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt;
    &lt;value&gt;slave12:8030&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm1&lt;/name&gt;
    &lt;value&gt;slave12:8031&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.admin.address.rm1&lt;/name&gt;
    &lt;value&gt;slave12:8033&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.address.rm1&lt;/name&gt;
    &lt;value&gt;slave12:8041&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt;
    &lt;value&gt;slave13:8032&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt;
    &lt;value&gt;slave13:8030&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm2&lt;/name&gt;
    &lt;value&gt;slave13:8031&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.admin.address.rm2&lt;/name&gt;
    &lt;value&gt;slave13:8033&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.address.rm2&lt;/name&gt;
    &lt;value&gt;slave13:8041&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.localizer.address&lt;/name&gt;
    &lt;value&gt;0.0.0.0:8040&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
   &lt;description&gt;NM Webapp address.&lt;/description&gt;
    &lt;name&gt;yarn.nodemanager.webapp.address&lt;/name&gt;
    &lt;value&gt;0.0.0.0:8042&lt;/value&gt;
  &lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.address&lt;/name&gt;
    &lt;value&gt;${yarn.resourcemanager.hostname}:8041&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;yarn.application.classpath&lt;/name&gt;
 &lt;value&gt;/data/hadoop/etc/hadoop:/data/hadoop/share/hadoop/common/lib/*:/data/hadoop/share/hadoop/common/*:/data/hadoop/share/hadoop/hdfs:/data/hadoop/share/hadoop/hdfs/lib/*:/data/hadoop/share/hadoop/hdfs/*:/data/hadoop/share/hadoop/mapreduce/lib/*:/data/hadoop/share/hadoop/mapreduce/*:/data/hadoop/share/hadoop/yarn:/data/hadoop/share/hadoop/yarn/lib/*
:/data/hadoop/share/hadoop/yarn/*&lt;/value&gt;    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre> 
<p>#修改workers</p> 
<pre><code>vi /data/hadoop/etc/hadoop/workers
master11
slave12
slave13</code></pre> 
<p>7  分发文件和配置</p> 
<pre><code>#master11
cd /data/  
scp  -r   hadoop/  slave12:/data
scp  -r   hadoop/  slave13:/data
scp  -r  hbase/  slave13:/data
scp  -r  hbase/  slave12:/data
scp  -r   zookeeper/  slave12:/data
scp  -r   zookeeper/  slave13:/data
#3台服务器的/etc/profile 变量一致
export JAVA_HOME=/usr/local/jdk
export PATH=$JAVA_HOME/bin:$PATH
CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export CLASSPATH

export HADOOP_HOME=/data/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$PATH:$HADOOP_HOME/sbin
export ZooKeeper_HOME=/data/zookeeper
export PATH=$PATH:$ZooKeeper_HOME/bin
#
export HBASE_LOG_DIR=/data/hbase/logs
export HBASE_MANAGES_ZK=false
export HBASE_HOME=/data/hbase
export PATH=$PATH:$HBASE_HOME/bin

export HIVE_HOME=/data/hive
export PATH=$PATH:$HIVE_HOME/bin
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
export HDFS_ZKFC_USER=root
export HDFS_DATANODE_SECURE_USER=root
export HDFS_JOURNALNODE_USER=root
</code></pre> 
<p>8 集群启动</p> 
<p>#HA模式第一次或删除在格式化版本</p> 
<pre><code>#第一次需要格式化,master11上面
start-dfs.sh
hdfs  namenode -format
ll /data/hadoop/data/dfs/name/current/
total 16
-rw-r--r--. 1 root root 399 May 13 20:21 fsimage_0000000000000000000
-rw-r--r--. 1 root root  62 May 13 20:21 fsimage_0000000000000000000.md5
-rw-r--r--. 1 root root   2 May 13 20:21 seen_txid
-rw-r--r--. 1 root root 218 May 13 20:21 VERSION
#同步数据到slave12节点(其余namenode节点)
scp  -r  /data/hadoop/data/dfs/name/*  slave12:/data/hadoop/data/dfs/name/
#成功如图</code></pre> 
<p><img alt="" height="186" src="https://images2.imgbox.com/ca/f2/QncXqya7_o.png" width="1008"></p> 
<pre><code>#在任意一台 NameNode上初始化 ZooKeeper 中的 HA 状态
[root@master11 hadoop]# jps
2400 QuorumPeerMain
4897 Jps
3620 JournalNode
3383 DataNode
#
hdfs zkfc -formatZK
#如下图</code></pre> 
<p> <img alt="" height="260" src="https://images2.imgbox.com/cc/3b/7JPV4Nu8_o.png" width="1200"></p> 
<p>#集群正常启动顺序</p> 
<pre><code>#zookeeper,3台服务器都执行
zkServer.sh start
#查看
[root@master11 ~]# zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /data/zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: follower
[root@slave12 data]# zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /data/zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: leader
[root@slave13 ~]# zkServer.sh  status
ZooKeeper JMX enabled by default
Using config: /data/zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: follower
#master11 ,hadoop集群一键启动
start-all.sh start
#一键停止
stop-all.sh
#jps 查看如图</code></pre> 
<p> <img alt="" height="126" src="https://images2.imgbox.com/38/48/k4YfCoPS_o.png" width="403"></p> 
<p><img alt="" height="135" src="https://images2.imgbox.com/10/ae/djto2ZeO_o.png" width="403"></p> 
<p><img alt="" height="103" src="https://images2.imgbox.com/03/61/SJFRijdA_o.png" width="403"></p> 
<p>#查看集群状态</p> 
<pre><code>#NameNode
[root@master11 ~]# hdfs  haadmin  -getServiceState nn1
active
[root@master11 ~]# hdfs  haadmin  -getServiceState nn2
standby
[root@master11 ~]# hdfs haadmin -ns mycluster -getAllServiceState
master11:8020                                      active    
slave12:8020                                       standby
#yarn
[root@master11 ~]# yarn rmadmin -getServiceState rm1
standby
[root@master11 ~]# yarn rmadmin -getServiceState rm2
active
</code></pre> 
<p>#查看HDFS web ui</p> 
<p> <img alt="" height="1088" src="https://images2.imgbox.com/82/cb/Z4qTDnfE_o.png" width="1200"></p> 
<p><img alt="" height="1087" src="https://images2.imgbox.com/0e/9d/ZFFdGeX1_o.png" width="1200"></p> 
<p>#查看 yarn集群</p> 
<p><img alt="" height="521" src="https://images2.imgbox.com/eb/71/2rYh8rTg_o.png" width="1200"></p> 
<p></p> 
<p>9 hadoop 测试使用</p> 
<pre><code>#创建目录
hdfs dchaungfs  -mkdir  /testdata
#查看
[root@master11 ~]# hdfs dfs  -ls /
Found 2 items
drwxr-xr-x   - root supergroup          0 2024-05-14 17:00 /hbase
drwxr-xr-x   - root supergroup          0 2024-05-14 20:32 /testdata
#上传文件
hdfs dfs  -put  jdk-8u191-linux-x64.tar.gz   /testdata
#查看文件
[root@master11 soft]# hdfs dfs  -ls /testdata/
Found 1 items
-rw-r--r--   3 root supergroup  191753373 2024-05-14 20:40 /testdata/jdk-8u191-linux-x64.tar.gz</code></pre> 
<p> <img alt="" height="435" src="https://images2.imgbox.com/f2/fa/iI3UMkOU_o.png" width="1016"></p> 
<p> <img alt="" height="405" src="https://images2.imgbox.com/3a/5d/toF16Z8w_o.png" width="1200"></p> 
<p> <img alt="" height="383" src="https://images2.imgbox.com/3e/7d/71jP6cEq_o.png" width="1200"></p> 
<p>10 启动Hbase,hadoop的active节点</p> 
<pre><code>[root@master11 ~]# hdfs  haadmin  -getServiceState nn1
active
#启动
start-hbase.sh
#查看
[root@master11 ~]# jps
16401 NodeManager
15491 NameNode
21543 HMaster
15848 JournalNode
1435 QuorumPeerMain
16029 DFSZKFailoverController
21902 Jps
15631 DataNode</code></pre> 
<p><img alt="" height="719" src="https://images2.imgbox.com/67/56/y4aCXngR_o.png" width="1200"> 11 安装Hive</p> 
<p>#解压和配置环境变量</p> 
<pre><code>tar zxvf apache-hive-4.0.0-bin.tar.gz
mv  apache-hive-4.0.0-bin/  /data/hive
#环境变量
vi /etc/profile
export HIVE_HOME=/data/hive
export PATH=$PATH:$HIVE_HOME/bin
source /etc/profile</code></pre> 
<p># 安装mysql ,可参考</p> 
<p><a class="link-info" href="https://blog.csdn.net/tonyhi6/article/details/138213266?spm=1001.2014.3001.5502" title="mysql 8.3 二进制版本安装">mysql 8.3 二进制版本安装</a></p> 
<p>#mysql驱动</p> 
<pre><code>mv mysql-connector-java-8.0.29.jar  /data/hive/lib/</code></pre> 
<pre><code>schematool -dbType mysql -initSchema
#报错
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/data/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/data/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Exception in thread "main" [com.ctc.wstx.exc.WstxLazyException] com.ctc.wstx.exc.WstxUnexpectedCharException: Unexpected character '=' (code 61); expected a semi-colon after the reference for entity 'characterEncoding'
 at [row,col,system-id]: [5,86,"file:/data/hive/conf/hive-site.xml"]
	at com.ctc.wstx.exc.WstxLazyException.throwLazily(WstxLazyException.java:40)
	at com.ctc.wstx.sr.StreamScanner.throwLazyError(StreamScanner.java:737)
	at com.ctc.wstx.sr.BasicStreamReader.safeFinishToken(BasicStreamReader.java:3745)
	at com.ctc.wstx.sr.BasicStreamReader.getTextCharacters(BasicStreamReader.java:914)
	at org.apache.hadoop.conf.Configuration$Parser.parseNext(Configuration.java:3434)
	at org.apache.hadoop.conf.Configuration$Parser.parse(Configuration.java:3213)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3106)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3072)
	at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2945)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2927)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1431)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1403)
	at org.apache.hadoop.hive.metastore.conf.MetastoreConf.newMetastoreConf(MetastoreConf.java:2120)
	at org.apache.hadoop.hive.metastore.conf.MetastoreConf.newMetastoreConf(MetastoreConf.java:2072)
	at org.apache.hive.beeline.schematool.HiveSchemaTool.main(HiveSchemaTool.java:144)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:330)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:245)
Caused by: com.ctc.wstx.exc.WstxUnexpectedCharException: Unexpected character '=' (code 61); expected a semi-colon after the reference for entity 'characterEncoding'
 at [row,col,system-id]: [5,86,"file:/data/hive/conf/hive-site.xml"]
	at com.ctc.wstx.sr.StreamScanner.throwUnexpectedChar(StreamScanner.java:666)
	at com.ctc.wstx.sr.StreamScanner.parseEntityName(StreamScanner.java:2080)
	at com.ctc.wstx.sr.StreamScanner.fullyResolveEntity(StreamScanner.java:1538)
	at com.ctc.wstx.sr.BasicStreamReader.readTextSecondary(BasicStreamReader.java:4765)
	at com.ctc.wstx.sr.BasicStreamReader.finishToken(BasicStreamReader.java:3789)
	at com.ctc.wstx.sr.BasicStreamReader.safeFinishToken(BasicStreamReader.java:3743)
	... 18 more
#解决 vi /data/hive/conf/hive-site.xml
&amp;字符 需要转义 改成 &amp;amp;
#成功提示 Initialization script completed
数据库如下图
</code></pre> 
<p> <img alt="" height="489" src="https://images2.imgbox.com/ef/18/lqovdNw5_o.png" width="281"></p> 
<p>#启动，hive 在master11,mysql 安装在slave12 </p> 
<pre><code>cd /data/hive/
nohup hive --service metastore &amp; （启动hive元数据服务）
nohup ./bin/hiveserver2 &amp; （启动jdbc连接服务）
#直接hive,提示“No current connection”
hive
[root@master11 hive]# hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/data/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/data/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/data/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/data/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Beeline version 4.0.0 by Apache Hive
beeline&gt; show  databases;
No current connection
beeline&gt; 
#在提示符 输入!connect jdbc:hive2://master11:10000，之后输入mysql用户和密码
beeline&gt; !connect jdbc:hive2://master11:10000
Connecting to jdbc:hive2://master11:10000
Enter username for jdbc:hive2://master11:10000: root
Enter password for jdbc:hive2://master11:10000: *********
Connected to: Apache Hive (version 4.0.0)
Driver: Hive JDBC (version 4.0.0)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://master11:10000&gt; show  databases;
INFO  : Compiling command(queryId=root_20240514222349_ac19af6a-3c43-49fd-bcd0-25fc0e5b76c6): show  databases
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Created Hive schema: Schema(fieldSchemas:[FieldSchema(name:database_name, type:string, comment:from deserializer)], properties:null)
INFO  : Completed compiling command(queryId=root_20240514222349_ac19af6a-3c43-49fd-bcd0-25fc0e5b76c6); Time taken: 0.021 seconds
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Executing command(queryId=root_20240514222349_ac19af6a-3c43-49fd-bcd0-25fc0e5b76c6): show  databases
INFO  : Starting task [Stage-0:DDL] in serial mode
INFO  : Completed executing command(queryId=root_20240514222349_ac19af6a-3c43-49fd-bcd0-25fc0e5b76c6); Time taken: 0.017 seconds
+----------------+
| database_name  |
+----------------+
| default        |
+----------------+
1 row selected (0.124 seconds)
0: jdbc:hive2://master11:10000&gt;</code></pre> 
<p>Hadoop3.4.0+HBase2.5.8+ZooKeeper3.8.4+Hive4.0+Sqoop 分布式高可用集群部署安装,已完成。欢迎大家一起交流哦。下一篇，项目实战。</p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/548fd7b52ebeda464f632a8d627f8a83/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">ChatGPT Prompt技术全攻略-探索篇：前沿Prompt工程技术</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2e5b201309532ae87983693d31f48fc7/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">三端植物大战僵尸杂交版来了</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>