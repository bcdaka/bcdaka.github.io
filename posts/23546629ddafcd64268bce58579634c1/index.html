<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>人工智能深度学习系列—深入解析：均方误差损失（MSE Loss）在深度学习中的应用与实践 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/23546629ddafcd64268bce58579634c1/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="人工智能深度学习系列—深入解析：均方误差损失（MSE Loss）在深度学习中的应用与实践">
  <meta property="og:description" content="人工智能深度学习系列—深度解析：交叉熵损失（Cross-Entropy Loss）在分类问题中的应用
人工智能深度学习系列—深入解析：均方误差损失（MSE Loss）在深度学习中的应用与实践
人工智能深度学习系列—深入探索KL散度：度量概率分布差异的关键工具
人工智能深度学习系列—探索余弦相似度损失：深度学习中的相似性度量神器
人工智能深度学习系列—深度学习中的边界框回归新贵：GHM（Generalized Histogram Loss）全解析
人工智能深度学习系列—深度学习损失函数中的Focal Loss解析
人工智能深度学习系列—Wasserstein Loss：度量概率分布差异的新视角
人工智能深度学习系列—GANs的对抗博弈：深入解析Adversarial Loss
人工智能深度学习系列—探索Jaccard相似度损失：图像分割领域的新利器
人工智能深度学习系列—深入探索IoU Loss及其变种：目标检测与分割的精度优化利器
人工智能深度学习系列—深度学习中的相似性追求：Triplet Loss 全解析
文章目录 1. 背景介绍2. Loss计算公式3. 使用场景使用场景扩展 4. 代码样例5. 总结 1. 背景介绍 在深度学习的世界里，损失函数犹如一把尺子，衡量着模型预测与实际结果之间的差距。均方误差损失（Mean Squared Error Loss，简称MSE Loss）作为回归问题中的常见损失函数，以其简单直观的特点，广泛应用于各种预测任务。本文将带您深入了解MSE Loss的背景、计算方法、使用场景以及如何在实际代码中应用它。
在机器学习中，损失函数是衡量模型性能的关键指标，它反映了模型预测值与真实值之间的差异。对于回归问题，我们的目标是最小化预测值与实际值之间的误差，而MSE Loss正是为此设计的。它通过计算预测值与真实值差的平方的平均值，为我们提供了一个量化误差的方法。
2. Loss计算公式 MSE Loss的计算公式如下：
MSE Loss = 1 n ∑ i = 1 n ( y i − y ^ i ) 2 \text{MSE Loss} = \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 MSE Loss=n1​∑i=1n​(yi​−y^​i​)2">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-06T20:03:31+08:00">
    <meta property="article:modified_time" content="2024-08-06T20:03:31+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">人工智能深度学习系列—深入解析：均方误差损失（MSE Loss）在深度学习中的应用与实践</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><a href="https://blog.csdn.net/u013889591/article/details/140820960?spm=1001.2014.3001.5501">人工智能深度学习系列—深度解析：交叉熵损失（Cross-Entropy Loss）在分类问题中的应用</a><br> <a href="https://blog.csdn.net/u013889591/article/details/140820368?spm=1001.2014.3001.5501">人工智能深度学习系列—深入解析：均方误差损失（MSE Loss）在深度学习中的应用与实践</a><br> <a href="https://blog.csdn.net/u013889591/article/details/140841744?spm=1001.2014.3001.5501">人工智能深度学习系列—深入探索KL散度：度量概率分布差异的关键工具</a><br> <a href="https://blog.csdn.net/u013889591/article/details/140869603?spm=1001.2014.3001.5501">人工智能深度学习系列—探索余弦相似度损失：深度学习中的相似性度量神器</a><br> <a href="https://blog.csdn.net/u013889591/article/details/140869464?spm=1001.2014.3001.5501">人工智能深度学习系列—深度学习中的边界框回归新贵：GHM（Generalized Histogram Loss）全解析</a><br> <a href="https://blog.csdn.net/u013889591/article/details/140860711?spm=1001.2014.3001.5501">人工智能深度学习系列—深度学习损失函数中的Focal Loss解析</a><br> <a href="https://blog.csdn.net/u013889591/article/details/140897171?spm=1001.2014.3001.5501">人工智能深度学习系列—Wasserstein Loss：度量概率分布差异的新视角</a><br> <a href="https://blog.csdn.net/u013889591/article/details/140897007?spm=1001.2014.3001.5501">人工智能深度学习系列—GANs的对抗博弈：深入解析Adversarial Loss</a><br> <a href="https://blog.csdn.net/u013889591/article/details/140886811?spm=1001.2014.3001.5501">人工智能深度学习系列—探索Jaccard相似度损失：图像分割领域的新利器</a><br> <a href="https://blog.csdn.net/u013889591/article/details/140888673?spm=1001.2014.3001.5501">人工智能深度学习系列—深入探索IoU Loss及其变种：目标检测与分割的精度优化利器</a><br> <a href="https://blog.csdn.net/u013889591/article/details/140869737?spm=1001.2014.3001.5501">人工智能深度学习系列—深度学习中的相似性追求：Triplet Loss 全解析</a></p> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#1__15" rel="nofollow">1. 背景介绍</a></li><li><a href="#2_Loss_20" rel="nofollow">2. Loss计算公式</a></li><li><a href="#3__27" rel="nofollow">3. 使用场景</a></li><li><ul><li><ul><li><a href="#_29" rel="nofollow">使用场景扩展</a></li></ul> 
  </li></ul> 
  </li><li><a href="#4__49" rel="nofollow">4. 代码样例</a></li><li><a href="#5__74" rel="nofollow">5. 总结</a></li></ul> 
</div> 
<p></p> 
<h2><a id="1__15"></a>1. 背景介绍</h2> 
<p>在深度学习的世界里，损失函数犹如一把尺子，衡量着模型预测与实际结果之间的差距。均方误差损失（Mean Squared Error Loss，简称MSE Loss）作为回归问题中的常见损失函数，以其简单直观的特点，广泛应用于各种预测任务。本文将带您深入了解MSE Loss的背景、计算方法、使用场景以及如何在实际代码中应用它。</p> 
<p>在机器学习中，损失函数是衡量模型性能的关键指标，它反映了模型预测值与真实值之间的差异。对于回归问题，我们的目标是最小化预测值与实际值之间的误差，而MSE Loss正是为此设计的。它通过计算预测值与真实值差的平方的平均值，为我们提供了一个量化误差的方法。<br> <img src="https://images2.imgbox.com/99/fb/XAS4meeG_o.jpg" alt="在这里插入图片描述" height="300"></p> 
<h2><a id="2_Loss_20"></a>2. Loss计算公式</h2> 
<p>MSE Loss的计算公式如下：<br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         MSE Loss 
        
       
         = 
        
        
        
          1 
         
        
          n 
         
        
        
        
          ∑ 
         
         
         
           i 
          
         
           = 
          
         
           1 
          
         
        
          n 
         
        
       
         ( 
        
        
        
          y 
         
        
          i 
         
        
       
         − 
        
        
         
         
           y 
          
         
           ^ 
          
         
        
          i 
         
        
        
        
          ) 
         
        
          2 
         
        
       
      
        \text{MSE Loss} = \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord text"><span class="mord">MSE Loss</span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.1901em; vertical-align: -0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8451em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position: relative; top: 0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8043em;"><span class="" style="top: -2.4003em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.2029em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2997em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.0641em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6944em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em;"><span class=""></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><br> 其中：</p> 
<ul><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          n 
         
        
       
         n 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span></span> 是样本数量。</li><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           y 
          
         
           i 
          
         
        
       
         y_i 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 是第<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          i 
         
        
       
         i 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span></span>个样本的真实值。</li><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
          
          
            y 
           
          
            ^ 
           
          
         
           i 
          
         
        
       
         \hat{y}_i 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6944em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em;"><span class=""></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>是第<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          i 
         
        
       
         i 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span></span>个样本的预测值。</li></ul> 
<h2><a id="3__27"></a>3. 使用场景</h2> 
<h4><a id="_29"></a>使用场景扩展</h4> 
<p>均方误差损失（MSE Loss）作为一种直观且计算效率高的损失函数，在深度学习及传统统计学领域中有着广泛的应用。以下是对MSE Loss使用场景的进一步扩展：</p> 
<ul><li><strong>线性回归（Linear Regression）</strong> 
  <ul><li>在线性回归任务中，MSE Loss通过最小化预测值与实际值之间平方差的平均，引导模型学习到数据的最佳拟合线。这种应用场景非常广泛，包括但不限于房价预测、气温变化、销售额分析等，任何需要预测连续数值的场景都可能受益于MSE Loss。</li></ul> </li><li><strong>时间序列预测（Time Series Forecasting）</strong> 
  <ul><li>时间序列预测是MSE Loss的另一个重要应用领域。在金融领域，它可以用来预测股票市场的趋势；在气象学中，它可以用于预测天气变化；在供应链管理中，它还可以帮助预测产品的需求波动。MSE Loss能够量化预测序列与实际观测序列之间的误差，为模型提供有效的反馈。</li></ul> </li><li><strong>参数估计（Parameter Estimation）</strong> 
  <ul><li>在统计学中，MSE Loss是估计模型参数时常用的准则之一。例如，在最小二乘法中，通过最小化MSE Loss，可以找到最佳拟合参数，确保模型预测与实际观测之间的差异最小。这在构建线性模型、多项式回归或其他形式的参数化模型时尤为重要。</li></ul> </li><li><strong>机器学习模型的正则化（Regularization in Machine Learning）</strong> 
  <ul><li>MSE Loss不仅用于直接的预测误差度量，还可以作为正则化项的一部分，帮助控制模型的复杂度，防止过拟合。例如，在岭回归（Ridge Regression）和套索回归（Lasso Regression）中，MSE Loss与正则化项结合使用，以达到模型优化的目的。</li></ul> </li><li><strong>计算机视觉（Computer Vision）</strong> 
  <ul><li>在计算机视觉领域，MSE Loss可以用于像素级的预测任务，如图像重建、去噪、图像超分辨率等。尽管在这些任务中MSE Loss可能不是首选损失函数，但在某些情况下，它仍然提供了一种简单有效的误差度量方式。</li></ul> </li><li><strong>强化学习（Reinforcement Learning）</strong> 
  <ul><li>在强化学习中，MSE Loss有时被用作价值函数或策略的损失函数，特别是在连续动作空间的设置中。通过最小化MSE Loss，可以帮助智能体学习如何根据环境状态做出决策。</li></ul> </li><li><strong>信号处理（Signal Processing）</strong> 
  <ul><li>在信号处理领域，MSE Loss用于评估信号重建的质量，如在滤波、去噪和信号估计等任务中。它提供了一种量化信号失真程度的方法。</li></ul> </li><li><strong>推荐系统（Recommender Systems）</strong> 
  <ul><li>在推荐系统中，MSE Loss可以用于评估预测评分与实际评分之间的差异，帮助模型学习更准确的用户偏好。</li></ul> </li></ul> 
<p>MSE Loss因其简单性、直观性和易于优化的特性，在多个领域内都有着不可或缺的作用。然而，它对异常值敏感，这可能在某些情况下导致问题。因此，在使用MSE Loss时，需要根据具体问题和数据特性综合考虑，有时还需要与其他损失函数结合使用，以达到更好的效果。</p> 
<h2><a id="4__49"></a>4. 代码样例</h2> 
<p>以下是使用Python和PyTorch库实现MSE Loss的示例代码：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 假设我们有一些预测值和真实值</span>
predictions <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2.5</span><span class="token punctuation">,</span> <span class="token number">3.0</span><span class="token punctuation">,</span> <span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
targets <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span> <span class="token number">3.5</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 定义MSE Loss</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 计算损失</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> targets<span class="token punctuation">)</span>

<span class="token comment"># 打印损失值</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Loss:"</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 反向传播，计算梯度</span>
loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 打印梯度</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Gradients:"</span><span class="token punctuation">,</span> predictions<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
</code></pre> 
<h2><a id="5__74"></a>5. 总结</h2> 
<p>均方误差损失（MSE Loss）以其直观的计算方式和广泛的适用性，在深度学习回归任务中占据了重要地位。然而，MSE Loss也有一些局限性，比如对异常值敏感，这可能在某些情况下导致模型性能受影响。因此，在实际应用中，我们可能需要根据具体问题选择或设计更合适的损失函数。无论如何，MSE Loss都是深度学习从业者必须掌握的基本工具之一。<br> <img src="https://images2.imgbox.com/4a/17/ypbThR8O_o.jpg" alt="在这里插入图片描述" height="300"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/76d607e22194183a99f08235cca38d34/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">C＋＋入门基础（二）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d7027ee1adc8b4959db3c2e875c9af3b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">过拟合与欠拟合</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>