<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>半监督学习概念与算法精讲 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/7fd5c0969a0b7b8f662dd868c227db78/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="半监督学习概念与算法精讲">
  <meta property="og:description" content="本文详细介绍常见的半监督学习算法及其实现，包括图形半监督学习、自训练、一致性正则化和生成对抗网络（GANs），并通过代码实战展示其具体应用。
关注TechLead，复旦AI博士，分享AI领域全维度知识与研究。拥有10&#43;年AI领域研究经验、复旦机器人智能实验室成员，国家级大学生赛事评审专家，发表多篇SCI核心期刊学术论文，上亿营收AI产品研发负责人。
一、半监督学习概念 引言 在机器学习领域，数据是驱动模型训练的核心资源。然而，获取大量带标签的数据往往是昂贵且耗时的过程。半监督学习（Semi-Supervised Learning, SSL）通过利用大量未标记的数据和少量标记的数据，有效地缓解了这一问题。SSL不仅能够减少对标记数据的依赖，还能够在许多实际应用中提升模型的性能。
监督学习与无监督学习的比较 监督学习（Supervised Learning）依赖于大量带标签的数据来训练模型。这种方法的优势在于其明确的目标和易于评估的性能。然而，获取大规模标记数据集的成本高昂。
无监督学习（Unsupervised Learning）不依赖于带标签的数据，而是试图通过数据的内部结构和分布来学习模式。无监督学习的典型应用包括聚类和降维，但其缺点是难以直接评估和解释结果。
半监督学习位于监督学习和无监督学习之间，通过结合少量标记数据和大量未标记数据来构建模型。其目标是充分利用未标记数据的信息，提升模型的泛化能力。
半监督学习的定义和基本框架 定义：半监督学习是一种学习范式，旨在通过使用大量未标记数据和少量标记数据来训练模型。在许多实际应用中，获取未标记数据相对容易，而标记数据则相对稀缺且昂贵。SSL正是利用这种数据不平衡来提高学习效率和模型性能。
基本框架：半监督学习通常包括以下几个关键步骤：
数据准备：将数据集划分为标记数据和未标记数据。标记数据通常用于监督训练，而未标记数据用于引导模型的学习过程。
模型训练：通过联合使用标记数据和未标记数据进行模型训练。常见的方法包括自训练、图形半监督学习、一致性正则化和生成对抗网络等。
模型评估：使用独立的验证集或交叉验证来评估模型性能，确保模型能够有效利用未标记数据提升性能。
半监督学习的优势 减少标记需求：SSL能够有效减少对大规模标记数据的需求，降低数据获取的成本和时间。
提升模型性能：通过利用大量未标记数据，SSL可以捕捉到更多的数据分布信息，提升模型的泛化能力和鲁棒性。
应用广泛：SSL在许多实际场景中具有广泛的应用，包括图像分类、自然语言处理、语音识别等领域。
半监督学习的挑战 尽管半监督学习具有显著的优势，但在实际应用中仍然面临一些挑战：
模型复杂性：结合标记和未标记数据的训练过程可能导致模型复杂性增加，要求更高的计算资源和优化策略。
数据质量：未标记数据的质量和分布对于SSL的效果至关重要。低质量或噪声数据可能会对模型产生负面影响。
算法设计：设计有效的SSL算法需要在利用未标记数据和避免过拟合之间找到平衡，这对算法的设计和实现提出了更高的要求。
二、常见的半监督学习算法 半监督学习（Semi-Supervised Learning, SSL）通过利用大量未标记数据和少量标记数据，有效地提升了模型的性能和泛化能力。以下是一些常见且重要的半监督学习算法，每种方法在实际应用中都有其独特的优势和适用场景。
图形半监督学习（Graph-Based Semi-Supervised Learning） 图形半监督学习通过构建数据点之间的图结构，利用图上的连接关系来传播标签信息。其核心思想是相似的数据点在图结构中通常会有较强的连接，因此可以通过未标记数据点与标记数据点的连接关系来推断未标记数据点的标签。
典型方法 图正则化（Graph Regularization）：通过在监督学习的目标函数中添加图正则项，使得相邻数据点的标签更趋于一致。例如，拉普拉斯正则化（Laplacian Regularization）是常用的图正则化方法之一。
图嵌入（Graph Embedding）：将数据点嵌入到低维空间中，使得相似的数据点在低维空间中的距离较近。例如，拉普拉斯特征映射（Laplacian Eigenmaps）和局部线性嵌入（Locally Linear Embedding, LLE）等方法。
自训练（Self-Training） 自训练是一种简单而有效的半监督学习方法。它通过迭代的方式，逐步将高置信度的未标记数据加入到标记数据集中，重新训练模型，从而逐步提升模型性能。
步骤 初始训练：使用初始的标记数据训练一个基础模型。标签预测：使用训练好的模型对未标记数据进行预测，选取高置信度的预测结果。标签扩展：将高置信度的未标记数据及其预测标签加入到标记数据集中。迭代训练：使用扩展后的标记数据集重新训练模型，重复上述步骤直到收敛。 优势与挑战 自训练方法简单易行，适用于多种模型和任务。然而，它依赖于初始模型的性能，如果初始模型性能较差，可能会引入错误的标签，导致错误的积累和模型性能下降。
一致性正则化（Consistency Regularization） 一致性正则化方法通过施加扰动，使模型对输入数据的微小变化保持鲁棒性。其核心思想是模型在面对相似的输入时应该产生相似的输出。
典型方法 Π模型（Π Model）：对输入数据添加噪声，通过最小化同一数据点不同扰动下的预测输出之间的差异来训练模型。
Mean Teacher：使用一个教师模型和一个学生模型，教师模型的参数是学生模型参数的指数滑动平均（EMA）。学生模型对输入数据添加噪声，教师模型对原始输入进行预测，通过最小化两者的预测差异来训练学生模型。
生成对抗网络（GANs）在半监督学习中的应用 生成对抗网络（Generative Adversarial Networks, GANs）通过生成器和判别器之间的对抗训练来生成逼真的数据样本。在半监督学习中，GANs不仅可以生成未标记数据，还可以利用生成的样本来改进分类器的性能。
典型方法 Semi-Supervised GAN：在标准GAN框架中，判别器不仅区分真实数据和生成数据，还将真实数据分为标记类别。生成器通过生成逼真的数据来欺骗判别器，从而提升分类器的性能。
Triple GAN：引入第三个网络——分类器，与生成器和判别器一起进行对抗训练，生成器生成的样本不仅需要欺骗判别器，还需要使分类器的预测结果与真实标签一致。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-17T10:04:54+08:00">
    <meta property="article:modified_time" content="2024-07-17T10:04:54+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">半监督学习概念与算法精讲</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>本文详细介绍常见的半监督学习算法及其实现，包括图形半监督学习、自训练、一致性正则化和生成对抗网络（GANs），并通过代码实战展示其具体应用。</p> 
</blockquote> 
<blockquote> 
 <p>关注TechLead，复旦AI博士，分享AI领域全维度知识与研究。拥有10+年AI领域研究经验、复旦机器人智能实验室成员，国家级大学生赛事评审专家，发表多篇SCI核心期刊学术论文，上亿营收AI产品研发负责人。</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/e1/22/AR8oyLIU_o.png" alt="file"></p> 
<h3><a id="_6"></a>一、半监督学习概念</h3> 
<h4><a id="_8"></a>引言</h4> 
<p>在机器学习领域，数据是驱动模型训练的核心资源。然而，获取大量带标签的数据往往是昂贵且耗时的过程。半监督学习（Semi-Supervised Learning, SSL）通过利用大量未标记的数据和少量标记的数据，有效地缓解了这一问题。SSL不仅能够减少对标记数据的依赖，还能够在许多实际应用中提升模型的性能。</p> 
<h4><a id="_12"></a>监督学习与无监督学习的比较</h4> 
<p><img src="https://images2.imgbox.com/bf/bb/PsrtAWuO_o.png" alt="file"></p> 
<p><strong>监督学习</strong>（Supervised Learning）依赖于大量带标签的数据来训练模型。这种方法的优势在于其明确的目标和易于评估的性能。然而，获取大规模标记数据集的成本高昂。</p> 
<p><strong>无监督学习</strong>（Unsupervised Learning）不依赖于带标签的数据，而是试图通过数据的内部结构和分布来学习模式。无监督学习的典型应用包括聚类和降维，但其缺点是难以直接评估和解释结果。</p> 
<p>半监督学习位于监督学习和无监督学习之间，通过结合少量标记数据和大量未标记数据来构建模型。其目标是充分利用未标记数据的信息，提升模型的泛化能力。</p> 
<h4><a id="_21"></a>半监督学习的定义和基本框架</h4> 
<p><strong>定义</strong>：半监督学习是一种学习范式，旨在通过使用大量未标记数据和少量标记数据来训练模型。在许多实际应用中，获取未标记数据相对容易，而标记数据则相对稀缺且昂贵。SSL正是利用这种数据不平衡来提高学习效率和模型性能。</p> 
<p><strong>基本框架</strong>：半监督学习通常包括以下几个关键步骤：</p> 
<ol><li> <p><strong>数据准备</strong>：将数据集划分为标记数据和未标记数据。标记数据通常用于监督训练，而未标记数据用于引导模型的学习过程。</p> </li><li> <p><strong>模型训练</strong>：通过联合使用标记数据和未标记数据进行模型训练。常见的方法包括自训练、图形半监督学习、一致性正则化和生成对抗网络等。</p> </li><li> <p><strong>模型评估</strong>：使用独立的验证集或交叉验证来评估模型性能，确保模型能够有效利用未标记数据提升性能。</p> </li></ol> 
<h4><a id="_33"></a>半监督学习的优势</h4> 
<ol><li> <p><strong>减少标记需求</strong>：SSL能够有效减少对大规模标记数据的需求，降低数据获取的成本和时间。</p> </li><li> <p><strong>提升模型性能</strong>：通过利用大量未标记数据，SSL可以捕捉到更多的数据分布信息，提升模型的泛化能力和鲁棒性。</p> </li><li> <p><strong>应用广泛</strong>：SSL在许多实际场景中具有广泛的应用，包括图像分类、自然语言处理、语音识别等领域。</p> </li></ol> 
<h4><a id="_41"></a>半监督学习的挑战</h4> 
<p>尽管半监督学习具有显著的优势，但在实际应用中仍然面临一些挑战：</p> 
<ol><li> <p><strong>模型复杂性</strong>：结合标记和未标记数据的训练过程可能导致模型复杂性增加，要求更高的计算资源和优化策略。</p> </li><li> <p><strong>数据质量</strong>：未标记数据的质量和分布对于SSL的效果至关重要。低质量或噪声数据可能会对模型产生负面影响。</p> </li><li> <p><strong>算法设计</strong>：设计有效的SSL算法需要在利用未标记数据和避免过拟合之间找到平衡，这对算法的设计和实现提出了更高的要求。</p> </li></ol> 
<h3><a id="_51"></a>二、常见的半监督学习算法</h3> 
<p>半监督学习（Semi-Supervised Learning, SSL）通过利用大量未标记数据和少量标记数据，有效地提升了模型的性能和泛化能力。以下是一些常见且重要的半监督学习算法，每种方法在实际应用中都有其独特的优势和适用场景。</p> 
<h4><a id="GraphBased_SemiSupervised_Learning_55"></a>图形半监督学习（Graph-Based Semi-Supervised Learning）</h4> 
<p>图形半监督学习通过构建数据点之间的图结构，利用图上的连接关系来传播标签信息。其核心思想是相似的数据点在图结构中通常会有较强的连接，因此可以通过未标记数据点与标记数据点的连接关系来推断未标记数据点的标签。</p> 
<h5><a id="_59"></a>典型方法</h5> 
<ol><li> <p><strong>图正则化（Graph Regularization）</strong>：通过在监督学习的目标函数中添加图正则项，使得相邻数据点的标签更趋于一致。例如，拉普拉斯正则化（Laplacian Regularization）是常用的图正则化方法之一。</p> </li><li> <p><strong>图嵌入（Graph Embedding）</strong>：将数据点嵌入到低维空间中，使得相似的数据点在低维空间中的距离较近。例如，拉普拉斯特征映射（Laplacian Eigenmaps）和局部线性嵌入（Locally Linear Embedding, LLE）等方法。</p> </li></ol> 
<h4><a id="SelfTraining_65"></a>自训练（Self-Training）</h4> 
<p>自训练是一种简单而有效的半监督学习方法。它通过迭代的方式，逐步将高置信度的未标记数据加入到标记数据集中，重新训练模型，从而逐步提升模型性能。</p> 
<h5><a id="_69"></a>步骤</h5> 
<ol><li><strong>初始训练</strong>：使用初始的标记数据训练一个基础模型。</li><li><strong>标签预测</strong>：使用训练好的模型对未标记数据进行预测，选取高置信度的预测结果。</li><li><strong>标签扩展</strong>：将高置信度的未标记数据及其预测标签加入到标记数据集中。</li><li><strong>迭代训练</strong>：使用扩展后的标记数据集重新训练模型，重复上述步骤直到收敛。</li></ol> 
<h5><a id="_76"></a>优势与挑战</h5> 
<p>自训练方法简单易行，适用于多种模型和任务。然而，它依赖于初始模型的性能，如果初始模型性能较差，可能会引入错误的标签，导致错误的积累和模型性能下降。</p> 
<h4><a id="Consistency_Regularization_80"></a>一致性正则化（Consistency Regularization）</h4> 
<p>一致性正则化方法通过施加扰动，使模型对输入数据的微小变化保持鲁棒性。其核心思想是模型在面对相似的输入时应该产生相似的输出。</p> 
<h5><a id="_84"></a>典型方法</h5> 
<ol><li> <p><strong>Π模型（Π Model）</strong>：对输入数据添加噪声，通过最小化同一数据点不同扰动下的预测输出之间的差异来训练模型。</p> </li><li> <p><strong>Mean Teacher</strong>：使用一个教师模型和一个学生模型，教师模型的参数是学生模型参数的指数滑动平均（EMA）。学生模型对输入数据添加噪声，教师模型对原始输入进行预测，通过最小化两者的预测差异来训练学生模型。</p> </li></ol> 
<h4><a id="GANs_90"></a>生成对抗网络（GANs）在半监督学习中的应用</h4> 
<p>生成对抗网络（Generative Adversarial Networks, GANs）通过生成器和判别器之间的对抗训练来生成逼真的数据样本。在半监督学习中，GANs不仅可以生成未标记数据，还可以利用生成的样本来改进分类器的性能。</p> 
<h5><a id="_94"></a>典型方法</h5> 
<ol><li> <p><strong>Semi-Supervised GAN</strong>：在标准GAN框架中，判别器不仅区分真实数据和生成数据，还将真实数据分为标记类别。生成器通过生成逼真的数据来欺骗判别器，从而提升分类器的性能。</p> </li><li> <p><strong>Triple GAN</strong>：引入第三个网络——分类器，与生成器和判别器一起进行对抗训练，生成器生成的样本不仅需要欺骗判别器，还需要使分类器的预测结果与真实标签一致。</p> </li></ol> 
<h4><a id="_100"></a>小结</h4> 
<p>常见的半监督学习算法各有千秋，适用于不同的应用场景和数据特征。图形半监督学习通过数据点之间的连接关系传播标签信息；自训练方法简单易行，适用于多种任务；一致性正则化通过增强模型鲁棒性提升性能；GANs 在半监督学习中的应用充分利用生成数据来改进分类器。这些方法在实际应用中常常结合使用，以期获得更好的模型性能和泛化能力。</p> 
<h3><a id="_104"></a>三、常见的半监督学习算法代码实战</h3> 
<p>在本章节中，我们将通过代码实战展示如何使用 PyTorch 实现常见的半监督学习算法，包括图形半监督学习、自训练、一致性正则化和生成对抗网络（GANs）。我们将通过详细的代码和注释，帮助读者深入理解这些算法的实现细节和工作原理。</p> 
<h4><a id="1__108"></a>1. 图形半监督学习</h4> 
<p>图形半监督学习通过构建数据点之间的图结构，利用图上的连接关系来传播标签信息。以下是一个基于 PyTorch 实现的简单图正则化算法示例。</p> 
<h5><a id="_112"></a>数据准备</h5> 
<p>我们将使用一个小型的合成数据集来演示图形半监督学习的实现。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> make_moons
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> StandardScaler

<span class="token comment"># 生成合成数据集</span>
X<span class="token punctuation">,</span> y <span class="token operator">=</span> make_moons<span class="token punctuation">(</span>n_samples<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span> noise<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>
scaler <span class="token operator">=</span> StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> scaler<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X<span class="token punctuation">)</span>

<span class="token comment"># 将数据分为标记和未标记数据</span>
n_labeled <span class="token operator">=</span> <span class="token number">20</span>
X_labeled <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span>n_labeled<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
y_labeled <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>y<span class="token punctuation">[</span><span class="token punctuation">:</span>n_labeled<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>
X_unlabeled <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>X<span class="token punctuation">[</span>n_labeled<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
</code></pre> 
<h5><a id="_136"></a>模型定义</h5> 
<p>我们定义一个简单的两层全连接神经网络作为基础模型。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">MLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MLP<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

model <span class="token operator">=</span> MLP<span class="token punctuation">(</span>input_dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> output_dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="_155"></a>图正则化</h5> 
<p>构建图结构，并定义图正则化的损失函数。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>neighbors <span class="token keyword">import</span> kneighbors_graph

<span class="token comment"># 构建K近邻图</span>
knn_graph <span class="token operator">=</span> kneighbors_graph<span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_neighbors<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'connectivity'</span><span class="token punctuation">)</span>
adj_matrix <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>knn_graph<span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

<span class="token comment"># 定义图正则化损失</span>
<span class="token keyword">def</span> <span class="token function">graph_regularization</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> X<span class="token punctuation">,</span> adj_matrix<span class="token punctuation">)</span><span class="token punctuation">:</span>
    output <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    diff <span class="token operator">=</span> output<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span> output<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
    loss <span class="token operator">=</span> <span class="token punctuation">(</span>diff<span class="token operator">**</span><span class="token number">2</span> <span class="token operator">*</span> adj_matrix<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> loss

<span class="token comment"># 定义总损失函数</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">total_loss</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> X_labeled<span class="token punctuation">,</span> y_labeled<span class="token punctuation">,</span> X_unlabeled<span class="token punctuation">,</span> adj_matrix<span class="token punctuation">,</span> lambda_reg<span class="token punctuation">)</span><span class="token punctuation">:</span>
    labeled_loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>model<span class="token punctuation">(</span>X_labeled<span class="token punctuation">)</span><span class="token punctuation">,</span> y_labeled<span class="token punctuation">)</span>
    reg_loss <span class="token operator">=</span> graph_regularization<span class="token punctuation">(</span>model<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>X_labeled<span class="token punctuation">,</span> X_unlabeled<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> adj_matrix<span class="token punctuation">)</span>
    <span class="token keyword">return</span> labeled_loss <span class="token operator">+</span> lambda_reg <span class="token operator">*</span> reg_loss
</code></pre> 
<h5><a id="_181"></a>模型训练</h5> 
<p>训练模型，并应用图正则化。</p> 
<pre><code class="prism language-python">optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
lambda_reg <span class="token operator">=</span> <span class="token number">0.1</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss <span class="token operator">=</span> total_loss<span class="token punctuation">(</span>model<span class="token punctuation">,</span> X_labeled<span class="token punctuation">,</span> y_labeled<span class="token punctuation">,</span> X_unlabeled<span class="token punctuation">,</span> adj_matrix<span class="token punctuation">,</span> lambda_reg<span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch [</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/100], Loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="2__198"></a>2. 自训练</h4> 
<p>自训练通过迭代的方式，逐步将高置信度的未标记数据加入到标记数据集中，重新训练模型。</p> 
<h5><a id="_202"></a>数据准备</h5> 
<p>使用与前面相同的数据集。</p> 
<pre><code class="prism language-python"><span class="token comment"># 使用与前面相同的数据集</span>
</code></pre> 
<h5><a id="_210"></a>模型定义</h5> 
<p>使用相同的 MLP 模型。</p> 
<pre><code class="prism language-python"><span class="token comment"># 使用相同的 MLP 模型</span>
</code></pre> 
<h5><a id="_218"></a>自训练</h5> 
<p>定义自训练的迭代过程。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token punctuation">,</span> TensorDataset

<span class="token comment"># 初始训练</span>
dataset <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>X_labeled<span class="token punctuation">,</span> y_labeled<span class="token punctuation">)</span>
dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">train_model</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> dataloader<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> X_batch<span class="token punctuation">,</span> y_batch <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            output <span class="token operator">=</span> model<span class="token punctuation">(</span>X_batch<span class="token punctuation">)</span>
            loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> y_batch<span class="token punctuation">)</span>
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

train_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> dataloader<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>

<span class="token comment"># 自训练迭代</span>
<span class="token keyword">for</span> iteration <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 预测未标记数据</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>X_unlabeled<span class="token punctuation">)</span>
        confidences<span class="token punctuation">,</span> pseudo_labels <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 选取高置信度的样本</span>
    high_confidence_mask <span class="token operator">=</span> confidences <span class="token operator">&gt;</span> <span class="token number">0.9</span>
    X_pseudo_labeled <span class="token operator">=</span> X_unlabeled<span class="token punctuation">[</span>high_confidence_mask<span class="token punctuation">]</span>
    y_pseudo_labeled <span class="token operator">=</span> pseudo_labels<span class="token punctuation">[</span>high_confidence_mask<span class="token punctuation">]</span>
    
    <span class="token comment"># 扩展标记数据集</span>
    X_labeled <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>X_labeled<span class="token punctuation">,</span> X_pseudo_labeled<span class="token punctuation">)</span><span class="token punctuation">)</span>
    y_labeled <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>y_labeled<span class="token punctuation">,</span> y_pseudo_labeled<span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 重新训练模型</span>
    dataset <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>X_labeled<span class="token punctuation">,</span> y_labeled<span class="token punctuation">)</span>
    dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    train_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> dataloader<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> criterion<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Iteration [</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>iteration <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/5] completed'</span></span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="3__265"></a>3. 一致性正则化</h4> 
<p>一致性正则化通过施加扰动，使模型对输入数据的微小变化保持鲁棒性。</p> 
<h5><a id="_269"></a>数据准备</h5> 
<p>使用与前面相同的数据集。</p> 
<pre><code class="prism language-python"><span class="token comment"># 使用与前面相同的数据集</span>
</code></pre> 
<h5><a id="_277"></a>模型定义</h5> 
<p>使用相同的 MLP 模型。</p> 
<pre><code class="prism language-python"><span class="token comment"># 使用相同的 MLP 模型</span>
</code></pre> 
<h5><a id="_285"></a>一致性正则化</h5> 
<p>定义一致性正则化的损失函数，并训练模型。</p> 
<pre><code class="prism language-python"><span class="token comment"># 定义一致性正则化损失</span>
<span class="token keyword">def</span> <span class="token function">consistency_loss</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> X<span class="token punctuation">,</span> noise_factor<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    X_noisy <span class="token operator">=</span> X <span class="token operator">+</span> noise_factor <span class="token operator">*</span> torch<span class="token punctuation">.</span>randn_like<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    output <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    output_noisy <span class="token operator">=</span> model<span class="token punctuation">(</span>X_noisy<span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>output <span class="token operator">-</span> output_noisy<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 总损失函数</span>
<span class="token keyword">def</span> <span class="token function">total_loss</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> X_labeled<span class="token punctuation">,</span> y_labeled<span class="token punctuation">,</span> X_unlabeled<span class="token punctuation">,</span> lambda_reg<span class="token punctuation">)</span><span class="token punctuation">:</span>
    labeled_loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>model<span class="token punctuation">(</span>X_labeled<span class="token punctuation">)</span><span class="token punctuation">,</span> y_labeled<span class="token punctuation">)</span>
    reg_loss <span class="token operator">=</span> consistency_loss<span class="token punctuation">(</span>model<span class="token punctuation">,</span> X_unlabeled<span class="token punctuation">)</span>
    <span class="token keyword">return</span> labeled_loss <span class="token operator">+</span> lambda_reg <span class="token operator">*</span> reg_loss

<span class="token comment"># 训练模型</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
lambda_reg <span class="token operator">=</span> <span class="token number">0.1</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss <span class="token operator">=</span> total_loss<span class="token punctuation">(</span>model<span class="token punctuation">,</span> X_labeled<span class="token punctuation">,</span> y_labeled<span class="token punctuation">,</span> X_unlabeled<span class="token punctuation">,</span> lambda_reg<span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch [</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/100], Loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="4_GANs_316"></a>4. 生成对抗网络（GANs）</h4> 
<p>GANs 通过生成器和判别器之间的对抗训练来生成逼真的数据样本，并利用这些样本改进分类器的性能。</p> 
<h5><a id="_320"></a>数据准备</h5> 
<p>使用与前面相同的数据集。</p> 
<pre><code class="prism language-python"><span class="token comment"># 使用与前面相同的数据集</span>
</code></pre> 
<h5><a id="_328"></a>模型定义</h5> 
<p>定义生成器和判别器模型。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Generator</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Generator<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> z<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>z<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

<span class="token keyword">class</span> <span class="token class-name">Discriminator</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Discriminator<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

generator <span class="token operator">=</span> Generator<span class="token punctuation">(</span>input_dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> output_dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
discriminator <span class="token operator">=</span> Discriminator<span class="token punctuation">(</span>input_dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> output_dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="_GANs_359"></a>训练 GANs</h5> 
<p>定义 GANs 的损失函数，并进行对抗训练。</p> 
<pre><code class="prism language-python"><span class="token comment"># 定义损失函数</span>
adversarial_loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 优化器</span>
optimizer_G <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>generator<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
optimizer_D <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>discriminator<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>

<span class="token comment"># 训练 GANs</span>
num_epochs <span class="token operator">=</span> <span class="token number">100</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 训练判别器</span>
    optimizer_D<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 真实数据</span>
    real_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
    real_labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>real_data<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    real_output <span class="token operator">=</span> discriminator<span class="token punctuation">(</span>real_data<span class="token punctuation">)</span>
    d_loss_real <span class="token operator">=</span> adversarial_loss<span class="token punctuation">(</span>real_output<span class="token punctuation">,</span> real_labels<span class="token punctuation">)</span>
    
    <span class="token comment"># 生成数据</span>
    z <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>real_data<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
    fake_data <span class="token operator">=</span> generator<span class="token punctuation">(</span>z<span class="token punctuation">)</span>
    fake_labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>real_data<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    fake_output <span class="token operator">=</span> discriminator<span class="token punctuation">(</span>fake_data<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    d_loss_fake <span class="token operator">=</span> adversarial_loss<span class="token punctuation">(</span>fake_output<span class="token punctuation">,</span> fake_labels<span class="token punctuation">)</span>
    
    <span class="token comment"># 总判别器损失</span>
    d_loss <span class="token operator">=</span> d_loss_real <span class="token operator">+</span> d_loss_fake
    d_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer_D<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 训练生成器</span>
    optimizer_G<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    fake_output <span class="token operator">=</span> discriminator<span class="token punctuation">(</span>fake_data<span class="token punctuation">)</span>
    g_loss <span class="token operator">=</span> adversarial_loss<span class="token punctuation">(</span>fake_output<span class="token punctuation">,</span> real_labels<span class="token punctuation">)</span>
    g_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer_G<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch [</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/100], D Loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>d_loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">, G Loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>g_loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="_407"></a>小结</h4> 
<p>本章节通过代码实战展示了如何使用 PyTorch 实现图形半监督学习、自训练、一致性正则化和生成对抗网络（GANs）在半监督学习中的应用。每种方法通过详细的代码和注释，帮助读者深入理解这些算法的实现细节和工作原理。希望通过这些实战案例，读者能够更好地掌握半监督学习的核心思想和技术，实现自己的半监督学习模型。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/356818f07fd567f56835b6dee9e18550/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Unity3D 如何读取策划给定的Excel表格详解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/10f9c916cf1a8f13d57fded1642ee12a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【乐吾乐2D可视化组态编辑器】事件</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>