<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LLama-Factory大模型训练框架，基于自己数据集微调qwen7B模型实战 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/7f813249c69f87a59057092ab6ee6b55/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="LLama-Factory大模型训练框架，基于自己数据集微调qwen7B模型实战">
  <meta property="og:description" content="一，项目简介 LLama-Factory，大模型训练框架，支持多种模型，多种训练方式，
项目github地址：link
项目特色 多种模型：LLaMA、LLaVA、Mistral、Mixtral-MoE、Qwen、Yi、Gemma、Baichuan、ChatGLM、Phi 等等。集成方法：（增量）预训练、（多模态）指令监督微调、奖励模型训练、PPO 训练、DPO 训练、KTO 训练、ORPO 训练等等。多种精度：16 比特全参数微调、冻结微调、LoRA 微调和基于 AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ 的 2/3/4/5/6/8 比特 QLoRA 微调。先进算法：GaLore、BAdam、DoRA、LongLoRA、LLaMA Pro、Mixture-of-Depths、LoRA&#43;、LoftQ、PiSSA 和 Agent 微调。实用技巧：FlashAttention-2、Unsloth、RoPE scaling、NEFTune 和 rsLoRA。实验监控：LlamaBoard、TensorBoard、Wandb、MLflow 等等。极速推理：基于 vLLM 的 OpenAI 风格 API、浏览器界面和命令行接口。 二， 支持训练模型以及地址 或者去魔搭社区，是真的快
link
模型名模型大小TemplateBaichuan 27B/13Bbaichuan2BLOOM/BLOOMZ560M/1.1B/1.7B/3B/7.1B/176B-ChatGLM36Bchatglm3Command R35B/104BcohereDeepSeek (Code/MoE)7B/16B/67B/236BdeepseekFalcon7B/11B/40B/180BfalconGemma/Gemma 2/CodeGemma2B/7B/9B/27BgemmaGLM-49Bglm4InternLM27B/20Bintern2Llama7B/13B/33B/65B-Llama 27B/13B/70Bllama2Llama 38B/70Bllama3LLaVA-1.57B/13BvicunaMistral/Mixtral7B/8x7B/8x22BmistralOLMo1B/7B-PaliGemma3BgemmaPhi-1.5/Phi-21.3B/2.7B-Phi-34B/7B/14BphiQwen/Qwen1.5/Qwen2 (Code/MoE)0.5B/1.5B/4B/7B/14B/32B/72B/110BqwenStarCoder 23B/7B/15B-XVERSE7B/13B/65BxverseYi/Yi-1.56B/9B/34ByiYi-VL6B/34Byi_vlYuan 22B/51B/102Byuan 三，硬件依赖 * 估算值
方法精度7B13B30B70B110B8x7B8x22BFullAMP120GB240GB600GB1200GB2000GB900GB2400GBFull1660GB120GB300GB600GB900GB400GB1200GBFreeze1620GB40GB80GB200GB360GB160GB400GBLoRA/GaLore/BAdam1616GB32GB64GB160GB240GB120GB320GBQLoRA810GB20GB40GB80GB140GB60GB160GBQLoRA46GB12GB24GB48GB72GB30GB96GBQLoRA24GB8GB16GB24GB48GB18GB48GB 四，安装环境和训练实战 4.1 环境安装 git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git cd LLaMA-Factory pip install -e &#34;.[torch,metrics]&#34; 4.2 构建自己的数据集 [{ &#34;input&#34;: &#34;2023年3月16日14时55分许，鄂温克族自治旗伊敏河镇发生一起一般事故，造成一人死亡，直接经济损失人民币200万元。&#34;, &#34;output&#34;: &#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-04T11:17:36+08:00">
    <meta property="article:modified_time" content="2024-07-04T11:17:36+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">LLama-Factory大模型训练框架，基于自己数据集微调qwen7B模型实战</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_0"></a>一，项目简介</h3> 
<p>LLama-Factory，大模型训练框架，支持多种模型，多种训练方式，</p> 
<p>项目github地址：<a href="https://github.com/hiyouga/LLaMA-Factory">link</a></p> 
<h4><a id="_4"></a>项目特色</h4> 
<ul><li><strong>多种模型</strong>：LLaMA、LLaVA、Mistral、Mixtral-MoE、Qwen、Yi、Gemma、Baichuan、ChatGLM、Phi 等等。</li><li><strong>集成方法</strong>：（增量）预训练、（多模态）指令监督微调、奖励模型训练、PPO 训练、DPO 训练、KTO 训练、ORPO 训练等等。</li><li><strong>多种精度</strong>：16 比特全参数微调、冻结微调、LoRA 微调和基于 AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ 的 2/3/4/5/6/8 比特 QLoRA 微调。</li><li><strong>先进算法</strong>：GaLore、BAdam、DoRA、LongLoRA、LLaMA Pro、Mixture-of-Depths、LoRA+、LoftQ、PiSSA 和 Agent 微调。</li><li><strong>实用技巧</strong>：FlashAttention-2、Unsloth、RoPE scaling、NEFTune 和 rsLoRA。</li><li><strong>实验监控</strong>：LlamaBoard、TensorBoard、Wandb、MLflow 等等。</li><li><strong>极速推理</strong>：基于 vLLM 的 OpenAI 风格 API、浏览器界面和命令行接口。</li></ul> 
<h3><a id="__13"></a>二， 支持训练模型以及地址</h3> 
<p>或者去魔搭社区，是真的快</p> 
<p><a href="https://modelscope.cn/models" rel="nofollow">link</a></p> 
<table><thead><tr><th>模型名</th><th>模型大小</th><th>Template</th></tr></thead><tbody><tr><td><a href="https://huggingface.co/baichuan-inc" rel="nofollow">Baichuan 2</a></td><td>7B/13B</td><td>baichuan2</td></tr><tr><td><a href="https://huggingface.co/bigscience" rel="nofollow">BLOOM/BLOOMZ</a></td><td>560M/1.1B/1.7B/3B/7.1B/176B</td><td>-</td></tr><tr><td><a href="https://huggingface.co/THUDM" rel="nofollow">ChatGLM3</a></td><td>6B</td><td>chatglm3</td></tr><tr><td><a href="https://huggingface.co/CohereForAI" rel="nofollow">Command R</a></td><td>35B/104B</td><td>cohere</td></tr><tr><td><a href="https://huggingface.co/deepseek-ai" rel="nofollow">DeepSeek (Code/MoE)</a></td><td>7B/16B/67B/236B</td><td>deepseek</td></tr><tr><td><a href="https://huggingface.co/tiiuae" rel="nofollow">Falcon</a></td><td>7B/11B/40B/180B</td><td>falcon</td></tr><tr><td><a href="https://huggingface.co/google" rel="nofollow">Gemma/Gemma 2/CodeGemma</a></td><td>2B/7B/9B/27B</td><td>gemma</td></tr><tr><td><a href="https://huggingface.co/THUDM" rel="nofollow">GLM-4</a></td><td>9B</td><td>glm4</td></tr><tr><td><a href="https://huggingface.co/internlm" rel="nofollow">InternLM2</a></td><td>7B/20B</td><td>intern2</td></tr><tr><td><a href="https://github.com/facebookresearch/llama">Llama</a></td><td>7B/13B/33B/65B</td><td>-</td></tr><tr><td><a href="https://huggingface.co/meta-llama" rel="nofollow">Llama 2</a></td><td>7B/13B/70B</td><td>llama2</td></tr><tr><td><a href="https://huggingface.co/meta-llama" rel="nofollow">Llama 3</a></td><td>8B/70B</td><td>llama3</td></tr><tr><td><a href="https://huggingface.co/llava-hf" rel="nofollow">LLaVA-1.5</a></td><td>7B/13B</td><td>vicuna</td></tr><tr><td><a href="https://huggingface.co/mistralai" rel="nofollow">Mistral/Mixtral</a></td><td>7B/8x7B/8x22B</td><td>mistral</td></tr><tr><td><a href="https://huggingface.co/allenai" rel="nofollow">OLMo</a></td><td>1B/7B</td><td>-</td></tr><tr><td><a href="https://huggingface.co/google" rel="nofollow">PaliGemma</a></td><td>3B</td><td>gemma</td></tr><tr><td><a href="https://huggingface.co/microsoft" rel="nofollow">Phi-1.5/Phi-2</a></td><td>1.3B/2.7B</td><td>-</td></tr><tr><td><a href="https://huggingface.co/microsoft" rel="nofollow">Phi-3</a></td><td>4B/7B/14B</td><td>phi</td></tr><tr><td><a href="https://huggingface.co/Qwen" rel="nofollow">Qwen/Qwen1.5/Qwen2 (Code/MoE)</a></td><td>0.5B/1.5B/4B/7B/14B/32B/72B/110B</td><td>qwen</td></tr><tr><td><a href="https://huggingface.co/bigcode" rel="nofollow">StarCoder 2</a></td><td>3B/7B/15B</td><td>-</td></tr><tr><td><a href="https://huggingface.co/xverse" rel="nofollow">XVERSE</a></td><td>7B/13B/65B</td><td>xverse</td></tr><tr><td><a href="https://huggingface.co/01-ai" rel="nofollow">Yi/Yi-1.5</a></td><td>6B/9B/34B</td><td>yi</td></tr><tr><td><a href="https://huggingface.co/01-ai" rel="nofollow">Yi-VL</a></td><td>6B/34B</td><td>yi_vl</td></tr><tr><td><a href="https://huggingface.co/IEITYuan" rel="nofollow">Yuan 2</a></td><td>2B/51B/102B</td><td>yuan</td></tr></tbody></table> 
<h4><a id="_44"></a>三，硬件依赖</h4> 
<p>* <em>估算值</em></p> 
<table><thead><tr><th>方法</th><th>精度</th><th>7B</th><th>13B</th><th>30B</th><th>70B</th><th>110B</th><th>8x7B</th><th>8x22B</th></tr></thead><tbody><tr><td>Full</td><td>AMP</td><td>120GB</td><td>240GB</td><td>600GB</td><td>1200GB</td><td>2000GB</td><td>900GB</td><td>2400GB</td></tr><tr><td>Full</td><td>16</td><td>60GB</td><td>120GB</td><td>300GB</td><td>600GB</td><td>900GB</td><td>400GB</td><td>1200GB</td></tr><tr><td>Freeze</td><td>16</td><td>20GB</td><td>40GB</td><td>80GB</td><td>200GB</td><td>360GB</td><td>160GB</td><td>400GB</td></tr><tr><td>LoRA/GaLore/BAdam</td><td>16</td><td>16GB</td><td>32GB</td><td>64GB</td><td>160GB</td><td>240GB</td><td>120GB</td><td>320GB</td></tr><tr><td>QLoRA</td><td>8</td><td>10GB</td><td>20GB</td><td>40GB</td><td>80GB</td><td>140GB</td><td>60GB</td><td>160GB</td></tr><tr><td>QLoRA</td><td>4</td><td>6GB</td><td>12GB</td><td>24GB</td><td>48GB</td><td>72GB</td><td>30GB</td><td>96GB</td></tr><tr><td>QLoRA</td><td>2</td><td>4GB</td><td>8GB</td><td>16GB</td><td>24GB</td><td>48GB</td><td>18GB</td><td>48GB</td></tr></tbody></table> 
<h4><a id="_58"></a>四，安装环境和训练实战</h4> 
<h5><a id="41__59"></a>4.1 环境安装</h5> 
<pre><code class="prism language-bash"><span class="token function">git</span> clone <span class="token parameter variable">--depth</span> <span class="token number">1</span> https://github.com/hiyouga/LLaMA-Factory.git
<span class="token builtin class-name">cd</span> LLaMA-Factory
pip <span class="token function">install</span> <span class="token parameter variable">-e</span> <span class="token string">".[torch,metrics]"</span>
</code></pre> 
<h5><a id="42__66"></a>4.2 构建自己的数据集</h5> 
<pre><code class="prism language-json"><span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span>
	<span class="token string-property property">"input"</span><span class="token operator">:</span> <span class="token string">"2023年3月16日14时55分许，鄂温克族自治旗伊敏河镇发生一起一般事故，造成一人死亡，直接经济损失人民币200万元。"</span><span class="token punctuation">,</span>
	<span class="token string-property property">"output"</span><span class="token operator">:</span> <span class="token string">"任务1：“是”，原文中提到了负面新闻，这些词汇与负面新闻相关。任务2：“不是”，由于原文没有提到了负面情绪，这和没有关系，因此不是。"</span><span class="token punctuation">,</span>
	<span class="token string-property property">"instruction"</span><span class="token operator">:</span> <span class="token string">"你是一个舆情分析专家，擅长分析一段文字的舆情类型。现在请你判断下述语句，任务1，是否与负面新闻，你的回答 只能从是或不是选择一个，任务2，是否与负面情绪，你的回答 只能从是或不是中选择一个。例如：待判断语句：今天合肥的天气真好。你的回复：1，不是，2，不是。现在待判断语句为：{}"</span>
<span class="token punctuation">}</span><span class="token punctuation">]</span>

</code></pre> 
<p>解析:在指令监督微调时，<code>instruction</code> 列对应的内容会与 <code>input</code> 列对应的内容拼接后作为人类指令，即人类指令为 <code>instruction\ninput</code>。而 <code>output</code> 列对应的内容为模型回答。</p> 
<p>如果指定，<code>system</code> 列对应的内容将被作为系统提示词。</p> 
<pre><code class="prism language-json"><span class="token punctuation">[</span>
  <span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"instruction"</span><span class="token operator">:</span> <span class="token string">"人类指令,就是你要问模型的pormopt（必填）"</span><span class="token punctuation">,</span>
    <span class="token string-property property">"input"</span><span class="token operator">:</span> <span class="token string">"人类输入,输入的句子（选填）"</span><span class="token punctuation">,</span>
    <span class="token string-property property">"output"</span><span class="token operator">:</span> <span class="token string">"模型回答（必填）"</span><span class="token punctuation">,</span>
    <span class="token string-property property">"system"</span><span class="token operator">:</span> <span class="token string">"系统提示词（选填）"</span><span class="token punctuation">,</span>
    <span class="token string-property property">"history"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
      <span class="token punctuation">[</span><span class="token string">"第一轮指令（选填）"</span><span class="token punctuation">,</span> <span class="token string">"第一轮回答（选填）"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
      <span class="token punctuation">[</span><span class="token string">"第二轮指令（选填）"</span><span class="token punctuation">,</span> <span class="token string">"第二轮回答（选填）"</span><span class="token punctuation">]</span>
    <span class="token punctuation">]</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">]</span>
</code></pre> 
<h6><a id="_94"></a>注册自己的数据集</h6> 
<p>将自己的数据集放到data目录下</p> 
<pre><code class="prism language-python">vim data<span class="token operator">/</span>dataset_info<span class="token punctuation">.</span>json
<span class="token comment">### 添加一行内容</span>
 <span class="token string">"my_train_data"</span><span class="token punctuation">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"file_name"</span><span class="token punctuation">:</span> <span class="token string">"my_train_data.json"</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
</code></pre> 
<p>记着名字，一会训练要指定数据集名称</p> 
<h5><a id="yaml_105"></a>五，修改对应的yaml文件</h5> 
<pre><code class="prism language-python"><span class="token comment">### model</span>
model_name_or_path<span class="token punctuation">:</span>原始模型地址

<span class="token comment">### method</span>
stage<span class="token punctuation">:</span> sft
do_train<span class="token punctuation">:</span> true
finetuning_type<span class="token punctuation">:</span> lora
lora_target<span class="token punctuation">:</span> <span class="token builtin">all</span>

<span class="token comment">### dataset</span>
dataset<span class="token punctuation">:</span> my_train<span class="token punctuation">,</span>alpaca_en_demo<span class="token punctuation">(</span>混合训练的样本集，防止知识遗忘，可以不用<span class="token punctuation">)</span>
template<span class="token punctuation">:</span> qwen
cutoff_len<span class="token punctuation">:</span> <span class="token number">4096</span>
max_samples<span class="token punctuation">:</span> <span class="token number">1000</span>
overwrite_cache<span class="token punctuation">:</span> true
preprocessing_num_workers<span class="token punctuation">:</span> <span class="token number">16</span>

<span class="token comment">### output</span>
output_dir<span class="token punctuation">:</span> saves<span class="token operator">/</span>qwen<span class="token operator">/</span>lora<span class="token operator">/</span>sft
logging_steps<span class="token punctuation">:</span> <span class="token number">10</span>
save_steps<span class="token punctuation">:</span> <span class="token number">100</span>
plot_loss<span class="token punctuation">:</span> true
overwrite_output_dir<span class="token punctuation">:</span> true

<span class="token comment">### train</span>
per_device_train_batch_size<span class="token punctuation">:</span> <span class="token number">1</span>
gradient_accumulation_steps<span class="token punctuation">:</span> <span class="token number">8</span>
learning_rate<span class="token punctuation">:</span> <span class="token number">1.0e-4</span>
num_train_epochs<span class="token punctuation">:</span> <span class="token number">3.0</span>
lr_scheduler_type<span class="token punctuation">:</span> cosine
warmup_ratio<span class="token punctuation">:</span> <span class="token number">0.1</span>
bf16<span class="token punctuation">:</span> true
ddp_timeout<span class="token punctuation">:</span> <span class="token number">180000000</span>

<span class="token comment">### eval</span>
val_size<span class="token punctuation">:</span> <span class="token number">0.1</span>
per_device_eval_batch_size<span class="token punctuation">:</span> <span class="token number">1</span>
eval_strategy<span class="token punctuation">:</span> steps
eval_steps<span class="token punctuation">:</span> <span class="token number">500</span>
</code></pre> 
<h5><a id="_148"></a>开始训练</h5> 
<p>lora 指令微调</p> 
<pre><code>llamafactory-cli train examples/train_lora/mytrain_lora_sft.yaml
</code></pre> 
<p>命令行</p> 
<pre><code class="prism language-python">CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span> python src<span class="token operator">/</span>train_bash<span class="token punctuation">.</span>py  <span class="token operator">-</span><span class="token operator">-</span>stage sft     <span class="token operator">-</span><span class="token operator">-</span>do_train     <span class="token operator">-</span><span class="token operator">-</span>model_name_or_path  <span class="token operator">/</span>app<span class="token operator">/</span>model  <span class="token operator">-</span><span class="token operator">-</span>dataset my_train_data    <span class="token operator">-</span><span class="token operator">-</span>finetuning_type lora     <span class="token operator">-</span><span class="token operator">-</span>lora_target q_proj<span class="token punctuation">,</span>v_proj     <span class="token operator">-</span><span class="token operator">-</span>output_dir <span class="token operator">/</span>app<span class="token operator">/</span>output   <span class="token operator">-</span><span class="token operator">-</span>overwrite_cache     <span class="token operator">-</span><span class="token operator">-</span>per_device_train_batch_size <span class="token number">1</span>     <span class="token operator">-</span><span class="token operator">-</span>gradient_accumulation_steps <span class="token number">1</span>     <span class="token operator">-</span><span class="token operator">-</span>lr_scheduler_type cosine     <span class="token operator">-</span><span class="token operator">-</span>logging_steps <span class="token number">10</span>     <span class="token operator">-</span><span class="token operator">-</span>save_steps <span class="token number">1000</span>     <span class="token operator">-</span><span class="token operator">-</span>learning_rate <span class="token number">5e-5</span>     <span class="token operator">-</span><span class="token operator">-</span>num_train_epochs <span class="token number">3.0</span>     <span class="token operator">-</span><span class="token operator">-</span>template yi
</code></pre> 
<h5><a id="_159"></a>合并模型</h5> 
<pre><code class="prism language-python">llamafactory<span class="token operator">-</span>cli export examples<span class="token operator">/</span>merge_lora<span class="token operator">/</span>my_lora_sft<span class="token punctuation">.</span>yaml
</code></pre> 
<pre><code>### vi examples\merge_lora\llama3_lora_sft.yaml改成自己路径就行了
### model
model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
adapter_name_or_path: saves/llama3-8b/lora/sft
template: llama3
finetuning_type: lora

### export
export_dir: models/llama3_lora_sft
export_size: 2
export_device: cpu
export_legacy_format: false
</code></pre> 
<p>或者</p> 
<pre><code>CUDA_VISBLE_DEVICES=0 python /app/src/export_model.py --model_name_or_path /app/model/ --adapter_name_or_path /app/output/checkpoint-3000/ --template default --finetuning_type lora --export_dir /app/lora_resul
t/20240422_1519 --export_size 2 --export_legacy_format False

</code></pre> 
<h5><a id="_184"></a>模型推理</h5> 
<p>vi inference/yam.py,修改对应路径</p> 
<pre><code>model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
adapter_name_or_path: saves/llama3-8b/lora/sft
template: llama3
finetuning_type: lora
</code></pre> 
<pre><code>llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
</code></pre> 
<p>或者<br> ·```<br> python /app/src/cli_demo.py --model_name_or_path /app/lora_result/20240422_1519/ --template=qwen</p> 
<pre><code>
未完待续·....
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ee9a0d2d0554501ac39354a99d7fbecb/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">字符串和正则表达式踩坑</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a0aed1d307426afe8ea71bb60a2a8d84/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">阿里云 ECS 服务器的安全组设置</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>