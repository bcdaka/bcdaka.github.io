<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Datawhale X 魔搭 AI夏令营第四期 魔搭-AIGC方向 task01笔记 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/75613d206e55c9923b8efb2a942482cf/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Datawhale X 魔搭 AI夏令营第四期 魔搭-AIGC方向 task01笔记">
  <meta property="og:description" content="赛题内容 参赛者需在可图Kolors 模型的基础上训练LoRA 模型，生成无限风格，如水墨画风格、水彩风格、赛博朋克风格、日漫风格......
基于LoRA模型生成 8 张图片组成连贯故事，故事内容可自定义；基于8图故事，评估LoRA风格的美感度及连贯性
目录
赛题内容
1 OpenAI DALL·E：起于文本，潜入图像，2021.01
1.1 GPT-3 (2020)：基于 transformer 架构的多模态大语言模型
1.2 DALL·E (2021.01)：transformer 架构扩展到计算机视觉领域
1.3 量化“文本-图像”匹配程度：CLIP 模型
1.4 小结
2 Diffusion：高斯去噪，扩散称王，2021.12
2.1 几种图像生成模型：GAN/VAE/Flow-based/Diffusion
2.2 正向图像扩散（forward image diffusion）
2.2.1 基本原理
2.2.2 数学描述
2.3 反向图像扩散（reverse image diffusion）
2.3.1 基本原理
2.3.2 数学表示
2.4 引导扩散（guiding the diffusion）
3 GLIDE：文本引导，定向扩散，2022.04
3.1 架构
3.2 工作原理
3.3 小结
4 DALL·E 2：取长补短，先验称奇，2022.04
4.1 架构：unCLIP = prior &#43; decoder
4.2 The prior">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-11T18:43:22+08:00">
    <meta property="article:modified_time" content="2024-08-11T18:43:22+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Datawhale X 魔搭 AI夏令营第四期 魔搭-AIGC方向 task01笔记</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 id="赛题内容">赛题内容</h2> 
<ol><li> <p>参赛者需在可图Kolors 模型的基础上训练LoRA 模型，生成无限风格，如水墨画风格、水彩风格、赛博朋克风格、日漫风格......</p> </li><li> <p>基于LoRA模型生成 8 张图片组成连贯故事，故事内容可自定义；基于8图故事，评估LoRA风格的美感度及连贯性</p> </li></ol> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="赛题内容-toc" style="margin-left:0px;"><a href="#%E8%B5%9B%E9%A2%98%E5%86%85%E5%AE%B9" rel="nofollow">赛题内容</a></p> 
<p id="1-openai-dalle起于文本潜入图像202101-toc" style="margin-left:40px;"><a href="#1-openai-dalle%E8%B5%B7%E4%BA%8E%E6%96%87%E6%9C%AC%E6%BD%9C%E5%85%A5%E5%9B%BE%E5%83%8F202101" rel="nofollow">1 OpenAI DALL·E：起于文本，潜入图像，2021.01</a></p> 
<p id="11-gpt-3-2020基于-transformer-架构的多模态大语言模型-toc" style="margin-left:80px;"><a href="#11-gpt-3-2020%E5%9F%BA%E4%BA%8E-transformer-%E6%9E%B6%E6%9E%84%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B" rel="nofollow">1.1 GPT-3 (2020)：基于 transformer 架构的多模态大语言模型</a></p> 
<p id="12-dalle-202101transformer-架构扩展到计算机视觉领域-toc" style="margin-left:80px;"><a href="#12-dalle-202101transformer-%E6%9E%B6%E6%9E%84%E6%89%A9%E5%B1%95%E5%88%B0%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F" rel="nofollow">1.2 DALL·E (2021.01)：transformer 架构扩展到计算机视觉领域</a></p> 
<p id="13-量化文本-图像匹配程度clip-模型-toc" style="margin-left:80px;"><a href="#13-%E9%87%8F%E5%8C%96%E6%96%87%E6%9C%AC-%E5%9B%BE%E5%83%8F%E5%8C%B9%E9%85%8D%E7%A8%8B%E5%BA%A6clip-%E6%A8%A1%E5%9E%8B" rel="nofollow">1.3 量化“文本-图像”匹配程度：CLIP 模型</a></p> 
<p id="14-小结-toc" style="margin-left:80px;"><a href="#14-%E5%B0%8F%E7%BB%93" rel="nofollow">1.4 小结</a></p> 
<p id="2-diffusion高斯去噪扩散称王202112-toc" style="margin-left:0px;"><a href="#2-diffusion%E9%AB%98%E6%96%AF%E5%8E%BB%E5%99%AA%E6%89%A9%E6%95%A3%E7%A7%B0%E7%8E%8B202112" rel="nofollow">2 Diffusion：高斯去噪，扩散称王，2021.12</a></p> 
<p id="21-几种图像生成模型ganvaeflow-baseddiffusion-toc" style="margin-left:40px;"><a href="#21-%E5%87%A0%E7%A7%8D%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8Bganvaeflow-baseddiffusion" rel="nofollow">2.1 几种图像生成模型：GAN/VAE/Flow-based/Diffusion</a></p> 
<p id="22-正向图像扩散forward-image-diffusion-toc" style="margin-left:40px;"><a href="#22-%E6%AD%A3%E5%90%91%E5%9B%BE%E5%83%8F%E6%89%A9%E6%95%A3forward-image-diffusion" rel="nofollow">2.2 正向图像扩散（forward image diffusion）</a></p> 
<p id="221-基本原理-toc" style="margin-left:80px;"><a href="#221-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86" rel="nofollow">2.2.1 基本原理</a></p> 
<p id="222-数学描述-toc" style="margin-left:80px;"><a href="#222-%E6%95%B0%E5%AD%A6%E6%8F%8F%E8%BF%B0" rel="nofollow">2.2.2 数学描述</a></p> 
<p id="23-反向图像扩散reverse-image-diffusion-toc" style="margin-left:40px;"><a href="#23-%E5%8F%8D%E5%90%91%E5%9B%BE%E5%83%8F%E6%89%A9%E6%95%A3reverse-image-diffusion" rel="nofollow">2.3 反向图像扩散（reverse image diffusion）</a></p> 
<p id="231-基本原理-toc" style="margin-left:80px;"><a href="#231-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86" rel="nofollow">2.3.1 基本原理</a></p> 
<p id="232-数学表示-toc" style="margin-left:80px;"><a href="#232-%E6%95%B0%E5%AD%A6%E8%A1%A8%E7%A4%BA" rel="nofollow">2.3.2 数学表示</a></p> 
<p id="24-引导扩散guiding-the-diffusion-toc" style="margin-left:40px;"><a href="#24-%E5%BC%95%E5%AF%BC%E6%89%A9%E6%95%A3guiding-the-diffusion" rel="nofollow">2.4 引导扩散（guiding the diffusion）</a></p> 
<p id="3-glide文本引导定向扩散202204-toc" style="margin-left:0px;"><a href="#3-glide%E6%96%87%E6%9C%AC%E5%BC%95%E5%AF%BC%E5%AE%9A%E5%90%91%E6%89%A9%E6%95%A3202204" rel="nofollow">3 GLIDE：文本引导，定向扩散，2022.04</a></p> 
<p id="31-架构-toc" style="margin-left:40px;"><a href="#31-%E6%9E%B6%E6%9E%84" rel="nofollow">3.1 架构</a></p> 
<p id="32-工作原理-toc" style="margin-left:40px;"><a href="#32-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86" rel="nofollow">3.2 工作原理</a></p> 
<p id="33-小结-toc" style="margin-left:40px;"><a href="#33-%E5%B0%8F%E7%BB%93" rel="nofollow">3.3 小结</a></p> 
<p id="4-dalle-2取长补短先验称奇202204-toc" style="margin-left:0px;"><a href="#4-dalle-2%E5%8F%96%E9%95%BF%E8%A1%A5%E7%9F%AD%E5%85%88%E9%AA%8C%E7%A7%B0%E5%A5%87202204" rel="nofollow">4 DALL·E 2：取长补短，先验称奇，2022.04</a></p> 
<p id="41-架构unclip--prior--decoder-toc" style="margin-left:40px;"><a href="#41-%E6%9E%B6%E6%9E%84unclip--prior--decoder" rel="nofollow">4.1 架构：unCLIP = prior + decoder</a></p> 
<p id="42-the-prior-toc" style="margin-left:40px;"><a href="#42-the-prior" rel="nofollow">4.2 The prior</a></p> 
<p id="421-为什么需要-prior-层为什么单单-clip-不够-toc" style="margin-left:80px;"><a href="#421-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-prior-%E5%B1%82%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8D%95%E5%8D%95-clip-%E4%B8%8D%E5%A4%9F" rel="nofollow">4.2.1 为什么需要 prior 层（为什么单单 CLIP 不够）</a></p> 
<p id="422-prior-选型decoder-only-transformer-toc" style="margin-left:80px;"><a href="#422-prior-%E9%80%89%E5%9E%8Bdecoder-only-transformer" rel="nofollow">4.2.2 prior 选型：decoder-only transformer</a></p> 
<p id="423-损失函数-toc" style="margin-left:80px;"><a href="#423-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0" rel="nofollow">4.2.3 损失函数</a></p> 
<p id="424-小结-toc" style="margin-left:80px;"><a href="#424-%E5%B0%8F%E7%BB%93" rel="nofollow">4.2.4 小结</a></p> 
<p id="42-the-decoder基于-glide-的改进-toc" style="margin-left:40px;"><a href="#42-the-decoder%E5%9F%BA%E4%BA%8E-glide-%E7%9A%84%E6%94%B9%E8%BF%9B" rel="nofollow">4.2 The decoder：基于 GLIDE 的改进</a></p> 
<p id="43-引导扩散和上采样-toc" style="margin-left:40px;"><a href="#43-%E5%BC%95%E5%AF%BC%E6%89%A9%E6%95%A3%E5%92%8C%E4%B8%8A%E9%87%87%E6%A0%B7" rel="nofollow">4.3 引导扩散和上采样</a></p> 
<p id="5-google-imagen删繁就简扩散三连202205-toc" style="margin-left:0px;"><a href="#5-google-imagen%E5%88%A0%E7%B9%81%E5%B0%B1%E7%AE%80%E6%89%A9%E6%95%A3%E4%B8%89%E8%BF%9E202205" rel="nofollow">5 Google Imagen：删繁就简，扩散三连，2022.05</a></p> 
<p id="51-架构t5-xxl--diffusion--diffusion--diffusion-toc" style="margin-left:40px;"><a href="#51-%E6%9E%B6%E6%9E%84t5-xxl--diffusion--diffusion--diffusion" rel="nofollow">5.1 架构：T5-XXL + Diffusion + Diffusion + Diffusion</a></p> 
<p id="52-与-glidedalle-2-等架构的不同-toc" style="margin-left:40px;"><a href="#52-%E4%B8%8E-glidedalle-2-%E7%AD%89%E6%9E%B6%E6%9E%84%E7%9A%84%E4%B8%8D%E5%90%8C" rel="nofollow">5.2 与 GLIDE、DALL·E 2 等架构的不同</a></p> 
<p id="521-使用预训练的-transformer-t5-xxl-而不是从头开始训练-toc" style="margin-left:80px;"><a href="#521-%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84-transformer-t5-xxl-%E8%80%8C%E4%B8%8D%E6%98%AF%E4%BB%8E%E5%A4%B4%E5%BC%80%E5%A7%8B%E8%AE%AD%E7%BB%83" rel="nofollow">5.2.1 使用预训练的 transformer (T5-XXL) 而不是从头开始训练</a></p> 
<p id="522-使用更高效的底层神经网络efficient-u-net-toc" style="margin-left:80px;"><a href="#522-%E4%BD%BF%E7%94%A8%E6%9B%B4%E9%AB%98%E6%95%88%E7%9A%84%E5%BA%95%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cefficient-u-net" rel="nofollow">5.2.2 使用更高效的底层神经网络（efficient U-net）</a></p> 
<p id="523-使用-conditioning-augmentation-来增强图像保真度image-fidelity-toc" style="margin-left:80px;"><a href="#523-%E4%BD%BF%E7%94%A8-conditioning-augmentation-%E6%9D%A5%E5%A2%9E%E5%BC%BA%E5%9B%BE%E5%83%8F%E4%BF%9D%E7%9C%9F%E5%BA%A6image-fidelity" rel="nofollow">5.2.3 使用 conditioning augmentation 来增强图像保真度（image fidelity）</a></p> 
<p id="53-小结-toc" style="margin-left:40px;"><a href="#53-%E5%B0%8F%E7%BB%93" rel="nofollow">5.3 小结</a></p> 
<hr id="hr-toc"> 
<p></p> 
<p></p> 
<p></p> 
<h3 id="1-openai-dalle起于文本潜入图像202101">1 OpenAI <strong><code>DALL·E</code></strong>：起于文本，潜入图像，<strong><code>2021.01</code></strong></h3> 
<h4 id="11-gpt-3-2020基于-transformer-架构的多模态大语言模型">1.1 <code>GPT-3</code> (2020)：基于 transformer 架构的多模态大语言模型</h4> 
<p>2020 年，OpenAI 发布了 GPT-3 模型 [1]，这是一个基于 <strong><code>Transformer</code></strong> 架构的多模态大语言模型，能够完成机器翻译、文本生成、语义分析等任务， 也迅速被视为最先进的语言建模方案（language modeling solutions）。</p> 
<blockquote> 
 <ul><li><a href="https://arthurchiao.art/blog/transformers-from-scratch-zh/" rel="nofollow" title="Transformer 是如何工作的：600 行 Python 代码实现 self-attention 和两类 Transformer（2019）">Transformer 是如何工作的：600 行 Python 代码实现 self-attention 和两类 Transformer（2019）</a></li><li><a href="https://arthurchiao.art/blog/gpt-as-a-finite-state-markov-chain-zh/" rel="nofollow" title="GPT 是如何工作的：200 行 Python 代码实现一个极简 GPT（2023）">GPT 是如何工作的：200 行 Python 代码实现一个极简 GPT（2023）</a></li></ul> 
</blockquote> 
<h4 id="12-dalle-202101transformer-架构扩展到计算机视觉领域">1.2 <code>DALL·E</code> (2021.01)：transformer 架构扩展到计算机视觉领域</h4> 
<p>DALL·E [7] 可以看作是将 Transformer（<strong>语言领域</strong>）的能力自然扩展到<strong>计算机视觉领域</strong>。</p> 
<p>如何根据提示文本生成图片？DALL·E 提出了一种两阶段算法：</p> 
<ol><li> <p>训练一个离散 VAE (Variational AutoEncoder) 模型，将图像（images）压缩成 <strong>image tokens</strong>。</p> <p>VAE 是一种神经网络架构，属于 probabilistic graphical models and variational Bayesian methods 家族。</p> </li><li> <p>将编码之后的<strong>文本片段</strong>（encoded text snippet）与 <strong>image tokens</strong> 拼在一起（<strong><code>concatenate</code></strong>）， 训练一个自回归 Transformer，学习<strong>文本和图像之间的联合分布</strong>。</p> </li></ol> 
<p>最终是在从网上获取的 250 million 个文本-图像对（text-image pairs）上进行训练的。</p> 
<h4 id="13-量化文本-图像匹配程度clip-模型">1.3 量化“文本-图像”匹配程度：<code>CLIP</code> 模型</h4> 
<p>训练得到模型之后，就能通过<strong>推理</strong>生成图像。但<strong>如何评估生成图像的好坏</strong>呢？</p> 
<p>OpenAI 提出了一种名为 CLIP 的 <strong><code>image and text linking</code></strong> 方案 [9]， 它能量化<strong>文本片段（text snippet）与其图像表示（image representation）的匹配程度</strong>。</p> 
<p>抛开所有技术细节，训练这类模型的思路很简单：</p> 
<ol><li>将文本片段进行编码，得到 TiTi；</li><li>将图像进行编码，得到 IiIi；</li></ol> 
<p>对 400 million 个 <code>(image, text)</code> 进行这样的操作，</p> 
<p></p> 
<p class="img-center"><img alt="" height="445" src="https://images2.imgbox.com/2c/16/u0elFQhm_o.png" width="672"></p> 
<p>F.g CLIP contrastive pre-training 工作原理 [9]. （文本大意：澳大利亚小狗）。</p> 
<p>基于这种<em>映射</em>方式，就能够评估生成的图像符合文本输入的程度。</p> 
<h4 id="14-小结">1.4 小结</h4> 
<p>DALL·E 在 AI 和其他领域都引发了广泛的关注和讨论。 不过热度还没持续太久，风头就被另一个方向抢走了。</p> 
<h2 id="2-diffusion高斯去噪扩散称王202112">2 <strong><code>Diffusion</code></strong>：高斯去噪，扩散称王，<strong><code>2021.12</code></strong></h2> 
<p>Sohl-Dickstein 等提出了一种图像生成的新思想 —— 扩散模型（diffusion models） [2]。 套用 AI 领域的熟悉句式，就是</p> 
<blockquote> 
 <p>All you need is <strong><code>diffusion</code></strong>.</p> 
</blockquote> 
<h3 id="21-几种图像生成模型ganvaeflow-baseddiffusion">2.1 几种图像生成模型：GAN/VAE/Flow-based/Diffusion</h3> 
<p></p> 
<p class="img-center"><img alt="" height="483" src="https://images2.imgbox.com/33/e9/kZTJCeMn_o.png" width="701"></p> 
<p>Fig. 几种生成式模型（generative models）[13]</p> 
<p>Diffusion 模型受到了非平衡热力学（non-equilibrium thermodynamics）的启发，但其背后是一些<strong>有趣的数学概念</strong>。 它仍然有大家已经熟悉的 encoder-decoder 结构，但底层思想与传统的 VAE（variational autoencoders）已经不同。</p> 
<p>要理解这个模型，需要从原理和数学上描述正向和反向扩散过程。</p> 
<blockquote> 
 <p>公式看不懂可忽略，仅靠本文这点篇幅也是不可能推导清楚的。感兴趣可移步 [13-15]。</p> 
</blockquote> 
<h3 id="22-正向图像扩散forward-image-diffusion">2.2 正向图像扩散（forward image diffusion）</h3> 
<h4 id="221-基本原理">2.2.1 基本原理</h4> 
<p><strong>向图像逐渐添加高斯噪声</strong>，直到图像完全无法识别。</p> 
<p>这个过程可以被形式化为<strong>顺序扩散马尔可夫链</strong>（Markov chain of sequential diffusion steps）。</p> 
<h4 id="222-数学描述">2.2.2 数学描述</h4> 
<ul><li>假设<strong>图像</strong>服从某种<strong>初始分布</strong> q(x0)q(x0)，</li><li>那么，我们可以对这个分布<strong>采样</strong>，<strong>得到一个图像</strong> x0x0，</li><li>接下来，我们希望执行一系列的扩散步骤，x0→x1→...→xTx0→x1→...→xT，<strong>每次扩散都使图像越来越模糊</strong>。</li><li>如何添加噪声？由一个 <strong><code>noising schedule</code></strong>（加噪计划/调度） {βt}Tt=1{βt}t=1T 定义， 对于每个 t=1,...,Tt=1,...,T，有 βt∈(0,1)βt∈(0,1)。</li></ul> 
<p>基于以上定义，我们就可以将正向扩散过程描述为</p> 
<p>q(xt∣xt−1)=N(√1−βtxt−1,βtI).q(xt∣xt−1)=N(1−βtxt−1,βtI).</p> 
<blockquote> 
 <p>注，</p> 
 <ol><li>NN 可能来自 next 首字母，表示下一个状态的概率分布（马尔科夫状态转移概率）；</li></ol> 
</blockquote> 
<p>几点解释：</p> 
<ol><li>随着加噪次数的增多 (T→∞)(T→∞)，<strong>最终分布</strong> q(xT)q(xT) 将趋近于常见的<strong>各向同性高斯分布</strong>（isotropic Gaussian distribution），这使得未来的采样非常简单和高效。</li><li> <p>使用高斯核加噪还有一个好处 —— 可以<strong>绕过中间加噪步骤，直接得到任意中间状态</strong>（intermediate latent state）， 这要归功于 reparametrization。直接采样，</p> q(xt∣x0)=N(√¯αtx0,(1−¯αt)I)=√¯αtx0+√1−¯αt⋅ϵ,q(xt∣x0)=N(α¯tx0,(1−α¯t)I)=α¯tx0+1−α¯t⋅ϵ, <p>其中 αt:=1−βtαt:=1−βt，¯αt:=∏tk=0αkα¯t:=∏k=0tαk，ϵ∼N(0,I)ϵ∼N(0,I)。 这里的 ϵϵ 表示高斯噪声 —— <strong>这个公式对于模型训练至关重要</strong>。</p> </li></ol> 
<h3 id="23-反向图像扩散reverse-image-diffusion">2.3 反向图像扩散（reverse image diffusion）</h3> 
<h4 id="231-基本原理">2.3.1 基本原理</h4> 
<p>正向过程定义好了，是否能定义一个反向过程 q(xt−1∣xt)q(xt−1∣xt)， 从噪声回溯到图像呢？</p> 
<p></p> 
<p class="img-center"><img alt="" height="1200" src="https://images2.imgbox.com/2c/e1/zIiKDHbR_o.png" width="1200"></p> 
<p>Fig. The Markov chain of forward (reverse) diffusion process of generating a sample by slowly adding (removing) noise [13]</p> 
<p>首先，从概念上来说，是不行的；</p> 
<p>其次，这需要 marginalization over the entire data distribution。 要从加噪样本返回到起始分布 q(x0)q(x0)，必须对所有可能从噪声中得到 x0x0 的方式进行 marginalization，包括所有中间状态。 这意味着计算积分 ∫q(x0:T)dx1:T∫q(x0:T)dx1:T，这是不可行的。</p> 
<p>不过，虽然无法精确计算，但可以近似！ 核心思想是以可学习网络（<strong><code>learnable network</code></strong>）的形式， <strong>近似反向扩散过程</strong>。</p> 
<h4 id="232-数学表示">2.3.2 数学表示</h4> 
<p></p> 
<p class="img-center"><img alt="" height="1200" src="https://images2.imgbox.com/cf/4d/4hXPWRHp_o.png" width="1200"></p> 
<p>Fig. An example of training a diffusion model for modeling a 2D swiss roll data. [2]</p> 
<p>实现这一目标的第一步是<strong>估计去噪步骤的均值和协方差</strong>（mean and covariance）：</p> 
<p>pθ(xt−1∣xt)=N(μθ(xt,t),Σθ(xt,t)).pθ(xt−1∣xt)=N(μθ(xt,t),Σθ(xt,t)).</p> 
<p>在实践中，</p> 
<ul><li>可以通过神经网络估计 μθ(xt,t)μθ(xt,t)，</li><li>Σθ(xt,t)Σθ(xt,t) 可以固定为与 noising schedule 相关的常数，例如 βtIβtI。</li></ul> 
<p>用这种方式估计 μθ(xt,t)μθ(xt,t) 是可行的，但 Ho 等 [3] 提出了另一种训练方法： 训练一个神经网络 ϵθ(xt,t)ϵθ(xt,t) 来预测前面公式 q(xt∣x0)q(xt∣x0) 中的噪声 ϵϵ。</p> 
<p>与 Ho 等的方法类似[3]，训练过程包括以下步骤：</p> 
<ol><li>采样图像 x0∼q(x0)x0∼q(x0)，</li><li>在扩散过程中选择特定的步骤 t∼U({1,2,...,T})t∼U({1,2,...,T})，</li><li>添加噪声 ϵ∼N(0,I)ϵ∼N(0,I)，</li><li>估计噪声 ϵθ(xt,t)=ϵθ(√¯αtx0+√1−¯αt⋅ϵ,t)ϵθ(xt,t)=ϵθ(α¯tx0+1−α¯t⋅ϵ,t)，</li><li>通过梯度下降学习网络上的损失 ∇θ∥ϵ−ϵθ(xt,t)∥2∇θ‖ϵ−ϵθ(xt,t)‖2。</li></ol> 
<p>一般来说，损失可以表示为</p> 
<p>Ldiffusion=Et,x0,ϵ[∥ϵ−ϵθ(xt,t)∥2],Ldiffusion=Et,x0,ϵ[‖ϵ−ϵθ(xt,t)‖2],</p> 
<p>这里的公式、参数化和推导都没有详细展开，<strong>想深入了解推导过程，强烈推荐 [13-15]</strong>。</p> 
<p>以上已经解释了为什么扩散模型也是一种生成模型。 一旦模型 ϵθ(xt,t)ϵθ(xt,t) 训练好，就可以用它从加躁的 xtxt 回溯到原始图像 x0x0。 由于从各向同性高斯分布中采样噪声非常简单，我们可以获得无限的图像变化。</p> 
<h3 id="24-引导扩散guiding-the-diffusion">2.4 引导扩散（guiding the diffusion）</h3> 
<p>如果在训练过程中<strong>向神经网络提供额外的信息</strong>，就可以<strong>引导图像的生成</strong>。 假设图像已经打标（labeled），关于图像的类别信息 yy 就可以送到 class-conditional diffusion model ϵθ(xt,t∣y)ϵθ(xt,t∣y) 中。</p> 
<ul><li> <p>引入指导的一种方式是<strong>训练一个单独的模型</strong>，该模型作为<strong>噪声图像的分类器</strong>（classifier of noisy images）。</p> <p>在每个去噪步骤中，分类器检查图像是否以正确的方向去噪，并将自己的损失函数梯度计入扩散模型的整体损失中。</p> </li><li> <p>Ho &amp; Salimans 提出了一种<strong>无需训练额外分类器</strong>，就能将类别信息输入模型的方法 [5]。</p> <p>训练过程中，模型 ϵθ(xt,t∣y)ϵθ(xt,t∣y) 有时（以固定概率）类别标签被替换为空标签 ∅∅，也就是不显示实际的类别 yy。 因此，它<strong>学会了在有和没有引导的情况下进行扩散</strong>。</p> <p>对于推理，模型进行两次预测，一次给定类别标签 ϵθ(xt,t∣y)ϵθ(xt,t∣y)，一次不给定 ϵθ(xt,t∣∅)ϵθ(xt,t∣∅)。 模型的最终预测变成乘以引导比例（guidance scale） s⩾1s⩾1，</p> ^ϵθ(xt,t∣y)=ϵθ(xt,t∣∅)+s⋅(ϵθ(xt,t∣y)−ϵθ(xt,t∣∅))ϵ^θ(xt,t∣y)=ϵθ(xt,t∣∅)+s⋅(ϵθ(xt,t∣y)−ϵθ(xt,t∣∅)) <p>这种无分类器引导（classifier-free guidance）复用主模型的理解力，不需要额外的分类器，Nichol 等的研究显示这种方式效果更好 [6]。</p> </li></ul> 
<h2 id="3-glide文本引导定向扩散202204">3 <strong><code>GLIDE</code></strong>：文本引导，定向扩散，<strong><code>2022.04</code></strong></h2> 
<p>以上介绍了扩散模型的工作原理，现在要回答的两个问题是：</p> 
<ul><li>如何使用文本信息（textual information）来引导扩散模型？</li><li>如何确保模型的质量足够好？</li></ul> 
<p>GLIDE 论文见提出了非常新颖和有趣的见解 [6]。</p> 
<h3 id="31-架构">3.1 架构</h3> 
<p>三个主要组件：</p> 
<ol><li>一个基于 <strong><code>UNet</code></strong> 的模型：负责扩散的<strong>视觉部分</strong>（visual part of the diffusion learning），</li><li>一个基于 <strong><code>Transformer</code></strong> 的模型：负责将文本片段转换成<strong>文本嵌入</strong>（creating a text embedding from a snippet of text），</li><li>一个 <strong><code>upsampling</code></strong> 扩散模型：<strong>增大输出图像的分辨率</strong>。</li></ol> 
<p>前两个组件生成一个文本引导的图像，最后一个组件用于扩大图像并保持质量。</p> 
<h3 id="32-工作原理">3.2 工作原理</h3> 
<p>GLIDE 模型的核心是著名的 UNet 架构 [8]，用于扩散。</p> 
<ul><li>串联几个下采样和上采样卷积的残差层。</li><li>还包括 attention 层，这对于同时进行文本处理至关重要。</li><li>模型有约 <strong><code>2.3b</code></strong> 个参数，并在与 DALL·E 相同的数据集上进行了训练。</li></ul> 
<p>用于引导的文本（text used for guidance）编码为 token，并送入 transformer 模型。 GLIDE 中使用的 transformer 有约 <strong><code>1.2b</code></strong> 个参数， 由 24 个 2048-width 的残差块构建。transformer 的输出有两个目的：</p> 
<ol><li>最终 embedding token 用作 ϵθ(xt,t∣y)ϵθ(xt,t∣y) 中的 class embedding yy，</li><li>final layer of token embeddings 添加到模型的<strong>每个</strong> attention layer 中。</li></ol> 
<p>很明显，为了生成图像的准确性，大量精力放在了确保模型获得足够的与文本相关的上下文（text-related context）。 根据 text snippet embedding，模型将编码的文本与 attention 上下文拼接（concatenate），并在训练期间使用无分类器引导。</p> 
<p>最后，使用扩散模型，通过一个 <strong><code>ImageNet upsampler</code></strong> 将图像从低分辨率转成高分辨率。</p> 
<h3 id="33-小结">3.3 小结</h3> 
<p></p> 
<p class="img-center"><img alt="" height="361" src="https://images2.imgbox.com/fe/42/XZ6jFElC_o.png" width="362"></p> 
<p>Fig. GLIDE 效果。提示词 "a corgi in a field"（田野里一只柯基） [6]</p> 
<p>GLIDE 融合了近年的几项技术精华，为文本引导图像生成带来了新的启示。 考虑到 DALL·E 模型是基于不同结构（非扩散）构建的，因此，可以说 GLIDE 开启了<strong>扩散式文生图时代</strong>。</p> 
<h2 id="4-dalle-2取长补短先验称奇202204">4 <strong><code>DALL·E 2</code></strong>：取长补短，先验称奇，<strong><code>2022.04</code></strong></h2> 
<p>OpenAI 团队马不停蹄，在 2022 年 4 月份以 DALL·E 2 [7] 再次震撼了整个互联网。 它<strong>组合</strong>了前面介绍的 CLIP 模型和 GLIDE 架构的精华。</p> 
<p></p> 
<p class="img-center"><img alt="" height="920" src="https://images2.imgbox.com/ef/ef/paymjyEc_o.png" width="1200"></p> 
<p>Fig. Visualization of DALL·E 2 two-stage mechanism. [13]</p> 
<h3 id="41-架构unclip--prior--decoder">4.1 架构：<code>unCLIP = prior + decoder</code></h3> 
<p>两个主要基础组件（也是两个阶段），</p> 
<ul><li>prior</li><li>decoder</li></ul> 
<p>二者组合产生图像输出。整个机制名为 <strong><code>unCLIP</code></strong>， 如果还记得前面介绍的 CLIP 机制，就能从 <code>unCLIP</code> 这个名字猜到底层可能是如何工作的。</p> 
<h3 id="42-the-prior">4.2 The prior</h3> 
<p>第一阶段称为 <code>prior</code>，作用是将标题 —— 例如 “a corgi playing a flame throwing trumpet” —— 从文本转换成<strong>文本嵌入</strong>（text embedding）。</p> 
<p>这个通过一个<strong>冻结参数的 CLIP 模型</strong>实现的。</p> 
<h4 id="421-为什么需要-prior-层为什么单单-clip-不够">4.2.1 为什么需要 prior 层（为什么单单 CLIP 不够）</h4> 
<p>前面介绍过，CLIP 模型记录的 text embedding 和 image embedding 的联合分布。所以， 直觉上来说，有了一个经过良好训练的 CLIP 模型，只要经过下面简单三步就能完成文生图的任务：</p> 
<ol><li>将文本（提示词）转换成对应的 text embedding；</li><li>将 text embedding 输入 CLIP 模型，获取最佳的 image embedding；</li><li>用 image embedding 通过扩散生成图像。</li></ol> 
<p>这里有问题吗？有，在<strong>第 2 步</strong>。DALL·E 2 的作者给了一个很好的解释：</p> 
<blockquote> 
 <p>“（在 CLIP 空间里）可能有无数的图像与给定的标题一致，因此两个编码器的输出不会完全一致。 因此，<strong>需要一个单独的 prior 模型来将 text embedding “翻译”为对应的 image embedding</strong> ”。</p> 
</blockquote> 
<p>下面是对比：</p> 
<p></p> 
<p class="img-center"><img alt="" height="269" src="https://images2.imgbox.com/ca/08/5V94HWw7_o.png" width="806"></p> 
<p>Fig. 分别通过三种方式生成的图片：仅标题（caption）、标题+CLIP 和 prior-based。[7]</p> 
<h4 id="422-prior-选型decoder-only-transformer">4.2.2 prior 选型：decoder-only transformer</h4> 
<p>作者对 prior 模型测试了两类模型：</p> 
<ol><li>自回归模型（autoregressive model）</li><li>扩散模型（diffusion model）</li></ol> 
<p>本文只讨论第二种：扩散 prior 模型。因为从计算角度来看，它的<strong>性能优于自回归模型</strong>。</p> 
<p>为了训练 prior 模型，选择一个 <strong><code>decoder-only Transformer</code></strong>， 通过以下几个输入进行训练：</p> 
<ol><li>已编码的文本（encoded text）</li><li>CLIP text embedding</li><li>embedding for the diffusion timestep</li><li>加噪的 image embedding</li></ol> 
<p>目标是<strong>输出一个无噪的 image embedding </strong>（unnoised image embedding）zizi。</p> 
<h4 id="423-损失函数">4.2.3 损失函数</h4> 
<p>直接预测未加噪声的 image embedding 而不是预测噪声更合适，这与之前讨论的 Ho 等提出的训练方式不同。 因此，回顾前面引导模型中扩散损失的公式</p> 
<p>Ldiffusion=Et,x0,ϵ[∥ϵ−ϵθ(xt,t∣y)∥2],Ldiffusion=Et,x0,ϵ[‖ϵ−ϵθ(xt,t∣y)‖2],</p> 
<p>我们可以将 prior 扩散损失（the prior diffusion loss）表示为</p> 
<p>Lprior:diffusion=Et[∥∥zi−fθ(zti,t∣y)∥∥2],Lprior:diffusion=Et[‖zi−fθ(zit,t∣y)‖2],</p> 
<p>其中</p> 
<ul><li>fθfθ：prior 模型</li><li>ztizit：带噪图像的嵌入</li><li>tt：时间戳</li><li>yy：用于引导的标题。</li></ul> 
<h4 id="424-小结">4.2.4 小结</h4> 
<p>以上就是 unCLIP 的前半部分，旨在生成一个能够将文本中的所有重要信息封装到 CLIP 样式的 image embedding 中的模型。 有了这个模型之后，就能根据用户输入的文本得到一个 image embedding。 而有了 image embedding，就能基于扩散模型<strong>反向</strong>（un-）生成最终的视觉输出 —— 这就是 <strong><code>unCLIP</code></strong> 名称的由来 —— 从 image embedding 回溯到图像，与训练 CLIP image encoder 的过程相反。</p> 
<p>接下来看 DALL·E 2 中是如何实现这个反向过程的。</p> 
<h3 id="42-the-decoder基于-glide-的改进">4.2 The decoder：基于 GLIDE 的改进</h3> 
<blockquote> 
 <p><strong>一个扩散模型的尽头是另一个扩散模型！</strong>（After one diffusion model it is time for another diffusion model!）。</p> 
</blockquote> 
<p>在 DALL·E 2 中，“另一个扩散模型”就是前面已经介绍过的 <strong><code>GLIDE</code></strong>。</p> 
<p>对 GLIDE 做了点修改，将 prior 输出的 CLIP image embedding 添加到 vanilla GLIDE text encoder 中。 其实这正是 <strong>prior 的训练目标 - 为 decoder 提供信息</strong>。</p> 
<h3 id="43-引导扩散和上采样">4.3 引导扩散和上采样</h3> 
<p>引导方式与普通的 GLIDE 一样。为改进效果，10% 概率将 CLIP embedding 设置为 ∅∅，50% 概率设置文本标题 yy。</p> 
<p>跟 GLIDE 一样，在图像生成之后，利用另一个扩散模型进行上采样。 这次用了两个上采样模型（而不是原始 GLIDE 中的一个），一个将图像从 64x64 增加到 256x256，另一个进一步提高分辨率到 1024x1024。</p> 
<h2 id="5-google-imagen删繁就简扩散三连202205">5 Google <strong><code>Imagen</code></strong>：删繁就简，扩散三连，<strong><code>2022.05</code></strong></h2> 
<p>DALL·E 2 发布不到两个月， Google Brain 团队也展示了自己的最新成果 - Imagen（Saharia 等 [7]）。</p> 
<h3 id="51-架构t5-xxl--diffusion--diffusion--diffusion">5.1 架构：<code>T5-XXL + Diffusion + Diffusion + Diffusion</code></h3> 
<p></p> 
<p class="img-center"><img alt="" height="644" src="https://images2.imgbox.com/70/e8/8fwa2q2R_o.png" width="635"></p> 
<p>Fig. Overview of Imagen architecture. [7]</p> 
<p>Imagen 架构在结构上非常简单：</p> 
<ol><li>预训练的<strong>文本模型</strong>用于创建 embedding，然后用这些 embedding 扩散成图像；</li><li>通过超分辨率扩散模型增加分辨率。</li></ol> 
<p>但架构中还是有一些新颖之处，比如模型本身和训练过程，总体来说还是先进一些。 这里只介绍下它与前面几个模型不同之处。</p> 
<h3 id="52-与-glidedalle-2-等架构的不同">5.2 与 GLIDE、DALL·E 2 等架构的不同</h3> 
<h4 id="521-使用预训练的-transformer-t5-xxl-而不是从头开始训练">5.2.1 使用预训练的 transformer (<strong><code>T5-XXL</code></strong>) 而不是从头开始训练</h4> 
<p>与 OpenAI 的工作相比，这是<strong>核心区别</strong>。</p> 
<p>对于 text embedding，</p> 
<ul><li>GLIDE 使用了一个<strong>新的、经过专门训练的</strong> transformer 模型；</li><li>Imagen 使用了一个<strong>预训练的、冻结的 T5-XXL</strong> 模型 [4]。</li></ul> 
<p>这里的想法是，T5-XXL 模型在语言处理方面比仅在图像标题上训练的模型<strong>有更多的上下文</strong>， 因此能够<strong>在不需要额外微调的情况下产生更有价值的 embedding</strong>。</p> 
<h4 id="522-使用更高效的底层神经网络efficient-u-net">5.2.2 使用更高效的底层神经网络（efficient U-net）</h4> 
<p>使用了称为 <strong><code>Efficient U-net</code></strong> 的升级版神经网络， 作为<strong>超分辨率</strong>扩散模型的核心。</p> 
<p>比之前的版本更节省内存，更简单，并且收敛速度更快。 主要来自残差块和网络内部值的额外缩放。细节详见 [7]。</p> 
<h4 id="523-使用-conditioning-augmentation-来增强图像保真度image-fidelity">5.2.3 使用 conditioning augmentation 来增强图像保真度（image fidelity）</h4> 
<p>Imagen 可以视为是<strong>一系列扩散模型</strong>，因此在<strong>模型连接处</strong> （areas where the models are linked）可以进行增强。</p> 
<ul><li>Ho 等提出了一种称为条件增强（conditioning augmentation）的解决方案[10]。 简单来说就是在将低分辨率图像<strong>输入超分辨率模型之前</strong>对其 apply <strong>多个 data augmentation 技术</strong>，如高斯模糊。</li><li>还有一些对于低 FID score 和高图像保真度至关重要的资源（例如 dynamic thresholding）， 论文 [7] 中有详细解释。但这些方法的核心已经在前几节都涵盖了。</li></ul> 
<h3 id="53-小结">5.3 小结</h3> 
<p>截至 2022.04，Google’s Imagen 是最好的 text-to-image generation 模型。</p> 
<p></p> 
<p class="img-center"><img alt="" height="411" src="https://images2.imgbox.com/37/69/HBPICdY4_o.png" width="1048"></p> 
<p>Fig. Imagen 根据提示词生成的一些图片。[7]</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/dffe689f7771bb6798406094e58dab3d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【C&#43;&#43;高阶】：特殊类设计和四种类型转换</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/33f88781dadcfad9759bfef4e8eff719/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Java语言程序设计——篇十三（1）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>