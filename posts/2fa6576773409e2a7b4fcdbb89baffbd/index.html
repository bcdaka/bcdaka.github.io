<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Docker搭建hadoop和spark集群 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/2fa6576773409e2a7b4fcdbb89baffbd/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Docker搭建hadoop和spark集群">
  <meta property="og:description" content="Docker搭建hadoop和spark集群 本次集群的配置如下
Docker 25.0.3JDK 1.8Hadoop 3.1.2Hive 3.1.2mysql 8.0.1mysql-connector-java-8.0.1.jarhive_jdbc_2.5.15.1040 拉取docker镜像，初始化hadoop集群 拉取docker镜像,该镜像为ubuntu:22.04,已经安装好了hadoop docker pull registry.cn-hangzhou.aliyuncs.com/hadoop_test/hadoop_base 使用docker images可以查看已经拉取的镜像的列表 docker images 建立使用桥接模式的docker子网，一般来说下载完docker会在宿主机虚拟出一个172.17.0.0/16的子网出来，这里我们使用桥接模式创建了一个172.19.0.0/16的子网出来 docker network create --driver=bridge --subnet=172.19.0.0/16 hadoop 使用拉取的镜像，启动三个容器，分别是Master,Slave1,Slave2作为集群的三个节点 docker run -it --network hadoop --ulimit nofile=65535 -h Slave1 --name Slave1 registry.cn-hangzhou.aliyuncs.com/hadoop_test/hadoop_base bash docker run -it --network hadoop --ulimit nofile=65535 -h Slave2 --name Slave2 registry.cn-hangzhou.aliyuncs.com/hadoop_test/hadoop_base bash docker run -it --network hadoop --ulimit nofile=100000 -h Master --name Master -p 9870:9870 -p 8088:8088 -p 10000:10000 registry.cn-hangzhou.aliyuncs.com/hadoop_test/hadoop_base bash 使用dokcer ps -a查看所用容器的状态 docker ps -a 可以使用以下命令启动容器 docker start Master Slave1 Slave2 使用dokcer attach命令进入容器的bash docker attach Master #Slave1 or Slave2 注意以下命令使用sudo是在非root用户下才需如此,默认是root用户,不需要使用sudo">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-01T11:23:11+08:00">
    <meta property="article:modified_time" content="2024-04-01T11:23:11+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Docker搭建hadoop和spark集群</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="Dockerhadoopspark_1"></a>Docker搭建hadoop和spark集群</h2> 
<p><strong>本次集群的配置如下</strong></p> 
<ul><li>Docker 25.0.3</li><li>JDK 1.8</li><li>Hadoop 3.1.2</li><li>Hive 3.1.2</li><li>mysql 8.0.1</li><li>mysql-connector-java-8.0.1.jar</li><li>hive_jdbc_2.5.15.1040</li></ul> 
<h3><a id="dockerhadoop_13"></a>拉取docker镜像，初始化hadoop集群</h3> 
<ol><li>拉取docker镜像,该镜像为ubuntu:22.04,已经安装好了hadoop</li></ol> 
<pre><code class="prism language-shell"><span class="token function">docker</span> pull registry.cn-hangzhou.aliyuncs.com/hadoop_test/hadoop_base
</code></pre> 
<ul><li>使用docker images可以查看已经拉取的镜像的列表</li></ul> 
<pre><code class="prism language-shell"><span class="token function">docker</span> images
</code></pre> 
<ol start="2"><li>建立使用桥接模式的docker子网，一般来说下载完docker会在宿主机虚拟出一个172.17.0.0/16的子网出来，这里我们使用桥接模式创建了一个172.19.0.0/16的子网出来</li></ol> 
<pre><code class="prism language-shell"><span class="token function">docker</span> network create <span class="token parameter variable">--driver</span><span class="token operator">=</span>bridge <span class="token parameter variable">--subnet</span><span class="token operator">=</span><span class="token number">172.19</span>.0.0/16 hadoop
</code></pre> 
<ol start="3"><li>使用拉取的镜像，启动三个容器，分别是Master,Slave1,Slave2作为集群的三个节点</li></ol> 
<pre><code class="prism language-shell"><span class="token function">docker</span> run <span class="token parameter variable">-it</span> <span class="token parameter variable">--network</span>  hadoop <span class="token parameter variable">--ulimit</span> <span class="token assign-left variable">nofile</span><span class="token operator">=</span><span class="token number">65535</span> <span class="token parameter variable">-h</span> Slave1 <span class="token parameter variable">--name</span> Slave1 registry.cn-hangzhou.aliyuncs.com/hadoop_test/hadoop_base <span class="token function">bash</span>
<span class="token function">docker</span> run <span class="token parameter variable">-it</span> <span class="token parameter variable">--network</span> hadoop  <span class="token parameter variable">--ulimit</span> <span class="token assign-left variable">nofile</span><span class="token operator">=</span><span class="token number">65535</span> <span class="token parameter variable">-h</span> Slave2 <span class="token parameter variable">--name</span> Slave2 registry.cn-hangzhou.aliyuncs.com/hadoop_test/hadoop_base <span class="token function">bash</span>
<span class="token function">docker</span> run <span class="token parameter variable">-it</span> <span class="token parameter variable">--network</span> hadoop <span class="token parameter variable">--ulimit</span> <span class="token assign-left variable">nofile</span><span class="token operator">=</span><span class="token number">100000</span>  <span class="token parameter variable">-h</span> Master <span class="token parameter variable">--name</span> Master <span class="token parameter variable">-p</span> <span class="token number">9870</span>:9870 <span class="token parameter variable">-p</span> <span class="token number">8088</span>:8088 <span class="token parameter variable">-p</span> <span class="token number">10000</span>:10000 registry.cn-hangzhou.aliyuncs.com/hadoop_test/hadoop_base <span class="token function">bash</span> 
</code></pre> 
<ul><li>使用dokcer ps -a查看所用容器的状态</li></ul> 
<pre><code class="prism language-shell"><span class="token function">docker</span> <span class="token function">ps</span> <span class="token parameter variable">-a</span>
</code></pre> 
<ul><li>可以使用以下命令启动容器</li></ul> 
<pre><code class="prism language-shell"><span class="token function">docker</span> start Master Slave1 Slave2
</code></pre> 
<ul><li>使用dokcer attach命令进入容器的bash</li></ul> 
<pre><code class="prism language-shell"><span class="token function">docker</span> attach Master <span class="token comment">#Slave1 or Slave2</span>
</code></pre> 
<p><strong>注意以下命令使用sudo是在非root用户下才需如此,默认是root用户,不需要使用sudo</strong></p> 
<ol start="4"><li>进入容器内部,配置ubuntu的apt清华镜像源</li></ol> 
<ul><li>备份源镜像文件</li></ul> 
<pre><code class="prism language-shell"><span class="token function">cp</span> /etc/apt/sources.list /etc/apt/sources.list.bak
</code></pre> 
<ul><li>修改/etc/apt/sources.list, 添加国内镜像源</li></ul> 
<pre><code class="prism language-shell"><span class="token function">vi</span> /etc/apt/sources.lists
</code></pre> 
<pre><code class="prism language-shell">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse
 
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse
 
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse

deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse
</code></pre> 
<ul><li>（同步）更新软件仓库</li></ul> 
<pre><code class="prism language-shell"><span class="token function">apt-get</span> update

<span class="token comment">#安装一些常用的服务与软件</span>
<span class="token function">apt-get</span> <span class="token function">install</span> <span class="token function">vim</span> openssh-server
</code></pre> 
<ol start="5"><li>对三台容器的/etc/host进行修改,对主机名进行映射</li></ol> 
<pre><code class="prism language-shell"><span class="token function">vim</span> /etc/host
</code></pre> 
<ul><li>在/etc/host文件中添加以下信息</li></ul> 
<pre><code class="prism language-shell"><span class="token number">172.19</span>.0.2 Master
<span class="token number">172.19</span>.0.3 Slave1
<span class="token number">172.19</span>.0.4 Slave2
</code></pre> 
<ol start="5"><li>ssh配置，拉取的镜像已经生成好了公钥和私钥对，我们只需要修改几个参数，就可以实现容器之间还有容器与宿主机的通信</li></ol> 
<ul><li>修改/etc/ssh/sshd_config， 修改下面几种参数</li></ul> 
<pre><code class="prism language-shell">PermitRootLogin <span class="token operator">=</span> <span class="token boolean">true</span> <span class="token comment"># 允许使用root用户登录</span>
PasswordAuthentication <span class="token operator">=</span> <span class="token function">yes</span>
PubKeyAuthentication <span class="token operator">=</span> <span class="token function">yes</span>
</code></pre> 
<p>6.拉取的镜像的环境变量默认配置在/etc/profile中</p> 
<ul><li>/etc/profile</li></ul> 
<pre><code class="prism language-shell"><span class="token comment">#java</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">JAVA_HOME</span><span class="token operator">=</span>/usr/lib/jvm/java-8-openjdk-amd64
<span class="token builtin class-name">export</span> <span class="token assign-left variable">JRE_HOME</span><span class="token operator">=</span><span class="token variable">${JAVA_HOME}</span>/jre
<span class="token builtin class-name">export</span> <span class="token assign-left variable">CLASSPATH</span><span class="token operator">=</span>.:<span class="token variable">${JAVA_HOME}</span>/lib:<span class="token variable">${JRE_HOME}</span>/lib
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token variable">${JAVA_HOME}</span>/bin:<span class="token environment constant">$PATH</span>
<span class="token comment">#hadoop</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_HOME</span><span class="token operator">=</span>/usr/local/hadoop
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$HADOOP_HOME</span>/bin:<span class="token variable">$HADOOP_HOME</span>/sbin
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_COMMON_HOME</span><span class="token operator">=</span><span class="token variable">$HADOOP_HOME</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_HDFS_HOME</span><span class="token operator">=</span><span class="token variable">$HADOOP_HOME</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_MAPRED_HOME</span><span class="token operator">=</span><span class="token variable">$HADOOP_HOME</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_YARN_HOME</span><span class="token operator">=</span><span class="token variable">$HADOOP_HOME</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_INSTALL</span><span class="token operator">=</span><span class="token variable">$HADOOP_HOME</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_COMMON_LIB_NATIVE_DIR</span><span class="token operator">=</span><span class="token variable">$HADOOP_HOME</span>/lib/native
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_LIBEXEC_DIR</span><span class="token operator">=</span><span class="token variable">$HADOOP_HOME</span>/libexec
<span class="token builtin class-name">export</span> <span class="token assign-left variable">JAVA_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$HADOOP_HOME</span>/lib/native:<span class="token variable">$JAVA_LIBRARY_PATH</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_CONF_DIR</span><span class="token operator">=</span><span class="token variable">$HADOOP_HOME</span>/etc/hadoop
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HDFS_DATANODE_USER</span><span class="token operator">=</span>root
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HDFS_DATANODE_SECURE_USER</span><span class="token operator">=</span>root
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HDFS_SECONDARYNAMENODE_USER</span><span class="token operator">=</span>root
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HDFS_NAMENODE_USER</span><span class="token operator">=</span>root
<span class="token builtin class-name">export</span> <span class="token assign-left variable">YARN_RESOURCEMANAGER_USER</span><span class="token operator">=</span>root
<span class="token builtin class-name">export</span> <span class="token assign-left variable">YARN_NODEMANAGER_USER</span><span class="token operator">=</span>root
</code></pre> 
<ul><li>启用容器的时候默认不会source /etc/profile,因此需要在bash的配置文件中添加语句 source /etc/profile,ssh 服务默认也是不会启用的，还需要在 bash的配置文件中添加service ssh start</li></ul> 
<pre><code class="prism language-shell"><span class="token function">vim</span> ~/.bashrc
</code></pre> 
<ul><li>添加以下内容</li></ul> 
<pre><code class="prism language-shell"><span class="token builtin class-name">source</span> /etc/profile
<span class="token function">service</span> <span class="token function">ssh</span> start
</code></pre> 
<ul><li>让环境变量生效</li></ul> 
<pre><code class="prism language-shell"><span class="token builtin class-name">source</span> ~/.bashrc
</code></pre> 
<ol start="7"><li>初始化hdfs,启动hadoop集群</li></ol> 
<ul><li>初始化</li></ul> 
<pre><code class="prism language-shell">hadoop namenode <span class="token parameter variable">-format</span>
</code></pre> 
<ul><li>启动全部和服务</li></ul> 
<pre><code class="prism language-shell">start-all.sh
</code></pre> 
<ul><li>输入jps指令，如果能查看到以下进程则代表hadoop集群成功启用了</li></ul> 
<pre><code class="prism language-shell"><span class="token number">1104</span> ResourceManager
<span class="token number">1456</span> NodeManager
<span class="token number">774</span> SecondaryNameNode
<span class="token number">326</span> NameNode
<span class="token number">521</span> DataNode
<span class="token number">1690</span> Jps
</code></pre> 
<h3><a id="Spark_192"></a>Spark集群搭建</h3> 
<p>拉取的镜像没有配置spark,手动配置</p> 
<ol><li>去官网下载spark</li></ol> 
<ul><li>在宿主机中,将spark的压缩包闯入容器内部</li></ul> 
<pre><code class="prism language-shell"><span class="token function">docker</span> <span class="token function">cp</span> spark-3.1.2-bin-hive3.1.2.tgz Master:/root
</code></pre> 
<ul><li>docker attach进入容器，解压到/usr/local/中</li></ul> 
<pre><code class="prism language-shell"><span class="token function">tar</span> <span class="token parameter variable">-xvf</span> /root/spark-3.1.2-bin-hive3.1.2 /usr/local/
</code></pre> 
<ul><li>修改文件夹名称</li></ul> 
<pre><code class="prism language-shell"><span class="token function">mv</span> spark-3.1.2-bin-hive3.1.2 spark
</code></pre> 
<ol start="2"><li>配置spark</li></ol> 
<ul><li>进入spark/conf/</li></ul> 
<pre><code class="prism language-shell"><span class="token builtin class-name">cd</span> spark/conf

<span class="token function">mv</span> spark-env.sh.template spark-env.sh
<span class="token function">vim</span> spark-env.sh
</code></pre> 
<ul><li>添加以下内容</li></ul> 
<pre><code class="prism language-shell"><span class="token builtin class-name">export</span> <span class="token assign-left variable">SPARK_DIST_CLASSPATH</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span>/usr/local/hadoop/bin/hadoop classpath<span class="token variable">)</span></span>
</code></pre> 
<ul><li>配置worker工作节点</li></ul> 
<pre><code class="prism language-shell"><span class="token function">vim</span> wokrer
</code></pre> 
<ul><li>添加</li></ul> 
<pre><code class="prism language-shell">Master
Slave1
Slave2
</code></pre> 
<ol start="3"><li>注意hadoop的guava与spark的guava包冲突，选择两者中高版本的，删除低版本的</li></ol> 
<ul><li>这里hadoop的guava包版本比较高</li></ul> 
<pre><code class="prism language-shell"><span class="token function">cp</span> /usr/local/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar /usr/local/spark/jars 
<span class="token function">mv</span> /usr/local/spark/jars/guava-14.0.1.jar /usr/local/spark/jars/guava-14.0.1.jar.bak
</code></pre> 
<ol start="3"><li>将spark文件文件传输到其他节点上</li></ol> 
<pre><code class="prism language-shell"><span class="token function">scp</span> <span class="token parameter variable">-r</span> /usr/local/spark Slave1:/usr/local
<span class="token function">scp</span> <span class="token parameter variable">-r</span> /usr/local/spark Slave2:/usr/local
</code></pre> 
<ol start="4"><li>启动spark</li></ol> 
<pre><code class="prism language-shell">/usr/local/spark/sbin/start-all.sh
</code></pre> 
<p>查看到以下信息</p> 
<pre><code class="prism language-shell">starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark/logs/spark--org.apache.spark.deploy.master.Master-1-Master.out
Slave1: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-Slave1.out
Slave2: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-Slave2.out
Master: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-Master.out
</code></pre> 
<h3><a id="pyspark__277"></a>pyspark 环境配置和任务提交</h3> 
<h4><a id="conda_279"></a>配置conda</h4> 
<p>选择conda来管理python虚拟环境</p> 
<ol><li>选择下载miniconda,更加轻量级的环境</li></ol> 
<pre><code class="prism language-shell"><span class="token function">wget</span> https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
</code></pre> 
<ol start="2"><li>安装miniconda</li></ol> 
<pre><code class="prism language-shell"><span class="token function">bash</span> Miniconda3-latest-Linux-x86_64.sh
</code></pre> 
<p>一直回车和yes即可，默认安装目录为 /root</p> 
<ol start="3"><li>如conda命令不生效请自行配置~/.bashrc</li></ol> 
<pre><code class="prism language-shell"><span class="token comment"># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;</span>
<span class="token comment"># !! Contents within this block are managed by 'conda init' !!</span>
<span class="token assign-left variable">__conda_setup</span><span class="token operator">=</span><span class="token string">"<span class="token variable"><span class="token variable">$(</span>'/root/miniconda3/bin/conda<span class="token string">' '</span>shell.bash<span class="token string">' '</span>hook' <span class="token operator"><span class="token file-descriptor important">2</span>&gt;</span> /dev/null<span class="token variable">)</span></span>"</span>
<span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token variable">$?</span> <span class="token parameter variable">-eq</span> <span class="token number">0</span> <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>
    <span class="token builtin class-name">eval</span> <span class="token string">"<span class="token variable">$__conda_setup</span>"</span>
<span class="token keyword">else</span>
    <span class="token keyword">if</span> <span class="token punctuation">[</span> <span class="token parameter variable">-f</span> <span class="token string">"/root/miniconda3/etc/profile.d/conda.sh"</span> <span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>
        <span class="token builtin class-name">.</span> <span class="token string">"/root/miniconda3/etc/profile.d/conda.sh"</span>
    <span class="token keyword">else</span>
        <span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token string">"/root/miniconda3/bin:<span class="token environment constant">$PATH</span>"</span>
    <span class="token keyword">fi</span>
<span class="token keyword">fi</span>
<span class="token builtin class-name">unset</span> __conda_setup
<span class="token comment"># &lt;&lt;&lt; conda initialize &lt;&lt;&lt;</span>
</code></pre> 
<p><strong>其他节点(Slave1,Slave2)也要执行以上操作</strong><br> 4. 使用conda创建虚拟环境</p> 
<pre><code class="prism language-shell">conda create <span class="token parameter variable">-n</span> spark <span class="token assign-left variable">python</span><span class="token operator">==</span><span class="token number">3.8</span>
</code></pre> 
<ul><li>激活环境</li></ul> 
<pre><code class="prism language-shell">conda activae spark
</code></pre> 
<ul><li>安装必要的库</li></ul> 
<pre><code class="prism language-shell">pip <span class="token function">install</span> <span class="token assign-left variable">spark</span><span class="token operator">==</span><span class="token number">3.1</span>.2 <span class="token comment"># pyspark版本要和容器的spark版本一致</span>
pip <span class="token function">install</span> numpy,flask,hdfs3
</code></pre> 
<ul><li>默认环境存储在/root/miniconda3/envs,需要将这份环境拷贝到其他节点</li></ul> 
<pre><code class="prism language-shell"><span class="token function">scp</span> <span class="token parameter variable">-r</span> /root/minicond3/envs/spark Slave1:/root/miniconda3/envs 
<span class="token function">scp</span> <span class="token parameter variable">-r</span> /root/minicond3/envs/spark Slave2:/root/miniconda3/envs
</code></pre> 
<h3><a id="_or_BUG_336"></a>其他问题 or BUG</h3> 
<ol><li>如何映设端口</li></ol> 
<ul><li>使用ssh的方法，使得容器的端口可以映射到宿主机</li></ul> 
<pre><code class="prism language-shell"><span class="token function">ssh</span> <span class="token parameter variable">-L</span> <span class="token number">0.0</span>.0.0:8080:0.0.0.0:8080 root@172.19.0.2
</code></pre> 
<p>在宿主机器的浏览器输入0.0.0.0:8080 就可以查看spark集群的运行状况</p> 
<ol start="2"><li>如何让容器对外网暴露服务</li></ol> 
<ul><li>在容器创建一个web程序flask.py</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">from</span> flask <span class="token keyword">import</span> Flask<span class="token punctuation">,</span> request

app <span class="token operator">=</span> Flask<span class="token punctuation">(</span>__name__<span class="token punctuation">)</span>

<span class="token comment"># 定义路由和处理函数</span>
<span class="token decorator annotation punctuation">@app<span class="token punctuation">.</span>route</span><span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">hello</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token string">'hello,world'</span>


<span class="token comment"># 运行应用</span>
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    app<span class="token punctuation">.</span>run<span class="token punctuation">(</span>host<span class="token operator">=</span><span class="token string">"0.0.0.0"</span><span class="token punctuation">,</span> port<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li>在容器内运行</li></ul> 
<pre><code class="prism language-shell">python flask.py
</code></pre> 
<ul><li>在宿主机发送请求</li></ul> 
<p>curl 127.0.0.1:10000# Docker搭建hadoop和spark集群</p> 
<p><strong>本次集群的配置如下</strong></p> 
<ul><li>Docker 25.0.3</li><li>JDK 1.8</li><li>Hadoop 3.1.2</li><li>Hive 3.1.2</li><li>mysql 8.0.1</li><li>mysql-connector-java-8.0.1.jar</li><li>hive_jdbc_2.5.15.1040</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/49df11dd80fab9f8588047412f17783c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">vsCode 刷 leetcode 使用 Cookie 登录</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6a2ef2c424ca941de4a8d4876a515b68/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Mac OS下Docker的安装与配置</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>