<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【AI基础】大模型部署工具之ollama的安装部署以及api调用 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/b43867cdedb5bd966f1e31397b42f1cc/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【AI基础】大模型部署工具之ollama的安装部署以及api调用">
  <meta property="og:description" content="ollama是大模型部署方案，对应docker，本质也是基于docker的容器化技术。
从前面的文章可以看到，部署大模型做的准备工作是比较繁琐的，包括各个环节的版本对应。ollama提供了一个很好的解决方案。
ollama主要针对主流的LLaMA架构的开源大模型设计，并且已被LangChain、Taskweaver等在内的多个热门项目高度集成。同时ollama提供了openAI兼容的api，可以最大限度的减少理解和开发成本。
一、下载安装ollama 1.1 安装 官方地址：https://ollama.com/
开源地址：https://github.com/ollama/ollama
下载后双击安装：
一路下一步即可。
1.2 检验 ollama安装后默认已经启动，我们可以通过访问其提供的api服务来进行检验。
参考官方文档：ollama的api · ollama/ollama · GitHub
这里运行ollama的机器为windows系统， ip为192.168.3.154。
1.2.1 通过localhost检验 运行命令： &gt; curl http://localhost:11434/api/generate -d &#34;{\&#34;model\&#34;: \&#34;qwen2\&#34;,\&#34;prompt\&#34;: \&#34;who are you?\&#34;,\&#34;stream\&#34;:false}&#34; 查看结果：
这里注意两点：
1、不要使用PowerShell(里面的curl参数不一样)，使用 cmd 或者 git Cmd 。
2、注意参数的引号，通过斜杠 \ 来转义。 1.2.2 通过IP地址检验 运行命令： &gt; curl http://192.168.3.154:11434/api/generate -d &#34;{\&#34;model\&#34;: \&#34;qwen2\&#34;,\&#34;prompt\&#34;: \&#34;who are you?\&#34;,\&#34;stream\&#34;:false}&#34; 查看结果：
提示连接不上：&#34;curl: (7) Failed to connect to 192.168.3.154 port 11434 after 2021 ms: Couldn&#39;t connect to server&#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-01T12:21:13+08:00">
    <meta property="article:modified_time" content="2024-07-01T12:21:13+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【AI基础】大模型部署工具之ollama的安装部署以及api调用</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>ollama是大模型部署方案，对应docker，本质也是基于docker的容器化技术。</p> 
<p>从前面的文章可以看到，部署大模型做的准备工作是比较繁琐的，包括各个环节的版本对应。ollama提供了一个很好的解决方案。</p> 
<p>ollama主要针对主流的LLaMA架构的开源大模型设计，并且已被LangChain、Taskweaver等在内的多个热门项目高度集成。同时ollama提供了openAI兼容的api，可以最大限度的减少理解和开发成本。</p> 
<h2>一、下载安装ollama</h2> 
<h3 style="background-color:transparent;">1.1 安装</h3> 
<p>官方地址：https://ollama.com/</p> 
<p>开源地址：https://github.com/ollama/ollama</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/45/d0/nJGeQ7GZ_o.png" width="1200"></p> 
<p>下载后双击安装：</p> 
<p><img alt="" height="1114" src="https://images2.imgbox.com/0c/90/WCHXrvK1_o.png" width="1200"></p> 
<p>一路下一步即可。</p> 
<h3 style="background-color:transparent;">1.2 检验</h3> 
<p>ollama安装后默认已经启动，我们可以通过访问其提供的api服务来进行检验。</p> 
<p>参考官方文档：<a href="https://github.com/ollama/ollama/blob/main/docs/api.md" title="ollama的api · ollama/ollama · GitHub">ollama的api · ollama/ollama · GitHub</a></p> 
<p>这里运行ollama的机器为windows系统， ip为192.168.3.154。</p> 
<h4 style="background-color:transparent;">1.2.1 通过localhost检验</h4> 
<p>运行命令： </p> 
<pre><code class="language-bash">&gt; curl http://localhost:11434/api/generate -d "{\"model\": \"qwen2\",\"prompt\": \"who are you?\",\"stream\":false}"</code></pre> 
<p>查看结果：</p> 
<p><img alt="" height="798" src="https://images2.imgbox.com/3c/d8/WvCAxjqm_o.png" width="1200"></p> 
<blockquote> 
 <p>这里注意两点：</p> 
 <p>1、不要使用<span style="color:#fe2c24;"><strong>PowerShell</strong></span>(里面的curl参数不一样)，使用 <strong><span style="color:#fe2c24;">cmd</span></strong> 或者 <span style="color:#fe2c24;"><strong>git Cmd</strong></span> 。</p> 
 <p>2、注意参数的引号，通过斜杠<strong><span style="color:#fe2c24;"> \ </span></strong>来转义。 </p> 
</blockquote> 
<h4 style="background-color:transparent;">1.2.2 通过IP地址检验</h4> 
<p>运行命令： </p> 
<pre><code class="language-bash">&gt; curl http://192.168.3.154:11434/api/generate -d "{\"model\": \"qwen2\",\"prompt\": \"who are you?\",\"stream\":false}"</code></pre> 
<p>查看结果：</p> 
<p><img alt="" height="184" src="https://images2.imgbox.com/06/2e/k1uIGP9C_o.png" width="1200"></p> 
<blockquote> 
 <p>提示连接不上："<span style="color:#fe2c24;">curl: (7) Failed to connect to 192.168.3.154 port 11434 after 2021 ms: Couldn't connect to server</span>"。 </p> 
</blockquote> 
<p>这是因为ollama安装后默认只能本地访问，接下来配置远程访问api。</p> 
<h3 style="background-color:transparent;">1.3 配置</h3> 
<p>这里配置主要是因为两个需求：远程可以访问ollama的api接口服务以及自定义大模型存放路径。</p> 
<p>ollama默认把大模型保存在路径 <span style="color:#1c7331;"><strong>用户目录/.ollama/models </strong></span>下：</p> 
<p><img alt="" height="538" src="https://images2.imgbox.com/05/0f/1GU9hoPB_o.png" width="1200"></p> 
<p>基于各种原因，我们可能不希望使用这个默认路径，可以通过环境变量的配置来更改大模型保存的目录。</p> 
<p>添加环境变量 <strong><span style="color:#fe2c24;">OLLAMA_HOST</span></strong> 以及 <span style="color:#fe2c24;"><strong>OLLAMA_MODELS</strong></span>：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/7b/5a/D5vpAM0p_o.png" width="1200"></p> 
<ul><li>- <strong><span style="color:#fe2c24;">OLLAMA_HOST</span></strong>，0.0.0.0， 配置后可以远程访问；</li><li>- <strong><span style="color:#fe2c24;">OLLAMA_MODELS</span></strong>, c:\ai\llms_ollama，配置后ollama拉取的大模型会存放在此路径；</li></ul> 
<blockquote> 
 <p><span style="color:#fe2c24;"><strong>这里有重要的一步，需要重启ollama，使配置生效。</strong></span></p> 
</blockquote> 
<p>在任务栏的ollama图标上点击右键，选择“Quit Ollama”退出ollama：</p> 
<p><img alt="" height="336" src="https://images2.imgbox.com/ae/73/BBxdDbRB_o.png" width="894"></p> 
<p>然后重新打开ollama：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/7a/37/z1lMgJyD_o.png" width="1200"></p> 
<h3>1.4 重新检验</h3> 
<p>这里通过IP地址重新进行检验。</p> 
<h4 style="background-color:transparent;">1.4.1 windows系统</h4> 
<p>运行命令：</p> 
<pre><code class="language-bash">&gt; curl http://192.168.3.154:11434/api/generate -d "{\"model\": \"qwen2\",\"prompt\": \"who are you?\",\"stream\":false}"</code></pre> 
<p>返回结果： </p> 
<p><img alt="" height="660" src="https://images2.imgbox.com/4d/2e/yDKnPKJA_o.png" width="1200"></p> 
<h4 style="background-color:transparent;">1.4.2 linux系统和mac系统</h4> 
<p>生成<span style="color:#fe2c24;"><strong>Completion</strong></span>：</p> 
<pre><code class="language-bash">&gt; curl http://192.168.3.154:11434/api/generate -d '{"model": "qwen2","prompt": "who are you?","stream":false}'</code></pre> 
<p>返回结果：</p> 
<p><img alt="" height="314" src="https://images2.imgbox.com/de/f3/fMOwTa99_o.png" width="1200"></p> 
<blockquote> 
 <p>这里注意参数的格式，和windows系统不一样，引号在这里直接使用不需要转义。 </p> 
</blockquote> 
<p>生成<strong><span style="color:#fe2c24;">Chat Completion</span></strong>：</p> 
<pre><code class="language-bash">&gt; curl http://192.168.3.154:11434/api/chat -d '{
  "model": "qwen2",
  "messages": [
    {
      "role": "user",
      "content": "who are you?"
    }
  ],
  "stream": false
}'</code></pre> 
<p>返回结果：</p> 
<p><img alt="" height="838" src="https://images2.imgbox.com/5a/94/uQ95cX6v_o.png" width="1200"> </p> 
<h2>二、部署运行大模型</h2> 
<p>接下来就是实际来部署一个大模型，这里以llama3为例。</p> 
<h3> 2.1 获取大模型部署命令</h3> 
<p>在ollama官网搜索llama3大模型：<a href="https://ollama.com/library" rel="nofollow" title="https://ollama.com/library">https://ollama.com/library</a></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/bf/80/JDw6r9Sz_o.png" width="1200"></p> 
<p>选择第一个llama3进入大模型详情页：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/af/0e/oMKKogOJ_o.png" width="1200"></p> 
<p>在上图可以看到默认有三个标签可以选择：最新版、8B和70B，这里我们选择 8B的，所以我们需要运行 <span style="color:#fe2c24;"><strong>ollama run llama3</strong></span>。如果我们需要部署70B的，则需要运行 ollama run llama3:70b。</p> 
<h3>2.2 部署大模型</h3> 
<p>我们可以直接运行 ollama run llama3，如果llama3没有下载过则会下载，否则直接运行。也可以先下载然后运行：</p> 
<pre><code class="language-bash">&gt; ollama pull llama3
&gt; ollama run llama3</code></pre> 
<p>可以看到，使用方式跟docker是一样的，大模型对应了docker中的镜像。</p> 
<p>下载完后会提示成功：</p> 
<p><img alt="" height="722" src="https://images2.imgbox.com/84/f9/EdKnKzbO_o.png" width="1200"></p> 
<h3>2.3 和大模型交互</h3> 
<p>接下来可以直接跟llama3对话，在三个箭头➡️后输入问题，llama3会给出回应：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/e6/3c/TVUWJZSd_o.png" width="1200"></p> 
<p>可以看到ollama成功部署了大模型，并成功运行。 </p> 
<h2>三、在LangChain中使用Ollama</h2> 
<h3>3.1 通过jupyter来运行</h3> 
<h4 style="background-color:transparent;">3.1.1 安装jupyter</h4> 
<p>参考 <a href="https://blog.csdn.net/tirestay/article/details/139687713#t1" title="【AI工具】jupyter notebook和jupyterlab对比和安装-CSDN博客">【AI工具】jupyter notebook和jupyterlab对比和安装-CSDN博客</a> 安装jupyterlab。</p> 
<h4>3.1.2 新建一个notebook</h4> 
<p><img alt="" height="771" src="https://images2.imgbox.com/c0/46/1Y0coxTb_o.png" width="1200"></p> 
<p>在新的文件中输入如下代码：</p> 
<pre><code class="language-python"># 引入ollama
from langchain_community.chat_models import ChatOllama

# 加载llama3模型
ollama_llm = ChatOllama(model="llama3")

# 构造Message
from langchain_core.messages import HumanMessage

messages = [
    HumanMessage(
        content="你好，请你介绍一下你自己",
    )
]

# 发送Message
chat_model_response = ollama_llm.invoke(messages)

# 输入Message
chat_model_response</code></pre> 
<p><img alt="" height="407" src="https://images2.imgbox.com/2e/06/wPp8EV7z_o.png" width="1200">这里有个细节，代码一共是五个输入块，这是为了在出错时，可以快速定位是哪一块出了问题。</p> 
<h4>3.1.3 运行</h4> 
<p>现在把鼠标定位在第一行，点击工具栏的运行按钮，一步一步的运行，运行5步后，输出了AI的自我介绍：</p> 
<p><img alt="" height="677" src="https://images2.imgbox.com/a2/d8/qWOKc9mI_o.png" width="1200"></p> 
<p>这样在LangChain中通过ollama，直接调用了大模型。</p> 
<p>可以再问一次二的问题：</p> 
<p><img alt="" height="781" src="https://images2.imgbox.com/fd/32/sh0SFfE2_o.png" width="1200"></p> 
<h3>3.2 直接通过python运行</h3> 
<h4>3.2.1 安装LangChain环境</h4> 
<p>参考 ：<a href="https://blog.csdn.net/tirestay/article/details/139477463#t2" title="【AI基础】第四步：保姆喂饭级-langchain+chatglm2-6b+m3e-base_m3e-base">【AI基础】第四步：保姆喂饭级-langchain+chatglm2-6b+m3e-base_m3e-base</a></p> 
<h4 style="background-color:transparent;">3.2.2 新建python文件</h4> 
<p>输入代码：</p> 
<pre><code class="language-python"># 引入ollama
from langchain_community.chat_models import ChatOllama
# 加载llama3模型
ollama_llm = ChatOllama(model="llama3")
# 构造Message
from langchain_core.messages import HumanMessage
messages = [
    HumanMessage(
        content="你好，请你介绍一下你自己",
    )
]
# 发送Message
chat_model_response = ollama_llm.invoke(messages)
# 输入Message
chat_model_response</code></pre> 
<h4>3.2.3 运行</h4> 
<p>执行命令运行：</p> 
<pre><code class="language-python">&gt; python dev_ollama.py</code></pre> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/ac/e0/Z5AM1pHS_o.png" width="1200"></p> 
<p>运行成功。 </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b596e51aa0707f7b8b44e43aefab99ba/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Linux[高级管理]——Squid代理服务器的部署和应用(反向代理详解)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/cb157b0ab38a72ea90db71a936c7ac50/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">css美化滚动条样式</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>