<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>智谱AI通用大模型：本地部署ChatGLM3-6B开源大模型 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/030630c40b85ba5b98248dbcaf0923c4/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="智谱AI通用大模型：本地部署ChatGLM3-6B开源大模型">
  <meta property="og:description" content="目录
一、ChatGLM3介绍
二、环境配置和检查
2.1 操作系统
2.2 硬件环境
2.3 软件环境
三、本地源码部署
3.1 克隆源码
3.2 下载模型文件
3.3 安装依赖
3.4 代码调用
四、运行Demo
4.1 设置本地模型环境变量
4.2 Gradio 网页版 Demo
4.3 Streamlit 网页版 Demo
4.4 命令行交互Demo
一、ChatGLM3介绍 ChatGLM3 是智谱AI和清华大学 KEG 实验室联合发布的新一代对话预训练模型。
开源模型序列 模型
介绍
代码链接
模型下载
ChatGLM3-6B
第三代 ChatGLM 对话模型。ChatGLM3-6B 采用了全新设计的 Prompt 格式，除正常的多轮对话外。同时原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景。ChatGLM3 Huggingface
魔搭社区 ChatGLM3-6B-base
第三代ChatGLM基座模型。ChatGLM3-6B-Base 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略。在语义、数学、推理、代码、知识等不同角度的数据集上测评显示，ChatGLM3-6B-Base 具有在 10B 以下的基础模型中最强的性能。 Huggingface
魔搭社区
ChatGLM3-6B-32k
第三代ChatGLM长上下文对话模型。在ChatGLM3-6B的基础上进一步强化了对于长文本的理解能力，能够更好的处理最多32K长度的上下文。 Huggingface
魔搭社区
ChatGLM3-6B-128k
ChatGLM3-6B-128K在ChatGLM3-6B的基础上进一步强化了对于长文本的理解能力，能够更好的处理最多128K长度的上下文。具体地，我们对位置编码进行了更新，并设计了更有针对性的长文本训练方法，在对话阶段使用 128K 的上下文长度训练。在实际的使用中，如果您面临的上下文长度基本在 8K 以内，我们推荐使用ChatGLM3-6B；如果您需要处理超过 8K 的上下文长度，我们推荐使用ChatGLM3-6B-128K。 Huggingface">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-28T08:53:32+08:00">
    <meta property="article:modified_time" content="2024-06-28T08:53:32+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">智谱AI通用大模型：本地部署ChatGLM3-6B开源大模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81ChatGLM3%E4%BB%8B%E7%BB%8D-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81ChatGLM3%E4%BB%8B%E7%BB%8D" rel="nofollow">一、ChatGLM3介绍</a></p> 
<p id="%E4%BA%8C%E3%80%81%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%92%8C%E6%A3%80%E6%9F%A5-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%92%8C%E6%A3%80%E6%9F%A5" rel="nofollow">二、环境配置和检查</a></p> 
<p id="2.1%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-toc" style="margin-left:40px;"><a href="#2.1%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F" rel="nofollow">2.1 操作系统</a></p> 
<p id="2.2%20%E7%A1%AC%E4%BB%B6%E7%8E%AF%E5%A2%83-toc" style="margin-left:40px;"><a href="#2.2%20%E7%A1%AC%E4%BB%B6%E7%8E%AF%E5%A2%83" rel="nofollow">2.2 硬件环境</a></p> 
<p id="2.3%20%E8%BD%AF%E4%BB%B6%E7%8E%AF%E5%A2%83-toc" style="margin-left:40px;"><a href="#2.3%20%E8%BD%AF%E4%BB%B6%E7%8E%AF%E5%A2%83" rel="nofollow">2.3 软件环境</a></p> 
<p id="%E4%B8%89%E3%80%81%E6%9C%AC%E5%9C%B0%E6%BA%90%E7%A0%81%E9%83%A8%E7%BD%B2-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81%E6%9C%AC%E5%9C%B0%E6%BA%90%E7%A0%81%E9%83%A8%E7%BD%B2" rel="nofollow">三、本地源码部署</a></p> 
<p id="3.1%20%E5%85%8B%E9%9A%86%E6%BA%90%E7%A0%81-toc" style="margin-left:40px;"><a href="#3.1%20%E5%85%8B%E9%9A%86%E6%BA%90%E7%A0%81" rel="nofollow">3.1 克隆源码</a></p> 
<p id="3.2%20%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6-toc" style="margin-left:40px;"><a href="#3.2%20%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6" rel="nofollow">3.2 下载模型文件</a></p> 
<p id="3.3%20%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96-toc" style="margin-left:40px;"><a href="#3.3%20%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96" rel="nofollow">3.3 安装依赖</a></p> 
<p id="3.4%20%E4%BB%A3%E7%A0%81%E8%B0%83%E7%94%A8-toc" style="margin-left:40px;"><a href="#3.4%20%E4%BB%A3%E7%A0%81%E8%B0%83%E7%94%A8" rel="nofollow">3.4 代码调用</a></p> 
<p id="%E5%9B%9B%E3%80%81%E8%BF%90%E8%A1%8CDemo-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81%E8%BF%90%E8%A1%8CDemo" rel="nofollow">四、运行Demo</a></p> 
<p id="4.1%20%E8%AE%BE%E7%BD%AE%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%9E%8B%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F-toc" style="margin-left:40px;"><a href="#4.1%20%E8%AE%BE%E7%BD%AE%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%9E%8B%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F" rel="nofollow">4.1 设置本地模型环境变量</a></p> 
<p id="4.2%20Gradio%20%E7%BD%91%E9%A1%B5%E7%89%88%20Demo-toc" style="margin-left:40px;"><a href="#4.2%20Gradio%20%E7%BD%91%E9%A1%B5%E7%89%88%20Demo" rel="nofollow">4.2 Gradio 网页版 Demo</a></p> 
<p id="4.3%C2%A0%20Streamlit%C2%A0%20%E7%BD%91%E9%A1%B5%E7%89%88%20Demo-toc" style="margin-left:40px;"><a href="#4.3%C2%A0%20Streamlit%C2%A0%20%E7%BD%91%E9%A1%B5%E7%89%88%20Demo" rel="nofollow">4.3 Streamlit  网页版 Demo</a></p> 
<p id="4.4%20%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BA%A4%E4%BA%92Demo-toc" style="margin-left:40px;"><a href="#4.4%20%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BA%A4%E4%BA%92Demo" rel="nofollow">4.4 命令行交互Demo</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E4%B8%80%E3%80%81ChatGLM3%E4%BB%8B%E7%BB%8D">一、ChatGLM3介绍</h2> 
<p>        ChatGLM3 是智谱AI和清华大学 KEG 实验室联合发布的新一代对话预训练模型。</p> 
<table border="1" cellpadding="1" cellspacing="1"><caption>
   开源模型序列 
 </caption><thead><tr><th style="width:158px;"> <p style="text-align:center;"><strong>模型</strong></p> </th><th style="width:417px;"> <p style="text-align:center;"><strong>介绍</strong></p> </th><th style="width:94px;"> <p style="text-align:center;"><strong>代码链接</strong></p> </th><th style="width:110px;"> <p style="text-align:center;">模型下载</p> </th></tr></thead><tbody><tr><td style="width:158px;"> <p><strong>ChatGLM3-6B</strong></p> </td><td style="width:417px;">第三代 ChatGLM 对话模型。ChatGLM3-6B 采用了全新设计的 Prompt 格式，除正常的多轮对话外。同时原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景。</td><td colspan="1" rowspan="4" style="width:94px;"><a class="link-info" href="https://github.com/THUDM/ChatGLM3" title="ChatGLM3">ChatGLM3</a></td><td style="width:110px;"> <p><a class="link-info" href="https://huggingface.co/THUDM/chatglm3-6b" rel="nofollow" title="Huggingface">Huggingface</a></p> <p></p> <p><a class="link-info" href="https://modelscope.cn/models/ZhipuAI/chatglm3-6b" rel="nofollow" title="魔搭社区 ">魔搭社区 </a></p> </td></tr><tr><td colspan="1" rowspan="1" style="width:158px;"> <p><strong>ChatGLM3-6B-base</strong></p> </td><td style="width:417px;">第三代ChatGLM基座模型。ChatGLM3-6B-Base 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略。在语义、数学、推理、代码、知识等不同角度的数据集上测评显示，ChatGLM3-6B-Base 具有在 10B 以下的基础模型中最强的性能。</td><td style="width:110px;"> <p><a class="link-info" href="https://huggingface.co/THUDM/chatglm3-6b-base" rel="nofollow" title="Huggingface">Huggingface</a></p>   <p><a class="link-info" href="https://modelscope.cn/models/ZhipuAI/chatglm3-6b-base" rel="nofollow" title="魔搭社区">魔搭社区</a></p> </td></tr><tr><td style="width:158px;"> <p><strong>ChatGLM3-6B-32k</strong></p> </td><td style="width:417px;">第三代ChatGLM长上下文对话模型。在ChatGLM3-6B的基础上进一步强化了对于长文本的理解能力，能够更好的处理最多32K长度的上下文。</td><td style="width:110px;"> <p><a class="link-info" href="https://huggingface.co/THUDM/chatglm3-6b-32k" rel="nofollow" title="Huggingface">Huggingface</a></p> <p></p> <p><a class="link-info" href="https://modelscope.cn/models/ZhipuAI/chatglm3-6b-32k" rel="nofollow" title="魔搭社区">魔搭社区</a></p> </td></tr><tr><td style="width:158px;"> <p><strong>ChatGLM3-6B-128k</strong></p> </td><td style="width:417px;">ChatGLM3-6B-128K在ChatGLM3-6B的基础上进一步强化了对于长文本的理解能力，能够更好的处理最多128K长度的上下文。具体地，我们对位置编码进行了更新，并设计了更有针对性的长文本训练方法，在对话阶段使用 128K 的上下文长度训练。在实际的使用中，如果您面临的上下文长度基本在 8K 以内，我们推荐使用ChatGLM3-6B；如果您需要处理超过 8K 的上下文长度，我们推荐使用ChatGLM3-6B-128K。</td><td style="width:110px;"> <p><a class="link-info" href="https://huggingface.co/THUDM/chatglm3-6b-128k" rel="nofollow" title="Huggingface">Huggingface</a></p> <p></p> <p><a class="link-info" href="https://modelscope.cn/models/ZhipuAI/chatglm3-6b-128k" rel="nofollow" title="魔搭社区">魔搭社区</a></p> </td></tr></tbody></table> 
<p>        智谱AI所有的开源模型对学术研究完全开放，部分模型（ChatGLM系列）在填写<a class="link-info" href="https://open.bigmodel.cn/mla/state/person" rel="nofollow" title="问卷">问卷</a>进行登记后亦允许免费商业使用。</p> 
<h2 id="%E4%BA%8C%E3%80%81%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%92%8C%E6%A3%80%E6%9F%A5">二、环境配置和检查</h2> 
<h3 id="2.1%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F">2.1 操作系统</h3> 
<p>        ChatGLM3-6B理论上可以在任何主流的操作系统中运行。ChatGLM开发组已经为主流操作系统做了一定的适配。<br> 但是，我们更推荐开发者在 Linux环境下运行我们的代码，以下说明也主要针对Linux系统。</p> 
<h3 id="2.2%20%E7%A1%AC%E4%BB%B6%E7%8E%AF%E5%A2%83">2.2 硬件环境</h3> 
<p>        最低要求：<br>         为了能够流畅运行 Int4 版本的 ChatGLM3-6B，我们在这里给出了最低的配置要求：<br>         内存：&gt;= 8GB<br>         显存:  &gt;= 5GB（1060 6GB,2060 6GB）</p> 
<p>        为了能够流畅运行 FP16 版本的，ChatGLM3-6B，我们在这里给出了最低的配置要求：<br>         内存：&gt;= 16GB<br>         显存:  &gt;= 13GB（4080 16GB）</p> 
<p>        如果使用CPU加载，可以忽略显存的要求，但是速度非常慢。</p> 
<h3 id="2.3%20%E8%BD%AF%E4%BB%B6%E7%8E%AF%E5%A2%83">2.3 软件环境</h3> 
<p>        Python环境<br>         请开发者按照仓库中的requirements.txt来安装对应的依赖，并需要注意:</p> 
<ul><li>python 版本推荐3.10 - 3.11</li><li>transformers 库版本推荐为 4.36.2</li><li>torch 推荐使用 2.0 及以上的版本，以获得最佳的推理性能</li></ul> 
<h2 id="%E4%B8%89%E3%80%81%E6%9C%AC%E5%9C%B0%E6%BA%90%E7%A0%81%E9%83%A8%E7%BD%B2">三、本地源码部署</h2> 
<h3 id="3.1%20%E5%85%8B%E9%9A%86%E6%BA%90%E7%A0%81">3.1 克隆源码</h3> 
<blockquote> 
 <p>git clone https://github.com/THUDM/ChatGLM3.git</p> 
</blockquote> 
<p><img alt="" height="321" src="https://images2.imgbox.com/b9/7a/dmnyrXVf_o.png" width="932"></p> 
<h3 id="3.2%20%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6">3.2 下载模型文件</h3> 
<p>        可以Hugging Face Hub 下载模型，如果从 HuggingFace 下载比较慢，也可以从 ModelScope 中下载：</p> 
<blockquote> 
 <p>#从ModelScope下载模型</p> 
 <p>git clone https://www.modelscope.cn/ZhipuAI/chatglm3-6b.git</p> 
</blockquote> 
<h3 id="3.3%20%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96">3.3 安装依赖</h3> 
<p>        通过<a class="link-info" href="https://blog.csdn.net/m0_37559973/article/details/138085180" title="Conda">Conda</a>安装一个新的python环境，在虚拟环境中安装依赖：</p> 
<blockquote> 
 <p>#1.0 新建python3.11环境</p> 
 <p>conda create --name zhipuai python=3.11</p> 
 <p>#1.1 激活zhipuai环境</p> 
 <p>conda activate zhipuai</p> 
 <p>#2. 进入 ChatGLM3 源码目录</p> 
 <p>cd ChatGLM3 </p> 
 <p>#3. <em>默认安装不指定镜像[pip install -r requirements.txt]安装过程可能会出现依赖或者其它奇怪的错误，建议指定镜像源下载</em><br> pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/</p> 
</blockquote> 
<p><img alt="" height="264" src="https://images2.imgbox.com/5c/45/sUvPvgy1_o.png" width="1004"></p> 
<p>        在安装依赖过程中有可能出现某些依赖安装不了，可以去国内主流的资源镜像平台(如<a class="link-info" href="https://pypi.tuna.tsinghua.edu.cn/simple/" rel="nofollow" title="清华大学镜像">清华大学镜像</a>)下载对应的whl文件，然后通过pip install 安装离线文件，安装好之后，重新执行pip install -r requirements.txt 安装依赖。</p> 
<h3 id="3.4%20%E4%BB%A3%E7%A0%81%E8%B0%83%E7%94%A8">3.4 代码调用</h3> 
<p>         <strong>GPU 部署</strong></p> 
<blockquote> 
 <p>model = AutoModel.from_pretrained("model/chatglm3-6b", trust_remote_code=True<span style="color:#fe2c24;"><strong>, device='cuda'</strong></span>)</p> 
</blockquote> 
<p>        <strong>模型量化</strong><br>         默认情况下，模型以 FP16 精度加载，运行上述代码需要大概 13GB 显存。如果你的 GPU 显存有限，可以尝试以量化方式加载模型，使用方法如下：</p> 
<blockquote> 
 <p>model = AutoModel.from_pretrained("model/chatglm3-6b", trust_remote_code=True).<span style="color:#fe2c24;">quantize(4)</span>.<span style="color:#fe2c24;">cuda()</span></p> 
</blockquote> 
<p>        模型量化会带来一定的性能损失，经过测试，ChatGLM3-6B 在 4-bit 量化下仍然能够进行自然流畅的生成。</p> 
<p>        <strong>CPU 部署</strong><br>         如果你没有 GPU 硬件的话，也可以在 CPU 上进行推理，但是推理速度会更慢。使用方法如下（需要大概 32GB 内存）</p> 
<blockquote> 
 <p>model = AutoModel.from_pretrained("model/chatglm3-6b", trust_remote_code=True)<span style="color:#fe2c24;">.float()</span></p> 
</blockquote> 
<p><span style="color:#0d0016;">        <strong>编写代码：</strong></span></p> 
<pre><code class="language-python">from transformers import AutoTokenizer,AutoModel

#使用本地模型
tokenizer = AutoTokenizer.from_pretrained('/data/weisx/zhipuai/model/chatglm3-6b',trust_remote_code=True)

#使用CPU
model = AutoModel.from_pretrained('/data/weisx/zhipuai/model/chatglm3-6b',trust_remote_code=True).float()

model = model.eval()

response,history = model.chat(tokenizer,"你好",history=[])

print(response)</code></pre> 
<p><img alt="" height="256" src="https://images2.imgbox.com/db/56/XJdVQRFD_o.png" width="1015"></p> 
<p></p> 
<h2 id="%E5%9B%9B%E3%80%81%E8%BF%90%E8%A1%8CDemo">四、运行Demo</h2> 
<h3 id="4.1%20%E8%AE%BE%E7%BD%AE%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%9E%8B%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F">4.1 设置本地模型环境变量</h3> 
<blockquote> 
 <p>export MODEL_PATH=/data/weisx/zhipuai/model/chatglm3-6b</p> 
</blockquote> 
<h3 id="4.2%20Gradio%20%E7%BD%91%E9%A1%B5%E7%89%88%20Demo">4.2 Gradio 网页版 Demo</h3> 
<p>       Gradio 是一个用于构建机器学习和数据科学项目的 Python 库，它允许用户通过简单的代码创建交互式网页，用户可以通过网页进行输入，并获取相应的输出。由于 Gradio 默认绑定的是本地地址（例如 127.0.0.1 或 localhost），而不是你的局域网或公网 IP 地址，所以 Gradio 创建的网页只能在本机访问，而不能在其他设备上访问。需要修改Gradio 启动参数，设置 share=True ，这样可以让你的应用通过本地网络访问，但不是所有环境都允许。</p> 
<blockquote> 
 <p>demo.launch(server_name="192.168.110.152", server_port=7870, inbrowser=True, share=True)</p> 
</blockquote> 
<p>        进入base_demo目录，启动Gradio :</p> 
<blockquote> 
 <p>python web_demo_gradio.py</p> 
</blockquote> 
<p><img alt="" height="309" src="https://images2.imgbox.com/d6/2e/QqjwnoB2_o.png" width="1017"></p> 
<p>        访问WEB:</p> 
<blockquote> 
 <p><a href="http://192.168.110.152:7870/" rel="nofollow" title="http://192.168.110.152:7870/">http://192.168.110.152:7870/</a></p> 
</blockquote> 
<p><img alt="" height="996" src="https://images2.imgbox.com/fa/d9/aK4zy7mB_o.png" width="1200"></p> 
<h3 id="4.3%C2%A0%20Streamlit%C2%A0%20%E7%BD%91%E9%A1%B5%E7%89%88%20Demo">4.3 Streamlit  网页版 Demo</h3> 
<p>        Streamlit是一个基于Python的开源库，专为机器学习工程师和数据科学家设计，用于快速构建和共享机器学习和数据科学领域的Web应用程序。</p> 
<p>          进入base_demo目录，启动 Streamlit:</p> 
<blockquote> 
 <p>streamlit run web_demo_streamlit.py</p> 
</blockquote> 
<p><img alt="" height="432" src="https://images2.imgbox.com/8b/08/SlyJsEps_o.png" width="875"></p> 
<p>        访问WEB：</p> 
<blockquote> 
 <p><a href="http://192.168.110.152:8501/" rel="nofollow" title="http://192.168.110.152:8501/">http://192.168.110.152:8501/</a></p> 
</blockquote> 
<p><img alt="" height="961" src="https://images2.imgbox.com/95/15/pzOr8HCR_o.png" width="1200"></p> 
<h3 id="4.4%20%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BA%A4%E4%BA%92Demo">4.4 命令行交互Demo</h3> 
<p>        进入base_demo目录，启动 命令行客户端:</p> 
<blockquote> 
 <p>python cli_demo.py</p> 
</blockquote> 
<p><img alt="" height="385" src="https://images2.imgbox.com/af/ec/3jzGGkY9_o.png" width="1016"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3d6187f8a84190edf260b0613cbbcf72/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">2024华为OD机试真题最新题库 (B&#43;C&#43;D卷) &#43;OJ在线刷题（C&#43;&#43;、Java、Python合集）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/945c598a7f32f1903731359601f57ee9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python 环境管理工具：Conda</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>