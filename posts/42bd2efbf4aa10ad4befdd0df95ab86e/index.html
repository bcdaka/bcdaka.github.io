<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AI模型：windows本地运行下载安装ollama运行llama3、llama2、Google CodeGemma、gemma等可离线运行数据模型【自留记录】 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/42bd2efbf4aa10ad4befdd0df95ab86e/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="AI模型：windows本地运行下载安装ollama运行llama3、llama2、Google CodeGemma、gemma等可离线运行数据模型【自留记录】">
  <meta property="og:description" content="AI模型：windows本地运行下载安装ollama运行llama3、llama2、Google CodeGemma、gemma等可离线运行数据模型【自留记录】 CodeGemma 没法直接运行，需要中间软件。下载安装ollama后，使用ollama运行CodeGemma等AI模型。
类似 前端本地需要安装 node.js 才可能跑vue、react项目
1、下载 ollama： 官网下载：https://ollama.com/download，很慢，原因不解释。
阿里云盘下载：https://www.alipan.com/s/jiwVVjc7eYb 提取码: ft90
百度云盘下载：https://pan.baidu.com/s/1o1OcY0FkycxMpZ7Ho8_5oA?pwd=8cft 提取码：8cft
2、安装 运行 OllamaSetup.exe ，安装过程不能选择自定义文件夹
3、测试安装是否成功 win &#43; R 输入 cmd ，回车输入：ollama
4、修改模型文件地址 (非必须) ollama模型默认安装地址在C:\Users&lt;用户名&gt;.ollama
因为模型较大，所以我们需要在环境变量内设置模型的安装位置，如下进行设置
变量名： OLLAMA_MODELS
变量值： E:\ollama（根据自己打算存放的地址自行填写）
5、官网下载安装模型 本文依 codegemma为例，如果使用其他模型一样的操作。
选择对应模型
2b： 最低配，有点SB。不智能，不推荐
命令：ollama run codegemma:2b
7b： 内存8G以上，建议16G电脑上这个版本更好一点，碾压2b版本。预计占用1.5G内存，CPU要求高，低压U估计压不住，时间太长
命令：ollama run codegemma:7b
7b全量： 说是更智能，没体验。建议16G或者32G电脑上这个版本，cpu要求更高
命令：ollama run codegemma:7b-code-fp16
带instruct： 能够理解自然语言输入，并根据指令生成相应的代码。
带code： 预训练的模型，专门用于代码补全和根据代码前缀和/或后缀生成代码。
带2b： 最新的预训练模型，提供了最多两倍更快的代码自动补全功能。它的目标是提高代码补全的速度和效率。就是回复的有点拉胯。
点击复制按钮
6、命令行粘贴回车运行 Ctrl &#43; V 即可
输入问答问题即可测试运行
7、API接口调用： 由于实际使用命令行问答很不方便，改造成api调用。都是 POST 接口">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-14T19:18:10+08:00">
    <meta property="article:modified_time" content="2024-05-14T19:18:10+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AI模型：windows本地运行下载安装ollama运行llama3、llama2、Google CodeGemma、gemma等可离线运行数据模型【自留记录】</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="AIwindowsollamallama3llama2Google_CodeGemmagemma_0"></a>AI模型：windows本地运行下载安装ollama运行llama3、llama2、Google CodeGemma、gemma等可离线运行数据模型【自留记录】</h3> 
<p>CodeGemma 没法直接运行，需要中间软件。下载安装ollama后，使用ollama运行CodeGemma等AI模型。<br> 类似 前端本地需要安装 node.js 才可能跑vue、react项目</p> 
<h3><a id="1_ollama_5"></a>1、下载 ollama：</h3> 
<p><img src="https://images2.imgbox.com/e3/09/DQF2cPyx_o.png" alt="在这里插入图片描述"></p> 
<p>官网下载：<a href="https://ollama.com/download" rel="nofollow">https://ollama.com/download</a>，很慢，原因不解释。</p> 
<p>阿里云盘下载：<a href="https://www.alipan.com/s/jiwVVjc7eYb" rel="nofollow">https://www.alipan.com/s/jiwVVjc7eYb</a> 提取码: ft90</p> 
<p>百度云盘下载：<a href="https://pan.baidu.com/s/1o1OcY0FkycxMpZ7Ho8_5oA?pwd=8cft" rel="nofollow">https://pan.baidu.com/s/1o1OcY0FkycxMpZ7Ho8_5oA?pwd=8cft </a> 提取码：8cft</p> 
<h3><a id="2_16"></a><strong>2、安装</strong></h3> 
<p>运行 OllamaSetup.exe ，安装过程不能选择自定义文件夹</p> 
<h3><a id="3_20"></a><strong>3、测试安装是否成功</strong></h3> 
<p><strong>win + R 输入 cmd ，回车输入：ollama</strong></p> 
<p><img src="https://images2.imgbox.com/05/a8/Kh7NLGlb_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="4_font_colorred_font_26"></a><strong>4、修改模型文件地址</strong> <font color="red">(非必须)</font></h3> 
<p>ollama模型默认安装地址在C:\Users&lt;用户名&gt;.ollama</p> 
<p>因为模型较大，所以我们需要在环境变量内设置模型的安装位置，如下进行设置</p> 
<p><img src="https://images2.imgbox.com/61/b2/U6ufkJQS_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e3/2b/5FrUj8WM_o.png" alt="在这里插入图片描述"><br> <strong>变量名：</strong> OLLAMA_MODELS<br> <strong>变量值：</strong> E:\ollama（根据自己打算存放的地址自行填写）</p> 
<h3><a id="5_37"></a><strong>5、官网下载安装模型</strong></h3> 
<p><strong>本文依 codegemma为例，如果使用其他模型一样的操作。</strong></p> 
<p><img src="https://images2.imgbox.com/d6/03/QMHCDXjn_o.png" alt="在这里插入图片描述"><br> 选择对应模型</p> 
<p><strong>2b：</strong> 最低配，有点SB。<font color="red">不智能，不推荐</font><br> 命令：ollama run codegemma:2b</p> 
<p><strong>7b：</strong> 内存8G以上，建议16G电脑上这个版本更好一点，碾压2b版本。预计占用1.5G内存，CPU要求高，低压U估计压不住，时间太长<br> 命令：ollama run codegemma:7b</p> 
<p><strong>7b全量：</strong> 说是更智能，没体验。建议16G或者32G电脑上这个版本，cpu要求更高<br> 命令：ollama run codegemma:7b-code-fp16</p> 
<p><strong>带instruct：</strong> 能够理解自然语言输入，并根据指令生成相应的代码。</p> 
<p><strong>带code：</strong> 预训练的模型，专门用于代码补全和根据代码前缀和/或后缀生成代码。</p> 
<p><strong>带2b：</strong> 最新的预训练模型，提供了最多两倍更快的代码自动补全功能。它的目标是提高代码补全的速度和效率。就是回复的有点拉胯。</p> 
<p><img src="https://images2.imgbox.com/2a/c0/ph4Fd76Y_o.png" alt="在这里插入图片描述"><br> 点击复制按钮</p> 
<h3><a id="6_63"></a><strong>6、命令行粘贴回车运行</strong></h3> 
<p>Ctrl + V 即可</p> 
<p><img src="https://images2.imgbox.com/ae/d5/mGXjgVmP_o.png" alt="在这里插入图片描述"></p> 
<p>输入问答问题即可测试运行</p> 
<h3><a id="7API_71"></a><strong>7、API接口调用：</strong></h3> 
<p>由于实际使用命令行问答很不方便，改造成api调用。都是 <strong>POST</strong> 接口</p> 
<p>/ai/generate：结果一起返回，等待时间较长<br> /ai/chat：对话模式，有一点结果就立马输出</p> 
<p>详细api文档说明：<a href="https://github.com/ollama/ollama/blob/main/docs/api.md?plain=1">https://github.com/ollama/ollama/blob/main/docs/api.md?plain=1</a></p> 
<p>支持json数据返回、图片问答、row数据等</p> 
<p><strong>模板案例：</strong></p> 
<pre><code class="prism language-javascript">	axios<span class="token punctuation">.</span><span class="token function">post</span><span class="token punctuation">(</span><span class="token template-string"><span class="token template-punctuation string">`</span><span class="token string">http://localhost:11434/api/generate</span><span class="token template-punctuation string">`</span></span><span class="token punctuation">,</span> <span class="token punctuation">{<!-- --></span>
        <span class="token literal-property property">model</span><span class="token operator">:</span> <span class="token string">"codegemma:7b"</span><span class="token punctuation">,</span>
        <span class="token literal-property property">prompt</span><span class="token operator">:</span> <span class="token string">"正则匹配大陆手机号码是否正确"</span><span class="token punctuation">,</span>
        <span class="token literal-property property">format</span><span class="token operator">:</span> <span class="token string">"json"</span><span class="token punctuation">,</span>
        <span class="token literal-property property">stream</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>
        <span class="token comment">// options: {<!-- --></span>
        <span class="token comment">//   num_keep: 15,</span>
        <span class="token comment">//   seed: 42,</span>
        <span class="token comment">//   num_predict: 100,</span>
        <span class="token comment">//   top_k: 20,</span>
        <span class="token comment">//   top_p: 0.9,</span>
        <span class="token comment">//   tfs_z: 0.5,</span>
        <span class="token comment">//   typical_p: 0.7,</span>
        <span class="token comment">//   repeat_last_n: 33,</span>
        <span class="token comment">//   temperature: 0.8,</span>
        <span class="token comment">//   repeat_penalty: 1.2,</span>
        <span class="token comment">//   presence_penalty: 1.5,</span>
        <span class="token comment">//   frequency_penalty: 1.0,</span>
        <span class="token comment">//   mirostat: 1,</span>
        <span class="token comment">//   mirostat_tau: 0.8,</span>
        <span class="token comment">//   mirostat_eta: 0.6,</span>
        <span class="token comment">//   penalize_newline: true,</span>
        <span class="token comment">//   // stop: ["\n", "user:"],</span>
        <span class="token comment">//   numa: false,</span>
        <span class="token comment">//   num_ctx: 1024,</span>
        <span class="token comment">//   num_batch: 2,</span>
        <span class="token comment">//   num_gqa: 1,</span>
        <span class="token comment">//   num_gpu: 1,</span>
        <span class="token comment">//   main_gpu: 0,</span>
        <span class="token comment">//   low_vram: false,</span>
        <span class="token comment">//   f16_kv: true,</span>
        <span class="token comment">//   vocab_only: false,</span>
        <span class="token comment">//   use_mmap: true,</span>
        <span class="token comment">//   use_mlock: false,</span>
        <span class="token comment">//   rope_frequency_base: 1.1,</span>
        <span class="token comment">//   rope_frequency_scale: 0.8,</span>
        <span class="token comment">//   num_thread: 8,</span>
        <span class="token comment">// },</span>
      <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<table><thead><tr><th>参数名称</th><th>是否必填</th><th>说明</th></tr></thead><tbody><tr><td>model</td><td>是</td><td>访问的模型名称</td></tr><tr><td>prompt</td><td>是</td><td>问题内容</td></tr><tr><td>stream</td><td>否</td><td>默认值：true，返回数据流。设置false，则返回对象数据</td></tr><tr><td>format</td><td>否</td><td>返回响应的格式。当前唯一接受的值是<code>json</code></td></tr><tr><td>keep_alive</td><td>否</td><td>控制模型在请求后加载到内存中的时间（默认值：“5m”）</td></tr><tr><td>options</td><td>否</td><td>额外的模型参数</td></tr><tr><td>images</td><td>否</td><td>图片数组</td></tr><tr><td>role</td><td>否</td><td>角色身份。支持参数：<code>system</code>, <code>user</code> or <code>assistant</code></td></tr><tr><td>其他…</td><td></td><td>自己看</td></tr></tbody></table> 
<p><strong>options 参数说明</strong></p> 
<p>参考文档：<a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values">https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values</a></p> 
<h3><a id="8_141"></a>8、调用测试</h3> 
<p><strong>入参：</strong><br> <img src="https://images2.imgbox.com/74/d8/LOau2O8o_o.png" alt="在这里插入图片描述"><br> <strong>输出：</strong><br> <img src="https://images2.imgbox.com/6f/b9/kayy9QyJ_o.png" alt="在这里插入图片描述"><br> <strong>设备说明：</strong></p> 
<p>测试模型：codegemma:7b<br> CPU：i7-13700H（问答时占用很高）<br> 内存：32G（实际占用1.5G样子，没啥压力）<br> 时间：7B回复简单问题，问答模式响应时间5-10秒样子。对象返回1.5-2分钟（设置options中：mirostat_eta: 0.1 则用时短一点，但是回答内容也会减少，设置GPU加速，能在30秒内）。受限没有使用GPU速度较慢</p> 
<hr> 
<p>备注：</p> 
<p>1、如果运行失败。电脑重启在 命令行 重新粘贴命令</p> 
<p>安装WebUi等可以查看：<br> 参考文档地址：<a href="https://blog.csdn.net/qq_39583774/article/details/136592951">https://blog.csdn.net/qq_39583774/article/details/136592951</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8b1dfc2c015421b63619ed4aa8d1b9dc/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Kimi智能助手：你的全天候AI伙伴</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/004088f2e752e6a9eb577814aa2312c4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Web】H&amp;NCTF 2024 题解(部分)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>