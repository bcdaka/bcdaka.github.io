<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【深度学习】最强算法模型之：潜在狄利克雷分配(LDA) - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/8dce0d004e72c6ca4f59d2d3869fb4eb/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【深度学习】最强算法模型之：潜在狄利克雷分配(LDA)">
  <meta property="og:description" content="潜在狄利克雷分配 1、引言2、潜在狄利克雷分配2.1 定义2.2 原理2.3 算法公式2.4 代码示例 3、总结 1、引言 小屌丝：鱼哥， 给我讲一讲LDA
小鱼：LDA？ 你指的是？
小屌丝：就是算法模型的LDA啊， 你想啥？
小鱼：哦，哦， 那就好，
小屌丝：你告诉我，你想啥了？
小鱼：不滴， 我就不
小屌丝：…你就说吧，我又不是外人
小鱼：…把耳朵凑过来，这只能悄悄说
小屌丝：鱼哥，你这… 咱不开车行不
小鱼：… 最近健身，骑自行车呢
小屌丝： … 我差点信了
小鱼：… 不扯了，咱还是开始 LDA吧。
小屌丝：那可不。
2、潜在狄利克雷分配 2.1 定义 潜在狄利克雷分配（LDA）是一种生成概率模型，用于集合（如文档集合或语料库）的离散数据（如文档中的单词）的集合中发现潜在的结构。
在LDA中，每个文档被视为由多个主题的混合生成，而每个主题又是由词汇表中单词的特定概率分布所定义。
2.2 原理 LDA的核心原理在于假设文档是由潜在的主题混合而成的，而每个主题则由一组单词的概率分布来定义。
LDA通过最大化文档的似然性来估计这些主题和它们的单词分布。
在文档生成过程中，首先根据主题分布选择一个主题，然后根据该主题的单词分布生成一个单词。
这个过程在文档中重复进行，直到生成完整的文档。
2.3 算法公式 LDA的数学表达涉及概率图模型中的节点和边，以及相应的条件概率分布。
具体来说，LDA定义了一个文档集合中每篇文档的主题分布 （ θ ） （θ） （θ），每个主题的单词分布 （ φ ） （φ） （φ），以及文档的生成过程。
LDA的主要公式包括：
主题分布θ的先验分布： D i r i c h l e t ( α ) Dirichlet(α) Dirichlet(α)单词分布φ的先验分布： D i r i c h l e t ( β ) Dirichlet(β) Dirichlet(β)文档中第n个词的主题分布： z n M u l t i n o m i a l ( θ ) z_n ~ Multinomial(θ) zn​ Multinomial(θ)给定主题z_n，文档中第n个词的单词分布：KaTeX parse error: Double subscript at position 22: …Multinomial(φ_z_̲n) 其中， α α α和 β β β是超参数，分别控制主题分布和单词分布的稀疏性。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-03-27T12:14:36+08:00">
    <meta property="article:modified_time" content="2024-03-27T12:14:36+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【深度学习】最强算法模型之：潜在狄利克雷分配(LDA)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>潜在狄利克雷分配</h4> 
 <ul><li><a href="#1_1" rel="nofollow">1、引言</a></li><li><a href="#2_22" rel="nofollow">2、潜在狄利克雷分配</a></li><li><ul><li><a href="#21__23" rel="nofollow">2.1 定义</a></li><li><a href="#22__27" rel="nofollow">2.2 原理</a></li><li><a href="#23__36" rel="nofollow">2.3 算法公式</a></li><li><a href="#24__52" rel="nofollow">2.4 代码示例</a></li></ul> 
  </li><li><a href="#3_113" rel="nofollow">3、总结</a></li></ul> 
</div> 
<p></p> 
<h2><a id="1_1"></a>1、引言</h2> 
<p><strong>小屌丝</strong>：鱼哥， 给我讲一讲LDA<br> <strong>小鱼</strong>：LDA？ 你指的是？</p> 
<p><strong>小屌丝</strong>：就是算法模型的LDA啊， 你想啥？<br> <strong>小鱼</strong>：哦，哦， 那就好，<br> <strong>小屌丝</strong>：你告诉我，你想啥了？<br> <strong>小鱼</strong>：不滴， 我就不<br> <strong>小屌丝</strong>：…你就说吧，我又不是外人<br> <strong>小鱼</strong>：…把耳朵凑过来，这只能悄悄说<br> <img src="https://images2.imgbox.com/ee/dc/XdN9dAvh_o.gif" alt="在这里插入图片描述"></p> 
<p><strong>小屌丝</strong>：鱼哥，你这… 咱不开车行不<br> <strong>小鱼</strong>：… 最近健身，骑自行车呢<br> <strong>小屌丝</strong>： … 我差点信了<br> <img src="https://images2.imgbox.com/bb/7c/TceAWYSS_o.gif" alt="在这里插入图片描述"></p> 
<p><strong>小鱼</strong>：… 不扯了，咱还是开始 LDA吧。<br> <strong>小屌丝</strong>：那可不。</p> 
<h2><a id="2_22"></a>2、潜在狄利克雷分配</h2> 
<h3><a id="21__23"></a>2.1 定义</h3> 
<p>潜在狄利克雷分配（LDA）是一种生成概率模型，用于集合（如文档集合或语料库）的离散数据（如文档中的单词）的集合中发现潜在的结构。</p> 
<p>在LDA中，每个文档被视为由多个主题的混合生成，而每个主题又是由词汇表中单词的特定概率分布所定义。</p> 
<h3><a id="22__27"></a>2.2 原理</h3> 
<p>LDA的核心原理在于假设文档是由潜在的主题混合而成的，而每个主题则由一组单词的概率分布来定义。</p> 
<p>LDA通过最大化文档的似然性来估计这些主题和它们的单词分布。</p> 
<p>在文档生成过程中，首先根据主题分布选择一个主题，然后根据该主题的单词分布生成一个单词。</p> 
<p>这个过程在文档中重复进行，直到生成完整的文档。</p> 
<h3><a id="23__36"></a>2.3 算法公式</h3> 
<p>LDA的数学表达涉及概率图模型中的节点和边，以及相应的条件概率分布。</p> 
<p>具体来说，LDA定义了一个文档集合中每篇文档的主题分布<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         （ 
        
       
         θ 
        
       
         ） 
        
       
      
        （θ） 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord cjk_fallback">（</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mord cjk_fallback">）</span></span></span></span></span>，每个主题的单词分布<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         （ 
        
       
         φ 
        
       
         ） 
        
       
      
        （φ） 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8778em; vertical-align: -0.1944em;"></span><span class="mord cjk_fallback">（</span><span class="mord mathnormal">φ</span><span class="mord cjk_fallback">）</span></span></span></span></span>，以及文档的生成过程。</p> 
<p>LDA的主要公式包括：</p> 
<ul><li><strong>主题分布θ的先验分布</strong>：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          D 
         
        
          i 
         
        
          r 
         
        
          i 
         
        
          c 
         
        
          h 
         
        
          l 
         
        
          e 
         
        
          t 
         
        
          ( 
         
        
          α 
         
        
          ) 
         
        
       
         Dirichlet(α) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">D</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right: 0.0197em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="mclose">)</span></span></span></span></span></li><li><strong>单词分布φ的先验分布</strong>：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          D 
         
        
          i 
         
        
          r 
         
        
          i 
         
        
          c 
         
        
          h 
         
        
          l 
         
        
          e 
         
        
          t 
         
        
          ( 
         
        
          β 
         
        
          ) 
         
        
       
         Dirichlet(β) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">D</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right: 0.0197em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0528em;">β</span><span class="mclose">)</span></span></span></span></span></li><li><strong>文档中第n个词的主题分布</strong>：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           z 
          
         
           n 
          
         
        
            
         
        
          M 
         
        
          u 
         
        
          l 
         
        
          t 
         
        
          i 
         
        
          n 
         
        
          o 
         
        
          m 
         
        
          i 
         
        
          a 
         
        
          l 
         
        
          ( 
         
        
          θ 
         
        
          ) 
         
        
       
         z_n ~ Multinomial(θ) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.044em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.044em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace nobreak"> </span><span class="mord mathnormal" style="margin-right: 0.109em;">M</span><span class="mord mathnormal">u</span><span class="mord mathnormal">lt</span><span class="mord mathnormal">in</span><span class="mord mathnormal">o</span><span class="mord mathnormal">mia</span><span class="mord mathnormal" style="margin-right: 0.0197em;">l</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span></span></span></span></span></li><li><strong>给定主题z_n，文档中第n个词的单词分布</strong>：<span class="katex--inline">KaTeX parse error: Double subscript at position 22: …Multinomial(φ_z_̲n)</span></li></ul> 
<p>其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
      
        α 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         β 
        
       
      
        β 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0528em;">β</span></span></span></span></span>是<strong>超参数</strong>，分别控制<strong>主题分布</strong>和<strong>单词分布</strong>的<strong>稀疏性</strong>。</p> 
<p><img src="https://images2.imgbox.com/17/02/9xbDOX2u_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="24__52"></a>2.4 代码示例</h3> 
<pre><code class="prism language-python"><span class="token comment"># -*- coding:utf-8 -*-</span>
<span class="token comment"># @Time   : 2024-01-21</span>
<span class="token comment"># @Author : Carl_DJ</span>

<span class="token triple-quoted-string string">'''
实现功能：
    使用Python的gensim库实现LDA主题模型的
'''</span>

<span class="token keyword">import</span> gensim  
<span class="token keyword">from</span> gensim <span class="token keyword">import</span> corpora  
<span class="token keyword">from</span> pprint <span class="token keyword">import</span> pprint  
  
<span class="token comment"># 假设我们有一些文档数据  </span>
documents <span class="token operator">=</span> <span class="token punctuation">[</span>  
    <span class="token string">"这是第一个文档。"</span><span class="token punctuation">,</span>  
    <span class="token string">"这是第二个文档，与第一个文档相似。"</span><span class="token punctuation">,</span>  
    <span class="token string">"第三个文档与前两个文档不同，讨论的是另一个主题。"</span><span class="token punctuation">,</span>  
<span class="token punctuation">]</span>  
  
<span class="token comment"># 创建文本语料库  </span>
texts <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>text <span class="token keyword">for</span> text <span class="token keyword">in</span> doc<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">for</span> doc <span class="token keyword">in</span> documents<span class="token punctuation">]</span>  
dictionary <span class="token operator">=</span> corpora<span class="token punctuation">.</span>Dictionary<span class="token punctuation">(</span>texts<span class="token punctuation">)</span>  
corpus <span class="token operator">=</span> <span class="token punctuation">[</span>dictionary<span class="token punctuation">.</span>doc2bow<span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token keyword">for</span> text <span class="token keyword">in</span> texts<span class="token punctuation">]</span>  
  
<span class="token comment"># 使用LDA模型  </span>
lda_model <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>LdaModel<span class="token punctuation">(</span>corpus<span class="token operator">=</span>corpus<span class="token punctuation">,</span> id2word<span class="token operator">=</span>dictionary<span class="token punctuation">,</span> num_topics<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> update_every<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> chunksize<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> passes<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">,</span> per_word_topics<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 打印主题  </span>
pprint<span class="token punctuation">(</span>lda_model<span class="token punctuation">.</span>print_topics<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  
  
 
<span class="token comment"># 获取文档的主题分布  </span>
doc_topics <span class="token operator">=</span> lda_model<span class="token punctuation">[</span>corpus<span class="token punctuation">]</span>  
<span class="token keyword">for</span> i<span class="token punctuation">,</span> doc_topic <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>doc_topics<span class="token punctuation">)</span><span class="token punctuation">:</span>  
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"文档 </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>i<span class="token punctuation">}</span></span><span class="token string"> 的主题分布: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>doc_topic<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>  
  


</code></pre> 
<ul><li><strong>打印主题 运行结果</strong></li></ul> 
<pre><code class="prism language-powershell"><span class="token comment">#输出结果示例  </span>
<span class="token punctuation">[</span><span class="token punctuation">(</span>0<span class="token punctuation">,</span> <span class="token string">'0.237*"文档" + 0.196*"第一个" + 0.179*"这是" + 0.145*"相似" + 0.100*"第二个"'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  
<span class="token punctuation">(</span>1<span class="token punctuation">,</span> <span class="token string">'0.263*"另一个" + 0.251*"主题" + 0.226*"讨论" + 0.140*"是" + 0.120*"不同"'</span><span class="token punctuation">)</span><span class="token punctuation">]</span>  
</code></pre> 
<ul><li><strong>获取文档的主题分布运行结果</strong></li></ul> 
<pre><code class="prism language-shell"><span class="token comment"># 输出结果示例  </span>
 文档 <span class="token number">0</span> 的主题分布: <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span>, <span class="token number">0.9999911059222225</span><span class="token punctuation">)</span><span class="token punctuation">]</span>  
 文档 <span class="token number">1</span> 的主题分布: <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0</span>, <span class="token number">0.9999999999999997</span><span class="token punctuation">)</span><span class="token punctuation">]</span>  
 文档 <span class="token number">2</span> 的主题分布: <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">0.9999999999999998</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
</code></pre> 
<h2><a id="3_113"></a>3、总结</h2> 
<p>潜在狄利克雷分配（LDA）是一种强大的无监督学习算法，它能够通过发现文档集合中的隐藏主题和主题分布，有效地揭示文档集合的内在结构。LDA的灵活性使其成为主题建模、文档分类、信息检索等多个任务中的理想选择。</p> 
<p>LDA的核心在于它的生成式模型框架，该框架允许我们同时建模文档的主题混合和每个主题下的单词分布。通过最大化文档的似然性，LDA能够学习到这些分布，进而揭示出文档中的主题信息。</p> 
<p>在实际应用中，LDA通常需要配合适当的预处理步骤（如分词、停用词去除、词干提取等）以及后续处理步骤（如主题可视化、主题解释等）来达到最佳效果。</p> 
<p>此外，LDA的性能也受到一些因素的影响，如主题数量的选择、超参数的设定以及语料库的大小和质量等。</p> 
<p>我是<a href="https://blog.csdn.net/wuyoudeyuer?type=blog"><strong>小鱼</strong></a>：</p> 
<ul><li><strong>CSDN 博客专家</strong>；</li><li><strong>阿里云 专家博主</strong>；</li><li><strong>51CTO博客专家</strong>；</li><li><strong>企业认证金牌面试官</strong>；</li><li><strong>多个名企认证&amp;特邀讲师等</strong>；</li><li><strong>名企签约职场面试培训、职场规划师</strong>；</li><li><strong>多个国内主流技术社区的认证专家博主</strong>；</li><li><strong>多款主流产品(阿里云等)测评一、二等奖获得者</strong>；</li></ul> 
<p>关注<strong>小鱼</strong>，学习<a href="https://blog.csdn.net/wuyoudeyuer/category_12468165.html"><strong>【机器学习】&amp;【深度学习】</strong></a>知识，不再迷路。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1daaf2fcae8bd6478badb1856703c178/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">PyWebIO，一个非常好用的 Python 库！</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/89a117916bb636c4d55a5377f2681e85/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">高级数据结构 ＜AVL树＞</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>