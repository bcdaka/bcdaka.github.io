<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AIGC：阿里开源大模型通义千问部署与实战 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/6f79eb4682678b5b70aa84ac67e3308b/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="AIGC：阿里开源大模型通义千问部署与实战">
  <meta property="og:description" content="1 引言 通义千问-7B（Qwen-7B）是阿里云研发的通义千问大模型系列的70亿参数规模的模型。Qwen-7B是基于Transformer的大语言模型, 在超大规模的预训练数据上进行训练得到。预训练数据类型多样，覆盖广泛，包括大量网络文本、专业书籍、代码等。同时，在Qwen-7B的基础上，我们使用对齐机制打造了基于大语言模型的AI助手Qwen-7B-Chat。相较于最初开源的Qwen-7B模型，我们现已将预训练模型和Chat模型更新到效果更优的版本。本仓库为Qwen-7B预训练模型的仓库。
体验地址：https://modelscope.cn/studios/qwen/Qwen-7B-Chat-Demo/summary
代码地址：https://github.com/QwenLM/Qwen
通义千问-7B（Qwen-7B）主要有以下特点：
大规模高质量训练语料：使用超过2.4万亿tokens的数据进行预训练，包含高质量中、英、多语言、代码、数学等数据，涵盖通用及专业领域的训练语料。通过大量对比实验对预训练语料分布进行了优化。强大的性能：Qwen-7B在多个中英文下游评测任务上（涵盖常识推理、代码、数学、翻译等），效果显著超越现有的相近规模开源模型，甚至在部分指标上相比更大尺寸模型也有较强竞争力。具体评测结果请详见下文。覆盖更全面的词表：相比目前以中英词表为主的开源模型，Qwen-7B使用了约15万大小的词表。该词表对多语言更加友好，方便用户在不扩展词表的情况下对部分语种进行能力增强和扩展。 2 通义千问介绍 2.1 介绍
我们开源了Qwen（通义千问）系列工作，当前开源模型的参数规模为18亿（1.8B）、70亿（7B）、140亿（14B）和720亿（72B）。本次开源包括基础模型Qwen，即Qwen-1.8B、Qwen-7B、Qwen-14B、Qwen-72B，以及对话模型Qwen-Chat，即Qwen-1.8B-Chat、Qwen-7B-Chat、Qwen-14B-Chat和Qwen-72B-Chat。模型链接在表格中，请点击了解详情。同时，我们公开了我们的技术报告，请点击上方论文链接查看。
当前基础模型已经稳定训练了大规模高质量且多样化的数据，覆盖多语言（当前以中文和英文为主），总量高达3万亿token。在相关基准评测中，Qwen系列模型拿出非常有竞争力的表现，显著超出同规模模型并紧追一系列最强的闭源模型。此外，我们利用SFT和RLHF技术实现对齐，从基座模型训练得到对话模型。Qwen-Chat具备聊天、文字创作、摘要、信息抽取、翻译等能力，同时还具备一定的代码生成和简单数学推理的能力。在此基础上，我们针对LLM对接外部系统等方面针对性地做了优化，当前具备较强的工具调用能力，以及最近备受关注的Code Interpreter的能力和扮演Agent的能力。我们将各个大小模型的特点列到了下表。
在这个项目中，你可以了解到以下内容
快速上手Qwen-Chat教程，玩转大模型推理量化模型相关细节，包括GPTQ和KV cache量化推理性能数据，包括推理速度和显存占用微调的教程，帮你实现全参数微调、LoRA以及Q-LoRA部署教程，以vLLM和FastChat为例搭建Demo的方法，包括WebUI和CLI Demo搭建API的方法，我们提供的示例为OpenAI风格的API更多关于Qwen在工具调用、Code Interpreter、Agent方面的内容长序列理解能力及评测使用协议 2.2 新闻 2023.11.30 🔥 我们推出 Qwen-72B 和 Qwen-72B-Chat，它们在 3T tokens上进行训练，并支持 32k 上下文。同时也发布了 Qwen-1.8B 和 Qwen-1.8B-Chat。我们还增强了 Qwen-72B-Chat 和 Qwen-1.8B-Chat 的系统指令（System Prompt）功能，请参阅示例文档。此外，我们还对昇腾910以及海光DCU实现了推理的支持，详情请查看ascend-support及dcu-support文件夹。2023年10月17日 我们推出了Int8量化模型Qwen-7B-Chat-Int8和Qwen-14B-Chat-Int8。2023年9月25日 在魔搭社区（ModelScope）和Hugging Face推出Qwen-14B和Qwen-14B-Chat模型，并开源 qwen.cpp 和 Qwen-Agent。Qwen-7B和Qwen-7B-Chat的代码和模型也同步得到更新。请使用最新的代码和模型！ 相比原版Qwen-7B，新版用了更多训练数据（从2.2T增加到2.4T tokens），序列长度从2048扩展至8192。整体中文能力以及代码能力均有所提升。2023年9月12日 支持Qwen-7B和Qwen-7B-Chat的微调，其中包括全参数微调、LoRA以及Q-LoRA。2023年8月21日 发布Qwen-7B-Chat的Int4量化模型，Qwen-7B-Chat-Int4。该模型显存占用低，推理速度相比半精度模型显著提升，在基准评测上效果损失较小。2023年8月3日 在魔搭社区（ModelScope）和Hugging Face同步推出Qwen-7B和Qwen-7B-Chat模型。同时，我们发布了技术备忘录，介绍了相关的训练细节和模型表现。 2.3 评测表现 Qwen系列模型相比同规模模型均实现了效果的显著提升。我们评测的数据集包括MMLU、C-Eval、 GSM8K、 MATH、HumanEval、MBPP、BBH等数据集，考察的能力包括自然语言理解、知识、数学计算和推理、代码生成、逻辑推理等。Qwen-72B在所有任务上均超越了LLaMA2-70B的性能，同时在10项任务中的7项任务中超越GPT-3.5.
ModelMMLUC-EvalGSM8KMATHHumanEvalMBPPBBHCMMLU5-shot5-shot8-shot4-shot0-shot3-shot3-shot5-shotLLaMA2-7B46.832.516.73.312.820.838.231.8LLaMA2-13B55.041.429.65.018.930.345.638.4LLaMA2-34B62.6-42.26.222.633.044.1-ChatGLM2-6B47.951.732.46.5--33.7-InternLM-7B51.053.431.26.310.414.037.051.8InternLM-20B62.158.852.67.925.635.652.559.0Baichuan2-7B54.756.324.65.618.324.241.657.1Baichuan2-13B59.559.052.810.117.130.249.062.0Yi-34B76.381.867.915.926.238.266.482.6XVERSE-65B70.868.660.3-26.3---Qwen-1.8B45.356.132.32.315.214.222.352.1Qwen-7B58.263.551.711.629.931.645.062.2Qwen-14B66.372.161.324.832.340.853.471.0Qwen-72B77.483.378.935.235.452.267.783.6 对于以上所有对比模型，我们列出了其官方汇报结果与OpenCompass结果之间的最佳分数。
更多的实验结果和细节请查看我们的技术备忘录。点击这里。
2.4 推理性能 这一部分将介绍模型推理的速度和显存占用的相关数据。下文的性能测算使用 此脚本 完成。
我们测算了BF16、Int8和Int4模型在生成2048个token时的平均推理速度（tokens/s）和显存使用。结果如下所示：
Model SizeQuantizationSpeed (Tokens/s)GPU Memory Usage1.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-12-21T10:14:32+08:00">
    <meta property="article:modified_time" content="2023-12-21T10:14:32+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AIGC：阿里开源大模型通义千问部署与实战</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>1 引言</h2> 
<p><strong>通义千问-7B（Qwen-7B）</strong>是阿里云研发的通义千问大模型系列的70亿参数规模的模型。Qwen-7B是基于Transformer的大语言模型, 在超大规模的预训练数据上进行训练得到。预训练数据类型多样，覆盖广泛，包括大量网络文本、专业书籍、代码等。同时，在Qwen-7B的基础上，我们使用对齐机制打造了基于大语言模型的AI助手Qwen-7B-Chat。相较于最初开源的Qwen-7B模型，我们现已将预训练模型和Chat模型更新到效果更优的版本。本仓库为Qwen-7B预训练模型的仓库。</p> 
<p>体验地址：<a class="link-info" href="https://modelscope.cn/studios/qwen/Qwen-7B-Chat-Demo/summary" rel="nofollow" title="https://modelscope.cn/studios/qwen/Qwen-7B-Chat-Demo/summary">https://modelscope.cn/studios/qwen/Qwen-7B-Chat-Demo/summary</a></p> 
<p>代码地址：<a class="link-info" href="https://github.com/QwenLM/Qwen" title="https://github.com/QwenLM/Qwen">https://github.com/QwenLM/Qwen</a></p> 
<p>通义千问-7B（Qwen-7B）主要有以下特点：</p> 
<ul><li> 大规模高质量训练语料：使用超过2.4万亿tokens的数据进行预训练，包含高质量中、英、多语言、代码、数学等数据，涵盖通用及专业领域的训练语料。通过大量对比实验对预训练语料分布进行了优化。</li><li>强大的性能：Qwen-7B在多个中英文下游评测任务上（涵盖常识推理、代码、数学、翻译等），效果显著超越现有的相近规模开源模型，甚至在部分指标上相比更大尺寸模型也有较强竞争力。具体评测结果请详见下文。</li><li>覆盖更全面的词表：相比目前以中英词表为主的开源模型，Qwen-7B使用了约15万大小的词表。该词表对多语言更加友好，方便用户在不扩展词表的情况下对部分语种进行能力增强和扩展。</li></ul> 
<h2>2 通义千问介绍</h2> 
<p>2.1 介绍</p> 
<p>我们开源了Qwen（通义千问）系列工作，当前开源模型的参数规模为18亿（1.8B）、70亿（7B）、140亿（14B）和720亿（72B）。本次开源包括基础模型Qwen，即Qwen-1.8B、Qwen-7B、Qwen-14B、Qwen-72B，以及对话模型Qwen-Chat，即Qwen-1.8B-Chat、Qwen-7B-Chat、Qwen-14B-Chat和Qwen-72B-Chat。模型链接在表格中，请点击了解详情。同时，我们公开了我们的<a href="https://arxiv.org/abs/2309.16609" rel="nofollow" title="技术报告">技术报告</a>，请点击上方论文链接查看。</p> 
<p><img alt="" height="387" src="https://images2.imgbox.com/9f/8a/5J2e5ANn_o.png" width="1200"></p> 
<p>当前基础模型已经稳定训练了大规模高质量且多样化的数据，覆盖多语言（当前以中文和英文为主），总量高达3万亿token。在相关基准评测中，Qwen系列模型拿出非常有竞争力的表现，显著超出同规模模型并紧追一系列最强的闭源模型。此外，我们利用SFT和RLHF技术实现对齐，从基座模型训练得到对话模型。Qwen-Chat具备聊天、文字创作、摘要、信息抽取、翻译等能力，同时还具备一定的代码生成和简单数学推理的能力。在此基础上，我们针对LLM对接外部系统等方面针对性地做了优化，当前具备较强的工具调用能力，以及最近备受关注的Code Interpreter的能力和扮演Agent的能力。我们将各个大小模型的特点列到了下表。</p> 
<p><img alt="" height="427" src="https://images2.imgbox.com/99/0c/tpDr1UGm_o.png" width="1200"></p> 
<p>在这个项目中，你可以了解到以下内容</p> 
<ul><li>快速上手Qwen-Chat教程，玩转大模型推理</li><li>量化模型相关细节，包括GPTQ和KV cache量化</li><li>推理性能数据，包括推理速度和显存占用</li><li>微调的教程，帮你实现全参数微调、LoRA以及Q-LoRA</li><li>部署教程，以vLLM和FastChat为例</li><li>搭建Demo的方法，包括WebUI和CLI Demo</li><li>搭建API的方法，我们提供的示例为OpenAI风格的API</li><li>更多关于Qwen在工具调用、Code Interpreter、Agent方面的内容</li><li>长序列理解能力及评测</li><li>使用协议</li></ul> 
<h3>2.2 新闻</h3> 
<ul><li>2023.11.30 🔥 我们推出 Qwen-72B 和 Qwen-72B-Chat，它们在 3T tokens上进行训练，并支持 32k 上下文。同时也发布了 Qwen-1.8B 和 Qwen-1.8B-Chat。我们还增强了 Qwen-72B-Chat 和 Qwen-1.8B-Chat 的系统指令（System Prompt）功能，请参阅<a href="https://github.com/QwenLM/Qwen/blob/main/examples/system_prompt.md" title="示例文档">示例文档</a>。此外，我们还对昇腾910以及海光DCU实现了推理的支持，详情请查看<code>ascend-support</code>及<code>dcu-support</code>文件夹。</li><li>2023年10月17日 我们推出了Int8量化模型Qwen-7B-Chat-Int8和Qwen-14B-Chat-Int8。</li><li>2023年9月25日 在魔搭社区（ModelScope）和Hugging Face推出Qwen-14B和Qwen-14B-Chat模型，并开源 <a href="https://github.com/QwenLM/qwen.cpp" title="qwen.cpp">qwen.cpp</a> 和 <a href="https://github.com/QwenLM/Qwen-Agent" title="Qwen-Agent">Qwen-Agent</a>。Qwen-7B和Qwen-7B-Chat的代码和模型也同步得到更新。请使用最新的代码和模型！ 
  <ul><li>相比原版Qwen-7B，新版用了更多训练数据（从2.2T增加到2.4T tokens），序列长度从2048扩展至8192。整体中文能力以及代码能力均有所提升。</li></ul></li><li>2023年9月12日 支持Qwen-7B和Qwen-7B-Chat的微调，其中包括全参数微调、LoRA以及Q-LoRA。</li><li>2023年8月21日 发布Qwen-7B-Chat的Int4量化模型，Qwen-7B-Chat-Int4。该模型显存占用低，推理速度相比半精度模型显著提升，在基准评测上效果损失较小。</li><li>2023年8月3日 在魔搭社区（ModelScope）和Hugging Face同步推出Qwen-7B和Qwen-7B-Chat模型。同时，我们发布了技术备忘录，介绍了相关的训练细节和模型表现。</li></ul> 
<h3>2.3 评测表现</h3> 
<p>Qwen系列模型相比同规模模型均实现了效果的显著提升。我们评测的数据集包括MMLU、C-Eval、 GSM8K、 MATH、HumanEval、MBPP、BBH等数据集，考察的能力包括自然语言理解、知识、数学计算和推理、代码生成、逻辑推理等。Qwen-72B在所有任务上均超越了LLaMA2-70B的性能，同时在10项任务中的7项任务中超越GPT-3.5.</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/b9/cb/RvctddqH_o.jpg" width="1200"></p> 
<table><thead><tr><th>Model</th><th>MMLU</th><th>C-Eval</th><th>GSM8K</th><th>MATH</th><th>HumanEval</th><th>MBPP</th><th>BBH</th><th>CMMLU</th></tr></thead><tbody><tr><td></td><td>5-shot</td><td>5-shot</td><td>8-shot</td><td>4-shot</td><td>0-shot</td><td>3-shot</td><td>3-shot</td><td>5-shot</td></tr><tr><td>LLaMA2-7B</td><td>46.8</td><td>32.5</td><td>16.7</td><td>3.3</td><td>12.8</td><td>20.8</td><td>38.2</td><td>31.8</td></tr><tr><td>LLaMA2-13B</td><td>55.0</td><td>41.4</td><td>29.6</td><td>5.0</td><td>18.9</td><td>30.3</td><td>45.6</td><td>38.4</td></tr><tr><td>LLaMA2-34B</td><td>62.6</td><td>-</td><td>42.2</td><td>6.2</td><td>22.6</td><td>33.0</td><td>44.1</td><td>-</td></tr><tr><td>ChatGLM2-6B</td><td>47.9</td><td>51.7</td><td>32.4</td><td>6.5</td><td>-</td><td>-</td><td>33.7</td><td>-</td></tr><tr><td>InternLM-7B</td><td>51.0</td><td>53.4</td><td>31.2</td><td>6.3</td><td>10.4</td><td>14.0</td><td>37.0</td><td>51.8</td></tr><tr><td>InternLM-20B</td><td>62.1</td><td>58.8</td><td>52.6</td><td>7.9</td><td>25.6</td><td>35.6</td><td>52.5</td><td>59.0</td></tr><tr><td>Baichuan2-7B</td><td>54.7</td><td>56.3</td><td>24.6</td><td>5.6</td><td>18.3</td><td>24.2</td><td>41.6</td><td>57.1</td></tr><tr><td>Baichuan2-13B</td><td>59.5</td><td>59.0</td><td>52.8</td><td>10.1</td><td>17.1</td><td>30.2</td><td>49.0</td><td>62.0</td></tr><tr><td>Yi-34B</td><td>76.3</td><td>81.8</td><td>67.9</td><td>15.9</td><td>26.2</td><td>38.2</td><td>66.4</td><td>82.6</td></tr><tr><td>XVERSE-65B</td><td>70.8</td><td>68.6</td><td>60.3</td><td>-</td><td>26.3</td><td>-</td><td>-</td><td>-</td></tr><tr><td><strong>Qwen-1.8B</strong></td><td>45.3</td><td>56.1</td><td>32.3</td><td>2.3</td><td>15.2</td><td>14.2</td><td>22.3</td><td>52.1</td></tr><tr><td><strong>Qwen-7B</strong></td><td>58.2</td><td>63.5</td><td>51.7</td><td>11.6</td><td>29.9</td><td>31.6</td><td>45.0</td><td>62.2</td></tr><tr><td><strong>Qwen-14B</strong></td><td>66.3</td><td>72.1</td><td>61.3</td><td>24.8</td><td>32.3</td><td>40.8</td><td>53.4</td><td>71.0</td></tr><tr><td><strong>Qwen-72B</strong></td><td><strong>77.4</strong></td><td><strong>83.3</strong></td><td><strong>78.9</strong></td><td><strong>35.2</strong></td><td><strong>35.4</strong></td><td><strong>52.2</strong></td><td><strong>67.7</strong></td><td><strong>83.6</strong></td></tr></tbody></table> 
<p>对于以上所有对比模型，我们列出了其官方汇报结果与<a href="https://opencompass.org.cn/leaderboard-llm" rel="nofollow" title="OpenCompass">OpenCompass</a>结果之间的最佳分数。</p> 
<p>更多的实验结果和细节请查看我们的技术备忘录。点击<a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf" rel="nofollow" title="这里">这里</a>。</p> 
<h3>2.4 推理性能</h3> 
<p>这一部分将介绍模型推理的速度和显存占用的相关数据。下文的性能测算使用 <a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py" rel="nofollow" title="此脚本">此脚本</a> 完成。</p> 
<p>我们测算了BF16、Int8和Int4模型在生成2048个token时的平均推理速度（tokens/s）和显存使用。结果如下所示：</p> 
<table><tbody><tr><td>Model Size</td><td>Quantization</td><td>Speed (Tokens/s)</td><td>GPU Memory Usage</td></tr><tr><td rowspan="3">1.8B</td><td>BF16</td><td>54.09</td><td>4.23GB</td></tr><tr><td>Int8</td><td>55.56</td><td>3.48GB</td></tr><tr><td>Int4</td><td>71.07</td><td>2.91GB</td></tr><tr><td rowspan="3">7B</td><td>BF16</td><td>40.93</td><td>16.99GB</td></tr><tr><td>Int8</td><td>37.47</td><td>11.20GB</td></tr><tr><td>Int4</td><td>50.09</td><td>8.21GB</td></tr><tr><td rowspan="3">14B</td><td>BF16</td><td>32.22</td><td>30.15GB</td></tr><tr><td>Int8</td><td>29.28</td><td>18.81GB</td></tr><tr><td>Int4</td><td>38.72</td><td>13.01GB</td></tr><tr><td rowspan="3">72B</td><td>BF16</td><td>8.48</td><td>144.69GB (2xA100)</td></tr><tr><td>Int8</td><td>9.05</td><td>81.27GB (2xA100)</td></tr><tr><td>Int4</td><td>11.32</td><td>48.86GB</td></tr><tr><td>72B + vLLM</td><td>BF16</td><td>17.60</td><td>2xA100</td></tr></tbody></table> 
<p>评测运行于单张A100-SXM4-80G GPU（除非提到使用2xA100），使用PyTorch 2.0.1、CUDA 11.8和Flash-Attention2。(72B + vLLM 使用 PyTorch 2.1.0和Cuda 11.8.)推理速度是生成2048个token的速度均值。</p> 
<p>注意：以上Int4/Int8模型生成速度使用autogptq库给出，当前<code>AutoModelForCausalLM.from_pretrained</code>载入的模型生成速度会慢大约20%。我们已经将该问题汇报给HuggingFace团队，若有解决方案将即时更新。</p> 
<p>我们还测量了不同上下文长度、生成长度、Flash-Attention版本的推理速度和 GPU 内存使用情况。可以在 Hugging Face 或 ModelScope 上的相应的模型介绍页面找到结果。</p> 
<h2>3 大模型部署与实战</h2> 
<h3>3.1 conda环境准备</h3> 
<p>conda环境准备详见：<a href="https://mp.csdn.net/mp_blog/creation/editor/130813001" title="annoconda">annoconda</a></p> 
<h3>3.2 运行环境构建</h3> 
<pre><code>git clone https://github.com/fishaudio/Bert-VITS2
cd Bert-VITS2

conda create -n vits2 python=3.9
conda activate vits2

pip install -r requirements.txt
pip install gradio
pip install mdtex2html</code></pre> 
<h3>3.3 预训练模型下载</h3> 
<p>Qwen-7B模型：<a class="link-info" href="https://huggingface.co/Qwen/Qwen-7B/tree/main" rel="nofollow" title="https://huggingface.co/Qwen/Qwen-7B/tree/main">https://huggingface.co/Qwen/Qwen-7B/tree/main</a></p> 
<p>也可以使用git下载，下载文成后，存储到根目录的Qwen文件夹下，如下所示：</p> 
<pre><code>(qwen) [root@localhost Qwen]# ll Qwen/Qwen-7B-Chat/
总用量 15083460
-rw-r--r-- 1 root root       8404 12月 20 10:51 cache_autogptq_cuda_256.cpp
-rw-r--r-- 1 root root      51991 12月 20 10:51 cache_autogptq_cuda_kernel_256.cu
-rw-r--r-- 1 root root        911 12月 20 10:51 config.json
-rw-r--r-- 1 root root       2345 12月 20 10:51 configuration_qwen.py
-rw-r--r-- 1 root root       1924 12月 20 10:51 cpp_kernels.py
-rw-r--r-- 1 root root        273 12月 20 10:51 generation_config.json
-rw-r--r-- 1 root root 1964066488 12月 20 10:52 model-00001-of-00008.safetensors
-rw-r--r-- 1 root root 2023960808 12月 20 10:53 model-00002-of-00008.safetensors
-rw-r--r-- 1 root root 2023960816 12月 20 10:54 model-00003-of-00008.safetensors
-rw-r--r-- 1 root root 2023960848 12月 20 10:55 model-00004-of-00008.safetensors
-rw-r--r-- 1 root root 2023960848 12月 20 10:57 model-00005-of-00008.safetensors
-rw-r--r-- 1 root root 2023960848 12月 20 10:58 model-00006-of-00008.safetensors
-rw-r--r-- 1 root root 2023960848 12月 20 10:59 model-00007-of-00008.safetensors
-rw-r--r-- 1 root root 1334845784 12月 20 10:59 model-00008-of-00008.safetensors
-rw-r--r-- 1 root root      55563 12月 20 10:59 modeling_qwen.py
-rw-r--r-- 1 root root      19547 12月 20 10:59 model.safetensors.index.json
-rw-r--r-- 1 root root      14604 12月 20 10:59 qwen_generation_utils.py
-rw-r--r-- 1 root root    2561218 12月 20 10:59 qwen.tiktoken
-rw-r--r-- 1 root root       9618 12月 20 10:59 tokenization_qwen.py
-rw-r--r-- 1 root root        174 12月 20 10:59 tokenizer_config.json</code></pre> 
<h3>3.4 运行模型</h3> 
<p>修改web_demo.py中的代码</p> 
<pre><code>    parser.add_argument("--server-name", type=str, default="127.0.0.1",
                        help="Demo server name.")

修改为：（改为本地局域网地址）

    parser.add_argument("--server-name", type=str, default="192.168.1.150",
                        help="Demo server name.")</code></pre> 
<p>运行web</p> 
<pre><code>python web_demo.py</code></pre> 
<p>启动界面如下：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/fb/66/6PIPVAk8_o.png" width="1200"></p> 
<h2>4 Qwen能力测试</h2> 
<h3>4.1 知识问答</h3> 
<p>问题：按照你认为的顺序列出中国古代最著名的都城，不超过10个</p> 
<pre><code>Qwen-Chat: 中国古代最著名的都城有：长安（西安）、洛阳、南京、开封、杭州、北京、成都、南京、上海。</code></pre> 
<p>问题：冬是24节气中的哪个节气</p> 
<pre><code>Qwen-Chat: 立冬是农历二十四节气之一，斗柄指向西北，太阳黄经达225°，于每年公历11月7-8日之间交节。此时，北斗七星已由夏季星空移到冬季星空。由于太阳直射南半球，北半球昼短夜长，天气也逐渐转冷，开始进入冬季。农业生产上，需要做好农作物的越冬防冻工作；同时人们也开始着手准备冬季的取暖设备，并储备足够的过冬食物。</code></pre> 
<p>问题：背诵24节气，并支出立夏是哪个节气</p> 
<pre><code>24节气是中国古代劳动人民根据太阳在黄道上的位置，把一年划分为24等份，每份为一个节气，始于立春，终于大寒，每个节气分别有3天（干支计法）或4天（太阳记时法）。以下是24节气名称及其对应日期：&lt;br&gt;1. 立春：每年公历2月3-5日&lt;br&gt;2. 雨水：每年公历2月18-20日&lt;br&gt;3. 惊蛰：每年公历3月5-7日&lt;br&gt;4. 春分：每年公历3月20-22日&lt;br&gt;5. 清明：每年公历4月4-6日&lt;br&gt;6. 谷雨：每年公历4月19-21日&lt;br&gt;7. 立夏：每年公历5月5-7日&lt;br&gt;8. 小满：每年公历5月20-22日&lt;br&gt;9. 芒种：每年公历6月5-7日&lt;br&gt;10. 夏至：每年公历6月21-22日&lt;br&gt;11. 小暑：每年公历7月6-8日&lt;br&gt;12. 大暑：每年公历7月22-24日&lt;br&gt;13. 立秋：每年公历8月7-9日&lt;br&gt;14. 处暑：每年公历8月22-24日&lt;br&gt;15. 白露：每年公历9月7-9日&lt;br&gt;16. 秋分：每年公历9月22-24日&lt;br&gt;17. 寒露：每年公历10月8-9日&lt;br&gt;18. 霜降：每年公历10月23-24日&lt;br&gt;19. 立冬：每年公历11月7-8日&lt;br&gt;20. 小雪：每年公历11月22-23日&lt;br&gt;21. 大雪：每年公历12月7-8日&lt;br&gt;22. 冬至：每年公历12月21-23日&lt;br&gt;23. 小寒：每年公历1月5-7日&lt;br&gt;24. 大寒：每年公历1月20-22日</code></pre> 
<h3>4.2 文本生成</h3> 
<p>问题：写一遍200字的描述月亮的散文</p> 
<pre><code>Qwen-Chat: 月亮，那宁静而神秘的自然现象，它不仅是夜晚最明亮的星体，也是人类内心深处的象征。月亮以其独特的光芒照亮了黑夜，让世界变得温柔而浪漫。它不仅带给我们光明，还给了我们许多故事和传说，让我们对宇宙充满了好奇与敬畏之心。无论是在东方还是西方，月亮都是文化中不可或缺的一部分，它给我们的生活带来了无尽的乐趣和启示。月亮的存在使我们感到温暖和安慰，让我们在黑暗中找到了希望和力量。月亮，你是那么的美丽，那么的神秘，你让人们在你的光辉下感到无比幸福和安宁。</code></pre> 
<h3>4.3 数理逻辑</h3> 
<p>问题：一堆土豆，原来每次运9吨，12次才能运完，现在每次运11吨，多少次可以运完</p> 
<pre><code>Qwen-Chat: 设原来堆土豆的重量为x吨，则每次运输9吨，需要12次才能运输完毕。可得：x = 9 * 12&lt;br&gt;而现在每次运输11吨，所以需要运输的次数为：x / 11 = 12次&lt;br&gt;因此，现在需要12次就可以将全部土豆运完。</code></pre> 
<h3>4.4 语言理解</h3> 
<p>问题：请总结塞万提斯的作品--《唐吉坷德》的主要内容和中心思想</p> 
<pre><code>Qwen-Chat: 《唐吉诃德》是西班牙作家塞万提斯的一部经典小说，它的主要情节是关于一个名叫唐吉诃德的乡村骑士，他误以为自己是一名英勇的骑士，开始了一系列荒谬而又滑稽的行为。在这个过程中，他遇到了各种各样的人物，包括他的朋友桑丘、一位富商和他的家臣，以及各种骗子和恶棍。&lt;br&gt;这部小说的核心思想是对现实主义文学的一种挑战，同时也反映了当时社会的一些问题，如愚昧无知、贪欲横行和道德败坏。唐吉诃德的行为虽然荒诞，但却是对社会现状的一种讽刺和批评。此外，小说还探讨了一些深刻的主题，如爱情、友情、家庭、权力和正义等。&lt;br&gt;总的来说，《唐吉诃德》是一部充满幽默和智慧的作品，通过主角唐吉诃德的经历，作者向读者传达了一个重要的信息：人类不应该被虚幻的理想所迷惑，而是应该勇敢面对现实，追求真理和正义。</code></pre> 
<h3>4.5 常识问题</h3> 
<p>问题：树上有9只鸟，猎人开一枪打死了一只，树上还剩几只鸟</p> 
<pre><code>Qwen-Chat: 如果树上原来有9只鸟，猎人开一枪打死了一只，那么树上剩下的鸟应该是8只。因为当一只鸟被打死后，其他鸟会受到影响，可能会受到惊吓或者逃跑，导致剩下的鸟的数量减少。但是，在这个问题中，由于没有提供更多信息，所以无法确定最后剩下的鸟的数量是否会少于8只。</code></pre> 
<h3>4.6 代码生成</h3> 
<p>问题：请用python编写一个冒泡排序算法，带测试例子</p> 
<pre><code>Qwen-Chat: 
```python
def bubble_sort(arr):
    n = len(arr)
    
    # 遍历所有数组元素
    for i in range(n):
        
        # Last i elements are already in place
        for j in range(0, n-i-1):
            
            # traverse the array from 0 to n-i-1
            # Swap if the element found is greater than the next element
            if arr[j] &gt; arr[j+1] :
                arr[j], arr[j+1] = arr[j+1], arr[j]
                
    return arr
```

以下是使用这个函数进行测试的例子：

```python
arr = [64, 34, 25, 12, 22, 11, 90]
print("Given array is")
print(arr)

sorted_arr = bubble_sort(arr)

print("\nSorted array is")
print(sorted_arr)
```

输出将是：

```
Given array is
[64, 34, 25, 12, 22, 11, 90]

Sorted array is
[11, 12, 22, 25, 34, 64, 90]
``` 

在这个例子中，我们首先定义了一个未排序的数组。然后，我们调用了bubble_sort函数来对数组进行排序，并将结果存储在sorted_arr变量中。最后，我们打印出原始数组和排序后的数组。</code></pre> 
<h3>4.7 医疗问题</h3> 
<p>问题：乳腺癌发病的原因是什么</p> 
<pre><code>Qwen-Chat: 乳腺癌是一种恶性肿瘤，其发病原因可能是多种因素共同作用的结果，包括遗传因素、环境因素和生活方式因素等。&lt;br&gt;1. 遗传因素：家族中有乳腺癌病史的人，患乳腺癌的风险比其他人高。研究发现，大约20%的乳腺癌病例是由遗传因素引起的。&lt;br&gt;2. 环境因素：环境污染、辐射暴露、内分泌失调等因素都可能导致乳腺癌的发生。例如，长期接触有害化学物质，如石棉、苯等，或暴露于放射线中，都可能增加患乳腺癌的风险。&lt;br&gt;3. 生活方式因素：不健康的生活方式也可能增加患乳腺癌的风险。例如，长期吸烟、饮酒、缺乏运动、饮食不均衡等都可能导致乳腺癌的发生。&lt;br&gt;4. 其他因素：包括年龄、性别、初潮年龄、绝经年龄、激素替代疗法等也可能影响乳腺癌的发生率。&lt;br&gt;需要注意的是，这些因素单独或组合起来并不能保证一定会发生乳腺癌，而且这些因素的影响程度也会因个体差异而有所不同。因此，预防乳腺癌的关键在于改变不良生活习惯，定期体检，并积极治疗早期疾病。</code></pre> 
<h3>4.8 内容总结</h3> 
<p>问题：对以下内容做个总结：今天一放学，我和同学一起蹦蹦跳跳地回家。因为，今天是美好的圣诞节平安夜。一回家，爸爸就告诉我今天在国际大厦吃饭。我听了，高兴得一蹦三尺高。等妈妈一回家，我和爸爸就拉着妈妈出门去吃饭。一路上，我看到今晚行人的脸上都洋溢着笑脸，给人一种祥和，温馨的感觉。走到国际大厦的门口就感觉到一股暖流，玻璃门外寒风刺骨，推门进入温暖如春。大厦里的圣诞老人不停地向我们招手。圣诞树那翠绿的树枝上面挂满了千盏万盏五彩缤纷的彩灯。一闪一闪地眨着闪亮的眼睛。一进大厦，我就直奔2楼，西餐厅里回旋着一首圣诞歌，歌声悠扬，让心情舒畅。墙上挂着一张和蔼可亲的圣诞老人的照片。我和爸爸，妈妈还有叔叔阿姨坐在一张桌子上，大家好像一个大家庭。没过多长时间，那些美味可口的菜就来了。一见那香喷喷的鸭肉，我再也控制不住自己，狼吞虎咽地吃起来。等我吃完这些可口以后，贪玩的我和许多小朋友在国际大厦跑来跑去，跑上跑下，一会儿去看那和蔼可亲的圣诞老人，一会儿去看那些五彩缤纷光芒的圣诞树。圣诞节是一个收礼物的好节日。小时候的我总是梦想有一天圣诞老人能到我家，送我美好的圣诞礼物。长大了，我知道小时候那些礼物都是爸爸妈妈送的。今天的圣诞节，我缠着爸爸妈妈给我礼物。妈妈笑着告诉我。妈妈笑着告诉我：“明天早上你醒来，圣诞老人的了礼物就回送到。”虽然我知道礼物是爸爸妈妈送的，但是晚上，我仍然拿了一双袜子放在床头上。早上醒来，我发现袜子里塞着一张35元的书卡，我太高兴了，在床上跳着喊到：“圣诞老人万岁！”我拿着书卡去上学，去向同学们炫耀炫耀。</p> 
<pre><code>Qwen-Chat: 好的，您的概括是：&lt;br&gt;这篇文章描述了作者在圣诞节那天的经历。那天放学后，作者和同学一起回家，然后在家中得知晚上要去国际大厦吃饭，十分开心。他们在路上看到了人们的笑容，感到祥和温馨。走进国际大厦后，他们去了2楼的西餐厅吃饭，听到了优美的圣诞歌曲，看到了圣诞老人的照片。晚餐结束后，他们和许多人一起玩耍，看圣诞树，度过了一个快乐的圣诞节。</code></pre> 
<p>5 代码调用qwen</p> 
<pre><code class="hljs">from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# Model names: "Qwen/Qwen-7B-Chat", "Qwen/Qwen-14B-Chat"
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat",
    device_map="auto",
    trust_remote_code=True
).eval()

# Specify hyperparameters for generation. But if you use transformers&gt;=4.32.0, there is no need to do this.
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# 1st dialogue turn
response, history = model.chat(tokenizer, "你好", history=None)
print(response)

# 2nd dialogue turn
response, history = model.chat(tokenizer, "肿瘤居家营养应该注意什么？", history=history)
print(response)

# 3rd dialogue turn
response, history = model.chat(tokenizer, "居家期间肌肉减少怎么办？", history=history)
print(response)</code></pre> 
<p>运行代码输出</p> 
<pre><code class="hljs">你好！有什么我能帮助你的吗？
肿瘤患者在居家期间需要注意以下几点：

1. 饮食均衡：饮食应多样化，保持均衡营养，多吃新鲜蔬菜和水果、高蛋白食物（如瘦肉、鱼虾等）。

2. 注意营养补充：如有需要，可以考虑适当补充维生素、矿物质、膳食纤维等。

3. 控制饮食热量：由于肿瘤治疗过程中可能需要进行化疗或放疗，因此应注意控制饮食热量，避免摄入过多的脂肪和糖分。

4. 定时定量进餐：定时定量进餐有助于维持身体健康，并帮助更好地管理食欲。

5. 增加水分摄入：癌症患者往往会出现恶心、呕吐等症状，因此需要增加水分摄入以维持身体水分平衡。

在居家期间，由于长时间缺乏运动，很容易出现肌肉减少的问题。为了改善这种情况，你可以尝试以下几种方法：

1. 定期运动：可以尝试做一些简单的家庭运动，例如散步、慢跑、瑜伽等，每天坚持锻炼一段时间。

2. 合理膳食：注意补充蛋白质和维生素，可以多食用一些瘦肉、鱼虾、蛋类等高蛋白食物，以及绿叶蔬菜和水果等富含维生素的食物。

3. 适当休息：合理安排休息时间，尽量避免熬夜，保证充足的睡眠。

4. 调整心态：保持良好的心态，避免过度焦虑和紧张，这对缓解肌肉减少也有很大帮助。

5. 寻求专业建议：如果你发现肌肉减少的情况严重，建议寻求专业的医疗建议，以便及时采取适当的治疗措施。</code></pre> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/28be1a5938712430b0d482e8b7ae87fa/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【学习笔记】LeetCode SQL刷题（高频50基础版&#43;进阶版）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2bba789817739f722c941eb8d77399d9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Android Studio 下载地址</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>