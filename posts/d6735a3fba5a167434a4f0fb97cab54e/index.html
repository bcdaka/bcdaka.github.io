<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习——线性回归 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/d6735a3fba5a167434a4f0fb97cab54e/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="机器学习——线性回归">
  <meta property="og:description" content="学习目标 了解线性回归的应用场景知道线性回归的定义 1 线性回归应用场景 房价预测
销售额度预测
贷款额度预测
举例：
2 什么是线性回归 2.1 定义与公式 线性回归(Linear regression)是利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式。
特点：只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归 线性回归用矩阵表示举例 那么怎么理解呢？我们来看几个例子
期末成绩：0.7×考试成绩&#43;0.3×平时成绩房子价格 = 0.02×中心区域的距离 &#43; 0.04×城市一氧化氮浓度 &#43; (-0.12×自住房平均房价) &#43; 0.254×城镇犯罪率 上面两个例子，我们看到特征值与目标值之间建立了一个关系，这个关系可以理解为线性模型。
2.2 线性回归的特征与目标的关系分析 线性回归当中主要有两种模型，一种是线性关系，另一种是非线性关系。在这里我们只能画一个平面更好去理解，所以都用单个特征或两个特征举例子。
假设刚才的房子例子，真实的数据之间存在这样的关系：
真实关系：真实房子价格 = 0.02×中心区域的距离 &#43; 0.04×城市一氧化氮浓度 &#43; (-0.12×自住房平均房价) &#43; 0.254×城镇犯罪率 那么现在呢，我们随意指定一个关系（猜测）
随机指定关系：预测房子价格 = 0.25×中心区域的距离 &#43; 0.14×城市一氧化氮浓度 &#43; 0.42×自住房平均房价 &#43; 0.34×城镇犯罪率 请问这样的话，会发生什么？真实结果与我们预测的结果之间是不是存在一定的误差呢？类似这样样子
既然存在这个误差，那我们就将这个误差给衡量出来
1 损失函数 总损失定义为：
如何去减少这个损失，使我们预测的更加准确些？既然存在了这个损失，我们一直说机器学习有自动学习的功能，在线性回归这里更是能够体现。这里可以通过一些优化方法去优化（其实是数学当中的求导功能）回归的总损失！！！
2 优化算法 如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值）
​
2.1 正规方程 2.1.1 什么是正规方程
理解：X为特征值矩阵，y为目标值矩阵。直接求到最好的结果
缺点：当特征过多过复杂时，求解速度太慢并且得不到结果
2.1.2 正规方程求解举例
以下表示数据为例：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-03-27T21:07:53+08:00">
    <meta property="article:modified_time" content="2023-03-27T21:07:53+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习——线性回归</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3 id="学习目标">学习目标</h3> 
<ul><li>了解线性回归的应用场景</li><li>知道线性回归的定义</li></ul> 
<hr> 
<h3 id="1-线性回归应用场景">1 线性回归应用场景</h3> 
<ul><li> <p>房价预测</p> </li><li> <p>销售额度预测</p> </li><li> <p>贷款额度预测</p> </li></ul> 
<p>举例：</p> 
<p><img alt="" height="435" src="https://images2.imgbox.com/f2/d1/laGuzLsR_o.png" width="1055"></p> 
<p></p> 
<p></p> 
<h3 id="2-什么是线性回归">2 什么是线性回归</h3> 
<h4 id="21-定义与公式">2.1 定义与公式</h4> 
<p>线性回归(Linear regression)是利用<strong>回归方程(函数)</strong>对<strong>一个或多个自变量(特征值)和因变量(目标值)之间</strong>关系进行建模的一种分析方式。</p> 
<ul><li>特点：只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归<img alt="" height="256" src="https://images2.imgbox.com/cd/db/U6pmWk0K_o.png" width="1006"></li></ul> 
<p></p> 
<p></p> 
<ul><li>线性回归用矩阵表示举例<img alt="" height="573" src="https://images2.imgbox.com/19/8f/ybQEYowe_o.png" width="782"></li></ul> 
<p></p> 
<p></p> 
<p>那么怎么理解呢？我们来看几个例子</p> 
<ul><li>期末成绩：0.7×考试成绩+0.3×平时成绩</li><li>房子价格 = 0.02×中心区域的距离 + 0.04×城市一氧化氮浓度 + (-0.12×自住房平均房价) + 0.254×城镇犯罪率</li></ul> 
<p>上面两个例子，<strong>我们看到特征值与目标值之间建立了一个关系，这个关系可以理解为线性模型</strong>。</p> 
<h4 id="22-线性回归的特征与目标的关系分析">2.2 线性回归的特征与目标的关系分析</h4> 
<p>线性回归当中主要有两种模型，<strong>一种是线性关系，另一种是非线性关系。</strong>在这里我们只能画一个平面更好去理解，所以都用单个特征或两个特征举例子。</p> 
<p></p> 
<hr> 
<p>假设刚才的房子例子，真实的数据之间存在这样的关系：</p> 
<pre><code>真实关系：真实房子价格 = 0.02×中心区域的距离 + 0.04×城市一氧化氮浓度 + (-0.12×自住房平均房价) + 0.254×城镇犯罪率
</code></pre> 
<p>那么现在呢，我们随意指定一个关系（猜测）</p> 
<pre><code>随机指定关系：预测房子价格 = 0.25×中心区域的距离 + 0.14×城市一氧化氮浓度 + 0.42×自住房平均房价 + 0.34×城镇犯罪率
</code></pre> 
<p>请问这样的话，会发生什么？真实结果与我们预测的结果之间是不是存在一定的误差呢？类似这样样子<img alt="" height="684" src="https://images2.imgbox.com/66/ac/kePvKQho_o.png" width="1060"></p> 
<p> </p> 
<p></p> 
<p>既然存在这个误差，那我们就将这个误差给衡量出来</p> 
<h3 id="1-损失函数">1 损失函数</h3> 
<p>总损失定义为：<img alt="" height="169" src="https://images2.imgbox.com/e9/dc/0xAVoCyi_o.png" width="908"></p> 
<p> </p> 
<p></p> 
<p></p> 
<p></p> 
<p>如何去减少这个损失，使我们预测的更加准确些？既然存在了这个损失，我们一直说机器学习有自动学习的功能，在线性回归这里更是能够体现。这里可以通过一些优化方法去优化（其实是数学当中的求导功能）回归的总损失！！！</p> 
<h3 id="2-优化算法">2 优化算法</h3> 
<p><strong>如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值）</strong></p> 
<p></p> 
<p></p> 
<p>​</p> 
<h4 id="21-正规方程">2.1 正规方程</h4> 
<p>2.1.1 什么是正规方程</p> 
<p><img alt="" height="101" src="https://images2.imgbox.com/3b/71/1770Cjjy_o.png" width="528"></p> 
<p> </p> 
<p></p> 
<blockquote> 
 <p>理解：X为特征值矩阵，y为目标值矩阵。直接求到最好的结果</p> 
 <p>缺点：当特征过多过复杂时，求解速度太慢并且得不到结果</p> 
</blockquote> 
<p><img alt="" height="464" src="https://images2.imgbox.com/0d/c9/c2vpp8Fv_o.png" width="896"></p> 
<p> </p> 
<p></p> 
<p>2.1.2 正规方程求解举例</p> 
<p>以下表示数据为例：<img alt="" height="489" src="https://images2.imgbox.com/f6/ba/yyRHwIvM_o.png" width="1110"></p> 
<p> </p> 
<p></p> 
<p>即：</p> 
<p><img alt="" height="266" src="https://images2.imgbox.com/d1/95/BkawoRI1_o.png" width="983"></p> 
<p> </p> 
<p></p> 
<p>运用正规方程方法求解参数：<img alt="" height="178" src="https://images2.imgbox.com/1a/de/Rm8XHt52_o.png" width="1084"></p> 
<p> </p> 
<p></p> 
<p>2.1.3 正规方程的推导</p> 
<p></p> 
<p></p> 
<p>把该损失函数转换成矩阵写法：<img alt="" height="316" src="https://images2.imgbox.com/74/d1/FnOLv2TD_o.png" width="1057"></p> 
<p> </p> 
<p></p> 
<p>其中y是真实值矩阵，X是特征值矩阵，w是权重矩阵</p> 
<p>对其求解关于w的最小值，起止y,X 均已知二次函数直接求导，导数为零的位置，即为最小值。</p> 
<p>求导：<img alt="" height="423" src="https://images2.imgbox.com/1e/29/0bS6btWU_o.png" width="958"></p> 
<p> </p> 
<p></p> 
<p>注：式(1)到式(2)推导过程中, X是一个m行n列的矩阵，并不能保证其有逆矩阵，但是右乘XT把其变成一个方阵，保证其有逆矩阵。</p> 
<p>式（5）到式（6）推导过程中，和上类似。</p> 
<p></p> 
<p></p> 
<p><a href="../ReadingExtension/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E6%8E%A8%E5%AF%BC%E6%96%B9%E5%BC%8F.html" rel="nofollow">正规方程的另一种推导方式</a></p> 
<h4 id="22--梯度下降gradient-descent">2.2 梯度下降(Gradient Descent)</h4> 
<p>2.2.1 什么是梯度下降</p> 
<p>梯度下降法的基本思想可以类比为一个下山的过程。</p> 
<p>假设这样一个场景：</p> 
<p>一个人<strong>被困在山上，需要从山上下来</strong>(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。</p> 
<p>因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。</p> 
<p>具体来说就是，以他当前的所处的位置为基准，<strong>寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走</strong>，（同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走）。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。<img alt="" height="539" src="https://images2.imgbox.com/cc/dc/gofDoLee_o.png" width="1018"></p> 
<p> </p> 
<p></p> 
<p>梯度下降的基本过程就和下山的场景很类似。</p> 
<p>首先，我们有一个<strong>可微分的函数</strong>。这个函数就代表着一座山。</p> 
<p>我们的目标就是找到<strong>这个函数的最小值</strong>，也就是山底。</p> 
<p>根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是<strong>找到给定点的梯度</strong> ，然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就是函数值变化最快的方向。 所以，我们重复利用这个方法，反复求取梯度，最后就能到达局部的最小值，这就类似于我们下山的过程。而求取梯度就确定了最陡峭的方向，也就是场景中测量方向的手段。</p> 
<p>2.2.2 梯度的概念</p> 
<p>梯度是微积分中一个很重要的概念</p> 
<p>​ <strong>在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率；</strong></p> 
<p>​ <strong>在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向；</strong></p> 
<p>这也就说明了为什么我们需要千方百计的求取梯度！我们需要到达山底，就需要在每一步观测到此时最陡峭的地方，梯度就恰巧告诉了我们这个方向。梯度的方向是函数在给定点上升最快的方向，那么梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的。所以我们只要沿着梯度的反方向一直走，就能走到局部的最低点！</p> 
<p>2.2.3 梯度下降举例</p> 
<p></p> 
<p></p> 
<p>我们假设有一个单变量的函数 :J(θ) = θ2</p> 
<p>函数的微分:J、(θ) = 2θ</p> 
<p>初始化，起点为： θ0 = 1</p> 
<p>学习率：α = 0.4</p> 
<p>我们开始进行梯度下降的迭代计算过程:<img alt="" height="559" src="https://images2.imgbox.com/53/c1/PQchEBvr_o.png" width="571"></p> 
<p> </p> 
<p></p> 
<p>如图，经过四次的运算，也就是走了四步，基本就抵达了函数的最低点，也就是山底<img alt="" height="809" src="https://images2.imgbox.com/34/81/2OoxXZvt_o.png" width="826"></p> 
<p> </p> 
<p></p> 
<p></p> 
<p></p> 
<p>我们假设有一个目标函数 ：:J(θ) = θ12 + θ22</p> 
<p>现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下 来，我们会从梯度下降算法开始一步步计算到这个最小值! 我们假设初始的起点为: θ0 = (1, 3)</p> 
<p>初始的学习率为:α = 0.1</p> 
<p>函数的梯度为:▽:J(θ) =&lt; 2θ1 ,2θ2&gt;</p> 
<p>进行多次迭代:<img alt="" height="720" src="https://images2.imgbox.com/7d/d1/sv1XyYoX_o.png" width="792"></p> 
<p> </p> 
<p></p> 
<p>我们发现，已经基本靠近函数的最小值点<img alt="" height="579" src="https://images2.imgbox.com/70/6b/EvXbGAO6_o.png" width="832"></p> 
<p> </p> 
<p></p> 
<p>2.2.4 梯度下降<strong>（</strong>Gradient Descent）公式<img alt="" height="147" src="https://images2.imgbox.com/c1/b5/LI7ooopp_o.png" width="917"></p> 
<p> </p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p>梯度前加一个负号，就意味着朝着梯度相反的方向前进！我们在前文提到，梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号</p> 
<p>我们通过两个图更好理解梯度下降的过程</p> 
<p><img alt="" height="615" src="https://images2.imgbox.com/f9/53/GpcqTSMq_o.png" width="1057"></p> 
<p> <img alt="" height="715" src="https://images2.imgbox.com/54/e7/8m1a3Ufb_o.png" width="1076"></p> 
<p></p> 
<ul><li> <p>线性关系</p> 
  <ul><li><img alt="" height="702" src="https://images2.imgbox.com/e8/ca/u2XxGvy2_o.png" width="517"></li></ul><p> <img alt="" height="176" src="https://images2.imgbox.com/de/d8/WWZqples_o.png" width="329"></p> <p></p> </li><li>导入模块</li><li> <pre><code>from sklearn.linear_model import LinearRegression
</code></pre> </li><li>构造数据集</li><li> <pre><code>x = [[80, 86],
[82, 80],
[85, 78],
[90, 90],
[86, 82],
[82, 90],
[78, 80],
[92, 94]]
y = [84.2, 80.6, 80.1, 90, 83.2, 87.6, 79.4, 93.4]
</code></pre> </li><li>机器学习-- 模型训练</li><li> <pre><code># 实例化API
estimator = LinearRegression()
# 使用fit方法进行训练
estimator.fit(x,y)

estimator.coef_

estimator.predict([[100, 80]])</code></pre> <p></p> <h2 id="24-线性回归的损失和优化">2.4 线性回归的损失和优化</h2> <h3>学习目标</h3> </li><li>知道线性回归中损失函数</li><li>知道使用正规方程对损失函数优化的过程</li><li>知道使用梯度下降法对损失函数优化的过程</li><li>yi为第i个训练样本的真实值</li><li>h(xi)为第i个训练样本特征值组合预测函数</li><li>又称最小二乘法</li><li>线性回归经常使用的两种优化算法 
  <ul><li>正规方程</li><li>梯度下降法</li></ul></li><li><strong>推导方式一：</strong></li><li><strong>推导方式二【拓展】：</strong></li><li><strong>1. 单变量函数的梯度下降**</strong></li><li><strong>2.多变量函数的梯度下降</strong></li><li> <p><strong>1) α是什么含义？</strong></p> <p>α在梯度下降算法中被称作为<strong>学习率</strong>或者<strong>步长</strong>，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大扯着蛋，哈哈，其实就是不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！</p> </li><li> <p><img alt="" height="463" src="https://images2.imgbox.com/6c/7d/dk0lCgRF_o.png" width="1047"></p> <p> </p> </li><li><strong>2) 为什么梯度要乘以一个负号</strong>？</li></ul> 
<h2 id="25-梯度下降法介绍">梯度下降法介绍</h2> 
<p></p> 
<h3 id="4-随机平均梯度下降算法（sag）">随机平均梯度下降算法（SAG）</h3> 
<p>在SG方法中，虽然避开了运算成本大的问题，但对于大数据训练而言，SG效果常不尽如人意，因为每一轮梯度更新都完全与上一轮的数据和梯度无关。</p> 
<p><strong>随机平均梯度算法克服了这个问题，在内存中为每一个样本都维护一个旧的梯度，随机选择第i个样本来更新此样本的梯度，其他样本的梯度保持不变，然后求得所有梯度的平均值，进而更新了参数。</strong></p> 
<p>如此，每一轮更新仅需计算一个样本的梯度，计算成本等同于SG，但收敛速度快得多。</p> 
<p>它们都是为了正确地调节权重向量，通过为每个权重计算一个梯度，从而更新权值，使目标函数尽可能最小化。其差别在于样本的使用方式不同。</p> 
<h3 id="1-全梯度下降算法（fg）">1 全梯度下降算法（FG）</h3> 
<p><strong>计算训练集所有样本误差</strong>，<strong>对其求和再取平均值作为目标函数</strong>。</p> 
<p>权重向量沿其梯度相反的方向移动，从而使当前目标函数减少得最多。</p> 
<p>因为在执行每次更新时，我们需要在整个数据集上计算所有的梯度，所以批梯度下降法的速度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。</p> 
<p><strong>批梯度下降法同样也不能在线更新模型，即在运行的过程中，不能增加新的样本</strong>。</p> 
<p>其是在整个训练数据集上计算损失函数关于参数θ的梯度：</p> 
<p><img alt="" height="117" src="https://images2.imgbox.com/44/53/0Zek7cV4_o.png" width="1028"></p> 
<p> </p> 
<p></p> 
<h3 id="2-随机梯度下降算法（sg）">2 随机梯度下降算法（SG）</h3> 
<p>由于FG每迭代更新一次权重都需要计算所有样本误差，而实际问题中经常有上亿的训练样本，故效率偏低，且容易陷入局部最优解，因此提出了随机梯度下降算法。</p> 
<p>其每轮计算的目标函数不再是全体样本误差，而仅是单个样本误差，即<strong>每次只代入计算一个样本目标函数的梯度来更新权重，再取下一个样本重复此过程，直到损失函数值停止下降或损失函数值小于某个可以容忍的阈值。</strong></p> 
<p>此过程简单，高效，通常可以较好地避免更新迭代收敛到局部最优解。其迭代形式为</p> 
<p><img alt="" height="90" src="https://images2.imgbox.com/07/78/5X6hBi8s_o.png" width="964"></p> 
<p> </p> 
<p></p> 
<blockquote> 
 <p>其中，x(i)表示一条训练样本的特征值，y(i)表示一条训练样本的标签值</p> 
</blockquote> 
<p>但是由于，SG每次只使用一个样本迭代，若遇上噪声则容易陷入局部最优解。</p> 
<h3 id="3-小批量梯度下降算法（minibatch）">3 小批量梯度下降算法（mini-batch）</h3> 
<p>小批量梯度下降算法是FG和SG的折中方案,在一定程度上兼顾了以上两种方法的优点。</p> 
<p><strong>每次从训练样本集上随机抽取一个小样本集，在抽出来的小样本集上采用FG迭代更新权重。</strong></p> 
<p>被抽出的小样本集所含样本点的个数称为batch_size，通常设置为2的幂次方，更有利于GPU加速处理。</p> 
<p>特别的，若batch_size=1，则变成了SG；若batch_size=n，则变成了FG.其迭代形式为</p> 
<p><img alt="" height="120" src="https://images2.imgbox.com/e6/7c/TsJWZkPr_o.png" width="935"></p> 
<p> </p> 
<p></p> 
<p> </p> 
<p></p> 
<p><strong>所以有了梯度下降这样一个优化算法，回归就有了"自动学习"的能力</strong></p> 
<p></p> 
<p></p> 
<p></p> 
<h3 id="3-梯度下降和正规方程的对比">3 梯度下降和正规方程的对比</h3> 
<table><thead><tr><th>梯度下降</th><th>正规方程</th></tr></thead><tbody><tr><td>需要选择学习率</td><td>不需要</td></tr><tr><td>需要迭代求解</td><td>一次运算得出</td></tr><tr><td>特征数量较大可以使用</td><td>需要计算方程，时间复杂度高O(n3)</td></tr></tbody></table> 
<h4 id="31-算法选择依据：">3.1 算法选择依据：</h4> 
<p></p> 
<ul><li>小规模数据： 
  <ul><li>正规方程：<strong>LinearRegression(不能解决拟合问题)</strong></li><li>岭回归</li></ul></li><li>大规模数据： 
  <ul><li>梯度下降法：<strong>SGDRegressor</strong></li></ul></li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6a7feb0e71a4348490e83bd87b17ca19/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">JDBC 技术 | Java连接MySQL数据库（四万字零基础保姆级超全详解）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a758942c762eba51f92671dede0d75ae/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">SQL SERVER专题实验4 复杂查询</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>