<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【AIGC】本地部署 ollama &#43; open-webui - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/c31d1a08fc9880bb156395213b63ce5e/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【AIGC】本地部署 ollama &#43; open-webui">
  <meta property="og:description" content="在之前的篇章《【AIGC】本地部署 ollama(gguf) 与项目整合》中我们已经使用 ollama 部署了一个基于预量化（gguf）的 Qwen1.5 模型，这个模型除了提供研发使用外，我还想提供给公司内部使用，因此还需要一个 ui 交互界面。
显然，这对于我们开发人员来说不是什么难事（毕竟 ollama 已经提供了相对比较完善的 API 接口了），但都 2024 年了与其自己开发还不如先找个开箱即用的…你看，这不已经有大神开发出 open-webui 了吗，我们开箱即用即可。
本文将记录部署过程中遇到的问题以及解决方式，希望对你有所帮助（open-webui 采用 docker 进行部署）。
1. 无法访问 huggingface 官网问题 由于 open-webui 有提供模型下载功能，因此需要在 docker 启动命令中添加 HF_ENDPOINT 环境变量：
sudo docker run -d \ ... -e HF_ENDPOINT=https://hf-mirror.com \ ... ghcr.io/open-webui/open-webui:main HF_ENDPOINT 将指向国内 hf-mirror 镜像站。
2. docker host 模式（ollama 宿主机直接部署的话） open-webui 容器采用 host 模式是最方便的的做法。由于我的 ollama 是宿主机直接部署的，open-webui 容器host 模式能够直接通过 127.0.0.1 进行通讯。如果你的 ollama 是 docker 容器，那么你可以将 open-webui 部署在与 ollama 同一个网络中，然后通过桥接只公开 open-webui 访问。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-04T10:04:32+08:00">
    <meta property="article:modified_time" content="2024-05-04T10:04:32+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【AIGC】本地部署 ollama &#43; open-webui</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>在之前的篇章<a href="https://blog.csdn.net/kida_yuan/article/details/138334646?spm=1001.2014.3001.5501">《【AIGC】本地部署 ollama(gguf) 与项目整合》</a>中我们已经使用 ollama 部署了一个基于预量化（gguf）的 Qwen1.5 模型，这个模型除了提供研发使用外，我还想提供给公司内部使用，因此还需要一个 ui 交互界面。</p> 
<p>显然，这对于我们开发人员来说不是什么难事（毕竟 ollama 已经提供了相对比较完善的 API 接口了），但都 2024 年了与其自己开发还不如先找个开箱即用的…你看，这不已经有大神开发出 open-webui 了吗，我们开箱即用即可。</p> 
<p>本文将记录部署过程中遇到的问题以及解决方式，希望对你有所帮助（open-webui 采用 docker 进行部署）。</p> 
<h5><a id="1__huggingface__5"></a>1. 无法访问 huggingface 官网问题</h5> 
<p>由于 open-webui 有提供模型下载功能，因此需要在 docker 启动命令中添加 HF_ENDPOINT 环境变量：</p> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> <span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token punctuation">\</span>
   <span class="token punctuation">..</span>.
   <span class="token parameter variable">-e</span> <span class="token assign-left variable">HF_ENDPOINT</span><span class="token operator">=</span>https://hf-mirror.com <span class="token punctuation">\</span>
   <span class="token punctuation">..</span>.
   ghcr.io/open-webui/open-webui:main
</code></pre> 
<p>HF_ENDPOINT 将指向国内 hf-mirror 镜像站。</p> 
<h5><a id="2_docker_host_ollama__15"></a>2. docker host 模式（ollama 宿主机直接部署的话）</h5> 
<p>open-webui 容器采用 host 模式是最方便的的做法。由于我的 ollama 是宿主机直接部署的，open-webui 容器host 模式能够直接通过 127.0.0.1 进行通讯。如果你的 ollama 是 docker 容器，那么你可以将 open-webui 部署在与 ollama 同一个网络中，然后通过桥接只公开 open-webui 访问。</p> 
<p>在启动命令中还需要设置环境变量 OLLAMA_BASE_URL 来指定 ollama 的访问地址：</p> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> <span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token punctuation">\</span>
   <span class="token parameter variable">--network</span><span class="token operator">=</span>host <span class="token punctuation">\</span>
   <span class="token parameter variable">-e</span> <span class="token assign-left variable">HF_ENDPOINT</span><span class="token operator">=</span>https://hf-mirror.com <span class="token punctuation">\</span>
   <span class="token parameter variable">-e</span> <span class="token assign-left variable">OLLAMA_BASE_URL</span><span class="token operator">=</span>http://127.0.0.1:11434 <span class="token punctuation">\</span>
   <span class="token punctuation">..</span>.
   ghcr.io/open-webui/open-webui:main
</code></pre> 
<h5><a id="3__27"></a>3. 关于镜像拉取</h5> 
<p>有人说 ghcr.io/open-webui/open-webui 镜像拉取很慢需要通过 <a href="https://dockerproxy.com/" rel="nofollow">Docker Proxy 镜像加速</a> 进行转换。这个… 我没有这种感受。各人看自己的情况吧，若感觉到慢的话还是先转换一下。</p> 
<h5><a id="4__29"></a>4. 数据挂载</h5> 
<p>open-webui 是需要注册使用的。注册数据会保存到容器内部，因此在 docker 部署时还需要挂载数据目录到宿主机以免发生误删容器导致数据丢失的情况。</p> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> <span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token punctuation">\</span>
   <span class="token parameter variable">--network</span><span class="token operator">=</span>host <span class="token punctuation">\</span>
   <span class="token parameter variable">-v</span> /home/ubuntu/Documents/open-webui/data:/app/backend/data <span class="token punctuation">\</span>
   <span class="token parameter variable">-e</span> <span class="token assign-left variable">HF_ENDPOINT</span><span class="token operator">=</span>https://hf-mirror.com <span class="token punctuation">\</span>
   <span class="token punctuation">..</span>.
   ghcr.io/open-webui/open-webui:main
</code></pre> 
<h5><a id="5__39"></a>5. 其他配置项</h5> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> <span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--network</span><span class="token operator">=</span>host <span class="token punctuation">\</span>
  <span class="token parameter variable">-v</span> /home/ubuntu/Documents/open-webui/data:/app/backend/data <span class="token punctuation">\</span>
  <span class="token parameter variable">-e</span> <span class="token assign-left variable">HF_ENDPOINT</span><span class="token operator">=</span>https://hf-mirror.com <span class="token punctuation">\</span>
  <span class="token parameter variable">-e</span> <span class="token assign-left variable">OLLAMA_BASE_URL</span><span class="token operator">=</span>http://127.0.0.1:11434 <span class="token punctuation">\</span>
  <span class="token parameter variable">-e</span> <span class="token assign-left variable">DEFAULT_USER_ROLE</span><span class="token operator">=</span>user <span class="token punctuation">\</span>
  <span class="token parameter variable">-e</span> <span class="token assign-left variable">DEFAULT_MODELS</span><span class="token operator">=</span>qwen1 <span class="token number">5</span>-14b <span class="token punctuation">\</span>
  <span class="token parameter variable">-e</span> <span class="token assign-left variable">ENABLE_IMAGE_GENERATION</span><span class="token operator">=</span>True <span class="token punctuation">\</span>
  <span class="token parameter variable">--name</span> open-webui <span class="token punctuation">\</span>
  <span class="token parameter variable">--restart</span> always <span class="token punctuation">\</span>
  ghcr.io/open-webui/open-webui:main
</code></pre> 
<ul><li>DEFAULT_USER_ROLE：默认用户类型</li><li>DEFAULT_MODELS：默认模型</li><li>ENABLE_IMAGE_GENERATION：是否能够生成图片（使用 sd 模型时需要建议开启）</li></ul> 
<p>配置完以上信息后就可以启动容器了。这时我们通过日志可以看到 open-webui 在创建 webui_secret_key 文件</p> 
<pre><code class="prism language-bash">No WEBUI_SECRET_KEY provided
Generating WEBUI_SECRET_KEY
Loading WEBUI_SECRET_KEY from .webui_secret_key
<span class="token punctuation">..</span>.
INFO:     Started server process <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
INFO:     Waiting <span class="token keyword">for</span> application startup.
</code></pre> 
<p>在所有文件都创建后才会真正启动</p> 
<pre><code class="prism language-bash">
  ___                    __        __   _     _   _ ___ 
 / _ <span class="token punctuation">\</span> _ __   ___ _ __   <span class="token punctuation">\</span> <span class="token punctuation">\</span>      / /__<span class="token operator">|</span> <span class="token operator">|</span>__ <span class="token operator">|</span> <span class="token operator">|</span> <span class="token operator">|</span> <span class="token operator">|</span>_ _<span class="token operator">|</span>
<span class="token operator">|</span> <span class="token operator">|</span> <span class="token operator">|</span> <span class="token operator">|</span> <span class="token string">'_ \ / _ \ '</span>_ <span class="token punctuation">\</span>   <span class="token punctuation">\</span> <span class="token punctuation">\</span> /<span class="token punctuation">\</span> / / _ <span class="token punctuation">\</span> <span class="token string">'_ \| | | || | 
| |_| | |_) |  __/ | | |   \ V  V /  __/ |_) | |_| || | 
 \___/| .__/ \___|_| |_|    \_/\_/ \___|_.__/ \___/|___|
      |_|                                               

      
v0.1.122 - building the best open-source AI user interface.      
https://github.com/open-webui/open-webui

INFO:apps.litellm.main:start_litellm_background
INFO:apps.litellm.main:run_background_process
INFO:apps.litellm.main:Executing command: ['</span>litellm<span class="token string">', '</span>--port<span class="token string">', '</span><span class="token number">14365</span><span class="token string">', '</span>--host<span class="token string">', '</span><span class="token number">127.0</span>.0.1<span class="token string">', '</span>--telemetry<span class="token string">', '</span>False<span class="token string">', '</span>--config<span class="token string">', '</span>/app/backend/data/litellm/config.yaml'<span class="token punctuation">]</span>
INFO:     Application startup complete.
INFO:apps.litellm.main:Subprocess started successfully.
</code></pre> 
<p>这时可通过浏览器输入 http://127.0.0.1:8080 进行访问。</p> 
<h5><a id="6_Windows__91"></a>6. Windows 无法连接问题</h5> 
<p>若 ollama 是部署在 Windows，且 open-webui 是部署在远程的机器上的话会发现 open-webui 无法远程访问 ollama 。这时 ollama 虽然已在 Windows 上正常运行，但你还需要配置一下环境变量。如下图：</p> 
<p><img src="https://images2.imgbox.com/de/fa/ZCWfPBGv_o.png" alt="image.png"></p> 
<p>这样 ollama 才能够公开访问。<br> 最后，访问 http://127.0.0.1:8080 ，在完成注册后可以看到以下页面（与 ChatGPT 一个样）：</p> 
<p><img src="https://images2.imgbox.com/b0/ff/nOyNBrHR_o.png" alt="image.png"></p> 
<p>左上角可以选择 ollama 现存模型。但是在你真正使用之前我还建议你先做下面的一步操作：</p> 
<p><img src="https://images2.imgbox.com/78/2e/jvcjyQhm_o.png" alt="image.png"></p> 
<p>打开“设置”对话框选择“通用”，再选择高级参数“显示”之后找到“保持活动”：</p> 
<p><img src="https://images2.imgbox.com/33/b7/cGp8TzAb_o.png" alt="image.png"></p> 
<p>这里填写一个较大的数。因为 gguf 预量化模型的本质是需要将模型加载到内存来使用的，如果“保持活动”参数过小（默认 5 分钟），模型就会在规定的时间后进行释放，需要用到的时候又要重新加载，而这个加载过程将会非常缓慢（相对于使用 GPU 模式来说），因此这里我设置了 24 小时，先保证它能够在办公时间内能够快速响应。</p> 
<p>之后就能愉快地与人工智能玩耍了，enjoy！</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/417c53ea3738e4b6d3c3485329f30393/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">大数据ELK（二）：Elasticsearch简单介绍_elk是什么时候开始流行的</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1ac0f083485d16f192ccea2208fca7a2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">几个好用的AI网站（视频/图片/论文/PPT生成）直接给链接</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>