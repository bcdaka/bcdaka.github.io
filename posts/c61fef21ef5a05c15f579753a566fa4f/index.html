<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LLaMa、Qwen、ChatGLM、ChatGLM2的区别 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/c61fef21ef5a05c15f579753a566fa4f/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="LLaMa、Qwen、ChatGLM、ChatGLM2的区别">
  <meta property="og:description" content="LLaMa、Qwen、ChatGLM、ChatGLM2的区别 以下比较的前提是首先和BERT(transfomer)的对比
PS: 大模型基础和进阶付费课程（自己讲的）：《AIGC大模型理论与工业落地实战》-CSDN学院 或者《AIGC大模型理论与工业落地实战》-网易云课堂
LLaMa： 去掉bias LayNorm方式：RMSnorm：https://zhuanlan.zhihu.com/p/650231190 # torch自带LayerNorm if self.norm_mode == &#39;torch_buildin&#39;: return F.layer_norm(hidden_states, self.normalized_shape, self.weight, self.bias, self.eps) # RMSnorm: t5、大模型系列均使用 elif self.norm_mode == &#39;rmsnorm&#39;: variance = hidden_states.float().pow(2).mean(-1, keepdim=True) o = (hidden_states.float() * torch.rsqrt(variance &#43; self.eps)).type_as(hidden_states) torch自带LayerNorm (F.layer_norm)： 这是PyTorch库中内置的Layer Normalization实现。输入参数包括：hidden_states（需要归一化的张量）、normalized_shape（归一化的维度，通常是最后一维）、weight和bias（可学习的缩放和平移参数）以及eps（为了数值稳定性添加的小常数）。它首先计算输入在指定维度上的均值和方差，然后使用这些统计量对输入进行归一化，并通过应用可学习的缩放和平移参数来恢复模型的表达能力。 RMSNorm (Root Mean Square Normalization)： 只计算输入的方差（即每个元素的平方的平均值），然后通过元素级操作计算归一化后的输出。具体步骤如下： 计算输入的平方的平均值（variance）。使用逆平方根（torch.rsqrt()）来计算方差的倒数（相当于标准差的倒数）。将输入与计算出的标准差倒数相乘，得到归一化的结果。 torch自带的LayerNorm是最完整的实现，包括了可学习的参数；而RMSNorm和自定义LayerNorm则省略了这些参数，可能会牺牲一些模型的表达能力，但在某些情况下可能更简单或更高效。RMSNorm特别适用于那些不需要额外参数的大规模模型。
feedForward不同, 三层全连接 # 普通bert的FFN： self.outDense(self.inter_act_fn(self.interDense(x))) # llama、qwen的FFN： self.outDense(self.inter_act_fn(self.interDense(x)) * self.interDense2(x)) 新增rotary相对位置编码(RoPE) InternLM： 模型结构: 基本和llama基本一致, 只是各个linear层多了bias; 和Qwen基本一致, 除了o有bias
FeedForward和Llama一致, 三个dense层 除了qkvo有bias, 其余均没有bias Qwen： FeedForward和Llama一致, 三个dense层 除了qkv有bias, 其余均没有bias 和InternLM基本一致, 唯一的差别是InternLM的multiHeadAttention.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-03-18T14:17:02+08:00">
    <meta property="article:modified_time" content="2024-03-18T14:17:02+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">LLaMa、Qwen、ChatGLM、ChatGLM2的区别</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="LLaMaQwenChatGLMChatGLM2_0"></a>LLaMa、Qwen、ChatGLM、ChatGLM2的区别</h3> 
<p>以下比较的前提是首先和BERT(transfomer)的对比</p> 
<p>PS: 大模型基础和进阶付费课程（自己讲的）：<a href="https://edu.csdn.net/course/detail/39082">《AIGC大模型理论与工业落地实战》-CSDN学院</a> 或者<a href="https://study.163.com/course/introduction/1213683819.htm?inLoc=ss_ssjg_qblb_%E5%A4%A7%E6%A8%A1%E5%9E%8B" rel="nofollow">《AIGC大模型理论与工业落地实战》-网易云课堂</a></p> 
<h4><a id="LLaMa_6"></a>LLaMa：</h4> 
<ol><li> <h5><a id="bias_8"></a>去掉bias</h5> </li><li> <h5><a id="LayNormRMSnormhttpszhuanlanzhihucomp650231190_10"></a>LayNorm方式：RMSnorm：https://zhuanlan.zhihu.com/p/650231190</h5> <pre><code class="prism language-python"><span class="token comment"># torch自带LayerNorm</span>
<span class="token keyword">if</span> self<span class="token punctuation">.</span>norm_mode <span class="token operator">==</span> <span class="token string">'torch_buildin'</span><span class="token punctuation">:</span>
	<span class="token keyword">return</span> F<span class="token punctuation">.</span>layer_norm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span> self<span class="token punctuation">.</span>normalized_shape<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span>
<span class="token comment"># RMSnorm: t5、大模型系列均使用</span>
<span class="token keyword">elif</span> self<span class="token punctuation">.</span>norm_mode <span class="token operator">==</span> <span class="token string">'rmsnorm'</span><span class="token punctuation">:</span>
	variance <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
	o <span class="token operator">=</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>rsqrt<span class="token punctuation">(</span>variance <span class="token operator">+</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
</code></pre> 
  <ol><li><strong>torch自带LayerNorm (<code>F.layer_norm</code>)</strong>： 
    <ul><li>这是PyTorch库中内置的Layer Normalization实现。</li><li>输入参数包括：<code>hidden_states</code>（需要归一化的张量）、<code>normalized_shape</code>（归一化的维度，通常是最后一维）、<code>weight</code>和<code>bias</code>（可学习的缩放和平移参数）以及<code>eps</code>（为了数值稳定性添加的小常数）。</li><li>它首先计算输入在指定维度上的均值和方差，然后使用这些统计量对输入进行归一化，并通过应用可学习的缩放和平移参数来恢复模型的表达能力。</li></ul> </li><li><strong>RMSNorm (Root Mean Square Normalization)</strong>： 
    <ul><li>只计算输入的方差（即每个元素的平方的平均值），然后通过元素级操作计算归一化后的输出。</li><li>具体步骤如下： 
      <ul><li>计算输入的平方的平均值（variance）。</li><li>使用逆平方根（<code>torch.rsqrt()</code>）来计算方差的倒数（相当于标准差的倒数）。</li><li>将输入与计算出的标准差倒数相乘，得到归一化的结果。</li></ul> </li></ul> </li></ol> <p>torch自带的LayerNorm是最完整的实现，包括了可学习的参数；而RMSNorm和自定义LayerNorm则省略了这些参数，可能会牺牲一些模型的表达能力，但在某些情况下可能更简单或更高效。RMSNorm特别适用于那些不需要额外参数的大规模模型。</p> </li><li> <h5><a id="feedForward__35"></a>feedForward不同, 三层全连接</h5> <pre><code class="prism language-python"><span class="token comment"># 普通bert的FFN：</span>
self<span class="token punctuation">.</span>outDense<span class="token punctuation">(</span>self<span class="token punctuation">.</span>inter_act_fn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>interDense<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># llama、qwen的FFN：</span>
self<span class="token punctuation">.</span>outDense<span class="token punctuation">(</span>self<span class="token punctuation">.</span>inter_act_fn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>interDense<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>interDense2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> </li><li> <h5><a id="rotaryRoPE_44"></a>新增rotary相对位置编码(RoPE)</h5> </li></ol> 
<h3><a id="InternLM_48"></a>InternLM：</h3> 
<p>模型结构: 基本和llama基本一致, 只是各个linear层多了bias; 和Qwen基本一致, 除了o有bias</p> 
<ol><li> <h5><a id="FeedForwardLlama_dense_51"></a>FeedForward和Llama一致, 三个dense层</h5> </li><li> <h5><a id="qkvobias_bias_52"></a>除了qkvo有bias, 其余均没有bias</h5> </li></ol> 
<h3><a id="Qwen_56"></a>Qwen：</h3> 
<ol><li> <h5><a id="FeedForwardLlama_dense_58"></a>FeedForward和Llama一致, 三个dense层</h5> </li><li> <h5><a id="qkvbias_bias_60"></a>除了qkv有bias, 其余均没有bias</h5> </li><li> <h5><a id="InternLM_InternLMmultiHeadAttentionobias_62"></a>和InternLM基本一致, 唯一的差别是InternLM的multiHeadAttention.o有bias</h5> </li></ol> 
<h3><a id="ChatGLM_66"></a>ChatGLM</h3> 
<ol><li> <h5><a id="rotaryRoPEupdownposition_encoding_2d_69"></a>rotary(RoPE)使用的updown+position_encoding_2d</h5> <p>这是ROPE的二维表达方式，普通的ROPE是一维</p> </li><li> <h5><a id="qkvconvertconcat_73"></a>qkv合并成一个权重convert时不是concat在一起的</h5> </li><li> <h5><a id="attention_masktoken_tokens_75"></a>attention_mask的最后一个token仅能访问之前的, 之前的tokens可以互相访问</h5> </li><li> <h5><a id="embeddinglayernorm_77"></a>跳跃连接有权重设计；embedding之后没有layernorm</h5> </li></ol> 
<pre><code class="prism language-python"><span class="token comment"># 原始bert，LayerNorm + multiHeadAttention + dropout + FFN + dropout + x+ FFN：其中x来自第一次FFN之后</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>attnLayerNorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span> conditional_emb<span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>pre_layernorm <span class="token keyword">else</span> hidden_states  <span class="token comment"># pre/post layernorm</span>
        self_attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>multiHeadAttention<span class="token punctuation">(</span>x<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> past_key_value<span class="token operator">=</span>past_key_value<span class="token punctuation">,</span> position_ids<span class="token operator">=</span>position_ids<span class="token punctuation">)</span> 
        residual <span class="token operator">=</span> x <span class="token keyword">if</span> self<span class="token punctuation">.</span>apply_residual_post_layernorm <span class="token keyword">else</span> hidden_states
        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout_add<span class="token punctuation">(</span>self_attn_output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> residual<span class="token punctuation">)</span>
        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>attnLayerNorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span> conditional_emb<span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>pre_layernorm <span class="token keyword">else</span> hidden_states
        
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>ffnLayerNorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span> conditional_emb<span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>pre_layernorm <span class="token keyword">else</span> hidden_states  <span class="token comment"># pre/post layernorm</span>
        feedforward_output <span class="token operator">=</span> self<span class="token punctuation">.</span>feedForward<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        residual <span class="token operator">=</span> x <span class="token keyword">if</span> self<span class="token punctuation">.</span>apply_residual_post_layernorm <span class="token keyword">else</span> hidden_states
        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout_add<span class="token punctuation">(</span>feedforward_output<span class="token punctuation">,</span> residual<span class="token punctuation">)</span>   <span class="token comment"># x在这</span>
        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>ffnLayerNorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span> conditional_emb<span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>pre_layernorm <span class="token keyword">else</span> hidden_states
        
<span class="token comment"># GhatGLM：LayerNorm + multiHeadAttention + alpha*x + FFN + alpha*x：多了一个alpha，其中x来自一开始的LayerNorm之后</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>attnLayerNorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
        alpha <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>num_hidden_layers<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">0.5</span>
        self_attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>multiHeadAttention<span class="token punctuation">(</span>x<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> past_key_value<span class="token operator">=</span>past_key_value<span class="token punctuation">,</span> <span class="token operator">**</span>model_kwargs<span class="token punctuation">)</span>
        hidden_states <span class="token operator">=</span> x <span class="token operator">*</span> alpha <span class="token operator">+</span> self_attn_output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>ffnLayerNorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
        hidden_states <span class="token operator">=</span> x <span class="token operator">*</span>alpha <span class="token operator">+</span>  self<span class="token punctuation">.</span>feedForward<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="ChatGLM2_102"></a>ChatGLM2</h3> 
<ol><li> <h5><a id="Unilmmask_104"></a>不使用Unilm式的mask</h5> 
  <ol start="2"><li> <h5><a id="flash_attention_106"></a>flash_attention</h5> <p>这段代码定义了一个名为<code>flash_attention_forward</code>的函数，用于实现Flash Attention机制。以下是该函数的详细解释：</p> 
    <ol><li> <p>函数接受以下参数：</p> 
      <ul><li><code>query_layer</code>：查询（query）层的输出。</li><li><code>key_layer</code>：键（key）层的输出。</li><li><code>value_layer</code>：值（value）层的输出。</li><li><code>attention_mask</code>：注意力掩码，用于指示哪些位置应该被忽略。</li><li><code>query_length</code>：查询序列的长度。</li><li><code>softmax_scale</code>：可选的softmax缩放因子。</li></ul> </li><li> <p><code>_get_unpad_data</code>函数用于处理未填充的数据。它计算每个批次中的序列长度、累积和最大序列长度，并返回未填充数据的索引、累积序列长度和最大序列长度。</p> </li><li> <p><code>_upad_input</code>函数负责对输入进行解压和平铺操作。它根据<code>attention_mask</code>计算出需要保留的序列部分，并将查询、键和值层的输出调整为未填充的形状。</p> </li><li> <p><code>dropout</code>变量用于控制dropout概率，如果模型处于训练状态，则使用<code>self.attention_probs_dropout_prob</code>作为概率。</p> </li><li> <p>将查询、键和值层的输出进行转置，以便于后续的矩阵运算。</p> </li><li> <p>根据是否为因果自注意力（即是否为序列到序列任务）和注意力掩码的形状，选择不同的计算方法：</p> 
      <ul><li>如果不是因果自注意力且注意力掩码为二维（表示仅包含键填充掩码），则使用Flash Attention进行计算。首先调用<code>_upad_input</code>函数对输入进行解压和平铺，然后调用<code>flash_attn_varlen_func</code>函数执行Flash Attention操作，并在计算完成后将结果恢复为原始形状。</li><li>如果不是因果自注意力但注意力掩码不符合要求，则发出警告并使用PyTorch内置的注意力计算方法。</li><li>如果是因果自注意力，则直接调用<code>flash_attn_func</code>函数执行Flash Attention操作。</li></ul> </li><li> <p>最后，将注意力输出转置回原来的形状并返回。</p> </li></ol> <p>总的来说，这段代码实现了Flash Attention机制，通过矩阵分解和局部注意力等技术优化了Transformer模型的计算效率和内存使用效率。在处理非因果自注意力任务时，它支持仅包含键填充掩码的情况，并在其他情况下退回到使用PyTorch内置的注意力计算方法。</p> </li><li> <h5><a id="multi_query_attention_135"></a>multi_query_attention</h5> </li></ol> </li></ol> 
<ul><li>"multi_query_attention"是一种创新的注意力机制设计，它扩展了传统Transformer中的单查询注意力机制。</li><li>在Multi-Query Attention中，每个位置的输入可能会生成多个查询向量，这些查询向量可以独立地参与注意力计算，并与键（key）和值（value）矩阵进行交互。</li><li>多个查询向量可以捕捉到输入的不同方面或信息源，增强模型的理解和表达能力。例如，一个查询可能关注词汇级别的信息，另一个查询可能关注句法或语义级别的信息。</li><li>具体实现可能包括对输入向量进行分解、复制或学习多个查询权重等技术。生成的多个查询向量将分别用于计算注意力分数，并与相应的值向量进行加权求和，得到最终的注意力输出。</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2f4402552f4ebfb7864386fee0bf7e97/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【功能超全】基于OpenCV车牌识别停车场管理系统软件开发【含python源码&#43;PyqtUI界面&#43;功能详解】-车牌识别python 深度学习实战项目</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/10d93f7654e250ac951157cad27309d6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python教学 | 有备无患！详解 Python 异常处理（try-except）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>