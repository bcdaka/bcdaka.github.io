<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ChatGLM-6B (介绍以及本地部署) - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/ff093bb8c31600446a9fbf82ffdf495e/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="ChatGLM-6B (介绍以及本地部署)">
  <meta property="og:description" content="中文ChatGPT平替——ChatGLM-6B ChatGLM-6B简介官方实例本地部署1.下载代码2.通过conda创建虚拟环境3.修改代码4.模型量化5.详细代码 调用示例 ChatGLM-6B 简介 ChatGLM-6B 是一个开源的、支持中英双语问答的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。ChatGLM-6B 使用了和 ChatGLM 相同的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。
ChatGLM 参考了 ChatGPT 的设计思路，在千亿基座模型 GLM-130B1 中注入了代码预训练，通过有监督微调（Supervised Fine-Tuning）等技术实现人类意图对齐。ChatGLM 当前版本模型的能力提升主要来源于独特的千亿基座模型 GLM-130B。它是不同于 BERT、GPT-3 以及 T5 的架构，是一个包含多目标函数的自回归预训练模型。2022年8月，我们向研究界和工业界开放了拥有1300亿参数的中英双语稠密模型 GLM-130B1，该模型有一些独特的优势：
双语： 同时支持中文和英文。高精度（英文）： 在公开的英文自然语言榜单 LAMBADA、MMLU 和 Big-bench-lite 上优于 GPT-3 175B（API: davinci，基座模型）、OPT-175B 和 BLOOM-176B。高精度（中文）： 在7个零样本 CLUE 数据集和5个零样本 FewCLUE 数据集上明显优于 ERNIE TITAN 3.0 260B 和 YUAN 1.0-245B。快速推理： 首个实现 INT4 量化的千亿模型，支持用一台 4 卡 3090 或 8 卡 2080Ti 服务器进行快速且基本无损推理。可复现性： 所有结果（超过 30 个任务）均可通过我们的开源代码和模型参数复现。跨平台： 支持在国产的海光 DCU、华为昇腾 910 和申威处理器及美国的英伟达芯片上进行训练与推理。 官方实例 &gt;&gt;&gt; from transformers import AutoTokenizer, AutoModel &gt;&gt;&gt; tokenizer = AutoTokenizer.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-03-17T17:43:53+08:00">
    <meta property="article:modified_time" content="2023-03-17T17:43:53+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ChatGLM-6B (介绍以及本地部署)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>中文ChatGPT平替——ChatGLM-6B</h4> 
 <ul><li><a href="#ChatGLM6B_2" rel="nofollow">ChatGLM-6B</a></li><li><ul><li><a href="#_4" rel="nofollow">简介</a></li><li><a href="#_23" rel="nofollow">官方实例</a></li><li><a href="#_47" rel="nofollow">本地部署</a></li><li><ul><li><a href="#1_49" rel="nofollow">1.下载代码</a></li><li><a href="#2conda_55" rel="nofollow">2.通过conda创建虚拟环境</a></li><li><a href="#3_70" rel="nofollow">3.修改代码</a></li><li><a href="#4_78" rel="nofollow">4.模型量化</a></li><li><a href="#5_111" rel="nofollow">5.详细代码</a></li></ul> 
   </li><li><a href="#_158" rel="nofollow">调用示例</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="ChatGLM6B_2"></a>ChatGLM-6B</h2> 
<h3><a id="_4"></a>简介</h3> 
<blockquote> 
 <p>ChatGLM-6B 是一个开源的、支持中英双语问答的对话语言模型，基于 <a href="https://github.com/THUDM/GLM">General Language Model (GLM)</a> 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。ChatGLM-6B 使用了和 <a href="https://chatglm.cn/" rel="nofollow">ChatGLM</a> 相同的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。</p> 
</blockquote> 
<blockquote> 
 <p>ChatGLM 参考了 ChatGPT 的设计思路，在千亿基座模型 GLM-130B<a href="https://chatglm.cn/blog#fn:1" rel="nofollow">1</a> 中注入了代码预训练，通过有监督微调（Supervised Fine-Tuning）等技术实现人类意图对齐。ChatGLM 当前版本模型的能力提升主要来源于独特的千亿基座模型 GLM-130B。它是不同于 BERT、GPT-3 以及 T5 的架构，是一个包含多目标函数的自回归预训练模型。2022年8月，我们向研究界和工业界开放了拥有1300亿参数的中英双语稠密模型 GLM-130B<a href="https://chatglm.cn/blog#fn:1" rel="nofollow">1</a>，该模型有一些独特的优势：</p> 
 <ul><li>双语： 同时支持中文和英文。</li><li>高精度（英文）： 在公开的英文自然语言榜单 LAMBADA、MMLU 和 Big-bench-lite 上优于 GPT-3 175B（API: davinci，基座模型）、OPT-175B 和 BLOOM-176B。</li><li>高精度（中文）： 在7个零样本 CLUE 数据集和5个零样本 FewCLUE 数据集上明显优于 ERNIE TITAN 3.0 260B 和 YUAN 1.0-245B。</li><li>快速推理： 首个实现 INT4 量化的千亿模型，支持用一台 4 卡 3090 或 8 卡 2080Ti 服务器进行快速且基本无损推理。</li><li>可复现性： 所有结果（超过 30 个任务）均可通过我们的开源代码和模型参数复现。</li><li>跨平台： 支持在国产的海光 DCU、华为昇腾 910 和申威处理器及美国的英伟达芯片上进行训练与推理。</li></ul> 
</blockquote> 
<p><img src="https://images2.imgbox.com/d9/b3/tR6yrlxz_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/8b/ad/eN0X1WWa_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_23"></a>官方实例</h3> 
<pre><code>&gt;&gt;&gt; from transformers import AutoTokenizer, AutoModel
&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
&gt;&gt;&gt; model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
&gt;&gt;&gt; response, history = model.chat(tokenizer, "你好", history=[])
&gt;&gt;&gt; print(response)
你好👋!我是人工智能助手 ChatGLM-6B,很高兴见到你,欢迎问我任何问题。
&gt;&gt;&gt; response, history = model.chat(tokenizer, "晚上睡不着应该怎么办", history=history)
&gt;&gt;&gt; print(response)
晚上睡不着可能会让你感到焦虑或不舒服,但以下是一些可以帮助你入睡的方法:

1. 制定规律的睡眠时间表:保持规律的睡眠时间表可以帮助你建立健康的睡眠习惯,使你更容易入睡。尽量在每天的相同时间上床,并在同一时间起床。
2. 创造一个舒适的睡眠环境:确保睡眠环境舒适,安静,黑暗且温度适宜。可以使用舒适的床上用品,并保持房间通风。
3. 放松身心:在睡前做些放松的活动,例如泡个热水澡,听些轻柔的音乐,阅读一些有趣的书籍等,有助于缓解紧张和焦虑,使你更容易入睡。
4. 避免饮用含有咖啡因的饮料:咖啡因是一种刺激性物质,会影响你的睡眠质量。尽量避免在睡前饮用含有咖啡因的饮料,例如咖啡,茶和可乐。
5. 避免在床上做与睡眠无关的事情:在床上做些与睡眠无关的事情,例如看电影,玩游戏或工作等,可能会干扰你的睡眠。
6. 尝试呼吸技巧:深呼吸是一种放松技巧,可以帮助你缓解紧张和焦虑,使你更容易入睡。试着慢慢吸气,保持几秒钟,然后缓慢呼气。

如果这些方法无法帮助你入睡,你可以考虑咨询医生或睡眠专家,寻求进一步的建议。

</code></pre> 
<h3><a id="_47"></a>本地部署</h3> 
<h4><a id="1_49"></a>1.下载代码</h4> 
<pre><code>git clone https://github.com/THUDM/ChatGLM-6B.git
</code></pre> 
<h4><a id="2conda_55"></a>2.通过conda创建虚拟环境</h4> 
<pre><code># 新建chatglm环境
conda create -n chatglm python=3.8
# 激活chatglm环境
conda activate chatglm
# 安装PyTorch环境（根据自己的cuda版本选择合适的torch版本）
pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113
# 安装gradio用于启动图形化web界面
pip install gradio
# 安装运行依赖
pip install -r requirement.txt
</code></pre> 
<h4><a id="3_70"></a>3.修改代码</h4> 
<ul><li>在web_demo.py的最后一句demo.queue().launch(share=True)，加两个server_name=“0.0.0.0”, server_port=1234参数。</li></ul> 
<pre><code>demo.queue().launch(share=True,server_name="0.0.0.0",server_port=9234)
</code></pre> 
<h4><a id="4_78"></a>4.模型量化</h4> 
<p>默认情况下，模型以 FP16 精度加载，运行上述代码需要大概 13GB 显存。如果你的 GPU 显存有限，可以尝试以量化方式加载模型，使用方法如下：</p> 
<ul><li><strong>GPU</strong></li></ul> 
<pre><code># FP16精度加载，需要13G显存
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
</code></pre> 
<pre><code># int8精度加载，需要10G显存
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().quantize(8).cuda()
</code></pre> 
<pre><code># int4精度加载，需要6G显存
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().quantize(4).cuda()
</code></pre> 
<ul><li><strong>CPU</strong></li></ul> 
<pre><code>#32G内存
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).float()
</code></pre> 
<pre><code>#16G内存
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).bfloat16()
</code></pre> 
<h4><a id="5_111"></a>5.详细代码</h4> 
<pre><code>from transformers import AutoModel, AutoTokenizer
import gradio as gr

tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
# model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
# 按需修改，目前只支持 4/8 bit 量化
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().quantize(4).cuda()
model = model.eval()

MAX_TURNS = 20
MAX_BOXES = MAX_TURNS * 2


def predict(input, history=[]):
    response, history = model.chat(tokenizer, input, history)
    updates = []
    for query, response in history:
        updates.append(gr.update(visible=True, value=query))
        updates.append(gr.update(visible=True, value=response))
    if len(updates) &lt; MAX_BOXES:
        updates = updates + [gr.Textbox.update(visible=False)] * (MAX_BOXES - len(updates))
    return [history] + updates


with gr.Blocks() as demo:
    state = gr.State([])
    text_boxes = []
    for i in range(MAX_BOXES):
        if i % 2 == 0:
            label = "提问："
        else:
            label = "回复："
        text_boxes.append(gr.Textbox(visible=False, label=label))

    with gr.Row():
        with gr.Column(scale=4):
            txt = gr.Textbox(show_label=False, placeholder="Enter text and press enter").style(container=False)
        with gr.Column(scale=1):
            button = gr.Button("Generate")
    button.click(predict, [txt, state], [state] + text_boxes)
demo.queue().launch(share=True,server_name="0.0.0.0",server_port=9234)

</code></pre> 
<h3><a id="_158"></a>调用示例</h3> 
<p><img src="https://images2.imgbox.com/42/de/vp5B6ozU_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/71/8f/NmWWahqL_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9576dcc93cc9a40822c2f01cfae25920/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【Java】还不懂this关键字？一分钟彻底弄懂this关键字</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3fbc148c4584f308eb659c7dbd7a9b27/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Access denied for user  root @ localhost  (using password: YES)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>