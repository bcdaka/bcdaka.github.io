<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Stable Diffusion高级教程 - 图生图(img2img)模式 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/80db4ff9914c3d38760bf3f4de54720e/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Stable Diffusion高级教程 - 图生图(img2img)模式">
  <meta property="og:description" content="前言
现在终于可以介绍 Stable Diffusion 除了文生图 (txt2img) 之外最重要的功能：图生图 (img2img)。顾名思义，除了根据正向和反向提示词之外，还需要基于一张图片生成图。这个模式下功能很多我们挨个说
img2img
图生图模式下的默认功能，我们先看一下主界面:
上面还是正面提示词和负面提示词，接着是一个上传图片的区域，写着「Drop Image Here - or - Click to Upload」。然后就是相关参数，大部分在文生图里面已经见过，只有Resize mode、Denoising strength是新增的，我们挨个介绍:
Resize mode。当上传图片尺寸和要生成的图的尺寸不同时，需要选择调整大小方案。
Sampling Method 用于去噪，平衡生成图的速度和质量。内置多种算法可供选择。目前看起来 DPM&#43;&#43; 2M Karras 用的比较多。
Sampling Steps 是去噪过程的采样步骤数。越多越好，但需要更长的时间。一般在 20-28 之间。
宽度和高度 (Width/Height)，输出图像的大小。按需调整即可。
Batch Count 批次数量，我更愿意用下面的 Batch size 调整生产图的总数。
Batch size，每一批次要生成的图像数量。可以在测试提示时多生成一些，因为每个生成的图像都会有所不同。生成的图像总数等于 Batch Count 乘以 Batch size。
CFG (Classifier Free Guidance) scale，提示词相关性， 用于控制模型应在多大程度上遵从您的提示。他有几个只可选: 1 (大多忽略你的提示)，3 (更有创意)，7 (遵循提示和自由之间的良好平衡)，15 (更加遵守提示)，30 (严格按照提示操作)，常用的是 7，做个平衡。测试时可以换这个值体验区别。
Denoising strength。降噪强度，常翻译成「重绘幅度」，取值范围是 0-1，描述新生成的图片与原图的相似程度，数值越小，采样越少，相似度越高，算的越快 (采样数 = Denoising strength* Sampling Steps)">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-12T09:59:10+08:00">
    <meta property="article:modified_time" content="2024-04-12T09:59:10+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Stable Diffusion高级教程 - 图生图(img2img)模式</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>前言</p> 
<p>现在终于可以介绍 Stable Diffusion 除了文生图 (txt2img) 之外最重要的功能：图生图 (img2img)。顾名思义，除了根据正向和反向提示词之外，还需要基于一张图片生成图。这个模式下功能很多我们挨个说</p> 
<p>img2img</p> 
<p>图生图模式下的默认功能，我们先看一下主界面:</p> 
<p><img src="https://images2.imgbox.com/5c/2e/bPdusl8J_o.png" alt="在这里插入图片描述"></p> 
<p>上面还是正面提示词和负面提示词，接着是一个上传图片的区域，写着「Drop Image Here - or - Click to Upload」。然后就是相关参数，大部分在文生图里面已经见过，只有<code>Resize mode</code>、<code>Denoising strength</code>是新增的，我们挨个介绍:</p> 
<ol><li> <p>Resize mode。当上传图片尺寸和要生成的图的尺寸不同时，需要选择调整大小方案。</p> </li><li> <p>Sampling Method 用于去噪，平衡生成图的速度和质量。内置多种算法可供选择。目前看起来 DPM++ 2M Karras 用的比较多。</p> </li><li> <p>Sampling Steps 是去噪过程的采样步骤数。越多越好，但需要更长的时间。一般在 20-28 之间。</p> </li><li> <p>宽度和高度 (Width/Height)，输出图像的大小。按需调整即可。</p> </li><li> <p>Batch Count 批次数量，我更愿意用下面的 Batch size 调整生产图的总数。</p> </li><li> <p>Batch size，每一批次要生成的图像数量。可以在测试提示时多生成一些，因为每个生成的图像都会有所不同。生成的图像总数等于 Batch Count 乘以 Batch size。</p> </li><li> <p>CFG (Classifier Free Guidance) scale，提示词相关性， 用于控制模型应在多大程度上遵从您的提示。他有几个只可选: 1 (大多忽略你的提示)，3 (更有创意)，7 (遵循提示和自由之间的良好平衡)，15 (更加遵守提示)，30 (严格按照提示操作)，常用的是 7，做个平衡。测试时可以换这个值体验区别。</p> </li><li> <p>Denoising strength。降噪强度，常翻译成「重绘幅度」，取值范围是 0-1，描述新生成的图片与原图的相似程度，数值越小，采样越少，相似度越高，算的越快 (采样数 = Denoising strength* Sampling Steps)</p> </li><li> <p>Seed，生成的每个图像都有自己的种子值，修改这个值可以控制图像的内容。</p> </li><li> <p>Script。用户可以编写脚本代码，以实现一些特殊定制的功能。这个未来可以具体说，目前还没有遇到。</p> </li></ol> 
<p>先具体说说<code>Resize mode</code>(当然上传的图片最好与生图设置的一致):</p> 
<ol><li> <p>Just resize：调整图片为生图设置的宽高。若上传图片的宽高与生成设置的宽高不一致，则该图片会被压扁。这个我非常不推荐使用，会让图片非常奇怪。</p> </li><li> <p>Crop and resize：裁切图片以符合生图的宽高，我最推荐的方式。</p> </li><li> <p>Resize and fill：裁切并调整图片宽高，若上传图片的宽高与生成设置的宽高不一致，则多出来的区域会自动填满。</p> </li><li> <p>Just resize (latent upscale)：调整图片大小为生图设置的宽高，并使用潜在空间放大。</p> </li></ol> 
<p>这个模式下最主要的就是调<code>Denoising strength</code>参数。我们用下面这张从网上找的新垣结衣的照片来体验:</p> 
<p><img src="https://images2.imgbox.com/42/b5/rsK0sboW_o.png" alt="在这里插入图片描述"></p> 
<p>首先注意，我选择这个图是有 2 个原因的:</p> 
<ol><li> <p>这个一张人像正面近像，在生成新图后更容易感受到 SD 的模型的作用</p> </li><li> <p>照片可以看到手部有动作，我会生成一张有问题的图让你感受到目前图生图模式的问题</p> </li></ol> 
<p>我希望通过 SD 把这个真人照片做出动漫的效果，咱们先来个较大的<code>Denoising strength</code>的值，为了方便对比我用了固定的 Seed:</p> 
<p><img src="https://images2.imgbox.com/68/05/837Vw653_o.png" alt="在这里插入图片描述"></p> 
<p>我直接把生成参数列出来:</p> 
<p>a woman with a short hair and a white shirt is posing for a picture with her hand on her chin, a photorealistic painting, Ayami Kojima, precisionism, perfect face<br> Negative prompt: dongwm-nt,bad finger, bad body<br> Steps: 20, Sampler: DPM++ 2M Karras, CFG scale: 8, Seed: 2345567052, Size: 512x512, Model hash: cbfba64e66, Model: CounterfeitV30_v30, Denoising strength: 0.65, Clip skip: 2</p> 
<p>这里有一点需要特别的提一下，正面提示词不是我写的。在图生图模式里，生成按钮左边有 2 个选项，分别是「Interrogate CLIP」和「Interrogate DeepBooru」。在上传图片后，可以通过「Interrogate CLIP」反推出提示词，我这个就是这么生成的。另外也说一下「Interrogate DeepBooru」，这说的是一个开源的女孩图片特征提取器，上传图片可以获得图片的标签，我已经把链接都放在了延伸阅读里面:</p> 
<p><img src="https://images2.imgbox.com/08/6f/l7tytRSz_o.png" alt="在这里插入图片描述"></p> 
<p>PS: 如果你选择<code>DeepBooru</code>反推，不能直接使用那些标签，你需要从中筛选需要的、合理的标签，否则结果会完全偏离。</p> 
<p>好的，说回来。之前已经说过，<code>Denoising strength</code>的值越大越和原图不符，所以如果你希望「微调」，这个值不应该大于 0.4，现在我们先取了一个更大的值，你可以看到生成图的人物手部的结果是有问题的。而且注意，负面提示词<code>dongwm-nt</code>本身是包含<code>bad finger,bad body</code>这些的。</p> 
<h5><a id="_79"></a>图生图不是万能的甚至很难达到你的预期</h5> 
<p>是的，这是我的体验。这个模式下如果你想要生成你想要的效果，对于大模型、微调模型、提示词、参数等都有要求，在前期，你很可能生成奇怪的图，你需要不断尝试总结经验。</p> 
<h5><a id="_Denoising_strength__83"></a>不同的 Denoising strength 效果的区别</h5> 
<p>我们使用 x/y/z 脚本试试不同的重绘幅度值看看生成的效果:</p> 
<p><img src="https://images2.imgbox.com/b5/93/cJAEJLJd_o.png" alt="在这里插入图片描述"></p> 
<p>可以看到随着 Denoising strength 变大，越来越不像原图了。</p> 
<p>同时，我们还可以重叠各种微调模型，下面是使用了 VAE、Lora 和 HyperNetwork 后的效果:</p> 
<p><img src="https://images2.imgbox.com/1f/83/NQcE1njM_o.png" alt="在这里插入图片描述"></p> 
<p>这就是微调模型的作用，不过注意，微调后手部后两张还是会有问题。</p> 
<p>PS，这个例子用的主模型是:https://civitai.com/models/4468/counterfeit-v30</p> 
<h4><a id="_Sketch_102"></a>绘图 (Sketch)</h4> 
<p>第二个 Tab 是 Sketch，他适合有美术基础的用户，可以给一张现有的图加东西，或者画出你想要的东西，然后再输入提示词完善，我这个没有画画细胞的人基本不用，在这里也举 2 个例子 (我也就这个水平啦)。</p> 
<p>因为我们一会要用笔刷编辑图片，我需要用到颜色，所以加启动参数，重启 webui:</p> 
<pre><code>./webui.sh --disable-safe-unpickle --gradio-img2img-tool color-sketch
</code></pre> 
<p>Ok, 先尝试基于现有图做修改的，我用了下面这张图:</p> 
<p><img src="https://images2.imgbox.com/dc/0a/MEp31rg7_o.png" alt="在这里插入图片描述"></p> 
<p>上传后就进入了编辑模式，然后我用笔刷选了个粉色的把头发涂变色 (当然提示词中并没有提到粉色头发):</p> 
<p><img src="https://images2.imgbox.com/4b/65/PRxfd94d_o.png" alt=""></p> 
<p>可以看到<code>Denoising strength</code>到了 0.7 才看起来正常，前面的那个「涂」的效果很明显。所以如果你使用和原图差别很大的颜色涂，那么需要更大的重绘幅度值，但是相对的，生成图和原图差别很大。如果选择对比色较少的例如黑色，那么重绘幅度 0.4 可能就够了。</p> 
<p>接着我们试试完全从零画一幅画 (叫「涂鸦」更合适)，为了展示 SD 的厉害之处，我特意选择了一个「复杂」的构图，在本灵魂画手非常努力作画后，看一下生成图的效果这样的:</p> 
<p><img src="https://images2.imgbox.com/fc/a7/nt1frkAT_o.png" alt="在这里插入图片描述"></p> 
<p>注意哈，因为这个模式需要上传图，所以我这里只是截了个终端的黑色区域作为背景图。我知道大家看不懂我的 Sketch🤦🏻 ♀️，解释一下，这幅画我希望展示蓝天白云，下面是草地和几棵树 (提示词也非常直白)，草地中间还有一点小溪（实在不知道溪水用什么颜色就直接换个绿区别一下）中间黑色的是背景我没涂东西，主要想看看 SD 会怎么理解。</p> 
<p>这个效果我还是很满意的，可以说 0.65 的图已经完全达到我的预想了。</p> 
<p>PS: 这种绘画的方法需要更大的<code>Denoising strength</code>值，否则用户就得具有极强的画画天赋啦</p> 
<h4><a id="_Inpaint_134"></a>局部绘制 (Inpaint)</h4> 
<p>用户指定在图像中特定区域进行修改，而保证其他区域不变。这个我认为图生图模式下最又实用价值的模式，类似换脸、换衣服、换背景等等需要都可以通过它来实现。在上面的 Sketch 里面的例子一，我曾经想给新垣结衣换头发颜色，但是需要<code>Denoising strength</code>值比较大才会看起来正常，但是通过也和原图差别非常大了。而局部绘制可以平缓的给新垣结衣换头发颜色。来试试:</p> 
<p><img src="https://images2.imgbox.com/74/d3/BQr6pc0I_o.png" alt="在这里插入图片描述"></p> 
<p>我涂黑了头发，当然差不多就可以，SD 会清楚你想把头发改颜色，另外要在提示词加上<code>pink hair</code>让 SD 朝着粉色头发来。可以看到随着<code>Denoising strength</code>增大，头发越来越粉。</p> 
<p>接着看一下参数:<br> <img src="https://images2.imgbox.com/71/4b/whlpeYCv_o.png" alt="在这里插入图片描述"></p> 
<p>这次新增 5 个参数:</p> 
<ol><li> <p>Mask blur。图片上的笔刷毛边柔和程度。我一般默认</p> </li><li> <p>Mask mode。选择要让 AI 填满涂黑区域 (Inpaint masked)，或是填满未涂黑区域 (Inpaint not masked)。</p> </li><li> <p>Masked content。要填充的内容类型。Fill：让 AI 参考涂黑附近的颜色填满区域；Original：在填满区域的时候参考原图底下的内容；latent noise：使用潜在空间填满，可能会生出跟原图完全不相关的内容；latent nothing：使用潜在空间填满，不加入噪声</p> </li><li> <p>Inpaint area。选择要填满整张图片 (Whole picture) 或是只填满涂黑的区域 (Only masked)</p> </li><li> <p>Only masked padding, pixels。像素内距。</p> </li></ol> 
<h4><a id="___Inpaint_sketch_159"></a>局部绘制 - 涂鸦蒙版 (Inpaint sketch)</h4> 
<p>局部绘制 Inpaint 的一个更细的分类，它们的区别是局部绘制中，用户涂黑的部分表示该部分可以被重绘，而在局部绘制 - 涂鸦蒙版（inpaint sketch）中，用户涂鸦的部分不仅表示可以重绘，用户涂鸦的内容还会成为图像生成的内容来源，换个表达方法，局部绘制 - 涂鸦蒙版是「局部绘制 (Inpaint)」+「绘图 (Sketch)」的组合，通过一个例子来理解:</p> 
<p><img src="https://images2.imgbox.com/a2/4e/ue9xloG4_o.png" alt="在这里插入图片描述"></p> 
<p>我涂了 2 个地方：1. 头发上的墙上背景，我希望它被 P 掉，2 粉色头发。然后提示词加了<code>pink hair</code>让 SD 能更理解我表达。所以最终生成的图里面的特点:</p> 
<ol><li> <p>除了头发和背景改变，对人物外貌动作等未改变 (因为没有产生绘制)，而且头发形状整体保持住了。</p> </li><li> <p>我 sketch 使用了一个偏紫的颜色，所以生成的图的头发颜色参考了这个颜色生成的微紫的粉色</p> </li><li> <p>可以看到 0.6 的效果是最好的，值越小越偏原图就能看出我涂鸦的痕迹，值越大头发走的越偏离我的意思</p> </li></ol> 
<h4><a id="___Inpaint_upload_175"></a>局部绘制 - 上传蒙版 (Inpaint upload)</h4> 
<p>可以在其他工具里 (例如 PS) 做好蒙版上传，而不是在 SD 里面创建蒙版。对于一些专业的用户这是一种更好的选择，因为在浏览器绘制蒙版的功能很简陋，其他专业软件做出来的效果会好得多。不过PS 这里就不举例了。<br> AI绘画SD整合包、各种模型插件、提示词、AI人工智能学习资料都已经打包好放在网盘中了，有需要的小伙伴文末扫码自行获取。</p> 
<h3><a id="_187"></a>写在最后</h3> 
<p>AIGC技术的未来发展前景广阔，随着人工智能技术的不断发展，AIGC技术也将不断提高。未来，AIGC技术将在游戏和计算领域得到更广泛的应用，使游戏和计算系统具有更高效、更智能、更灵活的特性。同时，AIGC技术也将与人工智能技术紧密结合，在更多的领域得到广泛应用，对程序员来说影响至关重要。未来，AIGC技术将继续得到提高，同时也将与人工智能技术紧密结合，在更多的领域得到广泛应用。</p> 
<p><font face="幼圆" size="4" color="red">感兴趣的小伙伴，赠送全套AIGC学习资料和安装工具，包含AI绘画、AI人工智能等前沿科技教程，模型插件，具体看下方。<br> </font><br> <img src="https://images2.imgbox.com/73/55/Vqex4Svd_o.jpg"></p> 
<p><strong>一、AIGC所有方向的学习路线</strong></p> 
<p>AIGC所有方向的技术点做的整理，形成各个领域的知识点汇总，它的用处就在于，你可以按照下面的知识点去找对应的学习资源，保证自己学得较为全面。</p> 
<p><img src="https://images2.imgbox.com/cc/35/lbLQQLSD_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/32/16/UnOymKbD_o.png" alt="在这里插入图片描述"></p> 
<p><strong>二、AIGC必备工具</strong></p> 
<p>工具都帮大家整理好了，安装就可直接上手！<br> <img src="https://images2.imgbox.com/44/00/eGlzRDQi_o.png" alt="在这里插入图片描述"></p> 
<p><strong>三、最新AIGC学习笔记</strong></p> 
<p>当我学到一定基础，有自己的理解能力的时候，会去阅读一些前辈整理的书籍或者手写的笔记资料，这些笔记详细记载了他们对一些技术点的理解，这些理解是比较独到，可以学到不一样的思路。<br> <img src="https://images2.imgbox.com/61/14/A3VT5Izv_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/52/b5/NP2dKJh6_o.png" alt="在这里插入图片描述"></p> 
<p><strong>四、AIGC视频教程合集</strong></p> 
<p>观看全面零基础学习视频，看视频学习是最快捷也是最有效果的方式，跟着视频中老师的思路，从基础到深入，还是很容易入门的。</p> 
<p><img src="https://images2.imgbox.com/f9/0b/o1X6JRF8_o.png" alt="在这里插入图片描述"></p> 
<p><strong>五、实战案例</strong></p> 
<p>纸上得来终觉浅，要学会跟着视频一起敲，要动手实操，才能将自己的所学运用到实际当中去，这时候可以搞点实战案例来学习。<br> <img src="https://images2.imgbox.com/5c/8c/nWv9MjOL_o.png" alt="在这里插入图片描述"></p> 
<img src="https://images2.imgbox.com/f2/a7/GcRHFylO_o.jpg"> 若有侵权，请联系删除
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ec05cd85ee321ce25677d20ef02e0a90/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Mac上Qt安装和配置教程_mac qt</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/11400fc1ecf6702af7d7e31bb4f9a2c3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">爬虫实战：我国城市的地铁数据以及分析</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>