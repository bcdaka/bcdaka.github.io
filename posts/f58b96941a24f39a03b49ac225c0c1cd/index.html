<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>嵌入式C&#43;&#43;、InfluxDB、Spark、MQTT协议、和Dash：树莓派集群物联网数据中心设计与实现（代码示例） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/f58b96941a24f39a03b49ac225c0c1cd/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="嵌入式C&#43;&#43;、InfluxDB、Spark、MQTT协议、和Dash：树莓派集群物联网数据中心设计与实现（代码示例）">
  <meta property="og:description" content="1. 项目概述 随着物联网技术的快速发展,如何高效地收集、存储和分析海量IoT设备数据成为一个重要课题。本文介绍了一个基于树莓派集群搭建的小型物联网数据中心,实现了从数据采集到分析可视化的完整流程。
该系统采用轻量级组件,适合资源受限的边缘计算环境。主要功能包括:
通过MQTT协议采集传感器数据使用Kafka进行数据传输InfluxDB存储时序数据Spark进行数据处理Grafana可视化展示Flask提供Web API接口 2. 系统设计 2.1 硬件架构 3个树莓派4B作为工作节点1个树莓派4B作为主节点1个外接硬盘用于数据存储 2.2 软件架构 数据采集:Mosquitto MQTT Broker数据传输:Apache Kafka数据存储:InfluxDB数据处理:Apache Spark数据分析:Jupyter Notebook, Pandas可视化:Grafana应用层:Flask 3. 代码实现 3.1 数据采集层 在数据采集层，我们使用MQTT协议来收集传感器数据。MQTT是一种轻量级的发布/订阅消息传输协议，非常适合物联网应用。
import paho.mqtt.client as mqtt import json import logging class IoTDataCollector: def __init__(self, broker_address, broker_port=1883): self.client = mqtt.Client() self.client.on_connect = self._on_connect self.client.on_message = self._on_message self.broker_address = broker_address self.broker_port = broker_port self.logger = logging.getLogger(__name__) def _on_connect(self, client, userdata, flags, rc): &#34;&#34;&#34; 当客户端连接到MQTT代理时调用此函数。 它订阅了所有传感器主题。 &#34;&#34;&#34; self.logger.info(f&#34;Connected with result code {rc}&#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-21T17:38:30+08:00">
    <meta property="article:modified_time" content="2024-07-21T17:38:30+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">嵌入式C&#43;&#43;、InfluxDB、Spark、MQTT协议、和Dash：树莓派集群物联网数据中心设计与实现（代码示例）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>1. 项目概述</h3> 
<p>随着物联网技术的快速发展,如何高效地收集、存储和分析海量IoT设备数据成为一个重要课题。本文介绍了一个基于树莓派集群搭建的小型物联网数据中心,实现了从数据采集到分析可视化的完整流程。</p> 
<p>该系统采用轻量级组件,适合资源受限的边缘计算环境。主要功能包括:</p> 
<ul><li>通过MQTT协议采集传感器数据</li><li>使用Kafka进行数据传输</li><li>InfluxDB存储时序数据</li><li>Spark进行数据处理</li><li>Grafana可视化展示</li><li>Flask提供Web API接口</li></ul> 
<h3>2. 系统设计</h3> 
<h4>2.1 硬件架构</h4> 
<p class="img-center"><img alt="" height="470" src="https://images2.imgbox.com/ff/dd/SLCiqzKv_o.png" width="964"></p> 
<ul><li>3个树莓派4B作为工作节点</li><li>1个树莓派4B作为主节点</li><li>1个外接硬盘用于数据存储</li></ul> 
<h4>2.2 软件架构</h4> 
<p class="img-center"><img alt="" height="241" src="https://images2.imgbox.com/27/33/hFDrHfCR_o.png" width="1006"></p> 
<ul><li>数据采集:Mosquitto MQTT Broker</li><li>数据传输:Apache Kafka</li><li>数据存储:InfluxDB</li><li>数据处理:Apache Spark</li><li>数据分析:Jupyter Notebook, Pandas</li><li>可视化:Grafana</li><li>应用层:Flask</li></ul> 
<h3>3. 代码实现</h3> 
<h4>3.1 数据采集层</h4> 
<p>在数据采集层，我们使用MQTT协议来收集传感器数据。MQTT是一种轻量级的发布/订阅消息传输协议，非常适合物联网应用。</p> 
<pre><code class="language-python">import paho.mqtt.client as mqtt
import json
import logging

class IoTDataCollector:
    def __init__(self, broker_address, broker_port=1883):
        self.client = mqtt.Client()
        self.client.on_connect = self._on_connect
        self.client.on_message = self._on_message
        self.broker_address = broker_address
        self.broker_port = broker_port
        self.logger = logging.getLogger(__name__)

    def _on_connect(self, client, userdata, flags, rc):
        """
        当客户端连接到MQTT代理时调用此函数。
        它订阅了所有传感器主题。
        """
        self.logger.info(f"Connected with result code {rc}")
        client.subscribe("sensors/#")

    def _on_message(self, client, userdata, msg):
        """
        当收到消息时调用此函数。
        它解析JSON消息并处理数据。
        """
        try:
            payload = json.loads(msg.payload.decode())
            self.logger.info(f"Received data: {payload} on topic {msg.topic}")
            # 这里可以添加数据处理逻辑，例如将数据传递给Kafka
        except json.JSONDecodeError:
            self.logger.error(f"Failed to parse message: {msg.payload}")

    def start(self):
        """
        启动MQTT客户端并开始监听消息。
        """
        self.client.connect(self.broker_address, self.broker_port, 60)
        self.client.loop_forever()

# 使用示例
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    collector = IoTDataCollector("localhost")
    collector.start()
</code></pre> 
<p>这段代码展示了如何创建一个MQTT客户端来接收IoT设备数据。它包含了错误处理和日志记录，这在实际应用中非常重要。</p> 
<h4>3.2 数据传输层</h4> 
<p>在数据传输层，我们使用Apache Kafka来处理高吞吐量的实时数据流。Kafka提供了可靠的消息队列服务，支持数据持久化和多订阅者模式。</p> 
<pre><code class="language-python">from kafka import KafkaProducer
import json
import logging
from retry import retry

class KafkaDataProducer:
    def __init__(self, bootstrap_servers):
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        self.logger = logging.getLogger(__name__)

    @retry(exceptions=Exception, tries=3, delay=1, backoff=2)
    def send_data(self, topic, data):
        """
        发送数据到指定的Kafka主题。
        包含重试机制以提高可靠性。
        """
        future = self.producer.send(topic, data)
        try:
            record_metadata = future.get(timeout=10)
            self.logger.info(f"Sent data to Kafka: topic={record_metadata.topic}, "
                             f"partition={record_metadata.partition}, "
                             f"offset={record_metadata.offset}")
        except Exception as e:
            self.logger.error(f"Error sending data to Kafka: {e}")
    def flush(self):
        """
        刷新并等待所有未完成的消息请求完成。
        在关闭生产者之前调用此方法很重要。
        """
        self.producer.flush()

    def close(self):
        """
        关闭Kafka生产者。
        这会确保所有未完成的消息请求在关闭之前完成。
        """
        self.producer.close()

# 使用示例
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    producer = KafkaDataProducer(['localhost:9092'])
    
    sensor_data = {
        'sensor_id': 1, 
        'temperature': 25.5, 
        'humidity': 60, 
        'timestamp': '2023-05-20T10:00:00Z'
    }
    
    try:
        producer.send_data('sensor_data', sensor_data)
    finally:
        producer.flush()
        producer.close()</code></pre> 
<p>这个Kafka生产者类(<code>KafkaDataProducer</code>)提供了以下关键功能：</p> 
<ol><li> <p><strong>可靠的消息发送</strong>：使用<code>send_data</code>方法发送数据到Kafka，包含了异常处理和日志记录。</p> </li><li> <p><strong>重试机制</strong>：通过<code>@retry</code>装饰器实现了自动重试，提高了系统的容错能力。</p> </li><li> <p><strong>异步操作</strong>：Kafka生产者的发送操作是异步的，使用<code>future.get()</code>等待发送结果。</p> </li><li> <p><strong>资源管理</strong>：提供了<code>flush</code>和<code>close</code>方法，确保在关闭生产者之前所有消息都被正确处理。</p> </li></ol> 
<p>使用Kafka作为数据传输层有以下优势：</p> 
<ul><li><strong>高吞吐量</strong>：Kafka能够处理大量的实时数据流。</li><li><strong>可靠性</strong>：支持数据复制和持久化，确保数据不会丢失。</li><li><strong>可扩展性</strong>：可以轻松扩展以处理增加的数据量。</li><li><strong>灵活性</strong>：支持多个生产者和消费者，适合复杂的数据流处理场景。</li></ul> 
<p>在物联网数据中心中，Kafka可以作为数据采集层和数据存储层之间的缓冲，解耦系统组件，提高整体系统的可靠性和扩展性。</p> 
<h4>3.3 数据存储层</h4> 
<p>对于数据存储层，我们选择使用InfluxDB，这是一个专门为时间序列数据优化的数据库，非常适合存储IoT传感器数据。以下是InfluxDB写入器的实现：</p> 
<pre><code class="language-python">from influxdb import InfluxDBClient
from datetime import datetime
import logging

class InfluxDBWriter:
    def __init__(self, host, port, database):
        self.client = InfluxDBClient(host=host, port=port)
        self.database = database
        self.logger = logging.getLogger(__name__)
        self._create_database()

    def _create_database(self):
        """
        如果数据库不存在，则创建数据库。
        """
        if self.database not in self.client.get_list_database():
            self.client.create_database(self.database)
        self.client.switch_database(self.database)
    def write_data(self, measurement, tags, fields):
        """
        将数据点写入InfluxDB。
        :param measurement: 测量的名称（类似于表名）
        :param tags: 标签数据（用于索引）
        :param fields: 字段数据（实际的度量值）
        """
        json_body = [
            {
                "measurement": measurement,
                "tags": tags,
                "time": datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ'),
                "fields": fields
            }
        ]
        try:
            self.client.write_points(json_body)
            self.logger.info(f"Data written to InfluxDB: {json_body}")
        except Exception as e:
            self.logger.error(f"Error writing to InfluxDB: {e}")

    def query_data(self, query):
        """
        从InfluxDB查询数据。
        :param query: InfluxQL查询字符串
        :return: 查询结果
        """
        try:
            result = self.client.query(query)
            return list(result.get_points())
        except Exception as e:
            self.logger.error(f"Error querying InfluxDB: {e}")
            return None

    def close(self):
        """
        关闭InfluxDB客户端连接。
        """
        self.client.close()

# 使用示例
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    influx_writer = InfluxDBWriter('localhost', 8086, 'iot_data')
    
    try:
        # 写入数据
        measurement = "temperature"
        tags = {"sensor_id": "1", "location": "room1"}
        fields = {"value": 25.5}
        influx_writer.write_data(measurement, tags, fields)

        # 查询数据
        query = 'SELECT * FROM temperature WHERE time &gt; now() - 1h'
        result = influx_writer.query_data(query)
        print(f"Query result: {result}")
    finally:
        influx_writer.close()</code></pre> 
<p>这个InfluxDB写入器类(<code>InfluxDBWriter</code>)提供了以下主要功能：</p> 
<ol><li> <p><strong>数据库初始化</strong>：在构造函数中，它会检查指定的数据库是否存在，如果不存在则创建。</p> </li><li> <p><strong>数据写入</strong>：<code>write_data</code>方法用于将数据点写入InfluxDB。它接受测量名称、标签和字段作为参数，并自动添加时间戳。</p> </li><li> <p><strong>数据查询</strong>：<code>query_data</code>方法允许执行InfluxQL查询，返回查询结果。</p> </li><li> <p><strong>错误处理</strong>：所有的数据库操作都包含了异常处理和日志记录，提高了代码的健壮性。</p> </li><li> <p><strong>资源管理</strong>：提供了<code>close</code>方法来正确关闭数据库连接。</p> </li></ol> 
<p>使用InfluxDB作为时间序列数据存储有以下优势：</p> 
<ul><li><strong>高性能</strong>：InfluxDB针对时间序列数据进行了优化，能够高效地处理大量的写入和查询操作。</li><li><strong>灵活的数据模型</strong>：支持标签和字段的概念，允许灵活地组织和查询数据。</li><li><strong>强大的查询语言</strong>：InfluxQL提供了丰富的查询功能，包括聚合、降采样等。</li><li><strong>数据保留策略</strong>：可以设置数据的自动过期和删除策略，方便管理长期数据。</li></ul> 
<h4>3.4 数据处理层</h4> 
<p>在数据处理层，我们使用Apache Spark进行大规模数据处理。Spark是一个强大的分布式计算引擎，适合处理大量的IoT数据。</p> 
<pre><code class="language-python">from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

class SparkDataProcessor:
    def __init__(self, app_name="IoTDataProcessor"):
        self.spark = SparkSession.builder \
            .appName(app_name) \
            .getOrCreate()

    def process_temperature_data(self, input_path, output_path):
        """
        处理温度数据：计算每个传感器的平均温度
        """
        schema = StructType([
            StructField("sensor_id", StringType(), True),
            StructField("timestamp", TimestampType(), True),
            StructField("temperature", FloatType(), True)
        ])

        df = self.spark.read.json(input_path, schema=schema)
        
        result = df.groupBy("sensor_id") \
            .agg(avg("temperature").alias("avg_temperature"))
        
        result.write.csv(output_path, header=True, mode="overwrite")

    def stop(self):
        """
        停止SparkSession
        """
        self.spark.stop()

# 使用示例
if __name__ == "__main__":
    processor = SparkDataProcessor()
    try:
        processor.process_temperature_data("input_data/*.json", "output_data/avg_temperatures")
    finally:
        processor.stop()
</code></pre> 
<p>这个Spark处理器展示了如何使用PySpark处理IoT数据。它读取JSON格式的温度数据，计算每个传感器的平均温度，并将结果保存为CSV文件。</p> 
<h4>3.5 数据分析层</h4> 
<p>在数据分析层，我们使用Python的数据分析库Pandas和机器学习库Scikit-learn来进行数据分析和预测。</p> 
<pre><code class="language-python">import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import joblib

class DataAnalyzer:
    def __init__(self, data_path):
        self.data = pd.read_csv(data_path)
        self.model = None

    def preprocess_data(self):
        """
        数据预处理：处理缺失值，转换日期等
        """
        self.data['timestamp'] = pd.to_datetime(self.data['timestamp'])
        self.data['hour'] = self.data['timestamp'].dt.hour
        self.data = self.data.dropna()

    def train_model(self):
        """
        训练一个简单的线性回归模型来预测温度
        """
        X = self.data[['hour', 'humidity']]
        y = self.data['temperature']
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        self.model = LinearRegression()
        self.model.fit(X_train, y_train)
        
        score = self.model.score(X_test, y_test)
        print(f"Model R2 score: {score}")

    def save_model(self, path):
        """
        保存训练好的模型
        """
        joblib.dump(self.model, path)

# 使用示例
if __name__ == "__main__":
    analyzer = DataAnalyzer("sensor_data.csv")
    analyzer.preprocess_data()
    analyzer.train_model()
    analyzer.save_model("temperature_prediction_model.joblib")</code></pre> 
<h4>3.6 可视化层</h4> 
<p>在可视化层，我们使用Plotly库来创建交互式的数据可视化。这里我们创建一个简单的仪表板来展示传感器数据。</p> 
<pre><code class="language-python">import plotly.graph_objs as go
import plotly.express as px
import pandas as pd
from dash import Dash, dcc, html
from dash.dependencies import Input, Output

class IoTDashboard:
    def __init__(self, data_path):
        self.df = pd.read_csv(data_path)
        self.app = Dash(__name__)
        self.setup_layout()

    def setup_layout(self):
        self.app.layout = html.Div([
            html.H1("IoT Sensor Dashboard"),
            dcc.Graph(id='temperature-graph'),
            dcc.Graph(id='humidity-graph'),
            dcc.Interval(
                id='interval-component',
                interval=5*1000,  # in milliseconds
                n_intervals=0
            )
        ])

        @self.app.callback(Output('temperature-graph', 'figure'),
                           Input('interval-component', 'n_intervals'))
        def update_temperature_graph(n):
            fig = px.line(self.df, x='timestamp', y='temperature', color='sensor_id',
                          title='Temperature Over Time')
            return fig

        @self.app.callback(Output('humidity-graph', 'figure'),
                           Input('interval-component', 'n_intervals'))
        def update_humidity_graph(n):
            fig = px.scatter(self.df, x='temperature', y='humidity', color='sensor_id',
                             title='Temperature vs Humidity')
            return fig

    def run(self):
        self.app.run_server(debug=True)

# 使用示例
if __name__ == '__main__':
    dashboard = IoTDashboard("sensor_data.csv")
    dashboard.run()
</code></pre> 
<p>这个仪表板使用Dash创建了一个web应用，展示了温度随时间的变化以及温度与湿度的关系。</p> 
<h3>4. 项目总结</h3> 
<p>本项目展示了如何构建一个基于树莓派集群的物联网数据中心。我们涵盖了从数据采集到数据分析和可视化的整个流程：</p> 
<ol><li>使用MQTT协议采集传感器数据</li><li>通过Kafka进行数据传输</li><li>用InfluxDB存储时序数据</li><li>利用Spark进行大规模数据处理</li><li>使用Pandas和Scikit-learn进行数据分析和预测</li><li>最后通过Plotly和Dash创建交互式仪表板</li></ol> 
<p>这个系统具有以下优点：</p> 
<ul><li>可扩展性：可以轻松添加更多的传感器和处理节点</li><li>实时性：能够实时处理和展示数据</li><li>灵活性：各个组件可以独立升级和替换</li><li>分析能力：支持复杂的数据处理和机器学习任务</li></ul> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/48ccb29b8979f3679527e76787a383db/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Java 集合框架：Java 中的优先级队列 PriorityQueue 的实现</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4c900484b0083c8617819bde125e946c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【深入C&#43;&#43;】二叉搜索树</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>