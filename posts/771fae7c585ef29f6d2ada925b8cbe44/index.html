<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Spark编程实验四：Spark Streaming编程 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/771fae7c585ef29f6d2ada925b8cbe44/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Spark编程实验四：Spark Streaming编程">
  <meta property="og:description" content="目录
一、目的与要求
二、实验内容
三、实验步骤
1、利用Spark Streaming对三种类型的基本数据源的数据进行处理
2、利用Spark Streaming对Kafka高级数据源的数据进行处理
3、完成DStream的两种有状态转换操作
4、把DStream的数据输出保存到文本文件或MySQL数据库中
四、结果分析与实验体会
一、目的与要求 1、通过实验掌握Spark Streaming的基本编程方法；
2、熟悉利用Spark Streaming处理来自不同数据源的数据。
3、熟悉DStream的各种转换操作。
4、熟悉把DStream的数据输出保存到文本文件或MySQL数据库中。
二、实验内容 1、参照教材示例，利用Spark Streaming对三种类型的基本数据源的数据进行处理。 2、参照教材示例，完成kafka集群的配置，利用Spark Streaming对Kafka高级数据源的数据进行处理，注意topic为你的姓名全拼。
3、参照教材示例，完成DStream的两种有状态转换操作。
4、参照教材示例，完成把DStream的数据输出保存到文本文件或MySQL数据库中。
三、实验步骤 1、利用Spark Streaming对三种类型的基本数据源的数据进行处理 （1）文件流
首先打开第一个终端作为数据流终端，创建一个logfile目录：
[root@bigdata zhc]# cd /home/zhc/mycode/sparkstreaming [root@bigdata sparkstreaming]# mkdir logfile [root@bigdata sparkstreaming]# cd logfile 然后打开第二个终端作为流计算终端，在“/logfile/”目录下面新建一个py程序：
[root@bigdata logfile]# vim FileStreaming.py 输入如下代码：
#/home/zhc/mycode/sparkstreaming/logfile/FileStreaming.py from pyspark import SparkContext, SparkConf from pyspark.streaming import StreamingContext conf = SparkConf() conf.setAppName(&#39;TestDStream&#39;) conf.setMaster(&#39;local[2]&#39;) sc = SparkContext(conf = conf) ssc = StreamingContext(sc, 10) lines = ssc.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-12-30T11:46:26+08:00">
    <meta property="article:modified_time" content="2023-12-30T11:46:26+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Spark编程实验四：Spark Streaming编程</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E7%9B%AE%E7%9A%84%E4%B8%8E%E8%A6%81%E6%B1%82-toc" style="margin-left:40px;"><a href="#%E4%B8%80%E3%80%81%E7%9B%AE%E7%9A%84%E4%B8%8E%E8%A6%81%E6%B1%82" rel="nofollow">一、目的与要求</a></p> 
<p id="%E4%BA%8C%E3%80%81%E5%AE%9E%E9%AA%8C%E5%86%85%E5%AE%B9-toc" style="margin-left:40px;"><a href="#%E4%BA%8C%E3%80%81%E5%AE%9E%E9%AA%8C%E5%86%85%E5%AE%B9" rel="nofollow">二、实验内容</a></p> 
<p id="%E4%B8%89%E3%80%81%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4-toc" style="margin-left:40px;"><a href="#%E4%B8%89%E3%80%81%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4" rel="nofollow">三、实验步骤</a></p> 
<p id="1%E3%80%81%E5%88%A9%E7%94%A8Spark%20Streaming%E5%AF%B9%E4%B8%89%E7%A7%8D%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E6%BA%90%E7%9A%84%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%A4%84%E7%90%86-toc" style="margin-left:80px;"><a href="#1%E3%80%81%E5%88%A9%E7%94%A8Spark%20Streaming%E5%AF%B9%E4%B8%89%E7%A7%8D%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E6%BA%90%E7%9A%84%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%A4%84%E7%90%86" rel="nofollow">1、利用Spark Streaming对三种类型的基本数据源的数据进行处理</a></p> 
<p id="2%E3%80%81%E5%88%A9%E7%94%A8Spark%20Streaming%E5%AF%B9Kafka%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E6%BA%90%E7%9A%84%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%A4%84%E7%90%86-toc" style="margin-left:80px;"><a href="#2%E3%80%81%E5%88%A9%E7%94%A8Spark%20Streaming%E5%AF%B9Kafka%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E6%BA%90%E7%9A%84%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%A4%84%E7%90%86" rel="nofollow">2、利用Spark Streaming对Kafka高级数据源的数据进行处理</a></p> 
<p id="3%E3%80%81%E5%AE%8C%E6%88%90DStream%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%9C%89%E7%8A%B6%E6%80%81%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C-toc" style="margin-left:80px;"><a href="#3%E3%80%81%E5%AE%8C%E6%88%90DStream%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%9C%89%E7%8A%B6%E6%80%81%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C" rel="nofollow">3、完成DStream的两种有状态转换操作</a></p> 
<p id="4%E3%80%81%E6%8A%8ADStream%E7%9A%84%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA%E4%BF%9D%E5%AD%98%E5%88%B0%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6%E6%88%96MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD-toc" style="margin-left:80px;"><a href="#4%E3%80%81%E6%8A%8ADStream%E7%9A%84%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA%E4%BF%9D%E5%AD%98%E5%88%B0%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6%E6%88%96MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD" rel="nofollow">4、把DStream的数据输出保存到文本文件或MySQL数据库中</a></p> 
<p id="%E5%9B%9B%E3%80%81%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E9%AA%8C%E4%BD%93%E4%BC%9A-toc" style="margin-left:40px;"><a href="#%E5%9B%9B%E3%80%81%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E9%AA%8C%E4%BD%93%E4%BC%9A" rel="nofollow">四、结果分析与实验体会</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h3 style="margin-left:.0001pt;text-align:justify;">一、目的与要求</h3> 
<p style="margin-left:.0001pt;text-align:justify;">1、通过实验掌握Spark Streaming的基本编程方法；<br> 2、熟悉利用Spark Streaming处理来自不同数据源的数据。<br> 3、熟悉DStream的各种转换操作。<br> 4、熟悉把DStream的数据输出保存到文本文件或MySQL数据库中。</p> 
<h3 id="%E4%BA%8C%E3%80%81%E5%AE%9E%E9%AA%8C%E5%86%85%E5%AE%B9" style="margin-left:.0001pt;text-align:justify;">二、实验内容</h3> 
<p style="margin-left:.0001pt;text-align:justify;">1、参照教材示例，利用Spark Streaming对三种类型的基本数据源的数据进行处理。 <br> 2、参照教材示例，完成kafka集群的配置，利用Spark Streaming对Kafka高级数据源的数据进行处理，<strong><strong>注意topic为你的姓名全拼</strong></strong>。<br> 3、参照教材示例，完成DStream的两种有状态转换操作。<br> 4、参照教材示例，完成把DStream的数据输出保存到文本文件或MySQL数据库中。</p> 
<h3 id="%E4%B8%89%E3%80%81%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4" style="margin-left:.0001pt;text-align:justify;">三、实验步骤</h3> 
<h4 id="1%E3%80%81%E5%88%A9%E7%94%A8Spark%20Streaming%E5%AF%B9%E4%B8%89%E7%A7%8D%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E6%BA%90%E7%9A%84%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%A4%84%E7%90%86">1、利用Spark Streaming对三种类型的基本数据源的数据进行处理</h4> 
<p><strong>（1）文件流</strong></p> 
<p>首先打开第一个终端作为数据流终端，创建一个logfile目录：</p> 
<pre><code class="language-css">[root@bigdata zhc]# cd /home/zhc/mycode/sparkstreaming
[root@bigdata sparkstreaming]# mkdir logfile
[root@bigdata sparkstreaming]# cd logfile
</code></pre> 
<p>然后打开第二个终端作为流计算终端，在“/logfile/”目录下面新建一个py程序：</p> 
<pre><code class="language-css">[root@bigdata logfile]# vim FileStreaming.py</code></pre> 
<p>输入如下代码：</p> 
<pre><code class="language-python">#/home/zhc/mycode/sparkstreaming/logfile/FileStreaming.py

from pyspark import SparkContext, SparkConf
from pyspark.streaming import StreamingContext

conf = SparkConf()
conf.setAppName('TestDStream')
conf.setMaster('local[2]')
sc = SparkContext(conf = conf)
ssc = StreamingContext(sc, 10)
lines = ssc.textFileStream('file:///home/zhc/mycode/sparkstreaming/logfile')
words = lines.flatMap(lambda line: line.split(' '))
wordCounts = words.map(lambda x : (x,1)).reduceByKey(lambda a,b:a+b)
wordCounts.pprint()
ssc.start()
ssc.awaitTermination()
</code></pre> 
<p>保存该文件并执行如下命令：</p> 
<pre><code class="language-css">[root@bigdata logfile]# spark-submit FileStreaming.py </code></pre> 
<p>然后我们进入数据流终端，在logfile目录下新建一个log2.txt文件，然后往里面输入一些英文语句后保存退出，再次切换到流计算终端，就可以看见打印出单词统计信息了。</p> 
<p><img alt="" height="452" src="https://images2.imgbox.com/7c/2e/WxIWxIGI_o.png" width="1200"></p> 
<p><strong>（2）套接字流</strong></p> 
<p><u>1）使用套接字流作为数据源</u></p> 
<p>继续在流计算端的sparkstreaming目录下创建一个socket目录，然后在该目录下创建一个NetworkWordCount.py程序：</p> 
<pre><code class="language-css">[root@bigdata sparkstreaming]# mkdir socket
[root@bigdata sparkstreaming]# cd socket
[root@bigdata socket]# vim NetworkWordCount.py
</code></pre> 
<p>输入如下代码：</p> 
<pre><code class="language-python">#/home/zhc/mycode/sparkstreaming/socket/NetworkWordCount.py

from __future__ import print_function
import sys
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: NetworkWordCount.py &lt;hostname&gt; &lt;port&gt;", file=sys.stderr)
        exit(-1)
    sc = SparkContext(appName="PythonStreamingNetworkWordCount")
    ssc = StreamingContext(sc, 5)
    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
    counts = lines.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)
    counts.pprint()
    ssc.start()
    ssc.awaitTermination()
</code></pre> 
<p>再在数据流终端启动Socket服务器端：</p> 
<pre><code class="language-css">[root@bigdata logfile]# nc -lk 9999
</code></pre> 
<p>然后再进入流计算终端，执行如下代码启动流计算：</p> 
<pre><code class="language-css">[root@bigdata socket]# spark-submit NetworkWordCount.py localhost 9999
</code></pre> 
<p>然后在数据流终端内手动输入一行英文句子后回车，多输入几次，流计算终端就会不断执行词频统计并打印出信息。</p> 
<p><img alt="" height="629" src="https://images2.imgbox.com/a8/c6/YSg0FtZn_o.png" width="1200"></p> 
<p><u>2）使用Socket编程实现自定义数据源</u></p> 
<p>下面我们再前进一步，把数据源头的产生方式修改一下，不要使用nc程序，而是采用自己编写的程序产生Socket数据源。在数据流终端执行以下命令，编写DataSourceSocket.py文件：</p> 
<pre><code class="language-css">[root@bigdata logfile]# cd /home/zhc/mycode/sparkstreaming/socket
[root@bigdata socket]# vim DataSourceSocket.py
</code></pre> 
<p>输入如下代码：</p> 
<pre><code class="language-python">#/home/zhc/mycode/sparkstreaming/socket/DataSourceSocket.py
import socket
# 生成socket对象
server = socket.socket()
# 绑定ip和端口
server.bind(('localhost', 9999))
# 监听绑定的端口
server.listen(1)
while 1:
    # 为了方便识别，打印一个“我在等待”
    print("I'm waiting the connect...")
    # 这里用两个值接受，因为连接上之后使用的是客户端发来请求的这个实例
    # 所以下面的传输要使用conn实例操作
    conn,addr = server.accept()
    # 打印连接成功
    print("Connect success! Connection is from %s " % addr[0])
    # 打印正在发送数据
    print('Sending data...')
    conn.send('I love hadoop I love spark hadoop is good spark is fast'.encode())
    conn.close()
    print('Connection is broken.')
</code></pre> 
<p>继续在数据流终端执行如下命令启动Socket服务端：</p> 
<pre><code class="language-css">[root@bigdata socket]# spark-submit DataSourceSocket.py</code></pre> 
<p><img alt="" height="491" src="https://images2.imgbox.com/42/7d/yrRC8iot_o.png" width="1200"></p> 
<p>再进入流计算终端，执行如下代码启动流计算：</p> 
<pre><code class="language-css">[root@bigdata socket]# spark-submit NetworkWordCount.py localhost 9999
</code></pre> 
<p><img alt="" height="606" src="https://images2.imgbox.com/85/29/3MKchRC8_o.png" width="1200"></p> 
<p><strong>（3）RDD队列流</strong></p> 
<p>继续在sparkstreaming目录下新建rddqueue目录并在该目录下创建RDDQueueStream.py程序：</p> 
<pre><code class="language-css">[root@bigdata sparkstreaming]# mkdir rddqueue
[root@bigdata sparkstreaming]# cd rddqueue
[root@bigdata rddqueue]# vim RDDQueueStream.py
</code></pre> 
<p>输入如下代码：</p> 
<pre><code class="language-python">#/home/zhc/mycode/sparkstreaming/rddqueue/RDDQueueStreaming.py
import time
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
if __name__ == "__main__":
    sc = SparkContext(appName="PythonStreamingQueueStream")
    ssc = StreamingContext(sc, 2)
    #创建一个队列，通过该队列可以把RDD推给一个RDD队列流
    rddQueue = []
    for i in range(5):
        rddQueue += [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)]
        time.sleep(1)
    #创建一个RDD队列流
    inputStream = ssc.queueStream(rddQueue)
    mappedStream = inputStream.map(lambda x: (x % 10, 1))
    reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)
    reducedStream.pprint()
    ssc.start()
    ssc.stop(stopSparkContext=True, stopGraceFully=True)
</code></pre> 
<p>保存退出后，进入流计算终端再执行如下命令：</p> 
<pre><code class="language-css">[root@bigdata rddqueue]# spark-submit RDDQueueStream.py</code></pre> 
<p><img alt="" height="666" src="https://images2.imgbox.com/7b/2d/26j3WL7C_o.png" width="1200"></p> 
<h4 id="2%E3%80%81%E5%88%A9%E7%94%A8Spark%20Streaming%E5%AF%B9Kafka%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E6%BA%90%E7%9A%84%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%A4%84%E7%90%86">2、利用Spark Streaming对Kafka高级数据源的数据进行处理</h4> 
<p><strong><em>此过程可以参照这篇博客的第四、五部分内容:</em></strong></p> 
<p><a class="has-card" href="https://blog.csdn.net/Morse_Chen/article/details/135273370?spm=1001.2014.3001.5501" title="【数据采集与预处理】数据接入工具Kafka-CSDN博客"><span class="link-card-box"><span class="link-title">【数据采集与预处理】数据接入工具Kafka-CSDN博客</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/c8/b0/9uv7XLF8_o.png" alt="icon-default.png?t=N7T8">https://blog.csdn.net/Morse_Chen/article/details/135273370?spm=1001.2014.3001.5501</span></span></a></p> 
<h4 id="3%E3%80%81%E5%AE%8C%E6%88%90DStream%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%9C%89%E7%8A%B6%E6%80%81%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C">3、完成DStream的两种有状态转换操作</h4> 
<p>说明：上面的词频统计程序NetworkWordCount.py采取了无状态转换操作。</p> 
<p><strong>（1）滑动窗口转换操作</strong></p> 
<p>在socket目录下创建WindowedNetworkWordCount.py程序并输入如下代码：</p> 
<pre><code class="language-python">#/home/zhc/mycode/sparkstreaming/socket/WindowedNetworkWordCount.py
from __future__ import print_function
import sys
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: WindowedNetworkWordCount.py &lt;hostname&gt; &lt;port&gt;", file=sys.stderr)
        exit(-1)
    sc = SparkContext(appName="PythonStreamingWindowedNetworkWordCount")
    ssc = StreamingContext(sc, 10)
    ssc.checkpoint("file:///home/zhc/mycode/sparkstreaming/socket/checkpoint")
    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
    counts = lines.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 30, 10)
    counts.pprint()
    ssc.start()
    ssc.awaitTermination()
</code></pre> 
<p>然后在数据流终端执执行如下命令运行nc程序：</p> 
<pre><code class="language-css">[root@bigdata sparkstreaming]# cd /home/zhc/mycode/sparkstreaming/socket
[root@bigdata socket]# nc -lk 9999
</code></pre> 
<p><img alt="" height="168" src="https://images2.imgbox.com/87/7e/zDl1gVCM_o.png" width="500"></p> 
<p>然后再在流计算终端运行WindowedNetworkWordCount.py代码：</p> 
<pre><code class="language-css">[root@bigdata socket]# spark-submit WindowedNetworkWordCount.py localhost 9999
</code></pre> 
<p>这时，可以查看流计算终端内显示的词频动态统计结果，可以看到，随着时间的流逝，词频统计结果会发生动态变化。 </p> 
<p><img alt="" height="667" src="https://images2.imgbox.com/d8/30/s4HskmG6_o.png" width="1200"></p> 
<p><strong>（2）updateStateByKey操作</strong></p> 
<p>在“/home/zhc/mycode/sparkstreaming/”路径下新建目录“/stateful”，并在该目录下新建代码文件NetworkWordCountStateful.py。</p> 
<pre><code class="language-css">[root@bigdata sparkstreaming]# mkdir stateful
[root@bigdata sparkstreaming]# cd stateful
[root@bigdata stateful]# vim NetworkWordCountStateful.py
</code></pre> 
<p>输入如下代码： </p> 
<pre><code class="language-python">#/home/zhc/mycode/sparkstreaming/stateful/NetworkWordCountStateful.py
from __future__ import print_function
import sys
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: NetworkWordCountStateful.py &lt;hostname&gt; &lt;port&gt;", file=sys.stderr)
        exit(-1)
    sc = SparkContext(appName="PythonStreamingStatefulNetworkWordCount")
    ssc = StreamingContext(sc, 1)
    ssc.checkpoint("file:///home/zhc/mycode/sparkstreaming/stateful/")           
    # RDD with initial state (key, value) pairs
    initialStateRDD = sc.parallelize([(u'hello', 1), (u'world', 1)]) 
    def updateFunc(new_values, last_sum):
        return sum(new_values) + (last_sum or 0) 
    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
    running_counts = lines.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).updateStateByKey(updateFunc, initialRDD=initialStateRDD) 
    running_counts.pprint()
    ssc.start()
    ssc.awaitTermination()
</code></pre> 
<p>在“数据源终端”，执行如下命令启动nc程序：</p> 
<pre><code class="language-css">[root@bigdata stateful]# nc  -lk  9999
</code></pre> 
<p>在“流计算终端”，执行如下命令提交运行程序：</p> 
<pre><code class="language-css">[root@bigdata stateful]# spark-submit NetworkWordCountStateful.py localhost 9999
</code></pre> 
<p>在数据源终端内手动输入一些单词并回车，再切换到流计算终端，可以看到已经输出了类似如下的词频统计信息： </p> 
<p><img alt="" height="484" src="https://images2.imgbox.com/67/c3/NXFfjND8_o.png" width="1200"></p> 
<p><img alt="" height="300" src="https://images2.imgbox.com/9c/19/htaF49a1_o.png" width="300"></p> 
<h4 id="4%E3%80%81%E6%8A%8ADStream%E7%9A%84%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA%E4%BF%9D%E5%AD%98%E5%88%B0%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6%E6%88%96MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD">4、把DStream的数据输出保存到文本文件或MySQL数据库中</h4> 
<p><strong>（1）把DStream输出到文本文件中</strong></p> 
<p>在stateful目录下新建NetworkWordCountStatefulText.py文件：</p> 
<pre><code class="language-css">[root@bigdata stateful]# vim NetworkWordCountStatefulText.py</code></pre> 
<p>输入如下代码： </p> 
<pre><code class="language-python">#/home/zhc/mycode/sparkstreaming/stateful/NetworkWordCountStatefulText.py
from __future__ import print_function
import sys
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: NetworkWordCountStateful.py &lt;hostname&gt; &lt;port&gt;", file=sys.stderr)
        exit(-1)
    sc = SparkContext(appName="PythonStreamingStatefulNetworkWordCount")
    ssc = StreamingContext(sc, 1)
    ssc.checkpoint("file:///home/zhc/mycode/sparkstreaming/stateful/statefultext")
    # RDD with initial state (key, value) pairs
    initialStateRDD = sc.parallelize([(u'hello', 1), (u'world', 1)])
    def updateFunc(new_values, last_sum):
        return sum(new_values) + (last_sum or 0)
    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
    running_counts = lines.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).updateStateByKey(updateFunc, initialRDD=initialStateRDD)
    running_counts.saveAsTextFiles("file:///home/zhc/mycode/sparkstreaming/stateful/statefultext/output")
    running_counts.pprint()
    ssc.start()
    ssc.awaitTermination()
</code></pre> 
<p>在“数据源终端”，执行如下命令启动nc程序：</p> 
<pre><code class="language-css">[root@bigdata stateful]# nc  -lk  9999
</code></pre> 
<p>在“流计算终端”，执行如下命令提交运行程序：</p> 
<pre><code class="language-css">[root@bigdata stateful]# spark-submit NetworkWordCountStatefulText.py localhost 9999
</code></pre> 
<p>在数据源终端内手动输入一些单词并回车，再切换到流计算终端，可以看到已经输出了类似如下的词频统计信息：  </p> 
<p><img alt="" height="376" src="https://images2.imgbox.com/2f/2e/NwOt4vC7_o.png" width="1200"></p> 
<p><img alt="" height="288" src="https://images2.imgbox.com/b9/0b/2RyrN4V4_o.png" width="300"></p> 
<p>在“/home/zhc/mycode/sparkstreaming/stateful/statefultext”目录下便可查看到如下输出目录结果：</p> 
<p><img alt="" height="393" src="https://images2.imgbox.com/c9/36/4zuENiqm_o.png" width="400"></p> 
<p>进入某个目录下，就可以看到类似part-00000的文件，里面包含了流计算过程的输出结果。</p> 
<p><img alt="" height="92" src="https://images2.imgbox.com/a3/29/hvMJIyOz_o.png" width="400"></p> 
<p><strong>（2）把DStream写入到MySQL数据库中</strong></p> 
<p>首先启动MySQL数据库：</p> 
<pre><code class="language-css">[root@bigdata stateful]# systemctl start mysqld.service
[root@bigdata stateful]# mysql -u root -p
</code></pre> 
<p>然后创建spark数据库和wordcount表：</p> 
<pre><code class="language-sql">mysql&gt; use spark;
mysql&gt; create table wordcount (word char(20), count int(4));
</code></pre> 
<p>然后再在终端安装python连接MySQL的模块：</p> 
<pre><code class="language-css">[root@bigdata stateful]# pip3 install PyMySQL</code></pre> 
<p><img alt="" height="164" src="https://images2.imgbox.com/7d/41/HKOe3JyX_o.png" width="1200"></p> 
<p>在stateful目录并在该目录下创建NetworkWordCountStatefulDB.py文件： </p> 
<pre><code class="language-css">[root@bigdata stateful]# vim NetworkWordCountStatefulDB.py</code></pre> 
<p>输入如下代码： </p> 
<pre><code class="language-python">#/home/zhc/mycode/sparkstreaming/stateful/NetworkWordCountStatefulDB.py
from __future__ import print_function 
import sys 
import pymysql 
from pyspark import SparkContext
from pyspark.streaming import StreamingContext 
if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: NetworkWordCountStateful &lt;hostname&gt; &lt;port&gt;", file=sys.stderr)
        exit(-1)
    sc = SparkContext(appName="PythonStreamingStatefulNetworkWordCount")
    ssc = StreamingContext(sc, 1)
    ssc.checkpoint("file:///home/zhc/mycode/sparkstreaming/stateful/statefuldb")  
    # RDD with initial state (key, value) pairs
    initialStateRDD = sc.parallelize([(u'hello', 1), (u'world', 1)]) 
    def updateFunc(new_values, last_sum):
        return sum(new_values) + (last_sum or 0) 
    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
    running_counts = lines.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).updateStateByKey(updateFunc, initialRDD=initialStateRDD) 
    running_counts.pprint() 
    def dbfunc(records):
        db = pymysql.connect(host="localhost",user="root",password="MYsql123!",database="spark")
        cursor = db.cursor() 
        def doinsert(p):
            sql = "insert into wordcount(word,count) values ('%s', '%s')" % (str(p[0]), str(p[1]))
            try:
                cursor.execute(sql)
                db.commit()
            except:
                db.rollback()
        for item in records:
            doinsert(item) 
    def func(rdd):
        repartitionedRDD = rdd.repartition(3)
        repartitionedRDD.foreachPartition(dbfunc)
    running_counts.foreachRDD(func)
    ssc.start()
    ssc.awaitTermination()
</code></pre> 
<p>在“数据源终端”，执行如下命令启动nc程序：</p> 
<pre><code class="language-css">[root@bigdata stateful]# nc  -lk  9999
</code></pre> 
<p>在“流计算终端”，执行如下命令提交运行程序：</p> 
<pre><code class="language-css">[root@bigdata stateful]# spark-submit NetworkWordCountStatefulDB.py localhost 9999
</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;">在数据源终端内手动输入一些单词并回车，再切换到流计算终端，可以看到已经输出了类似如下的词频统计信息：</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="469" src="https://images2.imgbox.com/b6/c2/1Q8xIohQ_o.png" width="1200"></p> 
<p><img alt="" height="308" src="https://images2.imgbox.com/ac/56/XiOHBA4j_o.png" width="300"> </p> 
<p>到MySQL终端便可以查看wordcount表中的内容：</p> 
<pre><code class="language-sql">mysql&gt; select * from wordcount;</code></pre> 
<p><img alt="" height="459" src="https://images2.imgbox.com/78/87/KTmqvVw0_o.png" width="200">.......<img alt="" height="449" src="https://images2.imgbox.com/de/cf/7aV2Isfd_o.png" width="180"></p> 
<h3 id="%E5%9B%9B%E3%80%81%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E9%AA%8C%E4%BD%93%E4%BC%9A" style="text-align:justify;">四、结果分析与实验体会</h3> 
<p style="margin-left:.0001pt;text-align:justify;">        Spark Streaming是一个用于实时数据处理的流式计算框架，它基于 Apache Spark 平台，提供了高可靠性、高吞吐量和容错性强等特点。在进行 Spark Streaming 编程的实验中，掌握了Spark Streaming的基本编程方法；能够利用Spark Streaming处理来自不同数据源的数据以及DStream的各种转换操作；把DStream的数据输出保存到文本文件或MySQL数据库中。<br>         理解DStream：DStream 是 Spark Streaming 的核心概念，代表连续的数据流。在编程时，我们可以通过输入源（比如 Kafka、Flume、HDFS）创建一个 DStream 对象，并对其进行转换和操作。需要注意的是，DStream 是以时间片为单位组织数据的，因此在编写代码时要考虑时间窗口的大小和滑动间隔。<br>         适当设置批处理时间间隔：批处理时间间隔决定了 Spark Streaming 处理数据的粒度，过小的时间间隔可能导致频繁的任务调度和资源开销，而过大的时间间隔则可能造成数据处理延迟。因此，在实验中需要根据具体场景和需求来选择合适的时间间隔。<br>         使用合适的转换操作：Spark Streaming 提供了丰富的转换操作，如 map、flatMap、filter、reduceByKey 等，可以实现对数据流的转换和处理。在实验中，需要根据具体业务逻辑和需求选择合适的转换操作，并合理组合这些操作，以获取期望的结果。<br>         考虑容错性和数据丢失：Spark Streaming 具备很好的容错性，可以通过记录数据流的偏移量来保证数据不会丢失。在实验中，需要注意配置合适的容错机制，确保数据处理过程中的异常情况能够被恢复，并尽量避免数据丢失。<br>         优化性能和资源利用：对于大规模的实时数据处理任务，性能和资源利用是非常重要的。在实验中，可以通过调整并行度、合理设置缓存策略、使用广播变量等手段来提高性能和资源利用效率。<br>         总的来说，Spark Streaming 是一个功能强大且易用的流式计算框架，通过合理使用其提供的特性和操作，可以实现各种实时数据处理需求。在实验中，需要深入理解其原理和机制，并根据具体需求进行合理配置和优化，以获得良好的性能和结果。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/68ac9c6fa0728c49206a0ec74db2666e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Flink 内容分享(一)：Fink原理、实战与性能优化(一)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1fcf9cd978386342662a27d6e2e421cb/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python写学生信息管理系统,python学生管理系统报告</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>