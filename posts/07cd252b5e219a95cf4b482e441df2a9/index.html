<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>大数据深度学习长短时记忆网络（LSTM）：从理论到PyTorch实战演示 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/07cd252b5e219a95cf4b482e441df2a9/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="大数据深度学习长短时记忆网络（LSTM）：从理论到PyTorch实战演示">
  <meta property="og:description" content="处理长期依赖问题 遗忘门机制梯度消失问题的缓解广泛的应用领域灵活的架构选项成熟的开源实现小结 4. LSTM的实战演示 4.1 使用PyTorch构建LSTM模型 定义LSTM模型 训练模型评估和预测 5. LSTM总结 解决长期依赖问题 广泛的应用领域灵活与强大开源支持持战与展望 总结反思 广泛的应用领域灵活与强大开源支持持战与展望总结反思 大数据深度学习长短时记忆网络（LSTM）：从理论到PyTorch实战演示 本文深入探讨了长短时记忆网络（LSTM）的核心概念、结构与数学原理，对LSTM与GRU的差异进行了对比，并通过逻辑分析阐述了LSTM的工作原理。文章还详细演示了如何使用PyTorch构建和训练LSTM模型，并突出了LSTM在实际应用中的优势。
1. LSTM的背景 人工神经网络的进化 人工神经网络（ANN）的设计灵感来源于人类大脑中神经元的工作方式。自从第一个感知器模型（Perceptron）被提出以来，人工神经网络已经经历了多次的演变和优化。
前馈神经网络（Feedforward Neural Networks）: 这是一种基本的神经网络，信息只在一个方向上流动，没有反馈或循环。卷积神经网络（Convolutional Neural Networks, CNN）: 专为处理具有类似网格结构的数据（如图像）而设计。循环神经网络（Recurrent Neural Networks, RNN）: 为了处理序列数据（如时间序列或自然语言）而引入，但在处理长序列时存在一些问题。 循环神经网络（RNN）的局限性 循环神经网络（RNN）是一种能够捕捉序列数据中时间依赖性的网络结构。但是，传统的RNN存在一些严重的问题：
梯度消失问题（Vanishing Gradient Problem）: 当处理长序列时，RNN在反向传播时梯度可能会接近零，导致训练缓慢甚至无法学习。梯度爆炸问题（Exploding Gradient Problem）: 与梯度消失问题相反，梯度可能会变得非常大，导致训练不稳定。长依赖性问题: RNN难以捕捉序列中相隔较远的依赖关系。 由于这些问题，传统的RNN在许多应用中表现不佳，尤其是在处理长序列数据时。
LSTM的提出背景 长短时记忆网络（LSTM）是一种特殊类型的RNN，由Hochreiter和Schmidhuber于1997年提出，目的是解决传统RNN的问题。
解决梯度消失问题: 通过引入“记忆单元”，LSTM能够在长序列中保持信息的流动。捕捉长依赖性: LSTM结构允许网络捕捉和理解长序列中的复杂依赖关系。广泛应用: 由于其强大的性能和灵活性，LSTM已经被广泛应用于许多序列学习任务，如语音识别、机器翻译和时间序列分析等。 LSTM的提出不仅解决了RNN的核心问题，还开启了许多先前无法解决的复杂序列学习任务的新篇章。
2. LSTM的基础理论 2.1 LSTM的数学原理 长短时记忆网络（LSTM）是一种特殊的循环神经网络，它通过引入一种称为“记忆单元”的结构来克服传统RNN的缺点。下面是LSTM的主要组件和它们的功能描述。
遗忘门（Forget Gate） 遗忘门的作用是决定哪些信息从记忆单元中遗忘。它使用sigmoid激活函数，可以输出在0到1之间的值，表示保留信息的比例。
[
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] &#43; b_f)
]
其中，(f_t)是遗忘门的输出，(\sigma)是sigmoid激活函数，(W_f)和(b_f)是权重和偏置，(h_{t-1})是上一个时间步的隐藏状态，(x_t)是当前输入。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-28T18:15:39+08:00">
    <meta property="article:modified_time" content="2024-04-28T18:15:39+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大数据深度学习长短时记忆网络（LSTM）：从理论到PyTorch实战演示</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <ul><li> 
  <ul><li> 
    <ul><li><a href="#_195" rel="nofollow">处理长期依赖问题</a> 
      <ul><li><a href="#_199" rel="nofollow">遗忘门机制</a></li><li><a href="#_203" rel="nofollow">梯度消失问题的缓解</a></li><li><a href="#_207" rel="nofollow">广泛的应用领域</a></li><li><a href="#_216" rel="nofollow">灵活的架构选项</a></li><li><a href="#_220" rel="nofollow">成熟的开源实现</a></li><li><a href="#_224" rel="nofollow">小结</a></li></ul> </li></ul> </li></ul> </li><li><a href="#4_LSTM_228" rel="nofollow">4. LSTM的实战演示</a></li><li> 
  <ul><li><a href="#41_PyTorchLSTM_230" rel="nofollow">4.1 使用PyTorch构建LSTM模型</a> 
    <ul><li> 
      <ul><li><a href="#LSTM_236" rel="nofollow">定义LSTM模型</a> 
        <ul><li><a href="#_260" rel="nofollow">训练模型</a></li><li><a href="#_294" rel="nofollow">评估和预测</a></li></ul> </li></ul> </li></ul> </li></ul> </li><li><a href="#5_LSTM_309" rel="nofollow">5. LSTM总结</a></li><li> 
  <ul><li> 
    <ul><li><a href="#_313" rel="nofollow">解决长期依赖问题</a> 
      <ul><li><a href="#_317" rel="nofollow">广泛的应用领域</a></li><li><a href="#_321" rel="nofollow">灵活与强大</a></li><li><a href="#_324" rel="nofollow">开源支持</a></li><li><a href="#_328" rel="nofollow">持战与展望</a></li><li> 
        <ul><li><a href="#_332" rel="nofollow">总结反思</a></li></ul> </li><li><a href="#_334" rel="nofollow">广泛的应用领域</a></li><li><a href="#_338" rel="nofollow">灵活与强大</a></li><li><a href="#_342" rel="nofollow">开源支持</a></li><li><a href="#_346" rel="nofollow">持战与展望</a></li><li><a href="#_350" rel="nofollow">总结反思</a></li></ul> </li></ul> </li></ul> </li></ul> 
<h3><a id="LSTMPyTorch_28"></a>大数据深度学习长短时记忆网络（LSTM）：从理论到PyTorch实战演示</h3> 
<blockquote> 
 <p>本文深入探讨了长短时记忆网络（LSTM）的核心概念、结构与数学原理，对LSTM与GRU的差异进行了对比，并通过逻辑分析阐述了LSTM的工作原理。文章还详细演示了如何使用PyTorch构建和训练LSTM模型，并突出了LSTM在实际应用中的优势。</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/29/a6/pNt1pcBh_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="1_LSTM_42"></a>1. LSTM的背景</h3> 
<h4><a id="_45"></a>人工神经网络的进化</h4> 
<p>人工神经网络（ANN）的设计灵感来源于人类大脑中神经元的工作方式。自从第一个感知器模型（Perceptron）被提出以来，人工神经网络已经经历了多次的演变和优化。</p> 
<ul><li><strong>前馈神经网络（Feedforward Neural Networks）</strong>: 这是一种基本的神经网络，信息只在一个方向上流动，没有反馈或循环。</li><li><strong>卷积神经网络（Convolutional Neural Networks, CNN）</strong>: 专为处理具有类似网格结构的数据（如图像）而设计。</li><li><strong>循环神经网络（Recurrent Neural Networks, RNN）</strong>: 为了处理序列数据（如时间序列或自然语言）而引入，但在处理长序列时存在一些问题。</li></ul> 
<h4><a id="RNN_56"></a>循环神经网络（RNN）的局限性</h4> 
<p>循环神经网络（RNN）是一种能够捕捉序列数据中时间依赖性的网络结构。但是，传统的RNN存在一些严重的问题：</p> 
<ul><li><strong>梯度消失问题（Vanishing Gradient Problem）</strong>: 当处理长序列时，RNN在反向传播时梯度可能会接近零，导致训练缓慢甚至无法学习。</li><li><strong>梯度爆炸问题（Exploding Gradient Problem）</strong>: 与梯度消失问题相反，梯度可能会变得非常大，导致训练不稳定。</li><li><strong>长依赖性问题</strong>: RNN难以捕捉序列中相隔较远的依赖关系。</li></ul> 
<p>由于这些问题，传统的RNN在许多应用中表现不佳，尤其是在处理长序列数据时。</p> 
<h4><a id="LSTM_70"></a>LSTM的提出背景</h4> 
<p>长短时记忆网络（LSTM）是一种特殊类型的RNN，由Hochreiter和Schmidhuber于1997年提出，目的是解决传统RNN的问题。</p> 
<ul><li><strong>解决梯度消失问题</strong>: 通过引入“记忆单元”，LSTM能够在长序列中保持信息的流动。</li><li><strong>捕捉长依赖性</strong>: LSTM结构允许网络捕捉和理解长序列中的复杂依赖关系。</li><li><strong>广泛应用</strong>: 由于其强大的性能和灵活性，LSTM已经被广泛应用于许多序列学习任务，如语音识别、机器翻译和时间序列分析等。</li></ul> 
<p>LSTM的提出不仅解决了RNN的核心问题，还开启了许多先前无法解决的复杂序列学习任务的新篇章。</p> 
<h3><a id="2_LSTM_84"></a>2. LSTM的基础理论</h3> 
<h4><a id="21_LSTM_87"></a>2.1 LSTM的数学原理</h4> 
<p><img src="https://images2.imgbox.com/21/77/nG0No5jY_o.png" alt="在这里插入图片描述"></p> 
<p>长短时记忆网络（LSTM）是一种特殊的循环神经网络，它通过引入一种称为“记忆单元”的结构来克服传统RNN的缺点。下面是LSTM的主要组件和它们的功能描述。</p> 
<p><img src="https://images2.imgbox.com/e8/aa/9QdudzeT_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="Forget_Gate_99"></a>遗忘门（Forget Gate）</h5> 
<p>遗忘门的作用是决定哪些信息从记忆单元中遗忘。它使用sigmoid激活函数，可以输出在0到1之间的值，表示保留信息的比例。</p> 
<p>[<br> f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)<br> ]</p> 
<p>其中，(f_t)是遗忘门的输出，(\sigma)是sigmoid激活函数，(W_f)和(b_f)是权重和偏置，(h_{t-1})是上一个时间步的隐藏状态，(x_t)是当前输入。</p> 
<h5><a id="Input_Gate_113"></a>输入门（Input Gate）</h5> 
<p>输入门决定了哪些新信息将被存储在记忆单元中。它包括两部分：sigmoid激活函数用来决定更新的部分，和tanh激活函数来生成候选值。</p> 
<p>[<br> i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)<br> ]<br> [<br> \tilde{C}<em>t = \tanh(W_C \cdot [h</em>, x_t] + b_C)<br> ]</p> 
<h5><a id="Cell_State_127"></a>记忆单元（Cell State）</h5> 
<p>记忆单元是LSTM的核心，它能够在时间序列中长时间保留信息。通过遗忘门和输入门的相互作用，记忆单元能够学习如何选择性地记住或忘记信息。</p> 
<p>[<br> C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t<br> ]</p> 
<h5><a id="Output_Gate_138"></a>输出门（Output Gate）</h5> 
<p>输出门决定了下一个隐藏状态（也即下一个时间步的输出）。首先，输出门使用sigmoid激活函数来决定记忆单元的哪些部分将输出，然后这个值与记忆单元的tanh激活的值相乘得到最终输出。</p> 
<p>[<br> o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)<br> ]<br> [<br> h_t = o_t \cdot \tanh(C_t)<br> ]</p> 
<p>LSTM通过这些精心设计的门和记忆单元实现了对信息的精确控制，使其能够捕捉序列中的复杂依赖关系和长期依赖，从而大大超越了传统RNN的性能。</p> 
<h4><a id="22_LSTM_155"></a>2.2 LSTM的结构逻辑</h4> 
<p>长短时记忆网络（LSTM）是一种特殊的循环神经网络（RNN），专门设计用于解决长期依赖问题。这些网络在时间序列数据上的性能优越，让我们深入了解其逻辑结构和运作方式。</p> 
<h5><a id="_161"></a>遗忘门：决定丢弃的信息</h5> 
<p>遗忘门决定了哪些信息从单元状态中丢弃。它考虑了当前输入和前一隐藏状态，并通过sigmoid函数输出0到1之间的值。</p> 
<h5><a id="_167"></a>输入门：选择性更新记忆单元</h5> 
<p>输入门决定了哪些新信息将存储在单元状态中。它由两部分组成：</p> 
<ul><li><strong>选择性更新</strong>：使用sigmoid函数确定要更新的部分。</li><li><strong>候选层</strong>：使用tanh函数产生新的候选值，可能添加到状态中。</li></ul> 
<h5><a id="_177"></a>更新单元状态</h5> 
<p>通过结合遗忘门的输出和输入门的输出，可以计算新的单元状态。旧状态的某些部分会被遗忘，新的候选值会被添加。</p> 
<h5><a id="_183"></a>输出门：决定输出的隐藏状态</h5> 
<p>输出门决定了从单元状态中读取多少信息来输出。这个输出将用于下一个时间步的LSTM单元，并可以用于网络的预测。</p> 
<h5><a id="_189"></a>门的相互作用</h5> 
<ul><li><strong>遗忘门</strong>: 负责控制哪些信息从单元状态中遗忘。</li><li><strong>输入门</strong>: 确定哪些新信息被存储。</li><li><strong>输出门</strong>: 控制从单元状态到隐藏状态的哪些信息流动。</li></ul> 
<p>这些门的交互允许LSTM以选择性的方式在不同时间步长的间隔中保持或丢弃信息。</p> 
<h5><a id="_200"></a>逻辑结构的实际应用</h5> 
<p>LSTM的逻辑结构使其在许多实际应用中非常有用，尤其是在需要捕捉时间序列中长期依赖关系的任务中。例如，在自然语言处理、语音识别和时间序列预测等领域，LSTM已经被证明是一种强大的模型。</p> 
<h5><a id="_206"></a>总结</h5> 
<p>LSTM的逻辑结构通过其独特的门控机制为处理具有复杂依赖关系的序列数据提供了强大的手段。其对信息流的精细控制和长期记忆的能力使其成为许多序列建模任务的理想选择。了解LSTM的这些逻辑概念有助于更好地理解其工作原理，并有效地将其应用于实际问题。</p> 
<h4><a id="23_LSTMGRU_212"></a>2.3 LSTM与GRU的对比</h4> 
<p><img src="https://images2.imgbox.com/18/b5/0Vlcw4CX_o.png" alt="在这里插入图片描述"></p> 
<p>长短时记忆网络（LSTM）和门控循环单元（GRU）都是循环神经网络（RNN）的变体，被广泛用于序列建模任务。虽然它们有许多相似之处，但也有一些关键差异。</p> 
<h5><a id="1__221"></a>1. 结构</h5> 
<h6><a id="LSTM_224"></a>LSTM</h6> 
<p>LSTM包括三个门：输入门、遗忘门和输出门，以及一个记忆单元。这些组件共同控制信息在时间序列中的流动。</p> 
<h6><a id="GRU_230"></a>GRU</h6> 
<p><img src="https://images2.imgbox.com/a3/80/ddNZQCdx_o.png" alt="在这里插入图片描述"></p> 
<p>GRU有两个门：更新门和重置门。它合并了LSTM的记忆单元和隐藏状态，并简化了结构。</p> 
<h5><a id="2__239"></a>2. 数学表达</h5> 
<h6><a id="LSTM_242"></a>LSTM</h6> 
<p>LSTM的数学表达包括以下方程：</p> 
<p>[<br> \begin{align*}<br> f_t &amp; = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)<br> i_t &amp; = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)<br> \tilde{C}*t &amp; = \tanh(W_C \cdot [h*, x_t] + b_C)<br> C_t &amp; = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}*t<br> o_t &amp; = \sigma(W_o \cdot [h*, x_t] + b_o)<br> h_t &amp; = o_t \cdot \tanh(C_t)<br> \end{align*}<br> ]</p> 
<h6><a id="GRU_260"></a>GRU</h6> 
<p>GRU的数学表达如下：</p> 
<p>[<br> \begin{align*}<br> z_t &amp; = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)<br> r_t &amp; = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)<br> n_t &amp; = \tanh(W_n \cdot [r_t \cdot h_{t-1}, x_t] + b_n)<br> h_t &amp; = (1 - z_t) \cdot n_t + z_t \cdot h_{t-1}<br> \end{align*}<br> ]</p> 
<h5><a id="3__276"></a>3. 性能和应用</h5> 
<ul><li><strong>复杂性</strong>: LSTM具有更复杂的结构和更多的参数，因此通常需要更多的计算资源。GRU则更简单和高效。</li><li><strong>记忆能力</strong>: LSTM的额外“记忆单元”可以提供更精细的信息控制，可能更适合处理更复杂的序列依赖性。</li><li><strong>训练速度和效果</strong>: 由于GRU的结构较简单，它可能在某些任务上训练得更快。但LSTM可能在具有复杂长期依赖的任务上表现更好。</li></ul> 
<h5><a id="_284"></a>小结</h5> 
<p>LSTM和GRU虽然都是有效的序列模型，但它们在结构、复杂性和应用性能方面有所不同。选择哪一个通常取决于具体任务和数据。LSTM提供了更精细的控制，而GRU可能更高效和快速。实际应用中可能需要针对具体问题进行实验以确定最佳选择。</p> 
<h3><a id="3_LSTM_290"></a>3. LSTM在实际应用中的优势</h3> 
<p><img src="https://images2.imgbox.com/8d/bd/mVed8uky_o.png" alt="在这里插入图片描述"></p> 
<p>长短时记忆网络（LSTM）是循环神经网络（RNN）的一种扩展，特别适用于序列建模和时间序列分析。LSTM的设计独具匠心，提供了一系列的优势来解决实际问题。</p> 
<h5><a id="_299"></a>处理长期依赖问题</h5> 
<p>LSTM的关键优势之一是能够捕捉输入数据中的长期依赖关系。这使其在理解和建模具有复杂时间动态的问题上具有强大的能力。</p> 
<h5><a id="_305"></a>遗忘门机制</h5> 
<p>通过遗忘门机制，LSTM能够学习丢弃与当前任务无关的信息，这对于分离重要特征和减少噪音干扰非常有用。</p> 
<h5><a id="_311"></a>梯度消失问题的缓解</h5> 
<p>传统的RNN易受梯度消失问题的影响，LSTM通过引入门机制和细胞状态来缓解这个问题。这提高了网络的训练稳定性和效率。</p> 
<h5><a id="_317"></a>广泛的应用领域</h5> 
<p>LSTM已被成功应用于许多不同的任务和领域，包括：</p> 
<ul><li><strong>自然语言处理</strong>: 如机器翻译，情感分析等。</li><li><strong>语音识别</strong>: 用于理解和转录人类语音。</li><li><strong>股票市场预测</strong>: 通过捕捉市场的时间趋势来预测股票价格。</li><li><strong>医疗诊断</strong>: 分析患者的历史医疗记录来进行早期预警和诊断。</li></ul> 
<h5><a id="_329"></a>灵活的架构选项</h5> 
<p>LSTM可以与其他深度学习组件（如卷积神经网络或注意力机制）相结合，以创建复杂且强大的模型。</p> 
<h5><a id="_335"></a>成熟的开源实现</h5> 
<p><img src="https://images2.imgbox.com/03/26/nDNUQIA8_o.png" alt="img"><br> <img src="https://images2.imgbox.com/91/e5/e4bx4WOK_o.png" alt="img"><br> <img src="https://images2.imgbox.com/df/e4/cghJo0Xa_o.png" alt="img"></p> 
<p><strong>既有适合小白学习的零基础资料，也有适合3年以上经验的小伙伴深入学习提升的进阶课程，涵盖了95%以上大数据知识点，真正体系化！</strong></p> 
<p><strong>由于文件比较多，这里只是将部分目录截图出来，全套包含大厂面经、学习笔记、源码讲义、实战项目、大纲路线、讲解视频，并且后续会持续更新</strong></p> 
<p><strong><a href="https://bbs.csdn.net/topics/618545628">需要这份系统化资料的朋友，可以戳这里获取</a></strong></p> 
<p>源实现</p> 
<p>[外链图片转存中…(img-ositNP7P-1714299319403)]<br> [外链图片转存中…(img-o6sCL3QO-1714299319404)]<br> [外链图片转存中…(img-SoPFt2Ni-1714299319404)]</p> 
<p><strong>既有适合小白学习的零基础资料，也有适合3年以上经验的小伙伴深入学习提升的进阶课程，涵盖了95%以上大数据知识点，真正体系化！</strong></p> 
<p><strong>由于文件比较多，这里只是将部分目录截图出来，全套包含大厂面经、学习笔记、源码讲义、实战项目、大纲路线、讲解视频，并且后续会持续更新</strong></p> 
<p><strong><a href="https://bbs.csdn.net/topics/618545628">需要这份系统化资料的朋友，可以戳这里获取</a></strong></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3f40e659fc2e1842d6fb2fff1a257c8c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【刷题】leetcode 1 . 两数之和</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a28e8d1fa2c4c1272d9dddd37143dddf/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Python】全面掌握 Collections Deque：队列与栈的高效实现及动态内存管理指南</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>