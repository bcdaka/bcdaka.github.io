<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ElasticSearch篇——认识、安装和使用IK分词器插件，一篇文章带你彻底拿下！ - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/9ba9a4d64fb5f5a5357cf971ad5a8a2e/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="ElasticSearch篇——认识、安装和使用IK分词器插件，一篇文章带你彻底拿下！">
  <meta property="og:description" content="一、什么是IK分词器
所谓分词，即把一段中文或者别的划分成一个个的关键字，我们在搜索时会把自己的信息进行分词，会把数据库中或者索引库中的数据进行分词，然后进行一个匹配的操作，默认的中文分词器是将每一个字看成一个词，比如“我爱中国”会被分成“我”、“爱”、“中”、“国”，这显然是不符合要求的，所以我们需要安装中文分词器IK来解决这个问题！
二、IK分词器的分词算法
1、ik__smart最少切分
2、ik_max_word最细粒度划分
三、安装IK
1、官网地址：GitHub - medcl/elasticsearch-analysis-ik: The IK Analysis plugin integrates Lucene IK analyzer into elasticsearch, support customized dictionary.
2、下载完毕之后，放入我们的elasticsearch的插件文件夹中
3、重启观察Elasticsearch
可以看到我们es启动的时候，加载了刚刚解压的ik插件 4、也可以通过ES的bin下的elasticsearch-plugin.bat查看安装的插件
（1）在bin目录下，打开cmd输入命令
elasticsearch-plugin list （2）可以查看当前安装的插件有哪些
5、使用kibana测试
（1）ik__smart最少切分：根据字典的配置，尽量少的对文档进行拆分
GET _analyze:这是通过RestFul风格请求分词器
“analyzer”：表示请求的分词要求（选择最少切分还是最细粒度！）
“text”:表示要分词的文本
（2）ik_max_word最细粒度划分：根据词库中的字典，最大程度的对文档内容进行拆分
四、配置IK分词器字典
假如我想对以下的内容进行分词，我不希望“长江东逝水”分开，这个时候就需要我们自己去配置一下属于我们自己的字典了
配置方法：
1、打开IK分词器的config配置文件夹
2、查看IKAnalyzer.cfg.xml配置文件
3、在目录下新建一个文件，后缀为.dic，把我们想配置的字典内容填进去
如果我们不希望把某个词给我拆开了，那么我就可以通过编写自己的配置字典，把文本输入进去，绑定到IKAnalyzer.cfg.xml配置文件中即可！
4、把我们自己配置的字典注入进来
5、重启ES和kibana
（1）观察ES细节，我们自己编写的字典被加载进来了！
（2）重启kibana即可
6、测试重新使用最少切分的算法，测试“滚滚长江东逝水”，是否把我们的“长江东逝水”拼接在一起了
成功！
将来我们所有需要自己配置特定的分词效果，直接在我们自己定义的dic文件中进行配置即可！！！！！！记得绑定到IKAnalyzer.cfg.xml配置文件中哦
至此，恭喜你已经掌握了安装和使用IK分词器，掌握了分词的基本规则。后续还会持续更新关于ES相关技术点，敬清期待~~~">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-12-09T14:11:48+08:00">
    <meta property="article:modified_time" content="2023-12-09T14:11:48+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ElasticSearch篇——认识、安装和使用IK分词器插件，一篇文章带你彻底拿下！</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>一、什么是IK分词器<br> 所谓分词，即把一段中文或者别的划分成一个个的关键字，我们在搜索时会把自己的信息进行分词，会把数据库中或者索引库中的数据进行分词，然后进行一个匹配的操作，默认的中文分词器是将每一个字看成一个词，比如“我爱中国”会被分成“我”、“爱”、“中”、“国”，这显然是不符合要求的，所以我们需要安装中文分词器IK来解决这个问题！</p> 
<p>二、IK分词器的分词算法<br> 1、ik__smart最少切分</p> 
<p>2、ik_max_word最细粒度划分</p> 
<p>三、安装IK<br> 1、官网地址：<a href="https://github.com/medcl/elasticsearch-analysis-ik" title="GitHub - medcl/elasticsearch-analysis-ik: The IK Analysis plugin integrates Lucene IK analyzer into elasticsearch, support customized dictionary.">GitHub - medcl/elasticsearch-analysis-ik: The IK Analysis plugin integrates Lucene IK analyzer into elasticsearch, support customized dictionary.</a></p> 
<p>2、下载完毕之后，放入我们的elasticsearch的插件文件夹中</p> 
<p><img alt="" height="244" src="https://images2.imgbox.com/12/0b/w7j6yXGo_o.png" width="422"></p> 
<p>3、重启观察Elasticsearch<br> 可以看到我们es启动的时候，加载了刚刚解压的ik插件 <img alt="" height="63" src="https://images2.imgbox.com/6a/fa/twY5pHGg_o.png" width="933"></p> 
<p>4、也可以通过ES的bin下的elasticsearch-plugin.bat查看安装的插件<br> （1）在bin目录下，打开cmd输入命令</p> 
<pre><code class="hljs">elasticsearch-plugin list</code></pre> 
<p>（2）可以查看当前安装的插件有哪些</p> 
<p><img alt="" height="73" src="https://images2.imgbox.com/b7/e5/enNBTzoS_o.png" width="1196"></p> 
<p>5、使用kibana测试<br> （1）ik__smart最少切分：根据字典的配置，尽量少的对文档进行拆分<br> GET _analyze:这是通过RestFul风格请求分词器<br> “analyzer”：表示请求的分词要求（选择最少切分还是最细粒度！）<br> “text”:表示要分词的文本</p> 
<p><img alt="" height="200" src="https://images2.imgbox.com/81/d2/O7i0Xddx_o.png" width="1195"></p> 
<p>（2）ik_max_word最细粒度划分：根据词库中的字典，最大程度的对文档内容进行拆分</p> 
<p><img alt="" height="783" src="https://images2.imgbox.com/15/1b/OvAFX34i_o.png" width="1184"></p> 
<p>四、配置IK分词器字典<br> 假如我想对以下的内容进行分词，我不希望“长江东逝水”分开，这个时候就需要我们自己去配置一下属于我们自己的字典了</p> 
<p><img alt="" height="458" src="https://images2.imgbox.com/b1/87/ndzbIlpQ_o.png" width="1135"></p> 
<p>配置方法：<br> 1、打开IK分词器的config配置文件夹</p> 
<p><img alt="" height="307" src="https://images2.imgbox.com/04/28/laTubObJ_o.png" width="344"></p> 
<p>2、查看IKAnalyzer.cfg.xml配置文件</p> 
<p><img alt="" height="304" src="https://images2.imgbox.com/29/7b/Bx5gDdxD_o.png" width="715"></p> 
<p>3、在目录下新建一个文件，后缀为.dic，把我们想配置的字典内容填进去<br><span style="color:#fe2c24;"><strong>如果我们不希望把某个词给我拆开了，那么我就可以通过编写自己的配置字典，把文本输入进去，绑定到IKAnalyzer.cfg.xml配置文件中即可！</strong></span></p> 
<p><img alt="" height="36" src="https://images2.imgbox.com/04/97/TWOTW4T4_o.png" width="136"></p> 
<p>4、把我们自己配置的字典注入进来</p> 
<p><img alt="" height="259" src="https://images2.imgbox.com/a4/e8/RkdFNqUU_o.png" width="708"></p> 
<p>5、重启ES和kibana<br> （1）观察ES细节，我们自己编写的字典被加载进来了！</p> 
<p><img alt="" height="109" src="https://images2.imgbox.com/c9/5c/v6Q1d8wu_o.png" width="1200"></p> 
<p>（2）重启kibana即可</p> 
<p>6、测试重新使用最少切分的算法，测试“滚滚长江东逝水”，是否把我们的“长江东逝水”拼接在一起了<br> 成功！</p> 
<p><img alt="" height="335" src="https://images2.imgbox.com/96/ec/gDvVmtxe_o.png" width="1182"></p> 
<p>将来我们所有需要自己配置特定的分词效果，直接在我们自己定义的dic文件中进行配置即可！！！！！！记得绑定到IKAnalyzer.cfg.xml配置文件中哦</p> 
<p></p> 
<p>至此，恭喜你已经掌握了安装和使用IK分词器，掌握了分词的基本规则。后续还会持续更新关于ES相关技术点，敬清期待~~~</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/263ccc0d5865f118fb3a0d52e51faded/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">大数据实验 实验七：Flink初级编程实践</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0ee59a5fcb4a94ff4e2ae1f6f171702b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">idea创建jsp项目,并连接数据库(SQL server)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>