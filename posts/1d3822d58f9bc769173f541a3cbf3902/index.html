<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（十） 使用 LoRA 微调常见问题答疑 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/1d3822d58f9bc769173f541a3cbf3902/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（十） 使用 LoRA 微调常见问题答疑">
  <meta property="og:description" content="LlaMA 3 系列博客 基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （一）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （二）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （三）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （四）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （五）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （六）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （七）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （八）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （九）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （十）">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-28T19:30:00+08:00">
    <meta property="article:modified_time" content="2024-05-28T19:30:00+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（十） 使用 LoRA 微调常见问题答疑</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="LlaMA_3__0"></a>LlaMA 3 系列博客</h2> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138713807?spm=1001.2014.3001.5501">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （一）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138715041?spm=1001.2014.3001.5501">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （二）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138718088?spm=1001.2014.3001.5501">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （三）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138719654?spm=1001.2014.3001.5501">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （四）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138790855" rel="nofollow">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （五）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138791888" rel="nofollow">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （六）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138794155" rel="nofollow">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （七）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138801566" rel="nofollow">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （八）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138802610" rel="nofollow">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （九）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138805410?spm=1001.2014.3001.5501">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （十）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138895944" rel="nofollow">构建安全的GenAI/LLMs核心技术解密之大模型对抗攻击（一）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138904869" rel="nofollow">构建安全的GenAI/LLMs核心技术解密之大模型对抗攻击（二）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138906078" rel="nofollow">构建安全的GenAI/LLMs核心技术解密之大模型对抗攻击（三）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138907964?spm=1001.2014.3001.5501">构建安全的GenAI/LLMs核心技术解密之大模型对抗攻击（四）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138942598" rel="nofollow">构建安全的GenAI/LLMs核心技术解密之大模型对抗攻击（五）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138891014?spm=1001.2014.3001.5501">你好 GPT-4o！</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138924149?spm=1001.2014.3001.5501">大模型标记器之Tokenizer可视化（GPT-4o）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138972848?spm=1001.2014.3001.5501">大模型标记器 Tokenizer之Byte Pair Encoding (BPE) 算法详解与示例</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138973896?spm=1001.2014.3001.5501">大模型标记器 Tokenizer之Byte Pair Encoding (BPE)源码分析</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138975453?spm=1001.2014.3001.5501">大模型之自注意力机制Self-Attention（一）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138975753?spm=1001.2014.3001.5501">大模型之自注意力机制Self-Attention（二）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138976281?spm=1001.2014.3001.5501">大模型之自注意力机制Self-Attention（三）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138948293" rel="nofollow">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （十一）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138998916" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之 Code Llama （一）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139002038" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之 Code Llama （二）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139002334" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之 Code Llama （三）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139003284" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之 Code Llama （四）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139003246" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之 Code Llama （五）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139005323" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之使用 Llama Guard 保护大模型对话（一）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139006237" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之使用 Llama Guard 保护大模型对话（二）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139002334">Llama 3 模型家族构建安全可信赖企业级AI应用之使用 Llama Guard 保护大模型对话（三）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139016349?spm=1001.2014.3001.5501">大模型之深入理解Transformer位置编码（Positional Embedding）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139026612?spm=1001.2014.3001.5501">大模型之深入理解Transformer Layer Normalization（一）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139028322" rel="nofollow">大模型之深入理解Transformer Layer Normalization（二）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139031974?spm=1001.2014.3001.5501">大模型之深入理解Transformer Layer Normalization（三）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139034624?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（一）初学者的起点</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139035105?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（二）矩阵操作的演练 </a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139035270?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（三）初始化一个嵌入层 </a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139035455?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（四）预先计算 RoPE 频率 </a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139038167?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（五）预先计算因果掩码 </a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139039119?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（六）首次归一化：均方根归一化（RMSNorm）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139039795?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（七） 初始化多查询注意力</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139044003?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（八）旋转位置嵌入</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139044601?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（九） 计算自注意力</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139047106?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（十） 残差连接及SwiGLU FFN</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139047892?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（十一）输出概率分布 及损失函数计算</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139048668?spm=1001.2014.3001.5501">大模型之使用PyTorch编写Meta的Llama 3实际功能代码（一）加载简化分词器及设置参数</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139055000" rel="nofollow">大模型之使用PyTorch编写Meta的Llama 3实际功能代码（二）RoPE 及注意力机制</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139058555" rel="nofollow">大模型之使用PyTorch编写Meta的Llama 3实际功能代码（三） FeedForward 及 Residual Layers</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139061993" rel="nofollow">大模型之使用PyTorch编写Meta的Llama 3实际功能代码（四） 构建 Llama3 类模型本身</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139062160" rel="nofollow">大模型之使用PyTorch编写Meta的Llama 3实际功能代码（五）训练并测试你自己的 minLlama3</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139062700" rel="nofollow">大模型之使用PyTorch编写Meta的Llama 3实际功能代码（六）加载已经训练好的miniLlama3模型</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139063356" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之使用 Llama Guard 保护大模型对话 （四）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139063994" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之使用 Llama Guard 保护大模型对话 （五）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139065219" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之使用 Llama Guard 保护大模型对话 （六）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139066008" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之使用 Llama Guard 保护大模型对话 （七）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139066714?spm=1001.2014.3001.5501">Llama 3 模型家族构建安全可信赖企业级AI应用之使用 Llama Guard 保护大模型对话 （八）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139080509" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之 CyberSecEval 2：量化 LLM 安全和能力的基准（一）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139089252" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之 CyberSecEval 2：量化 LLM 安全和能力的基准（二）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139089533" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之 CyberSecEval 2：量化 LLM 安全和能力的基准（三）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139090860?spm=1001.2014.3001.5501">Llama 3 模型家族构建安全可信赖企业级AI应用之 CyberSecEval 2：量化 LLM 安全和能力的基准（四）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139140454" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之code shield（一）Code Shield简介</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139141252" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之code shield（二）防止 LLM 生成不安全代码</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139142244" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之code shield（三）Code Shield代码示例</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139144847" rel="nofollow">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（一） LLaMA-Factory简介</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139148621?spm=1001.2014.3001.5501">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（二） LLaMA-Factory训练方法及数据集</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139163567" rel="nofollow">大模型之Ollama：在本地机器上释放大型语言模型的强大功能</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139171614" rel="nofollow">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（三）通过Web UI微调</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139172082" rel="nofollow">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（四）通过命令方式微调</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139173930" rel="nofollow">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（五） 基于已训练好的模型进行推理</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139175166?spm=1001.2014.3001.5501">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（六）Llama 3 已训练的大模型合并LoRA权重参数</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139238948" rel="nofollow">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（七） 使用 LoRA 微调 LLM 的实用技巧</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139239746" rel="nofollow">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（八） 使用 LoRA 微调 LLM 的实用技巧</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139240891" rel="nofollow">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（九） 使用 LoRA 微调常见问题答疑</a></p> 
<h2><a id="Llama_Supervised_FineTuningSFTLlama_3___LoRA__164"></a>Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（十） 使用 LoRA 微调常见问题答疑</h2> 
<h2><a id="Q5__166"></a>Q5: 如何避免过拟合？</h2> 
<p>通常，较大的 r 值可能导致更多的过拟合，因为它决定了可训练参数的数量。如果模型出现过度拟合，减小 r 值或增加数据集大小是首先需要探索的选项。此外，你可以尝试增加AdamW或SGD优化器中的权重衰减率，并且可以考虑增加LoRA层的丢弃值dropout 。</p> 
<p>尚未在实验中探索的LoRA丢弃dropout 参数（使用了固定的0.05丢弃率），这是未来研究的一个有趣话题。</p> 
<h2><a id="Q6_171"></a>Q6：其他优化器怎么样？</h2> 
<p>其他有趣的 LLM 优化器值得未来探索。其中一个优化器是5 月份发布的<a href="https://arxiv.org/abs/2305.14342" rel="nofollow">Sophia：用于语言模型预训练的可扩展随机二阶优化器</a>。<br> <img src="https://images2.imgbox.com/53/76/jt84GVWF_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/8f/b9/W7cGUyGB_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b6/28/gfjNOIp5_o.png" alt="在这里插入图片描述"></p> 
<p>Sophia 是一种二阶优化算法，有望对 LLM 特别有吸引力，因为在 LLM 中 Adam 和 AdamW 通常占主导地位。据该论文称，与 Adam 相比，Sophia 的速度快了 2 倍，使用 Sophia 训练的模型可以获得更好的建模性能。简而言之，Sophia 通过梯度曲率而不是像 Adam 那样通过梯度方差来规范梯度。</p> 
<h2><a id="Q7_183"></a>Q7：还有哪些因素影响内存使用？</h2> 
<p>除了精度和量化设置、模型大小、批量大小和可训练的 LoRA 参数数量之外，数据集也会影响内存使用情况。</p> 
<p>请注意，Llama 2 的块大小为 4048。例如，如果 LLM 的块大小为 4048 个 token，则它可以一次处理最多 4048 个 token 的序列。但是，较短的训练序列可以节省大量内存，因为可以屏蔽未来的 token。</p> 
<p>例如，Alpaca 数据集相对较小，最大长度为 1304 个标记。</p> 
<p><img src="https://images2.imgbox.com/88/6a/HamnZ5vv_o.png" alt="在这里插入图片描述"></p> 
<p>当 尝试长度高达 2048 个标记的其他数据集时， 注意到内存使用量从 17.86 GB 增加到了 26.96 GB。</p> 
<h2><a id="Q8_RLHF__194"></a>Q8：与完全微调和 RLHF 相比如何？</h2> 
<p>没有进行任何 RLHF 实验（对于那些好奇的人，<a href="https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives" rel="nofollow">在这里介绍了 RLHF</a> ），但确实考虑过完全微调。完全微调至少需要 2 个 GPU，并在 3.5 小时内完成，每个 GPU 使用 36.66 GB。然而，基准测试结果不是很好，可能是由于过度拟合或超参数不理想。<br> <img src="https://images2.imgbox.com/9c/3d/dnrSuz7r_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/8f/d5/7EKOz4AW_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/6e/b5/VwjK0bbP_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/8f/6c/f8SJwHc0_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/52/22/KvLAg3E6_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/78/ba/02KhaEMV_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/86/7b/6W88FDyO_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Llama_2_RLHF_215"></a>Llama 2 中的RLHF</h3> 
<p><img src="https://images2.imgbox.com/c0/39/lA1NfffR_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/ca/1f/WNeUOAlK_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/d2/2e/DsNHUSGI_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/9d/fd/4MvcvZex_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="RLHF__225"></a>RLHF 替代方案</h2> 
<p><img src="https://images2.imgbox.com/ca/68/X3dY3mi2_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/f1/33/kt3FxbXf_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/37/dd/nkjSK0Gc_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/51/c5/AnDtJA8Z_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/d6/78/MNaKcVzR_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_9LoRA__236"></a>问题 9：LoRA 权重可以合并吗？</h2> 
<p>是的，可以组合多组 LoRA 权重。在训练期间， 将 LoRA 权重与预训练权重分开，并在每次前向传递期间添加它们。</p> 
<p>但是，如果 实际应用程序包含多组 LoRA 权重，例如，每个应用程序客户一组，则最好单独存储这些权重以节省磁盘空间。但是，可以在训练后将预训练权重与 LoRA 权重合并以创建单个模型。这样， 就不必在每次前向传递中应用 LoRA 权重：</p> 
<pre><code class="prism language-css">weight += <span class="token punctuation">(</span>lora_B @ lora_A<span class="token punctuation">)</span> * scaling
</code></pre> 
<p>所示的权重更新并保存合并（添加）的权重。</p> 
<p>类似地， 可以继续添加多个 LoRA 权重集：</p> 
<pre><code class="prism language-css">weight += <span class="token punctuation">(</span>lora_B_set1 @ lora_A_set1<span class="token punctuation">)</span> * scaling_set1
weight += <span class="token punctuation">(</span>lora_B_set2 @ lora_A_set2<span class="token punctuation">)</span> * scaling_set2
weight += <span class="token punctuation">(</span>lora_B_set3 @ lora_A_set3<span class="token punctuation">)</span> * scaling_set3
...
</code></pre> 
<p>还没有做过实验来评估这种方法的性能，但从技术上讲，这已经可以通过Lit-GPT 中提供的<a href="https://github.com/Lightning-AI/litgpt/blob/main/litgpt/scripts/merge_lora.py">scripts/merge_lora.py</a>脚本来实现。</p> 
<pre><code class="prism language-css"># Copyright Lightning AI. Licensed under the Apache License 2.0<span class="token punctuation">,</span> see LICENSE file.

<span class="token string">""</span><span class="token string">"This script merges the LoRA weights with the base model"</span><span class="token string">""</span>
from pathlib import Path
from typing import Any<span class="token punctuation">,</span> Dict<span class="token punctuation">,</span> Optional<span class="token punctuation">,</span> Tuple

import lightning as L
import torch
import yaml

from litgpt.lora import GPT<span class="token punctuation">,</span> Config<span class="token punctuation">,</span> lora_filter<span class="token punctuation">,</span> merge_lora_weights
from litgpt.utils import CLI<span class="token punctuation">,</span> check_valid_checkpoint_dir


def merge_<span class="token function">lora</span><span class="token punctuation">(</span>
    <span class="token property">checkpoint_dir</span><span class="token punctuation">:</span> Path<span class="token punctuation">,</span> <span class="token property">pretrained_checkpoint_dir</span><span class="token punctuation">:</span> Optional[Path] = None<span class="token punctuation">,</span> <span class="token property">precision</span><span class="token punctuation">:</span> Optional[str] = None
<span class="token punctuation">)</span> -&gt; <span class="token property">None</span><span class="token punctuation">:</span>
    <span class="token string">""</span>"Merges the LoRA weights with the base model. See ``litgpt finetune lora``.

    Creates a new ``lit_model.pth`` file by merging the LoRA weights <span class="token punctuation">(</span>``lit_model.pth.lora``<span class="token punctuation">)</span>
    with the original checkpoint weights.

    <span class="token property">Args</span><span class="token punctuation">:</span>
        <span class="token property">checkpoint_dir</span><span class="token punctuation">:</span> Path to the checkpoint directory with trained LoRA weights<span class="token punctuation">,</span> which is the output of
            ``litgpt finetune lora``.
        <span class="token property">pretrained_checkpoint_dir</span><span class="token punctuation">:</span> Optional path to the checkpoint directory with the weights of the base model
            corresponding to the LoRA checkpoint. By default<span class="token punctuation">,</span> this will automatically be inferred from the metadata
            in the given `checkpoint_dir` directory. Only set this if the base model's checkpoint directory
            has moved or was renamed.
        <span class="token property">precision</span><span class="token punctuation">:</span> Optional precision setting to instantiate the model weights in. By default<span class="token punctuation">,</span> this will
            automatically be inferred from the metadata in the given ``checkpoint_dir`` directory.
    <span class="token string">""</span>"
    check_valid_checkpoint_<span class="token function">dir</span><span class="token punctuation">(</span>checkpoint_dir<span class="token punctuation">,</span> model_filename=<span class="token string">"lit_model.pth.lora"</span><span class="token punctuation">)</span>
    if pretrained_checkpoint_dir is not <span class="token property">None</span><span class="token punctuation">:</span>
        check_valid_checkpoint_<span class="token function">dir</span><span class="token punctuation">(</span>pretrained_checkpoint_dir<span class="token punctuation">)</span>
    if <span class="token punctuation">(</span>checkpoint_dir / <span class="token string">"lit_model.pth"</span><span class="token punctuation">)</span>.is_<span class="token function">file</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token function">print</span><span class="token punctuation">(</span><span class="token string">"LoRA weights have already been merged in this checkpoint."</span><span class="token punctuation">)</span>
        return

    lora_params<span class="token punctuation">,</span> pretrained_checkpoint_dir<span class="token punctuation">,</span> lora_precision = load_lora_<span class="token function">metadata</span><span class="token punctuation">(</span>checkpoint_dir<span class="token punctuation">)</span>
    precision = precision if precision is not None else lora_precision

    fabric = L.<span class="token function">Fabric</span><span class="token punctuation">(</span>devices=1<span class="token punctuation">,</span> precision=precision<span class="token punctuation">,</span> accelerator=<span class="token string">"cpu"</span><span class="token punctuation">)</span>
    config = Config.from_<span class="token function">file</span><span class="token punctuation">(</span>checkpoint_dir / <span class="token string">"model_config.yaml"</span><span class="token punctuation">,</span> **lora_params<span class="token punctuation">)</span>

    with fabric.init_<span class="token function">module</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch.<span class="token function">device</span><span class="token punctuation">(</span><span class="token string">"meta"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        model = <span class="token function">GPT</span><span class="token punctuation">(</span>config<span class="token punctuation">)</span>
        # we don't <span class="token selector">care about these to perform merging
        model.cos = None
        model.sin = None

    lora_path = checkpoint_dir / "lit_model.pth.lora"
    pretrained_checkpoint = torch.load(str(pretrained_checkpoint_dir / "lit_model.pth"), mmap=True)
    lora_checkpoint = torch.load(str(lora_path), mmap=True)
    lora_checkpoint = lora_checkpoint.get("model", lora_checkpoint)

    # Merge LoRA weights into the base model
    pretrained_checkpoint.update(lora_checkpoint)
    model.load_state_dict(pretrained_checkpoint, assign=True)
    # since LoRA finetuning only saves the LoRA weights, we treat the lora weights dtype as the expected dtype
    lora_dtype = next(iter(lora_checkpoint.values())).dtype
    model.to(dtype=lora_dtype, device="cpu")
    merge_lora_weights(model)

    # Remove LoRA parameters and the LoRA linear substring
    state_dict =</span> <span class="token punctuation">{<!-- --></span>k.<span class="token function">replace</span><span class="token punctuation">(</span><span class="token string">"linear."</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">:</span> v for k<span class="token punctuation">,</span> v in model.state_<span class="token function">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>.<span class="token function">items</span><span class="token punctuation">(</span><span class="token punctuation">)</span> if not lora_<span class="token function">filter</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> v<span class="token punctuation">)</span><span class="token punctuation">}</span>
    <span class="token selector">save_path = checkpoint_dir / "lit_model.pth"
    torch.save(state_dict, save_path)

    fabric.print(f"Saved merged weights to {str(checkpoint_dir / 'lit_model.pth')!r}")


def load_lora_metadata(checkpoint_dir: Path) -&gt; Tuple[Dict[str, Any], Path, Optional[str]]:
    hparams_file = checkpoint_dir / "hyperparameters.yaml"
    if not hparams_file.is_file():
        raise FileNotFoundError(
            f"The path {str(hparams_file)!r} is not a valid checkpoint directory. It is missing a"
            f" `hyperparameters.yaml` file. Please point to the checkpoint directory that was produced by"
            f" the `litgpt/finetune/lora.py` script."
        )

    with open(hparams_file, "r", encoding="utf-8") as file:
        hparams = yaml.safe_load(file)

    lora_params =</span> <span class="token punctuation">{<!-- --></span><span class="token property">k</span><span class="token punctuation">:</span> v for k<span class="token punctuation">,</span> v in hparams.<span class="token function">items</span><span class="token punctuation">(</span><span class="token punctuation">)</span> if k.<span class="token function">startswith</span><span class="token punctuation">(</span><span class="token string">"lora_"</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
    pretrained_checkpoint_dir = <span class="token function">Path</span><span class="token punctuation">(</span>hparams[<span class="token string">"checkpoint_dir"</span>]<span class="token punctuation">)</span>
    precision = hparams.<span class="token function">get</span><span class="token punctuation">(</span><span class="token string">"precision"</span><span class="token punctuation">)</span>
    return lora_params<span class="token punctuation">,</span> pretrained_checkpoint_dir<span class="token punctuation">,</span> precision
</code></pre> 
<p>这段代码是一个Python脚本，用于将LoRA（Low-Rank Adaptation）权重与基础模型合并。以下是代码的详细说明：</p> 
<ol><li> <p><strong>导入必要的库</strong>：</p> 
  <ul><li><code>pathlib.Path</code> 用于文件路径操作。</li><li><code>typing</code> 中的 <code>Any</code>, <code>Dict</code>, <code>Optional</code>, <code>Tuple</code> 用于类型注解。</li><li><code>lightning as L</code> 是一个用于简化PyTorch模型训练的库。</li><li><code>torch</code> 是PyTorch库，用于深度学习。</li><li><code>yaml</code> 用于读取YAML配置文件。</li><li><code>litgpt.lora</code> 中包含LoRA相关的类和函数。</li><li><code>litgpt.utils</code> 中包含一些实用工具。</li></ul> </li><li> <p><strong>定义 <code>merge_lora</code> 函数</strong>：</p> 
  <ul><li>参数： 
    <ul><li><code>checkpoint_dir</code>：包含训练好的LoRA权重的目录路径。</li><li><code>pretrained_checkpoint_dir</code>：可选参数，基础模型权重的目录路径。</li><li><code>precision</code>：可选参数，用于指定模型权重的精度。</li></ul> </li><li>功能：合并LoRA权重与基础模型权重，创建新的 <code>lit_model.pth</code> 文件。</li></ul> </li><li> <p><strong>检查目录有效性</strong>：</p> 
  <ul><li>使用 <code>check_valid_checkpoint_dir</code> 函数检查 <code>checkpoint_dir</code> 是否包含 <code>lit_model.pth.lora</code> 文件。</li></ul> </li><li> <p><strong>加载LoRA元数据</strong>：</p> 
  <ul><li>调用 <code>load_lora_metadata</code> 函数，从 <code>hyperparameters.yaml</code> 文件中加载LoRA参数、预训练模型的目录和精度设置。</li></ul> </li><li> <p><strong>初始化模型</strong>：</p> 
  <ul><li>使用 <code>L.Fabric</code> 初始化模型训练环境。</li><li>从 <code>model_config.yaml</code> 文件加载模型配置，并创建 <code>GPT</code> 模型实例。</li></ul> </li><li> <p><strong>加载权重</strong>：</p> 
  <ul><li>加载预训练模型权重和LoRA权重。</li></ul> </li><li> <p><strong>合并权重</strong>：</p> 
  <ul><li>将LoRA权重合并到预训练模型权重中。</li></ul> </li><li> <p><strong>更新模型状态字典</strong>：</p> 
  <ul><li>移除LoRA参数和线性子字符串，更新模型的状态字典。</li></ul> </li><li> <p><strong>保存合并后的权重</strong>：</p> 
  <ul><li>将更新后的模型状态字典保存为 <code>lit_model.pth</code> 文件。</li></ul> </li><li> <p><strong>定义 <code>load_lora_metadata</code> 函数</strong>：</p> 
  <ul><li>参数：<code>checkpoint_dir</code>。</li><li>功能：从 <code>hyperparameters.yaml</code> 文件中加载LoRA参数、预训练模型的目录和精度设置。</li><li>返回值：一个包含LoRA参数、预训练模型目录和精度设置的元组。</li></ul> </li></ol> 
<p>整体来看，这段代码是一个自动化脚本，用于将LoRA权重与基础模型权重合并，以便在微调后使用。它通过读取配置文件、加载权重、合并权重和保存新模型权重的步骤来实现这一过程。</p> 
<h2><a id="Q10_393"></a>Q10：逐层最优秩自适应怎么样？</h2> 
<p>为了简单起见，我们通常训练深度神经网络时，每一层都使用相同的学习率，而学习率就是我们需要优化的超参数。更进一步说，我们还可以为每个层选择不同的学习率（在 PyTorch 中，这并不太复杂）。然而，在实践中很少这样做，因为它会增加额外的开销，而且在训练深度神经网络时通常已经有太多的参数需要调整。</p> 
<p>类似于为不同的层选择不同的学习率， 也可以为不同的层选择不同的 LoRA 等级。我还没有找到任何关于此的实验，但详细介绍此方法的文档是“逐层最佳等级自适应”（也缩写为 LORA）。从理论上讲，这在实践中听起来是个好主意。然而，它也增加了优化超参数时的大量选择。</p> 
<p>这本杂志是个人热情项目，不提供直接报酬。但是，对于那些希望支持作者的人，请考虑购买<a href="https://sebastianraschka.com/books" rel="nofollow">作者的一本书</a>。如果您发现它们有见地且有益，请随时将它们推荐给您的朋友和同事。</p> 
<h2><a id="Gavin_400"></a>如果大家感兴趣，也欢迎购买Gavin大咖的系列新书。</h2> 
<p><img src="https://images2.imgbox.com/e7/59/DPdOUOLZ_o.jpg" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/44/cf/OEdn5Mw0_o.jpg" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/6b/50/W3Gfhdo0_o.jpg" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/3b/75/aU4GVkTZ_o.jpg" alt="在这里插入图片描述"></p> 
<h2><a id="_408"></a>大模型技术分享</h2> 
<p><img src="https://images2.imgbox.com/70/83/w3shCC0x_o.jpg" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/b4/a8/nq04Dpe6_o.jpg" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/66/b7/9XWvjZA2_o.jpg" alt="在这里插入图片描述"></p> 
<h2><a id="LLM_416"></a>《企业级生成式人工智能LLM大模型技术、算法及案例实战》线上高级研修讲座</h2> 
<pre><code>模块一：Generative AI 原理本质、技术内核及工程实践周期详解
模块二：工业级 Prompting 技术内幕及端到端的基于LLM 的会议助理实战
模块三：三大 Llama 2 模型详解及实战构建安全可靠的智能对话系统
模块四：生产环境下 GenAI/LLMs 的五大核心问题及构建健壮的应用实战
模块五：大模型应用开发技术：Agentic-based 应用技术及案例实战
模块六：LLM 大模型微调及模型 Quantization 技术及案例实战
模块七：大模型高效微调 PEFT 算法、技术、流程及代码实战进阶
模块八：LLM 模型对齐技术、流程及进行文本Toxicity 分析实战
模块九：构建安全的 GenAI/LLMs 核心技术Red Teaming 解密实战
模块十：构建可信赖的企业私有安全大模型Responsible AI 实战 
</code></pre> 
<h2><a id="Llama3Responsible_AI_429"></a>Llama3关键技术深度解析与构建Responsible AI、算法及开发落地实战</h2> 
<p>1、Llama开源模型家族大模型技术、工具和多模态详解：学员将深入了解Meta Llama 3的创新之处，比如其在语言模型技术上的突破，并学习到如何在Llama 3中构建trust and safety AI。他们将详细了解Llama 3的五大技术分支及工具，以及如何在AWS上实战Llama指令微调的案例。<br> 2、解密Llama 3 Foundation Model模型结构特色技术及代码实现：深入了解Llama 3中的各种技术，比如Tiktokenizer、KV Cache、Grouped Multi-Query Attention等。通过项目二逐行剖析Llama 3的源码，加深对技术的理解。<br> 3、解密Llama 3 Foundation Model模型结构核心技术及代码实现：SwiGLU Activation Function、FeedForward Block、Encoder Block等。通过项目三学习Llama 3的推理及Inferencing代码，加强对技术的实践理解。<br> 4、基于LangGraph on Llama 3构建Responsible AI实战体验：通过项目四在Llama 3上实战基于LangGraph的Responsible AI项目。他们将了解到LangGraph的三大核心组件、运行机制和流程步骤，从而加强对Responsible AI的实践能力。<br> 5、Llama模型家族构建技术构建安全可信赖企业级AI应用内幕详解：深入了解构建安全可靠的企业级AI应用所需的关键技术，比如Code Llama、Llama Guard等。项目五实战构建安全可靠的对话智能项目升级版，加强对安全性的实践理解。<br> 6、Llama模型家族Fine-tuning技术与算法实战：学员将学习Fine-tuning技术与算法，比如Supervised Fine-Tuning(SFT)、Reward Model技术、PPO算法、DPO算法等。项目六动手实现PPO及DPO算法，加强对算法的理解和应用能力。<br> 7、Llama模型家族基于AI反馈的强化学习技术解密：深入学习Llama模型家族基于AI反馈的强化学习技术，比如RLAIF和RLHF。项目七实战基于RLAIF的Constitutional AI。<br> 8、Llama 3中的DPO原理、算法、组件及具体实现及算法进阶：学习Llama 3中结合使用PPO和DPO算法，剖析DPO的原理和工作机制，详细解析DPO中的关键算法组件，并通过综合项目八从零开始动手实现和测试DPO算法，同时课程将解密DPO进阶技术Iterative DPO及IPO算法。<br> 9、Llama模型家族Safety设计与实现：在这个模块中，学员将学习Llama模型家族的Safety设计与实现，比如Safety in Pretraining、Safety Fine-Tuning等。构建安全可靠的GenAI/LLMs项目开发。<br> 10、Llama 3构建可信赖的企业私有安全大模型Responsible AI系统：构建可信赖的企业私有安全大模型Responsible AI系统，掌握Llama 3的Constitutional AI、Red Teaming。</p> 
<h2><a id="Sora_442"></a>解码Sora架构、技术及应用</h2> 
<p>一、为何Sora通往AGI道路的里程碑？<br> 1，探索从大规模语言模型(LLM)到大规模视觉模型(LVM)的关键转变，揭示其在实现通用人工智能(AGI)中的作用。<br> 2，展示Visual Data和Text Data结合的成功案例，解析Sora在此过程中扮演的关键角色。<br> 3，详细介绍Sora如何依据文本指令生成具有三维一致性(3D consistency)的视频内容。 4，解析Sora如何根据图像或视频生成高保真内容的技术路径。<br> 5，探讨Sora在不同应用场景中的实践价值及其面临的挑战和局限性。</p> 
<p>二、解码Sora架构原理<br> 1，DiT (Diffusion Transformer)架构详解<br> 2，DiT是如何帮助Sora实现Consistent、Realistic、Imaginative视频内容的？<br> 3，探讨为何选用Transformer作为Diffusion的核心网络，而非技术如U-Net。<br> 4，DiT的Patchification原理及流程，揭示其在处理视频和图像数据中的重要性。<br> 5，Conditional Diffusion过程详解，及其在内容生成过程中的作用。<br> 三、解码Sora关键技术解密<br> 1，Sora如何利用Transformer和Diffusion技术理解物体间的互动，及其对模拟复杂互动场景的重要性。<br> 2，为何说Space-time patches是Sora技术的核心，及其对视频生成能力的提升作用。<br> 3，Spacetime latent patches详解，探讨其在视频压缩和生成中的关键角色。<br> 4，Sora Simulator如何利用Space-time patches构建digital和physical世界，及其对模拟真实世界变化的能力。<br> 5，Sora如何实现faithfully按照用户输入文本而生成内容，探讨背后的技术与创新。<br> 6，Sora为何依据abstract concept而不是依据具体的pixels进行内容生成，及其对模型生成质量与多样性的影响。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f8720e0a34e3a1cf371a3f6cdd3ff733/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">SQLyog安装配置（注册码）连接MySQL</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/50bd4bc9d5035c6eb11581a0e3f32a76/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Android 项目Gradle文件讲解（Groovy和Kotlin）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>