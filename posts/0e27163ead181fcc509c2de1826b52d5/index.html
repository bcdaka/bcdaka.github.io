<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Spark在Windows下的环境搭建及pyspark的使用 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/0e27163ead181fcc509c2de1826b52d5/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Spark在Windows下的环境搭建及pyspark的使用">
  <meta property="og:description" content="一、JDK的安装 Spark是一个用于大数据处理的开源框架，它是用Scala编写的，而Scala是一种运行在Java虚拟机（JVM）上的编程语言，因此它依赖于Java的运行环境。所以首先需要安装JDK（JavaTM Platform Standard Edition Development Kit），并将环境变量配置好。
可参考我的另一篇博客：http://t.csdnimg.cn/6Kj8w
二、Spark的安装 1.下载Spark 从Spark官网进行下载：Apache Spark™ - Unified Engine for large-scale data analytics，点击Download
这里我下载2.2.0版本，滑到下面，选择Archived releases，点击Spark release archives
找到2.2.0版本
选择带有Hadoop版本的Spark： spark-2.2.0-bin-hadoop2.7.tgz 意思是Spark版本是2.2.0，还需安装hadoop2.7版本
单击即可下载
下载完成后将文件进行解压，得到大约200M的文件： spark-2.2.0-bin-hadoop2.7
最好解压到一个盘的根目录下，并重命名为Spark，简单不易出错。并且需要注意的是，在Spark的文件目录路径名中，不要出现空格和中文，类似于“Program Files”这样的文件夹名是不被允许的，我放的位置是D:\Spark
2.配置环境变量 系统变量创建SPARK_HOME：D:\Spark\spark-2.2.0-bin-hadoop2.7
系统变量中的Path添加：%SPARK_HOME%\bin 3.测试是否安装成功 Win&#43;R键打开运行窗口，输入cmd，命令行串口输入spark-shell
出现下图即安装成功
这时开启的是Spark的交互式命令行模式，但直接使用很有可能会碰到各种错误，如下图，这里主要是因为Spark是基于hadoop的，所以这里还需配置一个Hadoop的运行环境。
三、Hadoop的安装 1.下载Hadoop 下载上面spark对应版本的hadoop 2.7:Hadoop Releases
我这里选择2.7.1版本
选择好相应版本并点击后，进入详细的下载页面，如下图所示
上面的src版本就是源码，需要对Hadoop进行更改或者想自己进行编译的可以下载对应src文件，我这里下载的就是已经编译好的版本，即图中的“hadoop-2.7.1.tar.gz”文件
下载并解压到指定目录，我这里是D:\Hadoop
2.配置环境变量 系统变量创建HADOOP_HOME：D:\Hadoop\hadoop-2.7.1
系统变量中的Path添加：%HADOOP_HOME%\bin
3.安装winutils.exe winutils.exe是在window系统上安装hadoop时必要的文件，可在github上下载
github下载地址：https://github.com/steveloughran/winutils
选择对应安装的Hadoop版本号
进入到bin目录下
找到winutils.exe文件，单击下载
将下载好的winutils.exe文件放入到Hadoop的bin目录下，我是D:\Hadoop\hadoop-2.7.1\bin
4.测试是否安装成功 Win&#43;R键打开运行窗口，输入cmd，命令行串口输入hadoop
四、常见问题 正常情况下是可以运行成功并进入到Spark的命令行环境下的，但是可能会遇到如下错误：
&lt;console&gt;:14: error: not found: value spark import spark.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-01-03T01:38:18+08:00">
    <meta property="article:modified_time" content="2024-01-03T01:38:18+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Spark在Windows下的环境搭建及pyspark的使用</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>  一、JDK的安装</h3> 
<p>    Spark是一个用于大数据处理的开源框架，它是用Scala编写的，而Scala是一种运行在Java虚拟机（JVM）上的编程语言，因此它依赖于Java的运行环境。所以首先需要安装JDK（JavaTM Platform Standard Edition Development Kit），并将环境变量配置好。</p> 
<p>可参考我的另一篇博客：<a href="http://t.csdnimg.cn/6Kj8w" rel="nofollow" title="http://t.csdnimg.cn/6Kj8w">http://t.csdnimg.cn/6Kj8w</a></p> 
<h3>二、Spark的安装</h3> 
<h4>1.下载Spark</h4> 
<p>        从Spark官网进行下载：<a href="http://spark.apache.org/" rel="nofollow" title="Apache Spark™ - Unified Engine for large-scale data analytics">Apache Spark™ - Unified Engine for large-scale data analytics</a>，点击Download</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/8d/d9/k4KUKLCa_o.png" width="1200"></p> 
<p>这里我下载2.2.0版本，滑到下面，选择Archived releases，点击<a href="https://archive.apache.org/dist/spark/" rel="nofollow" title="Spark release archives">Spark release archives</a></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/d9/20/ldgrlyGq_o.png" width="1200"></p> 
<p>找到2.2.0版本</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/bc/d9/lXx8RW5l_o.png" width="1200"></p> 
<p>选择带有Hadoop版本的Spark： spark-2.2.0-bin-hadoop2.7.tgz     </p> 
<p>意思是Spark版本是2.2.0，还需安装hadoop2.7版本</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/8c/3d/CrvpuX73_o.png" width="1200"></p> 
<p>单击即可下载</p> 
<p>下载完成后将文件进行解压，得到大约200M的文件： spark-2.2.0-bin-hadoop2.7</p> 
<p><img alt="" height="215" src="https://images2.imgbox.com/98/a2/Q3ovEldF_o.png" width="279"></p> 
<p>最好解压到一个盘的根目录下，并重命名为Spark，简单不易出错。并且需要注意的是，在Spark的文件目录路径名中，不要出现空格和中文，类似于“Program Files”这样的文件夹名是不被允许的，我放的位置是D:\Spark</p> 
<h4>2.配置环境变量</h4> 
<p>系统变量创建SPARK_HOME：D:\Spark\spark-2.2.0-bin-hadoop2.7</p> 
<p><img alt="" height="179" src="https://images2.imgbox.com/ac/d0/W5fRl8Gz_o.png" width="698"></p> 
<p>系统变量中的Path添加：%SPARK_HOME%\bin </p> 
<h4>3.测试是否安装成功</h4> 
<p><strong>Win+R键</strong>打开运行窗口，输入cmd，命令行串口输入spark-shell</p> 
<p>出现下图即安装成功</p> 
<p><img alt="" height="509" src="https://images2.imgbox.com/f9/54/DsE6llRA_o.png" width="977"></p> 
<p>        这时开启的是Spark的交互式命令行模式，但直接使用很有可能会碰到各种错误，如下图，这里主要是因为Spark是基于hadoop的，所以这里还需配置一个Hadoop的运行环境。</p> 
<p><img alt="" height="245" src="https://images2.imgbox.com/b9/02/gpgJSX00_o.png" width="836"></p> 
<h3>三、<strong>Hadoop</strong>的安装</h3> 
<h4>1.下载<strong>Hadoop</strong></h4> 
<p>下载上面spark对应版本的hadoop 2.7:<a href="https://archive.apache.org/dist/hadoop/common/" rel="nofollow" title="Hadoop Releases">Hadoop Releases</a></p> 
<p>我这里选择2.7.1版本</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/4a/f7/HgIIuW2j_o.png" width="1200"></p> 
<p>选择好相应版本并点击后，进入详细的下载页面，如下图所示</p> 
<p><img alt="" height="533" src="https://images2.imgbox.com/79/8b/n9fOy5qw_o.png" width="1200"></p> 
<p>上面的src版本就是源码，需要对Hadoop进行更改或者想自己进行编译的可以下载对应src文件，我这里下载的就是已经编译好的版本，即图中的“hadoop-2.7.1.tar.gz”文件</p> 
<p>下载并解压到指定目录，我这里是D:\Hadoop</p> 
<p><img alt="" height="203" src="https://images2.imgbox.com/23/2b/C7kHq7Ou_o.png" width="306"></p> 
<h4>2.配置环境变量</h4> 
<p>系统变量创建HADOOP_HOME：D:\Hadoop\hadoop-2.7.1</p> 
<p><img alt="" height="179" src="https://images2.imgbox.com/3f/35/PnbbRWws_o.png" width="698"></p> 
<p>系统变量中的Path添加：%HADOOP_HOME%\bin</p> 
<h4>3.安装winutils.exe</h4> 
<p>winutils.exe是在window系统上安装hadoop时必要的文件，可在github上下载</p> 
<blockquote> 
 <p>github下载地址：https://github.com/steveloughran/winutils</p> 
</blockquote> 
<p>选择对应安装的Hadoop版本号</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/a0/7a/XMyTaqV0_o.png" width="1200"></p> 
<p>进入到bin目录下</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/9f/02/Y7SjIaAE_o.png" width="1200"></p> 
<p>找到winutils.exe文件，单击下载</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/09/2b/VModb3iA_o.png" width="1200"></p> 
<p>将下载好的winutils.exe文件放入到Hadoop的bin目录下，我是D:\Hadoop\hadoop-2.7.1\bin</p> 
<p><img alt="" height="248" src="https://images2.imgbox.com/1f/c1/8hYlFOq2_o.png" width="629"></p> 
<h4>4.测试是否安装成功</h4> 
<p><strong>Win+R键</strong>打开运行窗口，输入cmd，命令行串口输入hadoop</p> 
<p><img alt="" height="335" src="https://images2.imgbox.com/59/94/qKDD0D1p_o.png" width="701"></p> 
<h3> 四、常见问题</h3> 
<p>        正常情况下是可以运行成功并进入到Spark的命令行环境下的，但是可能会遇到如下错误：</p> 
<pre><code>&lt;console&gt;:14: error: not found: value spark
       import spark.implicits._
              ^
&lt;console&gt;:14: error: not found: value spark
       import spark.sql
              ^</code></pre> 
<p><strong> 解决办法是：</strong></p> 
<p>用以下命令创建 <strong>C：\tmp\hive</strong> diroctory 并授予访问权限（777是获取所有权限）</p> 
<pre><code>C:\Hadoop\winutils-master\hadoop-2.7.1\bin&gt;winutils.exe chmod -R 777 C:\tmp\hive</code></pre> 
<p>删除C盘的本地元存储<strong>metastore_db</strong>目录（如果存在的话）</p> 
<pre><code>C:\Users\&lt;User_Name&gt;\metastore_db</code></pre> 
<p>        然后再次开启一个新的cmd窗口，如果正常的话，应该就可以通过直接输入spark-shell来运行Spark了。正常的运行界面应该如下图所示：</p> 
<p><img alt="" height="512" src="https://images2.imgbox.com/bf/3c/xAR19lcn_o.png" width="977"></p> 
<h3>五、Python下Spark开发环境搭建</h3> 
<p>1、将Spark目录下的pyspark文件夹（D:\Spark\spark-2.2.0-bin-hadoop2.7\python\pyspark）复制到要使用的python环境的安装目录（E:\APP\python3.7.0\Lib\site-packages）里。如图所示：</p> 
<p><img alt="" height="438" src="https://images2.imgbox.com/34/b9/JyjS2HnF_o.png" width="1034"></p> 
<p>2.cmd进入目录（python环境下的Scripts）E:\APP\python3.7.0\Scripts，运行pip install py4j安装py4j库。如图所示：</p> 
<p><img alt="" height="194" src="https://images2.imgbox.com/87/e4/QKYioa3q_o.png" width="716"></p> 
<p>3.在系统变量中新建一个PYTHONPATH的系统变量，然后设置好下面变量值</p> 
<p>D:\Spark\spark-2.2.0-bin-hadoop2.7\python（根据自己目录更改）</p> 
<p>D:\Spark\spark-2.2.0-bin-hadoop2.7\python\lib\py4j-0.10.4-src.zip（根据自己目录更改）</p> 
<p><img alt="" height="182" src="https://images2.imgbox.com/e8/b4/3OBmKcIE_o.png" width="690"></p> 
<p><img alt="" height="537" src="https://images2.imgbox.com/c0/d7/GCoedyPW_o.png" width="545"></p> 
<p>后面就可以在VScode或者PyCharm等IDE中使用PySpark了！</p> 
<p>参考文献：<br><a href="https://stackoverflow.com/questions/44386153/why-does-spark-shell-fail-with-error-not-found-value-spark" rel="nofollow" title="Why does spark-shell fail with “error: not found: value spark”? - Stack Overflow">Why does spark-shell fail with “error: not found: value spark”? - Stack Overflow</a><br><a href="http://t.csdnimg.cn/UHP0E" rel="nofollow" title="http://t.csdnimg.cn/UHP0E">http://t.csdnimg.cn/UHP0E</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d43a1d7a6f6dc953ca35c0e9d0c6af07/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python 操作 Word 详解（python-docx）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ff44dec1842f04d0be95622ae01a0834/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python常用代码大全</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>