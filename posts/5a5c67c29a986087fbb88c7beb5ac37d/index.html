<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Transformer模型详解 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/5a5c67c29a986087fbb88c7beb5ac37d/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Transformer模型详解">
  <meta property="og:description" content="1. 前言 transformer结构是google在2017年的Attention Is All You Need论文中提出，在NLP的多个任务上取得了非常好的效果，可以说目前NLP发展都离不开transformer。最大特点是抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。 由于其出色性能以及对下游任务的友好性或者说下游任务仅仅微调即可得到不错效果，在计算机视觉领域不断有人尝试将transformer引入，近期也出现了一些效果不错的尝试，典型的如目标检测领域的detr和可变形detr，分类领域的vision transformer等等。 本文从transformer结构出发，结合视觉中的transformer成果(具体是vision transformer和detr)进行分析。
1.1. Transformer 概览 首先，让我们先将 Transformer 模型视为一个黑盒，如下图所示。在机器翻译任务中，将一种语言的一个句子作为输入，然后将其翻译成另一种语言的一个句子作为输出。
Transformer 模型（黑盒模式） Transformer 本质上是一个 Encoder-Decoder 架构。因此中间部分的 Transformer 可以分为两个部分：编码组件和解码组件。如下图所示： Transformer 模型（Encoder-Decoder 架构模式） 其中，编码组件由多层编码器（Encoder）组成（在论文中作者使用了 6 层编码器，在实际使用过程中你可以尝试其他层数）。解码组件也是由相同层数的解码器（Decoder）组成（在论文也使用了 6 层）。如下图所示：
每个编码器由两个子层组成：Self-Attention 层（自注意力层）和 Position-wise Feed Forward Network（前馈网络，缩写为 FFN）如下图所示。每个编码器的结构都是相同的，但是它们使用不同的权重参数。 编码器的输入会先流入 Self-Attention 层。它可以让编码器在对特定词进行编码时使用输入句子中的其他词的信息（可以理解为：当我们翻译一个词时，不仅只关注当前的词，而且还会关注其他词的信息）。后面我们将会详细介绍 Self-Attention 的内部结构。然后，Self-Attention 层的输出会流入前馈网络。
解码器也有编码器中这两层，但是它们之间还有一个注意力层（即 Encoder-Decoder Attention），其用来帮忙解码器关注输入句子的相关部分（类似于 seq2seq 模型中的注意力）。
1.2. 引入张量 现在我们已经了解了模型的主要组成部分，让我们开始研究各种向量/张量，以及他们在这些组成部分之间是如何流动的，从而将输入经过已训练的模型转换为输出。
和通常的 NLP 任务一样，首先，我们使用词嵌入算法（Embedding）将每个词转换为一个词向量。在 Transformer 论文中，词嵌入向量的维度是 512。
每个词被嵌入到大小为 512 的向量中。我们将用这些简单的框代表这些向量。 嵌入仅发生在最底层的编码器中。所有编码器都会接收到一个大小为 512 的向量列表——底部编码器接收的是词嵌入向量，其他编码器接收的是上一个编码器的输出。这个列表大小是我们可以设置的超参数——基本上这个参数就是训练数据集中最长句子的长度。
对输入序列完成嵌入操作后，每个词都会流经编码器的两层。
接下来，我们将换一个更短的句子作为示例，来说明在编码器的每个子层中发生了什么。
上面我们提到，编码器会接收一个向量作为输入。编码器首先将这些向量传递到 Self-Attention 层，然后传递到前馈网络，最后将输出传递到下一个编码器。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-01-31T17:45:21+08:00">
    <meta property="article:modified_time" content="2024-01-31T17:45:21+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Transformer模型详解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>1. 前言</h2> 
<p>transformer结构是google在2017年的Attention Is All You Need论文中提出，在NLP的多个任务上取得了非常好的效果，可以说目前NLP发展都离不开transformer。最大特点是抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。 由于其出色性能以及对下游任务的友好性或者说下游任务仅仅微调即可得到不错效果，在计算机视觉领域不断有人尝试将transformer引入，近期也出现了一些效果不错的尝试，典型的如目标检测领域的detr和可变形detr，分类领域的vision transformer等等。 本文从transformer结构出发，结合视觉中的transformer成果(具体是vision transformer和detr)进行分析。</p> 
<h3>1.1. Transformer 概览</h3> 
<p>首先，让我们先将 Transformer 模型视为一个黑盒，如下图所示。在机器翻译任务中，将一种语言的一个句子作为输入，然后将其翻译成另一种语言的一个句子作为输出。</p> 
<figure class="image"> 
 <img alt="" height="222" src="https://images2.imgbox.com/46/fa/JIOAn75L_o.png" width="936"> 
 <figcaption>
   Transformer 模型（黑盒模式） 
 </figcaption> 
</figure> 
<p>Transformer 本质上是一个 Encoder-Decoder 架构。因此中间部分的 Transformer 可以分为两个部分：编码组件和解码组件。如下图所示： </p> 
<figure class="image"> 
 <img alt="" height="436" src="https://images2.imgbox.com/fe/52/r9WUuMDW_o.png" width="648"> 
 <figcaption>
   Transformer 模型（Encoder-Decoder 架构模式） 
 </figcaption> 
</figure> 
<p>其中，编码组件由多层编码器（Encoder）组成（在论文中作者使用了 6 层编码器，在实际使用过程中你可以尝试其他层数）。解码组件也是由相同层数的解码器（Decoder）组成（在论文也使用了 6 层）。如下图所示：</p> 
<p><img alt="" height="617" src="https://images2.imgbox.com/21/f3/hGfDUv4I_o.png" width="871"></p> 
<p>每个编码器由两个子层组成：Self-Attention 层（自注意力层）和 Position-wise Feed Forward Network（前馈网络，缩写为 FFN）如下图所示。每个编码器的结构都是相同的，但是它们使用不同的权重参数。 </p> 
<p><img alt="" height="340" src="https://images2.imgbox.com/f1/dc/OoRH1kBQ_o.png" width="750"> 编码器的输入会先流入 Self-Attention 层。它可以让编码器在对特定词进行编码时使用输入句子中的其他词的信息（可以理解为：当我们翻译一个词时，不仅只关注当前的词，而且还会关注其他词的信息）。后面我们将会详细介绍 Self-Attention 的内部结构。然后，Self-Attention 层的输出会流入前馈网络。</p> 
<p>解码器也有编码器中这两层，但是它们之间还有一个注意力层（即 Encoder-Decoder Attention），其用来帮忙解码器关注输入句子的相关部分（类似于 seq2seq 模型中的注意力）。</p> 
<p><img alt="" height="299" src="https://images2.imgbox.com/33/2c/Hv9AOHG5_o.png" width="881"></p> 
<h3>1.2. 引入张量</h3> 
<p>现在我们已经了解了模型的主要组成部分，让我们开始研究各种向量/张量，以及他们在这些组成部分之间是如何流动的，从而将输入经过已训练的模型转换为输出。</p> 
<p>和通常的 NLP 任务一样，首先，我们使用词嵌入算法（Embedding）将每个词转换为一个词向量。在 Transformer 论文中，词嵌入向量的维度是 512。</p> 
<figure class="image"> 
 <img alt="" height="103" src="https://images2.imgbox.com/03/38/GwuvIg43_o.png" width="752"> 
 <figcaption>
   每个词被嵌入到大小为 512 的向量中。我们将用这些简单的框代表这些向量。 
 </figcaption> 
</figure> 
<p>嵌入仅发生在最底层的编码器中。所有编码器都会接收到一个大小为 512 的向量列表——底部编码器接收的是词嵌入向量，其他编码器接收的是上一个编码器的输出。这个列表大小是我们可以设置的超参数——基本上这个参数就是训练数据集中最长句子的长度。</p> 
<p>对输入序列完成嵌入操作后，每个词都会流经编码器的两层。<br><img alt="" height="593" src="https://images2.imgbox.com/89/aa/zjIECHMn_o.png" width="925"></p> 
<p>接下来，我们将换一个更短的句子作为示例，来说明在编码器的每个子层中发生了什么。</p> 
<p>上面我们提到，编码器会接收一个向量作为输入。编码器首先将这些向量传递到 Self-Attention 层，然后传递到前馈网络，最后将输出传递到下一个编码器。</p> 
<p><img alt="" src="https://images2.imgbox.com/73/8a/FxfRJo4C_o.png" width="600"></p> 
<h3>1.3. Transformer 整体结构</h3> 
<figure class="image"> 
 <img alt="" height="438" src="https://images2.imgbox.com/ee/66/dBjHWTEW_o.png" width="640"> 
 <figcaption>
   Transformer 的整体结构，左图Encoder和右图Decoder 
 </figcaption> 
</figure> 
<p>可以看到 <strong>Transformer 由 Encoder 和 Decoder 两个部分组成</strong>，Encoder 和 Decoder 都包含 6 个 block。Transformer 的工作流程大体如下：</p> 
<p><strong>第一步：</strong>获取输入句子的每一个单词的表示向量 <strong>X</strong>，<strong>X</strong>由单词的 Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embedding 相加得到。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/05/be/RAF7xK2v_o.png"> 
  <figcaption>
    Transformer 的输入表示 
  </figcaption> 
 </figure> 
</div> 
<p><strong>第二步：</strong>将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 <strong>x</strong>) 传入 Encoder 中，经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 <strong>C</strong>，如下图。单词向量矩阵用<img alt="X_{n \times d}" class="mathcode" src="https://images2.imgbox.com/69/ce/88LG5xFl_o.png">表示， n 是句子中单词个数，d 是表示向量的维度 (论文中 d=512)。每一个 Encoder block 输出的矩阵维度与输入完全一致。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/d7/44/CK1bvFA5_o.png"> 
  <figcaption>
    Transformer Encoder 编码句子信息 
  </figcaption> 
 </figure> 
</div> 
<p><strong>第三步</strong>：将 Encoder 输出的编码信息矩阵 <strong>C</strong>传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 <strong>Mask (掩盖)</strong> 操作遮盖住 i+1 之后的单词。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/6c/90/AIqpQmQC_o.png"> 
  <figcaption>
    Transofrmer Decoder 预测 
  </figcaption> 
 </figure> 
</div> 
<p>上图 Decoder 接收了 Encoder 的编码矩阵<strong> C</strong>，然后首先输入一个翻译开始符 "&lt;Begin&gt;"，预测第一个单词 "I"；然后输入翻译开始符 "&lt;Begin&gt;" 和单词 "I"，预测单词 "have"，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。</p> 
<h2 id="h_338817680_2">2. Transformer 的输入</h2> 
<p>Transformer 中单词的输入表示 <strong>x</strong>由<strong>单词 Embedding</strong> 和<strong>位置 Embedding</strong> （Positional Encoding）相加得到。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/3a/0e/f3pUgTe1_o.png"> 
  <figcaption>
    Transformer 的输入表示 
  </figcaption> 
 </figure> 
</div> 
<h3 id="h_338817680_3">2.1. 单词 Embedding</h3> 
<p>单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。</p> 
<h3 id="h_338817680_4">2.2. 位置 Embedding</h3> 
<p>Transformer 中除了单词的 Embedding，还需要使用位置 Embedding 表示单词出现在句子中的位置。<strong>因为 Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。</strong>所以 Transformer 中使用位置 Embedding 保存单词在序列中的相对或绝对位置。</p> 
<p>位置 Embedding 用 <strong>PE</strong>表示，<strong>PE</strong> 的维度与单词 Embedding 是一样的。PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者，计算公式如下：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/a1/07/f9r1biVQ_o.png"></p> 
<p>其中，pos 表示单词在句子中的位置，d 表示 PE的维度 (与词 Embedding 一样)，2i 表示偶数的维度，2i+1 表示奇数维度 (即 2i≤d, 2i+1≤d)。使用这种公式计算 PE 有以下的好处：</p> 
<ul><li>使 PE 能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第 21 位的 Embedding。</li><li>可以让模型容易地计算出相对位置，对于固定长度的间距 k，<strong>PE(pos+k)</strong> 可以用 <strong>PE(pos)</strong> 计算得到。因为 Sin(A+B) = Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) = Cos(A)Cos(B) - Sin(A)Sin(B)。</li></ul> 
<p>将单词的词 Embedding 和位置 Embedding 相加，就可以得到单词的表示向量 <strong>x</strong>，<strong>x </strong>就是 Transformer 的输入。</p> 
<h2 id="h_338817680_5">3. Self-Attention（自注意力机制）</h2> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/56/15/sR5EsE55_o.png"> 
  <figcaption>
    Transformer Encoder 和 Decoder 
  </figcaption> 
 </figure> 
</div> 
<p>上图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为<strong> Multi-Head Attention</strong>，是由多个 <strong>Self-Attention</strong>组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add &amp; Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。</p> 
<p>因为 <strong>Self-Attention</strong>是 Transformer 的重点，所以我们重点关注 Multi-Head Attention 以及 Self-Attention，首先详细了解一下 Self-Attention 的内部逻辑。</p> 
<h3 id="h_338817680_6">3.1. Self-Attention 结构</h3> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/0d/16/YKXjpL7J_o.png"> 
  <figcaption>
    Self-Attention 结构 
  </figcaption> 
 </figure> 
</div> 
<p>上图是Self-Attention的结构，在计算的时候需要用到矩阵<strong>Q(查询), K(键值), V(值)</strong>。在实际中，Self-Attention接收的是输入(单词的表示向量x组成的矩阵X) 或者上一个 Encoder block 的输出。而<strong>Q, K, V</strong>正是通过 Self-Attention 的输入进行线性变换得到的。</p> 
<h3 id="h_338817680_7">3.2. Q, K, V 的计算</h3> 
<p>Self-Attention 的输入用矩阵X进行表示，则可以使用线性变阵矩阵<strong>WQ, WK, WV</strong>计算得到<strong>Q, K, V</strong>。计算如下图所示，<strong>注意 X, Q, K, V 的每一行都表示一个单词。</strong></p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/8f/93/bDj2f0q9_o.png"> 
  <figcaption>
    Q, K, V 的计算 
  </figcaption> 
 </figure> 
</div> 
<h3 id="h_338817680_8">3.3. Self-Attention 的输出</h3> 
<p>得到矩阵 Q, K, V之后就可以计算出 Self-Attention 的输出了，计算的公式如下：</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/09/03/r8mjaQce_o.png"> 
  <figcaption>
    Self-Attention 的输出 
  </figcaption> 
 </figure> 
</div> 
<p>公式中计算矩阵<strong>Q</strong>和<strong>K</strong>每一行向量的内积，为了防止内积过大，因此除以<img alt="d_k" class="mathcode" src="https://images2.imgbox.com/19/16/jZmQpjU0_o.png">的平方根。<strong>Q</strong>乘以<strong>K</strong>的转置后，得到的矩阵行列数都为 n，n 为句子单词数，这个矩阵可以表示单词之间的 attention 强度。下图为<strong>Q</strong>乘以<img alt="K^T" class="mathcode" src="https://images2.imgbox.com/7b/71/dBsCWK15_o.png">，1234 表示的是句子中的单词。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/7f/d3/dF6q1pKU_o.png"> 
  <figcaption>
    Q乘以K的转置的计算 
  </figcaption> 
 </figure> 
</div> 
<p>得到<img alt="QK^T" class="mathcode" src="https://images2.imgbox.com/9d/f7/3Z6apsfY_o.png">之后，使用 Softmax 计算每一个单词对于其他单词的 attention 系数，公式中的 Softmax 是对矩阵的每一行进行 Softmax，即每一行的和都变为 1.</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/b0/1c/YXamMxY3_o.png"> 
  <figcaption>
    对矩阵的每一行进行 Softmax 
  </figcaption> 
 </figure> 
</div> 
<p>得到 Softmax 矩阵之后可以和<strong>V</strong>相乘，得到最终的输出<strong>Z</strong>。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/91/56/URxqZwdE_o.png"> 
  <figcaption>
    Self-Attention 输出 
  </figcaption> 
 </figure> 
</div> 
<p>上图中 Softmax 矩阵的第 1 行表示单词 1 与其他所有单词的 attention 系数，最终单词 1 的输出<img alt="Z_1" class="mathcode" src="https://images2.imgbox.com/9e/d4/yoMekMLL_o.png"></p> 
<p>等于所有单词 i 的值<img alt="V_i" class="mathcode" src="https://images2.imgbox.com/00/2c/zXEXLvBm_o.png">根据 attention 系数的比例加在一起得到，如下图所示：</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/42/45/hIV9nNT6_o.png"> 
  <figcaption>
    Zi 的计算方法 
  </figcaption> 
 </figure> 
</div> 
<h3 id="h_338817680_9">3.4. Multi-Head Attention</h3> 
<p>在上一步，我们已经知道怎么通过 Self-Attention 计算得到输出矩阵 Z，而 Multi-Head Attention 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的结构图。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/5e/45/dfGtFXzk_o.png"> 
  <figcaption>
    Multi-Head Attention 
  </figcaption> 
 </figure> 
</div> 
<p>从上图可以看到 Multi-Head Attention 包含多个 Self-Attention 层，首先将输入<strong>X</strong>分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵<strong>Z</strong>。下图是 h=8 时候的情况，此时会得到 8 个输出矩阵<strong>Z</strong>。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/e2/f9/qI5xnpco_o.png"> 
  <figcaption>
    多个 Self-Attention 
  </figcaption> 
 </figure> 
</div> 
<p>得到 8 个输出矩阵<img alt="Z_1" class="mathcode" src="https://images2.imgbox.com/55/63/a6r3uai2_o.png">到<img alt="Z_8" class="mathcode" src="https://images2.imgbox.com/86/19/VV6D2bvy_o.png">之后，Multi-Head Attention 将它们拼接在一起 <strong>(Concat)</strong>，然后传入一个<strong>Linear</strong>层，得到 Multi-Head Attention 最终的输出<strong>Z</strong>。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/26/3b/tlN94tPU_o.png"> 
  <figcaption>
    Multi-Head Attention 的输出 
  </figcaption> 
 </figure> 
</div> 
<p>可以看到 Multi-Head Attention 输出的矩阵<strong>Z</strong>与其输入的矩阵<strong>X</strong>的维度是一样的。</p> 
<h2 id="h_338817680_10">4. Encoder 结构</h2> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/a5/5e/UtRFqITE_o.png"> 
  <figcaption>
    Transformer Encoder block 
  </figcaption> 
 </figure> 
</div> 
<p>上图红色部分是 Transformer 的 Encoder block 结构，可以看到是由 Multi-Head Attention,<strong> Add &amp; Norm, Feed Forward, Add &amp; Norm </strong>组成的。刚刚已经了解了 Multi-Head Attention 的计算过程，现在了解一下 Add &amp; Norm 和 Feed Forward 部分。</p> 
<h3 id="h_338817680_11">4.1. Add &amp; Norm</h3> 
<p>Add &amp; Norm 层由 Add 和 Norm 两部分组成，其计算公式如下：</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/76/e5/O83gbI2u_o.png"> 
  <figcaption>
    Add &amp;amp;amp;amp;amp;amp; Norm 公式 
  </figcaption> 
 </figure> 
</div> 
<p>其中 <strong>X</strong>表示 Multi-Head Attention 或者 Feed Forward 的输入，MultiHeadAttention(<strong>X</strong>) 和 FeedForward(<strong>X</strong>) 表示输出 (输出与输入 <strong>X </strong>维度是一样的，所以可以相加)。</p> 
<p><strong>Add</strong>指 <strong>X</strong>+MultiHeadAttention(<strong>X</strong>)，是一种残差连接，通常用于解决多层网络训练的问题，可以让网络只关注当前差异的部分，在 ResNet 中经常用到：</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/c5/24/iRyhtJcN_o.png"> 
  <figcaption>
    残差连接 
  </figcaption> 
 </figure> 
</div> 
<p><strong>Norm</strong>指 Layer Normalization，通常用于 RNN 结构，Layer Normalization 会将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛。</p> 
<h3 id="h_338817680_12">4.2. Feed Forward</h3> 
<p>Feed Forward 层比较简单，是一个两层的全连接层，第一层的激活函数为 Relu，第二层不使用激活函数，对应的公式如下。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/15/dd/uj1GKi9G_o.png"> 
  <figcaption>
    Feed Forward 
  </figcaption> 
 </figure> 
</div> 
<p><strong>X</strong>是输入，Feed Forward 最终得到的输出矩阵的维度与<strong>X</strong>一致。</p> 
<h3 id="h_338817680_13">4.3. 组成 Encoder</h3> 
<p>通过上面描述的 Multi-Head Attention, Feed Forward, Add &amp; Norm 就可以构造出一个 Encoder block，Encoder block 接收输入矩阵<img alt="X_{n \times d}" class="mathcode" src="https://images2.imgbox.com/29/9f/qQQywLDA_o.png">，并输出一个矩阵<img alt="O_{n \times d}" class="mathcode" src="https://images2.imgbox.com/d9/05/DTce7DmZ_o.png">。通过多个 Encoder block 叠加就可以组成 Encoder。</p> 
<p>第一个 Encoder block 的输入为句子单词的表示向量矩阵，后续 Encoder block 的输入是前一个 Encoder block 的输出，最后一个 Encoder block 输出的矩阵就是<strong>编码信息矩阵 C</strong>，这一矩阵后续会用到 Decoder 中。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/03/0f/05AcEB71_o.png"> 
  <figcaption>
    Encoder 编码句子信息 
  </figcaption> 
 </figure> 
</div> 
<h2 id="h_338817680_14">5. Decoder 结构</h2> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/25/d9/DLk1hVX9_o.png"> 
  <figcaption>
    Transformer Decoder block 
  </figcaption> 
 </figure> 
</div> 
<p>上图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别：</p> 
<ul><li>包含两个 Multi-Head Attention 层。</li><li>第一个 Multi-Head Attention 层采用了 Masked 操作。</li><li>第二个 Multi-Head Attention 层的<strong>K, V</strong>矩阵使用 Encoder 的<strong>编码信息矩阵C</strong>进行计算，而<strong>Q</strong>使用上一个 Decoder block 的输出计算。</li><li>最后有一个 Softmax 层计算下一个翻译单词的概率。</li></ul> 
<h3 id="h_338817680_15" style="background-color:transparent;">5.1. 第一个 Multi-Head Attention</h3> 
<p>Decoder block 的第一个 Multi-Head Attention 采用了 Masked 操作，因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第 i+1 个单词。通过 Masked 操作可以防止第 i 个单词知道 i+1 个单词之后的信息。下面以 "我有一只猫" 翻译成 "I have a cat" 为例，了解一下 Masked 操作。</p> 
<p>下面的描述中使用了类似 Teacher Forcing 的概念，不熟悉 Teacher Forcing 的童鞋可以参考以下上一篇文章Seq2Seq 模型详解。在 Decoder 的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入 "&lt;Begin&gt;" 预测出第一个单词为 "I"，然后根据输入 "&lt;Begin&gt; I" 预测下一个单词 "have"。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/54/3a/F2Ob1xns_o.png"> 
  <figcaption>
    Decoder 预测 
  </figcaption> 
 </figure> 
</div> 
<p>Decoder 可以在训练的过程中使用 Teacher Forcing 并且并行化训练，即将正确的单词序列 (&lt;Begin&gt; I have a cat) 和对应输出 (I have a cat &lt;end&gt;) 传递到 Decoder。那么在预测第 i 个输出时，就要将第 i+1 之后的单词掩盖住，<strong>注意 Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 "&lt;Begin&gt; I have a cat &lt;end&gt;"。</strong></p> 
<p><strong>第一步：</strong>是 Decoder 的输入矩阵和 <strong>Mask </strong>矩阵，输入矩阵包含 "&lt;Begin&gt; I have a cat" (0, 1, 2, 3, 4) 五个单词的表示向量，<strong>Mask </strong>是一个 5×5 的矩阵。在 <strong>Mask </strong>可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/8c/3d/0vk66ZK3_o.png"> 
  <figcaption>
    输入矩阵与 Mask 矩阵 
  </figcaption> 
 </figure> 
</div> 
<p><strong>第二步：</strong>接下来的操作和之前的 Self-Attention 一样，通过输入矩阵<strong>X</strong>计算得到<strong>Q,K,V</strong>矩阵。然后计算<strong>Q</strong>和<img alt="K^T" class="mathcode" src="https://images2.imgbox.com/8d/73/D2zNygmC_o.png">的乘积<img alt="QK^T" class="mathcode" src="https://images2.imgbox.com/4e/c1/TVfQWGGu_o.png">。</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/55/74/Egv4RZKk_o.png"> 
  <figcaption>
    Q乘以K的转置 
  </figcaption> 
 </figure> 
</div> 
<p><strong>第三步：</strong>在得到<img alt="QK^T" class="mathcode" src="https://images2.imgbox.com/e2/f2/n3ubuOUa_o.png">之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用<strong>Mask</strong>矩阵遮挡住每一个单词之后的信息，遮挡操作如下：</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/18/2f/mq22nsLr_o.png"> 
  <figcaption>
    Softmax 之前 Mask 
  </figcaption> 
 </figure> 
</div> 
<p>得到 <strong>Mask</strong> <img alt="QK^T" class="mathcode" src="https://images2.imgbox.com/a8/59/x0M0cZwG_o.png">之后在 <strong>Mask </strong><img alt="QK^T" class="mathcode" src="https://images2.imgbox.com/cd/cf/e8AghPbX_o.png">上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。</p> 
<p><strong>第四步：</strong>使用 <strong>Mask </strong><img alt="QK^T" class="mathcode" src="https://images2.imgbox.com/bc/50/fP8QD8th_o.png"> 与矩阵<strong> V</strong>相乘，得到输出 <strong>Z</strong>，则单词 1 的输出向量<img alt="Z_1" class="mathcode" src="https://images2.imgbox.com/ad/08/7cFyiZrI_o.png">是只包含单词 1 信息的。</p> 
<p><strong>第五步：</strong>通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵<img alt="Z_i" class="mathcode" src="https://images2.imgbox.com/7e/71/6NtIpYPG_o.png">，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出<img alt="Z_i" class="mathcode" src="https://images2.imgbox.com/61/16/6KpMxMsZ_o.png"> 然后计算得到第一个 Multi-Head Attention 的输出<strong>Z</strong>，<strong>Z</strong>与输入<strong>X</strong>维度一样。</p> 
<h3 id="h_338817680_16">5.2. 第二个 Multi-Head Attention</h3> 
<p>Decoder block 第二个 Multi-Head Attention 变化不大， 主要的区别在于其中 Self-Attention 的 <strong>K, V</strong>矩阵不是使用 上一个 Decoder block 的输出计算的，而是使用 <strong>Encoder 的编码信息矩阵 C </strong>计算的。</p> 
<p>根据 Encoder 的输出 <strong>C</strong>计算得到 <strong>K, V</strong>，根据上一个 Decoder block 的输出<strong> Z</strong> 计算 <strong>Q</strong> (如果是第一个 Decoder block 则使用输入矩阵 <strong>X</strong> 进行计算)，后续的计算方法与之前描述的一致。</p> 
<p>这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 <strong>Mask</strong>)。</p> 
<h3 id="h_338817680_17">5.3. Softmax 预测输出单词</h3> 
<p>Decoder block 最后的部分是利用 Softmax 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 Z，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0 的信息，如下：</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/91/3e/YhMQ0gJt_o.png"> 
  <figcaption>
    Decoder Softmax 之前的 Z 
  </figcaption> 
 </figure> 
</div> 
<p>Softmax 根据输出矩阵的每一行预测下一个单词：</p> 
<div class="img-center"> 
 <figure class="image"> 
  <img alt="" src="https://images2.imgbox.com/b8/60/dqb3cnzi_o.png"> 
  <figcaption>
    Decoder Softmax 预测 
  </figcaption> 
 </figure> 
</div> 
<p>这就是 Decoder block 的定义，与 Encoder 一样，Decoder 是由多个 Decoder block 组合而成。</p> 
<h2 id="h_338817680_18">6. Transformer 总结</h2> 
<ul><li>Transformer 与 RNN 不同，可以比较好地并行训练。</li><li>Transformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding，否则 Transformer 就是一个词袋模型了。</li><li>Transformer 的重点是 Self-Attention 结构，其中用到的 <strong>Q, K, V</strong>矩阵通过输出进行线性变换得到。</li><li>Transformer 中 Multi-Head Attention 中有多个 Self-Attention，可以捕获单词之间多种维度上的相关系数 attention score。</li></ul> 
<h2>参考文献</h2> 
<p>Attention Is All You Need</p> 
<p><a href="https://blog.csdn.net/benzhujie1245com/article/details/117173090" title="Transformer 模型详解_空杯的境界的博客-CSDN博客_transformer">Transformer 模型详解_空杯的境界的博客-CSDN博客_transformer</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/338817680" rel="nofollow" title="Transformer模型详解（图解最完整版） - 知乎">Transformer模型详解（图解最完整版） - 知乎</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/308301901" rel="nofollow" title="3W字长文带你轻松入门视觉transformer - 知乎">3W字长文带你轻松入门视觉transformer - 知乎</a></p> 
<p><a href="https://www.bilibili.com/video/BV1Xp4y1b7ih/?spm_id_from=autoNext&amp;vd_source=c8e1b41fe6ad9c1cbfb22f0fee2da1f7" rel="nofollow" title="台大李宏毅21年机器学习课程 self-attention和transformer_哔哩哔哩_bilibili">台大李宏毅21年机器学习课程 self-attention和transformer_哔哩哔哩_bilibili</a></p> 
<p><a href="https://jalammar.github.io/illustrated-transformer/" rel="nofollow" title="The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.">The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e0c9a0e2ec6f0e3db116f9811e5a6418/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">stable diffusion使用相关</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/85c0733139487978ee3c7695573ec7d7/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">EMQX安装和Java订阅、发布mqtt消息</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>