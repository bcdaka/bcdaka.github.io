<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>在Windows上用Llama Factory微调Llama 3的基本操作 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/7f8de9c00ab451ca1e6b7cbf80360a45/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="在Windows上用Llama Factory微调Llama 3的基本操作">
  <meta property="og:description" content="这篇博客参考了一些文章，例如：教程：利用LLaMA_Factory微调llama3:8b大模型_llama3模型微调保存-CSDN博客
也可以参考Llama Factory的Readme：GitHub - hiyouga/LLaMA-Factory: Unify Efficient Fine-Tuning of 100&#43; LLMsUnify Efficient Fine-Tuning of 100&#43; LLMs. Contribute to hiyouga/LLaMA-Factory development by creating an account on GitHub.https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file#installation首先将Llama Factory clone到本地：GitHub - hiyouga/LLaMA-Factory: Unify Efficient Fine-Tuning of 100&#43; LLMs 其次创建一个conda环境：
conda create -n llama_factory python=3.10 激活环境后首先安装pytorch，具体参考这个页面：Start Locally | PyTorch，例如：
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia 而后进入到LLaMA-Factory文件夹，参考其Readme，运行：
pip install -e .[torch,metrics] 同时，按照其Readme，在Windows系统上还需要运行：
pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl 具体原因我就不展开讲了。然后依次运行：
Set CUDA_VISIBLE_DEVICES=0 Set GRADIO_SHARE=1 llamafactory-cli webui 就可以看到其webui了。不过这时候还没有模型参数文件，对于国内用户而言，可以在这里https://modelscope.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-07T17:18:08+08:00">
    <meta property="article:modified_time" content="2024-06-07T17:18:08+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">在Windows上用Llama Factory微调Llama 3的基本操作</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>这篇博客参考了一些文章，例如：<a href="https://blog.csdn.net/m0_60683691/article/details/138505394" title="教程：利用LLaMA_Factory微调llama3:8b大模型_llama3模型微调保存-CSDN博客">教程：利用LLaMA_Factory微调llama3:8b大模型_llama3模型微调保存-CSDN博客</a></p> 
<p>也可以参考Llama Factory的Readme：<a class="has-card" href="https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file#installation" title="GitHub - hiyouga/LLaMA-Factory: Unify Efficient Fine-Tuning of 100+ LLMs"><span class="link-card-box"><span class="link-title">GitHub - hiyouga/LLaMA-Factory: Unify Efficient Fine-Tuning of 100+ LLMs</span><span class="link-desc">Unify Efficient Fine-Tuning of 100+ LLMs. Contribute to hiyouga/LLaMA-Factory development by creating an account on GitHub.</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/82/db/QEM4pM4C_o.png" alt="icon-default.png?t=N7T8">https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file#installation</span></span></a>首先将Llama Factory clone到本地：<a href="https://github.com/hiyouga/LLaMA-Factory" title="GitHub - hiyouga/LLaMA-Factory: Unify Efficient Fine-Tuning of 100+ LLMs">GitHub - hiyouga/LLaMA-Factory: Unify Efficient Fine-Tuning of 100+ LLMs</a> </p> 
<p>其次创建一个conda环境：</p> 
<pre><code class="language-python">conda create -n llama_factory python=3.10</code></pre> 
<p>激活环境后首先安装pytorch，具体参考这个页面：<a href="https://pytorch.org/get-started/locally/" rel="nofollow" title="Start Locally | PyTorch">Start Locally | PyTorch</a>，例如：</p> 
<pre><code class="language-bash">conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia</code></pre> 
<p>而后进入到LLaMA-Factory文件夹，参考其Readme，运行：</p> 
<pre><code class="language-bash">pip install -e .[torch,metrics]</code></pre> 
<p>同时，按照其Readme，在Windows系统上还需要运行：</p> 
<pre><code class="language-bash">pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl</code></pre> 
<p>具体原因我就不展开讲了。然后依次运行：</p> 
<pre><code class="language-bash">Set CUDA_VISIBLE_DEVICES=0
Set GRADIO_SHARE=1
llamafactory-cli webui</code></pre> 
<p>就可以看到其webui了。不过这时候还没有模型参数文件，对于国内用户而言，可以在这里<a class="has-card" href="https://modelscope.cn/organization/LLM-Research" rel="nofollow" title="https://modelscope.cn/organization/LLM-Research"><span class="link-card-box"><span class="link-title">https://modelscope.cn/organization/LLM-Research</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/27/ba/0R8wGqfn_o.png" alt="icon-default.png?t=N7T8">https://modelscope.cn/organization/LLM-Research</span></span></a></p> 
<p>进行下载，例如可以下载Llama3中文版本（如果没有git lfs可以用前两个命令安装）：</p> 
<pre><code class="language-bash">conda install git-lfs
git-lfs install
git lfs clone https://www.modelscope.cn/LLM-Research/Llama3-8B-Chinese-Chat.git</code></pre> 
<p>下载好之后，可以构造自己的微调数据集，具体而言，按照这里的介绍：</p> 
<p><a href="https://github.com/hiyouga/LLaMA-Factory/tree/main/data" title="https://github.com/hiyouga/LLaMA-Factory/tree/main/data">https://github.com/hiyouga/LLaMA-Factory/tree/main/data</a></p> 
<p>Llama Factory支持alpaca and sharegpt的格式，前者类似于这种格式：</p> 
<blockquote> 
 <pre>[
  {
    "instruction": "human instruction (required)",
    "input": "human input (optional)",
    "output": "model response (required)",
    "system": "system prompt (optional)",
    "history": [
      ["human instruction in the first round (optional)", "model response in the first round (optional)"],
      ["human instruction in the second round (optional)", "model response in the second round (optional)"]
    ]
  }
]</pre> 
</blockquote> 
<p>我们构造数据集的时候，最简单的方法就是只构造instruction和output。把生成的json文件放到LLaMA-Factory\data目录下，然后打开dataset_info.json文件，增加这个文件名记录即可，例如我这里增加：</p> 
<blockquote> 
 <p>  "private_train": {<!-- --><br>     "file_name": "private_train.json"<br>   },</p> 
</blockquote> 
<p>选择自己的私有数据集，可以预览一下，然后就可以开始训练了。</p> 
<p>训练完成后切换到Export，然后在上面的“微调方法”——“检查点路径”中选择刚才存储的目录Train_2024_xxxx之类，然后指定导出文件的目录，然后就可以导出了。</p> 
<p>导出之后我们可以加载微调之后的模型并测试了。当然，如果训练数据集比较小的话，测试的效果也不会太好。如果大家只是想对微调效果和特定问题进行展示，可以训练模型到过拟合，呵呵呵。</p> 
<p>就记录这么多。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8081d2eca592b2e5965bcf3e1815980e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">数据库中 Plugin ‘caching_sha2_password‘ /‘mysql_native_password‘ is not loaded.问题解决...</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/976e67a2a8c84019e72914dfb5f5663e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">解决IDEA 社区版mybatis.xml文件sql关键字无法高亮的问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>