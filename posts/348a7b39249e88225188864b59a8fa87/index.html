<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AIGC实战——GPT(Generative Pre-trained Transformer) - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/348a7b39249e88225188864b59a8fa87/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="AIGC实战——GPT(Generative Pre-trained Transformer)">
  <meta property="og:description" content="AIGC实战——GPT 0. 前言1. GPT 简介2. 葡萄酒评论数据集3. 注意力机制3.1 查询、键和值3.2 多头注意力3.3 因果掩码 4. Transformer4.1 Transformer 块4.2 位置编码 5. 训练GPT6. GPT 分析6.1 生成文本6.2 注意力分数 小结系列链接 0. 前言 注意力机制能够用于构建先进的文本生成模型，Transformer 是用于序列建模的强大神经网络，该神经网络不需要复杂的循环或卷积架构，而只依赖于注意力机制。这种方法克服了循环神经网络 (Recurrent Neural Network, RNN) 方法难以并行化的缺陷( RNN 必须逐符号处理序列)。Transformers 高度可并行化运算，能够在大规模数据集上进行训练。在本节中，我们将学习文本生成模型如何利用 Transformer 架构提高文本性能，并介绍自回归模型 GPT (Generative Pre-Trained transformer)。
1. GPT 简介 OpenAI 于 2018 年提出 GPT (Generative Pre-Trained transformer)，将 Transformer 架构用于训练大规模文本数据，以预测序列数据中的下一个单词，然后再针对特定的下游任务进行微调。
GPT 的预训练过程使用大型文本语料库 BookCorpus (来自 7000 本不同类型书籍的 4.5GB 文本)训练模型。在预训练过程中，模型会根据先前的单词预测序列中的下一个单词，这一过程称为语言建模 (language modeling)，用于训练模型理解自然语言的结构和模式。
在预训练之后，通过使用一个特定任务的小规模数据集，可以对 GPT 模型进行微调。微调表示调整模型的参数以更好地适应当前任务。例如，可以针对分类、问答等任务对模型进行微调。
OpenAI 对 GPT 架构进行了改进和扩展，后续提出了 GPT-2、GPT-3、GPT-3.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-03-11T08:23:04+08:00">
    <meta property="article:modified_time" content="2024-03-11T08:23:04+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AIGC实战——GPT(Generative Pre-trained Transformer)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>AIGC实战——GPT</h4> 
 <ul><li><ul><li><a href="#0__1" rel="nofollow">0. 前言</a></li><li><a href="#1_GPT__3" rel="nofollow">1. GPT 简介</a></li><li><a href="#2__9" rel="nofollow">2. 葡萄酒评论数据集</a></li><li><a href="#3__19" rel="nofollow">3. 注意力机制</a></li><li><ul><li><a href="#31__30" rel="nofollow">3.1 查询、键和值</a></li><li><a href="#32__49" rel="nofollow">3.2 多头注意力</a></li><li><a href="#33__69" rel="nofollow">3.3 因果掩码</a></li></ul> 
   </li><li><a href="#4_Transformer_97" rel="nofollow">4. Transformer</a></li><li><ul><li><a href="#41_Transformer__98" rel="nofollow">4.1 Transformer 块</a></li><li><a href="#42__158" rel="nofollow">4.2 位置编码</a></li></ul> 
   </li><li><a href="#5_GPT_195" rel="nofollow">5. 训练GPT</a></li><li><a href="#6_GPT__220" rel="nofollow">6. GPT 分析</a></li><li><ul><li><a href="#61__223" rel="nofollow">6.1 生成文本</a></li><li><a href="#62__231" rel="nofollow">6.2 注意力分数</a></li></ul> 
   </li><li><a href="#_240" rel="nofollow">小结</a></li><li><a href="#_242" rel="nofollow">系列链接</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h3><a id="0__1"></a>0. 前言</h3> 
<p>注意力机制能够用于构建先进的文本生成模型，<code>Transformer</code> 是用于序列建模的强大神经网络，该神经网络不需要复杂的循环或卷积架构，而只依赖于注意力机制。这种方法克服了循环神经网络 (<code>Recurrent Neural Network</code>, <code>RNN</code>) 方法难以并行化的缺陷( <code>RNN</code> 必须逐符号处理序列)。<code>Transformers</code> 高度可并行化运算，能够在大规模数据集上进行训练。在本节中，我们将学习文本生成模型如何利用 <code>Transformer</code> 架构提高文本性能，并介绍自回归模型 <code>GPT</code> (<code>Generative Pre-Trained transformer</code>)。</p> 
<h3><a id="1_GPT__3"></a>1. GPT 简介</h3> 
<p><code>OpenAI</code> 于 <code>2018</code> 年提出 <code>GPT</code> (<code>Generative Pre-Trained transformer</code>)，将 <code>Transformer</code> 架构用于训练大规模文本数据，以预测序列数据中的下一个单词，然后再针对特定的下游任务进行微调。<br> <code>GPT</code> 的预训练过程使用大型文本语料库 <code>BookCorpus</code> (来自 <code>7000</code> 本不同类型书籍的 <code>4.5GB</code> 文本)训练模型。在预训练过程中，模型会根据先前的单词预测序列中的下一个单词，这一过程称为语言建模 (<code>language modeling</code>)，用于训练模型理解自然语言的结构和模式。<br> 在预训练之后，通过使用一个特定任务的小规模数据集，可以对 <code>GPT</code> 模型进行微调。微调表示调整模型的参数以更好地适应当前任务。例如，可以针对分类、问答等任务对模型进行微调。<br> <code>OpenAI</code> 对 <code>GPT</code> 架构进行了改进和扩展，后续提出了 <code>GPT-2</code>、<code>GPT-3</code>、<code>GPT-3.5</code> 和 <code>GPT-4</code> 等模型。这些模型在更大的数据集上进行训练，具有更大的容量，因此可以生成更复杂、更连贯的文本，<code>GPT</code> 模型已经被广泛应用于自然语言处理相关任务中。<br> 在本节中，我们将使用相同的组件和基本原理构建 <code>GPT</code> 模型，不同的是，我们使用较少的数据进行训练。</p> 
<h3><a id="2__9"></a>2. 葡萄酒评论数据集</h3> 
<p>在本节中，使用 <code>Kaggle</code> 中的<a href="https://www.kaggle.com/datasets/zynicide/wine-reviews" rel="nofollow">葡萄酒评论数据集</a>训练 <code>GPT</code> 模型，数据集中包含超过 <code>130,000</code> 条葡萄酒评论，以及相关描述和价格等元数据。<br> 可以在 <code>Kaggle</code> 中<a href="https://www.kaggle.com/datasets/zynicide/wine-reviews" rel="nofollow">下载数据集</a>，解压后，将葡萄酒评论和相关元数据保存到 <code>./data</code> 文件夹中。数据准备流程与<a href="https://blog.csdn.net/LOVEmy134611/article/details/135395814">使用 LSTM 训练生成模型</a>中所用的数据准备步骤相同：</p> 
<ul><li>加载数据并创建一个包含所有葡萄酒文本描述的字符串列表</li><li>用空格分隔标点符号，以便每个标点符号被视为一个单独的单词</li><li>通过 <code>TextVectorization</code> 层将字符串进行分词，并将每个字符串填充/截断为固定长度</li><li>创建训练集，其中输入是分词后的文本字符串，输出是将相同字符串向后移动一个符号的字符串</li></ul> 
<p><img src="https://images2.imgbox.com/4b/70/tRNuG07E_o.png" alt="数据预处理"></p> 
<h3><a id="3__19"></a>3. 注意力机制</h3> 
<p>理解 <code>GPT</code> 工作原理的第一步是理解注意力机制 (<code>attention mechanism</code>)，注意力机制使得 <code>Transformer</code> 架构在语言建模方面与循环神经网络有所不同。理解了注意力机制后，便能了解如何在 <code>Transformer</code> 架构(如 <code>GPT</code> )中使用注意力机制。<br> 当我们写作时，句子中的下一个单词受到已经写完的其他单词的影响。例如，假设以以下方式开始一个句子：</p> 
<pre><code class="prism language-shell"><span class="token string">"The write elephant tried to get into the boat but it was too"</span>
</code></pre> 
<p>显然，下一个词应该是与 “<code>big</code>” 意思相近的单词。句子中的其他词对于下一单词的选择非常重要。例如，它是 <code>elephant</code> 而不是 <code>bird</code>，这意味着我们更倾向于选取 “<code>big</code>” 而不是 “<code>small</code>”，如果句中使用 “<code>pool</code>” 而不是 “<code>boat</code>”，我们可能会选择 “<code>scared</code>” 替代 “<code>big</code>”。最后，把 “<code>elephant</code>” 放进小船里意味着尺寸是问题所在，如果大象试图摧毁小船，我们可能会选择 “<code>flexible</code>” 作为最后一个词，其中 “<code>it</code>” 指的是 “<code>boat</code>”。<br> 除此之外，句子中的其他词则并不重要。例如，大象是 “<code>white</code>” 或“black”对于选择最后一个词没有任何影响。同样，句子中的其他单词，如 <code>the</code>，<code>but</code>，<code>it</code> 等，为句子构成了正确的语法形式，但对于确定下一个所需单词并不重要。换句话说，我们只关注句子中的某些单词，而对其他词只付出少量关注，我们希望深度学习模型同样能够做到这一点。<br> <code>Transformer</code> 中的注意力机制(也称为注意力头)就是为了实现这一目的。它能够决定在输入中从哪些部分中获取信息，以便高效地提取有用的信息而不被无关细节所干扰。这使得它能够适应各种情况，因为它可以在推理时决定在哪些部分中寻找信息。<br> 相比之下，循环层试图构建一个通用的隐藏状态，用以在每个时间步捕捉输入的整体表示。这种方法的一个缺点是，合并到隐藏向量中的许多单词对于当前任务(如预测下一个单词)可能并不直接相关。注意力头则可以避免此问题，因为它们可以选择如何结合上下文的信息。</p> 
<h4><a id="31__30"></a>3.1 查询、键和值</h4> 
<p>接下来，我们介绍注意力机制如何决定在哪里寻找信息，我们继续使用上一节中所用示例。<br> 为了能够预测单词 “<code>too</code>” 之后的内容，前面的其他单词都会有所贡献，但它们的贡献取决于它们对于预测 “<code>too</code>” 后续单词的能力的自信程度。例如，单词 “<code>elephant</code>” 可能会自信地贡献出与尺寸相关的词，而单词 “<code>was</code>” 没有提供太多的信息来缩小可能性。<br> 换句话说，我们可以将注意力头看作一种信息检索系统，其中查询(“接在 <code>too</code> 之后的词是什么？”)被转化为键/值存储(句子中的其他单词)，输出结果是值的总和，并根据查询与每个键之间的相关度进行加权。此过程详细步骤如下图所示，我们的仍然使用上一节所用示例。</p> 
<p><img src="https://images2.imgbox.com/97/4b/MDf0aP3a_o.png" alt="注意力机制"></p> 
<p>查询 (<code>query</code>, <code>Q</code>) 可以表示当前任务(例如，“接在 <code>too</code> 之后的词是什么？”)。在本例中，通过将单词 “<code>too</code>” 的嵌入传递到权重矩阵 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          Q 
         
        
       
      
        W_Q 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9694em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 中，从维度 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
        
          e 
         
        
       
      
        d_e 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 变换为维度 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
        
          k 
         
        
       
      
        d_k 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，得到了查询。<br> 键向量 (<code>key</code>, <code>K</code>) 是句子中每个单词的表示，可以将其视为每个单词可以帮助完成的预测任务的贡献。它们与查询的产生方式类似，通过将每个嵌入传递到权重矩阵 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          K 
         
        
       
      
        W_K 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 中，将每个向量的维度从 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
        
          e 
         
        
       
      
        d_e 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 变换为 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
        
          k 
         
        
       
      
        d_k 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，键和查询的长度相同。<br> 在注意力头中，使用每对向量 (<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          Q 
         
        
          K 
         
        
       
      
        Q_K 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8778em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>) 之间的点积将每个键与查询进行比较(这就是键和查询必须具有相同的长度的原因)。键/查询对的点积结果数值越高，键与查询的相关度就越强，因此允许它对注意力头的输出做出更大的贡献。得到的向量乘以 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
        
          k 
         
        
       
      
        d_k 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，以保持向量和的方差稳定(大约等于 <code>1</code>)，并应用 <code>softmax</code> 函数确保贡献总和为 <code>1</code>，这便是注意力权重向量。<br> 值向量 (<code>value</code>, <code>V</code>) 也是句子中单词的表示，可以将其视为每个单词的未加权贡献。通过将每个嵌入传递到权重矩阵 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          V 
         
        
       
      
        W_V 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.2222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 中，将每个向量的维度从 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
        
          e 
         
        
       
      
        d_e 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 变换为 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
        
          v 
         
        
       
      
        d_v 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，得到值向量。需要注意的是，值向量不一定与键和查询具有相同的长度(但通常为简单起见，使用相同长度)。<br> 值向量乘以注意力权重，得到给定 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         Q 
        
       
      
        Q 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8778em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span></span>、<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         K 
        
       
      
        K 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span></span></span></span></span> 和 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         V 
        
       
      
        V 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span></span></span></span></span> 的注意力：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          A 
         
        
          t 
         
        
          t 
         
        
          e 
         
        
          n 
         
        
          t 
         
        
          i 
         
        
          o 
         
        
          n 
         
        
          ( 
         
        
          Q 
         
        
          , 
         
        
          K 
         
        
          , 
         
        
          V 
         
        
          ) 
         
        
          = 
         
        
          s 
         
        
          o 
         
        
          f 
         
        
          t 
         
        
          m 
         
        
          a 
         
        
          x 
         
        
          ( 
         
         
          
          
            Q 
           
           
           
             K 
            
           
             T 
            
           
          
          
           
           
             d 
            
           
             k 
            
           
          
         
        
          ) 
         
        
          V 
         
        
       
         Attention(Q,K,V)=softmax(\frac {QK^T} {\sqrt {d_k}})V 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 2.4483em; vertical-align: -0.93em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.5183em;"><span class="" style="top: -2.2528em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8572em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord" style="padding-left: 0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.8172em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail" style="min-width: 0.853em; height: 1.08em;"> 
                   <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
                    <path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path> 
                   </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1828em;"><span class=""></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.93em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span></span></span></span></span></span><br> 为了从注意力头获得最终输出向量，将注意力进行求和，得到长度为 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
        
          v 
         
        
       
      
        d_v 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 的向量。这个上下文向量捕捉了句子中的单词对于预测接在 <code>too</code> 之后的词的任务的共同意见。</p> 
<h4><a id="32__49"></a>3.2 多头注意力</h4> 
<p>在 <code>Keras</code> 中，可以构建 <code>MultiHeadAttention</code> 层，该层将多个注意力头的输出连接起来构成多头注意力 (<code>Multihead Attention</code>)，允许每个头学习不同的注意力机制，以便整个层可以学习更复杂的关系。<br> 连接后的输出通过最终的权重矩阵 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          O 
         
        
       
      
        W_O 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0278em;">O</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 进行投影，将向量投影到期望的输出维度上。在本节下，输出维度与查询的输入维度相同 (<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
        
          e 
         
        
       
      
        d_e 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>)，以便可以将层按顺序堆叠在一起。下图显示了 <code>MultiHeadAttention</code> 层输出的构建方式。</p> 
<p><img src="https://images2.imgbox.com/51/33/bZQRiG2C_o.png" alt="多头注意力"></p> 
<p>在 <code>Keras</code> 中创建 <code>MultiHeadAttention</code> 层：</p> 
<pre><code class="prism language-python">layers<span class="token punctuation">.</span>MultiHeadAttention<span class="token punctuation">(</span>
    <span class="token comment"># 此多头注意力层有四个注意力头</span>
    num_heads <span class="token operator">=</span> <span class="token number">4</span><span class="token punctuation">,</span>
    <span class="token comment"># 键(和查询)是长度为 128 的向量</span>
    key_dim <span class="token operator">=</span> <span class="token number">128</span><span class="token punctuation">,</span>
    <span class="token comment"># 值(也是每个头的输出)是长度为 64 的向量</span>
    value_dim <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span>
    <span class="token comment"># 输出向量的长度为 256</span>
    output_shape <span class="token operator">=</span> <span class="token number">256</span>
<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="33__69"></a>3.3 因果掩码</h4> 
<p>在以上介绍中，我们假设注意力头的查询输入是一个单一的向量。然而，为了在训练期间提高效率，理想情况下我们希望注意力层能够同时处理输入中的每个单词，为每个单词预测后续的单词。换句话说，我们希望我们的 <code>GPT</code> 模型能够同时处理一组查询向量(即矩阵)。<br> 为了将向量批处理成一个矩阵，并使用线性代数进行处理，我们需要一个额外的步骤，我们需要对查询/键的点积应用掩码，以避免未来单词的信息泄漏，这称为因果掩码 (<code>Causal Masking</code>)，如下图所示。</p> 
<p><img src="https://images2.imgbox.com/9c/b8/JVUJy6EB_o.png" alt="因果掩码"></p> 
<p>如果没有这个掩码，<code>GPT</code> 模型将能够完美地猜出句子中的下一个单词，因为它将使用来自单词本身的键作为一个特征。使用 <code>Keras</code> 创建因果掩码，下图显示了 <code>Numpy</code> 形式的结果数组(为了与图相匹配，将结果数组进行了转置)。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">causal_attention_mask</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> n_dest<span class="token punctuation">,</span> n_src<span class="token punctuation">,</span> dtype<span class="token punctuation">)</span><span class="token punctuation">:</span>
    i <span class="token operator">=</span> tf<span class="token punctuation">.</span><span class="token builtin">range</span><span class="token punctuation">(</span>n_dest<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>
    j <span class="token operator">=</span> tf<span class="token punctuation">.</span><span class="token builtin">range</span><span class="token punctuation">(</span>n_src<span class="token punctuation">)</span>
    m <span class="token operator">=</span> i <span class="token operator">&gt;=</span> j <span class="token operator">-</span> n_src <span class="token operator">+</span> n_dest
    mask <span class="token operator">=</span> tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>m<span class="token punctuation">,</span> dtype<span class="token punctuation">)</span>
    mask <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>mask<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> n_dest<span class="token punctuation">,</span> n_src<span class="token punctuation">]</span><span class="token punctuation">)</span>
    mult <span class="token operator">=</span> tf<span class="token punctuation">.</span>concat<span class="token punctuation">(</span>
        <span class="token punctuation">[</span>tf<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span>
    <span class="token punctuation">)</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>tile<span class="token punctuation">(</span>mask<span class="token punctuation">,</span> mult<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>causal_attention_mask<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>因果掩码仅在解码器 <code>Transformer</code>(如 <code>GPT</code> )中需要，在这些模型中，任务是根据先前的符号顺序生成符号，在训练期间掩码掉未来的符号是至关重要的。<br> 其他类型的 <code>Transformer</code> (例如编码器 <code>Transformer</code> )不需要因果掩码，因为它们不是通过预测下一个符号来训练的。例如，<code>Google</code> 的 <code>BERT</code> 在给定句子中预测待填空的单词，因此它可以使用该词前后的上下文。<br> <code>Transformer</code> 中的多头注意力机制中，层的可学习参数仅由每个注意力头的三个权重矩阵 (<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          Q 
         
        
       
      
        W_Q 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9694em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span>、<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          K 
         
        
       
      
        W_K 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>、<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          V 
         
        
       
      
        W_V 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.2222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>) 和一个用于重塑输出的权重矩阵 (<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          O 
         
        
       
      
        W_O 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0278em;">O</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>) 组成，在多头注意力层中完全没有卷积或循环机制。<br> 下一节中，我们将介绍如何使用多头注意力层构建 <code>Transformer</code> 块。</p> 
<h3><a id="4_Transformer_97"></a>4. Transformer</h3> 
<h4><a id="41_Transformer__98"></a>4.1 Transformer 块</h4> 
<p><code>Transformer</code> 块是 <code>Transformer</code> 内的一个组件，它在多头注意力层间应用了跳跃连接、前馈(全连接)层 (<code>Feed-forward layers</code>) 和归一化，<code>Transformer</code> 块的结构如下图所示。</p> 
<p><img src="https://images2.imgbox.com/14/19/OTB2R3NY_o.png" alt="Transformer 块"></p> 
<p>首先，将查询传递到多头注意力层并直接添加到输出中，这是一种跳跃连接，可以用其构建非常深的神经网络，而不会受到梯度消失问题的影响，因为跳跃连接提供了一个无梯度的数据通道，使网络能够直接传递信息。<br> 其次，在 <code>Transformer</code> 块中的层归一化用于提高训练过程的稳定性。我们已经介绍了如何使用批归一化层，在批归一化中，每个通道的输出被归一化为均值为 <code>0</code>，标准差为 <code>1</code>，归一化统计信息是根据批和空间维度计算的。<br> 而在 <code>Transformer</code> 块中的层归一化通过在通道维度上计算归一化统计信息，对批数据中每个序列的每个位置进行归一化。就统计信息的计算方式而言，它与批归一化完全相反，下图展示了批归一化和层归一化之间的差异。<br> 层归一化常用于基于文本的任务中，以避免在批处理中跨序列创建归一化依赖关系。然而，<code>Shen</code> 等人表明在 <code>Transformer</code> 内部可以使用一种形式的批归一化进行调整，从而优于传统的层归一化方法。</p> 
<p><img src="https://images2.imgbox.com/d0/e8/rpVSDin6_o.png" alt="层归一化"></p> 
<p>最后，在 <code>Transformer</code> 块中包含了一组前馈(全连接)层( <code>Feed-forward layers</code> )，以使组件能够在网络中深入提取更高级别的特征。<br> 使用 <code>Keras</code> 实现一个 <code>Transformer</code> 块。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">TransformerBlock</span><span class="token punctuation">(</span>layers<span class="token punctuation">.</span>Layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 在初始化函数中定义构成 TransformerBlock 层的子层</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> key_dim<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> ff_dim<span class="token punctuation">,</span> dropout_rate<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>TransformerBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads
        self<span class="token punctuation">.</span>key_dim <span class="token operator">=</span> key_dim
        self<span class="token punctuation">.</span>embed_dim <span class="token operator">=</span> embed_dim
        self<span class="token punctuation">.</span>ff_dim <span class="token operator">=</span> ff_dim
        self<span class="token punctuation">.</span>dropout_rate <span class="token operator">=</span> dropout_rate
        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> layers<span class="token punctuation">.</span>MultiHeadAttention<span class="token punctuation">(</span>
            num_heads<span class="token punctuation">,</span> key_dim<span class="token punctuation">,</span> output_shape<span class="token operator">=</span>embed_dim
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout_1 <span class="token operator">=</span> layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout_rate<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ln_1 <span class="token operator">=</span> layers<span class="token punctuation">.</span>LayerNormalization<span class="token punctuation">(</span>epsilon<span class="token operator">=</span><span class="token number">1e-6</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ffn_1 <span class="token operator">=</span> layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ff_dim<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ffn_2 <span class="token operator">=</span> layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout_2 <span class="token operator">=</span> layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout_rate<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ln_2 <span class="token operator">=</span> layers<span class="token punctuation">.</span>LayerNormalization<span class="token punctuation">(</span>epsilon<span class="token operator">=</span><span class="token number">1e-6</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        input_shape <span class="token operator">=</span> tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
        batch_size <span class="token operator">=</span> input_shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        seq_len <span class="token operator">=</span> input_shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token comment"># 创建因果掩码，以隐藏查询中未来的键</span>
        causal_mask <span class="token operator">=</span> causal_attention_mask<span class="token punctuation">(</span>
            batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> tf<span class="token punctuation">.</span><span class="token builtin">bool</span>
        <span class="token punctuation">)</span>
        <span class="token comment"># 创建多头注意力层，并指定注意力掩码</span>
        attention_output<span class="token punctuation">,</span> attention_scores <span class="token operator">=</span> self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>
            inputs<span class="token punctuation">,</span>
            inputs<span class="token punctuation">,</span>
            attention_mask<span class="token operator">=</span>causal_mask<span class="token punctuation">,</span>
            return_attention_scores<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        attention_output <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout_1<span class="token punctuation">(</span>attention_output<span class="token punctuation">)</span>
        <span class="token comment"># 第一个 add 和归一化层</span>
        out1 <span class="token operator">=</span> self<span class="token punctuation">.</span>ln_1<span class="token punctuation">(</span>inputs <span class="token operator">+</span> attention_output<span class="token punctuation">)</span>
        <span class="token comment"># 前馈层</span>
        ffn_1 <span class="token operator">=</span> self<span class="token punctuation">.</span>ffn_1<span class="token punctuation">(</span>out1<span class="token punctuation">)</span>
        ffn_2 <span class="token operator">=</span> self<span class="token punctuation">.</span>ffn_2<span class="token punctuation">(</span>ffn_1<span class="token punctuation">)</span>
        <span class="token comment"># 第二个 add 和归一化层</span>
        ffn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout_2<span class="token punctuation">(</span>ffn_2<span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>ln_2<span class="token punctuation">(</span>out1 <span class="token operator">+</span> ffn_output<span class="token punctuation">)</span><span class="token punctuation">,</span> attention_scores<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="42__158"></a>4.2 位置编码</h4> 
<p>在训练 <code>GPT</code> 模型之前，还有一个最后一步需要处理。在多头注意力层中，并没有关注键的顺序。每个键与查询之间的点积是并行计算的，而非像循环神经网络那样顺序计算。这样可以并行化处理以提高效率，但也带来了一个问题，因为我们需要注意力层能够考虑文本的上下文信息，例如，针对以下两个句子预测不同的输出：</p> 
<pre><code class="prism language-shell">The dog looked at the boy and … <span class="token punctuation">(</span>barked?<span class="token punctuation">)</span>
The boy looked at the dog and … <span class="token punctuation">(</span>smiled?<span class="token punctuation">)</span>
</code></pre> 
<p>为了解决这个问题，在创建初始的 <code>Transformer</code> 块的输入时，我们使用一种称为位置编码 (<code>positional encoding</code>) 的技术。我们不仅使用符号嵌入 (<code>token embedding</code>) 来编码每个符号，还使用位置嵌入 (<code>position embedding</code>) 来编码符号的位置。<br> 符号嵌入使用标准的嵌入层将每个符号转换为一个可学习向量。我们可以使用相同的方式创建位置编码，使用标准的嵌入层将每个整数位置转换为一个可学习向量。虽然 <code>GPT</code> 使用嵌入层来嵌入位置信息，但原始 <code>Transformer</code> 论文使用三角函数。<br> 为了构建联合的符号-位置编码，将符号嵌入添加到位置嵌入中，如下图所示。这样，就可以在一个向量中同时捕捉序列中每个词的含义和位置信息。</p> 
<p><img src="https://images2.imgbox.com/1d/bf/xz9LGgpK_o.png" alt="位置编码"></p> 
<p>使用 <code>Keras</code> 定义 <code>TokenAndPositionEmbedding</code> 层：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">TokenAndPositionEmbedding</span><span class="token punctuation">(</span>layers<span class="token punctuation">.</span>Layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> max_len<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>TokenAndPositionEmbedding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>max_len <span class="token operator">=</span> max_len
        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> vocab_size
        self<span class="token punctuation">.</span>embed_dim <span class="token operator">=</span> embed_dim
        <span class="token comment"># 符号使用 Embedding 层获取嵌入</span>
        self<span class="token punctuation">.</span>token_emb <span class="token operator">=</span> layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>
            input_dim<span class="token operator">=</span>vocab_size<span class="token punctuation">,</span> output_dim<span class="token operator">=</span>embed_dim
        <span class="token punctuation">)</span>
        <span class="token comment"># 符号的位置同样使用 Embedding 层获取嵌入</span>
        self<span class="token punctuation">.</span>pos_emb <span class="token operator">=</span> layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>input_dim<span class="token operator">=</span>max_len<span class="token punctuation">,</span> output_dim<span class="token operator">=</span>embed_dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        maxlen <span class="token operator">=</span> tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
        positions <span class="token operator">=</span> tf<span class="token punctuation">.</span><span class="token builtin">range</span><span class="token punctuation">(</span>start<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> limit<span class="token operator">=</span>maxlen<span class="token punctuation">,</span> delta<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        positions <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_emb<span class="token punctuation">(</span>positions<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>token_emb<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment"># TokenAndPositionEmbedding 层的输出是符号嵌入和位置嵌入之和</span>
        <span class="token keyword">return</span> x <span class="token operator">+</span> positions
</code></pre> 
<h3><a id="5_GPT_195"></a>5. 训练GPT</h3> 
<p>接下来，构建并训练 <code>GPT</code> 模型。将输入文本通过符号和位置嵌入层，然后通过 <code>Transformer</code> 块，网络的最终输出是一个使用 <code>softmax</code> 激活函数的 <code>Dense</code> 层，输出的维度是词汇表中的单词数。为了简单起见，我们仅使用一个 <code>Transformer</code>块( <code>GPT</code> 论文中使用 <code>12</code> 个)，整体架构如下图所示。</p> 
<p><img src="https://images2.imgbox.com/eb/49/sN6veHS1_o.png" alt="GPT架构"></p> 
<p>使用 <code>Keras</code> 实现此架构：</p> 
<pre><code class="prism language-python"><span class="token comment"># 用 0 填充输入</span>
inputs <span class="token operator">=</span> layers<span class="token punctuation">.</span>Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
<span class="token comment"># 文本使用 TokenAndPositionEmbedding 层获取嵌入</span>
x <span class="token operator">=</span> TokenAndPositionEmbedding<span class="token punctuation">(</span>MAX_LEN<span class="token punctuation">,</span> VOCAB_SIZE<span class="token punctuation">,</span> EMBEDDING_DIM<span class="token punctuation">)</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
<span class="token comment"># 嵌入通过 TransformerBlock 进行传递</span>
x<span class="token punctuation">,</span> attention_scores <span class="token operator">=</span> TransformerBlock<span class="token punctuation">(</span>N_HEADS<span class="token punctuation">,</span> KEY_DIM<span class="token punctuation">,</span> EMBEDDING_DIM<span class="token punctuation">,</span> FEED_FORWARD_DIM<span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
<span class="token comment"># 转换后的输出通过使用 softmax 激活函数的 Dense 层进行传递，以预测后续单词的分布</span>
outputs <span class="token operator">=</span> layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>VOCAB_SIZE<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"softmax"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
<span class="token comment"># 模型以单词符号序列作为输入，并输出预测的后续单词分布。还返回了 Transformer 块的输出，以便检查模型中注意力机制的作用</span>
gpt <span class="token operator">=</span> models<span class="token punctuation">.</span>Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span>inputs<span class="token punctuation">,</span> outputs<span class="token operator">=</span><span class="token punctuation">[</span>outputs<span class="token punctuation">,</span> attention_scores<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># 模型使用 SparseCategoricalCrossentropy 损失函数编译并训练</span>
gpt<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span><span class="token string">"adam"</span><span class="token punctuation">,</span> loss<span class="token operator">=</span><span class="token punctuation">[</span>losses<span class="token punctuation">.</span>SparseCategoricalCrossentropy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>gpt<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
gpt<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span>epochs<span class="token operator">=</span>EPOCHS<span class="token punctuation">,</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="6_GPT__220"></a>6. GPT 分析</h3> 
<p>训练 <code>GPT</code> 模型后，就可以使用 <code>GPT</code> 生成长文本，还可以获取 <code>TransformerBlock</code> 输出的注意力权重，以了解 <code>Transformer</code> 在生成过程中不同位置寻找信息的方式。</p> 
<h4><a id="61__223"></a>6.1 生成文本</h4> 
<p>我们可以通过以下过程生成新文本：</p> 
<ul><li>将现有的单词序列输入到网络中，以预测下一个单词</li><li>将此单词附加到现有序列中，并重复此过程</li></ul> 
<p>从网络输出的一组概率中进行抽样，使文本生成过程具有随机性，而不是确定性的。<br> 为了控制采样过程的确定性程度，重用<a href="https://blog.csdn.net/LOVEmy134611/article/details/135395814"> LSTM 文本生成</a>中的 <code>TextGenerator</code> 类，其使用温度参数来指定采样过程中的确定性程度，使用两个不同的温度值对比生成结果。温度为 <code>1.0</code> 的生成文本比温度为 <code>0.5</code> 的样本具有更高的随机性，因此准确率较低。由于模型从方差较大的概率分布中进行抽样，因此使用温度为 <code>1.0</code> 生成多个样本将具有更多的多样性。</p> 
<h4><a id="62__231"></a>6.2 注意力分数</h4> 
<p>我们还可以令模型输出，在决定句子中的下一个单词时，每个单词的注意力分数。<code>TransformerBlock</code> 输出每个注意力头的注意力权重，该权重是句子中前面单词的 <code>softmax</code> 分布。<br> 为了进行说明，下图显示了对于三个不同的输入，具有最高概率的前五个符号，以及相对于前面每个单词，两个注意力头的平均注意力。根据其注意力分数对前面的单词进行着色，较深的颜色表示对该单词的注意力更高。</p> 
<p><img src="https://images2.imgbox.com/0f/ed/AjiEeic0_o.png" alt="注意力分数"></p> 
<p>以这种方式查看网络，可以准确地了解网络从何处获取信息，以便对随后的每个单词做出准确预测。通过修改输入，观察是否可以使模型关注到句子中相距较远的单词，以充分证明基于注意力的模型相对于传统的循环模型更强大。</p> 
<h3><a id="_240"></a>小结</h3> 
<p>在本节中，我们介绍了 <code>Transformer</code> 模型架构，并构建了一个 <code>GPT</code> 模型，以实现文本生成。<code>GPT</code> 利用了注意力机制，消除了对循环层(如长短时记忆网络)的需求。注意力机制类似于一个信息检索系统，利用查询、键和值来决定从每个输入符号中提取多少信息。</p> 
<h3><a id="_242"></a>系列链接</h3> 
<p><a href="https://blog.csdn.net/LOVEmy134611/article/details/132463580">AIGC实战——生成模型简介</a><br> <a href="https://blog.csdn.net/LOVEmy134611/article/details/133967709">AIGC实战——深度学习 (Deep Learning, DL)</a><br> <a href="https://blog.csdn.net/LOVEmy134611/article/details/133967726">AIGC实战——卷积神经网络(Convolutional Neural Network, CNN)</a><br> <a href="https://blog.csdn.net/LOVEmy134611/article/details/133974426">AIGC实战——自编码器(Autoencoder)</a><br> <a href="https://blog.csdn.net/LOVEmy134611/article/details/133974527">AIGC实战——变分自编码器(Variational Autoencoder, VAE)</a><br> <a href="https://blog.csdn.net/LOVEmy134611/article/details/133974538">AIGC实战——使用变分自编码器生成面部图像</a><br> <a href="https://blog.csdn.net/LOVEmy134611/article/details/133974562">AIGC实战——生成对抗网络(Generative Adversarial Network, GAN)</a><br> <a href="https://blog.csdn.net/LOVEmy134611/article/details/133974577">AIGC实战——WGAN(Wasserstein GAN)</a><br> <a href="https://blog.csdn.net/LOVEmy134611/article/details/133974588">AIGC实战——条件生成对抗网络(Conditional Generative Adversarial Net, CGAN)</a><br> <a href="https://blog.csdn.net/LOVEmy134611/article/details/135395814">AIGC实战——自回归模型(Autoregressive Model)</a><br> <a href="https://blog.csdn.net/LOVEmy134611/article/details/135514046">AIGC实战——改进循环神经网络</a><br> <a href="https://blog.csdn.net/LOVEmy134611/article/details/135634657">AIGC实战——像素卷积神经网络(PixelCNN)</a><br> <a href="https://blog.csdn.net/LOVEmy134611/article/details/135984457">AIGC实战——归一化流模型(Normalizing Flow Model)</a><br> <a href="https://blog.csdn.net/LOVEmy134611/article/details/136125887">AIGC实战——能量模型(Energy-Based Model)</a><br> <a href="https://blog.csdn.net/LOVEmy134611/article/details/136288802">AIGC实战——扩散模型(Diffusion Model)</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d4d85d9b17e3cf1792eae6f4666e64f4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">TypeScript版本不匹配警告：如何更新以兼容@typescript-eslint/typescript-estree插件</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ebc267c2e9498508fbddbaedf5bd1e6a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">SpringBoot中配置nacos</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>