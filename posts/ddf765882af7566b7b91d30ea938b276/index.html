<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Llama模型家族之Stanford NLP ReFT源代码探索 （四）Pyvene论文学习 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/ddf765882af7566b7b91d30ea938b276/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Llama模型家族之Stanford NLP ReFT源代码探索 （四）Pyvene论文学习">
  <meta property="og:description" content="LlaMA 3 系列博客 基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （一）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （二）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （三）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （四）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （五）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （六）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （七）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （八）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （九）
基于 LlaMA 3 &#43; LangGraph 在windows本地部署大模型 （十）">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-09T21:08:52+08:00">
    <meta property="article:modified_time" content="2024-06-09T21:08:52+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Llama模型家族之Stanford NLP ReFT源代码探索 （四）Pyvene论文学习</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="LlaMA_3__0"></a>LlaMA 3 系列博客</h2> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138713807?spm=1001.2014.3001.5501">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （一）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138715041?spm=1001.2014.3001.5501">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （二）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138718088?spm=1001.2014.3001.5501">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （三）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138719654?spm=1001.2014.3001.5501">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （四）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138790855" rel="nofollow">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （五）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138791888" rel="nofollow">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （六）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138794155" rel="nofollow">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （七）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138801566" rel="nofollow">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （八）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138802610" rel="nofollow">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （九）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138805410?spm=1001.2014.3001.5501">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （十）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138895944" rel="nofollow">构建安全的GenAI/LLMs核心技术解密之大模型对抗攻击（一）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138904869" rel="nofollow">构建安全的GenAI/LLMs核心技术解密之大模型对抗攻击（二）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138906078" rel="nofollow">构建安全的GenAI/LLMs核心技术解密之大模型对抗攻击（三）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138907964?spm=1001.2014.3001.5501">构建安全的GenAI/LLMs核心技术解密之大模型对抗攻击（四）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138942598" rel="nofollow">构建安全的GenAI/LLMs核心技术解密之大模型对抗攻击（五）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138891014?spm=1001.2014.3001.5501">你好 GPT-4o！</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138924149?spm=1001.2014.3001.5501">大模型标记器之Tokenizer可视化（GPT-4o）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138972848?spm=1001.2014.3001.5501">大模型标记器 Tokenizer之Byte Pair Encoding (BPE) 算法详解与示例</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138973896?spm=1001.2014.3001.5501">大模型标记器 Tokenizer之Byte Pair Encoding (BPE)源码分析</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138975453?spm=1001.2014.3001.5501">大模型之自注意力机制Self-Attention（一）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138975753?spm=1001.2014.3001.5501">大模型之自注意力机制Self-Attention（二）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/138976281?spm=1001.2014.3001.5501">大模型之自注意力机制Self-Attention（三）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138948293" rel="nofollow">基于 LlaMA 3 + LangGraph 在windows本地部署大模型 （十一）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/138998916" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之 Code Llama （一）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139002038" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之 Code Llama （二）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139002334" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之 Code Llama （三）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139003284" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之 Code Llama （四）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139003246" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之 Code Llama （五）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139005323" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之使用 Llama Guard 保护大模型对话（一）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139006237" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之使用 Llama Guard 保护大模型对话（二）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139002334">Llama 3 模型家族构建安全可信赖企业级AI应用之使用 Llama Guard 保护大模型对话（三）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139016349?spm=1001.2014.3001.5501">大模型之深入理解Transformer位置编码（Positional Embedding）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139026612?spm=1001.2014.3001.5501">大模型之深入理解Transformer Layer Normalization（一）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139028322" rel="nofollow">大模型之深入理解Transformer Layer Normalization（二）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139031974?spm=1001.2014.3001.5501">大模型之深入理解Transformer Layer Normalization（三）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139034624?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（一）初学者的起点</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139035105?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（二）矩阵操作的演练 </a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139035270?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（三）初始化一个嵌入层 </a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139035455?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（四）预先计算 RoPE 频率 </a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139038167?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（五）预先计算因果掩码 </a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139039119?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（六）首次归一化：均方根归一化（RMSNorm）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139039795?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（七） 初始化多查询注意力</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139044003?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（八）旋转位置嵌入</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139044601?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（九） 计算自注意力</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139047106?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（十） 残差连接及SwiGLU FFN</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139047892?spm=1001.2014.3001.5501">大模型之一步一步使用PyTorch编写Meta的Llama 3代码（十一）输出概率分布 及损失函数计算</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139048668?spm=1001.2014.3001.5501">大模型之使用PyTorch编写Meta的Llama 3实际功能代码（一）加载简化分词器及设置参数</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139055000" rel="nofollow">大模型之使用PyTorch编写Meta的Llama 3实际功能代码（二）RoPE 及注意力机制</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139058555" rel="nofollow">大模型之使用PyTorch编写Meta的Llama 3实际功能代码（三） FeedForward 及 Residual Layers</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139061993" rel="nofollow">大模型之使用PyTorch编写Meta的Llama 3实际功能代码（四） 构建 Llama3 类模型本身</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139062160" rel="nofollow">大模型之使用PyTorch编写Meta的Llama 3实际功能代码（五）训练并测试你自己的 minLlama3</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139062700" rel="nofollow">大模型之使用PyTorch编写Meta的Llama 3实际功能代码（六）加载已经训练好的miniLlama3模型</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139063356" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之使用 Llama Guard 保护大模型对话 （四）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139063994" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之使用 Llama Guard 保护大模型对话 （五）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139065219" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之使用 Llama Guard 保护大模型对话 （六）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139066008" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之使用 Llama Guard 保护大模型对话 （七）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139066714?spm=1001.2014.3001.5501">Llama 3 模型家族构建安全可信赖企业级AI应用之使用 Llama Guard 保护大模型对话 （八）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139080509" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之 CyberSecEval 2：量化 LLM 安全和能力的基准（一）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139089252" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之 CyberSecEval 2：量化 LLM 安全和能力的基准（二）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139089533" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之 CyberSecEval 2：量化 LLM 安全和能力的基准（三）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139090860?spm=1001.2014.3001.5501">Llama 3 模型家族构建安全可信赖企业级AI应用之 CyberSecEval 2：量化 LLM 安全和能力的基准（四）</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139140454" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之code shield（一）Code Shield简介</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139141252" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之code shield（二）防止 LLM 生成不安全代码</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139142244" rel="nofollow">Llama 3 模型家族构建安全可信赖企业级AI应用之code shield（三）Code Shield代码示例</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139144847" rel="nofollow">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（一） LLaMA-Factory简介</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139148621?spm=1001.2014.3001.5501">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（二） LLaMA-Factory训练方法及数据集</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139163567" rel="nofollow">大模型之Ollama：在本地机器上释放大型语言模型的强大功能</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139171614" rel="nofollow">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（三）通过Web UI微调</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139172082" rel="nofollow">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（四）通过命令方式微调</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139173930" rel="nofollow">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（五） 基于已训练好的模型进行推理</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139175166?spm=1001.2014.3001.5501">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（六）Llama 3 已训练的大模型合并LoRA权重参数</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139238948" rel="nofollow">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（七） 使用 LoRA 微调 LLM 的实用技巧</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139239746" rel="nofollow">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（八） 使用 LoRA 微调 LLM 的实用技巧</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139240891" rel="nofollow">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（九） 使用 LoRA 微调常见问题答疑</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139254567" rel="nofollow">Llama模型家族之使用 Supervised Fine-Tuning（SFT）微调预训练Llama 3 语言模型（十） 使用 LoRA 微调常见问题答疑</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139263736" rel="nofollow">Llama模型家族训练奖励模型Reward Model技术及代码实战（一）简介</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139264495" rel="nofollow">Llama模型家族训练奖励模型Reward Model技术及代码实战（二）从用户反馈构建比较数据集</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139265624?spm=1001.2014.3001.5501">Llama模型家族训练奖励模型Reward Model技术及代码实战（三） 使用 TRL 训练奖励模型</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139349461" rel="nofollow">Llama模型家族之RLAIF 基于 AI 反馈的强化学习（一）RLHF简介</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139350327" rel="nofollow">Llama模型家族之RLAIF 基于 AI 反馈的强化学习（二）RLHF 与RAIF比较</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139350917" rel="nofollow">Llama模型家族之RLAIF 基于 AI 反馈的强化学习（三） RLAIF 的工作原理</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139351470" rel="nofollow">Llama模型家族之RLAIF 基于 AI 反馈的强化学习（四）RLAIF 优势</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139351709" rel="nofollow">Llama模型家族之RLAIF 基于 AI 反馈的强化学习（五）RLAIF 挑战</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139353505" rel="nofollow">Llama模型家族之RLAIF 基于 AI 反馈的强化学习（六） RLAIF 代码实战</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139383775?spm=1001.2014.3001.5501">Llama模型家族之RLAIF 基于 AI 反馈的强化学习（七） RLAIF 代码实战</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139433868" rel="nofollow">Llama模型家族之RLAIF 基于 AI 反馈的强化学习（八） RLAIF 代码实战</a></p> 
<p><a href="https://duanzhihua.blog.csdn.net/article/details/139435493" rel="nofollow">Llama模型家族之RLAIF 基于 AI 反馈的强化学习（九） RLAIF 代码实战</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139435932?spm=1001.2014.3001.5501">Llama模型家族之RLAIF 基于 AI 反馈的强化学习（十） RLAIF 代码实战</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139509724?spm=1001.2014.3001.5501">Llama模型家族之拒绝抽样(Rejection Sampling)（一）</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139510111?spm=1001.2014.3001.5501">Llama模型家族之拒绝抽样(Rejection Sampling)（二）均匀分布简介</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139540398?spm=1001.2014.3001.5501">Llama模型家族之拒绝抽样(Rejection Sampling)（三）确定缩放常数以优化拒绝抽样方法</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139542066?spm=1001.2014.3001.5501">Llama模型家族之拒绝抽样(Rejection Sampling)（四） 蒙特卡罗方法在拒绝抽样中的应用：评估线与样本接受标准</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139543105?spm=1001.2014.3001.5501">Llama模型家族之拒绝抽样(Rejection Sampling)（五） 蒙特卡罗算法在拒绝抽样中：均匀分布与样本接受标准</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139543305?spm=1001.2014.3001.5501">Llama模型家族之拒绝抽样(Rejection Sampling)（六） 拒绝抽样中的蒙特卡罗算法：重复过程与接受标准</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139543600?spm=1001.2014.3001.5501">Llama模型家族之拒绝抽样(Rejection Sampling)（七） 优化拒绝抽样：选择高斯分布以减少样本拒绝</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139543726?spm=1001.2014.3001.5501">Llama模型家族之拒绝抽样(Rejection Sampling)（八） 代码实现</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139547248?spm=1001.2014.3001.5501">Llama模型家族之拒绝抽样(Rejection Sampling)（九） 强化学习之Rejection Sampling</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139550316?spm=1001.2014.3001.5501">Llama模型家族之使用 ReFT技术对 Llama-3 进行微调（一）ReFT简介</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139550481?spm=1001.2014.3001.5501">Llama模型家族之使用 ReFT技术对 Llama-3 进行微调（二） PyReFT简介</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139550837?spm=1001.2014.3001.5501">Llama模型家族之使用 ReFT技术对 Llama-3 进行微调（三）为 ReFT 微调准备模型及数据集</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139551554?spm=1001.2014.3001.5501">Llama模型家族之使用 ReFT技术对 Llama-3 进行微调（四） ReFT 微调训练及模型推理</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139554666?spm=1001.2014.3001.5501">Llama模型家族之Stanford NLP ReFT源代码探索 （一）数据预干预</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139557359?spm=1001.2014.3001.5501">Llama模型家族之Stanford NLP ReFT源代码探索 （二）interventions.py 代码解析</a></p> 
<p><a href="https://blog.csdn.net/duan_zhihua/article/details/139562405?spm=1001.2014.3001.5501">Llama模型家族之Stanford NLP ReFT源代码探索 （三）reft_model.py代码解析</a></p> 
<h2><a id="LlamaStanford_NLP_ReFT_Pyvene_232"></a>Llama模型家族之Stanford NLP ReFT源代码探索 （四）Pyvene学习</h2> 
<p><img src="https://images2.imgbox.com/46/dc/XyvjA4OK_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="Pyvene_235"></a>Pyvene论文</h2> 
<p><a href="https://arxiv.org/abs/2403.07809" rel="nofollow">https://arxiv.org/abs/2403.07809</a><br> <img src="https://images2.imgbox.com/cf/50/zRd0x7vo_o.png" alt="在这里插入图片描述"><br> 这篇论文介绍了一个名为pyvene的Python库，用于在PyTorch模型中进行自定义干预操作。这些干预可以是静态的或包含可训练参数的复杂方案，并且可以通过直观的配置格式轻松实现。论文通过因果抽象和知识定位等解释性分析展示了该库的强大功能，并将其发布到Python Package Index(PyPI)上，提供了代码、文档和教程。这个库为神经模型的干预提供了一种统一而灵活的框架，并支持与其他研究人员分享干预后的模型。</p> 
<p><img src="https://images2.imgbox.com/15/b0/yYlAG9r6_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/49/c7/CTnG7N3U_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/32/28/znBDOpkL_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/d6/f2/MxpmKX0S_o.png" alt="在这里插入图片描述"></p> 
<p>在TinyStories-33M上进行推理时干预（Li等人，2023a）。模型被提示为“从前有一个”，并被要求完成这个故事。我们在每个解码步骤的所有层的MLP输出中添加了一个静态词嵌入（代表“快乐”或“悲伤”），其系数为0.3。提供了pyvene的完整实现。原始生成和干预后的生成都使用了贪婪解码。<br> <img src="https://images2.imgbox.com/11/e8/qrYAtHlM_o.png" alt="在这里插入图片描述"><br> 论文实验<br> 本文介绍了使用PyVene库进行模型干预和解释的案例研究。进行了两个实验：</p> 
<ul><li> <p>第一个实验是仿照Meng等人（2022）的工作，在GPT-2 XL中寻找事实之间的关联。在这个任务中，首先通过添加高斯噪声来干扰输入嵌入，并然后恢复每个层中的特定激活以识别与结果相关的信息。具体来说，他们恢复了每个标记在每个层中的Transformer块输出、MLP激活和注意力输出。实验结果显示，这种方法可以有效地找到事实之间的关联。</p> </li><li> <p>第二个实验是在一个简单的性别代词预测任务中展示干预和探针训练的效果。在这个任务中，使用了一维分布式对齐搜索，试图学习一个表示性别的子空间。他们使用了一个固定的长度为4的模板，其中名字是从一个词汇表中随机选择的，包括47个男性常用的名字和10个女性常用的名字，以及相应的性别代词作为输出标记。实验结果显示，可训练的干预可以在各个层和位置上找到更稀疏的性别表示，而线性探针则几乎在所有组件上都实现了100％的分类准确率。这表明即使在不相关的因果关系中，探针也可能实现很高的性能。</p> </li><li> <p>这些实验展示了PyVene库在模型干预和解释方面的强大功能。</p> </li></ul> 
<h3><a id="_261"></a>论文总结</h3> 
<p>本文介绍了一个名为pyvene的Python库，支持干预研究在神经模型上的应用。该库具有以下优点：</p> 
<ul><li>支持自定义干预类型和不同类型的模型架构。</li><li>支持复杂的干预方案，并且可以共享干预后的模型。</li><li>可以通过在线模型中心（如HuggingFace）与他人分享干预后的模型。</li><li>提供了灵活的方法来解释和改进模型。</li></ul> 
<h3><a id="_271"></a>未来展望</h3> 
<p>提出了两个主要的研究方向：</p> 
<ul><li>扩展默认的干预类型和模型类型。虽然pyvene是可扩展的，但拥有更多的内置类型可以帮助 更容易地吸引新用户。</li><li>pyvene旨在支持复杂的干预方案，但这会导致计算效率低下。随着语言模型越来越大， 希望调查如何通过多节点和多GPU训练来提高干预效率。</li></ul> 
<p>对模型内部状态的干预是人工智能许多领域的基本操作，包括模型编辑、转向、鲁棒性和可解释性。为了促进这种研究，我们引入了pyrene，这是一个开源Python库，支持对一系列不同PyTorch模块的可定制干预。pyrvene以直观的配置格式支持复杂的干预方案，其干预可以是静态的或包括可训练的参数。</p> 
<h2><a id="_InferenceTime_Intervention_290"></a>论文 Inference-Time Intervention</h2> 
<p><a href="https://arxiv.org/pdf/2306.03341" rel="nofollow">Inference-Time Intervention</a></p> 
<p><img src="https://images2.imgbox.com/69/58/dj10PV1I_o.png" alt="在这里插入图片描述"><br> 这篇论文介绍了一种名为“推理时间干预（ITI）”的技术，旨在提高大型语言模型（LLM）的“实际性”。该技术通过在推断过程中改变模型激活，在有限数量的注意力头中沿着一组方向进行调整，从而显著提高了LLAMA模型在TruthfulQA基准测试上的表现。论文使用了一个名为Alpaca的指令微调LLAMA，并将ITI应用于它，将其实际性从32.5％提高到65.1％。此外，作者还发现了一个实际性与帮助性之间的权衡，并展示了如何通过调整干预强度来平衡它们。ITI是一种最小侵入性和计算上经济实惠的技术，而且数据效率高：尽管需要大量的注释才能使RLHF等方法生效，但ITI仅使用几百个示例即可定位真实的方向。论文的研究结果表明，即使表面上产生谎言，LLM可能也具有内部表示某件事可能是实际的的能力。<br> <img src="https://images2.imgbox.com/28/3d/l38VD9HC_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/80/4b/W978Pjtl_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/66/84/dJQH8y8j_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_309"></a>大模型技术分享</h2> 
<p><img src="https://images2.imgbox.com/3e/f6/9elY36PF_o.jpg" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/3e/51/ZJAFMokH_o.jpg" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/97/75/2jqebYF0_o.jpg" alt="在这里插入图片描述"></p> 
<h2><a id="LLM_317"></a>《企业级生成式人工智能LLM大模型技术、算法及案例实战》线上高级研修讲座</h2> 
<pre><code>模块一：Generative AI 原理本质、技术内核及工程实践周期详解
模块二：工业级 Prompting 技术内幕及端到端的基于LLM 的会议助理实战
模块三：三大 Llama 2 模型详解及实战构建安全可靠的智能对话系统
模块四：生产环境下 GenAI/LLMs 的五大核心问题及构建健壮的应用实战
模块五：大模型应用开发技术：Agentic-based 应用技术及案例实战
模块六：LLM 大模型微调及模型 Quantization 技术及案例实战
模块七：大模型高效微调 PEFT 算法、技术、流程及代码实战进阶
模块八：LLM 模型对齐技术、流程及进行文本Toxicity 分析实战
模块九：构建安全的 GenAI/LLMs 核心技术Red Teaming 解密实战
模块十：构建可信赖的企业私有安全大模型Responsible AI 实战 
</code></pre> 
<h2><a id="Llama3Responsible_AI_330"></a>Llama3关键技术深度解析与构建Responsible AI、算法及开发落地实战</h2> 
<p>1、Llama开源模型家族大模型技术、工具和多模态详解：学员将深入了解Meta Llama 3的创新之处，比如其在语言模型技术上的突破，并学习到如何在Llama 3中构建trust and safety AI。他们将详细了解Llama 3的五大技术分支及工具，以及如何在AWS上实战Llama指令微调的案例。<br> 2、解密Llama 3 Foundation Model模型结构特色技术及代码实现：深入了解Llama 3中的各种技术，比如Tiktokenizer、KV Cache、Grouped Multi-Query Attention等。通过项目二逐行剖析Llama 3的源码，加深对技术的理解。<br> 3、解密Llama 3 Foundation Model模型结构核心技术及代码实现：SwiGLU Activation Function、FeedForward Block、Encoder Block等。通过项目三学习Llama 3的推理及Inferencing代码，加强对技术的实践理解。<br> 4、基于LangGraph on Llama 3构建Responsible AI实战体验：通过项目四在Llama 3上实战基于LangGraph的Responsible AI项目。他们将了解到LangGraph的三大核心组件、运行机制和流程步骤，从而加强对Responsible AI的实践能力。<br> 5、Llama模型家族构建技术构建安全可信赖企业级AI应用内幕详解：深入了解构建安全可靠的企业级AI应用所需的关键技术，比如Code Llama、Llama Guard等。项目五实战构建安全可靠的对话智能项目升级版，加强对安全性的实践理解。<br> 6、Llama模型家族Fine-tuning技术与算法实战：学员将学习Fine-tuning技术与算法，比如Supervised Fine-Tuning(SFT)、Reward Model技术、PPO算法、DPO算法等。项目六动手实现PPO及DPO算法，加强对算法的理解和应用能力。<br> 7、Llama模型家族基于AI反馈的强化学习技术解密：深入学习Llama模型家族基于AI反馈的强化学习技术，比如RLAIF和RLHF。项目七实战基于RLAIF的Constitutional AI。<br> 8、Llama 3中的DPO原理、算法、组件及具体实现及算法进阶：学习Llama 3中结合使用PPO和DPO算法，剖析DPO的原理和工作机制，详细解析DPO中的关键算法组件，并通过综合项目八从零开始动手实现和测试DPO算法，同时课程将解密DPO进阶技术Iterative DPO及IPO算法。<br> 9、Llama模型家族Safety设计与实现：在这个模块中，学员将学习Llama模型家族的Safety设计与实现，比如Safety in Pretraining、Safety Fine-Tuning等。构建安全可靠的GenAI/LLMs项目开发。<br> 10、Llama 3构建可信赖的企业私有安全大模型Responsible AI系统：构建可信赖的企业私有安全大模型Responsible AI系统，掌握Llama 3的Constitutional AI、Red Teaming。</p> 
<h2><a id="Sora_343"></a>解码Sora架构、技术及应用</h2> 
<p>一、为何Sora通往AGI道路的里程碑？<br> 1，探索从大规模语言模型(LLM)到大规模视觉模型(LVM)的关键转变，揭示其在实现通用人工智能(AGI)中的作用。<br> 2，展示Visual Data和Text Data结合的成功案例，解析Sora在此过程中扮演的关键角色。<br> 3，详细介绍Sora如何依据文本指令生成具有三维一致性(3D consistency)的视频内容。 4，解析Sora如何根据图像或视频生成高保真内容的技术路径。<br> 5，探讨Sora在不同应用场景中的实践价值及其面临的挑战和局限性。</p> 
<p>二、解码Sora架构原理<br> 1，DiT (Diffusion Transformer)架构详解<br> 2，DiT是如何帮助Sora实现Consistent、Realistic、Imaginative视频内容的？<br> 3，探讨为何选用Transformer作为Diffusion的核心网络，而非技术如U-Net。<br> 4，DiT的Patchification原理及流程，揭示其在处理视频和图像数据中的重要性。<br> 5，Conditional Diffusion过程详解，及其在内容生成过程中的作用。<br> 三、解码Sora关键技术解密<br> 1，Sora如何利用Transformer和Diffusion技术理解物体间的互动，及其对模拟复杂互动场景的重要性。<br> 2，为何说Space-time patches是Sora技术的核心，及其对视频生成能力的提升作用。<br> 3，Spacetime latent patches详解，探讨其在视频压缩和生成中的关键角色。<br> 4，Sora Simulator如何利用Space-time patches构建digital和physical世界，及其对模拟真实世界变化的能力。<br> 5，Sora如何实现faithfully按照用户输入文本而生成内容，探讨背后的技术与创新。<br> 6，Sora为何依据abstract concept而不是依据具体的pixels进行内容生成，及其对模型生成质量与多样性的影响。</p> 
<h3><a id="GPT__365"></a>GPT 自回归语言模型架构、数学原理及内幕-简介</h3> 
<p></p> 
<div class="csdn-video-box"> 
 <iframe id="ShQFUHGA-1717850042929" frameborder="0" src="https://player.bilibili.com/player.html?aid=1505327068" allowfullscreen="true" data-mediaembed="bilibili"></iframe> 
 <p>GPT 自回归语言模型架构、数学原理及内幕-简介</p> 
</div> 
<p></p> 
<h3><a id="_Transformer__Rasa_Internals__Retrieval_Model__370"></a>基于 Transformer 的 Rasa Internals 解密之 Retrieval Model 剖析-简介</h3> 
<p></p> 
<div class="csdn-video-box"> 
 <iframe id="0iof4qCM-1717850117930" frameborder="0" src="https://player.bilibili.com/player.html?aid=1155345003" allowfullscreen="true" data-mediaembed="bilibili"></iframe> 
 <p>基于 Transformer 的 Rasa Internals 解密之 Retrieval Model 剖析-简介</p> 
</div> 
<p></p> 
<h3><a id="Transformer_376"></a>Transformer语言模型架构、数学原理及内幕机制-简介</h3> 
<p></p> 
<div class="csdn-video-box"> 
 <iframe id="tU7gZQuB-1717850149773" frameborder="0" src="https://player.bilibili.com/player.html?aid=1505464319" allowfullscreen="true" data-mediaembed="bilibili"></iframe> 
 <p>Transformer语言模型架构、数学原理及内幕机制-简介</p> 
</div> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/841783d3a923462cb2a6448216778028/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">kafka如何保证消息不丢失</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a48d48d6cd9032d86c343fbf64b405c2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">联合体和枚举＜C语言＞</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>