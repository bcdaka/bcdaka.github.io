<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>中文大语言模型 Llama-2 7B（或13B） 本地化部署 （国内云服务器、GPU单卡16GB、中文模型、WEB页面TextUI、简单入门） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/1334b8dfff875bce0c97d25812df8881/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="中文大语言模型 Llama-2 7B（或13B） 本地化部署 （国内云服务器、GPU单卡16GB、中文模型、WEB页面TextUI、简单入门）">
  <meta property="og:description" content="本文目的是让大家先熟悉模型的部署，简单入门；所以只需要很小的算力，单台服务器 单GPU显卡（显存不低于12GB），操作系统需要安装 Ubuntu 18.04。
1 服务器&amp;操作系统 1.1服务器的准备 准备一台服务器 单张英伟达GPU显卡（显存不低于12GB），操作系统需要安装 Ubuntu 18.04 （具体安装过程忽略）。 重装系统前注意备份现有存储上的相关重要数据。 GPU显卡驱动先不安装； 后续介绍驱动和CUDA的安装步骤。
如果手上没有相应的服务器硬件设备、可以购买带GPU的云服务器， 以下可供选择参考。
上云精选_云服务器秒杀_开发者上云推荐-腾讯云腾讯云推出云产品限时特惠抢购活动：2C2G云服务器9元/月起；还有短信、CDN、COS等热门云产品限时特惠...https://cloud.tencent.com/act/pro/seckill_season?from=20210
地域选择国内适合的城市； 预装镜像为 Ubuntu 18.04
购买后一般云厂商会自动安装显卡的驱动和CUDA ， 因为目前大部分的项目对 cuda 11.7 和 11.8 版本兼容的比较好，后续我们要指定安装特定的版本； 所以如果云主机已经预装了GPU驱动，我们需要卸载。 如果不了解卸载的指令，可以在控制台 点击“重装系统”； 然后 注意 不要勾选后台自动安装 GPU 驱动； 这样重置后就是一台干净的 Ubuntu18.04的系统，且没有安装GPU的驱动。
1.2系统工具和环境 安装一些基础的相关软件工具
sudo apt update sudo apt install -y vim git wget curl net-tools language-pack-zh-hans language-pack-en 中文环境
echo &#39; LANG=&#34;en_US.utf8&#34;; export LANG LANGUAGE=&#34;en_US.utf8&#34;; export LANGUAGE LC_ALL=&#34;en_US.utf8&#34;; export LC_ALL LC_CTYPE=&#34;en_US.utf8&#34;; export LC_CTYPE SUPPORTED=en_US.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-09-05T11:04:53+08:00">
    <meta property="article:modified_time" content="2023-09-05T11:04:53+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">中文大语言模型 Llama-2 7B（或13B） 本地化部署 （国内云服务器、GPU单卡16GB、中文模型、WEB页面TextUI、简单入门）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 id="%E2%80%8B%E7%BC%96%E8%BE%91"><img alt="" height="720" src="https://images2.imgbox.com/88/56/8uHR747S_o.png" width="1200"></h2> 
<p></p> 
<p>        本文目的是让大家先熟悉模型的部署，简单入门；所以只需要很小的算力，单台服务器 单GPU显卡（显存不低于12GB），操作系统需要安装 Ubuntu 18.04。</p> 
<h2 id="1%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%87%86%E5%A4%87"><strong>1 服务器&amp;操作系统 </strong></h2> 
<h3>        1.1服务器的准备</h3> 
<p>        准备一台服务器 单张英伟达GPU显卡（显存不低于12GB），操作系统需要安装 Ubuntu 18.04 （具体安装过程忽略）。 重装系统前注意备份现有存储上的相关重要数据。 GPU显卡驱动先不安装； 后续介绍驱动和CUDA的安装步骤。</p> 
<p>        如果手上没有相应的服务器硬件设备、可以购买带GPU的云服务器， 以下可供选择参考。</p> 
<p><a class="has-card" href="https://cloud.tencent.com/act/pro/seckill_season?from=20210" rel="nofollow" title=" 上云精选_云服务器秒杀_开发者上云推荐-腾讯云"><span class="link-card-box"><span class="link-title"> 上云精选_云服务器秒杀_开发者上云推荐-腾讯云</span><span class="link-desc">腾讯云推出云产品限时特惠抢购活动：2C2G云服务器9元/月起；还有短信、CDN、COS等热门云产品限时特惠...</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/a0/4c/g72OMQBp_o.png" alt="icon-default.png?t=N7T8">https://cloud.tencent.com/act/pro/seckill_season?from=20210</span></span></a></p> 
<p><img alt="" height="423" src="https://images2.imgbox.com/5a/4f/E14fr6QE_o.png" width="267"></p> 
<p></p> 
<p>地域选择国内适合的城市；   预装镜像为 Ubuntu 18.04<img alt="" height="544" src="https://images2.imgbox.com/bf/f4/qNa5AFE0_o.png" width="779"></p> 
<p>        购买后一般云厂商会自动安装显卡的驱动和CUDA ， 因为目前大部分的项目对 cuda 11.7 和 11.8 版本兼容的比较好，后续我们要指定安装特定的版本； 所以如果云主机已经预装了GPU驱动，我们需要卸载。 如果不了解卸载的指令，可以在控制台 点击“重装系统”； </p> 
<p><img alt="" height="489" src="https://images2.imgbox.com/b3/87/ijNZOEpz_o.png" width="1200">        然后 注意 不要勾选后台自动安装 GPU 驱动； 这样重置后就是一台干净的 Ubuntu18.04的系统，且没有安装GPU的驱动。</p> 
<p><img alt="" height="929" src="https://images2.imgbox.com/e7/c3/i29J15US_o.png" width="1002"></p> 
<p></p> 
<h3>1.2系统工具和环境 </h3> 
<p>   安装一些基础的相关软件工具</p> 
<pre><code class="language-bash">sudo apt update
sudo apt install -y vim git wget curl net-tools language-pack-zh-hans language-pack-en</code></pre> 
<p>中文环境</p> 
<pre><code class="language-bash">echo '
LANG="en_US.utf8"; export LANG
LANGUAGE="en_US.utf8"; export LANGUAGE
LC_ALL="en_US.utf8"; export LC_ALL
LC_CTYPE="en_US.utf8"; export LC_CTYPE
SUPPORTED=en_US.UTF8:en_US:en; export SUPPORTED

TZ="Asia/Shanghai"; export TZ
' &gt;&gt; ~/.profile</code></pre> 
<pre><code class="language-bash">source ~/.profile
locale
#确认当前是UTF8</code></pre> 
<p></p> 
<h2 id="2%20%E5%AE%89%E8%A3%85%C2%A0%20%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8%C2%A0%20%E5%92%8C%C2%A0%20CUDA">2 安装  显卡驱动  和  CUDA</h2> 
<h3>2.1 确认操作系统当前没有安装驱动 </h3> 
<p>        确认当前系统没有安装GPU显卡驱动， 且没有加载nvidia相关模块</p> 
<pre><code class="language-bash">lsmod | grep nvidia
sudo lsof /dev/nvidia*

#以上命令 应该没有输出 nvidia 等字样的信息 </code></pre> 
<p>                 </p> 
<h3> 2.2 如何安装指定版本的显卡驱动（可选）</h3> 
<p>        后续 2.3 中 cuda 安装包中含有配合的显卡驱动，可以一步安装； 如果需要指定安装和 cuda 包中不同版本的驱动，可参考本节， 否则建议直接跳至 2.3 节。 </p> 
<p>根据显卡型号下载适合的驱动</p> 
<p><a class="has-card" href="https://www.nvidia.com/download/index.aspx" rel="nofollow" title="Official Drivers | NVIDIA"><span class="link-card-box"><span class="link-title">Official Drivers | NVIDIA</span><span class="link-desc">Download latest drivers for NVIDIA products including GeForce, TITAN, NVIDIA RTX, Data Center, GRID and more.</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/9b/29/2UzKomvK_o.png" alt="icon-default.png?t=N7T8">https://www.nvidia.com/download/index.aspx</span></span></a><a href="https://www.nvidia.com/en-us/drivers/unix/" rel="nofollow" title="Unix Drivers | NVIDIA">Unix Drivers | NVIDIA</a><br><a href="https://www.nvidia.com/en-us/drivers/unix/linux-amd64-display-archive/" rel="nofollow" title="Linux AMD64 Display Driver Archive | NVIDIA">Linux AMD64 Display Driver Archive | NVIDIA</a></p> 
<p> <img alt="" height="291" src="https://images2.imgbox.com/b5/f9/JUw9jGzs_o.png" width="612"></p> 
<p> 下载并安装显卡驱动 （需要 root 权限）</p> 
<pre><code class="language-bash">wget https://us.download.nvidia.com/XFree86/Linux-x86_64/535.98/NVIDIA-Linux-x86_64-535.98.run
sudo sh ./NVIDIA-Linux-x86_64-535.98.run
tail /var/log/nvidia-installer.log</code></pre> 
<p></p> 
<h3>2.3 安装 cuda （可一并安装显卡驱动） </h3> 
<p><a class="has-card" href="https://developer.nvidia.com/cuda-toolkit-archive" rel="nofollow" title="CUDA Toolkit Archive | NVIDIA Developer"><span class="link-card-box"><span class="link-title">CUDA Toolkit Archive | NVIDIA Developer</span><span class="link-desc">Previous releases of the CUDA Toolkit, GPU Computing SDK, documentation and developer drivers can be found using the links below. Please select the release you want from the list below, and be sure to check www.nvidia.com/drivers for more recent production drivers appropriate for your hardware configuration.</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/5d/df/8fcdgZtB_o.png" alt="icon-default.png?t=N7T8">https://developer.nvidia.com/cuda-toolkit-archive</span></span></a>下载 cuda 11.8 版本安装包 （内含 显卡驱动）</p> 
<pre><code class="language-bash">wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run
</code></pre> 
<p><a class="has-card" href="https://developer.nvidia.com/cuda-11-8-0-download-archive" rel="nofollow" title="CUDA Toolkit 11.8 Downloads | NVIDIA Developer"><span class="link-card-box"><span class="link-title">CUDA Toolkit 11.8 Downloads | NVIDIA Developer</span><span class="link-desc">Resources CUDA Documentation/Release NotesMacOS Tools Training Sample Code Forums Archive of Previous CUDA Releases FAQ Open Source PackagesSubmit a BugTarball and Zip Archive Deliverables</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/2d/8b/vTnf52sR_o.png" alt="icon-default.png?t=N7T8">https://developer.nvidia.com/cuda-11-8-0-download-archive</span></span></a></p> 
<p><img alt="" height="760" src="https://images2.imgbox.com/ed/c4/LxtdhenW_o.png" width="1200"></p> 
<p>安装 cuda 需要 root 权限; </p> 
<p>如跳过上节没有安装显卡驱动则：要勾选安装驱动项； 否则 ， 要去掉安装驱动的勾选项</p> 
<pre><code class="language-bash">sudo sh cuda_11.8.0_520.61.05_linux.run </code></pre> 
<h3>最后运行 nvidia-smi 确认 驱动 和 cuda 安装成功</h3> 
<pre><code class="language-bash">tail /var/log/cuda-installer.log
nvidia-smi</code></pre> 
<p>注意 ： 使用 nvidia-smi  查看  <span style="color:#fe2c24;">CUDA 版本必须是 11.8</span></p> 
<p><img alt="" height="455" src="https://images2.imgbox.com/8e/87/NavUcpFn_o.png" width="870"></p> 
<p></p> 
<h2 id="3%20%E5%87%86%E5%A4%87%C2%A0%20Python%20%E7%8E%AF%E5%A2%83%C2%A0%20%EF%BC%88%E5%AE%89%E8%A3%85conda%20%26%20%E9%85%8D%E7%BD%AE%E5%9B%BD%E5%86%85%E9%95%9C%E5%83%8F%E6%BA%90%EF%BC%89">3 准备  Python 环境  （安装conda &amp; 配置国内镜像源）</h2> 
<h3 id="%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0%C2%A03.1%E5%AE%89%E8%A3%85%20conda">        3.1安装 conda</h3> 
<pre><code class="language-bash">wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh
sh Miniconda3-py39_4.12.0-Linux-x86_64.sh
#都回答  yes
source ~/.bashrc
</code></pre> 
<h3>        3.2 <strong>为 pip 配置国内镜像源</strong></h3> 
<pre><code class="language-bash">pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/simple
pip config set install.trusted-host mirrors.tuna.tsinghua.edu.cn</code></pre> 
<h3>        3.3 <strong>为 conda 配置国内镜像源</strong></h3> 
<p>        编辑当前用户下的 .condarc 文件   </p> 
<pre><code class="language-bash">conda config --set show_channel_urls yes
vim ~/.condarc</code></pre> 
<p>        替换成如下内容：</p> 
<pre><code class="language-bash">channels:
  - defaults
show_channel_urls: true
default_channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
custom_channels:
  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</code></pre> 
<p>         清除索引缓存</p> 
<pre><code class="language-bash">conda clean --all --yes
conda clean -i</code></pre> 
<p></p> 
<h3>        3.4 以下国内常用的 pip 镜像源 仅供参考</h3> 
<pre><code class="language-bash">清华：https://mirrors.tuna.tsinghua.edu.cn/simple

阿里云：http://mirrors.aliyun.com/pypi/simple/

中国科技大学: https://pypi.mirrors.ustc.edu.cn/simple/

豆瓣：http://pypi.douban.com/simple/</code></pre> 
<p></p> 
<h3>        3.5 常用conda命令 仅供参考</h3> 
<pre><code class="language-bash">创建虚拟环境：conda create -n 环境名称 python=版本号
查看已有虚拟环境：conda env list
激活虚拟环境：conda activate 环境名称
删除虚拟环境：conda remove -n 环境名称 --all
查看当前环境下已安装的包：conda list
导出当前环境下的包：conda env export &gt; environment.yml
根据导出的包安装环境：conda env create -f environment.yml
安装包：conda install 包名
安装下载到本地的包：conda install --use-local  包路径
卸载当前环境下包：conda uninstall 包名
卸载指定虚拟环境中的包：conda remove --name 环境名称 包名</code></pre> 
<p><br>  </p> 
<h2 id="%E5%88%9B%E5%BB%BA%E7%8E%AF%E5%A2%83%20%E5%AE%89%E8%A3%85%E7%9B%B8%E5%85%B3%E5%8C%85%C2%A0">4 创建环境 安装相关包</h2> 
<h3>        4.1 Conda创建一个新的环境</h3> 
<pre><code class="language-bash"> conda create -n llm python=3.10.9
 conda activate llm</code></pre> 
<p><img alt="" height="50" src="https://images2.imgbox.com/67/54/tN1AlmeE_o.png" width="637"></p> 
<h3></h3> 
<h3>         4.2安装 Web 交互 UI 工程 text-generation-webui</h3> 
<p><a class="has-card" href="https://github.com/oobabooga/text-generation-webui" title="GitHub - oobabooga/text-generation-webui: A Gradio web UI for Large Language Models. Supports transformers, GPTQ, llama.cpp (ggml), Llama models."><span class="link-card-box"><span class="link-title">GitHub - oobabooga/text-generation-webui: A Gradio web UI for Large Language Models. Supports transformers, GPTQ, llama.cpp (ggml), Llama models.</span><span class="link-desc">A Gradio web UI for Large Language Models. Supports transformers, GPTQ, llama.cpp (ggml), Llama models. - GitHub - oobabooga/text-generation-webui: A Gradio web UI for Large Language Models. Supports transformers, GPTQ, llama.cpp (ggml), Llama models.</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/ed/8a/DyyEvNdY_o.png" alt="icon-default.png?t=N7T8">https://github.com/oobabooga/text-generation-webui</span></span></a></p> 
<p><img alt="" height="362" src="https://images2.imgbox.com/be/3f/cSOoJapV_o.png" width="1157"></p> 
<pre><code class="language-bash">wget https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga_linux.zip
unzip oobabooga_linux.zip
cd oobabooga_linux/
conda activate llm
bash ./start_linux.sh
</code></pre> 
<p> bash ./start_linux.sh 首次运行会下载大量数据， 时间较长。</p> 
<p><img alt="" height="234" src="https://images2.imgbox.com/9c/84/afDN6j88_o.png" width="736"></p> 
<p>成功后会 默认监听 7860 端口 开启web服务，如下所示：</p> 
<p><img alt="" height="122" src="https://images2.imgbox.com/0c/7c/2fL44Cfj_o.png" width="768"></p> 
<h3>              </h3> 
<p></p> 
<h2> 5 语言 交互 UI </h2> 
<h3>        5.1进入 web 交互页面</h3> 
<pre><code class="language-bash">conda activate llm

HF_TOKEN="hf_XXXXXXXXXXXXXXXXXXXXXX" # HuggingFace  的 Access Tokens
export HF_TOKEN

./start_linux.sh</code></pre> 
<p>./start_linux.sh 开启web服务，成功后会显示：</p> 
<p><img alt="" height="122" src="https://images2.imgbox.com/44/e2/pQgryhWx_o.png" width="768"></p> 
<p>使用 SSH Tunnel 建立隧道 将 服务器的 7860 端口 映射到本地， 然后使用浏览器打开 。</p> 
<h3></h3> 
<h3>         5.2 使用交互页面自动下载模型</h3> 
<p>        首先需要在 Model 模型页签中下载一个Llama2模型 。</p> 
<p>        输入模型名称路径：“FlagAlpha/Llama2-Chinese-7b-Chat”</p> 
<p>        然后点击下载按钮</p> 
<h3><img alt="" height="591" src="https://images2.imgbox.com/83/92/onbLAn8B_o.png" width="1200"><img alt="" height="188" src="https://images2.imgbox.com/c2/0d/O8vRyCkN_o.png" width="1200"></h3> 
<p>注意：如果下载某些模型 出现 Http 401 错误，则需要设置  HuggingFace  的 Access Tokens</p> 
<p>需要登录 <a href="https://huggingface.co/settings/tokens" rel="nofollow" title="Hugging Face – The AI community building the future.">Hugging Face – The AI community building the future.</a></p> 
<p>在设置页面的  Access Tokens 中创建  Tokens 并复制。然后在 ./start_linux.sh 启动前 设置“HF_TOKEN” 环境变量 。</p> 
<p><img alt="" height="660" src="https://images2.imgbox.com/37/74/NAp2pf8H_o.png" width="1200"></p> 
<p></p> 
<p>        另外，可以在 HuggingFace 上寻找其它开放的大预言模型， 如果使用 13B 或者 更大的模型推理，依据参数规模可能需要更高的GPU显存，甚至多张GPU来加载运行。 <a class="has-card" href="https://huggingface.co/" rel="nofollow" title="Hugging Face – The AI community building the future."><span class="link-card-box"><span class="link-title">Hugging Face – The AI community building the future.</span><span class="link-desc">We’re on a journey to advance and democratize artificial intelligence through open source and open science.</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/a6/d3/BUY62g41_o.png" alt="icon-default.png?t=N7T8">https://huggingface.co/</span></span></a>  </p> 
<h3>          5.3手动下载 Llama2 模型 （可选）</h3> 
<p>模型文件建议去官网下载，  国内Llama2 下载地址 仅供参考</p> 
<ul><li> <p>Llama2-7B官网版本：<a href="https://pan.xunlei.com/s/VN_kR2fwuJdG1F3CoF33rwpIA1?pwd=z9kf" rel="nofollow" title="迅雷云盘">迅雷云盘</a></p> </li><li> <p>Llama2-7B-Chat官网版本：<a href="https://pan.xunlei.com/s/VN_kQa1_HBvV-X9QVI6jV2kOA1?pwd=xmra" rel="nofollow" title="迅雷云盘">迅雷云盘</a></p> </li><li> <p>Llama2-13B官网版本：<a href="https://pan.xunlei.com/s/VN_izibaMDoptluWodzJw4cRA1?pwd=2qqb" rel="nofollow" title="迅雷云盘">迅雷云盘</a></p> </li><li> <p>Llama2-13B-Chat官网版本：<a href="https://pan.xunlei.com/s/VN_iyyponyapjIDLXJCNfqy7A1?pwd=t3xw" rel="nofollow" title="迅雷云盘">迅雷云盘</a></p> </li><li> <p>Llama2-7B Hugging Face版本：<a href="https://pan.xunlei.com/s/VN_t0dUikZqOwt-5DZWHuMvqA1?pwd=66ep" rel="nofollow" title="迅雷云盘">迅雷云盘</a></p> </li><li> <p>Llama2-7B-Chat Hugging Face版本：<a href="https://pan.xunlei.com/s/VN_oaV4BpKFgKLto4KgOhBcaA1?pwd=ufir" rel="nofollow" title="迅雷云盘">迅雷云盘</a></p> </li><li> <p>Llama2-13B Hugging Face版本：<a href="https://pan.xunlei.com/s/VN_yT_9G8xNOz0SDWQ7Mb_GZA1?pwd=yvgf" rel="nofollow" title="迅雷云盘">迅雷云盘</a></p> </li><li> <p>Llama2-13B-Chat Hugging Face版本：<a href="https://pan.xunlei.com/s/VN_yA-9G34NGL9B79b3OQZZGA1?pwd=xqrg" rel="nofollow" title="迅雷云盘">迅雷云盘</a></p> </li><li> <p>Llama2-70B-Chat Hugging Face版本：<a href="https://pan.xunlei.com/s/VNa_vCGzCy3h3N7oeFXs2W1hA1?pwd=uhxh#" rel="nofollow" title="迅雷云盘">迅雷云盘</a></p> </li></ul> 
<p>另外 Llama2 中文模型 供参考选择  ， 通过访问Llama2中文社区链接 仅供参考：</p> 
<p><a class="has-card" href="https://github.com/FlagAlpha/Llama2-Chinese" title="GitHub - FlagAlpha/Llama2-Chinese: Llama中文社区，最好的中文Llama大模型，完全开源可商用"><span class="link-card-box"><span class="link-title">GitHub - FlagAlpha/Llama2-Chinese: Llama中文社区，最好的中文Llama大模型，完全开源可商用</span><span class="link-desc">Llama中文社区，最好的中文Llama大模型，完全开源可商用. Contribute to FlagAlpha/Llama2-Chinese development by creating an account on GitHub.</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/9f/4c/UojANrJ6_o.png" alt="icon-default.png?t=N7T8">https://github.com/FlagAlpha/Llama2-Chinese</span></span></a><em> </em> 将下载好的模型文件目录放到 /data/ai/oobabooga_linux/text-generation-webui/models中</p> 
<p><img alt="" height="117" src="https://images2.imgbox.com/40/2b/yUI14uZm_o.png" width="963"></p> 
<p></p> 
<h3>        5.4加载模型</h3> 
<p>        在 Model 模型页签中 加载模型 。 </p> 
<p>        如下图： 刷新现有模型、 下拉菜单选中模型，点击 Load 加载 模型 ， 成功加载后会显示：  “Successfully loaded <code>FlagAlpha_Llama2-Chinese-7b-Chat</code>.”</p> 
<p><img alt="" height="521" src="https://images2.imgbox.com/9a/95/ts8oA1rO_o.png" width="1200"></p> 
<p><img alt="" height="380" src="https://images2.imgbox.com/96/b4/FS2nlxY0_o.png" width="1200"></p> 
<p></p> 
<h3>        5.3使用 Chat 页签 Web 交互 UI</h3> 
<p><img alt="" height="688" src="https://images2.imgbox.com/35/67/jQPxHzuq_o.png" width="849"></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2bed5ffb18415d4a67cf4be58e4d39f2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">动态规划：路径和子数组问题（C&#43;&#43;）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e762db1c016a571733bec58a4d90d2b3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【算法系列篇】分治-快排</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>