<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【AI学习】[2024北京智源大会]具身智能：具身智能关键技术研究：操纵、决策、导航 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/173a6aee0a397c4be2687e2fee677661/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【AI学习】[2024北京智源大会]具身智能：具身智能关键技术研究：操纵、决策、导航">
  <meta property="og:description" content="具身智能关键技术研究：操纵、决策、导航
董 豪 | 北京大学助理教授
依然是边看边做些记录
这张图的重点是在说，我们的大脑，也是不同的部分处理不同的功能。这里面有些功能，比如视觉、听觉理解等功能，LLM已经具备，而有些功能没有具备，这些就是具身智能的重点
这就是具身智能的三个关键研究点。
仿真是方便的数据来源。通过已有的3D数据，合成数据。
通过数据，训练抓取功能。还有目标姿态估计，这样才能放置物体，这也是可以通过仿真获得海量数据进行训练的。
有了抓取和姿态估计，就可以完成很多任务。
灵巧手也可以通过合成数据训练。这里先要回答，灵巧手和二指等有什么区别？区别在于人的手有一个预抓取动作。
具体的训练，通过强化学习。因为这不是一个静态的抓取，是连续的动作
抓取之后，然后是操纵。这里是通过强化学习训练。
探索仿真的边界。通过探索，仿真是有边界的。
然后这里是探索双手的操作。
准备开源的平台：OmniSim
实现仿真到仿真。比如一个开门，不可能在真实环境去开一千个门吧，需要利用仿真
那仿真的边界在哪呢？目前最好的是视觉。
对于一个门，没有把手，即使是人，也需要尝试。这里就是交互式的尝试，这些都需要机器人在真实世界，交互后，才能提升。
我们也希望机器人能在真实世界学习提升，而不是固化了。
这个就是在真实世界里学习。
柔性物体的操作，是比较难的！通过强化学习，一个方法解决多种任务
这是机械臂的操作
这是通过在大淘宝买的东西做的机械臂。虽然误差大，但是通过视觉的闭环，依然可以工作。
如果仿真中没有的物体这么办？通过借助大模型，因为大模型见多识广，可以举一反三。
将仿真的操作能力，注入大模型，最后部署的是大模型。
具体的方式，就是通过问大模型，让大模型告诉如何操作。
然后发现，即使比较差的仿真，加上大模型，也可以比较好的工作。大模型可以做到新类别的泛化。
那接下来，就需要把长长的动作，进行任务分解。
上面这个图，就是大模型把一个任务，分解为一系统API。
接下来，是通过视觉的问题任务，经过数据集、微调环节，解决机器人的视觉问答任务。
任务编排不是瓶颈，大模型都可以实现。难点还是机械臂的操作。
然后是收纳，这个需要机器人自动完成。方法是从网络上获取大量图片，比如获取什么是整洁的样子，通过网上的几千张样例图片，获得分布的知识。
最后是具身导航，这个现在提的不多，但是未来可能有大用处。
物体导航。比如在屋里找，相应的东西。
这个也可以通过多模态的视觉语言大模型完成。
这是另外一个导航任务。视觉语言导航，跟随人的指令
这是一个真实世界部署的大模型，而不是仿真。方法是通过多专家讨论的方式，并不需要训练什么模型。
这是提出的第三种导航范式，需求驱动导航，因为前两种对人并不友好。
这个是把之前的所有导航方式进行集成。前面的三种导航范式可以覆盖所有的各种任务。
最后总结一下具身智能的关键点。过去是通过仿真实现基本的操作和导航技能，包括自监督的学习；现在是通过大模型，解决仿真到真实世界的gap，利用大模型的知识；那接下来，还是要利用真实世界的数据。
备： 下面视频链接的第3个演讲，大约在视频的1小时25分钟开始
https://www.bilibili.com/video/BV1Zx4y147os/?spm_id_from=333.1007.tianma.2-3-6.click&amp;vd_source=986224b0c4e79ec28556778dc7d42405">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-07T23:05:49+08:00">
    <meta property="article:modified_time" content="2024-08-07T23:05:49+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【AI学习】[2024北京智源大会]具身智能：具身智能关键技术研究：操纵、决策、导航</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>具身智能关键技术研究：操纵、决策、导航<br> 董 豪 | 北京大学助理教授</p> 
<p>依然是边看边做些记录<br> <img src="https://images2.imgbox.com/1a/ed/L9qMpQo9_o.png" alt="在这里插入图片描述"><br> 这张图的重点是在说，我们的大脑，也是不同的部分处理不同的功能。这里面有些功能，比如视觉、听觉理解等功能，LLM已经具备，而有些功能没有具备，这些就是具身智能的重点</p> 
<p><img src="https://images2.imgbox.com/bc/72/4sBxB1P9_o.png" alt="在这里插入图片描述"><br> 这就是具身智能的三个关键研究点。</p> 
<p><img src="https://images2.imgbox.com/41/fb/8Za1F34o_o.png" alt="在这里插入图片描述"><br> 仿真是方便的数据来源。通过已有的3D数据，合成数据。<br> 通过数据，训练抓取功能。还有目标姿态估计，这样才能放置物体，这也是可以通过仿真获得海量数据进行训练的。<br> 有了抓取和姿态估计，就可以完成很多任务。</p> 
<p><img src="https://images2.imgbox.com/a7/a5/uyoDsb7A_o.png" alt="在这里插入图片描述"></p> 
<p>灵巧手也可以通过合成数据训练。这里先要回答，灵巧手和二指等有什么区别？区别在于人的手有一个预抓取动作。<br> 具体的训练，通过强化学习。因为这不是一个静态的抓取，是连续的动作</p> 
<p><img src="https://images2.imgbox.com/cc/09/Re2JrPk3_o.png" alt="在这里插入图片描述"><br> 抓取之后，然后是操纵。这里是通过强化学习训练。</p> 
<p><img src="https://images2.imgbox.com/ee/ff/G5fIZPA0_o.png" alt="在这里插入图片描述"><br> 探索仿真的边界。通过探索，仿真是有边界的。</p> 
<p><img src="https://images2.imgbox.com/07/87/jsTJAFu3_o.png" alt="在这里插入图片描述"><br> 然后这里是探索双手的操作。<br> <img src="https://images2.imgbox.com/e3/46/EECgB2VD_o.png" alt="在这里插入图片描述"><br> 准备开源的平台：OmniSim<br> 实现仿真到仿真。比如一个开门，不可能在真实环境去开一千个门吧，需要利用仿真</p> 
<p><img src="https://images2.imgbox.com/a5/e5/ieNNyXJo_o.png" alt="在这里插入图片描述"><br> 那仿真的边界在哪呢？目前最好的是视觉。<br> 对于一个门，没有把手，即使是人，也需要尝试。这里就是交互式的尝试，这些都需要机器人在真实世界，交互后，才能提升。<br> 我们也希望机器人能在真实世界学习提升，而不是固化了。<br> <img src="https://images2.imgbox.com/92/a7/bUo14b8j_o.png" alt="在这里插入图片描述"><br> 这个就是在真实世界里学习。</p> 
<p><img src="https://images2.imgbox.com/38/cf/P0zJv714_o.png" alt="在这里插入图片描述"><br> 柔性物体的操作，是比较难的！通过强化学习，一个方法解决多种任务</p> 
<p><img src="https://images2.imgbox.com/c2/b3/ZINAZIMW_o.png" alt="在这里插入图片描述"><br> 这是机械臂的操作</p> 
<p><img src="https://images2.imgbox.com/d7/87/arupUR0n_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/bc/13/7o7GP69F_o.png" alt="在这里插入图片描述"><br> 这是通过在大淘宝买的东西做的机械臂。虽然误差大，但是通过视觉的闭环，依然可以工作。<br> <img src="https://images2.imgbox.com/89/cc/1pMl5Ax0_o.png" alt="在这里插入图片描述"><br> 如果仿真中没有的物体这么办？通过借助大模型，因为大模型见多识广，可以举一反三。<br> 将仿真的操作能力，注入大模型，最后部署的是大模型。<br> 具体的方式，就是通过问大模型，让大模型告诉如何操作。</p> 
<p><img src="https://images2.imgbox.com/d0/79/DFg2AKyH_o.png" alt="在这里插入图片描述"><br> 然后发现，即使比较差的仿真，加上大模型，也可以比较好的工作。大模型可以做到新类别的泛化。</p> 
<p><img src="https://images2.imgbox.com/55/fa/DXII6Yev_o.png" alt="在这里插入图片描述"><br> 那接下来，就需要把长长的动作，进行任务分解。<br> 上面这个图，就是大模型把一个任务，分解为一系统API。</p> 
<p><img src="https://images2.imgbox.com/db/c7/gvY03Fjf_o.png" alt="在这里插入图片描述"><br> 接下来，是通过视觉的问题任务，经过数据集、微调环节，解决机器人的视觉问答任务。<br> <img src="https://images2.imgbox.com/51/f8/ojejyvMo_o.png" alt="在这里插入图片描述"><br> <strong>任务编排不是瓶颈，大模型都可以实现。难点还是机械臂的操作。</strong></p> 
<p><img src="https://images2.imgbox.com/7a/4e/cyH2MqSB_o.png" alt="在这里插入图片描述"><br> 然后是收纳，这个需要机器人自动完成。方法是从网络上获取大量图片，比如获取什么是整洁的样子，通过网上的几千张样例图片，获得分布的知识。<br> <img src="https://images2.imgbox.com/87/8a/z1ZKr3YX_o.png" alt="在这里插入图片描述"><br> 最后是具身导航，这个现在提的不多，但是未来可能有大用处。</p> 
<p><img src="https://images2.imgbox.com/f4/1a/akQuITM6_o.png" alt="在这里插入图片描述"><br> 物体导航。比如在屋里找，相应的东西。<br> <img src="https://images2.imgbox.com/5d/91/JKzaxNKm_o.png" alt="在这里插入图片描述"><br> 这个也可以通过多模态的视觉语言大模型完成。<br> <img src="https://images2.imgbox.com/cd/b9/dbIEihzs_o.png" alt="在这里插入图片描述"><br> 这是另外一个导航任务。视觉语言导航，跟随人的指令<br> <img src="https://images2.imgbox.com/24/06/uIZooxcP_o.png" alt="在这里插入图片描述"><br> 这是一个真实世界部署的大模型，而不是仿真。方法是通过多专家讨论的方式，并不需要训练什么模型。<br> <img src="https://images2.imgbox.com/41/b6/UgBqwXPi_o.png" alt="在这里插入图片描述"><br> 这是提出的第三种导航范式，需求驱动导航，因为前两种对人并不友好。</p> 
<p><img src="https://images2.imgbox.com/54/48/lWDQKKAA_o.png" alt="在这里插入图片描述"><br> 这个是把之前的所有导航方式进行集成。前面的三种导航范式可以覆盖所有的各种任务。<br> <img src="https://images2.imgbox.com/5f/16/1IfuXj44_o.png" alt="在这里插入图片描述"><br> 最后总结一下具身智能的关键点。过去是通过仿真实现基本的操作和导航技能，包括自监督的学习；现在是通过大模型，解决仿真到真实世界的gap，利用大模型的知识；那接下来，还是要利用真实世界的数据。</p> 
<h3><a id="_87"></a>备：</h3> 
<p>下面视频链接的第3个演讲，大约在视频的1小时25分钟开始<br> https://www.bilibili.com/video/BV1Zx4y147os/?spm_id_from=333.1007.tianma.2-3-6.click&amp;vd_source=986224b0c4e79ec28556778dc7d42405</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/89f56bd5831a96f0a270e1758c1c7a79/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">论文阅读报告: 在时间双向图上查询基于时间的的密集子图 | ICDE 2024</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c0d4129ba1c75b490b0c0392f0b9f24d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">速盾：cdn缓存的文件是无法被篡改的吧？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>