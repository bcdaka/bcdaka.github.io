<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Video-LLaMA An Instruction-tuned Audio-Visual Language Model for Video Understanding 用于视频理解的指令调谐视听语言 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/705493a87bd01d180bd552d55a794cf9/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Video-LLaMA An Instruction-tuned Audio-Visual Language Model for Video Understanding 用于视频理解的指令调谐视听语言">
  <meta property="og:description" content="1.摘要 我们提出了一个多模态框架Video-LLaMA1，它使大型语言模型(LLM)能够理解视频中的视觉和听觉内容。视频-来自冻结的预训练视频和音频编码器和冻结的LLM的美洲驼引导跨模式训练。不像以前的工作，补充线性最小二乘法只处理视觉或听觉信号(朱等，2023；刘等，2023；Huang et al .，2023a)，Video-LLaMA通过解决两个挑战来实现视频理解:(1)捕捉视觉场景的时间变化，(2)整合视听信号。为了应对第一个挑战，我们提出了一个视频Q-former来将预训练的图像编码器组装到我们的视频编码器中，并引入视频到文本生成任务来学习视频语言的对应性。对于第二个挑战，我们利用ImageBind (Girdhar等人，2023年)，一种对齐多种模态的通用嵌入模型，作为预训练的音频编码器，并在ImageBind之上引入音频Q-former，以学习LLM模块的合理听觉查询嵌入。为了将视频和音频编码器的输出与LLM的嵌入空间对齐，我们首先在大量视频/图像-字幕对上训练视频LLaMA，然后用中等数量但更高质量的视觉指令数据集调整我们的模型。我们发现视频——美洲驼表现出感知和理解视频内容的能力，并根据视频中呈现的视觉和听觉信息产生有意义的反应。
Video-LLaMA1: 大语言模型理解视频内容（视觉&#43;听觉内容）
目标：解决视频理解的两个挑战。
（1）捕捉视觉场景时间变化：
引入视频Q-former：学习视觉语言的对应性。
（2）整合试听信号：
引入音频Q-former：学习合理的试听听觉查询嵌入。【？怎么学的呢？】
视频Q-former的学习：
与训练图像编码器组装到视频编码器。
视频-&gt;文本生成 任务，学习视频和文本对应性。
将视频和音频编码器的输出与LLM的嵌入空间对齐:
视频|图像-字幕训练LLaMA。
中等数量，高质量视觉指令集调整模型。 2.研究意义及价值 对于许多应用场景来说，只有文本的人机交互是不够的，因为真实世界的信息通常是多模态的。为了进一步挖掘LLMs的潜力，许多研究者试图赋予LLMs理解多模态内容的能力。尽管它们有效，但是这些方法致力于将来自一个附加模态的输入与文本(即，图像或音频)对齐，这对于视频理解来说是不令人满意的。具体来说，使LLM能够理解视频需要对不同的模态进行综合处理，包括视觉输入、听觉输入和文本输出，这比只理解图像和只理解音频的任务更具挑战性。在这项工作中，为了填补视听LLM的空白，我们研究了构建多模态LLM的可能性，该多模态LLM支持视频输入，并允许用户围绕用户上传的视频与计算机聊天，该视频通常由多个视频帧和音频组成。 本文认为：
现实世界的信息是多模态的，只有文本的交互不够，所以我们要探索多模态LLMs的潜能。现有的方法总是视觉模态&#43;文本模态捆绑模式，所以当前模型对视频理解能力差，尤其是对音频的理解能力。该项工作探索了多模态LLMs，填补了音频LLM空白，实现计算机能够理解用户发送的视频的聊天形式。 实现思路： 保证跨通道与训练的效率：采用BLIP-2的思想
明确捕捉视觉场景变化：用与训练视觉编码器分解计算帧表示
帧嵌入层注入时间
视频Q-Former生成可是查询令牌
视频中音频的处理：与训练的音频编码器及音频Q-former来学习合理的听觉嵌入。
------------------------------------------------------------------------------------------------------------------------
文本输出与视频对齐：
多分支跨模态预训练学习——&gt;视觉语言对应&#43;音频语言对应。
视觉语言对齐：
首先：大规模视频字幕数据集使用视频CLIP到文本的生成任务对视觉相关组间进行预训练。
预训练阶段引入图像字幕数据集——&gt;加强对静态视觉概念的理解。
然后：使用视频对话的数据集微调这些组件，为指令优化做准备。
音频编码器&#43;语言编码器的对齐：
音频——&gt;使用音频文本数据集在音频文本生成任务上预训练相关组件。
音频文本的衔接：使用Imagebind将不同模态与公共嵌入空间对齐
音频文本数据集有限——&gt;视觉文本数据训练音频相关组件【这合理吗？】
总结：
这些组件学习将Imagebind提供的公共嵌入空间与LLMs的嵌入空间对齐。尽管没有经过音频-文本数据的明确训练，Video-LLaMA在推理过程中表现出显著的零镜头音频理解能力
-----------------------------------------------------------------------------------------------------------------------
Video-LLaMA与现有模型相比表现出色，对视听模态信息理解全面：
创新型：
1.一个多模态框架，弥补了音频LLM空白，使LLM同时处理视频中的视觉&#43;听觉内容。
2.多分支跨通道预训练框架来实现视觉语言对齐和音频语言对齐
3.开源了权重和代码
3.方法设计 Video-LLaMA旨在使冷冻LLM能够理解视频中的视觉和听觉内容
两个主要的分支：
视觉语言分支和音频语言分支，分别将视频帧和音频信号转换为与LLMs的文本输入兼容的查询表——多分支跨模态预训练和视听教学调谐
架构：
视觉-语言模型
音频-语言模型
多分支跨模态训练：A&#43;B两个分支
第一阶段，大规模视觉字幕数据集用于训练
第二阶段，高质量的指令跟随数据集用于微调。图像被视为单帧视频
视觉语言训练 音频语言 视频语言模型： 如图1的左部所示，它包括
一个冻结的预训练图像编码器，用于从视频帧中提取特征；
一个位置嵌入层，用于将时间信息注入视频帧；
一个视频Q-former，用于聚合帧级表示；
一个线性层，用于将输出视频表示投影到与LLM的文本嵌入相同的维度。
实现：利用BLIP-2(李等，2023b)的预训练视觉组件作为冻结视觉编码器，它包括来自EVA-CLIP(方等，2022)的ViT G/14和预训练Q-former">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-12-15T18:24:54+08:00">
    <meta property="article:modified_time" content="2023-12-15T18:24:54+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Video-LLaMA An Instruction-tuned Audio-Visual Language Model for Video Understanding 用于视频理解的指令调谐视听语言</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>1.摘要</h2> 
<blockquote> 
 <p>我们提出了一个多模态框架Video-LLaMA1，它使<span style="color:#fe2c24;"><strong>大型语言模型(LLM)能够理解视频中的视觉和听觉内容</strong></span>。视频-来自<strong>冻结的预训练视频和音频编码器和冻结的LLM的美洲驼引导跨模式训练</strong>。不像以前的工作，<strong>补充线性最小二乘法只处理视觉或听觉信号</strong>(朱等，2023；刘等，2023；Huang et al .，2023a)，<span style="color:#fe2c24;"><strong>Video-LLaMA通过解决两个挑战来实现视频理解:(1)捕捉视觉场景的时间变化，(2)整合视听信号</strong></span>。为了应对第一个挑战，我们提出了<strong>一个<span style="color:#fe2c24;">视频Q-former</span>来将预训练的图像编码器组装到我们的视频编码器中</strong>，并<strong>引入视频到文本生成任务来学习视频语言的对应</strong>性。对于第二个挑战，我们利用ImageBind (Girdhar等人，2023年)，一种<strong>对齐多种模态的通用嵌入模型</strong>，作为<strong>预训练的音频编码器</strong>，并在ImageBind之上引入<strong><span style="color:#fe2c24;">音频Q-former</span>，以学习LLM模块的合理听觉查询嵌入</strong>。为了<strong>将视频和音频编码器的输出与LLM的嵌入空间对齐</strong>，我们首先<strong>在大量视频/图像-字幕对上训练视频LLaMA，然后用中等数量但更高质量的视觉指令数据集调整我们的模型</strong>。我们发现视频——<strong><span style="color:#fe2c24;">美洲驼表现出感知和理解视频内容的能力，并根据视频中呈现的视觉和听觉信息产生有意义的反应</span></strong>。</p> 
</blockquote> 
<p><strong>Video-LLaMA1</strong>: <span style="color:#fe2c24;">大语言模型理解视频内容</span>（视觉+听觉内容）</p> 
<p><strong>目标</strong>：解决视频理解的两个挑战。</p> 
<p>        （1）<span style="color:#fe2c24;">捕捉视觉场景时间变化</span>：</p> 
<p>        引入视频Q-former：学习视觉语言的对应性。</p> 
<p>        （2）<span style="color:#fe2c24;">整合试听信号</span>：</p> 
<p>        引入音频Q-former：学习合理的试听听觉查询嵌入。<span style="color:#dad5e9;"><strong><span style="background-color:#fe2c24;">【？怎么学的呢？】</span></strong></span></p> 
<p><strong>视频Q-former的学习</strong>：</p> 
<p>        与训练图像编码器组装到视频编码器。</p> 
<p>        视频-&gt;文本生成 任务，学习视频和文本对应性。</p> 
<p><strong> 将视频和音频编码器的输出与LLM的嵌入空间对齐:</strong></p> 
<p>        视频|图像-字幕训练LLaMA。</p> 
<p>        <span style="color:#fe2c24;">中等数量，高质量视觉指令集调整模型</span>。     </p> 
<h2 style="background-color:transparent;">2.研究意义及价值</h2> 
<blockquote> 
 <ul><li> 对于许多应用场景来说，<strong><span style="color:#fe2c24;">只有文本的人机交互是不够的</span>，因为真实世界的信息通常是多模态的</strong>。为了进一步挖掘LLMs的潜力，许多研究者试图赋予LLMs理解多模态内容的能力。</li><li><strong>尽管它们有效，但是这些方法致力于将来<span style="color:#fe2c24;">自一个</span></strong><span style="color:#fe2c24;"><strong>附加模态的输入与文本</strong></span><strong>(即，图像或音频)对齐，这对于视频理解来说是不令人满意的。具体来说，</strong><strong>使LLM能够理解视频需要对不同的模态进行综合处理，包括视觉输入、听觉输入和文本输出，这比只理解图像和只理解音频的任务更具挑战性</strong><strong>。</strong></li><li>在这项工作中，<span style="color:#fe2c24;"><strong>为了填补视听LLM的空白</strong></span>，我们研究了构建多模态LLM的可能性，该多模态LLM支持视频输入，并允许用户围绕用户上传的视频与计算机聊天，该视频通常由多个视频帧和音频组成。</li></ul> 
</blockquote> 
<p><strong>本文认为</strong>：</p> 
<ol><li>现实世界的信息是多模态的，<span style="color:#fe2c24;"><strong>只有文本的交互不够</strong></span>，所以我们要探索多模态LLMs的潜能。</li><li>现有的方法<strong><span style="color:#fe2c24;">总是视觉模态+文本模态捆绑模式</span></strong>，所以当前模型对视频理解能力差，尤其是对音频的理解能力。</li><li>该项工作探索了多模态LLMs，<span style="color:#fe2c24;"><strong>填补了音频LLM空白</strong></span>，实现计算机能够理解用户发送的视频的聊天形式。</li></ol> 
<blockquote> 
 <p><strong>实现思路：        </strong></p> 
 <p>        保证跨通道与训练的效率：采用BLIP-2的思想</p> 
 <p>        明确捕捉视觉场景变化：用与训练视觉编码器分解计算帧表示</p> 
 <p>                帧嵌入层注入时间</p> 
 <p>                视频Q-Former生成可是查询令牌</p> 
 <p>        视频中音频的处理：与训练的音频编码器及音频Q-former来学习合理的听觉嵌入。</p> 
 <p>------------------------------------------------------------------------------------------------------------------------</p> 
 <p><strong>文本输出与视频对齐：</strong></p> 
 <p><strong>        </strong>多分支跨模态预训练学习——&gt;视觉语言对应+音频语言对应。</p> 
 <p><strong>视觉语言对齐</strong>：</p> 
 <p>        <strong>首先</strong>：大规模视频字幕数据集使用视频CLIP到文本的生成任务对视觉相关组间进行预训练。</p> 
 <p>                预训练阶段引入图像字幕数据集——&gt;加强对静态视觉概念的理解。</p> 
 <p>        <strong>然后</strong>：使用视频对话的数据集微调这些组件，为指令优化做准备。</p> 
 <p><strong>音频编码器+语言编码器的对齐</strong>：</p> 
 <p>                音频——&gt;使用音频文本数据集在音频文本生成任务上预训练相关组件。</p> 
 <p>                音频文本的衔接：<span style="color:#fe2c24;"><strong>使用Imagebind将不同模态与公共嵌入空间对齐</strong></span></p> 
 <p>                音频文本数据集有限——&gt;视觉文本数据训练音频相关组件【<strong><span style="color:#dad5e9;"><span style="background-color:#fe2c24;">这合理吗？</span></span></strong>】</p> 
 <p>总结：</p> 
 <p>        这些组件学习将Imagebind提供的公共嵌入空间与LLMs的嵌入空间对齐。<strong>尽管没有经过音频-文本数据的明确训练，Video-LLaMA在推理过程中表现出显著的零镜头音频理解能力</strong></p> 
 <p><strong>-----------------------------------------------------------------------------------------------------------------------</strong></p> 
 <p></p> 
 <p>Video-LLaMA与现有模型相比表现出色，对视听模态信息理解全面：</p> 
 <p><strong>创新型</strong>：</p> 
 <p>        1.一个多模态框架，弥补了音频LLM空白，使LLM同时处理视频中的视觉+听觉内容。</p> 
 <p>        2.<strong>多分支跨通道预训练框架</strong>来实现视觉语言对齐和音频语言对齐</p> 
 <p>        3.开源了权重和代码</p> 
</blockquote> 
<p><img alt="" height="823" src="https://images2.imgbox.com/da/a0/kldpWMDk_o.png" width="752"></p> 
<h2 style="background-color:transparent;">3.方法设计</h2> 
<blockquote> 
 <p>Video-LLaMA<strong>旨在使冷冻LLM能够理解视频中的视觉和听觉内容</strong></p> 
 <p>两个主要的分支：</p> 
 <p><strong>        视觉语言分支和音频语言分支</strong>，分别将视频帧和音频信号转换为与LLMs的文本输入兼容的查询表——多分支跨模态预训练和视听教学调谐</p> 
 <p><strong>架构</strong>：</p> 
 <p>        视觉-语言模型</p> 
 <p>        音频-语言模型</p> 
 <p>        多分支跨模态训练：A+B两个分支</p> 
 <p>                第一阶段，大规模<span style="color:#fe2c24;"><strong>视觉字幕</strong></span>数据集用于训练</p> 
 <p>                第二阶段，高质量的<strong>指令跟随数据集用于<span style="color:#fe2c24;">微调</span></strong>。图像被视为单帧视频</p> 
 <ul><li>                视觉语言训练</li><li>                音频语言</li></ul> 
</blockquote> 
<h3><strong>视频语言模型</strong>：</h3> 
<blockquote> 
 <p>如图1的左部所示，它包括</p> 
 <p>        一个<strong>冻结的预训练图像编码器</strong>，用于<span style="color:#fe2c24;"><strong>从视频帧中提取特征</strong></span>；</p> 
 <p>        一个<strong>位置嵌入层</strong>，用于<strong><span style="color:#fe2c24;">将时间信息注入视频帧</span></strong>；</p> 
 <p>        一个<strong>视频Q-former</strong>，用于<strong><span style="color:#fe2c24;">聚合帧级表示</span></strong>；</p> 
 <p>        一个<strong>线性层</strong>，用于<span style="color:#fe2c24;"><strong>将输出视频表示投影到与LLM的文本嵌入相同的维度</strong></span>。</p> 
 <p>        <span style="color:#fe2c24;"><strong>实现</strong></span>：利用BLIP-2(李等，2023b)的预训练视觉组件作为冻结视觉编码器，它包括来自EVA-CLIP(方等，2022)的ViT G/14和预训练Q-former</p> 
 <p><img alt="" height="1200" src="https://images2.imgbox.com/30/93/zr8te899_o.jpg" width="1200"></p> 
</blockquote> 
<h3 style="background-color:transparent;"><strong>音频语言模块</strong>：</h3> 
<blockquote> 
 <p>如图1的右部所示，它包括</p> 
 <p>        一个<strong>预训练的音频编码器</strong>，用于在<strong><span style="color:#fe2c24;">给定一小段原始音频的情况下计算特征</span></strong>；</p> 
 <p>        一个<strong>位置嵌入层</strong>，用于<span style="color:#fe2c24;"><strong>将时间信息注入音频段</strong></span>；</p> 
 <p>        一个<strong>音频Q-former</strong>，用于<span style="color:#fe2c24;"><strong>融合不同音频段的特征</strong></span>；</p> 
 <p>        一个<strong>线性层</strong>，用于将<span style="color:#fe2c24;"><strong>音频表示映射到LLMs的嵌入空间</strong></span>。</p> 
 <p>        <img alt="" height="1200" src="https://images2.imgbox.com/93/42/2BAgOUL7_o.jpg" width="1200"></p> 
</blockquote> 
<h3 style="background-color:transparent;">多分支跨模态训练-视频文本</h3> 
<blockquote> 
 <p><strong>阶段一</strong>：</p> 
 <p><strong>        目标</strong>：使用大数据，使视频特征包含尽可能多的视觉知识。</p> 
 <p><strong>        问题</strong>：视频表示使用冻结的LLMs生成的文本，不足以描述完整的视频。</p> 
 <p><strong>        原因</strong>：视频语义与视频文本语义并不完全一致</p> 
 <p><strong>        数据集</strong>：</p> 
 <p>                Webvid-2M：短视频数据集</p> 
 <p>                CC59K：CC3M过滤的图像字幕数据集</p> 
 <p><strong>        结果</strong>：能够生成视频信息内容，但遵循指令能力下降</p> 
 <p>-------------------------------------------------------------------------------------------------------------------------</p> 
 <p><strong>阶段二</strong>：</p> 
 <p>       <strong> 目标</strong>：视觉文本对齐 指令跟随能力——&gt;使用高质量数据集微调</p> 
 <p>        <strong>数据集</strong>：</p> 
 <p>                MiniGPT4：图像细节描述数据集</p> 
 <p>                LLaVA：图像指令数据集</p> 
 <p>                Video-chat: 视频指令数据集</p> 
 <p>        <strong>结果</strong>：理解图像和视频方面表现出非凡的能力</p> 
</blockquote> 
<h3 style="background-color:transparent;">多分支跨模态训练-音频文本</h3> 
<blockquote> 
 <p><strong>目的</strong>：将冻结音频编码器的输出嵌入与LLM的嵌入空间对准</p> 
 <p><strong>问题</strong>：音频文本数据的稀缺</p> 
 <p><strong>解决：变通思路</strong></p> 
 <p>        ImageBind音频编码器，<strong>具有将不同模态的beddings排列到一个公共空间的能力</strong>，在跨模态检索和生成任务中表现优秀。</p> 
 <p>        鉴于音频文本数据的稀缺和视觉文本数据的丰富，<strong>使用视觉文本数据训练音频语言分支，遵循与视觉分支相同的数据和过程</strong>。</p> 
 <p><strong>结果</strong>：</p> 
 <p>        由于ImageBind提供的共享嵌入空间，Video-LLaMA在推理过程中表现出理解音频的能力，即使音频接口从未在音频数据上训练过。</p> 
</blockquote> 
<h2 style="background-color:transparent;">4.相关工作</h2> 
<blockquote> 
 <p><strong>大型语言模型：</strong></p> 
 <p>        本文的工作基于这些LLM，并提供即插即用插件，使其能够理解视频中的视觉和听觉内容。</p> 
 <p><strong>多模态大型语言模型</strong>：</p> 
 <p>        现有的方法可以分为两大类。</p> 
 <p>        <strong>第一类包括使用LLM作为控制器和利用现有的多模态模型作为工具</strong>。</p> 
 <p>        当接收到用户的文本指令时，LLM识别出用户的注意力，并决定调用哪些工具。然后，它通过整合从这些现成的多模态模型中获得的结果，生成全面的响应。</p> 
 <p>        <strong>第二类集中于训练基本的大规模多模态模型</strong>。</p> 
 <p>        关键思想是将用于其他模态的预训练基础模型与文本LLM对齐。</p> 
 <p>        本文的工作属于第二类，训练基本模型来理解视频中的视觉和听觉内容。</p> 
</blockquote> 
<h2 style="background-color:transparent;">5.实验</h2> 
<blockquote> 
 <p>展示Video-LLaMA在基于视频/音频/图像的转换中的多模态指令跟随能力</p> 
 <p><strong>(1)视听整合感知能力-图2(a)和图3</strong></p> 
 <p>        同时理解听觉和视觉信息的独特能力。两种情况下的视频都包含音频。</p> 
 <p>        在每次对话中，分别提出两个与视觉和听觉内容相关的问题。如果模型只能接收一个模态，它将无法回答这两个问题。</p> 
 <p>        结果：在两种情况下都能准确地回答视觉和听觉问题。</p> 
 <p><strong>(2)捕捉视频中时间动态的能力-图2(b)和图4</strong></p> 
 <p>        识别动作的能力。它成功地描述了女孩的动作和船的移动方向。</p> 
 <p><strong>(3)感知和理解静态图像的能力。</strong></p> 
 <p>        感知和理解图片的能力-图2c+图5</p> 
 <p>                理解“不寻常”的概念和具体描述不寻常场景的能力-图2c</p> 
 <p>                不仅准确地描述了主要内容，而且还将它与狗和人之间的友好交互联系起来-图5</p> 
 <p><strong>(4)常识概念识别能力-图2d+图6</strong></p> 
 <p>        能成功识别著名的地标和人物，并能进行常识性的问答</p> 
 <p></p> 
 <p><img alt="" height="790" src="https://images2.imgbox.com/a3/4b/3gwsk0WD_o.png" width="966"></p> 
</blockquote> 
<p><img alt="" height="607" src="https://images2.imgbox.com/e9/96/ZyPuJiUw_o.png" width="1006">     <img alt="" height="636" src="https://images2.imgbox.com/66/82/OH5LVprn_o.png" width="1022"></p> 
<p><img alt="" height="805" src="https://images2.imgbox.com/79/b7/yWVg6AE9_o.png" width="875"></p> 
<p><img alt="" height="811" src="https://images2.imgbox.com/d7/db/kRLMLCZ5_o.png" width="1022"></p> 
<h2>6.总结与限制讨论</h2> 
<blockquote> 
 <p><strong>Video-LLaMA</strong>：一个前沿的多模态框架，它为大型语言模型提供了音频和视频支持。</p> 
 <p>视听人工智能助理的一个有前途的原型的潜力</p> 
 <p><strong>它仍然是早期的原型，并且具有一些局限性，包括:</strong></p> 
 <p>(1)有限的感知能力:</p> 
 <p>        Video-LLaMA的性能受到当前训练数据集的质量和规模的阻碍。</p> 
 <p>(2)处理长视频的能力有限。</p> 
 <p>        长视频(如电影和电视节目)包含大量信息，对计算资源提出了更高的要求。</p> 
 <p>(3)幻觉。</p> 
 <p>        Video-LLaMA 继承了冷冻LLMs的幻觉问题。</p> 
</blockquote> 
<h2 style="background-color:transparent;">7.读后感</h2> 
<p><strong>1.其创新点是支持音频吗？还是处理视频的时序信息吗？</strong></p> 
<p><strong>        其创新点是既能支持音频又能支持视频信息，以使模型能够从各个模态理解视频内容。</strong></p> 
<p><strong>2.这里的多模态和之前的多模态有什么区别？</strong></p> 
<p><strong>        之前的多模态，把各个模态映射到一个公共空间，这里的多模态把各个模态映射到文本空间。最终以文本模态来表达对内容的理解。</strong></p> 
<p><strong>3.本文工作的核心内容是什么？</strong></p> 
<p><strong>        弥补之前视频理解时听觉模态缺失的问题。</strong></p> 
<p><strong>        提供了听觉数据缺失问题的变通方法：</strong></p> 
<p>         ImageBind音频编码器，<strong>具有将不同模态的beddings排列到一个公共空间的能力</strong>，在跨模态检索和生成任务中表现优秀。</p> 
<p>        鉴于音频文本数据的稀缺和视觉文本数据的丰富，<strong>使用视觉文本数据训练音频语言分支，遵循与视觉分支相同的数据和过程</strong>。</p> 
<p><strong>4.视频理解能力指的是什么？</strong></p> 
<p><strong>        根据视频的视觉内容+听觉内容+文本内容——&gt;视频内容描述文本</strong></p> 
<p><strong>5.模型实现是怎样的？</strong></p> 
<p><strong>模态模板： 输入-&gt;编码器-&gt;位置层嵌入-&gt;Q-former-&gt;线性层-&gt;LLM</strong></p> 
<table align="left" border="1" cellpadding="1" cellspacing="1" style="width:600px;"><tbody><tr><td> <p><strong>冻结的预训练图像编码器</strong>，用于<span style="color:#fe2c24;"><strong>从视频帧中提取特征</strong></span>；</p> <p><strong>位置嵌入层</strong>，用于<strong><span style="color:#fe2c24;">将时间信息注入视频帧</span></strong>；</p> <p><strong>视频Q-former</strong>，用于<strong><span style="color:#fe2c24;">聚合帧级表示</span></strong>；</p> <p><strong>线性层</strong>，用于<span style="color:#fe2c24;"><strong>将输出视频表示投影到与LLM的文本嵌入相同的维度</strong></span>。</p> </td><td> <p><strong>预训练的音频编码器</strong>，用于在<strong><span style="color:#fe2c24;">给定一小段原始音频的情况下计算特征</span></strong>；</p> <p><strong>位置嵌入层</strong>，用于<span style="color:#fe2c24;"><strong>将时间信息注入音频段</strong></span>；</p> <p><strong>音频Q-former</strong>，用于<span style="color:#fe2c24;"><strong>融合不同音频段的特征</strong></span>；</p> <p><strong>线性层</strong>，用于将<span style="color:#fe2c24;"><strong>音频表示映射到LLMs的嵌入空间</strong></span>。</p> </td><td><strong>冻结的LLM</strong></td></tr></tbody></table> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1a0f88786826d1d99de3eb61a277f4fb/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">无脑利用API实现文心一言AI对话功能？(附代码)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e471d964256b83659973c14ddbe00eba/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">人工智能与大数据：未来的合作与挑战</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>