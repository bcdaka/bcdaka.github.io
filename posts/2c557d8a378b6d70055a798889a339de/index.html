<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Ubuntu系统下部署大语言模型：Ollama和OpenWebUI实现各大模型的人工智能自由 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/2c557d8a378b6d70055a798889a339de/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Ubuntu系统下部署大语言模型：Ollama和OpenWebUI实现各大模型的人工智能自由">
  <meta property="og:description" content="之前在window下安装过 Ollama和OpenWebUI搭建本地的人工智能web项目（可以看我之前写的文章），无奈电脑硬件配置太低，用qwen32b就很卡，卡出PPT了，于是又找了一台机器安装linux系统，在linux系统下测试一下速度能否可以快一些。
系统硬件介绍 Ubuntu 22.04.4 LTS
CPU: i5-10400F
内存：32G
硬盘： 512G SSD
显卡： NVIDIA GeForce GTX 1060 6GB
内网IP: 192.168.1.21
下载 Ollama 访问下载： https://ollama.com/
安装Ollama 方法1、命令行下载安装(耗时长) 安装命令：
$ sudo apt install curl $ curl -fsSL https://ollama.com/install.sh | sh 缺点： 国内网络环境要等很久
方法2 , 手动下载安装 1、手动下载 https://ollama.com/install.sh 这个文件
$ sudo mkdir ollama cd ollama $ sudo wget https://ollama.com/install.sh 2、注释掉下载部分 curl xxxx 手动下载ollama-linux-{ARCH}
$ sudo vim install.sh 修改文件： status &#34;Downloading ollama...&#34; #curl --fail --show-error --location --progress-bar -o $TEMP_DIR/ollama &#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-18T18:15:25+08:00">
    <meta property="article:modified_time" content="2024-05-18T18:15:25+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Ubuntu系统下部署大语言模型：Ollama和OpenWebUI实现各大模型的人工智能自由</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>之前在window下安装过 Ollama和OpenWebUI搭建本地的人工智能web项目（可以看我之前写的文章），无奈电脑硬件配置太低，用qwen32b就很卡，卡出PPT了，于是又找了一台机器安装linux系统，在linux系统下测试一下速度能否可以快一些。</p> 
<h2><a id="_3"></a>系统硬件介绍</h2> 
<p>Ubuntu 22.04.4 LTS</p> 
<p>CPU: i5-10400F</p> 
<p>内存：32G</p> 
<p>硬盘： 512G SSD</p> 
<p>显卡： NVIDIA GeForce GTX 1060 6GB</p> 
<p>内网IP: 192.168.1.21</p> 
<p><img src="https://images2.imgbox.com/cf/1d/5hJ6kZWB_o.png" alt="ubuntu-Ollama-OpenWebUI"></p> 
<h2><a id="_Ollama_19"></a>下载 Ollama</h2> 
<p>访问下载： https://ollama.com/</p> 
<p><img src="https://images2.imgbox.com/66/df/59d0ffeC_o.png" alt="image-20240517160214023"></p> 
<h3><a id="Ollama_29"></a>安装Ollama</h3> 
<h3><a id="1_31"></a>方法1、命令行下载安装(耗时长)</h3> 
<p>安装命令：</p> 
<pre><code>$ sudo apt  install curl

$ curl -fsSL https://ollama.com/install.sh | sh
</code></pre> 
<p><img src="https://images2.imgbox.com/4f/6f/KXqB5ZaU_o.png" alt="image-20240517160657340"></p> 
<p>缺点： 国内网络环境要等很久</p> 
<h3><a id="2___49"></a>方法2 , 手动下载安装</h3> 
<p>1、手动下载 https://ollama.com/install.sh 这个文件</p> 
<pre><code>$ sudo mkdir ollama
cd ollama
$ sudo wget https://ollama.com/install.sh
</code></pre> 
<p>2、注释掉下载部分 curl xxxx 手动下载ollama-linux-{ARCH}</p> 
<pre><code>$ sudo vim install.sh

修改文件：
status "Downloading ollama..."
#curl --fail --show-error --location --progress-bar -o $TEMP_DIR/ollama "https://ollama.com/download/ollama-linux-${ARCH}${VER_PARAM}"
</code></pre> 
<p>我电脑intel/amd cpu 所以 {ARCH} = amd64<br> 浏览器下载 https://ollama.com/download/ollama-linux-amd64 当然科学上网速度更快哟。 放在 install.sh 同目录下</p> 
<p>3、注释掉 #$SUDO install -o0 -g0 -m755 $TEMP_DIR/ollama $BINDIR/ollama</p> 
<p>改为下面一行：</p> 
<pre><code>$ sudo vim install.sh

修改文件：
status "Installing ollama to $BINDIR..."
$SUDO install -o0 -g0 -m755 -d $BINDIR
#$SUDO install -o0 -g0 -m755 $TEMP_DIR/ollama $BINDIR/ollama
$SUDO install -o0 -g0 -m755 ./ollama-linux-amd64  $BINDIR/ollama
</code></pre> 
<p>4 运行 install.sh ,安装</p> 
<pre><code>sh  ./install.sh
</code></pre> 
<p><img src="https://images2.imgbox.com/dd/b8/DE2QM6vf_o.png" alt="image-20240517171750382"></p> 
<p><img src="https://images2.imgbox.com/2f/28/La252D3A_o.png" alt="image-20240517171944028"></p> 
<p>重启电脑</p> 
<p>配置模型下载路径</p> 
<pre><code>cd 
sudo vim .bashrc

sudo mkdir -p /home/star/ollama/ollama_cache
</code></pre> 
<p>然后添加一行 配置 OLLAMA_MODELS 环境变量自定义路径</p> 
<pre><code>### ollama model dir 改为自己的路径
# export OLLAMA_MODELS=/path/ollama_cache
export OLLAMA_MODELS=/home/star/ollama/ollama_cache

</code></pre> 
<p>如果开始没配置OLLAMA_MODELS ，默认路径是/usr/share/ollama/.ollama/models</p> 
<h3><a id="ollama_126"></a>启动ollama服务</h3> 
<pre><code># ollama --help
Large language model runner

Usage:
  ollama [flags]
  ollama [command]

Available Commands:
  serve       Start ollama
  create      Create a model from a Modelfile
  show        Show information for a model
  run         Run a model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models
  ps          List running models
  cp          Copy a model
  rm          Remove a model
  help        Help about any command

Flags:
  -h, --help      help for ollama
  -v, --version   Show version information

Use "ollama [command] --help" for more information about a command.
</code></pre> 
<p>提示</p> 
<pre><code>star@star-ai:~$ ollama serve
Couldn't find '/home/star/.ollama/id_ed25519'. Generating new private key.
Your new public key is: 

ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIPmYsSi6aIsyhC4EHEsCdBtSOqnfKmNVSf0Ofz9sVzyB

Error: listen tcp 127.0.0.1:11434: bind: address already in use

</code></pre> 
<p>说明已经运行</p> 
<h3><a id="ollama_171"></a>修改ollama端口</h3> 
<pre><code>vim /etc/systemd/system/ollama.service
在 [Service] 下添加  Environment="OLLAMA_HOST=0.0.0.0"

cat /etc/systemd/system/ollama.service
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin"
Environment="OLLAMA_HOST=0.0.0.0"

[Install]
WantedBy=default.target
</code></pre> 
<p>重新加载配置，重启ollama</p> 
<pre><code>systemctl daemon-reload

systemctl restart ollama

关闭服务
systemctl stop ollama
启动服务
systemctl start ollama
</code></pre> 
<h3><a id="qwen_212"></a>运行qwen大模型</h3> 
<pre><code>ollama run  qwen
</code></pre> 
<p><img src="https://images2.imgbox.com/15/b6/iqlnEVYY_o.png" alt="image-20240517173411382"></p> 
<h2><a id="docker_222"></a>安装docker</h2> 
<p>一键安装脚本</p> 
<pre><code>sudo curl -sSL https://get.docker.com/ | sh

安装完成之后
star@star-ai:~$ sudo docker --version
Docker version 26.1.3, build b72abbb

</code></pre> 
<h2><a id="Open_WebUI_237"></a>安装Open WebUI</h2> 
<p>Open WebUI是一个用于在本地运行大型语言模型（LLM）的开源Web界面。</p> 
<p>参考： https://docs.openwebui.com/getting-started/#quick-start-with-docker-</p> 
<h4><a id="dockeropenwebui_245"></a>docker安装open-webui</h4> 
<pre><code>$ sudo docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
</code></pre> 
<p>要运行支持 Nvidia GPU 的 Open WebUI，请使用以下命令：</p> 
<pre><code>$ sudo docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda
</code></pre> 
<p>改国内的地址</p> 
<pre><code>$ sudo docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always  registry.cn-shenzhen.aliyuncs.com/funet8/open-webui:cuda
</code></pre> 
<p>报错：</p> 
<pre><code>sudo docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always  registry.cn-shenzhen.aliyuncs.com/funet8/open-webui:cuda
254b47e7994b2f0087ce0058918621523b39cf9b0e89018777c0cf98943ba2d1
docker: Error response from daemon: could not select device driver "" with capabilities: [[gpu]].
</code></pre> 
<p>ubuntu识别不了我的显卡</p> 
<pre><code>$ sudo nvidia-smi
Fri May 17 18:37:15 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce GTX 1060 6GB    Off |   00000000:01:00.0 Off |                  N/A |
| 40%   33C    P8              6W /  120W |      65MiB /   6144MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      1030      G   /usr/lib/xorg/Xorg                             56MiB |
|    0   N/A  N/A      1109      G   /usr/bin/gnome-shell                            4MiB |
+-----------------------------------------------------------------------------------------+
</code></pre> 
<p>安装nvidia-container-toolkit：<br> 确保你已经安装了nvidia-container-toolkit，并配置Docker以使用该工具包：</p> 
<pre><code>sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
</code></pre> 
<p>检查Docker默认运行时配置：<br> 确保Docker的默认运行时设置为nvidia。编辑Docker的配置文件（通常位于/etc/docker/daemon.json），并添加或修改如下内容：</p> 
<pre><code>sudo vim /etc/docker/daemon.json
添加：

{
  "default-runtime": "nvidia",
  "runtimes": {
    "nvidia": {
      "path": "nvidia-container-runtime",
      "runtimeArgs": []
    }
  }
}
编辑完文件后，重启Docker服务：
sudo systemctl restart docker

</code></pre> 
<p>检查NVIDIA Container Runtime兼容性：<br> 确保你的NVIDIA Container Runtime版本与Docker版本兼容。可以通过以下命令查看版本：</p> 
<pre><code>sudo docker version

nvidia-container-runtime --version

</code></pre> 
<p>完成上述步骤后，再次尝试运行你的Docker命令。如果问题仍然存在，请提供更多的系统信息和日志，以便进一步诊断问题。</p> 
<pre><code>sudo docker start open-webui
</code></pre> 
<p><img src="https://images2.imgbox.com/19/87/qduBsgv9_o.png" alt="image-20240517184705558"></p> 
<h2><a id="openwebui_362"></a>登录open-webui</h2> 
<p>用IP+端口访问</p> 
<p><img src="https://images2.imgbox.com/68/3f/bKQT0Lpb_o.png" alt="image-20240517184836945"></p> 
<h3><a id="_368"></a>修改语言为中文</h3> 
<p>OpenWebUI默认是英文的，所以修改语言为简体中文。</p> 
<p><img src="https://images2.imgbox.com/f6/e8/8kn07SNc_o.png" alt="image-20240518130431610"></p> 
<h3><a id="OpenWebUIOllama_376"></a>OpenWebUI不能连接Ollama</h3> 
<p>报错：WebUI could not connect to ollama</p> 
<p><img src="https://images2.imgbox.com/05/21/txBxIBA5_o.png" alt="image-20240518130617215"></p> 
<p>修改地址：http://192.168.1.21:11434</p> 
<p><img src="https://images2.imgbox.com/53/9a/lDxYnbLT_o.png" alt="image-20240518163725720"></p> 
<p>再下载千问的模型 qwen</p> 
<p><img src="https://images2.imgbox.com/21/01/qBplzeN9_o.png" alt="image-20240518164005249"></p> 
<h2><a id="_400"></a>下载大模型</h2> 
<p>ollama官方的模型仓库参见这里：https://ollama.com/library</p> 
<p><img src="https://images2.imgbox.com/3e/f7/5scphBtk_o.png" alt="image-20240518165100596"></p> 
<p>根据自己的CPU和GPU选择合适的大模型，否则会很卡。</p> 
<p>比如测试用的1060使用qwen:72b就很卡，问一个问题要等很久，几乎是不能用的状态。</p> 
<pre><code>阿里巴巴的大模型：
ollama run  qwen
ollama run qwen:14b
ollama run qwen:32b
ollama run qwen:72b
ollama run qwen:110b   # 110b 表示该模型包含了 1100 亿（110 billion）个参数


脸书大模型：
ollama run llama2
ollama run llama3
ollama run llama3:8b

谷歌的大模型：
ollama run gemma

微软的大模型
ollama run phi3

</code></pre> 
<h2><a id="_436"></a>删除模型</h2> 
<pre><code>显示所有模型
# ollama list

删除模型
# ollama rm llama3:latest
</code></pre> 
<p><img src="https://images2.imgbox.com/8d/d1/MbAlOLse_o.png" alt="image-20240518162719946"></p> 
<h2><a id="ubuntuGPU_450"></a>ubuntu查看GPU负载</h2> 
<pre><code>nvidia-smi
</code></pre> 
<p>确实在ubuntu20.04系统下确实比window10系统使用Ollama更加流畅。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f70073e95e9fe255a10b47f5c1f6924f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">探索 api.maynor1024.live：一站式 AI 服务平台</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1f2ee6c9605a2e8d6b5aede5d89641c6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【机器学习聚类算法实战-4】机器学习聚类算法之k-均值聚类、分层聚类算法、凝聚聚类和谱聚类实例分析</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>