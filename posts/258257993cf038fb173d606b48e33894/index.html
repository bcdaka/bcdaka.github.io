<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>论文阅读笔记（通道注意力） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/258257993cf038fb173d606b48e33894/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="论文阅读笔记（通道注意力）">
  <meta property="og:description" content="论文阅读笔记（通道注意力） 摘要Abstract1. SENet1.1 研究背景1.2 创新点1.3 SE块的构建过程1.3.1 注意力和门机制1.3.2 SE块具体运行过程1.3.3 通道间依赖关系的提取1.3.4 自适应重新校正(Excitation) 1.4 SE结合先进架构的灵活应用1.5 实验1.6 模型的实现1.7 结论 2. SE-Resnet50 代码实现总结 摘要 本周阅读了 Squeeze-and-Excitation Networks 这篇文献，该文献提出了SENet架构，SENet（Squeeze-and-Excitation Networks）是一种深度学习架构，它通过引入注意力机制来增强卷积神经网络（CNN）的特征学习能力。其核心是SE模块，它通过显式地建模特征通道之间的相互依赖关系，使网络能够自适应地调整每个通道的响应。SE模块通过学习每个特征通道的重要性权重，然后根据这些权重对特征进行重标定，从而提升有用的特征并抑制无关的特征。本文将详细介绍SENet架构。
Abstract This week read the paper Squeeze-and-Excitation Networks, which proposes the SENet architecture, a deep learning architecture that enhances the feature learning of Convolutional Neural Networks (CNNs) by introducing an attention mechanism to enhance their feature learning capability. At its core is the SE module, which enables the network to adaptively adjust the response of each channel by explicitly modeling the interdependencies between feature channels.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-16T18:03:55+08:00">
    <meta property="article:modified_time" content="2024-06-16T18:03:55+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">论文阅读笔记（通道注意力）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>论文阅读笔记（通道注意力）</h4> 
 <ul><li><a href="#_1" rel="nofollow">摘要</a></li><li><a href="#Abstract_3" rel="nofollow">Abstract</a></li><li><a href="#1_SENet_8" rel="nofollow">1. SENet</a></li><li><ul><li><a href="#11__9" rel="nofollow">1.1 研究背景</a></li><li><a href="#12__16" rel="nofollow">1.2 创新点</a></li><li><a href="#13_SE_23" rel="nofollow">1.3 SE块的构建过程</a></li><li><ul><li><a href="#131__41" rel="nofollow">1.3.1 注意力和门机制</a></li><li><a href="#132_SE_45" rel="nofollow">1.3.2 SE块具体运行过程</a></li><li><a href="#133__54" rel="nofollow">1.3.3 通道间依赖关系的提取</a></li><li><a href="#134_Excitation_68" rel="nofollow">1.3.4 自适应重新校正(Excitation)</a></li></ul> 
   </li><li><a href="#14_SE_88" rel="nofollow">1.4 SE结合先进架构的灵活应用</a></li><li><a href="#15__100" rel="nofollow">1.5 实验</a></li><li><a href="#16__118" rel="nofollow">1.6 模型的实现</a></li><li><a href="#17__143" rel="nofollow">1.7 结论</a></li></ul> 
  </li><li><a href="#2_SEResnet50__146" rel="nofollow">2. SE-Resnet50 代码实现</a></li><li><a href="#_258" rel="nofollow">总结</a></li></ul> 
</div> 
<p></p> 
<h2><a id="_1"></a>摘要</h2> 
<p>本周阅读了 Squeeze-and-Excitation Networks 这篇文献，该文献提出了SENet架构，SENet（Squeeze-and-Excitation Networks）是一种深度学习架构，它通过引入注意力机制来增强卷积神经网络（CNN）的特征学习能力。其核心是SE模块，它通过显式地建模特征通道之间的相互依赖关系，使网络能够自适应地调整每个通道的响应。SE模块通过学习每个特征通道的重要性权重，然后根据这些权重对特征进行重标定，从而提升有用的特征并抑制无关的特征。本文将详细介绍SENet架构。</p> 
<h2><a id="Abstract_3"></a>Abstract</h2> 
<p>This week read the paper Squeeze-and-Excitation Networks, which proposes the SENet architecture, a deep learning architecture that enhances the feature learning of Convolutional Neural Networks (CNNs) by introducing an attention mechanism to enhance their feature learning capability. At its core is the SE module, which enables the network to adaptively adjust the response of each channel by explicitly modeling the interdependencies between feature channels.The SE module enhances useful features and suppresses irrelevant features by learning the importance weights of each feature channel and then recalibrating the features according to these weights. In this paper, we describe the SENet architecture in detail.</p> 
<p>论文出处：<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf" rel="nofollow">Squeeze-and-Excitation Networks</a></p> 
<h2><a id="1_SENet_8"></a>1. SENet</h2> 
<h3><a id="11__9"></a>1.1 研究背景</h3> 
<p>卷积神经网络（CNN）已被证明是处理各种视觉任务的有效模型。 对于每个卷积层，学习一组滤波器来表达沿输入通道的局部空间连接模式。卷积滤波器有望通过在局部感受野内将空间和通道信息融合在一起而成为信息组合。 通过堆叠一系列与非线性和下采样交错的卷积层，CNN 能够捕获具有全局感受野的分层模式作为强大的图像描述。</p> 
<p>在本文中，作者通过引入一个新的架构单元（我们将其称为“挤压和激励”（SE）块）来研究架构设计的另一个方面 - 通道关系。作者提出了一种允许网络执行特征重新校准的机制，通过该机制它可以学习使用全局信息来选择性地强调信息丰富的特征并抑制不太有用的特征。通过简单地堆叠一组 SE 构建块就可以生成 SE 网络。 SE 块还可以用作架构中任何深度的原始块的直接替代品。</p> 
<p>最后一届ImageNet 2017竞赛 Image Classification任务的冠军.作者采用SENet block和ResNeXt结合在ILSVRC 2017的分类项目中拿到第一，在ImageNet数据集上将top-5 error降低到2.251%，原先的最好成绩是2.991%。</p> 
<h3><a id="12__16"></a>1.2 创新点</h3> 
<p>SENet的核心思想是通过网络根据损失函数学习特征权重，使得有效的特征图（feature map）权重更大，而对当前任务效果较小或无效的特征图权重减小。以下便是这篇文献的创新点：</p> 
<ol><li>SENet引入了SE模块，该模块嵌入到现有的分类网络中，通过显式地建模特征通道之间的相互依赖关系来提升网络性能。</li><li>SE模块中的Squeeze操作通过全局平均池化（Global Average Pooling）将每个特征通道压缩成一个实数，这个实数具有全局感受野，使得网络能够捕捉全局上下文信息。</li><li>SENet在多个视觉任务和数据集上展示了其有效性，例如在ImageNet和COCO数据集上取得优异的性能，同时在ILSVRC 2017竞赛中获得分类任务第一名。</li></ol> 
<h3><a id="13_SE_23"></a>1.3 SE块的构建过程</h3> 
<p><img src="https://images2.imgbox.com/06/62/8HCXwSTG_o.png" alt="在这里插入图片描述" height="260"><br> 对于任何给定的变换(例如卷积或一组卷积)<br> <img src="https://images2.imgbox.com/b0/91/PuhqeXPX_o.png" alt="在这里插入图片描述" height="50"><br> 我们可以构造一个相应的SE块来执行特征重新校准（对变换完了的新特征进行重新校准）。如图所示。执行步骤如下：</p> 
<ol><li>特征U首先通过squeeze操作，该操作跨越空间维度W×H聚合特征映射来产生通道描述符（也就是聚合一张Fm）。这个描述符嵌入了通道特征响应的全局分布(对一张Fm的描述)，使来自网络全局感受野的信息能够被其较低层利用（指这个模块的较底层部分，就是一提取完就给最近的层用）。</li><li>这之后是一个excitation（激励）操作，其中通过基于通道依赖性的自门机制为每个通道学习特定采样的激活，控制每个通道的激励。</li><li>然后特征映射U被重新加权以生成SE块的输出，然后可以将其直接输入到随后的层中。</li><li></ol> 
<p>SE网络可以通过简单地堆叠SE构建块的集合来生成。SE块也可以用作架构中任意深度的原始块的直接替换。然而，虽然构建块的模板是通用的，正如我们6.3节中展示的那样，但它在不同深度的作用适应于网络的需求。</p> 
<p>SE块在网络不同部分的作用有所不同：</p> 
<ol><li>在前面的层中，它学习以类不可知的方式激发信息特征，增强共享的较低层表示的质量。</li><li>在后面的层中，SE块越来越专业化，并以高度类特定的方式响应不同的输入。</li></ol> 
<p>因此，SE块进行特征重新校准的好处可以通过整个网络进行累积。</p> 
<h4><a id="131__41"></a>1.3.1 注意力和门机制</h4> 
<p>从广义上讲，可以将注意力视为一种工具，将可用处理资源的分配偏向于输入信号的信息最丰富的组成部分。这种机制的发展和理解一直是神经科学社区的一个长期研究领域，并且近年来作为一个强大补充，已经引起了深度神经网络的极大兴趣。注意力已经被证明可以改善一系列任务的性能，从图像的定位和理解到基于序列的模型。它通常结合门函数（例如softmax或sigmoid）和序列技术来实现。（Highway Networks）高速网络[36]采用门机制来调节快捷连接，使得可以学习非常深的架构。王等人[42]受到语义分割成功的启发，引入了一个使用沙漏模块[27]的强大的trunk-and-mask注意力机制。这个高容量的单元被插入到中间阶段之间的深度残差网络中。</p> 
<p>相比之下，作者提出的SE块是一个轻量级的门机制，专门用于以计算有效的方式对通道关系进行建模，并设计用于增强整个网络中模块的表示能力。</p> 
<h4><a id="132_SE_45"></a>1.3.2 SE块具体运行过程</h4> 
<p>Squeeze-and-Excitation块是一个计算单元，可以为任何给定的变换构建（就是给某个变换结束了，对它产生的结果进行一个处理）。</p> 
<p>为了简化说明，在接下来的表示中，我们将Ftr看作一个标准的卷积算子。V=[v1,v2,…,vC]表示学习到的一组滤波器核，vc指的是第c个滤波器的参数。然后我们可以将Ftr的输出写作U=[u1,u2,…,uC]，其中<br> <img src="https://images2.imgbox.com/73/80/fEgtf18D_o.png" alt="在这里插入图片描述" height="80"><br> 这里∗表示卷积，vc=[v1c,v2c,…,vC′c]，X=[x1,x2,…,xC′]（为了简洁表示，忽略偏置项）。这里vsc是2D空间核，因此表示vc的一个单通道，作用于对应的通道X。Uc表示输出的第C个通道的Fm，Vc代表在输入的第C个通道上使用的卷积核，Vsc代表该卷积核对应输入的某个通道的2D空间核。</p> 
<p>从上式不难知道输出的某个Fm是通过所有通道的和来产生的，所以通道依赖性（这里指的通道依赖性指的是上一批通道，也就是输入信号的那批通道，后面我们处理的是后面一批通道）被隐式地嵌入到vc中，而vc是滤波器(卷积核)，它的作用是捕获图片像素点的空间相关性，所以我们说通道间的依赖性与滤波器捕获的空间相关性是纠缠在一起的。</p> 
<h4><a id="133__54"></a>1.3.3 通道间依赖关系的提取</h4> 
<p>输出特征中每个通道的信号：要找通道间信号的关系，先把他们输出。</p> 
<p><strong>问题：</strong></p> 
<p>每个通道在这里是指刚刚输出的每张Fm，所以，输出每个通道信息就是说输出每张Fm，而Fm上有很多像素点，这些像素点都是由上一步的局部感受野处理得到的，没有哪个像素点可以代表整个通道(Fm):</p> 
<p>每个学习到的滤波器都对局部感受野进行操作，因此变换输出U的每个单元(每张Fm)都无法利用该区域之外的上下文信息。在网络较低的层次上其感受野尺寸很小，这个问题变得更严重。</p> 
<p><strong>解决方法：</strong></p> 
<p>使用全局平均池化将整个通道压缩成一个通道描述符。第c个通道(第c个Fm)表示如下：<br> <img src="https://images2.imgbox.com/ce/0c/0LcscS8S_o.png" alt="在这里插入图片描述" height="80"><br> 上一步的转换输出U可以被解释为局部描述子的集合，这些描述子的统计信息对于整个图像来说是有表现力的。特征工程工作中普遍使用这些信息。作者选择最简单的全局平均池化，同时也可以采用更复杂的汇聚策略。</p> 
<h4><a id="134_Excitation_68"></a>1.3.4 自适应重新校正(Excitation)</h4> 
<p>通过一定方式提取完了通道信息，我们就要开始利用这些通道信息了，利用他们来全面捕获通道依赖性。</p> 
<p>能捕获通道间依赖关系的机制必须要满足下面两个条件：</p> 
<ol><li>它必须是灵活的（特别是它必须能够学习通道之间的非线性交互）</li><li>它必须学习一个非互斥的关系(non-mutually-exclusive relationship )，因为允许多个通道被强调, 相对于独热激活。（这里感觉第一条灵活的非线性交互互其实包含了第二条，第二条只是为了强调下非互斥关系）</li></ol> 
<p><strong>解决方案：</strong></p> 
<p>为了满足这些标准，我们选择采用一个简单的门机制，并使用sigmoid激活（这里得到的标量S就相当于一会要加在输出信息U的某个Fm上的权重了）：<br> <img src="https://images2.imgbox.com/35/53/RQy0VBNG_o.png" alt="在这里插入图片描述" height="50"><br> δ是指Relu激活函數，W1和W2是用于降维和升维的两个全连接层的权重，目的是为了(1)限制模型复杂度(2)辅助模型泛化。</p> 
<p>通过在非线性周围形成两个全连接（FC）层的瓶颈来参数化门机制，即降维层参数为W1，降维比例为r。一个ReLU，然后是一个参数为W2的升维层。</p> 
<p>块的最终输出通过重新调节变换输出U得到：</p> 
<p><img src="https://images2.imgbox.com/35/1d/XEBkrEM1_o.png" alt="在这里插入图片描述" height="50"><br> 其中X˜=[x˜1,x˜2,…,x˜C]和Fscale(uc,sc)指的是特征映射uc∈RW×H和标量sc之间的对应通道乘积。激活作为适应特定输入描述符z的通道权重。在这方面，SE块本质上引入了以输入为条件的动态特性，有助于提高特征辨别力。</p> 
<h3><a id="14_SE_88"></a>1.4 SE结合先进架构的灵活应用</h3> 
<p>SE块的灵活性意味着它可以直接应用于标准卷积之外的变换。为了说明这一点，我们通过将SE块集成到两个流行的网络架构系列Inception和ResNet中来开发SENets。前面之所以用一个看似加一个无关的Ftr其实是现在用将它们替换为Inception和Residual模块了。</p> 
<p><strong>1. SE-Inception模块：</strong></p> 
<p>通过对架构中的每个模块进行更改，我们构建了一个SE-Inception网络。如下图：<br> <img src="https://images2.imgbox.com/07/5f/V3ZDVXa4_o.png" alt="在这里插入图片描述" height="300"><br> <strong>2. SE-ResNet模块：</strong></p> 
<p>在这里，SE块变换Ftr被认为是残差模块的非恒等分支。压缩和激励都在恒等分支相加之前起作用。如下图：</p> 
<p><img src="https://images2.imgbox.com/46/3d/M92FIdM0_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="15__100"></a>1.5 实验</h3> 
<p>SENet通过堆叠一组SE块来构建。实际上，它是通过用原始块的SE对应部分（即SE残差块）替换每个原始块（即残差块）而产生的。我们在表1中描述了SE-ResNet-50和SE-ResNeXt-50的架构。fc后面的内括号表示SE模块中两个全连接层的输出维度。</p> 
<p><img src="https://images2.imgbox.com/78/dd/rLwR6dmC_o.png" alt="在这里插入图片描述" height="500"><br> 在实践中提出的SE块是可行的，它必须提供可接受的模型复杂度和计算开销。每个SE块利用压缩阶段的全局平均池化操作和激励阶段中的两个小的全连接层，接下来是几乎算不上计算量的通道缩放操作。</p> 
<p>为了说明模块的成本，作为例子我们比较了ResNet-50和SE-ResNet-50，其中SE-ResNet-50的精确度明显优于ResNet-50，接近更深的ResNet-101网络（如表2所示）。对于224×224像素的输入图像，ResNet-50单次前向传播需要∼ 3.86 GFLOP。总的来说，SE-ResNet-50需要∼ 3.87 GFLOP，相对于原始的ResNet-50只相对增加了0.26%。</p> 
<p>下图是ImageNet验证集上的单裁剪图像错误率（％）和复杂度比较。original列是指原始论文中报告的结果。为了进行公平比较，我们重新训练了基准模型，并在re-implementation列中报告分数。SENet列是指已添加SE块后对应的架构。括号内的数字表示与重新实现的基准数据相比的性能改善。†表示该模型已经在验证集的非黑名单子集上进行了评估（在[38]中有更详细的讨论），这可能稍微改善结果。在实践中，训练的批数据大小为256张图像，ResNet-50的一次前向传播和反向传播花费190 ms，而SE-ResNet-50则花费209 ms（两个时间都在具有8个NVIDIA Titan X GPU的服务器上执行）。我们认为这是一个合理的开销，因为在现有的GPU库中，全局池化和小型内积操作(最后一步乘权重)的优化程度较低。此外，由于其对嵌入式设备应用的重要性，我们还对每个模型的CPU推断时间进行了基准测试：对于224×224像素的输入图像，ResNet-50花费了164ms，相比之下，SE-ResNet-50花费了167ms。SE块所需的小的额外计算开销对于其对模型性能的贡献来说是合理的。</p> 
<p><img src="https://images2.imgbox.com/bf/81/vXROtRmJ_o.png" alt="在这里插入图片描述" height="300"></p> 
<p><strong>对SE引入的附加参数的讨论:</strong></p> 
<p>SE的所有附加参数其实都在其中的两个全连接层了，他们构成了网络总容量的一小部分。增加的参数的计算方法如下：<br> <img src="https://images2.imgbox.com/94/4b/PaiGbgBA_o.png" alt="在这里插入图片描述" height="80"><br> 单个SE块增加的参数为：c*c/r + (c/r)*c=(2c^2)/r . 其中r表示减少比率（我们在所有的实验中将r设置为16），S指的是阶段数量（每个阶段是指在共同的空间维度的特征映射上运行的块的集合），Cs表示阶段s的输出通道的维度，Ns表示重复的块编号。（一个网络S个阶段，每个阶段重复N个SE块）</p> 
<h3><a id="16__118"></a>1.6 模型的实现</h3> 
<p><strong>训练过程</strong></p> 
<ol><li>使用随机大小裁剪到224×224像素（299×299用于Inception-ResNet-v2[38]和SE-Inception-ResNet-v2）</li><li>随机的水平翻转</li><li>输入图像通过通道减去均值进行归一化</li><li>采用数据均衡策略进行小批量采样，以补偿类别的不均匀分布</li><li>网络在我们的分布式学习系统“ROCS”上进行训练</li><li>使用同步SGD进行优化，动量为0.9，小批量数据的大小为1024（在4个服务器的每个GPU上分成32张图像的子批次，每个服务器包含8个GPU）</li><li>初始学习率设为0.6</li><li>每30个迭代周期减少10倍</li><li>所有模型都从零开始训练100个迭代周期</li></ol> 
<p><strong>imagenet测试</strong><br> 1.验证集上的处理过程：</p> 
<p>在验证集上使用中心裁剪图像评估来报告top-1和top-5错误率，其中每张图像短边首先归一化为256，然后从每张图像中裁剪出224×224个像素，（对于Inception-ResNet-v2和SE-Inception-ResNet-v2，每幅图像的短边首先归一化到352，然后裁剪出299×299个像素）。</p> 
<p>2.SE块对不同深度基础网络的影响（以ResNet为例）：</p> 
<p>我们首先将SE-ResNet与一系列标准ResNet架构进行比较。每个ResNet及其相应的SE-ResNet都使用相同的优化方案进行训练。</p> 
<p><img src="https://images2.imgbox.com/6d/e3/Bm1Vy1KH_o.png" alt="在这里插入图片描述" height="300"><br> 虽然应该注意SE块本身增加了深度，但是它们的计算效率极高，即使在扩展的基础架构的深度达到收益递减的点上也能产生良好的回报。而且，我们看到通过对各种不同深度的训练，性能改进是一致的，这表明SE块引起的改进可以与增加基础架构更多深度结合使用。</p> 
<h3><a id="17__143"></a>1.7 结论</h3> 
<p>在本文中，作者提出了 SE 块，这是一种新颖的架构单元，旨在通过使其能够执行动态通道特征重新校准来提高网络的表示能力。 大量的实验证明了 SENet 的有效性，它在多个数据集上实现了最先进的性能。</p> 
<h2><a id="2_SEResnet50__146"></a>2. SE-Resnet50 代码实现</h2> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> math
<span class="token keyword">from</span> torchsummary <span class="token keyword">import</span> summary

<span class="token keyword">class</span> <span class="token class-name">SElayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>ratio<span class="token punctuation">,</span>input_aisle<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SElayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pool<span class="token operator">=</span>nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc<span class="token operator">=</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_aisle<span class="token punctuation">,</span>input_aisle<span class="token operator">//</span>ratio<span class="token punctuation">,</span>bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_aisle<span class="token operator">//</span>ratio<span class="token punctuation">,</span>input_aisle<span class="token punctuation">,</span>bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>
                             <span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">#读取批数据图片数量及通道数</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span>
        <span class="token comment">#经池化后输出a*b的矩阵</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment">#经全连接层输出【啊，吧，1，1】矩阵</span>
        <span class="token keyword">return</span> x <span class="token operator">*</span> y<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment">#输出权重乘以特征图</span>
<span class="token keyword">class</span> <span class="token class-name">SE_block</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outchannels<span class="token punctuation">,</span> ratio<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SE_block<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>backbone<span class="token operator">=</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>outchannels<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>outchannels <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                    nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>outchannels <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>outchannels <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span>outchannels <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                    nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>outchannels <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>outchannels <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span>outchannels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                    nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>outchannels<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                    <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>Selayer<span class="token operator">=</span>SElayer<span class="token punctuation">(</span>ratio<span class="token punctuation">,</span> outchannels<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        residual<span class="token operator">=</span>x
        x<span class="token operator">=</span>self<span class="token punctuation">.</span>backbone<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x<span class="token operator">=</span>self<span class="token punctuation">.</span>Selayer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x<span class="token operator">=</span>x<span class="token operator">+</span>residual
        <span class="token keyword">return</span> x
<span class="token keyword">class</span> <span class="token class-name">SE_Idenitity</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inchannel<span class="token punctuation">,</span>outchannels<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>ratio<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SE_Idenitity<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>backbone <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
                                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>inchannel<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>outchannels<span class="token operator">//</span><span class="token number">4</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>outchannels<span class="token operator">//</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>outchannels<span class="token operator">//</span><span class="token number">4</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span>outchannels<span class="token operator">//</span><span class="token number">4</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>outchannels<span class="token operator">//</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>outchannels<span class="token operator">//</span><span class="token number">4</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span>outchannels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>outchannels<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                    <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>residual<span class="token operator">=</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>inchannel<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>outchannels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>selayer <span class="token operator">=</span> SElayer<span class="token punctuation">(</span>ratio<span class="token punctuation">,</span> outchannels<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        residual<span class="token operator">=</span>self<span class="token punctuation">.</span>residual<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x<span class="token operator">=</span>self<span class="token punctuation">.</span>backbone<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x<span class="token operator">=</span>self<span class="token punctuation">.</span>selayer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x<span class="token operator">=</span>x<span class="token operator">+</span>residual
        <span class="token keyword">return</span> x
<span class="token keyword">class</span> <span class="token class-name">SE_Resnet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>layer_num<span class="token punctuation">,</span>outchannel_num<span class="token punctuation">,</span>num_class<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SE_Resnet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>initial<span class="token operator">=</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                   nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>residual<span class="token operator">=</span>self<span class="token punctuation">.</span>make_layer<span class="token punctuation">(</span>layer_num<span class="token punctuation">,</span>outchannel_num<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pool<span class="token operator">=</span>nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span>num_class<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">make_layer</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>number_layer<span class="token punctuation">,</span>number_outchannel<span class="token punctuation">)</span><span class="token punctuation">:</span>
        layer_list<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>

        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>number_layer<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> i<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">:</span>
                stride<span class="token operator">=</span><span class="token number">1</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                stride<span class="token operator">=</span><span class="token number">2</span>
            layer_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>SE_Idenitity<span class="token punctuation">(</span>number_outchannel<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>number_outchannel<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>stride<span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>number_layer<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                layer_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>SE_block<span class="token punctuation">(</span>number_outchannel<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layer_list<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x<span class="token operator">=</span>self<span class="token punctuation">.</span>initial<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x<span class="token operator">=</span>self<span class="token punctuation">.</span>residual<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x<span class="token operator">=</span>self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">SE_Resnet50</span><span class="token punctuation">(</span>num_class<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model<span class="token operator">=</span>SE_Resnet<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">512</span><span class="token punctuation">,</span><span class="token number">1024</span><span class="token punctuation">,</span><span class="token number">2048</span><span class="token punctuation">]</span><span class="token punctuation">,</span>num_class<span class="token punctuation">)</span>
    <span class="token keyword">return</span> model
<span class="token keyword">def</span> <span class="token function">SE_Resnet101</span><span class="token punctuation">(</span>num_class<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model<span class="token operator">=</span>SE_Resnet<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">23</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">512</span><span class="token punctuation">,</span><span class="token number">1024</span><span class="token punctuation">,</span><span class="token number">2048</span><span class="token punctuation">]</span><span class="token punctuation">,</span>num_class<span class="token punctuation">)</span>
    <span class="token keyword">return</span> model
<span class="token keyword">def</span> <span class="token function">SE_Resnet151</span><span class="token punctuation">(</span>num_class<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model <span class="token operator">=</span> SE_Resnet<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">36</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">]</span><span class="token punctuation">,</span> num_class<span class="token punctuation">)</span>
    <span class="token keyword">return</span> model
device<span class="token operator">=</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span>
Xception<span class="token operator">=</span>SE_Resnet50<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
summary<span class="token punctuation">(</span>Xception<span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">224</span><span class="token punctuation">,</span><span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h2><a id="_258"></a>总结</h2> 
<p>本周学习了SENet这篇经典的通道注意力网络架构，同时也对其代码实现有了一定的了解，下周我将学习空间注意力模块。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6f48ce811cecf7aae729661e2cd68970/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">函数(上)(C语言)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/29e20ae92a3c8f294f412104f1eff3a1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python WordCloud库：词云图制作</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>