<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>llama.cpp部署 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/4996263169449586b76aedf1527ea7da/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="llama.cpp部署">
  <meta property="og:description" content="llama.cpp 介绍 部署 介绍 大模型的研究分为训练和推理两个部分：
训练的过程，实际上就是在寻找模型参数，使得模型的损失函数最小化；推理结果最优化的过程； 训练完成之后，模型的参数就固定了，这时候就可以使用模型进行推理，对外提供服务。
llama.cpp 主要解决的是推理过程中的性能问题。主要有两点优化：
llama.cpp 使用的是 C 语言写的机器学习张量库 ggmlllama.cpp 提供了模型量化的工具 计算类 Python 库的优化手段之一就是使用 C 重新实现，这部分的性能提升非常明显。另外一个是量化，量化是通过牺牲模型参数的精度，来换取模型的推理速度。llama.cpp 提供了大模型量化的工具，可以将模型参数从 32 位浮点数转换为 16 位浮点数，甚至是 8、4 位整数。
除此之外，llama.cpp 还提供了服务化组件，可以直接对外提供模型的 API 。
使用量化模型 下载编译 llama.cpp
git clone https://github.com/ggerganov/llama.cpp cd llama.cpp make 在目录下会生成一系列可执行文件
main：使用模型进行推理quantize：量化模型server：提供模型 API 服务… 准备 llama.cpp 支持的模型
llama.cpp 支持转换的模型格式有 PyTorch 的 .pth 、huggingface 的 .safetensors 、还有之前 llamma.cpp 采用的 ggmlv3。
在 huggingface 上找到合适格式的模型，下载至 llama.cpp 的 models 目录下。
git clone https://huggingface.co/4bit/Llama-2-7b-chat-hf .">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-03-24T21:33:14+08:00">
    <meta property="article:modified_time" content="2024-03-24T21:33:14+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">llama.cpp部署</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="llamacpp_0"></a>llama.cpp</h2> 
<h3><a id="_2"></a>介绍</h3> 
<h3><a id="_6"></a>部署</h3> 
<h4><a id="_8"></a>介绍</h4> 
<p>大模型的研究分为训练和推理两个部分：</p> 
<ul><li>训练的过程，实际上就是在寻找模型参数，使得模型的损失函数最小化；</li><li>推理结果最优化的过程；</li></ul> 
<p>训练完成之后，模型的参数就固定了，这时候就可以使用模型进行推理，对外提供服务。</p> 
<p>llama.cpp 主要解决的是推理过程中的性能问题。主要有两点优化：</p> 
<ul><li>llama.cpp 使用的是 C 语言写的机器学习张量库 ggml</li><li>llama.cpp 提供了模型量化的工具</li></ul> 
<p>计算类 Python 库的优化手段之一就是使用 C 重新实现，这部分的性能提升非常明显。另外一个是量化，量化是通过牺牲模型参数的精度，来换取模型的推理速度。llama.cpp 提供了大模型量化的工具，可以将模型参数从 32 位浮点数转换为 16 位浮点数，甚至是 8、4 位整数。</p> 
<p>除此之外，llama.cpp 还提供了服务化组件，可以直接对外提供模型的 API 。</p> 
<h4><a id="_26"></a>使用量化模型</h4> 
<p><em><strong>下载编译 llama.cpp</strong></em></p> 
<pre><code class="prism language-shel">git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make
</code></pre> 
<p>在目录下会生成一系列可执行文件</p> 
<ul><li>main：使用模型进行推理</li><li>quantize：量化模型</li><li>server：提供模型 API 服务</li><li>…</li></ul> 
<p><em><strong>准备 llama.cpp 支持的模型</strong></em></p> 
<p>llama.cpp 支持转换的模型格式有 PyTorch 的 .pth 、huggingface 的 .safetensors 、还有之前 llamma.cpp 采用的 ggmlv3。</p> 
<p>在 huggingface 上找到合适格式的模型，下载至 llama.cpp 的 models 目录下。</p> 
<pre><code class="prism language-shell"><span class="token function">git</span> clone https://huggingface.co/4bit/Llama-2-7b-chat-hf ./models/Llama-2-7b-chat-hf
</code></pre> 
<p><em><strong>格式转化</strong></em></p> 
<ul><li>依赖</li></ul> 
<p>llama.cpp 项目下带有 requirements.txt 文件，直接安装依赖即可。</p> 
<pre><code class="prism language-shell">pip <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements.txt
</code></pre> 
<ul><li>转换模型</li></ul> 
<pre><code class="prism language-shell">python convert.py ./models/Llama-2-7b-chat-hf <span class="token parameter variable">--vocabtype</span> spm

params <span class="token operator">=</span> Params<span class="token punctuation">(</span>n_vocab<span class="token operator">=</span><span class="token number">32000</span>, <span class="token assign-left variable">n_embd</span><span class="token operator">=</span><span class="token number">4096</span>, <span class="token assign-left variable">n_mult</span><span class="token operator">=</span><span class="token number">5504</span>, <span class="token assign-left variable">n_layer</span><span class="token operator">=</span><span class="token number">32</span>, <span class="token assign-left variable">n_ctx</span><span class="token operator">=</span><span class="token number">2048</span>, <span class="token assign-left variable">n_ff</span><span class="token operator">=</span><span class="token number">11008</span>, <span class="token assign-left variable">n_head</span><span class="token operator">=</span><span class="token number">32</span>, <span class="token assign-left variable">n_head_kv</span><span class="token operator">=</span><span class="token number">32</span>, <span class="token assign-left variable">f_norm_eps</span><span class="token operator">=</span>1e-05, <span class="token assign-left variable">f_rope_freq_base</span><span class="token operator">=</span>None, <span class="token assign-left variable">f_rope_scale</span><span class="token operator">=</span>None, <span class="token assign-left variable">ftype</span><span class="token operator">=</span>None, <span class="token assign-left variable">path_model</span><span class="token operator">=</span>PosixPath<span class="token punctuation">(</span><span class="token string">'models/Llama-2-7b-chat-hf'</span><span class="token punctuation">))</span>
Loading vocab <span class="token function">file</span> <span class="token string">'models/Llama-2-7b-chat-hf/tokenizer.model'</span>, <span class="token builtin class-name">type</span> <span class="token string">'spm'</span>
<span class="token punctuation">..</span>.
Wrote models/Llama-2-7b-chat-hf/ggml-model-f16.gguf
</code></pre> 
<p>vocabtype 指定分词算法，默认值是 spm，如果是 bpe，需要显示指定。</p> 
<p><em><strong>量化模型</strong></em></p> 
<ul><li>使用 quantize 量化模型</li></ul> 
<p>quantize 提供各种精度的量化。</p> 
<pre><code class="prism language-shell">./quantize

usage: ./quantize <span class="token punctuation">[</span>--help<span class="token punctuation">]</span> <span class="token punctuation">[</span>--allow-requantize<span class="token punctuation">]</span> <span class="token punctuation">[</span>--leave-output-tensor<span class="token punctuation">]</span> model-f32.gguf <span class="token punctuation">[</span>model-quant.gguf<span class="token punctuation">]</span> <span class="token builtin class-name">type</span> <span class="token punctuation">[</span>nthreads<span class="token punctuation">]</span>

  --allow-requantize: Allows requantizing tensors that have already been quantized. Warning: This can severely reduce quality compared to quantizing from 16bit or 32bit
  --leave-output-tensor: Will leave output.weight un<span class="token punctuation">(</span>re<span class="token punctuation">)</span>quantized. Increases model size but may also increase quality, especially when requantizing

Allowed quantization types:
   <span class="token number">2</span>  or  Q4_0   <span class="token builtin class-name">:</span>  <span class="token number">3</span>.56G, +0.2166 ppl @ LLaMA-v1-7B
   <span class="token number">3</span>  or  Q4_1   <span class="token builtin class-name">:</span>  <span class="token number">3</span>.90G, +0.1585 ppl @ LLaMA-v1-7B
   <span class="token number">8</span>  or  Q5_0   <span class="token builtin class-name">:</span>  <span class="token number">4</span>.33G, +0.0683 ppl @ LLaMA-v1-7B
   <span class="token number">9</span>  or  Q5_1   <span class="token builtin class-name">:</span>  <span class="token number">4</span>.70G, +0.0349 ppl @ LLaMA-v1-7B
  <span class="token number">10</span>  or  Q2_K   <span class="token builtin class-name">:</span>  <span class="token number">2</span>.63G, +0.6717 ppl @ LLaMA-v1-7B
  <span class="token number">12</span>  or  Q3_K   <span class="token builtin class-name">:</span> <span class="token builtin class-name">alias</span> <span class="token keyword">for</span> Q3_K_M
  <span class="token number">11</span>  or  Q3_K_S <span class="token builtin class-name">:</span>  <span class="token number">2</span>.75G, +0.5551 ppl @ LLaMA-v1-7B
  <span class="token number">12</span>  or  Q3_K_M <span class="token builtin class-name">:</span>  <span class="token number">3</span>.07G, +0.2496 ppl @ LLaMA-v1-7B
  <span class="token number">13</span>  or  Q3_K_L <span class="token builtin class-name">:</span>  <span class="token number">3</span>.35G, +0.1764 ppl @ LLaMA-v1-7B
  <span class="token number">15</span>  or  Q4_K   <span class="token builtin class-name">:</span> <span class="token builtin class-name">alias</span> <span class="token keyword">for</span> Q4_K_M
  <span class="token number">14</span>  or  Q4_K_S <span class="token builtin class-name">:</span>  <span class="token number">3</span>.59G, +0.0992 ppl @ LLaMA-v1-7B
  <span class="token number">15</span>  or  Q4_K_M <span class="token builtin class-name">:</span>  <span class="token number">3</span>.80G, +0.0532 ppl @ LLaMA-v1-7B
  <span class="token number">17</span>  or  Q5_K   <span class="token builtin class-name">:</span> <span class="token builtin class-name">alias</span> <span class="token keyword">for</span> Q5_K_M
  <span class="token number">16</span>  or  Q5_K_S <span class="token builtin class-name">:</span>  <span class="token number">4</span>.33G, +0.0400 ppl @ LLaMA-v1-7B
  <span class="token number">17</span>  or  Q5_K_M <span class="token builtin class-name">:</span>  <span class="token number">4</span>.45G, +0.0122 ppl @ LLaMA-v1-7B
  <span class="token number">18</span>  or  Q6_K   <span class="token builtin class-name">:</span>  <span class="token number">5</span>.15G, <span class="token parameter variable">-0.0008</span> ppl @ LLaMA-v1-7B
   <span class="token number">7</span>  or  Q8_0   <span class="token builtin class-name">:</span>  <span class="token number">6</span>.70G, +0.0004 ppl @ LLaMA-v1-7B
   <span class="token number">1</span>  or  F16    <span class="token builtin class-name">:</span> <span class="token number">13</span>.00G              @ 7B
   <span class="token number">0</span>  or  F32    <span class="token builtin class-name">:</span> <span class="token number">26</span>.00G              @ 7B

</code></pre> 
<ul><li>执行量化模型</li></ul> 
<pre><code class="prism language-shell">./quantize ./models/Llama-2-7b-chat-hf/ggml-model-f16.gguf ./models/Llama-2-7b-chat-hf/ggml-model-q4_0.gguf Q4_0

llama_model_quantize_internal: model size  <span class="token operator">=</span> <span class="token number">12853.02</span> MB
llama_model_quantize_internal: quant size  <span class="token operator">=</span>  <span class="token number">3647.87</span> MB
llama_model_quantize_internal: hist: <span class="token number">0.036</span> <span class="token number">0.015</span> <span class="token number">0.025</span> <span class="token number">0.039</span> <span class="token number">0.056</span> <span class="token number">0.076</span> <span class="token number">0.096</span> <span class="token number">0.112</span> <span class="token number">0.118</span> <span class="token number">0.112</span> <span class="token number">0.096</span> <span class="token number">0.077</span> <span class="token number">0.056</span> <span class="token number">0.039</span> <span class="token number">0.025</span> <span class="token number">0.021</span>

</code></pre> 
<p>量化之后，模型的大小从 13G 降低到 3.6G，但模型精度从 16 位浮点数降低到 4 位整数。</p> 
<h4><a id="_GGUF__130"></a>运行 GGUF 模型</h4> 
<p><em><strong>下载模型</strong></em></p> 
<p>在 llama.cpp 项目的首页 https://github.com/ggerganov/llama.cpp 有列举支持的模型</p> 
<p>去 https://huggingface.co/models 找 GGUF 格式的大模型版本，下载模型文件放在 llama.cpp 项目 models 目录下。</p> 
<pre><code class="prism language-shell"><span class="token function">git</span> clone https://huggingface.co/rozek/LLaMA-2-7B-32K-Instruct_GGUF ./models/LLaMA-2-7B-32K-Instruct_GGUF
</code></pre> 
<p>仓库中包含各种量化位数的模型，Q2、Q3、Q4、Q5、Q6、Q8、F16。量化模型的命名方法遵循: “Q” + 量化比特位 + 变种。</p> 
<p>量化位数越少，对硬件资源的要求越低，但是模型的精度也越低。</p> 
<blockquote> 
 <p>使用 <code>git clone</code> 命令下载的模型会加载失败，提示 magic 不匹配。</p> 
 <p>在网页直接下载的模型可以正常使用。</p> 
</blockquote> 
<p><em><strong>推理</strong></em></p> 
<p>在 llama.cpp 项目的根目录，编译源码之后，执行下面的命令，使用模型进行推理。</p> 
<pre><code class="prism language-shell">./main <span class="token parameter variable">-m</span> ./models/llama-2-7b-langchain-chat-GGUF/llama-2-7b-langchain-chat-q4_0.gguf <span class="token parameter variable">-p</span> <span class="token string">"What color is the sun?"</span> <span class="token parameter variable">-n</span> <span class="token number">1024</span>

 What color is the sun?
 nobody knows. It’s not a specific color, <span class="token function">more</span> a range of colors. Some people say it<span class="token string">'s yellow; some say orange, while others believe it to be red or white. Ultimately, we can only imagine what color the sun might be because we can'</span>t see its exact color from this planet due to its immense distance away<span class="token operator">!</span>
It’s fascinating how something so fundamental to our daily lives remains a mystery even after decades of scientific inquiry into its properties and behavior.” <span class="token punctuation">[</span>end of text<span class="token punctuation">]</span>

llama_print_timings:        load <span class="token function">time</span> <span class="token operator">=</span>   <span class="token number">376.57</span> ms
llama_print_timings:      sample <span class="token function">time</span> <span class="token operator">=</span>    <span class="token number">56.40</span> ms /   <span class="token number">105</span> runs   <span class="token punctuation">(</span>    <span class="token number">0.54</span> ms per token,  <span class="token number">1861.77</span> tokens per second<span class="token punctuation">)</span>
llama_print_timings: prompt <span class="token builtin class-name">eval</span> <span class="token function">time</span> <span class="token operator">=</span>   <span class="token number">366.68</span> ms /     <span class="token number">7</span> tokens <span class="token punctuation">(</span>   <span class="token number">52.38</span> ms per token,    <span class="token number">19.09</span> tokens per second<span class="token punctuation">)</span>
llama_print_timings:        <span class="token builtin class-name">eval</span> <span class="token function">time</span> <span class="token operator">=</span> <span class="token number">15946.81</span> ms /   <span class="token number">104</span> runs   <span class="token punctuation">(</span>  <span class="token number">153.33</span> ms per token,     <span class="token number">6.52</span> tokens per second<span class="token punctuation">)</span>
llama_print_timings:       total <span class="token function">time</span> <span class="token operator">=</span> <span class="token number">16401.43</span> ms
</code></pre> 
<p>当然，也可以用上面量化的模型进行推理。</p> 
<pre><code class="prism language-shell">./main <span class="token parameter variable">-m</span>  ./models/Llama-2-7b-chat-hf/ggml-model-q4_0.gguf <span class="token parameter variable">-p</span> <span class="token string">"What color is the sun?"</span> <span class="token parameter variable">-n</span> <span class="token number">1024</span>

What color is the sun?
 sierp <span class="token number">10</span>, <span class="token number">2017</span> at <span class="token number">12</span>:04 pm - Reply
The sun does not have a color because it emits light <span class="token keyword">in</span> all wavelengths of the visible spectrum and beyond. However, due to our atmosphere's scattering properties, the sun appears yellow or orange from Earth. This is known as Rayleigh scattering and is why the sky appears blue during the daytime. <span class="token punctuation">[</span>end of text<span class="token punctuation">]</span>

llama_print_timings:        load <span class="token function">time</span> <span class="token operator">=</span> <span class="token number">90612.21</span> ms
llama_print_timings:      sample <span class="token function">time</span> <span class="token operator">=</span>    <span class="token number">52.31</span> ms /    <span class="token number">91</span> runs   <span class="token punctuation">(</span>    <span class="token number">0.57</span> ms per token,  <span class="token number">1739.76</span> tokens per second<span class="token punctuation">)</span>
llama_print_timings: prompt <span class="token builtin class-name">eval</span> <span class="token function">time</span> <span class="token operator">=</span>   <span class="token number">523.38</span> ms /     <span class="token number">7</span> tokens <span class="token punctuation">(</span>   <span class="token number">74.77</span> ms per token,    <span class="token number">13.37</span> tokens per second<span class="token punctuation">)</span>
llama_print_timings:        <span class="token builtin class-name">eval</span> <span class="token function">time</span> <span class="token operator">=</span> <span class="token number">15266.91</span> ms /    <span class="token number">90</span> runs   <span class="token punctuation">(</span>  <span class="token number">169.63</span> ms per token,     <span class="token number">5.90</span> tokens per second<span class="token punctuation">)</span>
llama_print_timings:       total <span class="token function">time</span> <span class="token operator">=</span> <span class="token number">15911.47</span> ms
</code></pre> 
<p>四位量化模型，在没有 GPU 的情况下，基本能够实现实时推理。敲完命令，按回车，就能看到模型的回复。</p> 
<p>main 命令有一系列参数可选，其中比较重要的参数有：</p> 
<p>-ins 交互模式，可以连续对话，上下文会保留<br> -c 控制上下文的长度，值越大越能参考更长的对话历史（默认：512）<br> -n 控制回复生成的最大长度（默认：128）<br> –temp 温度系数，值越低回复的随机性越小</p> 
<p><em><strong>交互模式下使用模型</strong></em></p> 
<pre><code class="prism language-shell">./main <span class="token parameter variable">-m</span> ./models/llama-2-7b-langchain-chat-GGUF/llama-2-7b-langchain-chat-q4_0.gguf <span class="token parameter variable">-ins</span>

<span class="token operator">&gt;</span> 世界上最大的鱼是什么？
卡加内利亚鲨为世界最大的鱼，体长达60英尺（18）。牠们的头部相当于一只小车，身体非常丑，腹部有两个气孔，气孔之间还有一个大口径的鳃，用于进行捕食。牠们通常是从水中搴出来到陆地上抓到的小鱼，然后产生大量液体以解脱自己的身体。

<span class="token operator">&gt;</span> 现在还有这种鱼吗？
作者所提到的“卡加内利亚鲨”，应该是指的是“卡加内利亚鳄”。卡加内利亚鳄是一种大型淡水肉食性鱼类，分布于欧洲和非洲部分区域。这种鱼的体长最大可达60英尺（18），是世界上已知最大的鱼之一。

不过，现在这种鱼已经消失了，因为人类对戒备和保护水生生物的意识程度低下，以及环境污染等多方面原因。

</code></pre> 
<p>交互模式下，以对话的形式，有上下文的连续使用大模型。</p> 
<h4><a id="_API__210"></a>提供模型的 API 服务</h4> 
<p>有两种方式，一种是使用 llama.cpp 提供的 API 服务，另一种是使用第三方提供的工具包。</p> 
<p><em><strong>使用 llama.cpp server 提供 API 服务</strong></em></p> 
<p>前面编译之后，会在 llama.cpp 项目的根目录下生成一个 server 可执行文件，执行下面的命令，启动 API 服务。</p> 
<pre><code class="prism language-shell">./server <span class="token parameter variable">-m</span> ./models/llama-2-7b-langchain-chat-GGUF/llama-2-7b-langchain-chat-q4_0.gguf <span class="token parameter variable">--host</span> <span class="token number">0.0</span>.0.0 <span class="token parameter variable">--port</span> <span class="token number">8080</span>

llm_load_tensors: mem required  <span class="token operator">=</span> <span class="token number">3647.96</span> MB <span class="token punctuation">(</span>+  <span class="token number">256.00</span> MB per state<span class="token punctuation">)</span>
<span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>
llama_new_context_with_model: kv self size  <span class="token operator">=</span>  <span class="token number">256.00</span> MB
llama_new_context_with_model: compute buffer total size <span class="token operator">=</span>   <span class="token number">71.97</span> MB

llama server listening at http://0.0.0.0:8080

<span class="token punctuation">{<!-- --></span><span class="token string">"timestamp"</span>:1693789480,<span class="token string">"level"</span><span class="token builtin class-name">:</span><span class="token string">"INFO"</span>,<span class="token string">"function"</span><span class="token builtin class-name">:</span><span class="token string">"main"</span>,<span class="token string">"line"</span>:1593,<span class="token string">"message"</span><span class="token builtin class-name">:</span><span class="token string">"HTTP server listening"</span>,<span class="token string">"hostname"</span><span class="token builtin class-name">:</span><span class="token string">"0.0.0.0"</span>,<span class="token string">"port"</span>:8080<span class="token punctuation">}</span>

</code></pre> 
<p>这样就启动了一个 API 服务，可以使用 curl 命令进行测试。</p> 
<pre><code class="prism language-shell"><span class="token function">curl</span> <span class="token parameter variable">--request</span> POST <span class="token punctuation">\</span>
    <span class="token parameter variable">--url</span> http://localhost:8080/completion <span class="token punctuation">\</span>
    <span class="token parameter variable">--header</span> <span class="token string">"Content-Type: application/json"</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--data</span> <span class="token string">'{"prompt": "What color is the sun?","n_predict": 512}'</span>

<span class="token punctuation">{<!-- --></span><span class="token string">"content"</span><span class="token builtin class-name">:</span><span class="token string">"....."</span>,<span class="token string">"generation_settings"</span>:<span class="token punctuation">{<!-- --></span><span class="token string">"frequency_penalty"</span>:0.0,<span class="token string">"grammar"</span><span class="token builtin class-name">:</span><span class="token string">""</span>,<span class="token string">"ignore_eos"</span>:false,<span class="token string">"logit_bias"</span>:<span class="token punctuation">[</span><span class="token punctuation">]</span>,<span class="token string">"mirostat"</span>:0,<span class="token string">"mirostat_eta"</span>:0.10000000149011612,<span class="token string">"mirostat_tau"</span>:5.0,<span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">}</span><span class="token punctuation">}</span>

</code></pre> 
<p><em><strong>使用第三方工具包提供的 API 服务</strong></em></p> 
<p>在 llamm.cpp 项目的首页 https://github.com/ggerganov/llama.cpp 中有提到各种语言编写的第三方工具包，可以使用这些工具包提供 API 服务，包括 Python、Go、Node.js、Ruby、Rust、C#/.NET、Scala 3、Clojure、React Native、Java 等语言的实现。</p> 
<p>以 Python 为例，使用 llama-cpp-python 提供 API 服务。</p> 
<ul><li>安装依赖</li></ul> 
<pre><code class="prism language-shell">pip <span class="token function">install</span> llama-cpp-python <span class="token parameter variable">-i</span> https://mirrors.aliyun.com/pypi/simple/
</code></pre> 
<p>如果需要针对特定的硬件进行优化，就配置 “CMAKE_ARGS” 参数，详情请参数 https://github.com/abetlen/llama-cpp-python 。我本地是 CPU 环境，就没有进行额外的配置。</p> 
<ul><li>启动 API 服务</li></ul> 
<pre><code class="prism language-shell">python <span class="token parameter variable">-m</span> llama_cpp.server <span class="token parameter variable">--model</span> ./models/llama-2-7b-langchain-chat-GGUF/llama-2-7b-langchain-chat-q4_0.gguf

llama_new_context_with_model: kv self size  <span class="token operator">=</span> <span class="token number">1024.00</span> MB
llama_new_context_with_model: compute buffer total size <span class="token operator">=</span>  <span class="token number">153.47</span> MB
AVX <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> AVX2 <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> AVX512 <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX512_VBMI <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX512_VNNI <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> FMA <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> NEON <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> ARM_FMA <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> F16C <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> FP16_VA <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> WASM_SIMD <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> BLAS <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> SSE3 <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> SSSE3 <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> VSX <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span>
INFO:     Started server process <span class="token punctuation">[</span><span class="token number">57637</span><span class="token punctuation">]</span>
INFO:     Waiting <span class="token keyword">for</span> application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://localhost:8000 <span class="token punctuation">(</span>Press CTRL+C to quit<span class="token punctuation">)</span>

</code></pre> 
<ul><li>使用 curl 测试 API 服务</li></ul> 
<pre><code class="prism language-shell"><span class="token function">curl</span> <span class="token parameter variable">-X</span> <span class="token string">'POST'</span> <span class="token punctuation">\</span>
  <span class="token string">'http://localhost:8000/v1/chat/completions'</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">-H</span> <span class="token string">'accept: application/json'</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">-H</span> <span class="token string">'Content-Type: application/json'</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">-d</span> <span class="token string">'{
  "messages": [
    {
      "content": "You are a helpful assistant.",
      "role": "system"
    },
    {
      "content": "Write a poem for Chinese?",
      "role": "user"
    }
  ]
}'</span>

<span class="token punctuation">{<!-- --></span><span class="token string">"id"</span><span class="token builtin class-name">:</span><span class="token string">"chatcmpl-c3eec466-6073-41e2-817f-9d1e307ab55f"</span>,<span class="token string">"object"</span><span class="token builtin class-name">:</span><span class="token string">"chat.completion"</span>,<span class="token string">"created"</span>:1693829165,<span class="token string">"model"</span><span class="token builtin class-name">:</span><span class="token string">"./models/llama-2-7b-langchain-chat-GGUF/llama-2-7b-langchain-chat-q4_0.gguf"</span>,<span class="token string">"choices"</span>:<span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span><span class="token string">"index"</span>:0,<span class="token string">"message"</span>:<span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token builtin class-name">:</span><span class="token string">"assistant"</span>,<span class="token string">"content"</span><span class="token builtin class-name">:</span><span class="token string">"I am not programmed to write poems in different languages. How about I"</span><span class="token punctuation">}</span>,<span class="token string">"finish_reason"</span><span class="token builtin class-name">:</span><span class="token string">"length"</span><span class="token punctuation">}</span><span class="token punctuation">]</span>,<span class="token string">"usage"</span>:<span class="token punctuation">{<!-- --></span><span class="token string">"prompt_tokens"</span>:26,<span class="token string">"completion_tokens"</span>:16,<span class="token string">"total_tokens"</span>:42<span class="token punctuation">}</span><span class="token punctuation">}</span>

</code></pre> 
<ul><li>使用 openai 调用 API 服务</li></ul> 
<pre><code class="prism language-shell"><span class="token comment"># -*- coding: utf-8 -*-</span>

<span class="token function">import</span> openai
openai.api_key <span class="token operator">=</span> <span class="token string">'random'</span>
openai.api_base <span class="token operator">=</span> <span class="token string">'http://localhost:8000/v1'</span>
messages <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span><span class="token string">'role'</span><span class="token builtin class-name">:</span> <span class="token string">'system'</span>, <span class="token string">'content'</span><span class="token builtin class-name">:</span> u<span class="token string">'你是一个真实的人，老实回答提问，不要耍滑头'</span><span class="token punctuation">}</span><span class="token punctuation">]</span>
messages.append<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">'role'</span><span class="token builtin class-name">:</span> <span class="token string">'user'</span>, <span class="token string">'content'</span><span class="token builtin class-name">:</span> u<span class="token string">'你昨晚去哪里了'</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
response <span class="token operator">=</span> openai.ChatCompletion.create<span class="token punctuation">(</span>
    <span class="token assign-left variable">model</span><span class="token operator">=</span><span class="token string">'random'</span>,
    <span class="token assign-left variable">messages</span><span class="token operator">=</span>messages,
<span class="token punctuation">)</span>
print<span class="token punctuation">(</span>response<span class="token punctuation">[</span><span class="token string">'choices'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'message'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'content'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="_322"></a>参考文档</h4> 
<p><a href="https://www.chenshaowen.com/blog/llama-cpp-that-is-a-llm-deployment-tool.html" rel="nofollow">大模型部署工具 llama.cpp</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/78fa06af806f498b9a051f7b594277af/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">macOS访问samba文件夹的正确姿势，在哪里更改“macOS的连接身份“？还真不好找！</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d26c94789ef174ad30edfa46f2ce8176/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">怎么在python里面安装库,python中怎么安装库</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>