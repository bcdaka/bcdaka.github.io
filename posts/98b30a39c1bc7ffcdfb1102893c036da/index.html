<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【AI学习】简单聊聊后训练（Post-Training）的重要性 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/98b30a39c1bc7ffcdfb1102893c036da/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【AI学习】简单聊聊后训练（Post-Training）的重要性">
  <meta property="og:description" content="最近的学习，越来越领会后训练的重要性。预训练只能学到语言本身的统计概率、基础通用的语义，如果希望模型发挥更好的作用，后训练显得越来越重要。
这里，我想，可能存在这样一个逻辑，在预训练阶段，只是学习到了网络的平均水平，而通过后期的对齐和引导，模型是可以输出高于网上数据的平均水平的结果，进一步的，模型也可以自动的引导这个提升过程，就像《SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions》文章所看到的那样。
在前面，Meta推出 Llama 3 的介绍中，提到了后训练的重要性，“我们在后训练阶段投入了大量的工作，我想大家都喜欢谈论预训练，以及我们扩大了规模，使用的 GPU 数量达到了数万个，以及在预训练中使用了多少数据，但实际上，真正的关键在于后训练阶段。这就是我们目前花费大部分时间的地方，我们在这里生成了大量的人工注释，执行了大量的有监督微调（SFT），我们正在做的事情，比如拒绝采样、PPO、DPO，尝试在这些模型的可用性、人类方面的特征以及预训练中的大规模数据之间找到平衡，这就是我们如何思考这些问题的”。
恰好，今天看到两条微博，其中都提到了后训练。
微博转载一 来自微博@宝玉xp老师的微博：
OpenAI 创始人 John Schulman 访谈节选：为什么 GPT-4 比一年前更“聪明”了？主要都是后训练（Post-Training）带来的！
另外他认为，在强化学习研究领域，研究人员需要具备丰富的经验和敏锐的直觉。了解整个技术堆栈，并对各个部分充满好奇心是关键。此外，从第一性原理出发思考问题，而不仅仅依靠实验证据，也能够帮助研究人员在数据操控和环境设置方面做出更好的决策。
Dwarkesh Patel：在未来，用于训练的计算力中，预训练与后训练的比例是否会明显偏向后训练呢？
John Schulman：确实，有一些观点支持这种说法。
Dwarkesh Patel：我是说，现在这个比例非常不平衡。
John Schulman：但你可以认为，模型生成的输出质量比网上的大多数内容都要高。因此，让模型自己思考似乎更有道理，而不仅仅是训练来模仿网络上的内容。所以，我认为从第一性原理上来说，这是有说服力的。我会说，我们通过后训练取得了很多进步。因此，我不确定。所以，我希望我们会继续推动这种方法，并且可能会增加投入到后训练中的计算力。
Dwarkesh Patel：当前的 GPT-4 的 ELO 分数比最初发布的版本高出了大约 100 分。这是否全都是后训练带来的改进呢？
John Schulman：对，我会说大部分都是后训练带来的。
Dwarkesh Patel：这很有意思。
John Schulman：因此，有很多不同的改进方向。我们会考虑数据质量，数据数量，进行更多的部署和收集新数据的迭代，改变你收集的注解种类。因此，有很多因素叠加在一起。但是全部加在一起，就会带来一个相当不错的，有效的计算力提升。
Dwarkesh Patel：后训练的优化程度对于竞争优势有多大影响呢？
John Schulman：目前，我会区别公司是通过我们的模型有多大等等。那么，找出你之前提到的所有这些数据的复杂问题的公司，会占据大优势吗？
John Schulman：我认为这确实是一个优势，因为这是一个非常复杂的任务。因此，你必须有很多有技能的人来执行它。因此，存在大量的隐性知识。同时也需要大量的组织知识。我认为后训练的过程，创建一个具备人们所关心的所有功能的模型，是十分复杂的。这需要付出大量的努力，它是大量研发工作的积累。我会说这种情况在某种程度上形成了一种壁垒，要想立即启动这种模型并非易事。
Dwarkesh Patel：看起来那些正在进行最严肃的预训练努力的公司，也在进行严肃的后训练努力。因此，看起来这种模型有可能被复制或有更多的类似努力出现。
John Schulman：另外，还有一种情况使得这个壁垒并非那么明显，那就是你可以提取模型，或者复制别人的模型输出，或者使用别人的模型进行比较。我认为大公司可能并不会这样做，因为这违反了服务条款，也会损害他们的自尊心，但我预计一些规模较小的参与者可能正在这样做以便更好地起步。
Dwarkesh Patel：那些真正擅长进行这种强化学习（RL）研究的人有什么样的特质呢？我听说这种研究非常具有挑战性，但是什么样的直觉能帮助你找到操控数据和设置环境的方法呢？
John Schulman：我觉得有相当多的经验是关键。自从研究生时期以来，我一直在研究 RL 算法，涉及到数据收集、到注释过程，再到与语言模型的交互。所以，我算是涉猎了这些领域。我认为，在这类研究中表现出色的人通常对整个技术堆栈有全面的了解，并且对其中的各个部分充满好奇心。他们不仅依靠实验证据来更新自己的观点，还会从第一性原理出发思考问题。比如，假设深度学习是有效的，那么理想的收集数据的类型应该是什么，等等。
微博转载二 来自@i陆三金老师的微博：
Scale AI 创始人 Alexandr Wang 对于 OpenAI 和 Google 最近发布会的评价，他反复强调 post-training（后训练）非常重要，不过他就是做这部分数据的，并表示下周有重要发布，还请理性看待他的言论：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-17T22:01:38+08:00">
    <meta property="article:modified_time" content="2024-05-17T22:01:38+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【AI学习】简单聊聊后训练（Post-Training）的重要性</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>最近的学习，越来越领会后训练的重要性。预训练只能学到语言本身的统计概率、基础通用的语义，如果希望模型发挥更好的作用，后训练显得越来越重要。<br> 这里，我想，可能存在这样一个逻辑，在预训练阶段，只是学习到了网络的平均水平，而通过后期的对齐和引导，模型是可以输出高于网上数据的平均水平的结果，进一步的，模型也可以自动的引导这个提升过程，就像<a href="https://blog.csdn.net/bylander/article/details/138634657">《SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions》</a>文章所看到的那样。</p> 
<p>在前面，<a href="https://blog.csdn.net/bylander/article/details/138230810">Meta推出 Llama 3 的介绍</a>中，提到了后训练的重要性，“我们在后训练阶段投入了大量的工作，我想大家都喜欢谈论预训练，以及我们扩大了规模，使用的 GPU 数量达到了数万个，以及在预训练中使用了多少数据，但实际上，真正的关键在于后训练阶段。这就是我们目前花费大部分时间的地方，我们在这里生成了大量的人工注释，执行了大量的有监督微调（SFT），我们正在做的事情，比如拒绝采样、PPO、DPO，尝试在这些模型的可用性、人类方面的特征以及预训练中的大规模数据之间找到平衡，这就是我们如何思考这些问题的”。</p> 
<p>恰好，今天看到两条微博，其中都提到了后训练。</p> 
<h3><a id="_6"></a>微博转载一</h3> 
<p>来自微博@宝玉xp老师的微博：</p> 
<p>OpenAI 创始人 John Schulman 访谈节选：为什么 GPT-4 比一年前更“聪明”了？主要都是后训练（Post-Training）带来的！</p> 
<p>另外他认为，在强化学习研究领域，研究人员需要具备丰富的经验和敏锐的直觉。了解整个技术堆栈，并对各个部分充满好奇心是关键。此外，从第一性原理出发思考问题，而不仅仅依靠实验证据，也能够帮助研究人员在数据操控和环境设置方面做出更好的决策。</p> 
<hr> 
<p>Dwarkesh Patel：在未来，用于训练的计算力中，预训练与后训练的比例是否会明显偏向后训练呢？</p> 
<p>John Schulman：确实，有一些观点支持这种说法。</p> 
<p>Dwarkesh Patel：我是说，现在这个比例非常不平衡。</p> 
<p>John Schulman：但你可以认为，模型生成的输出质量比网上的大多数内容都要高。因此，让模型自己思考似乎更有道理，而不仅仅是训练来模仿网络上的内容。所以，我认为从第一性原理上来说，这是有说服力的。我会说，我们通过后训练取得了很多进步。因此，我不确定。所以，我希望我们会继续推动这种方法，并且可能会增加投入到后训练中的计算力。</p> 
<p>Dwarkesh Patel：当前的 GPT-4 的 ELO 分数比最初发布的版本高出了大约 100 分。这是否全都是后训练带来的改进呢？</p> 
<p>John Schulman：对，我会说大部分都是后训练带来的。</p> 
<p>Dwarkesh Patel：这很有意思。</p> 
<p>John Schulman：因此，有很多不同的改进方向。我们会考虑数据质量，数据数量，进行更多的部署和收集新数据的迭代，改变你收集的注解种类。因此，有很多因素叠加在一起。但是全部加在一起，就会带来一个相当不错的，有效的计算力提升。</p> 
<p>Dwarkesh Patel：后训练的优化程度对于竞争优势有多大影响呢？</p> 
<p>John Schulman：目前，我会区别公司是通过我们的模型有多大等等。那么，找出你之前提到的所有这些数据的复杂问题的公司，会占据大优势吗？</p> 
<p>John Schulman：我认为这确实是一个优势，因为这是一个非常复杂的任务。因此，你必须有很多有技能的人来执行它。因此，存在大量的隐性知识。同时也需要大量的组织知识。我认为后训练的过程，创建一个具备人们所关心的所有功能的模型，是十分复杂的。这需要付出大量的努力，它是大量研发工作的积累。我会说这种情况在某种程度上形成了一种壁垒，要想立即启动这种模型并非易事。</p> 
<p>Dwarkesh Patel：看起来那些正在进行最严肃的预训练努力的公司，也在进行严肃的后训练努力。因此，看起来这种模型有可能被复制或有更多的类似努力出现。</p> 
<p>John Schulman：另外，还有一种情况使得这个壁垒并非那么明显，那就是你可以提取模型，或者复制别人的模型输出，或者使用别人的模型进行比较。我认为大公司可能并不会这样做，因为这违反了服务条款，也会损害他们的自尊心，但我预计一些规模较小的参与者可能正在这样做以便更好地起步。</p> 
<p>Dwarkesh Patel：那些真正擅长进行这种强化学习（RL）研究的人有什么样的特质呢？我听说这种研究非常具有挑战性，但是什么样的直觉能帮助你找到操控数据和设置环境的方法呢？</p> 
<p>John Schulman：我觉得有相当多的经验是关键。自从研究生时期以来，我一直在研究 RL 算法，涉及到数据收集、到注释过程，再到与语言模型的交互。所以，我算是涉猎了这些领域。我认为，在这类研究中表现出色的人通常对整个技术堆栈有全面的了解，并且对其中的各个部分充满好奇心。他们不仅依靠实验证据来更新自己的观点，还会从第一性原理出发思考问题。比如，假设深度学习是有效的，那么理想的收集数据的类型应该是什么，等等。</p> 
<h3><a id="_45"></a>微博转载二</h3> 
<p>来自@i陆三金老师的微博：<br> Scale AI 创始人 Alexandr Wang 对于 OpenAI 和 Google 最近发布会的评价，他反复强调 post-training（后训练）非常重要，不过他就是做这部分数据的，并表示下周有重要发布，还请理性看待他的言论：</p> 
<ul><li>从很多方面来看，Gemini 1.5 Flash 都是 Google 发布的精华。1M 上下文小模型的 Flash 性能令人难以置信。</li></ul> 
<p>目前，OpenAI 的 GPT-4o 是最好的大型模型，谷歌的 Gemini 1.5 Flash 是最好的小型模型。</p> 
<p>比赛开始了。</p> 
<ul><li>无论如何，收敛水平都令人着迷——4o 和 Astra、Veo 和 Sora 之间的相似性等。两个实验室似乎在遵循相对相似的技术轨迹。</li></ul> 
<p>在我看来，对于行业来说，分歧比趋同更有利。唉……</p> 
<ul><li>GPT-4o 的评估收益令人震惊，这似乎几乎完全归功于出色的 post-training。</li></ul> 
<p>将刚发布时的 GPT-4 与 GPT 4o 进行比较，它们是天壤之别，这显示了通过出色的数据进行出色的 post-training 的威力。</p> 
<ul><li>事实上，当今顶级模型（Claude 3、Llama3、GPT-4o、Gemini 1.5）之间的大部分差异似乎完全是由 post-training（后训练）而非 pre-training（预训练）造成的。</li></ul> 
<p>这可能是因为预训练中的供应链问题——算力不足 + 数据稀缺。</p> 
<ul><li>不管怎么说，这也表明了我们对未来发展的预期：</li></ul> 
<p>进一步推进 post-training，这需要丰富的前沿数据。</p> 
<p>post-training 数据的丰富程度可能会完全制约前进的步伐。</p> 
<ul><li>一旦你开始考虑所有的前沿研究领域，这一点就显得格外正确：</li></ul> 
<p>agents、多模态、多语言、专家思维链、企业工作流等。</p> 
<p>所有这些都依赖于现在根本不存在的数据，而我们需要生产资料。</p> 
<ul><li>AI 数据是一个非常棘手的问题，没有简单的解决办法。</li></ul> 
<p>纯合成数据并非我们的救星——合成数据无法将模型的边界推向现有能力之外。它只是一种工具，而非灵丹妙药。</p> 
<p>这是一个人工智能与人类专家共生的难题。</p> 
<ul><li>但我们真的需要一步步改变。过去二十年来，人工智能的每一次重大突破都是由更好、更多的数据推动的，最早可以追溯到 ImageNet 上最初的 AlexNet 深度神经网络。</li></ul> 
<p>Scaling laws 清楚地说明了我们的方向–我们需要更多的数据！</p> 
<ul><li>解决人工智能数据问题真的很重要。这是我毕生的事业。</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/fc29204957ab101dc24ed2702764585a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">7款读文献的AI神器，可总结分析文件、读论文必备！</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5e5b89b88bc038257cff73bd7bdc45f5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【C&#43;&#43;】初识类和对象</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>