<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>详解各种LLM系列｜（5）LLaMA 3模型解析（Meta重磅发布！） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/2b40b2664a5c3870a9f26471750638e6/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="详解各种LLM系列｜（5）LLaMA 3模型解析（Meta重磅发布！）">
  <meta property="og:description" content="一、引言 Blog链接：https://ai.meta.com/blog/meta-llama-3/
MODEL CARD: https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md
体验链接：https://meta.ai/ or https://huggingface.co/chat/
4月18日，Meta突然发布Llama 3， 在Llama 2的基础上进行了进一步的升级，包括使用更高质量的数据集、模型架构的改进、引入新的信任和安全工具（如Llama Guard 2、Code Shield和CyberSec Eval 2）等；
这次Llama 3 的发布包括了8B 和 70B 两种规模的预训练和指令微调生成文本模型。
Llama 3型号将很快在AWS、Databricks、Google Cloud、huggingFace、Kaggle、IBM WatsonX、微软Azure、NVIDIA NIM和Snowflake上推出，并得到AMD、AWS、戴尔、英特尔、NVIDIA和高通提供的硬件平台的支持
二、卓越的性能 2.1 标准测试 这次的 Llama 在性能上展现了大幅度提升，包括最直接的 8k 上下文（之前是4k），以及可以更好地完成输出任务。
通过pre-training和post-training的改进，Llama 3的预训练和指令微调模型是目前在8B和70B参数尺度上存在的最好的模型（截止至发布日期）。
Post-training的改进大大降低了错误拒绝率，改善了一致性，增加了模型响应的多样性；Llama 3在推理、代码生成和指令跟踪等功能上有极大的提升，具体看一下对比数据：
(Llama 3 Pretrained模型)
(Llama 3 Instruct模型) （这里再附一张 Llama 2 和 3 的对比）
2.2 人类偏好测试 在Llama 3的开发过程中，为了优化实际场景的性能，Meta开发了一个新的高质量的人类评价集。这个评估集包含1800个提示，涵盖了12个关键用例：征求建议、头脑风暴、分类、封闭式问题回答、编码、创造性写作、提取、作为一个角色/角色中、开放式问题回答、推理、重写和总结。
下面的图表显示了模型对这些类别的人类评估的汇总结果：
根据人类评估者的偏好排名，Llama 的 70B 参数模型在实际应用场景中的表现，尤其是在指令跟随方面，相较于其他相当规模的模型表现出了显著的优势。
三、优化之处 3.1 模型架构 3.1.1 Tokenzier 分词器：与Llama 2不同的是，Llama 3将tokenizer由sentencepiece换成tiktoken，词汇量从 的32K增加到 128K，增加了 4 倍 （更大的词汇库能够更高效地编码文本，增加编码效率，可以实现更好的下游性能。不过这也会导致嵌入层的输入和输出矩阵尺寸增大，模型参数量也会增大）。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-17T14:04:46+08:00">
    <meta property="article:modified_time" content="2024-06-17T14:04:46+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">详解各种LLM系列｜（5）LLaMA 3模型解析（Meta重磅发布！）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>一、引言</h2> 
<p>Blog链接：<a href="https://ai.meta.com/blog/meta-llama-3/" rel="nofollow" title="https://ai.meta.com/blog/meta-llama-3/">https://ai.meta.com/blog/meta-llama-3/</a></p> 
<p>MODEL CARD: <a href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md" title="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md">https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md</a></p> 
<p>体验链接：<a href="https://meta.ai/" rel="nofollow" title="https://meta.ai/">https://meta.ai/</a>  or  <a href="https://huggingface.co/chat/" rel="nofollow" title="https://huggingface.co/chat/">https://huggingface.co/chat/</a></p> 
<p>4月18日，Meta突然发布Llama 3， 在Llama 2的基础上进行了进一步的升级，包括使用更高质量的数据集、模型架构的改进、引入新的信任和安全工具（如Llama Guard 2、Code Shield和CyberSec Eval 2）等；</p> 
<p>这次Llama 3 的发布包括了8B 和 70B 两种规模的预训练和指令微调生成文本模型。</p> 
<p><img alt="" height="123" src="https://images2.imgbox.com/a1/44/fU3UNT9Q_o.png" width="998"></p> 
<p></p> 
<p>Llama 3型号将很快在AWS、Databricks、Google Cloud、huggingFace、Kaggle、IBM WatsonX、微软Azure、NVIDIA NIM和Snowflake上推出，并得到AMD、AWS、戴尔、英特尔、NVIDIA和高通提供的硬件平台的支持</p> 
<hr> 
<h2></h2> 
<h2>二、卓越的性能</h2> 
<h3>2.1 标准测试</h3> 
<p>这次的 Llama 在性能上展现了大幅度提升，包括最直接的 8k 上下文（之前是4k），以及可以更好地完成输出任务。</p> 
<p>通过pre-training和post-training的改进，Llama 3的预训练和指令微调模型是目前在8B和70B参数尺度上存在的最好的模型（截止至发布日期）。</p> 
<p>Post-training的改进大大降低了错误拒绝率，改善了一致性，增加了模型响应的多样性；Llama 3在推理、代码生成和指令跟踪等功能上有极大的提升，具体看一下对比数据：</p> 
<p><em>(Llama 3 Pretrained模型)</em></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/39/cf/Rcns1tWe_o.png" width="1200"></p> 
<p> </p> 
<p><em>(Llama 3 Instruct模型)</em> </p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/88/4c/Kf4q0xOz_o.png" width="1200"></p> 
<p></p> 
<p><em>（这里再附一张 Llama 2 和 3 的对比）</em></p> 
<p><img alt="" height="812" src="https://images2.imgbox.com/d9/28/UeWvfrUv_o.png" width="960"></p> 
<p></p> 
<h3>2.2 人类偏好测试</h3> 
<p>在Llama 3的开发过程中，为了优化实际场景的性能，Meta开发了一个新的高质量的人类评价集。这个评估集包含1800个提示，涵盖了12个关键用例：征求建议、头脑风暴、分类、封闭式问题回答、编码、创造性写作、提取、作为一个角色/角色中、开放式问题回答、推理、重写和总结。</p> 
<p>下面的图表显示了模型对这些类别的人类评估的汇总结果：</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/cb/b6/RkJ6zJHR_o.png" width="1200"></p> 
<p></p> 
<p>根据人类评估者的偏好排名，Llama 的 70B 参数模型在实际应用场景中的表现，尤其是在指令跟随方面，相较于其他相当规模的模型表现出了显著的优势。</p> 
<p></p> 
<h2 style="margin-left:0px;"><span style="color:#1c2b33;"><span style="background-color:#ffffff;">三、优化之处</span></span></h2> 
<h3 style="margin-left:0px;">3.1 模型架构</h3> 
<h4 style="margin-left:0px;">3.1.1 Tokenzier</h4> 
<p style="margin-left:0px;">分词器：与Llama 2不同的是，Llama 3将tokenizer由sentencepiece换成tiktoken，词汇量从 的32K增加到 128K，增加了 4 倍 （更大的词汇库能够更高效地编码文本，增加编码效率，可以实现更好的下游性能。不过这也会导致嵌入层的输入和输出矩阵尺寸增大，模型参数量也会增大）。</p> 
<p style="margin-left:0;"><br> 序列长度：输入上下文长度从 4096（Llama 2）增加到 8192。但相对于GPT-4 的 128K来说还是相当小。</p> 
<p style="margin-left:0;"></p> 
<h4 style="margin-left:0px;">3.1.2 GQA</h4> 
<p style="margin-left:0;">Llama 3 中选择了相对标准的纯解码器decoder-only transformer架构，总体上与 Llama 2 相比没有重大变化。在 Llama 2 中只有34B &amp; 70B使用了分组查询注意 (GQA)，但为了提高模型的推理效率，Llama 3所有模型都采用了GQA。</p> 
<p style="margin-left:0;"></p> 
<h3 style="margin-left:0px;">3.2 训练数据</h3> 
<p><span style="color:#fe2c24;"> (Always the secret sauce!)</span></p> 
<p><img alt="" height="123" src="https://images2.imgbox.com/ff/c5/6bRIhFAJ_o.png" width="998"></p> 
<p> </p> 
<p>训练数据量：Llama 3 的预训练数据集增加至15T （比Llama 2大7倍，且其中包含的代码量是Llama 2的4倍， 这有助于Llama 3在代码能力以及逻辑推理能力的性能提升），这些数据都是从公开来源收集的高质量数据集。</p> 
<p>多语言数据：为了更好地适用于多语言使用场景，超过5%的Llama 3预训练数据集由覆盖30多种语言的高质量非英语数据组成（主要还是以英语为主，因此并不期望其他语言的表现能达到英语的水平）。</p> 
<p>数据过滤：为了确保Llama 3在最高质量的数据上进行训，Llama 3的开发团队开发了一系列数据过滤管道。这些管道包括使用启发式过滤器、NSFW过滤器、语义重复数据删除方法和文本分类器来预测数据质量。（前几代的Llama在识别高质量数据方面表现得非常出色，因此使用Llama 2来为支持Llama 3的文本质量分类器来生成训练数据）。</p> 
<p>数据类型组合：Llama 3的开发团队还进行了大量的实验，以评估在最终的预训练数据集中混合不同来源数据（包括琐事问题、STEM、编码、历史知识等）的最佳方法。</p> 
<p> </p> 
<h3 style="margin-left:0px;">3.3 预训练放大 </h3> 
<h4>3.3.1 为何研究缩放定律</h4> 
<p>为了更加有效地利用与训练数据，Llama 3的研究团队投入了大量的精力来扩大预训练，并为下游基准评估开发了一系列详细的缩放定律。这些缩放定律可以：</p> 
<p>（1）帮助选择最佳的数据组合，并就如何最好地规划训练计算提供决策支持。</p> 
<p>（2）允许在实际训练模型之前预测最大的模型在关键任务上的性能（例如，Llama 3 在HumanEval基准上评估的代码生成）； 这有助于确保最终模型在各种用例和功能中具有强大的性能。</p> 
<p></p> 
<h4>3.3.2 具体的缩放定律研究结果</h4> 
<p>对于像 8B 参数这样“小”的模型来说，Chinchilla扩展法则最优训练计算量对应于 ~200B Tokens，然鹅，在Llama 3研究团队对8B和70B参数模型进行了多达15T标记的训练后，模型性能仍以对数线性地趋势增强。</p> 
<p>从目前模型效果来看，Meta使用的Scaling Law法则是非常有效的，Meta得到了一个非常强大的模型，它非常小，易于使用和推理，而且mate表示，即使这样，该模型似乎也没有在标准意义上“收敛”，性能还能改善。这就意味着，一直以来我们使用的 LLM 训练是不足的，远远没有达到使模型收敛的那个点。</p> 
<p>较大的模型可以用更少的训练计算来匹配这些较小的模型的性能，但较小的模型通常更受欢迎，因为它们在推理过程中效率更高。</p> 
<p> </p> 
<p></p> 
<hr> 
<p><em>笔者NOTE：这个结论与Mixtral-7B所总结出来的预训练缩放规律（提升大语言模型的能力这个问题是三维的——模型能力、训练成本、推理成本）不谋而合</em></p> 
<p></p> 
<h3>3.4 并行训练</h3> 
<p>为了训练最大的Llama 3模型，开发团队结合了三种类型的并行化：数据并行化、模型并行化和管道并行化。</p> 
<p>使用了两种不同的GPU集群训练：</p> 
<p>（1）当在16K GPU上同时训练时，最有效的实现实现了每个GPU超过400 TFLOPS的计算利用率。</p> 
<p>（2）在两个定制的24K GPU集群上执行训练运行时，为了最大限度地延长GPU的正常运行时间，开发团队开发了一个先进的新训练堆栈，可以自动检测、处理和维护错误； 同时，开发团队极大地改进了硬件可靠性和无声数据损坏的检测机制，并开发了新的可扩展存储系统，减少了检查点和回滚的开销。</p> 
<p>这些改进导致整体有效训练时间超过95%。综合起来，这些改进使Llama 3的训练效率比Llama 2提高了约3倍。</p> 
<p></p> 
<h3>3.5 指令微调</h3> 
<p>为了在聊天用例中充分释放预训练模型的潜力，Meta对指令调整方法进行了创新。训练方法结合了监督微调 （SFT）、拒绝采样、近端策略优化 （PPO） 和直接策略优化 （DPO） 的组合。SFT中使用的提示词的质量以及PPO和DPO中使用的偏好排序对对齐模型的性能有巨大的影响。</p> 
<p>通过PPO和DPO学习偏好排名也大大提高了Llama 3在推理和编码任务上的表现。</p> 
<p></p> 
<h2 style="margin-left:0px;">四、Huggingface 集成</h2> 
<p>Llama 3版本引入了Meta基于Llama 2架构的4个新的开放LLM模型。它们有两种大小:8B和70B参数，每个参数都有基础(预训练)和指令调整版本。所有变体都可以在各种类型的消费者硬件上运行，并且上下文长度为8K令牌：</p> 
<ul><li><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B" rel="nofollow" title="Meta-Llama-3-8b">Meta-Llama-3-8b</a>: Base 8B model</li><li><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" rel="nofollow" title="Meta-Llama-3-8b-instruct">Meta-Llama-3-8b-instruct</a>: Instruct fine-tuned version of the base 8b model</li><li><a href="https://huggingface.co/meta-llama/Meta-Llama-3-70B" rel="nofollow" title="Meta-Llama-3-70b">Meta-Llama-3-70b</a>: Base 70B model</li><li><a href="https://huggingface.co/meta-llama/Meta-Llama-3-70B-instruct" rel="nofollow" title="Meta-Llama-3-70b-instruct">Meta-Llama-3-70b-instruct</a>: Instruct fine-tuned version of the base 70b model</li></ul> 
<p>Huggingface 现已经将Llama 3集成至Transformers库中，并且可以自动量化模型；</p> 
<p>同时推出了与Huggingface推理端点的集成、 Google Cloud 的集成、与 Amazon SageMaker 的集成等，都有助于更快更高效的进行推理体验；</p> 
<p>Huggingface还推出了使用 🤗 TRL 进行微调的方式，具体请参考以下两个链接：</p> 
<p style="margin-left:0;"><a class="has-card" href="https://huggingface.co/blog/llama3" rel="nofollow" title="参考：https://huggingface.co/blog/llama3"><span class="link-card-box"><span class="link-title">参考：https://huggingface.co/blog/llama3</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/a5/66/drBEK4wi_o.png" alt="icon-default.png?t=N7T8">https://huggingface.co/blog/llama3</span></span></a><a class="has-card" href="https://www.datacamp.com/tutorial/llama3-fine-tuning-locally" rel="nofollow" title="Fine-Tuning Llama 3 and Using It Locally: A Step-by-Step Guide | DataCamp"><span class="link-card-box"><span class="link-title">Fine-Tuning Llama 3 and Using It Locally: A Step-by-Step Guide | DataCamp</span><span class="link-desc">In this tutorial, we'll fine-tune Llama 3 on a dataset of patient-doctor conversations. After merging, converting, and quantizing the model, it will be ready for private local use via the Jan application.</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/c0/53/hjntDa6D_o.png" alt="icon-default.png?t=N7T8">https://www.datacamp.com/tutorial/llama3-fine-tuning-locally</span></span></a></p> 
<h2 style="margin-left:0px;">五、使用体验</h2> 
<h3 style="margin-left:0px;">5.1 在哪儿能用</h3> 
<p>Huggingface chat: <a class="has-card" href="https://huggingface.co/chat/models/meta-llama/Meta-Llama-3-70B-instruct" rel="nofollow" title="https://huggingface.co/chat/models/meta-llama/Meta-Llama-3-70B-instruct"><span class="link-card-box"><span class="link-title">https://huggingface.co/chat/models/meta-llama/Meta-Llama-3-70B-instruct</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/ed/34/WcgcZrNj_o.png" alt="icon-default.png?t=N7T8">https://huggingface.co/chat/models/meta-llama/Meta-Llama-3-70B-instruct</span></span></a></p> 
<p>Nvidia Development: <a class="has-card" href="https://build.nvidia.com/explore/discover" rel="nofollow" title="Try NVIDIA NIM APIs"><span class="link-card-box"><span class="link-title">Try NVIDIA NIM APIs</span><span class="link-desc">Experience the leading models to build enterprise generative AI apps now.</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/1a/68/Ahypxjac_o.png" alt="icon-default.png?t=N7T8">https://build.nvidia.com/explore/discover</span></span></a></p> 
<p>Repilcate: <a class="has-card" href="https://replicate.com/meta/meta-llama-3-70b-instruct" rel="nofollow" title="meta/meta-llama-3-70b-instruct – Run with an API on Replicate"><span class="link-card-box"><span class="link-title">meta/meta-llama-3-70b-instruct – Run with an API on Replicate</span><span class="link-desc">A 70 billion parameter language model from Meta, fine tuned for chat completions</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/29/ba/vTQVUKUv_o.png" alt="icon-default.png?t=N7T8">https://replicate.com/meta/meta-llama-3-70b-instruct</span></span></a></p> 
<p>Meta Official: <a class="has-card" href="https://www.meta.ai/?utm_source=llama_meta_site&amp;utm_medium=web&amp;utm_content=Llama_nav&amp;utm_campaign=April_moment" rel="nofollow" title="https://www.meta.ai/?utm_source=llama_meta_site&amp;utm_medium=web&amp;utm_content=Llama_nav&amp;utm_campaign=April_moment"><span class="link-card-box"><span class="link-title">https://www.meta.ai/?utm_source=llama_meta_site&amp;utm_medium=web&amp;utm_content=Llama_nav&amp;utm_campaign=April_moment</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/2b/8e/2pzr9w3k_o.png" alt="icon-default.png?t=N7T8">https://www.meta.ai/?utm_source=llama_meta_site&amp;utm_medium=web&amp;utm_content=Llama_nav&amp;utm_campaign=April_moment</span></span></a></p> 
<p></p> 
<h3 style="margin-left:0px;">5.3 部署流程</h3> 
<p style="margin-left:0;">除了在线使用，Llama 3 - 8B也可以直接部署至消费级GPU，非量化版本占用显存约15G，16FP量化版本占用显存不到10GB。</p> 
<p style="margin-left:0;"></p> 
<p>要想免费使用Llama 3 并部署至本地，常见的有如下3种方式：</p> 
<p>1. 克隆Github仓库后，   安装python，pip相关的包，官网在线填写个人信息申请模型下载链接</p> 
<p><a class="has-card" href="https://github.com/meta-llama/llama3" title="GitHub - meta-llama/llama3: The official Meta Llama 3 GitHub site"><span class="link-card-box"><span class="link-title">GitHub - meta-llama/llama3: The official Meta Llama 3 GitHub site</span><span class="link-desc">The official Meta Llama 3 GitHub site. Contribute to meta-llama/llama3 development by creating an account on GitHub.</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/21/47/Zm2Ji7x1_o.png" alt="icon-default.png?t=N7T8">https://github.com/meta-llama/llama3</span></span></a></p> 
<p>2. LM-studio</p> 
<p><a class="has-card" href="https://lmstudio.ai/blog/llama-3" rel="nofollow" title="Use Llama 3 in LM Studio | LM Studio"><span class="link-card-box"><span class="link-title">Use Llama 3 in LM Studio | LM Studio</span><span class="link-desc">Llama 3 by MetaAI</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/c0/bd/7h1O26g6_o.png" alt="icon-default.png?t=N7T8">https://lmstudio.ai/blog/llama-3</span></span></a></p> 
<p>3. Ollama</p> 
<p><a class="has-card" href="https://ollama.com/library/llama3" rel="nofollow" title="llama3"><span class="link-card-box"><span class="link-title">llama3</span><span class="link-desc">Get up and running with large language models.</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/b7/27/v1iIdbch_o.png" alt="icon-default.png?t=N7T8">https://ollama.com/library/llama3</span></span></a></p> 
<p>其中以ollama部署最为便捷和友好，部署时间可在5分钟内完成。</p> 
<p></p> 
<p>除此之外，Meta还发布了包括使用单/多节点gpu进行推理、微调的方法，以及一些实际的应用场景：</p> 
<p><img alt="" height="694" src="https://images2.imgbox.com/cf/9c/i1WeeAXd_o.png" width="1149"></p> 
<p> </p> 
<p>具体参考以下链接：</p> 
<p><a class="has-card" href="https://github.com/meta-llama/llama-recipes" title="GitHub - meta-llama/llama-recipes: Scripts for fine-tuning Meta Llama3 with composable FSDP &amp; PEFT methods to cover single/multi-node GPUs. Supports default &amp; custom datasets for applications such as summarization and Q&amp;A. Supporting a number of candid inference solutions such as HF TGI, VLLM for local or cloud deployment. Demo apps to showcase Meta Llama3 for WhatsApp &amp; Messenger."><span class="link-card-box"><span class="link-title">GitHub - meta-llama/llama-recipes: Scripts for fine-tuning Meta Llama3 with composable FSDP &amp; PEFT methods to cover single/multi-node GPUs. Supports default &amp; custom datasets for applications such as summarization and Q&amp;A. Supporting a number of candid inference solutions such as HF TGI, VLLM for local or cloud deployment. Demo apps to showcase Meta Llama3 for WhatsApp &amp; Messenger.</span><span class="link-desc">Scripts for fine-tuning Meta Llama3 with composable FSDP &amp; PEFT methods to cover single/multi-node GPUs. Supports default &amp; custom datasets for applications such as summarization and Q&amp;A. Supporting a number of candid inference solutions such as HF TGI, VLLM for local or cloud deployment. Demo apps to showcase Meta Llama3 for WhatsApp &amp; Messenger. - meta-llama/llama-recipes</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/ca/55/NA8Wnce1_o.png" alt="icon-default.png?t=N7T8">https://github.com/meta-llama/llama-recipes</span></span></a></p> 
<p style="margin-left:0;"></p> 
<hr> 
<p>Llama 3 8B和70B型号标志Meta计划发布Llama 3的开始，未来还会有更多（最大的Llama 3模型有超过400B个参数，虽然这些模型仍在训练中）。</p> 
<p>在接下来的几个月里，Meta将发布多个具有新功能的模型，包括多模态、用多种语言交谈的能力、更长的上下文窗口和更强大的整体功能。</p> 
<p></p> 
<p style="margin-left:0;">参考资料：</p> 
<p style="margin-left:0;"><a href="https://ai.meta.com/blog/meta-llama-3/" rel="nofollow" title="https://ai.meta.com/blog/meta-llama-3/">https://ai.meta.com/blog/meta-llama-3/</a></p> 
<p style="margin-left:0;"><a href="https://www.51cto.com/article/787502.html" rel="nofollow" title="https://www.51cto.com/article/787502.html">https://www.51cto.com/article/787502.html</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0075ebed4ce4c016c8370afbc2156b4f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">java本地缓存(map,Guava,echcache,caffeine)优缺点，以及适用场景</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/52e4e11ca2d61d07bb4a05695daa0cdd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Pytorch深度解析：Transformer嵌入层源码逐行解读</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>