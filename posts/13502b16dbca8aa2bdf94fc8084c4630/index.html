<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>图像生成发展起源：从VAE、VQ-VAE、扩散模型DDPM、DETR到ViT、Swin transformer - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/13502b16dbca8aa2bdf94fc8084c4630/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="图像生成发展起源：从VAE、VQ-VAE、扩散模型DDPM、DETR到ViT、Swin transformer">
  <meta property="og:description" content="前言 2018年我写过一篇博客，叫：《一文读懂目标检测：R-CNN、Fast R-CNN、Faster R-CNN、YOLO、SSD》，该文相当于梳理了2019年之前CV领域的典型视觉模型，比如
2014 R-CNN2015 Fast R-CNN、Faster R-CNN2016 YOLO、SSD2017 Mask R-CNN、YOLOv22018 YOLOv3 随着2019 CenterNet的发布，特别是2020发布的DETR(End-to-End Object Detection with Transformers)之后，自此CV迎来了生成式下的多模态时代(本文介绍其中有下划线的部分，其他部分下一篇介绍)
1月3月4月5月6月8月9月10月11月2020DETRDDPM DDIM
VisionTransf.. 2021 CLIP
DALL·E
SwinTransf.. MAE
SwinTransf..V2
2022BLIPDALL·E 2 StableDiffusion BEiT-3
Midjourney V3
2023BLIP2 VisualChatGPT GPT4
Midjourney V5
SAM(Segment Anything Model)DALLE3 但看这些模型接二连三的横空出世，都不用说最后爆火的GPT4，便可知不少CV同学被卷的不行
说到GPT4，便不得不提ChatGPT，实在是太火了，改变了很多行业，使得国内外绝大部分公司的产品、服务都值得用LLM全部升级一遍(比如微软的365 Copilot、阿里所有产品、金山WPS等等)
而GPT4相比GPT3.5或GPT3最本质的改进就是增加了多模态的能力，使得ChatGPT很快就能支持图片的输入形式，从而达到图生文和文生图的效果，而AI绘画随着去年stable diffusion和Midjourney的推出，使得文生图火爆异常，各种游戏的角色设计、网上店铺的商品/页面设计都用上了AI绘画这样的工具，更有不少朋友利用AI绘画取得了不少的创收，省时省力还能赚钱，真香
但面对这么香的技术，其背后的一系列原理到底是什么呢，本文特从头开始，不只是简单的讲一下扩散模型的原理，而是在反复研读相关论文之后，准备把20年起相关的CV多模态模型全部梳理一遍，从VE、VAE、DDPM到ViT/Swin transformer、CLIP/BLIP，再到stable diffusion/Midjourney、GPT4，当然，实际写的时候，会分成两篇甚至多篇文章，比如
第一篇，即本文《AI绘画能力的起源：从VAE、扩散模型DDPM、DETR到ViT/MAE/Swin transformer》第二篇，即下篇《CV多模态和AIGC的原理解析：从CLIP、BLIP到Stable Diffusion、Midjourney》 就当2020年之后的CV视觉发展史了，且过程中会尽可能写透彻每一个模型的原理，举两个例子
网上介绍VAE的文章都太数学化(更怕那种表面正确其实关键的公式是错的误导人)，如果更边推导边分析背后的理论意义(怎么来的 出发点是什么 为什么要这么做 这么做的意义是什么)，则会更好理解，这就跟变介绍原理边coding实现 会更好理解、理解更深 一个道理如果完全展开DDPM推导的所有细节，假定需要100步的话，本文正在朝展开80步而努力，截止5月份之前，绝大部分的中文资料只展开了60步(正在因为只展开了60%，让很多初学者卡到中途)，所以你害怕的不是公式，你只是怕公式的展开不够细致，毕竟对每一个人而言，公式展开越细致 越不怕
(如果本文有任何一个公式展开的不够细致、不够一目了然，请随时指出，一定及时二次展开) 第一部分 编码器VE与变分自编码器VAE 1.1 AE：编码器(数据压缩为低维表示)-解码器(低维表示恢复为原始数据)架构 自编码器(Autoencoder，简称AE)是一种无监督学习的神经网络，用于学习输入数据的压缩表示。具体而言，可以将其分为两个部分：编码器和解码器
编码器：编码器是一个神经网络，负责将输入数据（如图像、文本等）压缩为一个低维表示，且表示为
解码器：解码器是另一个神经网络，负责将编码器生成的低维表示恢复为原始数据，且表示为">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-10T12:06:09+08:00">
    <meta property="article:modified_time" content="2024-06-10T12:06:09+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">图像生成发展起源：从VAE、VQ-VAE、扩散模型DDPM、DETR到ViT、Swin transformer</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>前言</h2> 
<p>2018年我写过一篇博客，叫：《<a class="link-info" href="https://blog.csdn.net/v_july_v/article/details/80170182" title="一文读懂目标检测：R-CNN、Fast R-CNN、Faster R-CNN、YOLO、SSD">一文读懂目标检测：R-CNN、Fast R-CNN、Faster R-CNN、YOLO、SSD</a>》，该文相当于梳理了2019年之前CV领域的典型视觉模型，比如</p> 
<ul><li>2014 R-CNN</li><li>2015 Fast R-CNN、Faster R-CNN</li><li>2016 YOLO、SSD</li><li>2017 Mask R-CNN、YOLOv2</li><li>2018 YOLOv3</li></ul> 
<p>随着2019 CenterNet的发布，特别是2020发布的DETR(End-to-End Object Detection with Transformers)之后，自此CV迎来了生成式下的多模态时代(本文介绍其中有下划线的部分，其他部分下一篇介绍)</p> 
<table border="1" cellpadding="1" cellspacing="1"><tbody><tr><td></td><td style="width:58px;"><strong>1月</strong></td><td style="width:135px;"><strong>3月</strong></td><td style="width:95px;"><strong>4月</strong></td><td><strong>5月</strong></td><td><strong>6月</strong></td><td style="width:123px;"><strong>8月</strong></td><td style="width:123px;"><strong>9月</strong></td><td style="width:128px;"><strong>10月</strong></td><td style="width:145px;"><strong>11月</strong></td></tr><tr><td><strong>2020</strong></td><td style="width:58px;"></td><td style="width:135px;"></td><td style="width:95px;"></td><td><u>DETR</u></td><td><u>DDPM</u></td><td style="width:123px;"></td><td style="width:123px;"></td><td style="width:128px;"> <p><u>DDIM</u></p> <p><u>VisionTransf..</u> </p> </td><td style="width:145px;"></td></tr><tr><td><strong>2021</strong></td><td style="width:58px;"> <p>CLIP</p> <p>DALL·E</p> </td><td style="width:135px;"><u>SwinTransf..</u></td><td style="width:95px;"></td><td></td><td></td><td style="width:123px;"></td><td style="width:123px;"></td><td style="width:128px;"></td><td style="width:145px;"> <p><u>MAE</u></p> <p><u>SwinTransf..V2</u></p> </td></tr><tr><td><strong>2022</strong></td><td style="width:58px;">BLIP</td><td style="width:135px;"></td><td style="width:95px;">DALL·E 2</td><td></td><td></td><td style="width:123px;"> <p>StableDiffusion </p> <p>BEiT-3</p> <p>Midjourney V3</p> </td><td style="width:123px;"></td><td style="width:128px;"></td><td style="width:145px;"></td></tr><tr><td><strong>2023</strong></td><td style="width:58px;">BLIP2</td><td style="width:135px;"> <p>VisualChatGPT </p> <p>GPT4</p> <p>Midjourney V5</p> </td><td style="width:95px;">SAM(Segment Anything Model)</td><td></td><td></td><td style="width:123px;"></td><td style="width:123px;">DALLE3</td><td style="width:128px;"></td><td style="width:145px;"></td></tr></tbody></table> 
<p>但看这些模型接二连三的横空出世，都不用说最后爆火的GPT4，便可知不少CV同学被卷的不行</p> 
<p>说到GPT4，便不得不提ChatGPT，实在是太火了，改变了很多行业，使得国内外绝大部分公司的产品、服务都值得用LLM全部升级一遍(比如微软的365 Copilot、阿里所有产品、金山WPS等等)</p> 
<p>而GPT4相比GPT3.5或GPT3最本质的改进就是增加了多模态的能力，使得ChatGPT很快就能支持图片的输入形式，从而达到图生文和文生图的效果，而AI绘画随着去年stable diffusion和Midjourney的推出，使得文生图火爆异常，各种游戏的角色设计、网上店铺的商品/页面设计都用上了AI绘画这样的工具，更有不少朋友利用AI绘画取得了不少的创收，省时省力还能赚钱，真香</p> 
<p>但面对这么香的技术，其背后的一系列原理到底是什么呢，本文特从头开始，不只是简单的讲一下扩散模型的原理，而是在反复研读相关论文之后，准备把20年起相关的CV多模态模型全部梳理一遍，从VE、VAE、DDPM到ViT/Swin transformer、CLIP/BLIP，再到stable diffusion/Midjourney、GPT4，当然，实际写的时候，会分成两篇甚至多篇文章，比如</p> 
<ul><li>第一篇，即本文《AI绘画能力的起源：从VAE、扩散模型DDPM、DETR到ViT/MAE/Swin transformer》</li><li>第二篇，即下篇《<a class="link-info" href="https://blog.csdn.net/v_JULY_v/article/details/131205615" title="CV多模态和AIGC的原理解析：从CLIP、BLIP到Stable Diffusion、Midjourney">CV多模态和AIGC的原理解析：从CLIP、BLIP到Stable Diffusion、Midjourney</a>》</li></ul> 
<p>就当2020年之后的CV视觉发展史了，且过程中会尽可能写透彻每一个模型的原理，举两个例子</p> 
<ol><li>网上介绍VAE的文章都太数学化(更怕那种表面正确其实关键的公式是错的误导人)，如果更边推导边分析背后的理论意义(怎么来的 出发点是什么 为什么要这么做 这么做的意义是什么)，则会更好理解，这就跟变介绍原理边coding实现 会更好理解、理解更深 一个道理</li><li>如果完全展开DDPM推导的所有细节，假定需要100步的话，本文正在朝展开80步而努力，截止5月份之前，绝大部分的中文资料只展开了60步(正在因为只展开了60%，让很多初学者卡到中途)，所以<strong>你害怕的不是公式，你只是怕公式的展开不够细致，毕竟对每一个人而言，公式展开越细致 越不怕</strong><br> (如果本文有任何一个公式展开的不够细致、不够一目了然，请随时指出，一定及时二次展开)</li></ol> 
<h2>第一部分 编码器VE与变分自编码器VAE</h2> 
<h3>1.1 AE：编码器(数据<img alt="X" class="mathcode" src="https://images2.imgbox.com/df/0a/9QedpQNA_o.png">压缩为低维表示<img alt="z" class="mathcode" src="https://images2.imgbox.com/6d/06/Kn4QHwzm_o.png">)-解码器(低维表示恢复为原始数据<img alt="\hat{X}" class="mathcode" src="https://images2.imgbox.com/69/22/tPloebu3_o.png">)架构</h3> 
<p>自编码器(Autoencoder，简称AE)是一种无监督学习的神经网络，用于学习输入数据的压缩表示。具体而言，可以将其分为两个部分：编码器和解码器</p> 
<p class="img-center"><img alt="" height="341" src="https://images2.imgbox.com/16/a6/ujrwVwcB_o.png" width="600"></p> 
<ul><li> <p>编码器：编码器是一个神经网络，负责将输入数据<img alt="X" class="mathcode" src="https://images2.imgbox.com/59/5a/UKc7YkZB_o.png">（如图像、文本等）压缩为一个低维表示<img alt="z" class="mathcode" src="https://images2.imgbox.com/4c/89/JJ3CoFjQ_o.png">，且表示为<img alt="z = g(X)" class="mathcode" src="https://images2.imgbox.com/e6/3f/recZT4tr_o.png"></p> </li><li> <p>解码器：解码器是另一个神经网络，负责将编码器生成的低维表示恢复为原始数据<img alt="\hat{X}" class="mathcode" src="https://images2.imgbox.com/a4/dd/Ycglq9Yo_o.png">，且表示为<img alt="\hat{X} = f(z)" class="mathcode" src="https://images2.imgbox.com/a9/46/7ipHbdCD_o.png"></p> </li></ul> 
<p>从而最终完成这么一个过程：<img alt="X \rightarrow z \rightarrow \hat{X}" class="mathcode" src="https://images2.imgbox.com/ae/5d/DKWP97hs_o.png">，而其训练目标即是最小化输入数据<img alt="X" class="mathcode" src="https://images2.imgbox.com/b1/ae/pn0NdJ14_o.png">与解码器重建数据<img alt="\hat{X}" class="mathcode" src="https://images2.imgbox.com/31/64/XELvKT2I_o.png">之间的差异，所以自编码器常用的一个损失函数为<img alt="l = || X - \hat{X} || ^2" class="mathcode" src="https://images2.imgbox.com/bc/11/qrrTJcsJ_o.png"></p> 
<p>这个自编码的意义在于</p> 
<ol><li>模型训练结束后，我们就可以认为编码<img alt="z" class="mathcode" src="https://images2.imgbox.com/d6/d5/hZs6c0wq_o.png">囊括了输入数据<img alt="X" class="mathcode" src="https://images2.imgbox.com/71/05/FSeAWsRm_o.png">的大部分信息，也因此我们可以直接利用<img alt="z" class="mathcode" src="https://images2.imgbox.com/34/cf/zMImbZHN_o.png">表达原始数据，从而达到数据降维的目的</li><li>解码器只需要输入某些低维向量<img alt="z" class="mathcode" src="https://images2.imgbox.com/bb/75/Lfanjjbr_o.png">，就能够输出高维的图片数据<img alt="\hat{X}" class="mathcode" src="https://images2.imgbox.com/0d/97/TwvPFrjD_o.png">，那我们能否<span style="color:#ed7976;">把解码器模型直接当做生成模型</span>，在低维空间中随机生成某些向量<img alt="z" class="mathcode" src="https://images2.imgbox.com/ad/11/1nmwUllP_o.png">，再喂给解码器<img alt="f(z)" class="mathcode" src="https://images2.imgbox.com/a2/81/7dtI7UKf_o.png">来生成图片呢？</li></ol> 
<p>对于第二点，理论上可以这么做，但问题在于</p> 
<ol><li>绝大多数随机生成的<img alt="z,f(z)" class="mathcode" src="https://images2.imgbox.com/a0/2b/gI8IBlQa_o.png">只会生成一些没有意义的噪声，之所以如此，原因在于没有显性的对<img alt="z" class="mathcode" src="https://images2.imgbox.com/bd/3d/PpyOG4le_o.png">的分布<img alt="p(z)" class="mathcode" src="https://images2.imgbox.com/fa/f4/nPD902CL_o.png">进行建模，我们并不知道哪些<img alt="z" class="mathcode" src="https://images2.imgbox.com/40/26/UD92jDDr_o.png">能够生成有用的图片</li><li>而且我们用来训练<img alt="f(z)" class="mathcode" src="https://images2.imgbox.com/f0/f3/gIWPR5UG_o.png">的数据是有限的，<img alt="f" class="mathcode" src="https://images2.imgbox.com/4b/99/BBZOzK7d_o.png">可能只会对极有限的<img alt="z" class="mathcode" src="https://images2.imgbox.com/95/6c/yEVDkC2u_o.png">有响应。而整个低维空间又是一个比较大的空间，如果只在这个空间上随机采样的话，我们自然不能指望总能恰好采样到能够生成有用的图片的<img alt="z" class="mathcode" src="https://images2.imgbox.com/7e/b7/GlXw4Ami_o.png"></li></ol> 
<p>有问题自然便得探索对应的解决方案，而VAE(自变分编码器，Variational Autoencoders)则是在AE的基础上，<strong>显性的对<img alt="z" class="mathcode" src="https://images2.imgbox.com/e4/15/pMPcISL9_o.png">的分布<img alt="p(z)" class="mathcode" src="https://images2.imgbox.com/55/e1/7HHmp1XB_o.png">进行建模</strong>(比如符合某种常见的概率分布)<strong>，使得自编码器成为一个合格的生成模型</strong></p> 
<h3 id="h_563543020_0">1.2 Variational AutoEncoder (VAE)：学习一个分布</h3> 
<h4>1.2.1 VAE：标数据的分布<img alt="\hat{X}" class="mathcode" src="https://images2.imgbox.com/62/fa/txHq0t90_o.png">和目标分布<img alt="X" class="mathcode" src="https://images2.imgbox.com/4e/ed/ih4SViqs_o.png">尽量接近</h4> 
<p>VAE和GAN一样，都是从隐变量<img alt="Z" class="mathcode" src="https://images2.imgbox.com/9d/0a/HRZokfUf_o.png">生成目标数据， 具体如下图所示(本1.2节的部分图来自<a class="link-info" href="https://kexue.fm/archives/5253" rel="nofollow" title="苏剑林">苏剑林</a>)：</p> 
<p class="img-center"><img alt="" height="296" src="https://images2.imgbox.com/ce/31/9fs1qBTf_o.png" width="600"></p> 
<ol><li>先用某种分布随机生成一组隐变量<img alt="Z = \left \{ Z_1,Z_2,\cdots ,Z_k \right \}" class="mathcode" src="https://images2.imgbox.com/f3/ac/SWbtHIlF_o.png">(<span style="color:#7b7f82;">假设隐变量服从正态分布</span>)</li><li>然后这个<img alt="Z" class="mathcode" src="https://images2.imgbox.com/d6/c7/0mw5JCGQ_o.png">隐变量经过一个生成器生成一组数据<img alt="\hat{X} = \left \{ \hat{X_1},\hat{X_2},\cdots ,\hat{X_k} \right \}" class="mathcode" src="https://images2.imgbox.com/f8/5f/6lN498y9_o.png"></li></ol> 
<p>而VAE和GAN都希望这组生成数据的分布<img alt="\hat{X}" class="mathcode" src="https://images2.imgbox.com/74/6d/VvOsgvVe_o.png">和目标分布<img alt="X" class="mathcode" src="https://images2.imgbox.com/92/95/npyTYu1p_o.png">尽量接近，看似美好，但有两个问题</p> 
<ul><li>一方面，“尽量接近”并没有一个确定的关于<img alt="\hat{X}" class="mathcode" src="https://images2.imgbox.com/4b/ee/eaLIDTu6_o.png"> 和 <img alt="X" class="mathcode" src="https://images2.imgbox.com/37/13/U9FUxDBV_o.png">的相似度的评判标准，比如KL散度便不行，原因在于KL散度是针对两个已知的概率分布求相似度的，而 <img alt="\hat{X}" class="mathcode" src="https://images2.imgbox.com/45/d7/NkHfR1aH_o.png">和 <img alt="X" class="mathcode" src="https://images2.imgbox.com/30/2d/SlgRCmbg_o.png"> 的概率分布目前都是未知(只有一批采样数据 没有分布表达式)</li><li>二方面，经过采样出来的每一个<img alt="Z_k" class="mathcode" src="https://images2.imgbox.com/31/d3/efINSyla_o.png">，不一定对应着每一个原来的<img alt="X_k" class="mathcode" src="https://images2.imgbox.com/c0/69/VIGr9JOY_o.png">，故最后没法直接最小化<img alt="D^2(X_k,\hat{X_k})" class="mathcode" src="https://images2.imgbox.com/14/35/Jm7M9IwD_o.png"></li></ul> 
<p>实际是怎么做的呢？如苏剑林所说，与自动编码器由编码器与解码器两部分构成相似，VAE利用两个神经网络建立两个概率密度分布模型：</p> 
<p class="img-center"><img alt="" height="297" src="https://images2.imgbox.com/b9/a7/FRyNuHLI_o.png" width="600"></p> 
<ul><li><span style="color:#ed7976;">其中一个模型用于原始输入数据<img alt="X=\{X_1,...,X_k\}" class="mathcode" src="https://images2.imgbox.com/f7/12/FVkrNEBw_o.png">的变分推断，生成隐变量<img alt="Z" class="mathcode" src="https://images2.imgbox.com/0a/79/LgzCz16f_o.png">的变分概率分布<img alt="p(Z|X_k)" class="mathcode" src="https://images2.imgbox.com/e5/c0/q6QQOjmi_o.png">，称为推断网络</span><br><strong>而VAE的核心就是，我们不仅假设 </strong><img alt="p(Z)" class="mathcode" src="https://images2.imgbox.com/e2/bf/hI0ibK13_o.png"><strong>是正态分布，而且假设每个</strong><img alt="p(Z|X_k)" class="mathcode" src="https://images2.imgbox.com/ef/21/eIZ7udcS_o.png"><strong> 也是正态分布。</strong>什么意思呢？即针对每个采样点<img alt="X_k" class="mathcode" src="https://images2.imgbox.com/df/80/YpRXC1z5_o.png">获得一个专属于它和 <img alt="Z" class="mathcode" src="https://images2.imgbox.com/54/dd/RiDvt855_o.png"> 的一个正态分布<img alt="p(Z|X_k)" class="mathcode" src="https://images2.imgbox.com/a1/79/BteZIaBB_o.png"><br><br> 换言之，有 <img alt="k" class="mathcode" src="https://images2.imgbox.com/31/3e/BbFsBvnP_o.png"> 个 <img alt="X" class="mathcode" src="https://images2.imgbox.com/56/2a/0lINzEIX_o.png"> sample，就有<img alt="k" class="mathcode" src="https://images2.imgbox.com/ef/4f/8GsMCUQu_o.png">个正态分布 <img alt="p(Z|X_k)" class="mathcode" src="https://images2.imgbox.com/e7/cb/r81foRiT_o.png">，毕竟没有任何两个采样点是完全一致的，而后面要训练一个生成器 <img alt="\hat{X_k}=f(Z)" class="mathcode" src="https://images2.imgbox.com/61/e5/ZgVn7lvT_o.png">，希望能够把从分布 <img alt="p(Z|X_k)" class="mathcode" src="https://images2.imgbox.com/ae/aa/pTbgOHu7_o.png">采样出来的一个 <img alt="Z_k" class="mathcode" src="https://images2.imgbox.com/76/cb/vcYL9QCV_o.png"> 还原为 <img alt="X_k" class="mathcode" src="https://images2.imgbox.com/67/8d/56ig8SS9_o.png"><br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/43/06/MTJ9khlG_o.png">  而如果<span style="color:#4d4d4d;">从 </span><img alt="p(Z)" class="mathcode" src="https://images2.imgbox.com/32/d2/uQfS3RIn_o.png"><span style="color:#4d4d4d;">中采样一个 </span><img alt="Z_k" class="mathcode" src="https://images2.imgbox.com/65/5a/qodE6GeS_o.png"><span style="color:#4d4d4d;">，没法知道这个 </span><img alt="Z_k" class="mathcode" src="https://images2.imgbox.com/9a/3a/6c0yf9d2_o.png"><span style="color:#4d4d4d;"> 对应于真实的 </span><img alt="X_k" class="mathcode" src="https://images2.imgbox.com/e8/d0/KRG9tJ4J_o.png"><span style="color:#4d4d4d;"> 呢？<br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/14/d7/f41aoXOm_o.png">  现在 </span><img alt="p(Z|X_k)" class="mathcode" src="https://images2.imgbox.com/44/64/ZVh74JVy_o.png"><span style="color:#4d4d4d;"> 专属于</span><img alt="X_k" class="mathcode" src="https://images2.imgbox.com/38/fc/x6WgdD2X_o.png"><span style="color:#4d4d4d;">，我们有理由说从这个分布采样出来的 </span><img alt="Z_k" class="mathcode" src="https://images2.imgbox.com/07/50/Ta0cfqEY_o.png"><span style="color:#4d4d4d;"> 可以还原到对应的</span><img alt="X_k" class="mathcode" src="https://images2.imgbox.com/6a/2d/0grbbnEV_o.png"><span style="color:#4d4d4d;"> 中去</span> <p class="img-center"><img alt="" height="312" src="https://images2.imgbox.com/03/40/pezOsPR4_o.png" width="600"></p> 而如何确定这 <img alt="k" class="mathcode" src="https://images2.imgbox.com/51/f5/MmZvdN4r_o.png"><span style="color:#4d4d4d;"> 个正态分布呢，众所周知，确定一个正太分布只需确定其均值<img alt="u" class="mathcode" src="https://images2.imgbox.com/be/b9/EZMsdWTB_o.png">和方差<img alt="\sigma ^2" class="mathcode" src="https://images2.imgbox.com/4f/51/n5owXeTy_o.png"> 即可，故可通过已知的</span><img alt="X_k" class="mathcode" src="https://images2.imgbox.com/b0/5c/Kf5x58nC_o.png"><span style="color:#4d4d4d;"> 和 假设</span>的<img alt="Z" class="mathcode" src="https://images2.imgbox.com/c5/68/mAAD0dzR_o.png"><span style="color:#4d4d4d;"> </span>去确定均值和方差，具体可以构建两个神经网络<img alt="\mu _k = f_1(X_k)" class="mathcode" src="https://images2.imgbox.com/20/92/ZVaaCTQZ_o.png">，<img alt="log \sigma _{k}^{2} = f_2(X_k)" class="mathcode" src="https://images2.imgbox.com/ff/8e/pUKKBAqL_o.png">去计算「<em><span style="color:#7b7f82;">值得一提的是，选择拟合<img alt="log \sigma _{k}^{2}" class="mathcode" src="https://images2.imgbox.com/e9/b1/92KGqmd1_o.png">而不是直接拟合<img alt="\sigma _{k}^{2}" class="mathcode" src="https://images2.imgbox.com/d2/51/TY6Uj0x1_o.png">，是因为<img alt="\sigma _{k}^{2}" class="mathcode" src="https://images2.imgbox.com/51/b6/Il7PSQzb_o.png">总是非负的，需要加激活函数处理，而拟合<img alt="log \sigma _{k}^{2}" class="mathcode" src="https://images2.imgbox.com/a6/a4/yUzLEsrG_o.png">不需要加激活函数，因为它可正可负</span></em>」</li><li><span style="color:#ed7976;">另一个模型，则根据生成的隐变量<img alt="Z" class="mathcode" src="https://images2.imgbox.com/ab/c0/UTKx9dk8_o.png">的变分概率分布<img alt="p(Z)" class="mathcode" src="https://images2.imgbox.com/84/7f/RpYLvYaR_o.png">，还原生成原始数据的近似概率分布<img alt="p(\hat{X}|Z)" class="mathcode" src="https://images2.imgbox.com/ce/69/LMOiJzkE_o.png">，称为生成网络 </span><br> 因为已经学到了这 <img alt="k" class="mathcode" src="https://images2.imgbox.com/cb/a0/0JGTEwnQ_o.png"> 个正态分布，那可以直接从专属分布<img alt="p(Z|X_k)" class="mathcode" src="https://images2.imgbox.com/d5/af/7JFLw8vh_o.png">中采样一个<img alt="Z_k" class="mathcode" src="https://images2.imgbox.com/81/bb/spdA0qIf_o.png">出来，然后经过一个生成器得到<img alt="\hat{X_k} = f(Z_k)" class="mathcode" src="https://images2.imgbox.com/2c/bc/mAqX3aPt_o.png">，那接下来只需要最小化方差 <img alt="D^2(X_k,\hat{X_k})" class="mathcode" src="https://images2.imgbox.com/1e/f1/OhU14QQl_o.png"> 就行 <p class="img-center"><img alt="" height="407" src="https://images2.imgbox.com/9d/13/CWDLlejd_o.png" width="600"></p> </li></ul> 
<p>仔细理解的时候有没有发现一个问题？为什么在文章最开头，我们强调了没法直接比较 <img alt="X" class="mathcode" src="https://images2.imgbox.com/72/2c/6l3BF5gQ_o.png"> 与 <img alt="\hat{X}" class="mathcode" src="https://images2.imgbox.com/19/a1/74NvW4kg_o.png"> 的分布，而在这里，我们认为可以直接比较这俩？注意，这里的 <img alt="Z_k" class="mathcode" src="https://images2.imgbox.com/db/bf/U04ehnol_o.png"> 是专属于或针对于<img alt="X_k" class="mathcode" src="https://images2.imgbox.com/c4/e4/nIpDu6yS_o.png">的隐变量，那么和 <img alt="\hat{X_k}" class="mathcode" src="https://images2.imgbox.com/22/0e/RNYVJHim_o.png">本身就有对应关系，因此右边的蓝色方框内的“生成器”，是一一对应的生成。</p> 
<p>另外，大家可以看到，均值和方差的计算本质上都是encoder。也就是说，VAE其实利用了两个encoder去分别学习均值和方差</p> 
<h4 id="h_563543020_3">1.2.2 VAE的Variational到底是个啥：需要方差持续存在从而带来噪声</h4> 
<p>这里还有一个<strong>非常重要</strong>的问题，如苏剑林所说，由于我们通过最小化<img alt="D^2(X_k,\hat{X_k})" class="mathcode" src="https://images2.imgbox.com/ba/8d/Xbr6Acf3_o.png">来训练右边的生成器，最终模型会逐渐使得 <strong><img alt="X_k" class="mathcode" src="https://images2.imgbox.com/fb/f7/X4KS1bOw_o.png"></strong> 和<img alt="\hat{X_k}" class="mathcode" src="https://images2.imgbox.com/7b/65/sGxlQq38_o.png">趋于一致。但是注意，因为 <img alt="Z_k" class="mathcode" src="https://images2.imgbox.com/8c/4a/V1Xtr16T_o.png">是重新随机采样过的，而不是直接通过均值和方差encoder学出来的，这个生成器的输入 <img alt="Z" class="mathcode" src="https://images2.imgbox.com/4e/41/Nqn9b7r5_o.png">是有噪声的</p> 
<ol><li>仔细思考一下，这个噪声的大小其实就用方差来度量。为了使得分布的学习尽量接近，我们希望噪声越小越好，所以我们会尽量使得方差趋于 0</li><li>但是方差不能为 0，因为我们还想要给模型一些训练难度。如果方差为 0，模型永远只需要学习高斯分布的均值，这样就丢失了随机性，VAE就变成AE了……这就是为什么VAE要在AE前面加一个Variational：我们希望方差能够持续存在，从而带来噪声</li><li>那如何解决这个问题呢？其实保证有方差就行，但是VAE给出了一个优雅的答案：不仅需要保证有方差，还要让所有 <img alt="p(Z|X)" class="mathcode" src="https://images2.imgbox.com/f1/76/Ur7JDAGg_o.png">趋于标准正态分布<img alt="N(0,1)" class="mathcode" src="https://images2.imgbox.com/84/59/ThCSr88v_o.png">，根据定义可知 <p class="img-center"><img alt="\begin{aligned} p(Z) &amp; =\sum_{X} p(Z \mid X) p(X) \\ &amp; =\sum_{X} \mathcal{N}(0, I) p(X) \\ &amp; =\mathcal{N}(0, I) \sum_{X} p(X) \\ &amp; =\mathcal{N}(0, I) \end{aligned}" src="https://images2.imgbox.com/df/bf/sEUUywMQ_o.png"></p> 这个式子的关键意义在于告诉我吗：<strong>如果所有<img alt="p(Z|X)" class="mathcode" src="https://images2.imgbox.com/cc/a3/VH82Z4Lk_o.png">都趋于<img alt="N(0,1)" class="mathcode" src="https://images2.imgbox.com/b5/1c/AvJGooO8_o.png">，那么我们可以保证<img alt="p(Z)" class="mathcode" src="https://images2.imgbox.com/e8/51/QrFaXT2M_o.png">也趋于<img alt="N(0,1)" class="mathcode" src="https://images2.imgbox.com/a1/7f/j4rT1utN_o.png"></strong>，从而实现先验的假设，这样就形成了一个闭环！那怎么让所有<img alt="p(Z|X)" class="mathcode" src="https://images2.imgbox.com/e7/bc/TBmlaZeP_o.png">趋于<img alt="N(0,1)" class="mathcode" src="https://images2.imgbox.com/25/8b/ZlVIAknF_o.png">呢？还是老套路：加loss<br><br> 到此为止，我们可以把VAE进一步画成： <p class="img-center"><img alt="" height="518" src="https://images2.imgbox.com/f1/69/8kXtpjZJ_o.png" width="600"></p> </li></ol> 
<p>现在我们来回顾一下VAE到底做了啥。VAE在AE的基础上</p> 
<p class="img-center"><img alt="" height="300" src="https://images2.imgbox.com/40/95/tADrM1I1_o.png" width="600"></p> 
<ul><li>一方面，对均值的encoder添加高斯噪声(正态分布的随机采样)，使得decoder(即生成器)有噪声鲁棒性</li><li>二方面，为了防止噪声消失，将所有<img alt="p(Z|X)" class="mathcode" src="https://images2.imgbox.com/01/96/fClkL9xr_o.png">趋近于标准正态分布，<strong>将encoder的均值尽量降为 0，而将方差尽量保持住</strong></li></ul> 
<p>这样一来，当decoder训练的不好的时候，整个体系就可以降低噪声；当decoder逐渐拟合的时候，就会增加噪声</p> 
<h3>1.3 VAE的改进：VQ-VAE/VQ-VAE2</h3> 
<h4>1.3.1 什么是VQ-VAE</h4> 
<p>VQ即Vector Quantised，它编码出的向量是离散的，也就是把VAE做量化，所以VQ-VAE最后得到的编码向量的每个元素都是一个整数</p> 
<ol><li>现实生活中，很多信息(声音、图片)都是连续的，你的大部分任务都是一个回归任务。但是等你真正将其表示出来或真正解决这些任务的时候，我们都将其离散化了。图像变成了像素，语音也抽样过了，大部分工作的很好的也都是分类模型(回归任务转换成分类任务)</li><li>如果还是之前VAE的模式，就不好把模型做大，分布也不好学<br> 故最终，取而代之的不是去直接预测分布<img alt="z" class="mathcode" src="https://images2.imgbox.com/d0/6a/SeEBYtkf_o.png">，而是用一个codebook代替。codebook可以理解为聚类的中心，大小一般是K*D（K=8192，Dim=512/768），也就是有8192个长为D的向量 <p class="img-center"><img alt="" height="446" src="https://images2.imgbox.com/6c/a4/cuXNbk4L_o.png" width="600"></p> </li></ol> 
<h4><strong>1.3.2 VQ-VAE的算法流程</strong></h4> 
<ol><li><img alt="x" class="mathcode" src="https://images2.imgbox.com/eb/d9/c2ewIVqR_o.png">输入编码器得到高宽分别为<img alt="(h,w)" class="mathcode" src="https://images2.imgbox.com/f3/72/3HHJQErv_o.png">的特征图<img alt="f" class="mathcode" src="https://images2.imgbox.com/24/e4/umwci0Oq_o.png"></li><li>然后计算特征图里的向量和codebook里的向量(聚类中心)的相似性</li><li>接着把和特征图最接近的聚类中心向量的编号(比如1-8192)存到矩阵<img alt="z" class="mathcode" src="https://images2.imgbox.com/21/aa/cYUF2euM_o.png">里面</li><li>训练完成之后，不再需要编码特征<img alt="f" class="mathcode" src="https://images2.imgbox.com/aa/b2/xWs9IuVD_o.png">，而是取出矩阵<img alt="z" class="mathcode" src="https://images2.imgbox.com/b6/e3/b7UM7F6c_o.png">中的编号对应的codebook里面的向量，生成一个新的特征图<img alt="f_q" class="mathcode" src="https://images2.imgbox.com/31/72/CkhIR283_o.png"> (<span style="color:#7b7f82;">经过量化后的特征，即quantised feature</span>)</li><li>最后和之前一样，使用<img alt="f_q" class="mathcode" src="https://images2.imgbox.com/6b/93/TTE1G6ug_o.png">解码重构原图</li></ol> 
<p>此时这个量化特征就非常可控了，因为它们永远都是从codebook里面来的，而非随机生成，这样优化起来相对容易，如下图所示</p> 
<p class="img-center"><img alt="" height="312" src="https://images2.imgbox.com/a4/5f/r30d9ogU_o.png" width="1000"></p> 
<ul><li>左图：VQ-VAE的模型结构<br> 其中，红色线的梯度<img alt="\triangledown _{z} L" class="mathcode" src="https://images2.imgbox.com/98/c4/IuxcOxBN_o.png">迫使encoder在下一次forword时改变其输出(参数更新)<br> 由于编码器的输出和解码器的输入共享D维空间，梯度包含了编码器如何改变参数以降低损失的有效信息</li><li>右图：embedding space可视化。编码器输出<img alt="z(x)" class="mathcode" src="https://images2.imgbox.com/8e/43/CaQfu5zn_o.png">会mapped到最相近（nearest）的点<img alt="e_2" class="mathcode" src="https://images2.imgbox.com/73/ee/jrAcQWp8_o.png"></li></ul> 
<p>VQ-VAE也可以用来做CV领域的自监督学习，比如BEIT就是把DALL·E训练好的codebook拿来用。将图片经过上面同样的过程quantise成的特征图作为ground truth，自监督模型来训练一个网络</p> 
<p>后续还有VL-BEIT (vision language BEIT)的工作，也是类似的思路，只不过是用一个Transformer编码器来做多模态的任务</p> 
<h4>1.3.2 VQ-VAE2</h4> 
<p>// 待更</p> 
<h2>第二部分 扩散模型DDPM：先前向加噪后反向去噪从而建立噪声估计模型</h2> 
<p>在写本部分之前，我反复看了网上很多阐述DDPM的文章，实话说，一开始看到那种一上来就一堆公式的，起初基本看不下去，虽然后来慢慢的都看得下去了，但如果对于一个初次接触DDPM的初学者来说，一上来就一堆公式确实容易把人绕晕，但如果没有公式，则又没法透彻理解背后的算法步骤</p> 
<p>两相权衡，本文将侧重算法每一步的剖析，而公式更多为解释算法原理而服务，说白了，侧重原理 其次公式，毕竟原理透彻了，公式也就自然而然的能推出来了</p> 
<p>言归正传，咱们先来了解下扩散模型的极简发展史</p> 
<h3>2.1 扩散模型发展史：DDPM、improved DDPM、Latent Diffusion Model到DALL·E/DALL·E2</h3> 
<h4>2.1.1 从扩散模型概念的提出到DDPM</h4> 
<h5><strong><u><span style="color:#fe2c24;"><em>1  </em></span>2015年，Sohl-Dickstein提出「扩散模型」的概念</u></strong></h5> 
<p>2015年，斯坦福大学的一博士后Sohl-Dickstein通过此篇论文《<a class="link-info" href="http://proceedings.mlr.press/v37/sohl-dickstein15.html" rel="nofollow" title="Deep Unsupervised Learning using Nonequilibrium Thermodynamics">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</a>》提出扩散模型的概念<br> 简单来讲，扩散模型的灵感来自非平衡热力学，通过定义了一个扩散步骤的马尔可夫链，以缓慢地将「符合高斯分布的随机噪声」添加到数据中，然后反转扩散过程以从噪声中构建所需的数据样本</p> 
<p class="img-center"><img alt="" height="329" src="https://images2.imgbox.com/ec/2c/7xxdsP52_o.png" width="600"></p> 
<h5><strong><u><span style="color:#fe2c24;"><em>2  </em></span>2019年，斯坦福的宋飏等人估计数据分布梯度的生成模型</u></strong></h5> 
<p>随后，2019年，斯坦福一在读博士宋飏和其导师通过此文《Generative Modeling by Estimating Gradients of the Data Distribution》提出了一种新方法来构建生成模型：即不需要估计数据的概率分布(<em>数据概率的分布类似高维曲面</em>)，相反，它估计的是分布的梯度(<em>分布的梯度可以看成是高维曲面的斜率</em>)</p> 
<p>「<span style="color:#7b7f82;">顺带说一下，后来宋飏等人推出了扩散模型的改进，即DALLE 3的解码器之一致性模型Consistency Models(</span><em><span style="color:#7b7f82;">详见：<a class="link-info" href="https://blog.csdn.net/v_JULY_v/article/details/134355194" title="AI绘画神器DALLE 3的解码器：一步生成的扩散模型之Consistency Models">AI绘画神器DALLE 3的解码器：一步生成的扩散模型之Consistency Models</a></span></em><span style="color:#7b7f82;">)</span>」</p> 
<h5><u><strong><span style="color:#fe2c24;"><em>3</em></span>  2020年，UC Berkeley的Jonathan Ho等人正式提出：DDPM</strong></u></h5> 
<p>再之后，2020年6月，UC Berkeley的Jonathan Ho等人意识到宋飏的工作可以改进 Sohl-Dickstein的扩散模型，很快，便通过论文《Denoising Diffusion Probabilistic Models》正式提出对于普通扩散模型的改进版：DDPM(全称即论文名称：Denoising Diffusion Probabilistic Models)</p> 
<p>DDPM主要有两个贡献</p> 
<ul><li><span style="color:#be191c;"><strong>一方面，从预测转换图像改进为预测噪声 </strong></span>(即如DiT论文所说，reformulating diffusion models to predict noise instead of pixel，可惜强调这点的文章太少了，可它是DDPM的关键，更是DDPM的本质）<br> 作者认为，<strong>每次直接从<img alt="x_{t}" class="mathcode" src="https://images2.imgbox.com/cb/25/xbCsf3m0_o.png">预测<img alt="x_{t-1}" class="mathcode" src="https://images2.imgbox.com/80/11/ffEkcwWL_o.png">，这种图像到图像的转化不太好优化，所以直接去预测从<img alt="x_{t}" class="mathcode" src="https://images2.imgbox.com/e0/55/ki8zcQNd_o.png">到<img alt="x_{t-1}" class="mathcode" src="https://images2.imgbox.com/1f/d6/BtsCxVS4_o.png">这一步所添加的噪声<img alt="\varepsilon" class="mathcode" src="https://images2.imgbox.com/2c/a8/inesEehy_o.png"></strong>，这样就简化了问题：毕竟噪声一旦被预测出来，<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/09/ed/EqurpK1d_o.png">减去噪声即得<strong><img alt="x_{t-1}" class="mathcode" src="https://images2.imgbox.com/47/77/vCDVEoVz_o.png"></strong><br> 这种操作就有点类似ResNet的残差结构。每次新增一些层，模型不是直接从<img alt="x" class="mathcode" src="https://images2.imgbox.com/01/fa/IPcJHBo2_o.png"> 去预测<img alt="y" class="mathcode" src="https://images2.imgbox.com/f7/7c/goxPzPzx_o.png">，而是让新增的层去预测(<img alt="y-x" class="mathcode" src="https://images2.imgbox.com/7a/9b/3Xwl5DIv_o.png">)。这样新增层不用全部重新学习，而是学习原来已经学习到的<img alt="x" class="mathcode" src="https://images2.imgbox.com/c1/96/zZAy0Ru2_o.png"> 和真实值<img alt="y" class="mathcode" src="https://images2.imgbox.com/19/1b/C1X9li4H_o.png"> 之间的残差就行(residual)<br><br><strong>DDPM采用了一个U-Net 结构的Autoencoder来对<img alt="t" class="mathcode" src="https://images2.imgbox.com/03/38/fxiM3WA6_o.png"> 时刻的高斯噪声<img alt="z" class="mathcode" src="https://images2.imgbox.com/36/a9/RUgDh6E5_o.png">进行预测</strong>，训练目标即希望预测的噪声<img alt="f_{\theta}\left(x_{t}, t\right)" class="mathcode" src="https://images2.imgbox.com/a1/15/kfC7dqCn_o.png">和真实的噪声<img alt="z" class="mathcode" src="https://images2.imgbox.com/07/01/DolP0IS6_o.png">一致，所以目标函数为<img alt="f_{\theta}\left(x_{t}, t\right)" class="mathcode" src="https://images2.imgbox.com/73/cc/0geProhQ_o.png">和<img alt="z" class="mathcode" src="https://images2.imgbox.com/bc/cc/Fx0cvZh1_o.png">的<img alt="L_1" class="mathcode" src="https://images2.imgbox.com/20/4c/arZK4oQb_o.png"> Loss：<br><img alt="p\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)=\left\|z-f_{\theta}\left(x_{t}, t\right)\right\|" class="mathcode" src="https://images2.imgbox.com/f1/4a/nckSIwrN_o.png"><br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/b9/6c/E6cixCO4_o.png">  这里的标签<img alt="z" class="mathcode" src="https://images2.imgbox.com/55/ca/lKVGaoI2_o.png">是正向扩散过程中，我们每一步真实添加的噪声(<em><span style="color:#1a439c;">所以噪声是已知的，预测噪声时，其可以拿来当作Ground truth</span></em>)<br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/65/21/EhtleZXL_o.png">  这里的<img alt="f_{\varepsilon }" class="mathcode" src="https://images2.imgbox.com/1c/b2/XDv4J4Z8_o.png">就对应了U-Net 模型结构<br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/87/95/M1auKFve_o.png">  至于<img alt="t" class="mathcode" src="https://images2.imgbox.com/3f/92/SGATH0jD_o.png"> 就是U-Net 的输入<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/7a/ad/biEJXVRu_o.png">之外的另一个输入time embedding「<span style="color:#7b7f82;"><em>类似transformer里的正弦位置编码，主要用于告诉 U-Net模型，现在到了反向过程的第几步，相当于引导U-Net生成</em></span>」<br> 最终，通过这个简单的<img alt="L_1" class="mathcode" src="https://images2.imgbox.com/ea/f1/Zf8k6rv7_o.png">损失函数，模型就可以训练起来了</li><li><strong>二方面，DDPM只预测正态分布的均值</strong><br> 虽然正态分布由均值和方差决定，但作者在这里发现，其实模型不需要学方差，只需要学习均值就行。逆向过程中高斯分布的方差项直接使用一个常数，模型的效果就已经很好。所以就再一次降低了模型的优化难度</li></ul> 
<blockquote> 
 <p>为方便大家更好的理解本文，特地解释下什么叫U-Net网络(<em><span style="color:#7b7f82;">对应论文为：<a class="link-info" href="https://arxiv.org/abs/1505.04597" rel="nofollow" title="U-Net: Convolutional Networks for Biomedical Image Segmentation">U-Net: Convolutional Networks for Biomedical Image Segmentation</a></span></em>)</p> 
 <hr> 
 <p>在目前绝大部分的图像或视频扩散方法中，主导的骨干网络一般是由一系列卷积和自注意力层构成的 U-Net 架构</p> 
 <p class="img-center"><img alt="" height="530" src="https://images2.imgbox.com/6e/b7/AbIJT8hA_o.png" width="800"></p> 
 <p>它总共有23个卷积层的网络，由一个收缩路径(左侧)和一个扩张路径(右侧)组成</p> 
 <ul><li>收缩路径遵循卷积网络的典型架构。 它由两个3x3卷积(无填充卷积)的重复结构组成，每个卷积后面跟着ReLU和一个2x2最大池化操作，步长为2，用于下采样，在每个下采样步骤中，我们将特征通道的数量加倍</li><li>扩张路径中的每个步骤都包括特征图的上采样，然后是一个2x2卷积(“上卷积”)，将特征通道数量减半，与收缩路径中相应裁剪的特征图进行连接(<span style="color:#7b7f82;"><em>a concatenation with the correspondingly cropped feature map from the contracting path</em></span>)，以及两个3x3卷积，每个卷积后面跟着一个ReLU。由于每次卷积都会丢失边界像素(t<span style="color:#7b7f82;"><em>he loss of border pixels in every convolution</em></span>)，所以裁剪是必要的</li><li>在最后一层，使用1x1卷积将每个64个分量的特征向量映射到所需的类别数</li></ul> 
 <hr> 
 <p>人们之所以偏好 U-Net，是因为 Transformer 中全注意力机制的内存需求会随输入序列长度而二次方增长，而在处理视频这样的高维信号时，这样的增长模式会让计算成本变得非常高</p> 
 <p>当然，在此文《<a class="link-info" href="https://blog.csdn.net/v_JULY_v/article/details/134655535" title="视频生成的原理解析：从Gen2、Emu Video到PixelDance、SVD、Pika 1.0、W.A.L.T">视频生成的原理解析：从Gen2、Emu Video到PixelDance、SVD、Pika 1.0、W.A.L.T</a>》的第六部分你会看到，已有最新的研究把Transformer用做扩散模型的骨干网络</p> 
</blockquote> 
<p><strong>DDPM也有些类似VAE，也可以将其当做一个encoder-decoder的结构</strong>，但是有几点区别：</p> 
<ol><li>扩散过程是编码器一步步的走到<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/6a/be/FxXiFlLz_o.png">，而且是一个固定的过程；而VAE的编码器是可以学习的；</li><li>DDPM的每一步输出输出都是同样维度大小的，但对一般的自编码器（AE/VAE等），往往中间的bottleneck特征会比输入小很多</li><li>扩散模型有步数step的概念（time step、time embedding），模型要经过很多步才能生成图片，且在所有step中，U-Net都是共享参数的</li></ol> 
<h4><strong>2.1.2 improved DDPM、Diffusion Model Beat GANs、</strong>Latent Diffusion Model<strong>到DALL·E、DALL·E2</strong></h4> 
<h5><u><strong><span style="color:#fe2c24;"><em>4</em></span>  2021年2月，</strong>OpenAI提出<strong>improved DDPM</strong></u></h5> 
<p>DDPM使得扩散模型可以在真实数据集上work得很好之后，一下子吸引了很多人的兴趣。因为DDPM在数学上简洁美观，无论正向还是逆向，都是高斯分布，可以做很多推理证明，而且还有很多不错的性质，2021年2月，OpenAI的Alex Nichol和Prafulla Dhariwal推出了 improved DDPM「<em><span style="color:#7b7f82;">其对应论文为：《<a class="link-info" href="https://arxiv.org/abs/2102.09672" rel="nofollow" title="Improved Denoising Diffusion Probabilistic Models">Improved Denoising Diffusion Probabilistic Models</a>》</span></em>」</p> 
<p>improved DDPM相比DDPM做了几点改动：</p> 
<ol><li>DDPM的逆向过程中，高斯分布的方差项直接使用一个常数而不用学习<br> improved DDPM作者就觉得如果对方差也进行学习的话，效果应该会更好，改了之后果然取样和生成效果都好了很多</li><li>DDPM添加噪声时采用的线性的variance schedule改为余弦schedule，效果更好(<em>类似学习率从线性改为余弦</em>)</li><li>简单尝试了scale大模型之后，生成效果更好</li></ol> 
<h5><u><span style="color:#fe2c24;"><em>5</em></span>  2021年5月 Diffusion Model Beat GANs：使用classifier guidance的方法，引导模型进行采样和生成</u></h5> 
<p>上面第三点对OpenAI来说，无疑是个好消息。所以improved DDPM的二作和三作马上着手研究，发布了《Diffusion Models Beat GANs on Image Synthesis》这篇论文，比之前的improved DDPM又做了一些改进：</p> 
<ol><li>使用大模型：加大加宽网络、使用更多的自注意力头attention head，加大自注意力scale (single-scale attention改为multi-scale attention)</li><li>提出了新的归一化方式——Adaptive Group Normalization，相当于根据步数进行自适应的归一化，这个方法是对group归一化的一个改进：<img alt="\text{AdaGN}(h,y=[y_s,y_b]) = y_s\text{GroupNorm}(h)+y_b" class="mathcode" src="https://images2.imgbox.com/2f/c1/4y2E4vhR_o.png"><br> 上面公式中的<img alt="h" class="mathcode" src="https://images2.imgbox.com/cc/35/0FOkO92T_o.png">是残差块激活函数的输出，<img alt="y" class="mathcode" src="https://images2.imgbox.com/b8/6d/By1oHaM1_o.png">是一个线性层对时步和后面用到的类别信息的嵌入。组归一化是对输入的通道方向进行分组归一化的归一化方法，可以理解为局部LayerNorm <p class="img-center"><img alt="在这里插入图片描述" height="256" src="https://images2.imgbox.com/dc/95/33Cbw129_o.png" width="1000"></p> </li><li><span style="color:#ed7976;"><strong>使用classifier guidance的方法，引导模型进行采样和生成</strong></span><br> 这样不仅使生成的图片更逼真，而且加速了反向采样过程。论文中，只需要25次采样，就可以从噪声生成图片<br><br> 所谓classifier guided diffusion<br><em><strong>1</strong></em> <strong>即在反向过程训练U-Net的同时，也训练一个简单的图片分类器</strong>。这个分类器是在ImageNet上训练的，只不过图片加了很多噪声 (<span style="color:#7b7f82;">毕竟扩散模型的输入始终是加了很多噪声的，跟真实的ImageNet图片是很不一样的，是从头训练的</span>)<br><em><strong>2</strong> </em>当采样<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/f6/e6/5fHJE3AB_o.png">之后，直接扔给分类器，就可以看到图片分类是否正确，这时候就可以算一个交叉熵目标函数，对应的就得到了一个梯度。之后使用分类器对<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/82/44/fjhlu1Ez_o.png">的梯度信息<img alt="" height="20" src="https://images2.imgbox.com/12/af/MpHi8neT_o.png" width="104">指导扩散模型的采样和生成<br><em><strong>3</strong></em> 这个<strong>梯度暗含了当前图片是否包含物体，以及这个物体是否真实的信息。通过这种梯度的引导，就可以帮助U-Net将图片生成的更加真实，要包含各种细节纹理，而不是意思到了就行，要和真实物体匹配上</strong><br>   <p>当然，除了最简单最原始的classifier guidance之外，还有很多其它的引导方式<br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/e5/d9/sPVg57mc_o.png">  CLIP guidance：将简单的分类器换成CLIP之后，文本和图像就联系起来了。此时<strong>不光可以利用这个梯度引导模型采用和生成，而且可以利用文本指导其采样和生成</strong><br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/85/0a/idCucUit_o.png">  image侧引导：除了利用图像重建进行像素级别的引导，还可以做图像特征和风格层面的引导，只需要一个gram matrix就行<br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/d9/0a/AxqBERBu_o.png">  text 侧：可以用训练好的NLP大模型做引导<br> 以上所有引导方式，都是下面目标函数里的<img alt="y" class="mathcode" src="https://images2.imgbox.com/82/c9/CmSVRNBM_o.png">，即模型的输入不光是<img alt="x_{t}" class="mathcode" src="https://images2.imgbox.com/bd/2b/2OaKF15X_o.png">和time embedding，还有condition，加了condition之后，可以让模型的生成又快又好<img alt="p\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)=\left\|z-f_{\theta}\left(x_{t}, t, y\right)\right\|" class="mathcode" src="https://images2.imgbox.com/2b/49/rvIOpCuA_o.png"><br> 且值得一提的是，额外引入一个网络来指导，推理的时候比较复杂 (扩散模型需要反复迭代，每次迭代都需要额外算一个分数)，所以引出了后续2022年7月的一个工作：classifier free guidance</p> </li></ol> 
<h5><u><span style="color:#fe2c24;"><em>6</em></span>  2021年12月 潜在扩散空间Latent Diffusion Model</u></h5> 
<p>2021年年底，此篇论文《<a class="link-info" href="https://arxiv.org/abs/2112.10752" rel="nofollow" title="High-Resolution Image Synthesis with Latent Diffusion Models">High-Resolution Image Synthesis with Latent Diffusion Models</a>》提出了潜在扩散模型，也是后续奠定stable diffusion的核心论文(<em><span style="color:#7b7f82;">关于SD详见本系列的另一篇文章《<a class="link-info" href="https://blog.csdn.net/v_JULY_v/article/details/131205615" title="AI绘画原理解析：从CLIP到DALLE1/2、DALLE 3、Stable Diffusion">AI绘画原理解析：从CLIP到DALLE1/2、DALLE 3、Stable Diffusion</a>》</span></em>)</p> 
<p>为何要弄这么个隐空间或潜在空间呢？</p> 
<p>原因很简单，为了使扩散模型在有限的计算资源上训练，并且保留它们的质量和灵活性，故首先训练了一个强大的预训练自编码器，这个自编码器所学习到的是一个潜在的空间，这个潜在的空间要比像素空间要小的多(<span style="color:#7b7f82;"><em>可以简单粗暴的理解为就是一个被压缩或被降维的空间</em></span>)，把扩散模型在这个潜在的空间去训练，大大的降低了对算力的要求，这也是Stable Diffusion比原装Diffusion速度快的原因</p> 
<h5><u><span style="color:#fe2c24;"><em>7</em></span>  2022年7月 Classifier-Free Diffusion Guidance</u></h5> 
<p>所谓classifier free guidance的方式(<span style="color:#7b7f82;"><em>对应论文为《Jonathan Ho and Tim Salimans. <a class="link-info" href="https://arxiv.org/abs/2207.12598" rel="nofollow" title="Classifier-free diffusion guidance">Classifier-free diffusion guidance</a>》</em></span>)，只是改变了模型输入的内容，除了 conditional输入外(随机高斯噪声输入加引导信息)，还有 unconditional 的采样输入，两种输入都会被送到同一个 diffusion model，从而让其能够具有无条件和有条件生成的能力</p> 
<ol><li>得到有条件输出<img alt="f_{\theta }(x_{t},t,y)" class="mathcode" src="https://images2.imgbox.com/fe/c2/ZDpDx8Kz_o.png">和无条件输出<img alt="f_{\theta }(x_{t},t,\phi)" class="mathcode" src="https://images2.imgbox.com/e0/c3/D2RG7pfY_o.png">后，就可以用前者监督后者，来引导扩散模型进行训练了</li><li>最后反向扩散做生成时，我们用无条件的生成，也能达到类似有条件生成的效果，这样一来就摆脱了分类器的限制，所以叫classifier free guidance<br> 比如在训练时使用图像-文本对，这时可以使用文本做指导信号，也就是训练时使用文本作为<img alt="y" class="mathcode" src="https://images2.imgbox.com/11/2b/JrUczBKz_o.png">生成图像。然后<img alt="y" class="mathcode" src="https://images2.imgbox.com/35/42/p7sXFk8m_o.png">把去掉，替换为一个空集<img alt="\phi" class="mathcode" src="https://images2.imgbox.com/f7/99/Mcw9sHWC_o.png">（空的序列），生成另外的输出</li></ol> 
<p>总之，扩散模型本来训练就很贵了，classifier free guidance这种方式在训练时需要生成两个输出，所以训练更贵了。但是这个方法确实效果好，所以在GLIDE 、DALL·E2和Imagen里都用了，而且都提到这是一个很重要的技巧，用了这么多技巧之后，GLIDE终于是一个很好的文生图模型了，只用了35亿参数，生成效果和分数比120亿参数的DALL·E还要好</p> 
<h5><strong><u><span style="color:#fe2c24;"><em>8</em></span>  2021-2022年 DALL·E/DALL·E2：条件引导生成</u></strong></h5> 
<p>2021年，OpenAI一看GLIDE这个方向靠谱，就马上跟进，不再考虑DALL·E的VQ-VAE路线了，而是将GLIDE改为层级式生成（56→256→1024）并加入prior网络等等，于是</p> 
<ul><li>Jonathan Ho和他在谷歌研究中心的同事Tim Salimans，与其他地方的团队合作，展示了如何结合大型语言模型的信息与图像生成扩散模型，即用文本（比如“金鱼在海滩上喝可口可乐”）指导扩散过程，从而生成图像，最终得到了DALL·E2 (<em><span style="color:#7b7f82;">其具体的训练细节在下一篇文章《<a class="link-info" href="https://blog.csdn.net/v_JULY_v/article/details/131205615" title="AI绘画与多模态原理解析：从CLIP到DALLE 3、Stable Diffusion、MDJ">AI绘画与多模态原理解析：从CLIP到DALLE 3、Stable Diffusion、MDJ</a>》会重点讲解 </span></em>）</li><li>总之，DALL·E2 这样的从文本到图像模型成功的背后原因，就是这种<strong>“引导扩散”(guided diffusion)</strong>过程</li></ul> 
<p class="img-center"><img alt="" height="231" src="https://images2.imgbox.com/e6/68/siFZQpAA_o.png" width="700"></p> 
<hr> 
<p>回到DDPM，每一个噪声都是在前一时刻增加噪声而来的，从最开始的<img alt="x_0" class="mathcode" src="https://images2.imgbox.com/d3/d6/92DSlC6V_o.png">时刻开始，最终得到<img alt="x_T" class="mathcode" src="https://images2.imgbox.com/5a/90/QcwuSmHW_o.png">时刻的纯噪声图像。不过问题来是为什么要加噪声？</p> 
<ol><li>Diffusion的最终目标是去噪以生成图片，而为了推导出逆向的去噪方法，必须了解增加噪声的原理。同时，添加噪声的过程其实就是不断构建标签的过程。如果在前一时刻可以预测出来后一时刻的噪声，便能很方便地实现还原操作 (就和人走路一样，不管你从哪来，哪怕走过万水千山，最后都可按原路返回至原出发点)<br> 说白了 <strong>当你学会了怎么加噪(前向扩散)，就一定能知道怎么去噪(逆向生成)</strong>，毕竟知道怎么来 也必知道怎么回 <p class="img-center"><img alt="" height="148" src="https://images2.imgbox.com/89/9f/ZmjE1UAw_o.png" width="1000"></p> </li><li>且在噪声的添加过程中，每一步都要保持尽量相同的噪声扩散幅度。比如，在给上图加噪的过程中，前期的分布非常均匀，添加一些噪声便可以将原始分布改变，但到后期，需要添加更多的噪声，方可保证噪声扩散幅度相同(这就像往水中加糖，为了使糖的甜味增长相同，后期需要加更多的糖)</li></ol> 
<p>所以DDPM为了从随机噪声中直接生成图片，首先需要训练一个噪声估计模型，然后将输入的随机噪声还原成图片，相当于就两个关键，一个是训练过程，一个是推理过程</p> 
<ol><li>训练过程：随机生成噪声<img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/04/27/9TI1T2GO_o.png">，经过<img alt="T" class="mathcode" src="https://images2.imgbox.com/08/bd/l6GurqN8_o.png">步将噪声扩散到输入原始图片<img alt="x_0" class="mathcode" src="https://images2.imgbox.com/00/c7/JLFaZREU_o.png">中，破坏后的图片<img alt="x_T" class="mathcode" src="https://images2.imgbox.com/f8/22/ljK4RtmE_o.png">，学习破坏图片的预估噪声<img alt="\epsilon _\theta (x_t,t)" class="mathcode" src="https://images2.imgbox.com/bd/24/5bHT95im_o.png">，并用<span style="color:#7b7f82;">L2 loss</span>约束预估噪声<img alt="\epsilon _\theta (x_t,t)" class="mathcode" src="https://images2.imgbox.com/6c/5f/7A2RlufK_o.png">与原始输入噪声<img alt="\epsilon" src="https://images2.imgbox.com/ca/ed/SRfiGzOI_o.png">的距离</li><li>推理过程：即输入噪声，经过预估噪声模型还原成图片</li></ol> 
<h3>2.2 DDPM的两个过程：从前向过程到逆向过程</h3> 
<h4 id="h_566618077_2" style="background-color:transparent;">2.2.1 前向过程(加噪)：通过高斯噪音随机加噪<img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/ce/da/wKYEGPLM_o.png"> ——给图片打马赛克</h4> 
<p>前向过程(forward process)也称为扩散过程(diffusion process)，简单理解就是对原始图片<img alt="x_0" class="mathcode" src="https://images2.imgbox.com/60/37/ere1ruzz_o.png">通过逐步添加「方差为<img alt="\beta _t" class="mathcode" src="https://images2.imgbox.com/06/26/9ug0ZuUJ_o.png">的高斯噪声」变成<img alt="x_T" class="mathcode" src="https://images2.imgbox.com/3d/f3/nfhyaHW5_o.png">，从而达到破坏图片的目的，如下图 </p> 
<p class="img-center"><img alt="" height="97" src="https://images2.imgbox.com/d1/ed/KI68y6Nl_o.png" width="600"></p> 
<p>在从<img alt="x_{t-1}" class="mathcode" src="https://images2.imgbox.com/7b/17/2BhhIIc3_o.png">到<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/8d/e6/Uo4XEOl7_o.png">的过程中，其对应的分布<img alt="q(x_t|x_{t-1})" class="mathcode" src="https://images2.imgbox.com/64/00/wI7CFvQ1_o.png">是一个正太分布，且其均值是<img alt="u_t = \sqrt{1-\beta _t }x_{t-1}" class="mathcode" src="https://images2.imgbox.com/2c/13/OQG04I57_o.png">，方差为<img alt="\beta _t" class="mathcode" src="https://images2.imgbox.com/d9/dd/QfSUTfxU_o.png">，则有 </p> 
<p class="img-center"><img alt="q(x_t|x_{t-1}) = N(x_t;u_t = \sqrt{1-\beta _t }x_{t-1},\beta _t \mathbb{I})" class="mathcode" src="https://images2.imgbox.com/8d/da/PWl6kKLy_o.png"></p> 
<p>对于这个公式，解释下3点</p> 
<ol><li> <p>正态分布的概率密度函数具有以下形式：</p> <p><img alt="f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}" class="mathcode" src="https://images2.imgbox.com/4a/d9/OvR54UW2_o.png"></p> <p>顺带帮你再回顾下：正态分布有两个参数：均值<img alt="\mu" class="mathcode" src="https://images2.imgbox.com/52/65/At9iGdBV_o.png">和方差<img alt="\sigma ^2" class="mathcode" src="https://images2.imgbox.com/05/92/0JgJzEI1_o.png">。其中，<img alt="\mu" class="mathcode" src="https://images2.imgbox.com/8c/38/OHxa3lN0_o.png">是分布的均值，决定了分布的中心位置，<img alt="\sigma" class="mathcode" src="https://images2.imgbox.com/1c/7e/q8zrsIrr_o.png">是标准差，决定了分布的宽度</p> </li><li>上面的方差之所以表示为<img alt="\beta _t \mathbb{I}" class="mathcode" src="https://images2.imgbox.com/ce/41/Wx6ick9Y_o.png">，原因在于我们一般处于多维情况下，而<img alt="\mathbb{I}" class="mathcode" src="https://images2.imgbox.com/28/7a/X1yPrZVi_o.png">是单位矩阵，表明每个维度有相同的标准偏差<img alt="\beta _t" class="mathcode" src="https://images2.imgbox.com/c1/1e/uctyMrgH_o.png"><br> 且<img alt="\beta _t \in (0,1)" class="mathcode" src="https://images2.imgbox.com/cb/64/r1pXGSnk_o.png">是事先给定的常量，代表从<img alt="x_{t-1}" class="mathcode" src="https://images2.imgbox.com/97/e2/jFFYAL6g_o.png">到<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/a7/17/R1eeWeLH_o.png">这一步的方差，且正因为<img alt="\beta _t" class="mathcode" src="https://images2.imgbox.com/84/a5/vSRtJ9Vz_o.png">设置的比较小，所以使得<img alt="q(x_t|x_{t-1})" class="mathcode" src="https://images2.imgbox.com/a7/c0/d339VHal_o.png">的均值在<img alt="x_{t-1}" class="mathcode" src="https://images2.imgbox.com/a3/f5/n2IVcA3l_o.png">附近<br> 换言之，<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/a9/bc/eDUPEzCl_o.png">相当于就是在<img alt="x_{t-1}" class="mathcode" src="https://images2.imgbox.com/c3/3a/glRwB0fp_o.png">的基础上加了一些噪声，而且是渐进式逐步增加/扩散的，当然 从加噪大小的角度上讲，<strong>前期加噪较弱，后期加噪加强</strong>，所以在DDPM的论文中，作者取<img alt="\beta _1,\cdots \beta _T" class="mathcode" src="https://images2.imgbox.com/be/10/oPwuXyB8_o.png">为从0.0001到0.02的线性递增序列</li><li>此外，值得一提的是，因为是马尔可夫链，所以其联合分布便是：<br><img alt="q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right)=q\left(\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{T} \mid \mathbf{x}_{0}\right)=\prod_{t=1}^{T} q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\right)" class="mathcode" src="https://images2.imgbox.com/4a/0d/5tdfn1dZ_o.png"><br> 如下图所示<br><img alt="" height="580" src="https://images2.imgbox.com/54/47/6Z7LZumn_o.png" width="1200"></li></ol> 
<hr> 
<p>接下来，如果我们定义 <img alt="\alpha _t = 1 - \beta_t" class="mathcode" src="https://images2.imgbox.com/18/78/UZk0iFH3_o.png">,   且<img alt="\left \{ {\alpha_t} \right \}_{t=1}^{T}" class="mathcode" src="https://images2.imgbox.com/b8/09/QZcn1dvL_o.png">『<span style="color:#7b7f82;"><em>被称为Noise schedule，通常是一些列很小的值</em></span>』，以及 <img alt="\epsilon _{t-1} \sim N(0,1)" class="mathcode" src="https://images2.imgbox.com/71/a7/5YK9oWEg_o.png">是高斯噪声(<span style="color:#7b7f82;"><em>即满足正太分布</em></span>)，便可以得到<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/88/f0/mPT8N8UT_o.png">的采样值</p> 
<p style="text-align:center;"><img alt="x_t = \sqrt{\alpha _t}x_{t-1} + \sqrt{1-\alpha _t} \epsilon _{t-1}" class="mathcode" src="https://images2.imgbox.com/76/03/86KzUf74_o.png"></p> 
<p>把上述公式迭代变换下，可以直接得出 <img alt="x_0" class="mathcode" src="https://images2.imgbox.com/97/09/AHgWVSUy_o.png"> 到 <img alt="x_t" class="mathcode" src="https://images2.imgbox.com/a8/f9/0hHeiQqT_o.png"> 的公式，如下：</p> 
<p style="text-align:center;"><img alt="x_{t}=\sqrt{\bar{\alpha}_{t}} \boldsymbol{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \epsilon_{0}" class="mathcode" src="https://images2.imgbox.com/53/5a/oCwwU1jm_o.png"></p> 
<p>其中 <img alt="\bar{\alpha _t} = \prod_{i=1}^{t} \alpha _i" class="mathcode" src="https://images2.imgbox.com/a2/11/YBssZCuG_o.png">，<img alt="\epsilon \sim N(0,1)" class="mathcode" src="https://images2.imgbox.com/14/67/lpyO4ayv_o.png"> 也是一个高斯噪声</p> 
<p>换言之，所以 <img alt="x_t" class="mathcode" src="https://images2.imgbox.com/f7/b1/joUfkH7Q_o.png"> 在 <img alt="x_0" class="mathcode" src="https://images2.imgbox.com/ce/f7/SoUw8xxK_o.png">条件下的分布就是均值为 <img alt="\sqrt{\bar{\alpha _t}}x_0" class="mathcode" src="https://images2.imgbox.com/3d/63/dE7o5Rbl_o.png"> , 方差为 <img alt="1-\bar{\alpha _t}" class="mathcode" src="https://images2.imgbox.com/fd/38/u5sDYHFK_o.png">的正态分布 (<em><span style="color:#7b7f82;">下式的意义在于，只需要给出<img alt="x_0" class="mathcode" src="https://images2.imgbox.com/ec/ce/QmXzyICI_o.png">，便可以计算出任意时刻<img alt="t" class="mathcode" src="https://images2.imgbox.com/55/c8/QqiFywBk_o.png">的<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/69/1f/aDPvs0SY_o.png"></span></em>)</p> 
<p style="text-align:center;"><img alt="q\left(\mathbf{x}_{t} \mid \mathbf{x}_{0}\right)=\mathcal{N}\left(\mathbf{x}_{t} ; \sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0},\left(1-\bar{\alpha}_{t}\right) \mathbf{I}\right)" class="mathcode" src="https://images2.imgbox.com/0a/39/wtHSyKQo_o.png"></p> 
<blockquote> 
 <p>考虑到可能会有读者对这个<img alt="x_0" class="mathcode" src="https://images2.imgbox.com/73/da/MSM9c6c6_o.png"> 到 <img alt="x_t" class="mathcode" src="https://images2.imgbox.com/4a/11/wb9FXBda_o.png"> 的一步到位感到困惑，而一般的同类文章不会展开的特别细，故本文细致展开下(能拆10步则10步，确保阅读无障碍)</p> 
 <ol><li>首先通过<img alt="x_t = \sqrt{\alpha _t}x_{t-1} + \sqrt{1-\alpha _t} \epsilon _{t-1}" class="mathcode" src="https://images2.imgbox.com/f6/20/0ydjhrUG_o.png">可知，<img alt="x_{t-1} = \sqrt{\alpha _{t-1}}x_{t-2} + \sqrt{1-\alpha _{t-1}} \epsilon _{t-2}" class="mathcode" src="https://images2.imgbox.com/c0/28/7APsTH4s_o.png">，把这个代入到<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/d0/be/Gd58E1NC_o.png">的表达式后，再展开即可得<br><img alt="\begin{aligned} \boldsymbol{x}_{t} &amp; =\sqrt{\alpha_{t}} \boldsymbol{x}_{t-1}+\sqrt{1-\alpha_{t}} \boldsymbol{\epsilon}_{t-1}^{*} \\ &amp; =\sqrt{\alpha_{t}}\left(\sqrt{\alpha_{t-1}} \boldsymbol{x}_{t-2}+\sqrt{1-\alpha_{t-1}} \boldsymbol{\epsilon}_{t-2}^{*}\right)+\sqrt{1-\alpha_{t}} \epsilon_{t-1}^{*} \\ &amp; =\sqrt{\alpha_{t} \alpha_{t-1}} \boldsymbol{x}_{t-2}+\sqrt{\alpha_{t}-\alpha_{t} \alpha_{t-1}} \boldsymbol{\epsilon}_{t-2}^{*}+\sqrt{1-\alpha_{t}} \epsilon_{t-1}^{*} \end{aligned}" class="mathcode" src="https://images2.imgbox.com/cd/b6/UnJvvXRG_o.png"></li><li>考虑到两个独立正态分布的随机变量之和是正态的，其均值是两个均值之和，其方差是两个方差之和（即标准差的平方是标准差的平方）<span style="color:#7b7f82;">「<em>比如两个方差不同的高斯分布<img alt="\mathcal{N}(\mathbf{0}, \sigma_1^2\mathbf{I})" class="mathcode" src="https://images2.imgbox.com/0f/cb/3pSIfq1v_o.png">和<img alt="\mathcal{N}(\mathbf{0}, \sigma_2^2\mathbf{I})" class="mathcode" src="https://images2.imgbox.com/4b/a7/yexFq1u5_o.png">相加等于一个新的高斯分布<img alt="\mathcal{N}(\mathbf{0}, (\sigma_1^2 + \sigma_2^2)\mathbf{I})" class="mathcode" src="https://images2.imgbox.com/2d/82/HJtV9eY1_o.png"></em><span style="color:#4f4f4f;">」，然后再通过重参数技巧可得</span></span><br><img alt="x_t \begin{array}{l} =\sqrt{\alpha_{t} \alpha_{t-1}} \boldsymbol{x}_{t-2}+\sqrt{​{\sqrt{\alpha_{t}-\alpha_{t} \alpha_{t-1}}}^{2}+{\sqrt{1-\alpha_{t}}}^{2}} \boldsymbol{\epsilon}_{t-2} \\ =\sqrt{\alpha_{t} \alpha_{t-1}} \boldsymbol{x}_{t-2}+\sqrt{\alpha_{t}-\alpha_{t} \alpha_{t-1}+1-\alpha_{t}} \boldsymbol{\epsilon}_{t-2} \\ =\sqrt{\alpha_{t} \alpha_{t-1}} \boldsymbol{x}_{t-2}+\sqrt{1-\alpha_{t} \alpha_{t-1}} \boldsymbol{\epsilon}_{t-2} \\ =\ldots \end{array}" class="mathcode" src="https://images2.imgbox.com/0f/45/hLgRs8eF_o.png"><br><br> 对此，本文参考文献中的这篇《Understanding Diffusion Models: A Unified Perspective》也解释了这几个步骤<img alt="" height="130" src="https://images2.imgbox.com/db/65/7p6KXBio_o.png" width="800"></li><li>最后定义一个累积混合系数， <img alt="\bar{\alpha _t} = \prod_{i=1}^{t}\alpha _i" class="mathcode" src="https://images2.imgbox.com/68/c7/9ymQ4zts_o.png">，即<img alt="\sqrt{\bar{\alpha _t}} = \sqrt{\alpha _t} \sqrt{\alpha _{t-1}} \cdots \sqrt{\alpha _1}" class="mathcode" src="https://images2.imgbox.com/54/78/sFtxZJqy_o.png">，可得<br><img alt="x_t \begin{array}{l} =\sqrt{\prod_{i=1}^{t} \alpha_{i}} \boldsymbol{x}_{0}+\sqrt{1-\prod_{i=1}^{t} \alpha_{i} \boldsymbol{\epsilon}_{0}} \\ =\sqrt{\bar{\alpha}_{t}} \boldsymbol{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \boldsymbol{\epsilon}_{0} \\ \sim \mathcal{N}\left(\boldsymbol{x}_{t} ; \sqrt{\bar{\alpha}_{t}} \boldsymbol{x}_{0},\left(1-\bar{\alpha}_{t}\right) \mathbf{I}\right) \end{array}" class="mathcode" src="https://images2.imgbox.com/78/32/UZB7vEFb_o.png"></li></ol> 
</blockquote> 
<h4 style="background-color:transparent;">2.2.2 逆向过程(去噪)：求解真实后验分布<img alt="q(x_{t-1}|x_t)" class="mathcode" src="https://images2.imgbox.com/ed/bc/VBQNpQZt_o.png">—— 复原被加噪的图片使之清晰化</h4> 
<h5>2.2.2.1 没法直接通过<img alt="\mathbf{x}_{0}=\frac{1}{\sqrt{\bar{\alpha}_{t}}}\left(\mathbf{x}_{t}-\sqrt{1-\bar{\alpha}_{t}} \epsilon\right)" class="mathcode" src="https://images2.imgbox.com/23/b3/yOMON1l1_o.png">去做逆向</h5> 
<p><span style="color:#ed7976;">逆向过程就是通过估测噪声，多次迭代逐渐将被破坏的 <img alt="x_t" class="mathcode" src="https://images2.imgbox.com/ac/fd/HgoL5sJW_o.png">恢复成 <img alt="x_0" class="mathcode" src="https://images2.imgbox.com/fa/12/HO5sShgO_o.png"></span>，如下图</p> 
<p class="img-center"><img alt="" height="101" src="https://images2.imgbox.com/bc/1b/Kb74ysgv_o.png" width="600"></p> 
<p>更具体而言，正向扩散和逆扩散过程都是马尔可夫，唯一的区别就是正向扩散里每一个条件概率的高斯分布的均值和方差都是已经确定的（依赖于 <img alt="\beta _t" class="mathcode" src="https://images2.imgbox.com/9f/99/5qhDSQCc_o.png"> 和 <img alt="x_0" class="mathcode" src="https://images2.imgbox.com/69/5e/EIbjtCTg_o.png">），而逆扩散过程里面的均值和方差需要通过网络学出来，怎么个学法呢？</p> 
<ol><li>有人可能要说，直接把上一节得到的<img alt="x_t = \sqrt{\bar{\alpha_t}}x_{0} + \sqrt{1-\bar{\alpha_t}} \epsilon_0" class="mathcode" src="https://images2.imgbox.com/62/a6/R6KL5HSd_o.png">移个项不就行了(<span style="color:#7b7f82;">先把带<span style="color:#7b7f82;"><img alt="x_0" class="mathcode" src="https://images2.imgbox.com/20/64/xpJLbRdt_o.png"></span>的项移到等式左边，然后所有项各自除以<img alt="\sqrt{\bar{\alpha }}" class="mathcode" src="https://images2.imgbox.com/fe/76/eID2qtak_o.png">，最后把等式右边的<img alt="1/\sqrt{\bar{\alpha }}" class="mathcode" src="https://images2.imgbox.com/fb/3d/ykxz8u4q_o.png">提取到括号外边即可</span>)？<br><img alt="\mathbf{x}_{0}=\frac{1}{\sqrt{\bar{\alpha}_{t}}}\left(\mathbf{x}_{t}-\sqrt{1-\bar{\alpha}_{t}} \epsilon_0\right)" class="mathcode" src="https://images2.imgbox.com/f0/99/JcoH7osk_o.png"><br><br> 但问题在于<img alt="x_t = \sqrt{\bar{\alpha_t}}x_{0} + \sqrt{1-\bar{\alpha_t}} \epsilon_0" class="mathcode" src="https://images2.imgbox.com/d6/84/jodQPsE5_o.png">中的<img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/ad/bd/LU1x27yU_o.png">是个随机变量，意味着 <img alt="x_t" class="mathcode" src="https://images2.imgbox.com/11/8c/TqfhM4Qy_o.png"> 也是个随机变量，其具体取值由 <img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/21/0d/O3S6eGt9_o.png">实际取值决定「<em><span style="color:#7b7f82;">相当于现在我们有一个具体的 <img alt="x_t" class="mathcode" src="https://images2.imgbox.com/cb/8f/nrEoWIJM_o.png">，它对应着 <img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/20/f6/sqapoi19_o.png"> 的某个取值，但是什么值我们并不知道</span></em>」，所以我们只能以前向过程的 <img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/27/1c/bEkyJQqE_o.png"> 取值为标签，训练一个模型去估计它，即：<br><img alt="\mathbf{x}_{\theta}\left(\mathbf{x}_{t}, t\right):=\frac{1}{\sqrt{\bar{\alpha}_{t}}}\left(\mathbf{x}_{t}-\sqrt{1-\bar{\alpha}_{t}} \epsilon_{\theta}\left(\mathbf{x}_{t}, t\right)\right)" class="mathcode" src="https://images2.imgbox.com/40/1c/pgv5Ms3e_o.png"><br> 其中<br><span style="color:#1a439c;"><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/a7/f4/WGjgu5gv_o.png">  <img alt="\epsilon_\theta (x_t, t)" class="mathcode" src="https://images2.imgbox.com/aa/b1/ztyvNdTO_o.png">就是所谓的模型，用来近似真实的</span>(即前向过程采样出来的)<span style="color:#1a439c;"><img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/54/d6/CT5xySEE_o.png"></span><br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/44/e3/FDm61gZQ_o.png">  相应地，<img alt="\mathbf{x}_{\theta}\left(\mathbf{x}_{t}, t\right)" class="mathcode" src="https://images2.imgbox.com/56/30/BsvsnHs4_o.png">就是 <img alt="x_0" class="mathcode" src="https://images2.imgbox.com/a0/db/Ex7dUGFS_o.png"> 的近似，或者，你也可以无视<img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/39/65/DM3x59JU_o.png">，直接把 <img alt="\mathbf{x}_{\theta}\left(\mathbf{x}_{t}, t\right)" class="mathcode" src="https://images2.imgbox.com/a2/1c/EoBRcFh1_o.png"> 视为模型<br><br> 为了训练它，最直接的想法就是用 L2 损失<img alt="\left\|\epsilon-\epsilon_{\theta}\left(\mathbf{x}_{t}, t\right)\right\|^{2}" class="mathcode" src="https://images2.imgbox.com/b4/a3/OZIJZN9o_o.png">或者<img alt="\left\|\mathbf{x}_{0}-\mathbf{x}_{\theta}\left(\mathbf{x}_{t}, t\right)\right\|^{2}" class="mathcode" src="https://images2.imgbox.com/d9/e7/MywFOGpx_o.png"><p class="img-center"><img alt="" height="280" src="https://images2.imgbox.com/48/4e/TF2i520j_o.png" width="700"></p> <p>理论上没问题，但是实际效果很差，为什么呢？如果直接用 <img alt="\mathbf{x}_{\theta}\left(\mathbf{x}_{t}, t\right)" class="mathcode" src="https://images2.imgbox.com/9b/83/s4tlRpdD_o.png">，那么中间的<img alt="\mathbf{x}_{2}, \mathbf{x}_{3}, \ldots, \mathbf{x}_{T-1}" class="mathcode" src="https://images2.imgbox.com/d6/f3/V7Wce0K4_o.png">都没用了，整个 DDPM 就退化成了 VAE 的结构，但是<br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/75/78/jKBS8Jy6_o.png">  VAE 的生成模型和后验都是自己学习出来的，二者双向奔赴共同优化去寻找最优解<br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/75/33/T1ZNfPiD_o.png">  而 DDPM 的后验是人为指定的（即<img alt="\mathbf{x}_{0} = 1/{\sqrt{\bar{\alpha}_{t}}}\left(\mathbf{x}_{t}-\sqrt{1-\bar{\alpha}_{t}} \epsilon_0\right)" class="mathcode" src="https://images2.imgbox.com/ac/6d/ArcIF6jG_o.png">），并且由于 <img alt="\bar{\alpha}_{t} \rightarrow 0" class="mathcode" src="https://images2.imgbox.com/b2/59/FMU1yVcJ_o.png">，<img alt="q(x_t|x_0)" class="mathcode" src="https://images2.imgbox.com/46/79/wDA8MI4t_o.png">基本上就是一个标准正态分布，磨灭掉了几乎所有的输入信息，全靠生成模型这一边去恢复，难度未免过大..</p> </li><li>所以，实际应用中，我们是一点一点来的，比如<strong>先生成<strong><img alt="x_{t-1}" class="mathcode" src="https://images2.imgbox.com/67/7e/cgfxTn0B_o.png"></strong>、然后<strong><img alt="x_{t-2}" class="mathcode" src="https://images2.imgbox.com/b9/78/ZFzBywOI_o.png"></strong>……由于每一步的变化都比较小，保留了上一步足够的信息，生成模型的负担就轻了很多</strong><br> 如果我们能够逆转前向过程并从<strong>真实分布 <img alt="q(x_{t-1}|x_t)" class="mathcode" src="https://images2.imgbox.com/2d/c1/ZOYQL3Hy_o.png">采样</strong>，就可以从高斯噪声 <img alt="x_t \sim N( 0, I )" class="mathcode" src="https://images2.imgbox.com/2d/2d/xLudSK5C_o.png">还原出原图分布 <img alt="x_0 \sim q(x)" class="mathcode" src="https://images2.imgbox.com/e6/f7/dmLKhiwr_o.png"> <br><br> 因为我们可以证明<span style="color:#ed7976;">如果前向<img alt="q(x_t|x_{t-1})" class="mathcode" src="https://images2.imgbox.com/47/97/Pf61RA2Q_o.png">满足高斯分布且 <img alt="\beta _t" class="mathcode" src="https://images2.imgbox.com/0a/b3/2XhoeCw9_o.png"> 足够小，其逆向<img alt="q(x_{t-1}|x_t)" class="mathcode" src="https://images2.imgbox.com/37/16/6l3t7FQJ_o.png">仍然是一个高斯分布</span> <p>那样，我们便可以使用「参数为 θ 的U-Net+attention 结构<img alt="p_\theta" class="mathcode" src="https://images2.imgbox.com/3f/49/0dYzBbdi_o.png">」去预测这样的一个逆向的分布(类似VAE)：<br><img alt="p_{\theta}\left(X_{0: T}\right)=p\left(x_{t}\right) \prod_{t=1}^{T} p_{\theta}\left(x_{t-1} \mid x_{t}\right)" class="mathcode" src="https://images2.imgbox.com/31/d6/k6UOVFKs_o.png"><br><img alt="p_{\theta}\left(x_{t-1} \mid x_{t}\right)=\mathcal{N}\left(x_{t-1} ; \mu_{\theta}\left(x_{t}, t\right), \Sigma_{\theta}\left(x_{t}, t\right)\right)" class="mathcode" src="https://images2.imgbox.com/55/ed/EfaS5b3r_o.png"><br> 不过在DDPM的论文中，作者把条件概率 <img alt="p_\theta (x_{t-1}|x_t)" class="mathcode" src="https://images2.imgbox.com/88/f0/CMl4PPxa_o.png"> 的方差直接取了<img alt="\beta _t" class="mathcode" src="https://images2.imgbox.com/af/ce/jVnmqK01_o.png">，而不是上面说的需要网络去估计的 <img alt="\Sigma_{\theta}\left(x_{t}, \mathrm{t}\right)" class="mathcode" src="https://images2.imgbox.com/e8/8f/UGCxUMQV_o.png">，所以说实际上<span style="color:#ed7976;">只有均值需要网络去估计</span></p> </li><li> <p>然现在的问题是，我们无法直接去推断 <img alt="q(x_{t-1}|x_t)" class="mathcode" src="https://images2.imgbox.com/52/6e/S5ENa2IW_o.png">，即<img alt="q(x_{t-1}|x_t)" class="mathcode" src="https://images2.imgbox.com/ff/30/fERZglgX_o.png"> is unknown</p> <p class="img-center"><img alt="" height="211" src="https://images2.imgbox.com/fa/c9/2wAG1EBj_o.png" width="600"></p> <p>所以，接下来的问题 自然而然 就<span style="color:#be191c;">转换成了我们希望求解<img alt="q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)" class="mathcode" src="https://images2.imgbox.com/8e/f7/7qzdjRye_o.png"></span>，因为我们已知前向过程<img alt="q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\right)" class="mathcode" src="https://images2.imgbox.com/7e/2c/7NFcJM5S_o.png">，所以自然想到使用贝叶斯公式：</p> <p class="img-center"><img alt="q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)=\frac{q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\right) q\left(\mathbf{x}_{t-1}\right)}{q\left(\mathbf{x}_{t}\right)}" class="mathcode" src="https://images2.imgbox.com/42/ec/kjakhdr4_o.png"></p> <p>可惜 <img alt="q(x_t)" class="mathcode" src="https://images2.imgbox.com/c3/5e/MhqONPUA_o.png">和 <img alt="q(x_{t-1})" class="mathcode" src="https://images2.imgbox.com/9e/3e/c7zdyxhE_o.png">是未知的，事情到这里似乎走入了僵局，好在我们发现<img alt="q(x_t|x_0)" class="mathcode" src="https://images2.imgbox.com/4f/d0/fV9nw6aA_o.png">和 <img alt="q(x_{t-1}|x_0)" class="mathcode" src="https://images2.imgbox.com/0f/77/AOvzomXo_o.png">是已知的，这样一变换，下述等式右边的三项就都可知了<br><img alt="q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)=\frac{q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\right) q\left(\mathbf{x}_{t-1}|x_0\right)}{q\left(\mathbf{x}_{t}|x_0\right)}" class="mathcode" src="https://images2.imgbox.com/11/53/Pphc94a0_o.png"><br> 相当于如果给上式加上 <img alt="x_0" class="mathcode" src="https://images2.imgbox.com/f7/d9/mLxfcnJY_o.png"> 为条件，则立马柳暗花明，而一旦知道了 <img alt="x_0,q(x_{t-1}|x_t, x_0)" class="mathcode" src="https://images2.imgbox.com/de/36/fyAQqIcW_o.png">，便可以直接写出<img alt="q\left(x_{t-1} \mid x_{t}, x_{0}\right)=\mathcal{N}\left(x_{t-1} ; \tilde{\mu}\left(x_{t}, x_{0}\right), \tilde{\beta}_{t} \mathbf{I}\right)" class="mathcode" src="https://images2.imgbox.com/ae/45/lm93EyoT_o.png"></p> </li></ol> 
<h5>2.2.2.2 因<img alt="q(x_{t-1}|x_t)" class="mathcode" src="https://images2.imgbox.com/95/a8/lP2OMBBX_o.png">无法直接求解，故加上<img alt="x_0" class="mathcode" src="https://images2.imgbox.com/b3/d8/C7c1b0Yx_o.png">：问题转换成了求解<img alt="q\left(x_{t-1} \mid x_{t}, x_{0}\right)" class="mathcode" src="https://images2.imgbox.com/eb/b3/52MWAugc_o.png"></h5> 
<p>接下来，我们便好好推导下</p> 
<p class="img-center"><img alt="" height="293" src="https://images2.imgbox.com/bc/e2/qzq2Iifc_o.png" width="800"></p> 
<blockquote> 
 <p>解释下上面7.1~7.5这5个步骤的推导</p> 
 <ol><li> <p>7.1依据的是<br><img alt="P(A|B) = \frac{P(AB)}{P(B)}" class="mathcode" src="https://images2.imgbox.com/de/a5/bjxIJrgr_o.png"></p> </li><li>7.2中，分母部分依据的是<br><img alt="P(AB) = P(A)P(B|A)" class="mathcode" src="https://images2.imgbox.com/ba/a0/IijLqTq0_o.png"><br> 分子部分依据的是<br><img alt="P(ABC)=P(A)P(B|A)P(C|AB)" class="mathcode" src="https://images2.imgbox.com/88/7f/N3LX6l8M_o.png"><br><br> 注，此处的A B与上面7.1的A B非同一个具体的指向，只是公式层面的原有表达</li><li>7.3依据的是分子分母同时除以<img alt="q(x_0)" class="mathcode" src="https://images2.imgbox.com/84/14/VyxJY7lM_o.png"></li><li>至于7.3到7.4<br><img alt="\begin{array}{l} =q\left(x_{t} \mid x_{t-1}, x_{0}\right) \frac{q\left(x_{t-1} \mid x_{0}\right)}{q\left(x_{t} \mid x_{0}\right)} \quad 7.3 \\ \propto \exp \left(-\frac{1}{2}\left(\frac{\left(x_{t}-\sqrt{\alpha_{t}} x_{t-1}\right)^{2}}{\beta_{t}}+\frac{\left(x_{t-1}-\sqrt{\bar{\alpha}_{t-1}} x_{0}\right)^{2}}{1-\bar{a}_{t-1}}-\frac{\left(x_{t}-\sqrt{\bar{\alpha}_{t}} x_{0}\right)^{2}}{1-\bar{a}_{t}}\right)\right) \quad 7.4 \end{array}" class="mathcode" src="https://images2.imgbox.com/f8/17/5vVR1rDD_o.png"><br> 依据的是<br><img alt="\rho(\mathrm{x})=\frac{1}{\sqrt{2 \pi \sigma}} \mathrm{e}^{-\frac{1}{2}\left(\frac{\mathrm{x}-\mu}{\sigma}\right)^{2}}" class="mathcode" src="https://images2.imgbox.com/9a/bb/eBQETrop_o.png"><br> 且由前向扩散过程的特性『<em><strong><span style="color:#7b7f82;">别忘了2.2.1节中，有 <img alt="q\left(\mathbf{x}_{t} \mid \mathbf{x}_{0}\right)=\mathcal{N}\left(\mathbf{x}_{t} ; \sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0},\left(1-\bar{\alpha}_{t}\right) \mathbf{I}\right)" class="mathcode" src="https://images2.imgbox.com/57/df/flCAdy2S_o.png"></span></strong></em>』，可知<br><img alt="q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}, \mathbf{x}_{0}\right)=q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\right)=\mathcal{N}\left(\mathbf{x}_{t} ; \sqrt{1-\beta_{t}} \mathbf{x}_{t-1}, \beta_{t} \mathbf{I}\right)" class="mathcode" src="https://images2.imgbox.com/b5/c1/GS5SyW6q_o.png"><br><img alt="q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{0}\right)=\mathcal{N}\left(\mathbf{x}_{t-1} ; \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0},\left(1-\bar{\alpha}_{t-1}\right) \mathbf{I}\right)" class="mathcode" src="https://images2.imgbox.com/fc/51/T8Tg3k8U_o.png"><br><img alt="q\left(\mathbf{x}_{t} \mid \mathbf{x}_{0}\right)=\mathcal{N}\left(\mathbf{x}_{t} ; \sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0},\left(1-\bar{\alpha}_{t}\right) \mathbf{I}\right)" class="mathcode" src="https://images2.imgbox.com/52/63/8SHLyXNu_o.png"></li><li>最后，再解释下怎么从7.4到的7.5<br><img alt="\begin{array}{l} =q\left(x_{t} \mid x_{t-1}, x_{0}\right) \frac{q\left(x_{t-1} \mid x_{0}\right)}{q\left(x_{t} \mid x_{0}\right)} \quad 7.3 \\ \propto \exp \left(-\frac{1}{2}\left(\frac{\left(x_{t}-\sqrt{\alpha_{t}} x_{t-1}\right)^{2}}{\beta_{t}}+\frac{\left(x_{t-1}-\sqrt{\bar{\alpha}_{t-1}} x_{0}\right)^{2}}{1-\bar{a}_{t-1}}-\frac{\left(x_{t}-\sqrt{\bar{\alpha}_{t}} x_{0}\right)^{2}}{1-\bar{a}_{t}}\right)\right)\\ =\exp \left(-\frac{1}{2}(\underbrace{\left(\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar{\alpha}_{t-1}}\right) x_{t-1}^{2}}_{x_{t-1} \text {}}-\underbrace{\left(\frac{2 \sqrt{\alpha_{t}}}{\beta_{t}} x_{t}+\frac{2 \sqrt{\bar{a}_{t-1}}}{1-\bar{\alpha}_{t-1}} x_{0}\right) x_{t-1}}_{\text {}x_{t-1}\text{}}+\underbrace{C\left(x_{t}, x_{0}\right)})\right) \cdot 7.5 \end{array}" class="mathcode" src="https://images2.imgbox.com/c7/ac/h9v51BZh_o.png"><br> 先举一个最简单的例子，比如对于 <img alt="\frac{1}{2} (Ax^2 + Bx + C)" class="mathcode" src="https://images2.imgbox.com/c0/1b/DfDcVfkU_o.png">，稍加转化下即是<img alt="\frac{1}{2} A(x + \frac{B}{2A})^2 + C" class="mathcode" src="https://images2.imgbox.com/b6/38/aJysodGJ_o.png">，而这个<br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/d4/e3/GAFzG9Z1_o.png">  <img alt="A" class="mathcode" src="https://images2.imgbox.com/14/3d/wPSZGcUY_o.png">则对应于7.5中的<img alt="\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar{\alpha}_{t-1}}" class="mathcode" src="https://images2.imgbox.com/31/dd/itBNp9Tc_o.png"><br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/a6/61/Pyq2wG8h_o.png">  <img alt="B" class="mathcode" src="https://images2.imgbox.com/26/20/jWWr6Pf1_o.png">则对应于7.5中的<img alt="- (\frac{2 \sqrt{\alpha_{t}}}{\beta_{t}} x_{t}+\frac{2 \sqrt{\bar{a}_{t-1}}}{1-\bar{\alpha}_{t-1}} x_{0} )" class="mathcode" src="https://images2.imgbox.com/83/18/hqVvIEIo_o.png"><br> 且其均值为<img alt="- \frac{B}{2A}" class="mathcode" src="https://images2.imgbox.com/da/2a/L4K6bhCL_o.png">，方差为<img alt="\frac{1}{A}" class="mathcode" src="https://images2.imgbox.com/83/39/rBeIDgcz_o.png">，从而有<br><img alt="\mu_{t}\left(\mathbf{x}_{t}, \mathbf{x}_{0}\right)=\frac{-B}{2 A}" class="mathcode" src="https://images2.imgbox.com/ad/ff/ZzF86UQX_o.png"><br><img alt="\begin{array}{l} =\left(\frac{\sqrt{\alpha_{t}}}{\beta_{t}} \mathbf{x}_{t}+\frac{\sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_{t-1}} \mathbf{x}_{0}\right) /\left(\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar{\alpha}_{t-1}}\right) \\ =\left(\frac{\sqrt{\alpha_{t}}}{\beta_{t}} \mathbf{x}_{t}+\frac{\sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_{t-1}} \mathbf{x}_{0}\right) \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}} \cdot \beta_{t} \\ =\frac{\sqrt{\alpha_{t}}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_{t}} \mathbf{x}_{t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} \mathbf{x}_{0} \end{array}" class="mathcode" src="https://images2.imgbox.com/8c/eb/nT1enOEp_o.png"><br><img alt="\tilde{\beta _t} = \frac{1}{A} = 1 /\left(\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar{\alpha}_{t-1}}\right)=1 /\left(\frac{\alpha_{t}-\bar{\alpha}_{t}+\beta_{t}}{\beta_{t}\left(1-\bar{\alpha}_{t-1}\right)}\right)=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}} \cdot \beta_{t}" class="mathcode" src="https://images2.imgbox.com/57/0c/1FOVtoBq_o.png"></li></ol> 
</blockquote> 
<h5>2.2.2.3 <img alt="q(x_{t-1}|x_t, x_0)" class="mathcode" src="https://images2.imgbox.com/98/09/jTfomwpj_o.png">中方差和均值的推导</h5> 
<p>好，接下来关键来了</p> 
<p>根据<img alt="x_{t}=\sqrt{\bar{\alpha}_{t}} \boldsymbol{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \epsilon_{0}" class="mathcode" src="https://images2.imgbox.com/2c/7c/bNXDUO7A_o.png">，可知<img alt="x_{0}=\frac{1}{\sqrt{\bar{\alpha}_{t}}}\left(x_{t}-\sqrt{1-\bar{\alpha}_{t}} \epsilon_{0}\right)" class="mathcode" src="https://images2.imgbox.com/11/e6/CTH19sXM_o.png">，代入上面<img alt="\mu _t(x_t,x_0)" class="mathcode" src="https://images2.imgbox.com/bc/da/wtZZp1sg_o.png">的表达式 可得</p> 
<p style="text-align:center;"><img alt="{\mu}(x_t,x_0)=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}{\epsilon_0}\right)" class="mathcode" src="https://images2.imgbox.com/9c/27/QkAD2o8Y_o.png"></p> 
<blockquote> 
 <p>大部分文章对上面这个的推导都是一步到位的，但本文为细致起见，故还是一步步来推导下</p> 
 <ol><li>首先直接把<img alt="x_{0}=\frac{1}{\sqrt{\bar{\alpha}_{t}}}\left(x_{t}-\sqrt{1-\bar{\alpha}_{t}} \epsilon_{0}\right)" class="mathcode" src="https://images2.imgbox.com/39/32/KFovjVB2_o.png">和<img alt="\beta _t = 1-\alpha _t" class="mathcode" src="https://images2.imgbox.com/5d/75/WFvMdi8l_o.png">代入进去，可得 <p class="img-center"><img alt="\begin{aligned} \boldsymbol{\mu}_{}\left(\boldsymbol{x}_{t}, \boldsymbol{x}_{0}\right) &amp; =\frac{\sqrt{\alpha_{t}}\left(1-\bar{\alpha}_{t-1}\right) \boldsymbol{x}_{t}+\sqrt{\bar{\alpha}_{t-1}}\left(1-\alpha_{t}\right) \boldsymbol{x}_{0}}{1-\bar{\alpha}_{t}} \\ &amp; =\frac{\sqrt{\alpha_{t}}\left(1-\bar{\alpha}_{t-1}\right) \boldsymbol{x}_{t}+\sqrt{\bar{\alpha}_{t-1}}\left(1-\alpha_{t}\right) \frac{\boldsymbol{x}_{t}-\sqrt{1-\bar{\alpha}_{t}} \epsilon_{0}}{\sqrt{\bar{\alpha}_{t}}}}{1-\bar{\alpha}_{t}} \end{aligned}" class="mathcode" src="https://images2.imgbox.com/d0/3e/JwfO4YSc_o.png"></p> </li><li>接下来，我们可以进一步观察到 分子中的后半部分有<img alt="\sqrt{\bar{\alpha}_{t-1}}\left(1-\alpha_{t}\right) \frac{\boldsymbol{x}_{t}-\sqrt{1-\bar{\alpha}_{t}} \epsilon_{0}}{\sqrt{\bar{\alpha}_{t}}}" class="mathcode" src="https://images2.imgbox.com/c9/e8/RCkkINPm_o.png">这一项，怎么进一步化简呢？<br><em>接下来非常关键(<strong>截止23年5月份之前，暂时没看到有其他中英文资料解释了这个细节</strong>)</em><br><br> 好在之前有定义：<img alt="\bar{\alpha _t} = \prod_{i=1}^{t}\alpha _i" class="mathcode" src="https://images2.imgbox.com/88/cb/frJ1DrSx_o.png">，即<img alt="\sqrt{\bar{\alpha _t}} = \sqrt{\alpha _t} \sqrt{\alpha _{t-1}} \cdots \sqrt{\alpha _1}" class="mathcode" src="https://images2.imgbox.com/b7/be/Swqs0RIp_o.png">，从而有<img alt="\sqrt{\bar{\alpha_t}} = \sqrt{\alpha_t} \sqrt{\bar{\alpha_{t-1}}}" class="mathcode" src="https://images2.imgbox.com/60/c9/j505zCnD_o.png"><p>所以我们可以针对这一项<img alt="\sqrt{\bar{\alpha}_{t-1}}\left(1-\alpha_{t}\right) \frac{\boldsymbol{x}_{t}-\sqrt{1-\bar{\alpha}_{t}} \epsilon_{0}}{\sqrt{\bar{\alpha}_{t}}}" class="mathcode" src="https://images2.imgbox.com/31/cf/v3F07G8H_o.png">的分子分母同时除以<img alt="\sqrt{\bar{\alpha_{t-1}}}" class="mathcode" src="https://images2.imgbox.com/c6/cc/7NxzSv9O_o.png">，得到</p> <img alt="\mu _t(x_t,x_0) = \frac{\sqrt{\alpha_{t}}\left(1-\bar{\alpha}_{t-1}\right) \boldsymbol{x}_{t}+\left(1-\alpha_{t}\right) \frac{\boldsymbol{x}_{t}-\sqrt{1-\bar{\alpha}_{t}} \epsilon_{0}}{\sqrt{\alpha_{t}}}}{1-\bar{\alpha}_{t}}" class="mathcode" src="https://images2.imgbox.com/e0/c4/qAffPFS8_o.png"></li><li> <p>之后的推导就比较简单了</p> <p class="img-center"><img alt="u(x_t,x_0) \begin{array}{l} =\frac{\sqrt{\alpha_{t}}\left(1-\bar{\alpha}_{t-1}\right) \boldsymbol{x}_{t}}{1-\bar{\alpha}_{t}}+\frac{\left(1-\alpha_{t}\right) \boldsymbol{x}_{t}}{\left(1-\bar{\alpha}_{t}\right) \sqrt{\alpha_{t}}}-\frac{\left(1-\alpha_{t}\right) \sqrt{1-\bar{\alpha}_{t}} \boldsymbol{\epsilon}_{0}}{\left(1-\bar{\alpha}_{t}\right) \sqrt{\alpha_{t}}} \\ =\left(\frac{\sqrt{\alpha_{t}}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_{t}}+\frac{1-\alpha_{t}}{\left(1-\bar{\alpha}_{t}\right) \sqrt{\alpha_{t}}}\right) \boldsymbol{x}_{t}-\frac{\left(1-\alpha_{t}\right) \sqrt{1-\bar{\alpha}_{t}}}{\left(1-\bar{\alpha}_{t}\right) \sqrt{\alpha_{t}}} \boldsymbol{\epsilon}_{0} \\ =\left(\frac{\alpha_{t}\left(1-\bar{\alpha}_{t-1}\right)}{\left(1-\bar{\alpha}_{t}\right) \sqrt{\alpha_{t}}}+\frac{1-\alpha_{t}}{\left(1-\bar{\alpha}_{t}\right) \sqrt{\alpha_{t}}}\right) \boldsymbol{x}_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}} \sqrt{\alpha_{t}}} \boldsymbol{\epsilon}_{0} \end{array}" height="111" src="https://images2.imgbox.com/82/2e/cqOIxOWJ_o.png" width="500"></p> <p>以下分别对上面的三行公式做解释说明：<br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/b5/e3/snf35Wfy_o.png">  接着把上阶段2得到的式子的分子拆成三项，且三项中最后两项的分子分母同时乘以<img alt="\sqrt{\alpha _t}" class="mathcode" src="https://images2.imgbox.com/3d/a5/AyhXYpgV_o.png"><br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/b9/b8/450lElTh_o.png">  然后再把上一步骤中分子三项中的前两项通过提取出<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/77/9e/Nk4kKyDF_o.png">从而实现合并<br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/b7/06/LMDeRcVo_o.png">  前两项合并之后，再对前两项中第一项的分子分母同时乘以<img alt="\sqrt{\alpha _t}" class="mathcode" src="https://images2.imgbox.com/e1/1b/Iwnp6mTN_o.png">，然后对第三项的分子分母同时除以<img alt="\sqrt{1-\alpha _t}" class="mathcode" src="https://images2.imgbox.com/c3/89/Jy6VvPKK_o.png">，即可得<img alt="\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}} \sqrt{\alpha_{t}}} \boldsymbol{\epsilon}_{0}" class="mathcode" src="https://images2.imgbox.com/7f/8c/umRQXi3m_o.png">，原因很简单，因为：<img alt="1-\bar{\alpha_t} = \left ( \sqrt{1-\bar{\alpha _t}} \right )^2" class="mathcode" src="https://images2.imgbox.com/5c/97/5ucu8MBK_o.png"></p> </li><li> <p> 接下来，针对上面阶段3得到的式子的前两项再做合并，合并中用到了一个细节，即<img alt="\alpha _t \times \bar{\alpha _{t-1}} = \bar{\alpha _t}" class="mathcode" src="https://images2.imgbox.com/e1/88/9EWhdcnQ_o.png">，原因也同样很简单，根据上面阶段2出现的这个式子<img alt="\sqrt{\bar{\alpha_t}} = \sqrt{\alpha_t} \sqrt{\bar{\alpha_{t-1}}}" class="mathcode" src="https://images2.imgbox.com/32/38/BnHrTwSF_o.png">而来，再之后就更eazy 便不再赘述了</p> <p class="img-center"><img alt="\mu(x_t,x_0) \begin{array}{l} =\frac{\alpha_{t}-\bar{\alpha}_{t}+1-\alpha_{t}}{\left(1-\bar{\alpha}_{t}\right) \sqrt{\alpha_{t}}} \boldsymbol{x}_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}} \sqrt{\alpha_{t}}} \boldsymbol{\epsilon}_{0} \\ =\frac{1-\bar{\alpha}_{t}}{\left(1-\bar{\alpha}_{t}\right) \sqrt{\alpha_{t}}} \boldsymbol{x}_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}} \sqrt{\alpha_{t}}} \boldsymbol{\epsilon}_{0} \\ =\frac{1}{\sqrt{\alpha_{t}}} \boldsymbol{x}_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}} \sqrt{\alpha_{t}}} \boldsymbol{\epsilon}_{0} \end{array}" height="108" src="https://images2.imgbox.com/a7/33/RKzTn54p_o.png" width="400"></p> </li></ol> 
</blockquote> 
<p>总之，从最终得到的结果可以看出，在给定 <img alt="x_0" class="mathcode" src="https://images2.imgbox.com/c8/19/PTU9GRfb_o.png"> 的条件下，后验条件高斯分布的均值只和超参数<img alt="\alpha _t" class="mathcode" src="https://images2.imgbox.com/50/a2/l0VLtH3m_o.png">、<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/fb/f3/Yoyr2gfx_o.png">、<img alt="\epsilon_0" class="mathcode" src="https://images2.imgbox.com/13/80/bXwMgpl0_o.png">有关，即</p> 
<p style="text-align:center;"><img alt="{\mu}(x_t,x_0)=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}{\epsilon_0}\right)" class="mathcode" src="https://images2.imgbox.com/5a/03/H5eUaw21_o.png"></p> 
<p>方差只与超参数<img alt="\alpha" class="mathcode" src="https://images2.imgbox.com/cf/e1/ofMjQWMp_o.png">有关，即</p> 
<p class="img-center"><img alt="\tilde{\beta _t} = \frac{1}{A} = 1 /\left(\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar{\alpha}_{t-1}}\right)=1 /\left(\frac{\alpha_{t}-\bar{\alpha}_{t}+\beta_{t}}{\beta_{t}\left(1-\bar{\alpha}_{t-1}\right)}\right)=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}} \cdot \beta_{t}" class="mathcode" src="https://images2.imgbox.com/e7/b2/IrqZqRzC_o.png"></p> 
<p>从而通过以上的方差和均值，我们就<span style="color:#be191c;">得到了<img alt="q(x_{t-1}|x_t, x_0)" class="mathcode" src="https://images2.imgbox.com/1a/a5/JSxwPLoH_o.png">的解析形式</span></p> 
<h3 id="h_566618077_4">2.3 DDPM如何训练：最小化「噪声估计模型<img alt="\epsilon _\theta (x_t,t)" class="mathcode" src="https://images2.imgbox.com/b5/c8/m5UEkaFk_o.png">估计的噪声」与「真实噪声」之间的差距</h3> 
<p>继续下文之前，先总结一下</p> 
<blockquote> 
 <p>生成模型的本质是根据给定的样本(训练数据)生成新样本</p> 
 <ul><li>具体而言，给定一批训练数据<img alt="X" class="mathcode" src="https://images2.imgbox.com/f2/f2/dyY7MHMW_o.png">，假设其服从某种复杂的真实分布<img alt="p(x)" class="mathcode" src="https://images2.imgbox.com/27/d0/V1iaE4Y5_o.png">，则给定的训练数据可视为从该真实分布中采样的观测样本<img alt="x" class="mathcode" src="https://images2.imgbox.com/63/1b/NwWquZHe_o.png"></li><li>如果能从这些观测样本<img alt="x" class="mathcode" src="https://images2.imgbox.com/bb/64/mDDAZrtr_o.png">中估计出训练数据的真实分布，相当于就可以从该分布(估计出的接近真实分布的分布)中不断的采样出新的样本了，故说白了，生产模型的目标就是估计训练数据的真实分布，并假定其真实分布为<img alt="q(x)" class="mathcode" src="https://images2.imgbox.com/02/9a/FNCGzEEC_o.png"></li><li>从而问题自然而然就变成了尽可能缩小估计的分布<img alt="q(x)" class="mathcode" src="https://images2.imgbox.com/5d/a1/YXiKHRKO_o.png">与真实分布<img alt="p(x)" class="mathcode" src="https://images2.imgbox.com/37/7b/2kQ2ZJmn_o.png">之间的差距</li></ul> 
</blockquote> 
<p>接下来介绍这个模型要怎么优化，即网络该怎么训练：去估计分布<img alt="q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)" class="mathcode" src="https://images2.imgbox.com/c4/39/NAwbPY2n_o.png">的条件概率<img alt="p_\theta (x_{t-1}|x_t)" class="mathcode" src="https://images2.imgbox.com/cd/37/1ix09tAv_o.png">的均值<img alt="u_\theta (x_t,t)" class="mathcode" src="https://images2.imgbox.com/12/ff/DFx3wBAX_o.png"> 和方差<img alt="\Sigma_{\theta}\left(x_{t}, \mathrm{t}\right)" class="mathcode" src="https://images2.imgbox.com/35/6e/DuIbZcIz_o.png"></p> 
<p>与之前介绍的VAE相比，扩散模型的隐变量是和原始数据是同维度的，而且encoder(即扩散/加噪过程)是固定的</p> 
<p class="img-center"><img alt="" height="484" src="https://images2.imgbox.com/6e/17/qttypuq9_o.png" width="700"></p> 
<h4>2.3.1 确立目标函数</h4> 
<p>既然扩散模型是隐变量模型，那么我们可以基于<strong>变分推断</strong>来得到<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Evidence_lower_bound" rel="nofollow" title="variational lower bound">variational lower bound</a>（<strong>VLB</strong>，又称<strong>ELBO</strong>）作为最大化优化目标，当然实际训练时一般对VLB取负，即我们要最小化目标分布的负对数似然：</p> 
<p class="img-center"><img alt="\begin{aligned} -\log p_{\theta}\left(\mathbf{x}_{0}\right) &amp; \leq-\log p_{\theta}\left(\mathbf{x}_{0}\right)+D_{\mathrm{KL}}\left(q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right) \| p_{\theta}\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right)\right) \\ &amp; =-\log p_{\theta}\left(\mathbf{x}_{0}\right)+\mathbb{E}_{\mathbf{x}_{1: T} \sim q\left(\mathbf{x}_{\left.1: T \mid \mathbf{x}_{0}\right)}\right.}\left[\log \frac{q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{0: T}\right) / p_{\theta}\left(\mathbf{x}_{0}\right)}\right] \\ &amp; =-\log p_{\theta}\left(\mathbf{x}_{0}\right)+\mathbb{E}_{q}\left[\log \frac{q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{0: T}\right)}+\log p_{\theta}\left(\mathbf{x}_{0}\right)\right] \\ &amp; =\mathbb{E}_{q}\left[\log \frac{q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{0: T}\right)}\right] \end{aligned}" class="mathcode" height="195" src="https://images2.imgbox.com/1c/2e/NRArqvi4_o.png" width="600"></p> 
<blockquote> 
 <p>考虑到本文的定位起见，逐一解释下上面推导的每一行</p> 
 <ol><li> <p>第一行：由 KL 散度的非负性质（KL 散度始终大于等于零），我们得到如下不等式：<img alt="-\log p_{\theta}\left(\mathbf{x}_{0}\right) \leq-\log p_{\theta}\left(\mathbf{x}_{0}\right)+D_{\mathrm{KL}}\left(q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right) \| p_{\theta}\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right)\right)" class="mathcode" src="https://images2.imgbox.com/be/c5/a21h55R1_o.png"></p> </li><li> <p>第二行：将 KL 散度的定义代入上式可得</p> <p>其中 <img alt="E" class="mathcode" src="https://images2.imgbox.com/6d/be/jcXxq6kp_o.png"> 表示期望，即对分布 <img alt="q(x_{1:T} | x_0)" class="mathcode" src="https://images2.imgbox.com/8a/14/PD9N7M6T_o.png"> 中的所有可能值求期望</p> </li><li> <p>第三行：对上式进行简化，将<img alt="-log p_{\theta }(x_0)" class="mathcode" src="https://images2.imgbox.com/63/22/LQOJ1TqC_o.png">项移到期望内部</p> <p>其中 <img alt="E_q" class="mathcode" src="https://images2.imgbox.com/91/54/Lys8txlz_o.png">表示对分布 <img alt="q(x_{1:T} | x_0)" class="mathcode" src="https://images2.imgbox.com/39/be/DXyc4xk2_o.png"> 中的所有可能值求期望</p> </li><li> <p>第四行：<img alt="-log p_{\theta }(x_0)" class="mathcode" src="https://images2.imgbox.com/38/21/e6ZTdUKi_o.png"> 和 <img alt="+log p_{\theta }(x_0)" class="mathcode" src="https://images2.imgbox.com/1d/fd/VwzA1kFe_o.png"> 相互抵消可得</p> </li></ol> 
</blockquote> 
<p>令</p> 
<p style="text-align:center;"><img alt="\text { Let } L_{\mathrm{VLB}}=\mathbb{E}_{q\left(\mathbf{x}_{0: T)}\right.}\left[\log \frac{q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{0: T}\right)}\right] \geq-\mathbb{E}_{q\left(\mathbf{x}_{0}\right)} \log p_{\theta}\left(\mathbf{x}_{0}\right)" class="mathcode" src="https://images2.imgbox.com/06/f7/OviWqc1L_o.png"></p> 
<p>所以 <img alt="L_{VLB}" class="mathcode" src="https://images2.imgbox.com/4b/a0/LkNfXqVz_o.png">就是我们的上界，我们要最小化它，接着进行变形</p> 
<p class="img-center"><img alt="" height="542" src="https://images2.imgbox.com/a5/29/KlU3Jx6Y_o.png" width="800"></p> 
<blockquote> 
 <p>老规矩，上面整个推导总计九行，下面逐行解释下上面推导的每一行(纵使其他所有文章都不解释，本文也要给你解释的明明白白)</p> 
 <ol><li> <p>第一行，直接给出了 <img alt="L_{\mathrm{VLB}}" class="mathcode" src="https://images2.imgbox.com/c7/6b/xKy06nws_o.png"> 的定义，即<strong>计算概率分布 <img alt="q" class="mathcode" src="https://images2.imgbox.com/0d/2f/fiHm48f9_o.png"> 和<img alt="p_{\theta}" class="mathcode" src="https://images2.imgbox.com/a3/7f/kD3Lna37_o.png">之间的对数比值的期望</strong>(注意，这是咱们的目标)</p> <p class="img-center"><img alt="L_{\mathrm{VLB}}=\mathbb{E}_{q\left(\mathbf{x}_{0: T)}\right.}\left[\log \frac{q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{0: T}\right)}\right]" class="mathcode" src="https://images2.imgbox.com/05/09/LYzYL4MM_o.png"></p> <p><span style="color:#7b7f82;">考虑到七月SD课的一学员对第一步有疑问，再多解释一下<br> 在第1步中，公式的意思是对于所有可能的的<img alt="x_{0}, x_{1}, \ldots, x_{T}" class="mathcode" src="https://images2.imgbox.com/09/b8/E252zmvo_o.png">路径，我们要计算中括号内部表达式的期望值</span></p> <p><span style="color:#7b7f82;">这里是一个联合分布，表示所有<img alt="x_t(t=0,1,...,T)" class="mathcode" src="https://images2.imgbox.com/c8/e9/rZO9RBjX_o.png">一起的分布，而期望是在这个分布下计算的</span></p> <p><span style="color:#7b7f82;">这意味着我们在所有这个分布支持的路径上平均这个中括号里的量<br> 所以，如果​<img alt="x_{0}, x_{1}, \ldots, x_{T}" class="mathcode" src="https://images2.imgbox.com/c2/08/mz51nqUZ_o.png">都服从分布q，那么我们确实是在对中括号里的式子求整体期望</span></p> </li><li> <p>第二行，将条件概率 <img alt="q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right)" class="mathcode" src="https://images2.imgbox.com/92/11/cUnM5XSw_o.png"> 和联合概率 <img alt="p_{\theta}\left(\mathbf{x}_{0: T}\right)" class="mathcode" src="https://images2.imgbox.com/8b/0b/HFbk82VG_o.png"> 展开为一系列条件概率的乘积<br><img alt="q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right)=\prod_{t=1}^{T} q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\right)" class="mathcode" src="https://images2.imgbox.com/58/98/2GVntOQf_o.png"><br> 考虑到<img alt="p_{\theta}\left(\mathbf{x}_{0: T}\right)" class="mathcode" src="https://images2.imgbox.com/e9/33/L66t9kKE_o.png">实际上就是<img alt="p_{\theta}\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right)" class="mathcode" src="https://images2.imgbox.com/ff/0c/YaLm2A6O_o.png">，所以有<br><img alt="p_{\theta}\left(\mathbf{x}_{0: T}\right)= p_{\theta }(x_{0}) \prod_{t=1}^{T} p_{\theta }(x_{t} | x_{0: t-1}) = p_{\theta}\left(\mathbf{x}_{T}\right) \prod_{t=1}^{T} p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)" class="mathcode" src="https://images2.imgbox.com/0e/f4/HzCru5QN_o.png"></p> <p>然后把上述结果分别分别代入<img alt="q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right)" class="mathcode" src="https://images2.imgbox.com/61/c2/gZlwZXMd_o.png">和 <img alt="p_{\theta}(\mathbf{x}_{0:T})" class="mathcode" src="https://images2.imgbox.com/03/e7/gJyR2scv_o.png">，即可得到第二行的结果</p> <p class="img-center"><img alt="\mathbb{E}_{q}\left[\log \frac{\prod_{t=1}^{T} q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\right)}{p_{\theta}\left(\mathbf{x}_{T}\right) \prod_{t=1}^{T} p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)}\right]" class="mathcode" src="https://images2.imgbox.com/43/53/pYU6qAH2_o.png"></p> </li><li> <p>第三行，将乘积转换为求和，并将<img alt="p_\theta(\mathbf{x}_T)" class="mathcode" src="https://images2.imgbox.com/dd/0b/5HlU9Ygo_o.png">项移到前面</p> <p class="img-center"><img alt="\mathbb{E}_{q}\left[-\log p_{\theta}\left(\mathbf{x}_{T}\right)+\sum_{t=1}^{T} \log \frac{q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\right)}{p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)}\right]" class="mathcode" src="https://images2.imgbox.com/d8/1e/FjYsXych_o.png"></p> </li><li> <p>第四行，调整求和的范围，使其从2开始，从而达到将 <img alt="t=1" class="mathcode" src="https://images2.imgbox.com/01/31/ZcFHD8sG_o.png"> 的项分离出来的目的</p> <p class="img-center"><img alt="\mathbb{E}_{q}\left[-\log p_{\theta}\left(\mathbf{x}_{T}\right)+\sum_{t=2}^{T} \log \frac{q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\right)}{p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)}+\log \frac{q\left(\mathbf{x}_{1} \mid \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{0} \mid \mathbf{x}_{1}\right)}\right]" class="mathcode" src="https://images2.imgbox.com/bf/03/OXR1WtrP_o.png"></p> </li><li> 
   <hr><p><strong>第五行</strong>，将 <img alt="t" class="mathcode" src="https://images2.imgbox.com/a9/1b/DCN2WHxM_o.png"> 项的对数比值分解为两个对数比值的和，其中一个涉及<img alt="\mathbf{x}_{t-1}" class="mathcode" src="https://images2.imgbox.com/6d/09/APkOevPA_o.png"><em>和 </em><img alt="\mathbf{x}_{t}" class="mathcode" src="https://images2.imgbox.com/7b/6f/x2tAUr0s_o.png">，另一个涉及<img alt="\mathbf{x}_{t}" class="mathcode" src="https://images2.imgbox.com/dc/fd/krtNQo4n_o.png"><em>和 </em><img alt="\mathbf{x}_{0}" class="mathcode" src="https://images2.imgbox.com/6f/db/Oov9GHKg_o.png">，相当于<span style="color:#be191c;">补了个<img alt="x_0" class="mathcode" src="https://images2.imgbox.com/4f/97/LiC6oZ5Q_o.png"></span></p> <p class="img-center"><img alt="\mathbb{E}_{q}\left[-\log p_{\theta}\left(\mathbf{x}_{T}\right)+\sum_{t=2}^{T} \log \left(\frac{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}, \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)} \cdot \frac{q\left(\mathbf{x}_{t} \mid \mathbf{x}_{0}\right)}{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{0}\right)}\right)+\log \frac{q\left(\mathbf{x}_{1} \mid \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{0} \mid \mathbf{x}_{1}\right)}\right]" class="mathcode" src="https://images2.imgbox.com/58/42/ezKj0M0C_o.png"></p> <p><span style="color:#0d0016;">这里得着重解释下</span><br> 把第四行的第二项的分子和分母都乘以<img alt="q(\mathbf{x}_{t-1}|\mathbf{x}_0)" class="mathcode" src="https://images2.imgbox.com/2f/27/0pGGXQcG_o.png">，即得<br><span style="color:#0d0016;"><img alt="\log \frac{q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\right)}{p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)}=\log \left(\frac{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}, \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)} \cdot \frac{q\left(\mathbf{x}_{t} \mid \mathbf{x}_{0}\right)}{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{0}\right)}\right)" class="mathcode" src="https://images2.imgbox.com/ea/49/xc2fbn7B_o.png"></span></p> <p>这里面的关键是，即同时乘以<img alt="q(\mathbf{x}_{t-1}|\mathbf{x}_0)" class="mathcode" src="https://images2.imgbox.com/4a/0f/fLTnUb39_o.png">后，怎么就得到上式了呢，分母部分一目了然，直接乘上的<img alt="q(\mathbf{x}_{t-1}|\mathbf{x}_0)" class="mathcode" src="https://images2.imgbox.com/9d/83/3C7q8dk5_o.png">，但分子部分呢，<strong>明明应该是<img alt="q(x_t|x_{t-1}) q(x_{t-1}|x_0)" class="mathcode" src="https://images2.imgbox.com/94/64/LIIMcu25_o.png">，则就变成了这个呢：<img alt="q(x_{t-1}|x_t,x_0) q(x_t|x_0)" class="mathcode" src="https://images2.imgbox.com/d5/15/KgdQNIXy_o.png"></strong>？好问题! 原因在于这两个式子是等价的，即(定义为等式1)</p> <p class="img-center"><img alt="q(x_t|x_{t-1}) q(x_{t-1}|x_0) = q(x_{t-1}|x_t,x_0) q(x_t|x_0)" class="mathcode" src="https://images2.imgbox.com/cd/70/KOJ4CBkP_o.png"></p> <p>为何等价呢，或者说上面这个等式1是怎么来的？其实也简单，因有</p> <p>p(A, B, C) = p(A|B, C) p(B, C) = <strong>p(A|B, C) p(B|C)</strong> p(C)</p> <p>p(A, B, C) = p(B|A, C) p(A, C) =<strong> p(B|A, C) p(A|C)</strong> p(C)</p> <p>故有『<span style="color:#7b7f82;">下面五个等式先后依据：马尔科夫假设倒推、条件概率定义、分母中联合概率定义、分子中联合概率定义、分子分母同时约掉<img alt="q(x_0)" class="mathcode" src="https://images2.imgbox.com/06/a2/Cy5JEFzP_o.png"></span>』</p> <p class="img-center"><img alt="\begin{aligned} q\left(x_{t} \mid x_{t-1}\right) &amp;=q\left(x_{t} \mid x_{t-1}, x_{0}\right) \\&amp;= \frac{q\left(x_{t}, x_{t-1}, x_{0}\right)}{q\left(x_{t-1}, x_{0}\right)} \\&amp; =\frac{q\left(x_{t-1} \mid x_{t}, x_{0}\right) q\left(x_{t} \mid x_{0}\right) q\left(x_{0}\right)}{q\left(x_{t-1}, x_{0}\right)} \\ &amp; = \frac{q(x_{t-1}\mid x_{t},x_{0}) q(x_t\mid x_0) q(x_0)}{q(x_{t-1}\mid x_0)q(x_0)} \\ &amp; =\frac{q\left(x_{t-1} \mid x_{t}, x_{0}\right) q\left(x_{t} \mid x_{0}\right)}{q\left(x_{t-1} \mid x_{0}\right)} \end{aligned}" class="mathcode" src="https://images2.imgbox.com/f3/cb/5iwMNiun_o.png"></p> </li><li> 
   <hr><p>第六行，将第五行的中间项一分为二，即拆分为两个求和项</p> <p class="img-center"><img alt="\mathbb{E}_{q}\left[-\log p_{\theta}\left(\mathbf{x}_{T}\right)+\sum_{t=2}^{T} \log \frac{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}, \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)}+\sum_{t=2}^{T} \log \frac{q\left(\mathbf{x}_{t} \mid \mathbf{x}_{0}\right)}{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{0}\right)}+\log \frac{q\left(\mathbf{x}_{1} \mid \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{0} \mid \mathbf{x}_{1}\right)}\right]" class="mathcode" src="https://images2.imgbox.com/e6/0c/TqcGVXvU_o.png"></p> </li><li> <p><strong>第七行</strong>，将第五行中间部分得到的两个求和项的第二个求和项的最后一项<img alt="t = T" class="mathcode" src="https://images2.imgbox.com/b7/8d/Kw6Tpjvp_o.png">分离出来，说白了，将第二个求和项的范围调整为从1到<img alt="T-1" class="mathcode" src="https://images2.imgbox.com/88/39/MJas2Tsg_o.png">，啥意思呢<br> 首先，第五行中间部分的两个求和项可以表示为</p> <p class="img-center"><img alt="\sum_{t=2}^{T} \log \frac{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}, \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)}+\sum_{t=2}^{T} \log \frac{q\left(\mathbf{x}_{t} \mid \mathbf{x}_{0}\right)}{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{0}\right)} = \sum_{t=2}^{T} \left[ \log \frac{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}, \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)} + \log \frac{q\left(\mathbf{x}_{t} \mid \mathbf{x}_{0}\right)}{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{0}\right)} \right]" class="mathcode" src="https://images2.imgbox.com/36/11/zwt5o5TH_o.png"></p> <p>接下来，关键的一步在于，上面中括号里的第二个求和项在求和过程中相邻两项会相互抵消 「<span style="color:#7b7f82;">依据：<img alt="log_a{M/N} = log_aM - log_a N" class="mathcode" src="https://images2.imgbox.com/03/eb/ewdIphLX_o.png">」</span><br> 具体地，当<img alt="t=k" class="mathcode" src="https://images2.imgbox.com/16/23/NSG5HwO7_o.png">时的<img alt="q\left(\mathbf{x}_{k} \mid \mathbf{x}_{0}\right)" class="mathcode" src="https://images2.imgbox.com/05/ea/reaqY8xW_o.png">会和当<img alt="t=k+1" class="mathcode" src="https://images2.imgbox.com/84/fe/DFwQXWgH_o.png">时的<img alt="q\left(\mathbf{x}_{k} \mid \mathbf{x}_{0}\right)" class="mathcode" src="https://images2.imgbox.com/16/20/P93E9Jnt_o.png">相互抵消，这样的抵消会发生在每一对相邻的项上，从2到<img alt="T-1" class="mathcode" src="https://images2.imgbox.com/ad/83/6FGy5ixk_o.png">，最后，只剩下<img alt="t=T" class="mathcode" src="https://images2.imgbox.com/35/d4/8l6LuWkL_o.png">和<img alt="t=2" class="mathcode" src="https://images2.imgbox.com/d2/5d/Whka9C37_o.png">时的两项，即<img alt="\log \frac{q\left(\mathbf{x}_{T} \mid \mathbf{x}_{0}\right)}{q\left(\mathbf{x}_{1} \mid \mathbf{x}_{0}\right)}" class="mathcode" src="https://images2.imgbox.com/80/18/E2YsAgd7_o.png"><br> 从而得到最终整个第7行所示的结果，如下</p> <p class="img-center"><img alt="\mathbb{E}_{q}\left[-\log p_{\theta}\left(\mathbf{x}_{T}\right)+\sum_{t=2}^{T} \log \frac{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}, \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)}+\log \frac{q\left(\mathbf{x}_{T} \mid \mathbf{x}_{0}\right)}{q\left(\mathbf{x}_{1} \mid \mathbf{x}_{0}\right)}+\log \frac{q\left(\mathbf{x}_{1} \mid \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{0} \mid \mathbf{x}_{1}\right)}\right]" class="mathcode" src="https://images2.imgbox.com/4a/c8/Q6R8V8Z0_o.png"></p> </li><li> <p><strong>第八行</strong>，上一行第7行总共4个带log的项，把最后两个log项拆开成4个式子，抵消两个，还分别剩一个<img alt="logq(x_{T}|x_0)" class="mathcode" src="https://images2.imgbox.com/98/94/1wKM5u6Y_o.png">、一个<img alt="-logp_\theta(x_0|x_1)" class="mathcode" src="https://images2.imgbox.com/09/18/Pu8df7Cu_o.png">，然后<img alt="logq(x_{T}|x_0)" class="mathcode" src="https://images2.imgbox.com/01/71/LI2Elb1F_o.png">与最初4项中的第1项<img alt="-logp_\theta(x_T)" class="mathcode" src="https://images2.imgbox.com/45/e0/ivenVxu8_o.png">合并，即可得到整个第八行的结果</p> <p class="img-center"><img alt="\mathbb{E}_{q}\left[\log \frac{q\left(\mathbf{x}_{T} \mid \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{T}\right)}+\sum_{t=2}^{T} \log \frac{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}, \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)}-\log p_{\theta}\left(\mathbf{x}_{0} \mid \mathbf{x}_{1}\right)\right]" class="mathcode" src="https://images2.imgbox.com/4d/74/FEIedI2S_o.png"></p> </li><li> <p>第九行，将最后一项中的负号移到对数里面，并将整个表达式重写为一系列 KL 散度项的和，这些项分别为 <img alt="L_{T}" class="mathcode" src="https://images2.imgbox.com/46/6b/yL9Ni6UM_o.png">、<img alt="L_{t-1}" class="mathcode" src="https://images2.imgbox.com/ab/31/aRndwWv2_o.png"> 和 <img alt="L_{0}" class="mathcode" src="https://images2.imgbox.com/74/9f/PrZZbUXH_o.png"></p> <p class="img-center"><img alt="\mathbb{E}_{q}[\underbrace{D_{\mathrm{KL}}\left(q\left(\mathbf{x}_{T} \mid \mathbf{x}_{0}\right) \| p_{\theta}\left(\mathbf{x}_{T}\right)\right)}_{L_{T}}+\sum_{t=2}^{T} \underbrace{D_{\mathrm{KL}}\left(q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}, \mathbf{x}_{0}\right) \| p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)\right)}_{L_{t-1}}-\underbrace{\log p_{\theta}\left(\mathbf{x}_{0} \mid \mathbf{x}_{1}\right)}_{L_{0}}]" class="mathcode" src="https://images2.imgbox.com/aa/e2/gvdRextu_o.png"></p> <p><span style="color:#7b7f82;">考虑到七月SD课的一学员对这一步有疑问，再多解释一下<br> 对于第9步，KL散度是一种衡量两个概率分布p和q之间差异的方法，这里的公式是在计算q和p之间的KL散度的期望值<br> 尽管KL散度自己就是一个期望值的形式，但<strong>这里的<img alt="E_q" class="mathcode" src="https://images2.imgbox.com/b1/95/cBWQtqmN_o.png">指的是在分布q下对KL散度本身求期望</strong><br> 这意味着你不仅计算了q和p之间的差异，而且你要考虑<strong>所有q分布可能产生的不同序列<img alt="x_{0}, x_{1}, \ldots, x_{T}" class="mathcode" src="https://images2.imgbox.com/2f/5d/IaFcllmM_o.png">，对这些序列的KL散度进行平均</strong></span></p> </li></ol> 
 <p>最后得到的表达式表示了最初我们想求解的<img alt="L_{\mathrm{VLB}}" class="mathcode" src="https://images2.imgbox.com/e3/60/NbnmdGko_o.png">最终是一系列 KL 散度项之和，我们可以利用这个结果进行参数优化，使得两个概率分布之间的差异最小</p> 
</blockquote> 
<h4>2.3.2 <strong>拉近估计分布<img alt="p_\theta (x_{t-1}\mid x_t)" class="mathcode" src="https://images2.imgbox.com/09/e4/wcWAAPqJ_o.png"> 和真实后验分布<img alt="q(x_{t-1}|x_t,x_0)" class="mathcode" src="https://images2.imgbox.com/7c/40/75R1Esbi_o.png"> </strong></h4> 
<p>对于上面公式最后第九行得到的结果</p> 
<ol><li>首先，<img alt="L_T" class="mathcode" src="https://images2.imgbox.com/70/ec/NQbuOzKz_o.png">是和优化无关的(由于前向过程 <img alt="q" class="mathcode" src="https://images2.imgbox.com/8e/d1/WBc6MBZS_o.png"> 没有可学习参数，而 <img alt="x_T" class="mathcode" src="https://images2.imgbox.com/dc/ba/ftDJKywa_o.png"> 则是纯高斯噪声，因此 <img alt="L_T" class="mathcode" src="https://images2.imgbox.com/1c/33/q0YBWw06_o.png"> 可以当做常量忽略)，所以不用管，只用看右边的<img alt="L_{t-1}" class="mathcode" src="https://images2.imgbox.com/ca/ca/GUODO2wN_o.png"></li><li>然后，<img alt="L_{t-1}" class="mathcode" src="https://images2.imgbox.com/8a/20/yvbznEKR_o.png"> 是KL散度，则可以看做<strong>拉近估计分布<img alt="p_\theta (x_{t-1}\mid x_t)" class="mathcode" src="https://images2.imgbox.com/b6/c3/38GPAG39_o.png"> 和真实后验分布<img alt="q(x_{t-1}|x_t,x_0)" class="mathcode" src="https://images2.imgbox.com/90/91/2SNX6XOe_o.png"> 这两个分布之间的距离</strong>： <br> 对于真实后验分布 <img alt="q(x_{t-1}|x_t,x_0)" class="mathcode" src="https://images2.imgbox.com/38/99/APbCgbMx_o.png"> ，我们已经在上一节2.2.2节推导出其解析形式，这是一个高斯分布，其均值和方差为<br><img alt="\tilde{\mu}_{t}=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \epsilon_{0}\right)" class="mathcode" src="https://images2.imgbox.com/d6/72/4lzKvuFu_o.png"><br><img alt="\tilde{\beta}_{t}=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}} \cdot \beta_{t}" class="mathcode" src="https://images2.imgbox.com/f5/9c/Zixn5pPD_o.png"><br> 对于估计分布 <img alt="p_\theta (x_{t-1}\mid x_t)" class="mathcode" src="https://images2.imgbox.com/92/d0/np97w63e_o.png">，其是我们网络期望拟合的目标分布，也是一个高斯分布，均值用网络估计，方差被设置为了一个和 <img alt="\beta_t" class="mathcode" src="https://images2.imgbox.com/82/b4/rt0xbAXG_o.png"> 有关的常数<br><img alt="p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)=\mathcal{N}\left(\mathbf{x}_{t-1} ; \boldsymbol{\mu}_{\theta}\left(\mathbf{x}_{t}, t\right), \boldsymbol{\Sigma}_{\theta}\left(\mathbf{x}_{t}, t\right)\right)" class="mathcode" src="https://images2.imgbox.com/eb/dc/NOBE26Lj_o.png"></li><li>考虑到，如果有两个分布 p,q 都是高斯分布，则他们的KL散度为<br><img alt="K L(p, q)=\log \frac{\sigma_{2}}{\sigma_{1}}+\frac{\sigma_{1}^{2}+\left(\mu_{1}-\mu_{2}\right)^{2}}{2 \sigma_{2}^{2}}-\frac{1}{2}" class="mathcode" src="https://images2.imgbox.com/38/03/TLKFvzKl_o.png"><br> 然后因为这两个分布的方差全是常数，和优化无关『<em><span style="color:#7b7f82;">说白了，去掉方差<img alt="\sigma" class="mathcode" src="https://images2.imgbox.com/37/f7/RZnFa5ru_o.png">相关的项，只留下均值相关的 <img alt="(\mu _1 - \mu _2)^2" class="mathcode" src="https://images2.imgbox.com/99/02/qtQBwHtY_o.png"></span></em>』，所以其实优化目标就是两个分布均值的二范数，从而可得<img alt="" height="102" src="https://images2.imgbox.com/a5/37/rCK01HNs_o.png" width="600"> 
  <hr><strong><span style="color:#fe2c24;">“ </span></strong><span style="color:#7b7f82;"><strong>怎么来的？</strong>我再细致解释下，对于这个公式而言</span> <p><span style="color:#7b7f82;"><img alt="L_{t} =\mathbb{E}_{q}\left[\left\|\tilde{\boldsymbol{\mu}}_{t}\left(\mathbf{x}_{t}, \mathbf{x}_{0}\right)-\boldsymbol{\mu}_{\theta}\left(\mathbf{x}_{t}, t\right)\right\|^{2}\right]" class="mathcode" src="https://images2.imgbox.com/81/8e/CuHZeX1k_o.png"></span></p> <p><span style="color:#7b7f82;">这里的 <img alt="\mathbb{E}_{q}" class="mathcode" src="https://images2.imgbox.com/f8/60/xx2RWlvt_o.png">是在分布 <img alt="q" class="mathcode" src="https://images2.imgbox.com/0b/8f/0SXhsxbw_o.png"> 下的期望，当我们<img alt="\tilde{\mu _t}" class="mathcode" src="https://images2.imgbox.com/52/93/B5ZZLYtz_o.png">的表达式代入后，得到：<br><img alt="L_{t}=\mathbb{E}_{q}\left[\left\|\frac{1}{\sqrt{\alpha_{t}}}\left(\mathbf{x}_{t}-\frac{\beta_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \epsilon\right)-\boldsymbol{\mu}_{\theta}\left(\mathbf{x}_{t}, t\right)\right\|^{2}\right]" class="mathcode" src="https://images2.imgbox.com/c6/42/GojWSVNI_o.png"></span></p> <p><span style="color:#7b7f82;">在这个式子中，<img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/2b/53/rfD9Q9t1_o.png"> 是一个服从标准正态分布的随机变量，而<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/b1/74/HhoZMVLf_o.png">则取决于<img alt="x_0" class="mathcode" src="https://images2.imgbox.com/b9/c0/GuGGicGq_o.png">和<img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/a1/c5/4YxUiLh0_o.png">。因此，这个期望 <img alt="\mathbb{E}_{q}" class="mathcode" src="https://images2.imgbox.com/46/c1/HqrIq0zu_o.png">实际上是在 <img alt="x_0" class="mathcode" src="https://images2.imgbox.com/c9/27/dvyhOh9w_o.png">和<img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/ca/0f/85ErlFcR_o.png"> 的联合分布下的期望(在 <img alt="x_0" class="mathcode" src="https://images2.imgbox.com/1c/b3/8EKprDgV_o.png">和<img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/45/17/M8dP7qBt_o.png"> 的所有可能值上取平均)，于是我们得到：<br><img alt="L_{t}=\mathbb{E}_{\mathbf{x}_{0}, \epsilon}\left[\left\|\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_{t}\left(\mathbf{x}_{0}, \epsilon\right)-\frac{\beta_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \epsilon\right)-\boldsymbol{\mu}_{\theta}\left(\mathbf{x}_{t}\left(\mathbf{x}_{0}, \epsilon\right), t\right)\right\|^{2}\right]" class="mathcode" src="https://images2.imgbox.com/0a/99/5bPlJ8CP_o.png"><br><img alt="E_{x_0,\epsilon}" class="mathcode" src="https://images2.imgbox.com/f0/dd/M7Z4LZ62_o.png"> 代表就是在 <img alt="x_0" class="mathcode" src="https://images2.imgbox.com/b9/57/KuUsHev7_o.png">和<img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/2d/e3/4kfVyl70_o.png"> 的联合分布下的期望，<img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/0d/14/8xzELA1j_o.png"> 依然是从标准正态分布<img alt="N(0,1)" class="mathcode" src="https://images2.imgbox.com/96/4e/wVHm9LLH_o.png">中采样的噪声 </span><strong><span style="color:#fe2c24;">”</span></strong></p> </li><li>这个时候我们可以直接整个网络出来直接学习 <img alt="u_\theta (x_t,t)" class="mathcode" src="https://images2.imgbox.com/6a/2c/IhaRQgr5_o.png"> ，然后<img alt="u_\theta (x_t,t)" class="mathcode" src="https://images2.imgbox.com/1f/51/OcDaGPYU_o.png">再去预测<br><img alt="" height="40" src="https://images2.imgbox.com/17/62/t5pgMvXn_o.png" width="150"><br> 因为<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/2a/ad/QOc7MBeK_o.png">是<img alt="u_\theta" class="mathcode" src="https://images2.imgbox.com/b9/12/RaACnv8C_o.png"> 的输入，其它的量都是常数，所以其中的未知量其实只有<img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/f5/ee/bptNrI7h_o.png">，所以我们干脆把需要学习的<img alt="u_\theta (x_t,t)" class="mathcode" src="https://images2.imgbox.com/39/ab/M8Hj1hj5_o.png">定义成：<br><img alt="\boldsymbol{\mu}_{\theta}\left(\mathbf{x}_{t}, t\right)=\frac{1}{\sqrt{\alpha_{t}}}\left(\mathbf{x}_{t}-\frac{\beta_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \epsilon_{\theta}\left(\mathbf{x}_{t}, t\right)\right)" class="mathcode" src="https://images2.imgbox.com/5b/e4/5EuFNYfK_o.png"><br> 也就是说，不用网络预测<img alt="\tilde{u_t}(x_t,x_0)" class="mathcode" src="https://images2.imgbox.com/71/47/SmCH6BCk_o.png">，而是<strong><span style="color:#ed7976;">用网络<img alt="\epsilon _\theta (x_t,t)" class="mathcode" src="https://images2.imgbox.com/74/67/3RqcXeg7_o.png">先预测噪声<img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/7e/dc/b2qvE7L2_o.png"> </span></strong><span style="color:#ed7976;">(注意，这是个关键步骤)</span>，然后把预测出来的噪声带入到定义好的表达式中去计算出预测的均值即可</li><li>所以，最终把<img alt="\boldsymbol{\mu}_{\theta}\left(\mathbf{x}_{t}, t\right)" class="mathcode" src="https://images2.imgbox.com/7f/06/2cb7t09l_o.png">这个公式，代入到步骤3得到的公式中，可得：<br><img alt="" height="161" src="https://images2.imgbox.com/86/49/qrK6p0lv_o.jpg" width="700"><br> 经过这样一番推导之后就是个 L2 loss，网络的输入是一张和噪声线性组合的图片，然后要通过<img alt="\epsilon_{\theta}\left(\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \epsilon, t\right)" class="mathcode" src="https://images2.imgbox.com/84/c7/4TCtZiz1_o.png">估计出来这个噪声</li></ol> 
<p>由上可知，DDPM的关键是<strong>训练 <img alt="\epsilon _\theta (x_t,t)" class="mathcode" src="https://images2.imgbox.com/dc/49/e3SLhHFr_o.png">模型，使其预测的 <img alt="\hat{\epsilon }" class="mathcode" src="https://images2.imgbox.com/62/d7/l8xcEYx4_o.png">与真实用于破坏的<img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/f0/4d/cxATSm2R_o.png"> 相近</strong>，用L2距离刻画相近程度就好，总之，我们的Loss就是如下公式『<span style="color:#7b7f82;"><em>相当于训练时，网络输入为 <img alt="x_t" class="mathcode" src="https://images2.imgbox.com/72/ac/FX2aLno5_o.png">(由 <img alt="x_0" class="mathcode" src="https://images2.imgbox.com/63/0d/7irKJjlt_o.png"> 和噪声 <img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/9f/11/llxLgqmg_o.png"> 线性组合而成) 和时刻<img alt="t" class="mathcode" src="https://images2.imgbox.com/1b/2f/ndULMJ7W_o.png"> ，输出要尽可能的拟合输入的噪声 <img alt="\epsilon" class="mathcode" src="https://images2.imgbox.com/e9/a7/XfftRusQ_o.png"> (通过L2 loss约束)</em></span>』</p> 
<p class="img-center"><img alt="" height="54" src="https://images2.imgbox.com/97/8a/ljrPT3BT_o.png" width="600"></p> 
<p>下图可以总结噪声估计模型的训练过程 (<span style="color:#7b7f82;"><em>依然是经典的那一套：</em></span><em><span style="color:#ed7976;">对比预测噪声predicted noise与真实噪声true noise <img alt="\varepsilon" class="mathcode" src="https://images2.imgbox.com/11/3d/w9T1AWcU_o.png">之间的差距 建loss 反向传播</span><span style="color:#7b7f82;">，训练好之后，好预测噪声，毕竟模糊的图片减掉噪声 不就得到清晰的图片了么</span></em>)</p> 
<p class="img-center"><img alt="" height="679" src="https://images2.imgbox.com/49/52/kJ7K1G2J_o.png" width="600"></p> 
<p>而整个训练过程可如下图描述</p> 
<p class="img-center"><img alt="" height="419" src="https://images2.imgbox.com/83/77/wnvxmTmv_o.png" width="800"></p> 
<p>DDPM论文中对应的伪代码为</p> 
<p class="img-center"><img alt="" height="281" src="https://images2.imgbox.com/8e/19/C36aQXRJ_o.png" width="600"></p> 
<h3>2.4 如何通过训练好的DDPM生成图片</h3> 
<p>通过上文2.2节的最后，我们得知</p> 
<blockquote> 
 <p>从最终得到的结果可以看出，在给定 <img alt="x_0" class="mathcode" src="https://images2.imgbox.com/08/ab/FyvHY7TQ_o.png"> 的条件下</p> 
 <p>后验条件高斯分布的均值只和超参数<img alt="\alpha _t" class="mathcode" src="https://images2.imgbox.com/53/1d/AqkqQ7Pz_o.png">、<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/19/3f/rbCAnwDP_o.png">、<img alt="\epsilon_0" class="mathcode" src="https://images2.imgbox.com/48/11/eIFLtrLa_o.png">有关，即</p> 
 <p style="text-align:center;"><img alt="{\mu}(x_t,x_0)=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}{\epsilon_0}\right)" class="mathcode" src="https://images2.imgbox.com/31/a5/dIglchS4_o.png"></p> 
 <p>方差只与超参数<img alt="\alpha" class="mathcode" src="https://images2.imgbox.com/c9/3e/9I6V4ldv_o.png">有关，即</p> 
 <p class="img-center"><img alt="\tilde{\beta _t} = \frac{1}{A} = 1 /\left(\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar{\alpha}_{t-1}}\right)=1 /\left(\frac{\alpha_{t}-\bar{\alpha}_{t}+\beta_{t}}{\beta_{t}\left(1-\bar{\alpha}_{t-1}\right)}\right)=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}} \cdot \beta_{t}" class="mathcode" src="https://images2.imgbox.com/e2/92/SD3buipF_o.png"></p> 
 <p>从而通过以上的方差和均值，我们就得到了<img alt="q(x_{t-1}|x_t, x_0)" class="mathcode" src="https://images2.imgbox.com/a5/84/b0i8PZzt_o.png">的解析形式</p> 
</blockquote> 
<p>通过2.2节的最后，我们估计到了噪声估测模型 <img alt="\epsilon _\theta (x_t,t)" class="mathcode" src="https://images2.imgbox.com/fc/e3/i8VuG4Y8_o.png"> ，接下来要生成模型就很简单了。从N(0,1)中随机生成一个噪声作为<img alt="X_T" class="mathcode" src="https://images2.imgbox.com/ef/e9/fMDErcxJ_o.png">，然后再用该模型逐步从估测噪声，并用去噪公式逐渐恢复到 <img alt="x_0" class="mathcode" src="https://images2.imgbox.com/63/8e/qG4LjKru_o.png"> 即可，见如下伪代码</p> 
<p class="img-center"><img alt="" height="261" src="https://images2.imgbox.com/95/ca/qEr6zpHY_o.png" width="600"></p> 
<p>用通俗的语言来说，如何去噪生成清晰图片呢？如上述第4行代码所述，<span style="color:#ed7976;">比较模糊的图片<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/f3/74/JE07v1pR_o.png"> 减掉噪声估计器预测出来的噪声</span> (<span style="color:#7b7f82;">只是实际操作时，减掉的是与一个常数<img alt="\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}" class="mathcode" src="https://images2.imgbox.com/3e/0d/gKqYAeEM_o.png">相乘之后的噪声</span>)，得到的结果乘以<img alt="1/\sqrt{a_t}" class="mathcode" src="https://images2.imgbox.com/bf/86/pVryxKLU_o.png">之后，最后再加上一个带<img alt="\sigma_t" class="mathcode" src="https://images2.imgbox.com/13/ef/mbz5rVjC_o.png">的<img alt="z" class="mathcode" src="https://images2.imgbox.com/3c/0e/JS74JLke_o.png"></p> 
<p class="img-center"><img alt="" height="242" src="https://images2.imgbox.com/2e/4c/r05y3iw7_o.png" width="800"></p> 
<p>换言之，推理时，我们从各项独立的高斯分布<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/7d/be/47XTnSp7_o.png"> 开始，一共 <img alt="T" class="mathcode" src="https://images2.imgbox.com/7e/1c/SerR6Fbu_o.png"> 步，每一步其实都是用了一次reparameterization trick</p> 
<p>每一步具体来说，我们有了 <img alt="x_t" class="mathcode" src="https://images2.imgbox.com/d7/e4/oiowGoUF_o.png"> ， 想要得到 <img alt="x_{t-1}" class="mathcode" src="https://images2.imgbox.com/be/0c/2ku4ERFv_o.png">，因为我们之前逆扩散过程建模有：  </p> 
<p class="img-center"><img alt="p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)=\mathcal{N}\left(\mathbf{x}_{t-1} ; \mu_{\theta}\left(\mathbf{x}_{t}, t\right), \Sigma_{\theta}^{2}\left(\mathbf{x}_{t}, t\right)\right)" class="mathcode" src="https://images2.imgbox.com/a8/02/85idIPEX_o.png"></p> 
<p class="img-center"><img alt="=\mathcal{N}\left(\mathbf{x}_{t-1} ; \frac{1}{\sqrt{\alpha_{t}}}\left(\mathbf{x}_{t}-\frac{\beta_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \epsilon_{\theta}\left(\mathbf{x}_{t}, t\right)\right), \beta_{t}\right)" class="mathcode" src="https://images2.imgbox.com/9d/f8/WqwjMZYi_o.png"></p> 
<p>所以由reparameterization trick我们有：</p> 
<p class="img-center"><img alt="" height="75" src="https://images2.imgbox.com/a3/46/0eVEydcY_o.png" width="600"></p> 
<p>每一轮都这样迭代，最终就得到了生成的图片，如下图所示</p> 
<p class="img-center"><img alt="" height="827" src="https://images2.imgbox.com/4c/ab/BmegWtOA_o.png" width="600"></p> 
<hr> 
<h2>第三部分 DETR：首次通过结合CNN+Transformer端对端解决<strong>object detection</strong></h2> 
<p>注，本小节的内容主要参考科技猛兽<a class="link-info" href="https://zhuanlan.zhihu.com/p/340149804" rel="nofollow" title="此文">此文</a>的相关部分</p> 
<p>一般目标检测的任务是预测一系列的Bounding Box的坐标以及Label，而大多数检测器的具体做法是</p> 
<ul><li>要么基于proposal，比如RCNN系列的工作，类似Faster R-CNN、Mask R-CNN</li><li>要么基于anchor，比如YOLO</li></ul> 
<p>把问题构建成为一个分类和回归问题来间接地完成这个任务，但最后都会生成很多个预测框(确定框的坐标及框内是什么物体)，从而不可避免的出现很多冗余的框，而要去除这些冗余的框，则都需要做一个NMS(non-maximum suppersion，非极大值抑制)的后处理(使得最后调参不易、部署不易)，所以如果要是有一个端对端的模型，不需要做NMS之类的后处理 也不需要太多先验知识则该有多好</p> 
<p>而通过论文《End-to-End Object Detection with Transformers》提出的DETR则满足了大家这个期待，其取代了现在的模型需要手工设计的工作，效果不错且可扩展性强(在DETR上加个专用的分割头便可以做全景分割)，其解决的方法是把检测问题看做是一个集合预测的问题(即set prediction problem，说白了，各种预测框本质就是一个集合)，其基本流程如下图所示</p> 
<p class="img-center"><img alt="" height="157" src="https://images2.imgbox.com/70/96/TPxzoL7c_o.png" width="800"></p> 
<ol><li>CNN抽特征且拉直</li><li>全局建模，给到transformer-encoder去进一步学习全局信息<br> 通过借助Transformer中的的self-attention机制，可以显式地对一个序列中的所有elements两两之间的interactions进行建模或交互，如此就知道了图片中哪块是哪个物体，从而对于同一个物体只需出一个预测框即可</li><li>接着通过不带掩码机制的transformer-decoder生成很多预测框<br> 注意是并行预测(即并行出框，而不是像原始transformer预测下一个token时一个一个往外蹦)<br> 相当于一次性生成 <img alt="N" class="mathcode" src="https://images2.imgbox.com/7d/6f/zuITlOtP_o.png"> 个box prediction，其中 <img alt="N" class="mathcode" src="https://images2.imgbox.com/3a/a9/nQp1efM8_o.png">是一个事先设定的远远大于image中object个数的一个整数(比如100)</li><li>预测框和真实框做二分图匹配<br> 最后通过bipartite matching loss的方法，基于<u>预测的100个boxex和ground truth boxes的二分图</u>做匹配，计算loss的大小，从而使得预测的box的位置和类别更接近于ground truth<br> 当然，这第4步更多是做模型训练的时候用，如果模型训练好了去做推理时，该第4步就不需要了，可以直接在预测的100个框中设定个阈值，比如置信度大于0.7的预测框保留下来，其他视为背景物体而 舍弃</li></ol> 
<h3>3.1 DETR整体结构：backbone，encoder，decoder和FFN</h3> 
<h4>3.1.1 DETR结构之前两部分：backbone与encoder</h4> 
<p>更细致的讲，DETR整体结构可以分为四个部分：backbone，encoder，decoder和FFN，如下图所示</p> 
<p class="img-center"><img alt="" height="203" src="https://images2.imgbox.com/93/d1/kYwGQUiY_o.png" width="800"></p> 
<p>对于前两部分而言</p> 
<p>一开始的backbone面对的是 <img alt="x_{img} \in B \times 3 \times H_0 \times W_0" class="mathcode" src="https://images2.imgbox.com/0e/bc/l8OfciiT_o.png">维的图像，首先把它转换为<img alt="f \in R^{B \times C \times H \times W}" class="mathcode" src="https://images2.imgbox.com/3d/6b/3iuGk3Ou_o.png">维的feature map（一般来说，通道数<img alt="C = 2048" class="mathcode" src="https://images2.imgbox.com/84/c5/ditGmprV_o.png">或256，<img alt="H = \frac{H_0}{32}" class="mathcode" src="https://images2.imgbox.com/8b/71/qSbuATIr_o.png">，<img alt="W = \frac{W_0}{32}" class="mathcode" src="https://images2.imgbox.com/93/c5/hz5KsGOz_o.png">）</p> 
<p>然后由于encoder的输入是<img alt="f \in R^{B \times C \times H \times W}" class="mathcode" src="https://images2.imgbox.com/05/b3/ka2CwCVm_o.png">维的feature map，故正式输入encoder之前还需要依次进行以下过程(图源：科技猛兽)：</p> 
<p class="img-center"><img alt="" height="573" src="https://images2.imgbox.com/3a/ad/qmZ8EJ2v_o.png" width="800"></p> 
<ol><li><strong>通道数压缩</strong>(其实就是降维操作)<strong>：</strong>用 1×1 convolution处理将通道数channels数量从 <img alt="C" class="mathcode" src="https://images2.imgbox.com/29/6f/dtP8i9Cr_o.png"> 压缩到<img alt="d" class="mathcode" src="https://images2.imgbox.com/26/fa/WvBp62Zf_o.png">，即得到<img alt="z_0 \in R^{B \times d \times H \times W}" class="mathcode" src="https://images2.imgbox.com/05/b5/VQKDrju3_o.png">维的新feature map</li><li><strong>转化为序列化数据：</strong>将空间的维度(高<img alt="H" class="mathcode" src="https://images2.imgbox.com/69/71/y2yw1GR6_o.png">和宽<img alt="W" class="mathcode" src="https://images2.imgbox.com/95/1a/GQbdydub_o.png">)压缩为一个维度<img alt="HW" class="mathcode" src="https://images2.imgbox.com/17/5b/jfyY1Kwh_o.png">，即把上一步得到的<img alt="z_0 \in R^{B \times d \times H \times W}" class="mathcode" src="https://images2.imgbox.com/f6/2c/yMAjfP6q_o.png">(<img alt="d = 256" class="mathcode" src="https://images2.imgbox.com/2b/6a/UPCTDa0X_o.png">)维的feature map通过reshape成(<img alt="HW,B,256" class="mathcode" src="https://images2.imgbox.com/82/bb/6aIfOwlt_o.png">)维的feature map<br> 这步相当于把编码矩阵的维度是 <img alt="(B,256,H,W)" class="mathcode" src="https://images2.imgbox.com/d4/f8/tGJUG9VL_o.png"> ，序列化成维度为 <img alt="(HW,B,256)" class="mathcode" src="https://images2.imgbox.com/bd/66/YbYDI3mY_o.png">维的张量</li><li><strong>位置编码：</strong>在上一步得到了<img alt="z_0 \in R^{B \times d \times H \times W}" class="mathcode" src="https://images2.imgbox.com/56/ea/Kp2vpU4m_o.png">维的feature map之后，再对 <img alt="z_0 \in R^{B \times d \times H \times W}" class="mathcode" src="https://images2.imgbox.com/7a/93/8kQMgcZM_o.png">维的feature map做positional encoding，最后也做下reshape：高<img alt="H" class="mathcode" src="https://images2.imgbox.com/7d/d6/B7P7syIo_o.png">和宽<img alt="W" class="mathcode" src="https://images2.imgbox.com/38/9c/zDmVeEZV_o.png">压缩为一个维度<img alt="HW" class="mathcode" src="https://images2.imgbox.com/78/8b/1RFeircB_o.png">，使得其与上面input embedding维度是一致的</li></ol> 
<h4>3.1.2 详解DETR的位置编码</h4> 
<p>对于上节第三步的位置编码，再好好解释说明下<br> 首先，通过此文《<span style="color:#4d4d4d;"><a href="https://blog.csdn.net/v_JULY_v/article/details/130090649" title="从零实现Transformer的简易版与强大版：从300多行到3000多行">从零实现Transformer的简易版与强大版：从300多行到3000多行</a></span>》的1.1.2节可知，原始transformer中的Positional Encoding的表达式为：</p> 
<p class="img-center" style="margin-left:0;text-align:center;"><img alt="PE_{(pos,2i+1)} = cos\left ( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right )" src="https://images2.imgbox.com/7c/9d/nifirBUw_o.png"></p> 
<p class="img-center" style="margin-left:0;text-align:center;"><img alt="PE_{(pos,2i)} = sin\left ( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right )" src="https://images2.imgbox.com/42/5e/4PWOUDri_o.png"></p> 
<p>其中，<img alt="d" class="mathcode" src="https://images2.imgbox.com/48/88/5o479jKu_o.png"> 就是这个 <img alt="d \times HW" class="mathcode" src="https://images2.imgbox.com/10/cd/9RAvRyaI_o.png">维的feature map的第一维，<img alt="pos \in [1,HW]" class="mathcode" src="https://images2.imgbox.com/9d/70/9niA6HPo_o.png">表示token在sequence中的位置，sequence的长度是<img alt="HW" class="mathcode" src="https://images2.imgbox.com/dc/3a/2xwlTQh8_o.png">，例如第一个token 的 <img alt="pos = 0" class="mathcode" src="https://images2.imgbox.com/38/66/WT2mQqlu_o.png">，第二个token的<img alt="pos = 1" class="mathcode" src="https://images2.imgbox.com/e1/8a/a2v3AVAw_o.png"><br><img alt="i" class="mathcode" src="https://images2.imgbox.com/80/6e/12eA1vYi_o.png">，或者准确意义上是<img alt="2i" class="mathcode" src="https://images2.imgbox.com/a1/ce/cF3C6L3Z_o.png">和<img alt="2i+1" class="mathcode" src="https://images2.imgbox.com/b3/ca/GFLYDsaP_o.png">表示了Positional Encoding的维度，<img alt="i" class="mathcode" src="https://images2.imgbox.com/f9/0f/6rSC2OWq_o.png"> 的取值范围是：<img alt="[0,...,d_{model}/2]" class="mathcode" src="https://images2.imgbox.com/cb/61/pmW68o4o_o.png">，所以当 <img alt="pos" class="mathcode" src="https://images2.imgbox.com/0c/5a/YoYF1c2O_o.png"> 为1时，对应的Positional Encoding可以写成(<span style="color:#7b7f82;"><em>注意到</em></span><img alt="d = 256" class="mathcode" src="https://images2.imgbox.com/57/78/m6scCvq0_o.png">)：</p> 
<p class="img-center" style="margin-left:0;text-align:center;"><img alt="PE_1 = [sin(\frac{1}{10000^{\frac{0}{256}}}),cos(\frac{1}{10000^{\frac{0}{256}}}), sin(\frac{1}{10000^{\frac{2}{256}}}),cos(\frac{1}{10000^{\frac{2}{256}}}), sin(\frac{1}{10000^{\frac{4}{256}}}), cos(\frac{1}{10000^{\frac{4}{256}}}),..., sin(\frac{1}{10000^{\frac{255}{256}}}),cos(\frac{1}{10000^{\frac{255}{256}}})]" class="mathcode" src="https://images2.imgbox.com/0e/ff/tqqA12MN_o.png"></p> 
<p>其次，DETR与原版transformer中的位置编码有两点不同</p> 
<ol><li><strong>第一点不同的是，原版Transformer只考虑 <img alt="x" class="mathcode" src="https://images2.imgbox.com/23/45/4JLCO9Vd_o.png"> 方向的位置编码，但是DETR考虑了 <img alt="xy" class="mathcode" src="https://images2.imgbox.com/6a/c4/xvcRSP5e_o.png"> 方向的位置编码</strong><span style="color:#4d4d4d;">，因为图像特征是2-D特征。采用的依然是 sin cos 模式，但是需要考虑 </span><img alt="xy" class="mathcode" src="https://images2.imgbox.com/7d/1d/v78yurwL_o.png"><span style="color:#4d4d4d;"> 两个方向。不是类似vision transoformer做法简单的将其拉伸为 </span><img alt="d \times HW" class="mathcode" src="https://images2.imgbox.com/88/5f/IFeQmED6_o.png"><span style="color:#4d4d4d;">，然后从 </span><img alt="[1,HW]" class="mathcode" src="https://images2.imgbox.com/75/83/1jBHmz7g_o.png"><span style="color:#4d4d4d;">进行长度为256的位置编码，而是考虑了 </span><img alt="xy" class="mathcode" src="https://images2.imgbox.com/fb/25/oeYQ4mm3_o.png"><span style="color:#4d4d4d;"> 方向同时编码，每个方向各编码128维向量，这种编码方式更符合图像特点</span><br><br> Positional Encoding的输出张量是：<span style="color:#4d4d4d;"><img alt="(B,d,H,W)" class="mathcode" src="https://images2.imgbox.com/7a/3a/7NV7AD4p_o.png"></span><span style="color:#4d4d4d;">,</span><span style="color:#4d4d4d;"><img alt="d = 256" class="mathcode" src="https://images2.imgbox.com/be/0b/wVGmSze3_o.png"></span><span style="color:#4d4d4d;"> ，其中 </span><span style="color:#4d4d4d;"><img alt="d" class="mathcode" src="https://images2.imgbox.com/db/91/J6AHXie8_o.png"></span><span style="color:#4d4d4d;"> 代表位置编码的长度， </span><span style="color:#4d4d4d;"><img alt="H,W" class="mathcode" src="https://images2.imgbox.com/2b/6c/IF7b1drd_o.png"></span><span style="color:#4d4d4d;">代表张量的位置。意思是说，这个特征图上的任意一个点 </span><span style="color:#4d4d4d;"><img alt="(H_1,W_1)" class="mathcode" src="https://images2.imgbox.com/67/78/1KdNwNKc_o.png"></span><span style="color:#4d4d4d;">有个位置编码，这个编码的长度是256，其中，前128维代表 </span><span style="color:#4d4d4d;"><img alt="H_1" class="mathcode" src="https://images2.imgbox.com/c7/25/O782TqKK_o.png"></span><span style="color:#4d4d4d;"> 的位置编码，后128维代表<img alt="W_1" class="mathcode" src="https://images2.imgbox.com/6e/54/2wXtJSNM_o.png"> 的位置编码</span> <p class="img-center"><img alt="a) P E_{\left(p_{x}, 2 i\right)}=\sin \left(\operatorname{pos}_{x} / 10000^{2 i / 128}\right) \\\ b) P E_{\left(p_{0}, 2 i+1\right)}=\cos \left(\operatorname{pos}_{x} / 10000^{2 i / 128}\right) \\c) P E_{\left(p o s_{y}, 2 i\right)}=\sin \left(\operatorname{pos}_{y} / 10000^{2 i / 128}\right) \\ d) P E_{\left(p o s_{y}, 2 i+1\right)}=\cos \left(\operatorname{pos}_{y} / 10000^{2 i / 128}\right)" class="mathcode" src="https://images2.imgbox.com/74/29/JnkkHiFY_o.png"></p> 假设你想计算任意一个位置<span style="color:#4d4d4d;"><img alt="(pos_x,pos_y)" class="mathcode" src="https://images2.imgbox.com/3e/0c/ewC5whl5_o.png"></span><span style="color:#4d4d4d;"> ，</span><span style="color:#4d4d4d;"><img alt="pos_x \in[1,HW]" class="mathcode" src="https://images2.imgbox.com/49/fb/SFvZOiza_o.png"></span><span style="color:#4d4d4d;">，</span><span style="color:#4d4d4d;"><img alt="pos_y \in[1,HW]" class="mathcode" src="https://images2.imgbox.com/14/d6/AnBO0dhb_o.png"></span><span style="color:#4d4d4d;">的Positional Encoding</span><br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/9d/6e/3VziF1ro_o.png"> 先把<span style="color:#4d4d4d;"><img alt="pos_x" class="mathcode" src="https://images2.imgbox.com/d4/36/cqwbxqFs_o.png"></span><span style="color:#4d4d4d;">代入上面4个公式中的</span><span style="color:#4d4d4d;"><img alt="a" class="mathcode" src="https://images2.imgbox.com/e4/bc/JslMxzDz_o.png"></span><span style="color:#4d4d4d;"> 式和 </span><span style="color:#4d4d4d;"><img alt="b" class="mathcode" src="https://images2.imgbox.com/89/1d/3V0QvqvE_o.png"></span><span style="color:#4d4d4d;"> 式可以计算得到128维的向量，它代表 </span><span style="color:#4d4d4d;"><img alt="pos_x" class="mathcode" src="https://images2.imgbox.com/15/46/7pm0IDPh_o.png"></span><span style="color:#4d4d4d;"> 的位置编码</span><br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/af/52/DhAJB8d7_o.png"> 再把 <span style="color:#4d4d4d;"><img alt="pos_y" class="mathcode" src="https://images2.imgbox.com/1d/52/eBqfEPvg_o.png"></span><span style="color:#4d4d4d;"> 代入上面4个公式中的 </span><span style="color:#4d4d4d;"><img alt="c" class="mathcode" src="https://images2.imgbox.com/bd/62/tZ7wjOG5_o.png"></span><span style="color:#4d4d4d;"> 式和</span><span style="color:#4d4d4d;"><img alt="d" class="mathcode" src="https://images2.imgbox.com/03/22/WgI2B6p1_o.png"></span><span style="color:#4d4d4d;"> 式可以计算得到128维的向量，它代表 </span><span style="color:#4d4d4d;"><img alt="pos_y" class="mathcode" src="https://images2.imgbox.com/fb/02/bCjINne1_o.png"></span><span style="color:#4d4d4d;"> 的位置编码</span><br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/75/1a/rig4cgsb_o.png"> 把这2个128维的向量拼接起来，就得到了一个256维的向量，它代表<span style="color:#4d4d4d;"><img alt="(pos_x,pos_y)" class="mathcode" height="20" src="https://images2.imgbox.com/52/33/Fq1X1io7_o.png" width="88"></span><span style="color:#4d4d4d;">的位置编码</span><br><span style="color:#4d4d4d;">从而</span>计算所有位置的编码，就得到了<span style="color:#4d4d4d;"><img alt="(256,H,W)" class="mathcode" src="https://images2.imgbox.com/fc/ef/pZEgDVa2_o.png"></span><span style="color:#4d4d4d;">的张量，代表这个batch的位置编码</span></li><li><strong>第二点不同的是，原版Transformer只在Encoder之前使用了Positional Encoding，而且是在输入上进行Positional Encoding，再把输入经过transformation matrix变为Query，Key和Value这几个张量</strong><br><strong>但是DETR在Encoder的每一个Multi-head Self-attention之前都使用了Positional Encoding，且只对Query和Key使用了Positional Encoding，即：只把维度为<img alt="(HW,B,256)" class="mathcode" src="https://images2.imgbox.com/c1/9c/OFhdgx85_o.png">维的位置编码与维度为<img alt="(HW,B,256)" class="mathcode" src="https://images2.imgbox.com/bc/a0/tYYk2khw_o.png">维的Query和Key相加，而不与Value相加</strong><br><br> 下图为DETR的Transformer的详细结构，读者可以对比下原版Transformer的结构 可以发现，除了Positional Encoding设置的不一样外，Encoder其他的结构是一致的。每个Encoder Layer包含一个multi-head self-attention 的module和一个前馈网络Feed Forward Network</li></ol> 
<blockquote> 
 <p><img alt="" height="460" src="https://images2.imgbox.com/77/82/Pmcm4Htz_o.png" width="344"><img alt="" height="460" src="https://images2.imgbox.com/09/72/YZoY1E2q_o.png" width="578"></p> 
</blockquote> 
<p> 所以，了解了DETR的位置编码之后，你应该明白了其实input embedding和位置编码维度其实是一样的，从而也就可以直接相加，使得Encoder最终输出的是<img alt="(HW,b,256)" class="mathcode" src="https://images2.imgbox.com/ae/8f/2LuxDeB2_o.png">维的编码矩阵Embedding，按照原版Transformer的做法，把这个东西给Decoder</p> 
<h4>3.1.3 DETR结构的后两部分：decoder和FFN</h4> 
<p>通过上节最后的对比图，可知DETR的Decoder和原版Transformer的decoder也是不太一样的</p> 
<ul><li>对于原版Transformer，其decoder的最后一个框(上节最后对比图的左图右上角所示)：output probability，代表我们一次只产生一个单词的softmax，根据这个softmax得到这个单词的预测结果，即：predicts the output sequence one element at a time</li><li>不同的是，DETR的Transformer Decoder是一次性处理全部的object queries(上节最后对比图的右图右上角所示)，即一次性输出全部的predictions(<span style="color:#7b7f82;"><em>而不像原始的Transformer是auto-regressive的，从左到右一个词一个词地输出</em></span>)，即：decodes the N objects in parallel at each decoder layer</li></ul> 
<p>至于DETR的Decoder主要有两个输入：</p> 
<ol><li>第一个输入是Transformer Encoder输出的Embedding与 position encoding(<em><span style="color:#7b7f82;">在下图右侧第二个multi-head self-attention处</span></em>)相加之后 给到<img alt="K" class="mathcode" src="https://images2.imgbox.com/2b/54/7mczF60f_o.png"><br> 其中的Embedding就是上文提到的<strong><strong><img alt="(HW,B,256)" class="mathcode" src="https://images2.imgbox.com/2a/15/OF9c40p9_o.png"></strong></strong>的编码矩阵</li><li>第二个输入是Object queries<br> 所谓Object queries是一个维度为<img alt="(100,b,256)" class="mathcode" src="https://images2.imgbox.com/5f/14/OFhqAK6j_o.png">维的张量，数值类型是nn.Embedding(意味着是可以学习的)。Object queries矩阵内部通过学习建模了100个物体之间的全局关系，例如房间里面的桌子旁边(A类)一般是放椅子(B类)，而不会是放一头大象(C类)，那么在推理时候就可以利用该全局注意力更好的进行解码预测输出 <p class="img-center"><img alt="" height="570" src="https://images2.imgbox.com/06/e3/GwaO2RF9_o.png" width="600"></p> <u>关于上图右侧第1个multi-head self-attention的Q K V</u><br> 如上图所示，它的Q K是这么来的：<span style="color:#7b7f82;">Decoder原本的输入一开始初始化成维度为<img alt="(100,b,256)" class="mathcode" src="https://images2.imgbox.com/b4/da/tcWq8WLR_o.png">维的全部元素都为0的张量，然后和Object queries加在一起</span>之后充当第1个multi-head self-attention的<span style="color:#4da8ee;">Query</span>和<span style="color:#4da8ee;">Key</span>，至于<span style="color:#4da8ee;">Value</span>则是Decoder原本的输入，也就是全0的张量<br><br><u>关于上图右侧第2个multi-head self-attention的Q K V</u><br> 它的<span style="color:#4da8ee;">Key</span>和<span style="color:#4da8ee;">Value</span>来自Encoder的输出张量，维度为<img alt="(hw,b,256)" class="mathcode" src="https://images2.imgbox.com/37/f6/33N8rAGr_o.png">，其中Key值还进行位置编码(正如上面第一个输入所述)<br> 至于其<span style="color:#4da8ee;">Query</span>值一部分来自第1个Add and Norm的输出，维度为<img alt="(100,b,256)" class="mathcode" src="https://images2.imgbox.com/31/c4/Y73AD7W3_o.png">的张量，另一部分来自Object queries，充当可学习的位置编码<br> 所以，第2个multi-head self-attention的Key和Value的维度为<img alt="(hw,b,256)" class="mathcode" src="https://images2.imgbox.com/ff/7d/wwNHx3NC_o.png">，而Query的维度为<img alt="(100,b,256)" class="mathcode" src="https://images2.imgbox.com/65/77/a39w0F9X_o.png"></li></ol> 
<p>每个Decoder的输出维度为<img alt="(1,b,100,256)" class="mathcode" src="https://images2.imgbox.com/46/7c/qj5zzNuM_o.png">，送入后面的前馈网络</p> 
<p>到这里你会发现：Object queries充当的其实是位置编码的作用，只不过它是可以学习的位置编码，所以，我们对Encoder和Decoder的每个self-attention的Query和Key的位置编码做个归纳，如下图所示，Value没有位置编码</p> 
<p class="img-center"><img alt="" height="211" src="https://images2.imgbox.com/74/e6/zQ1nEKog_o.png" width="600"></p> 
<h4><span style="color:#000000;">3.1.4 </span>损失函数部分解读</h4> 
<p>得到了Decoder的输出以后，如前文所述，应该是输出维度为<img alt="(b,100,256)" class="mathcode" src="https://images2.imgbox.com/10/2f/aTOk1g29_o.png">的张量。接下来要送入2个前馈网络FFN得到class和Bounding Box(如再度引用的下图所示)，它们会得到 <img alt="N=100" class="mathcode" src="https://images2.imgbox.com/c7/75/qq9AaSml_o.png"> 个预测目标 包含类别和Bounding Box(<span style="color:#7b7f82;">当然这个100肯定是大于图中的目标总数的，如果不够100，则采用背景填充</span></p> 
<p class="img-center"><img alt="" height="209" src="https://images2.imgbox.com/ef/c1/zJYRwDte_o.png" width="800"></p> 
<p>所以，DETR输出张量的维度为<strong><img alt="(b,100,class+1)" class="mathcode" src="https://images2.imgbox.com/f9/4a/TY650zX9_o.png"></strong>，和<strong><img alt="(b,100,4)" class="mathcode" src="https://images2.imgbox.com/5b/ee/DG4LshC5_o.png"></strong>『<span style="color:#7b7f82;"><em>对应COCO数据集来说：<img alt="class+1=92" class="mathcode" src="https://images2.imgbox.com/3c/25/aD41nSqj_o.png"> ， 4 指的是每个预测目标归一化的<img alt="(c_x,c_y,w,h)" class="mathcode" src="https://images2.imgbox.com/56/1e/Q6ZPFfxl_o.png">，归一化就是除以图片宽高进行归一化</em></span>』，对于这两个维度</p> 
<ul><li>前者代表分类分支，指100个预测框的类型</li><li>后者代表回归分支，指100个预测框的Bounding Box(<span style="color:#7b7f82;">仅仅计算有物体位置，背景集合忽略</span>)</li></ul> 
<p>但是读者可能会有疑问：预测框和真值是怎么一一对应的？换句话说：你怎么知道第47个预测框对应图片里的狗，第88个预测框对应图片里的车？..</p> 
<ol><li>这就需要用到经典的双边匹配算法了，也就是常说的匈牙利算法，该算法广泛应用于最优分配问题<br> 一幅图片，我们把第 <img alt="i" class="mathcode" src="https://images2.imgbox.com/ff/12/1oD1Nyqh_o.png"> 个物体的真值表达为 <img alt="y_i = (c_i,b_i)" class="mathcode" src="https://images2.imgbox.com/0a/3a/mZr4OIHV_o.png">，其中，<img alt="c_i" class="mathcode" src="https://images2.imgbox.com/36/33/GnecFj0y_o.png">表示它的 class ，<img alt="b_i" class="mathcode" src="https://images2.imgbox.com/90/9e/YvEnmYZv_o.png"> 表示它的 Bounding Box<br> 然后定义<img alt="\hat{y}=\left\{\hat{y}_{i}\right\}_{i=1}^{N}" class="mathcode" src="https://images2.imgbox.com/c8/3b/7afOlzyB_o.png">为网络输出的<img alt="N" class="mathcode" src="https://images2.imgbox.com/43/44/QRlBtVDg_o.png"> 个预测值<br> 对于第<img alt="i" class="mathcode" src="https://images2.imgbox.com/85/d9/wCN1CDJe_o.png"> 个真值<img alt="GT" class="mathcode" src="https://images2.imgbox.com/17/ab/1x25kciY_o.png"> ，<strong><img alt="\sigma (i)" class="mathcode" src="https://images2.imgbox.com/a4/3f/YVXf9NG9_o.png"></strong>为匈牙利算法得到的与真值<img alt="GT_i" class="mathcode" src="https://images2.imgbox.com/fb/d5/7qmowGCQ_o.png"> 对应的预测值prediction的索引，举个例子，比如 <img alt="i = 3" class="mathcode" src="https://images2.imgbox.com/ea/72/uydXXcZw_o.png">，<img alt="\sigma (i) = 18" class="mathcode" src="https://images2.imgbox.com/36/72/AOixDF0g_o.png"> ，意思就是：与第3个真值对应的预测值是第18个</li><li>那如何根据匈牙利算法找到与每个真值对应的预测值到底是哪个呢？ <p class="img-center"><img alt="\hat{\sigma}=\arg \min _{\sigma \in \Sigma_{N}} \sum_{i}^{N} L_{\text {match }}\left(y_{i}, \hat{y}_{\sigma(i)}\right)" class="mathcode" src="https://images2.imgbox.com/3b/00/JgC8oHzb_o.png"></p> 对于某一个真值<img alt="y_i" class="mathcode" src="https://images2.imgbox.com/36/b9/Aejh7fUa_o.png"> ，假设已经找到这个真值对应的预测值<img alt="\hat{y}_{\sigma (i)}" class="mathcode" src="https://images2.imgbox.com/8a/98/PhT45hLa_o.png">，这里的<img alt="\Sigma _N" class="mathcode" src="https://images2.imgbox.com/e9/3a/XHCdIiVl_o.png">是所有可能的排列(代表从真值索引到预测值索引的所有的映射)，然后用<img alt="L_{match}" class="mathcode" src="https://images2.imgbox.com/01/44/s3V13vdM_o.png">最小化<img alt="y_i" class="mathcode" src="https://images2.imgbox.com/66/7e/HNu9AC5C_o.png">和<img alt="\hat{y}_{\sigma (i)}" class="mathcode" src="https://images2.imgbox.com/cc/e0/p7LPzNWt_o.png">的距离</li><li>这个<img alt="L_{match}" class="mathcode" src="https://images2.imgbox.com/d9/90/3EWhPGdX_o.png">具体是： <p class="img-center"><img alt="-1_{\left\{c_{i} \neq \varnothing\right\}} \hat{p}_{\sigma(i)}\left(c_{i}\right)+1_{\left\{c_{i} \neq \varnothing\right\}} L_{b o x}\left(b_{i}, \hat{b}_{\sigma(i)}\right)" class="mathcode" src="https://images2.imgbox.com/5e/d9/Hn2o5dbZ_o.png"></p> 意思是：假设当前从真值索引到预测值索引的所有映射为<img alt="\sigma" class="mathcode" src="https://images2.imgbox.com/38/7d/APmDHdTk_o.png">，对于图片中的每个真值 <img alt="i" class="mathcode" src="https://images2.imgbox.com/43/42/UgFKCkhY_o.png"><br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/1c/e6/FhByLaKr_o.png">  先找到对应的预测值<img alt="\sigma (i)" class="mathcode" src="https://images2.imgbox.com/b8/50/jbDatzHH_o.png">，再看看<strong>分类网络</strong>的结果<strong><img alt="\hat{p}_{\sigma(i)}\left(c_{i}\right)" class="mathcode" src="https://images2.imgbox.com/08/66/Mc4514Sy_o.png"></strong>，取反作为 <img alt="L_{match}" class="mathcode" src="https://images2.imgbox.com/0d/74/9qmh2AtX_o.png">的第1部分<br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/76/89/PKqvEO1q_o.png">  再计算<strong>回归网络</strong>的结果<strong><img alt="\hat{b}_{\sigma(i)}" class="mathcode" src="https://images2.imgbox.com/e9/a3/zMxuHQBn_o.png"></strong>与真值的 Bounding Box 的差异，即 <img alt="L_{b o x}\left(b_{i}, \hat{b}_{\sigma(i)}\right)" class="mathcode" src="https://images2.imgbox.com/45/c2/TnghUtUd_o.png">，作为<img alt="L_{match}" class="mathcode" src="https://images2.imgbox.com/38/a4/6XV4SNeN_o.png">的第2部分<br> 所以，可以使得<img alt="L_{match}" class="mathcode" src="https://images2.imgbox.com/41/d1/3geioDjU_o.png">最小的排列 <img alt="\hat{\sigma }" class="mathcode" src="https://images2.imgbox.com/0f/58/t5blB1KA_o.png"> 就是我们要找的排列，即：对于图片中的每个真值<img alt="i" class="mathcode" src="https://images2.imgbox.com/15/ab/kVfFhSbQ_o.png">来讲，<img alt="\hat{\sigma}(i)" class="mathcode" src="https://images2.imgbox.com/0d/30/3NKlUuvK_o.png">就是这个真值所对应的预测值的索引<br> 上述这个匈牙利算法的过程与Anchor或Proposal有异曲同工的地方，只是此时我们找的是一对一匹配</li><li>接下来就是使用上一步得到的排列<img alt="\hat{\sigma }" class="mathcode" src="https://images2.imgbox.com/0d/ef/72N0Myji_o.png">，计算匈牙利损失<br><img alt="L_{\text {Hungarian }}(y, \hat{y})=\sum_{i=1}^{N}\left[-\log \hat{p}_{\hat{\sigma}(i)}\left(c_{i}\right)+1_{\left\{c_{i} \neq \varnothing\right\}} L_{b o x}\left(b_{i}, \hat{b}_{\hat{\sigma}(i)}\right)\right]" class="mathcode" src="https://images2.imgbox.com/81/bb/o5FmS8MR_o.png"><br> 式中的<img alt="L_{box}" class="mathcode" src="https://images2.imgbox.com/98/bc/48w26AX0_o.png">具体为： <p class="img-center"><img alt="" height="48" src="https://images2.imgbox.com/fe/87/Qs6764gg_o.png" width="600"></p> 最常用的<img alt="L_1" class="mathcode" src="https://images2.imgbox.com/9c/78/fPtKqOjy_o.png"> loss对于大小 Bounding Box 会有不同的标度，即使它们的相对误差是相似的。为了缓解这个问题，作者使用了<img alt="L_1" class="mathcode" src="https://images2.imgbox.com/ae/96/qgyx2fI6_o.png"> loss和广义IoU损耗<img alt="L_{iou}" class="mathcode" src="https://images2.imgbox.com/d9/da/h9ILVihR_o.png">的线性组合，它是比例不变的<br> Hungarian意思就是匈牙利，也就是前面的<img alt="L_{match}" class="mathcode" src="https://images2.imgbox.com/6b/27/qXROvJSX_o.png">，上述意思是需要计算<img alt="M" class="mathcode" src="https://images2.imgbox.com/c7/34/uexa7D1s_o.png">个 GTBounding Box 和<img alt="N" class="mathcode" src="https://images2.imgbox.com/84/e0/fC3RMq9U_o.png">个输预测出集合两两之间的广义距离，距离越近表示越可能是最优匹配关系，也就是两者最密切，广义距离的计算考虑了分类分支和回归分支</li></ol> 
<h2>第四部分 从ViT到MAE</h2> 
<h3>4.1 Vision Transformer：用标准的Transformer直接干CV任务</h3> 
<p>继DETR、DDPM之后，Google的Alexey Dosovitskiy等人于2020年10月发布的这篇论文《<a class="link-info" href="https://arxiv.org/abs/2010.11929" rel="nofollow" title="An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>》提出的ViT彻底引燃了多模态的火热，更是直接挑战了此前CNN在视觉领域长达近10年的绝对统治地位「<em><span style="color:#7b7f82;">如果忘了CNN长啥样的，请回顾此文《<a class="link-info" href="https://blog.csdn.net/v_JULY_v/article/details/51812459" title="CNN笔记：通俗理解卷积神经网络">CNN笔记：通俗理解卷积神经网络</a>》</span></em>」</p> 
<p>这个工作是怎么一步步出来的呢？自从Google在2017年发布的transformer在NLP领域大杀四方的时候，就一直不断有人想把如此强大且充满魔力的transformer用到CV领域中，但前路曲折啊</p> 
<ol><li>一开始面对的问题就是，当把transformer中对NLP的各个token之间两两互相做相似度计算的self-attention引入到图片各个像素点之间两两做self-attention时，你会发现计算复杂度瞬间爆炸(<em><span style="color:#7b7f82;">transformer 的计算复杂度是序列长度 n 的 平方 <img alt="O(n^2)" class="mathcode" src="https://images2.imgbox.com/82/9f/AdHtb0fg_o.png"></span></em>)<br> 原因很简单，一句话才多少个token(顶多几百而已)，但一张图片呢？比如一张像素比较低的<img alt="224 \times 224" class="mathcode" src="https://images2.imgbox.com/61/55/6nl1dn1V_o.png">分辨率的图片，就已经达到了 50176 个像素点，再考虑到RGB三个维度，直接就是15万起步，你品..</li><li>兴高采烈之下理想遇挫，才发现没那么简单，咋办呢，那就降低序列长度呗。比如要么像CVPR Wang et al. 2018的工作把网络中间的特征图当做transformer的输入，毕竟ResNet 50 最后一个 stage, res4 的 feature map也就<img alt="14 \times 14 = 196" class="mathcode" src="https://images2.imgbox.com/7a/e2/sbPcTjjC_o.png">的大小，要么借鉴CNN的卷积机制用一个局部窗口去抓图片的特征，从而降低图片的复杂度，但这一系列工作虽然逻辑上通畅，但因为硬件上无法加速等，导致模型没法太大</li><li>总之，之前的工作要么把CNN和self-attention结合起来，要么把self-attention取代CNN，但都没取得很好的扩展效果，看来得再次冲击transformer for CV (<span style="color:#7b7f82;">为何如此执着？还不是因为理想实在过于美丽，要不然人到中年 每每听到beyond的“原谅我这一生不羁放纵爱自由”便如此共鸣强烈</span>)，但还是得回头开头的老大难问题：如何处理图片的复杂度<br><br> 于此，Vision Transformer(ViT)来了：不再一个一个像素点的处理，而是把整个图片切分成一个个图片块(比如分割为九宫格)，这些图片块作为transformer的输入</li></ol> 
<p>下面，我们来仔细探究下ViT到底是怎么做的</p> 
<h4>4.1.1 ViT的架构：Embedding层 + Transformer Encoder + MLP Head</h4> 
<p>简单而言，Vision Transformer(ViT)的模型框架由三个模块组成：</p> 
<p class="img-center"><img alt="" height="416" src="https://images2.imgbox.com/d0/36/aRWErqAG_o.png" width="800"></p> 
<ul><li><strong>Embedding层（线性投射层Linear Projection of Flattened Patches）</strong><br><span style="color:#1a439c;">以ViT_base_patch16为例，一张224 x 224的图片先分割成 16 x 16 的 patch </span>，很显然会因此而存在 <img alt="(224\times 224/16\times 16)^2=196" class="mathcode" src="https://images2.imgbox.com/dc/52/W3skvuim_o.png"> 个 patch(<span style="color:#7b7f82;"><em>这个patch数如果泛化到一般情况就是图片长宽除以patch的长宽，即<img alt="N = HW/P^2" class="mathcode" src="https://images2.imgbox.com/c8/07/OAtdWwhA_o.png"></em></span>)<br> 且图片的长宽由原来的224  x 224 变成：14  x 14(<em><span style="color:#7b7f82;">因为224/16 = 14</span></em>)<br><br><em><span style="color:#7b7f82;"><strong>你可能还没意识到这个操作的价值，这相当于把图片需要处理的像素单元从5万多直接降到了</strong></span><span style="color:#ed7976;"><strong>14x14 = 196</strong></span><span style="color:#7b7f82;"><strong>个像素块，如果一个像素块当做一个token，那针对196个像素块/token去做self-attention不就轻松多了么(</strong>顺带提一句，其实在ViT之前，已经有人做了类似的工作，比如ICLR 2020的一篇paper便是针对CIFAR-10中<img alt="32\times 32" class="mathcode" src="https://images2.imgbox.com/ac/de/XSW1HFAX_o.png">的图片抽 <img alt="2\times 2" class="mathcode" src="https://images2.imgbox.com/b9/f6/Vadpo584_o.png">的像素块<strong>)</strong></span></em> 
  <table border="1" cellpadding="1" cellspacing="1" style="width:500px;"><tbody><tr><td>16*16</td><td>16*16</td><td>16*16</td><td>16*16</td><td>16*16</td><td>16*16</td><td>16*16</td><td>16*16</td><td>16*16</td><td>16*16</td><td>16*16</td><td>16*16</td><td>16*16</td><td>16*16</td></tr><tr><td>16*16</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>16*16</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>16*16</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><span style="color:#7b7f82;">...</span></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><span style="color:#4d4d4d;">而对于图片而言，还得考虑RGB channel这个因素，故每个 patch 的 维度便是 [16, 16, 3]，</span>但标准Transformer的输入是一个一维的token序列，所以需要把这个三维的维度通过线性映射<span style="color:#1a439c;">linear projection</span>成一维的维度，从而使得每个 <span style="color:#4d4d4d;">patch </span>最终的输出维度是：<img alt="16\times 16\times 3 = 768" class="mathcode" src="https://images2.imgbox.com/33/7b/MYmb6hxC_o.png"><br> 这样图片就<span style="color:#ed7976;">由原来的 [224, 224, 3] 的大小变成了 [14, 14, 768] 的大小</span>，<span style="color:#7b7f82;">相当于之前图片横竖都是224个像素点的，现在横竖只是14个像素点了，而每个像素点的维度(相当于每个token的序列长度)为768</span><br><br> 之后经过 <span style="color:#1a439c;">Flatten</span> 就得到 <img alt="[196, 768]" class="mathcode" src="https://images2.imgbox.com/fa/7e/y0mLqn9e_o.png">，接着再经过一个维度为<img alt="768 \times 768" class="mathcode" src="https://images2.imgbox.com/08/30/IWD9WtNr_o.png">的Linear projection (<em>本质上就是一个全连接层，用大写 <img alt="E" class="mathcode" src="https://images2.imgbox.com/cf/a7/9OHsKhqW_o.png">表示，这个768的维度即embedding_dim可以变，简写为<img alt="D" class="mathcode" src="https://images2.imgbox.com/28/2b/Gja1Yp2G_o.png">，比如原始的transformer设置的维度为512</em>)，故最终的维度还是为<img alt="196\times 768" class="mathcode" src="https://images2.imgbox.com/89/6e/JPol9qnI_o.png"><br>   <p><span style="color:#7b7f82;">至于在代码实现中，可通过一个卷积层来实现，卷积核大小为16，步长为16，输入维度是3，通过对整个图片进行卷积操作：[224, 224, 3] -&gt; [14, 14, 768]，然后把H以及W两个维度展平即可[14, 14, 768] -&gt; [196, 768]</span></p> <pre><code>Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)</code></pre> <span style="color:#1a439c;">接下来，为了做最后的分类</span>，故在所有tokens的前面加一个可以通过学习得到的 [class] token作为这些patchs的全局输出，相当于BERT中的<span style="color:#ed7976;">分类字符CLS</span> (这里的加是concat拼接)，得益于self-attention机制，所有token两两之间都会做交互，故这个[class] token也会有与其他所有token交互的信息<br> 且为了保持维度一致，[class] token的维度为 [1, 768] ，通过Concat操作，[196, 768]  与 [1, 768] 拼接得到<span style="color:#ed7976;"> [197, 768]</span><br><br> 由于self-attention本身没有考虑输入的<span style="color:#1a439c;">位置信息</span>，无法对序列建模。而图片切成的patches也是有顺序的，打乱之后就不是原来的图片了，故随后和transformer一样，就是对于这些 token 添加位置信息，也就是 position embedding，VIT的做法是<span style="color:#ed7976;">在每个token前面加上位置向量position embedding</span>(这里的加是直接向量相加即sum，不是concat)，这里和 transformer 一致，都是可训练的参数，因为要加到所有 token 上，所以<span style="color:#ed7976;">维度也是 [197, 768]</span></li><li><strong>Transformer Encoder</strong> <p class="img-center"><img alt="" height="484" src="https://images2.imgbox.com/b6/3a/4qKNkSBV_o.png" width="600"></p> 维度为[197, 768]的embedded patches进来后，先做一次Layer Norm<br> 然后再做Multi-head Attention，通过乘以三个不同的Q K V矩阵得到三个不同的Q K V向量，且<span style="color:#ed7976;">ViT_base_patch16设计的是12个头，故每个头的维度为：[197, 768/12] = [197, 64]</span>，最后把12个头拼接起来，会再次得到[197, 768]的维度<br> 接着再做Norm<br> 最后是MLP，维度上先放大4倍到[197, 3072]，之后又缩小回去恢复到[197, 768]的维度</li><li><strong>MLP Head（最终用于分类的层结构）</strong><br> MLP里面，是用tanh作为一个非线性的激活函数，去根据[class] token做分类的预测</li></ul> 
<p>再通过小绿豆根据ViT的源码画的这个图总结一下</p> 
<p class="img-center"><img alt="vit-b/16" height="945" src="https://images2.imgbox.com/ff/0f/Di6Filpn_o.png" width="1000"></p> 
<p> 此外，上述这三个阶段的过程最终可以用如下4个公式表达</p> 
<blockquote> 
 <p class="img-center"><img alt="" height="170" src="https://images2.imgbox.com/95/b7/ZDUpFXsB_o.png" width="1000"></p> 
 <p>对于公式(1)，<img alt="x_{p}^{N}" class="mathcode" src="https://images2.imgbox.com/e8/51/m9jtgSXU_o.png">表示图片patch，总共<img alt="N" class="mathcode" src="https://images2.imgbox.com/37/66/GLoNiuIQ_o.png">个patch，<img alt="x_{p}^{N} E" class="mathcode" src="https://images2.imgbox.com/2a/ab/OKy5nPCf_o.png">则表示patch embedding，<img alt="x_{class}" class="mathcode" src="https://images2.imgbox.com/9a/a0/EjX8v1dF_o.png">表示class embedding，因为需要用它做最后的输出/分类<br> 对于公式(2)，<img alt="z'_l" class="mathcode" src="https://images2.imgbox.com/ab/af/XZWQtCrC_o.png">则表示多头注意力的结果(<span style="color:#7b7f82;"><em><img alt="z_{l-1}" class="mathcode" src="https://images2.imgbox.com/44/a0/RnSeyYqv_o.png">先Norm再multi-head attention，得到的结果再与<img alt="z_{l-1}" class="mathcode" src="https://images2.imgbox.com/89/88/DOCrlfVT_o.png">做残差连接</em></span>)<br> 对于公式(3)，<img alt="z_l" class="mathcode" src="https://images2.imgbox.com/b3/ea/ffmkpVAn_o.png">表示最终整个transformer decoder的输出(<span style="color:#7b7f82;"><em><img alt="z'_l" class="mathcode" src="https://images2.imgbox.com/11/b5/abyXDA3c_o.png">先做Norm再做MLP，得到的结果再与<img alt="z'_l" class="mathcode" src="https://images2.imgbox.com/99/42/SJJrZbUC_o.png">做残差连接</em></span>)</p> 
</blockquote> 
<h4>4.1.2 ViT与CNN在先验知识上的对比</h4> 
<p>值得一提的是，由于ViT不像CNN那样对图像有比较多的先验知识，即没有用太多的归纳偏置</p> 
<ol><li>具体来说，CNN的局部性locality (<span style="color:#7b7f82;">以滑动窗口的形式一点一点在图片上进行卷积，故会假设图片上相邻的区域会有近似的特征</span>)，和平移等变性translation equivariance「<span style="color:#7b7f82;">无论先做平移还是先做卷积，最后的 结果都是一样的，类似<img alt="f(g(x)) = g(f(x))" class="mathcode" src="https://images2.imgbox.com/cb/0e/M5yCnOVD_o.png">，毕竟卷积核就像一个模板一样，输入一致的情况下，不论图片移动到哪里，最后的输出都是一样的</span>」贯穿整个CNN模型的始终</li><li>而对于ViT而言，也就在最后的MLP用到了局部且平移等变性，以及针对每个图片的patch加了位置编码，除这两点之外，ViT没有再专门针对CV问题做任何额外的处理，说白了，就是干：直接拿transformer干CV</li></ol> 
<p>所以在中小型的数据集上训练的结果不如CNN也是可以理解的。既如此，transformer的全局建模能力比较强，而CNN又不用那么多的训练数据，那是否可以把这个模型的优势给结合起来呢？比如做一个混合网络，前头是CNN，后头是transformer呢，答案是：也是可以的! 但这是不是就类似上文介绍过的DETR呢？读者可以继续深入思考下。</p> 
<h3>4.2 MAE</h3> 
<p class="img-center"><img alt="" height="516" src="https://images2.imgbox.com/50/ff/vNE87T7y_o.png" width="1000"></p> 
<p> // 待更</p> 
<h2>第五部分 Swin Transformer</h2> 
<h3>5.1 Swin Transformer：多尺度的ViT</h3> 
<p>swin transformer作为多尺度的ViT更加适合处理视觉问题，且证明transformer不但能在ViT所证明的分类任务上取得很好的效果，在检测、分割上也能取得很好的效果，而在结构上，swin transformer主要做了以下两点改进</p> 
<p class="img-center"><img alt="" height="493" src="https://images2.imgbox.com/5f/c6/T9I6PwIU_o.png" width="1000"></p> 
<ol><li>获取图像多尺寸的特征<br> 对于ViT而言，经过12层每一层的transformer都是16✖️16的patch块(相当于16倍下采样率)，虽然通过transformer全局的自注意力操作可以达到全局的建模能力，但它对多尺寸特征的把握则相对弱些，而这个多尺寸的特征有多重要呢，比如对于目标检测而言，用的比较广的一个方法叫FPN，这个方法用的一个分层式的CNN，而每一个卷积层因为不同的感受野则会获取到不同尺寸的特征<br> 而swin transformer就是为解决ViT只有单一尺寸的特征而来的</li><li>降低序列长度是图像处理中一个很关键的问题，虽然ViT把整张图片打成了16✖️16的patch，但但图像比较大的时候，计算的复杂度还是比较高<br> 而Swin Transformer使用窗口Window的形式将16✖️16的特征图划分成了多个不相交的区域(比如16个4✖️4的，4个8✖️8的)，并且只在每个小窗口(4✖️4或8✖️8)内进行多头注意力计算，大大减少计算量<br> 之所以能这样操作的依据在于借鉴了CNN中locality先验知识(<em><span style="color:#7b7f82;">CNN是以滑动窗口的形式一点一点地在图片上进行卷积，原因在于图片上相邻的区域会有相邻的特征</span></em>)，即同一个物体的不同部位在一个小窗口的范围内是临近着的，从而在小窗口内算自注意力够用<br> 且swin transformer使用patch merging，可以把相邻的四个小的patch合成一个大的patch(即patch merging)，提高了感受野，这样就能获取多尺度的特征(类似CNN中的池化效果)，这些特征通过FPN结构就可以做检测，通过UNet结构就可以做分割了</li></ol> 
<h4>5.1.1 如何让4个互相独立不重叠的窗口彼此交互做自注意力操作：经过一系列shift操作</h4> 
<p>接下来，我们看下swin transformer中移动窗口的设计，图中灰色的小patch就是4✖️4的大小，然后最左侧的4个红色小窗口中均默认有7✖️7=49个小patch(当然，示意图中只展示了16个小patch)，如果做接下来几个操作</p> 
<p class="img-center"><img alt="" height="298" src="https://images2.imgbox.com/21/c5/ukbJF2Hz_o.png" width="800"></p> 
<ol><li>把最左侧的整个大窗口layer 1向右下角整体移动两个小patch，并把移动之后的大窗口的最右侧的<strong><span style="color:#4da8ee;">宽为2个小patch、高为6个小patch的部分</span></strong>平移到大窗口之外的左侧</li><li>且同时把移动之后的大窗口最底部<strong>宽为8个小patch、高为2个小patch部分</strong>的一分为二：<br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/a4/dd/wa5TLcX7_o.png">  一方面 其左侧部分(宽为6个小patch 高为2个小patch)整体平移到大窗口之外的最上方<br><img alt="\rightarrow" class="mathcode" src="https://images2.imgbox.com/08/bc/fFDqJwli_o.png">  二方面 最后遗留下来的右下角的2个小patch移动到大窗口之外的最左上角 <p class="img-center"><img alt="" height="283" src="https://images2.imgbox.com/ac/97/MubQFNJQ_o.png" width="800"></p> </li></ol> 
<p>则成为图中右侧所示的大窗口layer 1+1，从而使得之前互相独立不重叠的4个小窗口在经过这一系列shift操作之后，彼此之间可以进行互动做自注意力的计算了</p> 
<blockquote> 
 <p>貌似还是有点抽象是不？没事，我画个图 就一目了然了</p> 
 <hr> 
 <p>如下所示，在右侧加粗的4个新的小窗口内部，每个小窗口都有其他小窗口的信息了(每个小窗口都由之前的单一颜色的patch组成，变成了由4种不同的颜色patch组成，相当于具备了全局的注意力，够直观吧?!)</p> 
 <p class="img-center"><img alt="" height="382" src="https://images2.imgbox.com/a0/cd/TD08pX7T_o.png" width="800"></p> 
</blockquote> 
<h4>5.1.2 Swin Transformer模型总览图</h4> 
<p>以下是整个swin transformer模型的总览图</p> 
<p class="img-center"><img alt="" height="299" src="https://images2.imgbox.com/76/35/DOIG4sZw_o.png" width="1000"></p> 
<p> 从左至右走一遍整个过程则是</p> 
<ol><li><span style="color:#ed7976;"><u>stage 1</u></span><br><strong>patch partition</strong><br> 对于一张原始图片224✖️224✖️3，打成4✖️4的patch(则每个patch的维度是4✖️4✖️3 = 48，其中3是图片的RGB通道)，从而会存在<img alt="224^2/4^2 = 56\times 56" class="mathcode" src="https://images2.imgbox.com/bb/ed/auJYtenW_o.png">个patch，相当于整个图片由[224,224,3]的维度变成了[56,56,48]的维度</li><li><strong>linear embedding</strong><br> 为了变成transformer能接受的值，[56,56,48]的维度变成[56,56,96]，最前面的两个维度拉直之后，则维度变成了[3136,96]，很明显在ViT里在这一步对应的维度是[196,768]，故这个3136的维度太大了，咋办呢？</li><li><strong>swin transformer block：基于7✖️7个小patch的小窗口计算自注意力</strong><br> 好在swin transformer引入了基于窗口的自注意力机制，而每个窗口默认只有七七四十九个patch，所以序列长度就只有49了，也就解决了计算复杂度的问题</li><li><span style="color:#ed7976;"><u>stage 2</u></span><br><strong>patch mergeing(很像lower level任务中很常用的一个上采样方式：pixel shuffle)</strong><br> patch mergeing的作用在于把临近的小patch合并成一个大patch，比如针对下图中维度为[H,W,C]的张量<br> 由于下采样两倍，所以选点的时候，是每隔一个点选一个，最终整个大张量变成了4个小张量(每隔张量大小为[H/2,W/2])<br> 之后把这4个张量的在C的维度上拼接起来，拼接之后的张量的维度则就变成了[H/2,W/2,4C]<br> 接着，在C这个维度上通过一个1✖️1的卷积操作把张量的维度降了下来(<em><span style="color:#7b7f82;">这个1✖️1的卷积操作类似于linear的作用</span></em>)，最终变成了[H/2,W/2,2C] <p class="img-center"><img alt="" height="164" src="https://images2.imgbox.com/23/a5/TdnMvNbr_o.png" width="1000"></p> 所以对应到模型总览图上则是：[56,56,96]的维度变成了[28,28,192]的维度 <p class="img-center"><img alt="" height="314" src="https://images2.imgbox.com/61/8b/QKNstAfn_o.png" width="1200"></p> </li><li><span style="color:#ed7976;"><u>stage 3</u></span><br> 维度上从[28,28,192]变成[14,14,384]</li><li><u><span style="color:#ed7976;">stage 4</span></u><br> 维度上从[14,14,384]变成[7,7,768] </li></ol> 
<p>整个前向传播过程走完了之后，可能有读者问，swin transformer如何做分类呢？它为了和CNN保持一致，没有像ViT在输入序列上加一个用于最后分类的CLS token，而是在得到最后的特征图之后，用了一个golbal average polling(即全局池化)的操作，直接把[7,7,768]中的7✖️1取平均并拉直成1，使得最终的维度变成[1,768]</p> 
<p>//待更新..</p> 
<hr> 
<h2>参考文献与推荐阅读</h2> 
<ol><li><a class="link-info" href="https://kexue.fm/archives/5253" rel="nofollow" title="变分自编码器（一）：原来是这么一回事">变分自编码器（一）：原来是这么一回事</a>，包含对重参数技巧的介绍</li><li>VAE原始论文</li><li>关于VAE的几篇文章：<a class="link-info" href="https://zhuanlan.zhihu.com/p/64485020" rel="nofollow" title="一文理解变分自编码器（VAE）">一文理解变分自编码器（VAE）</a>、<a class="link-info" href="https://zhuanlan.zhihu.com/p/348498294" rel="nofollow" title="机器学习方法—优雅的模型（一）：变分自编码器（VAE）">机器学习方法—优雅的模型（一）：变分自编码器（VAE）</a>、</li><li>苏剑林关于扩散模型的几篇文章：<a class="link-info" href="https://kexue.fm/archives/9119" rel="nofollow" title="（一）：DDPM = 拆楼 + 建楼">（一）：DDPM = 拆楼 + 建楼</a>、<a class="link-info" href="https://kexue.fm/archives/9152" rel="nofollow" title="（二）：DDPM = 自回归式VAE">（二）：DDPM = 自回归式VAE</a></li><li><a class="link-info" href="https://www.zhihu.com/question/545764550" rel="nofollow" title="怎么理解今年 CV 比较火的扩散模型（DDPM）？">怎么理解今年 CV 比较火的扩散模型（DDPM）？</a></li><li> <p>知乎上关于扩散模型的几篇文章：<a class="link-info" href="https://zhuanlan.zhihu.com/p/566618077" rel="nofollow" title="全网最简单的扩散模型DDPM教程">全网最简单的扩散模型DDPM教程</a>、<a class="link-info" href="https://zhuanlan.zhihu.com/p/610012156" rel="nofollow" title="Diffusion扩散模型大白话讲解">Diffusion扩散模型大白话讲解</a>、<a class="link-info" href="https://www.zhihu.com/question/545764550/answer/2926355957" rel="nofollow" title="扩散生成模型: 唯美联姻物理概念与机器学习">扩散生成模型: 唯美联姻物理概念与机器学习</a> </p> </li><li><a class="link-info" href="https://calvinyluo.com/2022/08/26/diffusion-tutorial.html" rel="nofollow" title="Understanding Diffusion Models: A Unified Perspective">Understanding Diffusion Models: A Unified Perspective</a>(写于2022年8月，此文写的非常细致，另，这是其<a class="link-info" href="https://arxiv.org/pdf/2208.11970" rel="nofollow" title="PDF版本">PDF版本</a>)</li><li><a class="link-info" href="https://shao.fun/blog/w/how-diffusion-models-work.html" rel="nofollow" title="扩散模型是如何工作的：从0开始的数学原理">扩散模型是如何工作的：从0开始的数学原理</a></li><li><a class="link-info" href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" rel="nofollow" title="What are Diffusion Models?">What are Diffusion Models?</a>，写于2021年7月</li><li><a class="link-info" href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/" rel="nofollow" title="Introduction to Diffusion Models for Machine Learning">Introduction to Diffusion Models for Machine Learning</a></li><li>关于扩散模型的几篇论文<br><a class="link-info" href="https://cvpr2022-tutorial-diffusion-models.github.io/" rel="nofollow" title="CVPR 2022 Tutorial:  Denoising Diffusion-based Generative Modeling: Foundations and Applications">CVPR 2022 Tutorial: Denoising Diffusion-based Generative Modeling: Foundations and Applications</a><br><a class="link-info" href="https://arxiv.org/pdf/2105.05233" rel="nofollow" title="Diffusion Models Beat GANs on Image Synthesis">Diffusion Models Beat GANs on Image Synthesis</a></li><li>关于DDPM的几篇文章(如果相关文章与本文有冲突，建议以本文为准，因为有些文章有笔误或错误)：<a class="link-info" href="https://blog.csdn.net/qq_45752541/article/details/127956235" title="DDPM概率扩散模型（原理+代码)">DDPM概率扩散模型（原理+代码)</a>、<a class="link-info" href="https://www.cnblogs.com/MTandHJ/p/15698607.html" rel="nofollow" title="Denoising Diffusion Probabilistic Models (DDPM)">Denoising Diffusion Probabilistic Models (DDPM)</a>、<a class="link-info" href="https://xyfjason.top/2022/09/29/%E4%BB%8EVAE%E5%88%B0DDPM/" rel="nofollow" title="从VAE到DDPM">从VAE到DDPM</a>、<a class="link-info" href="https://www.jianshu.com/p/9e6a28f1018a" rel="nofollow" title="扩散模型原理解析">扩散模型原理解析</a></li><li><a class="link-info" href="https://zhuanlan.zhihu.com/p/558937247?utm_id=0" rel="nofollow" title="大一统视角理解扩散模型Understanding Diffusion Models: A Unified Perspective 阅读笔记">大一统视角理解扩散模型Understanding Diffusion Models: A Unified Perspective 阅读笔记</a></li><li><a class="link-info" href="https://mp.weixin.qq.com/s/frtG8V_4CuNn2MjL3N8F-Q" rel="nofollow" title="Stable Diffusion一周年：扩散模型编年简史">Stable Diffusion一周年：扩散模型编年简史</a></li><li><a class="link-info" href="https://blog.csdn.net/qq_56591814/article/details/127749105" title="DALL·E2（生成模型串讲，从GANs、VE/VAE/VQ-VAE/DALL·E到扩散模型DDPM/ADM）">DALL·E2（生成模型串讲，从GANs、VE/VAE/VQ-VAE/DALL·E到扩散模型DDPM/ADM）</a></li><li><a class="link-info" href="https://swarma.org/?p=39798" rel="nofollow" title="AI生成艺术的底层原理：非平衡物理的扩散模型">AI生成艺术的底层原理：非平衡物理的扩散模型</a></li><li><a class="link-info" href="https://zhuanlan.zhihu.com/p/340149804" rel="nofollow" title="Vision Transformer 超详细解读 (原理分析+代码解读) (一)">Vision Transformer 超详细解读 (原理分析+代码解读) (一)</a></li><li><a class="link-info" href="https://www.bilibili.com/video/BV1GB4y1X72R/?spm_id_from=888.80997.embed_other.whitelist" rel="nofollow" title="DETR论文的解读之一">DETR论文的解读之一</a></li><li><a class="link-info" href="https://arxiv.org/pdf/2005.12872" rel="nofollow" title="End-to-End Object Detection with Transformers">End-to-End Object Detection with Transformers</a>，DETR原始论文</li><li><a class="link-info" href="https://zhuanlan.zhihu.com/p/342261872" rel="nofollow" title="Vision Transformer 超详细解读 (原理分析+代码解读) (二)">Vision Transformer 超详细解读 (原理分析+代码解读) (二)</a></li><li><a class="link-info" href="https://arxiv.org/pdf/2010.11929" rel="nofollow" title="AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE">AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</a><br> ViT原始论文，挑战CNN的在CV领域的统治地位</li><li><a class="link-info" href="https://www.bilibili.com/video/BV15P4y137jb/?spm_id_from=333.788" rel="nofollow" title="ViT论文逐段精读">ViT论文逐段精读</a>，这是针对<a class="link-info" href="https://blog.csdn.net/qq_56591814/article/details/127358168" title="该视频解读的笔记之一(神洛华)">该视频解读的笔记之一(神洛华)</a>、<a class="link-info" href="https://blog.csdn.net/qq_42014059/article/details/123964312" title="针对该视频解读的笔记之二(MT_Joy)">针对该视频解读的笔记之二(MT_Joy)</a></li><li><a class="link-info" href="https://blog.csdn.net/like_jmo/article/details/126094133" title="Vision Transformer 论文 + 详解">Vision Transformer 论文 + 详解</a>，<a class="link-info" href="https://blog.csdn.net/qq_37541097/article/details/118242600" title="Vision Transformer详解by 小绿豆">Vision Transformer详解by 小绿豆</a></li><li><a class="link-info" href="https://arxiv.org/pdf/2111.06377" rel="nofollow" title="Masked Autoencoders Are Scalable Vision Learners">Masked Autoencoders Are Scalable Vision Learners</a><br> MAE原始论文</li><li><a class="link-info" href="https://www.bilibili.com/video/BV1sq4y1q77t/?spm_id_from=333.788&amp;vd_source=02a7bf3dbb14104d4c31a9017ba6bd89" rel="nofollow" title="MAE 论文逐段精读">MAE 论文逐段精读</a>，此文则为<a class="link-info" href="https://blog.csdn.net/qq_42014059/article/details/122085478" title="对该视频解读所做的笔记">对该视频解读所做的笔记</a></li><li><a class="link-info" href="https://arxiv.org/pdf/2103.14030" rel="nofollow" title="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a><br> Swin transformer原始论文</li><li><a class="link-info" href="https://www.bilibili.com/video/BV13L4y1475U/?vd_source=02a7bf3dbb14104d4c31a9017ba6bd89" rel="nofollow" title="Swin Transformer论文精读">Swin Transformer论文精读</a>，此文是针对<a class="link-info" href="https://blog.csdn.net/qq_56591814/article/details/127358168" title="该视频解读所做的笔记(神洛华)">该视频解读所做的笔记(神洛华)</a>、<a class="link-info" href="https://blog.csdn.net/qq_37541097/article/details/121119988" title="Swin-Transformer网络结构详解(by小绿豆)">Swin-Transformer网络结构详解(by小绿豆)</a></li><li><a class="link-info" href="https://zhuanlan.zhihu.com/p/367111046" rel="nofollow" title="图解Swin Transformer">图解Swin Transformer</a>、<a class="link-info" href="https://www.zhihu.com/question/451860144/answer/1837109348" rel="nofollow" title="如何看待微软亚洲研究院的Swin Transformer？">如何看待微软亚洲研究院的Swin Transformer？</a></li><li><a class="link-info" href="https://www.bilibili.com/video/BV12o4y1N7XF?p=2&amp;vd_source=02a7bf3dbb14104d4c31a9017ba6bd89" rel="nofollow" title="李宏毅关于生成式AI模型diffusion model/stable diffusion概念讲解">李宏毅关于生成式AI模型diffusion model/stable diffusion概念讲解</a></li><li>我个人推荐/站台的人邮新书《扩散模型：从原理到实战》</li><li><a class="link-info" href="https://www.julyedu.com/course/getDetail/485" rel="nofollow" title="七月在线AIGC下SD/MDJ的原理与实战课 [深度探究AI绘画/多模态]">七月在线AIGC下SD/MDJ的原理与实战课 [深度探究AI绘画/多模态]</a></li></ol> 
<hr> 
<h2>创作、修改/完善记录</h2> 
<ol><li><u>第一大阶段</u><br> 4.28，因要发布AI绘画与CV多模态原理解析的博客，从VAE开始写起<br> 反复理了一个下午，总算把VAE写清楚了，也看了很多同类文章，之前推导看不下去的 都能看得下去了</li><li>5.1，拆解DDPM的前向过程，其他同类文章对于一个公式 可能一步或两三步到位我而言，能拆10步则10步 阅读无障碍，​不然 何必我来写</li><li>5.3，连续抠了两天DDPM的推导 总算有雏形了</li><li>5.4，今天又抠了一天的DDPM公式推导，增加了很多同类文章里没有的细节，很爽 这就是我提笔的价值和意义所在了</li><li>5.6，完善2.3节<br> 从5.2日起，连抠了整整4天DDPM的前向逆向推导，总算快写清楚了整个推导过程<br> 大家害怕的不是公式，只是怕公式的展开不够细致，毕竟对每一个人而言，公式展开越细致 越不怕<br><br> 且解读<img alt="L_{VLB}" class="mathcode" src="https://images2.imgbox.com/3d/78/oXiWnzDt_o.png">九行推导的每一行<br> 解读代码时 对每一行代码都加注释<br> ​拆解公式时 对每一行公式都做解释<br> ​<br> ​已是一个固定的风格，这样人人都能理解</li><li>5.10，继续完善2.3节<br> 理解DDPM不难，但里面的公式推导特别多，为了让每位朋友可以一目了然的理解每一个公式的推导<br> 只要能拆开的一定拆开 要解释的一定解释</li><li>5.12，再次完善2.2.2节逆向过程(去噪)<br> 为让整个推导看下来不费劲，修改部分描述以更流畅，使得最终尽可能就像看小说一样</li><li>5.14，开始更新3.1节DETR的部分</li><li>5.15，修改完善3.1节DETR结构之前两部分：backbone与encoder</li><li>5.16，开始更新3.1节DETR结构的后两部分：decoder和FFN，以及损失函数部分解读</li><li><u>第二大阶段</u><br> 5.19，修改完善3.1节3.1节DETR结构的后两部分：decoder和FFN，以及损失函数部分解读<br> 尽可能让行文清晰直观 一目了然 避免看着费劲/别扭</li><li>6.6，开始写ViT的部分<br> 且为了尽可能让对ViT的介绍一目了然、清晰明确，做了反复多轮的修改<br> 同时也创造了记录，只用一天便写清楚了ViT的介绍，算有史以来最快速度写清楚一个模型<br> ​但ViT这个工作真心6，Google这篇论文也写的真心好 每一句话 每一个配图都恰到好处(值得反复看好几遍)，有类似感触的第一想到的是OpenAI那篇CLIP论文 ​​​</li><li>6.9，开始写Swin Transformer的部分</li><li>6.12，继续写Swin Transformer的部分</li><li>6.26，新增扩散模型的极简发展史</li><li>6.27，给「2.2.2 逆向过程(去噪)：求解真实后验分布<img alt="q(x_{t-1}|x_t, x_0)" class="mathcode" src="https://images2.imgbox.com/66/c5/ZxdfEacG_o.png">—— 复原被加噪的图片使之清晰化」和<br> 「2.3 DDPM如何训练：通过噪声估计模型<img alt="\epsilon _\theta (x_t,t)" src="https://images2.imgbox.com/79/fa/HY0EaGFd_o.png">预测真实噪声——最小化估计噪声与真实噪声之间的差距」<br> 这两节的内容分别加了相应的4级标题，以让行文逻辑更清晰</li><li>6.28，新增关于「DDPM相比之前扩散模型的两个贡献」<br> 一方面，从预测转换图像改进为预测噪声<br> 二方面，DDPM只预测正态分布的均值<br><br> 且把扩展模型的发展史单独成为一节，即：2.1 极简发展史：从扩散模型、DDPM、improved DDPM到DALL·E/DALL·E2</li><li>6.29，新增一节「1.2.3 VAE的改进：VQ-VAE/VQ-VAE2」</li><li>7.5，加了这句话：用通俗的语言来说，如何去噪生成清晰图片呢，如上述第4行代码所述，比较模糊的图片<img alt="x_t" class="mathcode" src="https://images2.imgbox.com/d9/fd/xaRDXepL_o.png"> 减掉噪声估计器预测出来的噪声即可 (只是实际操作时，减掉的是与一个常数<img alt="\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}" class="mathcode" src="https://images2.imgbox.com/17/7e/23P6B9aS_o.png">相乘之后的噪声)</li><li>7.22，因第二天我要在「七月在线」的课程上讲stable diffusion的原理，故梳理回顾下本文的前两部分，回顾中顺带润色了相关描述和个别笔误<br> 比如根据参考文献7明确<img alt="x_{t}=\sqrt{\bar{\alpha}_{t}} \boldsymbol{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \epsilon_{0}" class="mathcode" src="https://images2.imgbox.com/b5/ee/hCDk1Lkq_o.png">，之前2.2.2.3节中有不对的笔误<img alt="x_{0}=\frac{1}{\sqrt{\bar{\alpha}_{t}}}\left(x_{t}-\sqrt{1-\bar{\alpha}_{t}} \epsilon_{t}\right)" class="mathcode" src="https://images2.imgbox.com/67/b1/KRd2FBFA_o.png"><br> 且因此进一步明确<br><img alt="{\mu}(x_t,x_0)=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}{\epsilon_0}\right)" class="mathcode" src="https://images2.imgbox.com/fb/a5/M9NB0J9m_o.png"></li><li>11.13，优化关于VQ-VAE、条件引导生成相关的内容<br> 关于文生图的一系列模型，其最后本质就两个核心，一个是扩散模型的正向、逆向过程，一个就是这个条件引导生成</li><li>11.18，因11.25/26要在fanbook上讲一个SD专题的公开课，围绕：SD原理 部署 二次开发<br> 故把本文前两部分的内容再重点梳理了下，优化了部分描述</li><li>12.15，为了补充对U-net网络的介绍，在“2.1 极简发展史”中补充介绍新增一个最新的研究：把Transformer用做扩散模型的骨干网络</li><li>12.26，因为写longlora，而注意到Swin Transformer，故修订Swin Transformer那一节中的部分措辞，以让行文更清晰</li><li><span style="color:#ed7976;">24</span>年2.19，因写OpenAI首个视频生成模型sora的原理解析时，阅读到DiT论文，其中这一句说的非常好：reformulating diffusion models to predict noise instead of pixel<br> 故把该句加到了本文中</li><li>2.23，修订此节“4.1.1 ViT的架构：Embedding层 + Transformer Encoder + MLP Head”的内容，以更加明确清晰<br> 且补充对U-Net网络的介绍</li><li>2.26，增加此部分的内容：<u>2021年12月 潜在扩散空间Latent Diffusion Model</u></li><li>3.6，修订VQ-VAE相关的内容</li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e6c36128640ed1e053c415018a16ac06/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【HarmonyOS】鸿蒙应用模块化实现</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/69a7e94ab37953b7b56970947618afdd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">重新认识Word —— 制作简历</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>