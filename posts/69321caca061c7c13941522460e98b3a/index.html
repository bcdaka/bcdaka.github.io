<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【AIGC】本地部署通义千问 1.5 (PyTorch) - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/69321caca061c7c13941522460e98b3a/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【AIGC】本地部署通义千问 1.5 (PyTorch)">
  <meta property="og:description" content="今天想分享一下 Qwen 1.5 官方用例的二次封装（ huggingface 说明页也有提供源码），其实没有太多的技术含量。主要是想记录一下如何从零开始在不使用第三方工具的前提下，以纯代码的方式本地部署一套大模型，相信这对于技术人员来说还是非常有用的。
虽然现在人人都可以用像 ollama 这种工具一键部署本地大模型，但想通过这种方式将大模型深度接入现有系统可就有点麻烦。 因此我觉得还是有必要各位分享一下这种纯代码的模式，希望能够帮助到更多的人。
1. 模型下载 通过之前的文章可以知道，现在国内可以通过 https://hf-mirror.com/ 下载 huggingface 大模型。在配置好环境变量后，可以通过以下命令下载 Qwen：
yuanzhenhui@MacBook-Pro ~ % cd /Users/yuanzhenhui/Documents/code_space/git/processing/python/tcm_assistant/transformer/model/qwen yuanzhenhui@MacBook-Pro qwen % huggingface-cli download --resume-download Qwen/Qwen1.5-7B-Chat --local-dir . Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details. Fetching 14 files: 7%|█▊ | 1/14 [00:00&lt;00:06, 1.87it/s]downloading https://hf-mirror.com/Qwen/Qwen1.5-7B-Chat/resolve/294483ad23713036574b30587b186713373f4271/README.md to /Users/yuanzhenhui/.cache/huggingface/hub/models--Qwen--Qwen1.5-7B-Chat/blobs/0963c198257a0607c4d2def66a84aec172240afd.incomplete README.md: 4.26kB [00:00, 6.40MB/s] Fetching 14 files: 100%|████████████████████████| 14/14 [00:01&lt;00:00, 12.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-12T11:49:06+08:00">
    <meta property="article:modified_time" content="2024-04-12T11:49:06+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【AIGC】本地部署通义千问 1.5 (PyTorch)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>今天想分享一下 Qwen 1.5 官方用例的二次封装（ huggingface 说明页也有提供源码），其实没有太多的技术含量。主要是想记录一下如何从零开始在不使用第三方工具的前提下，以纯代码的方式本地部署一套大模型，相信这对于技术人员来说还是非常有用的。</p> 
<p>虽然现在人人都可以用像 ollama 这种工具一键部署本地大模型，但想通过这种方式将大模型深度接入现有系统可就有点麻烦。 因此我觉得还是有必要各位分享一下这种纯代码的模式，希望能够帮助到更多的人。</p> 
<h4><a id="1__3"></a>1. 模型下载</h4> 
<p>通过之前的文章可以知道，现在国内可以通过 <a href="https://hf-mirror.com/" rel="nofollow">https://hf-mirror.com/</a> 下载 huggingface 大模型。在配置好环境变量后，可以通过以下命令下载 Qwen：</p> 
<pre><code class="prism language-bash">yuanzhenhui@MacBook-Pro ~ % <span class="token builtin class-name">cd</span> /Users/yuanzhenhui/Documents/code_space/git/processing/python/tcm_assistant/transformer/model/qwen
yuanzhenhui@MacBook-Pro qwen % huggingface-cli download --resume-download Qwen/Qwen1.5-7B-Chat --local-dir <span class="token builtin class-name">.</span>
Consider using <span class="token variable"><span class="token variable">`</span>hf_transfer<span class="token variable">`</span></span> <span class="token keyword">for</span> faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer <span class="token keyword">for</span> <span class="token function">more</span> details.
Fetching <span class="token number">14</span> files:   <span class="token number">7</span>%<span class="token operator">|</span>█▊                       <span class="token operator">|</span> <span class="token number">1</span>/14 <span class="token punctuation">[</span>00:0<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:06,  <span class="token number">1</span>.87it/s<span class="token punctuation">]</span>downloading https://hf-mirror.com/Qwen/Qwen1.5-7B-Chat/resolve/294483ad23713036574b30587b186713373f4271/README.md to /Users/yuanzhenhui/.cache/huggingface/hub/models--Qwen--Qwen1.5-7B-Chat/blobs/0963c198257a0607c4d2def66a84aec172240afd.incomplete
README.md: <span class="token number">4</span>.26kB <span class="token punctuation">[</span>00:00, <span class="token number">6</span>.40MB/s<span class="token punctuation">]</span>
Fetching <span class="token number">14</span> files: <span class="token number">100</span>%<span class="token operator">|</span>████████████████████████<span class="token operator">|</span> <span class="token number">14</span>/14 <span class="token punctuation">[</span>00:0<span class="token operator"><span class="token file-descriptor important">1</span>&lt;</span>00:00, <span class="token number">12</span>.81it/s<span class="token punctuation">]</span>
/Users/yuanzhenhui/Documents/code_space/git/processing/python/tcm_assistant/transformer/model/qwen
</code></pre> 
<p>由于我之前已经 checkout 过一遍了，因此会显示上面的输出。如果是第一次 checkout 那么你可能要等一段时间才能全部下载完成（毕竟还挺大的）。</p> 
<p>如果你是 MacOS 的用户那么模型的路径将会如下所示：</p> 
<pre><code class="prism language-bash"><span class="token punctuation">(</span>base<span class="token punctuation">)</span> yuanzhenhui@MacBook-Pro hub % <span class="token builtin class-name">pwd</span>
/Users/yuanzhenhui/.cache/huggingface/hub
<span class="token punctuation">(</span>base<span class="token punctuation">)</span> yuanzhenhui@MacBook-Pro hub % <span class="token function">ls</span>
models--BAAI--bge-large-zh-v1.5	models--Qwen--Qwen1.5-7B-Chat	version.txt
<span class="token punctuation">(</span>base<span class="token punctuation">)</span> yuanzhenhui@MacBook-Pro hub % 
</code></pre> 
<p>由于下载的是 “Qwen/Qwen1.5-7B-Chat” 因此下载下来后会以“models–Qwen–Qwen1.5-7B-Chat”名称进行保存。</p> 
<p>这时有小伙伴会问，huggingface-cli 命令中不是有写“–local-dir .”参数吗？如果模型不是保存在这个参数指定的位置，那么这个参数有什么用途？这个“.”（本地目录）又有什么意义？</p> 
<p>其实通过 local-dir 存放的是大模型的软连接和配置文件，如下图所示：</p> 
<pre><code class="prism language-bash"><span class="token punctuation">(</span>base<span class="token punctuation">)</span> yuanzhenhui@MacBook-Pro qwen % tree <span class="token parameter variable">-l</span>
<span class="token builtin class-name">.</span>
├── LICENSE
├── README.md
├── config.json
├── generation_config.json
├── merges.txt
├── model-00001-of-00004.safetensors -<span class="token operator">&gt;</span> <span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/.cache/huggingface/hub/models--Qwen--Qwen1.5-7B-Chat/blobs/9e8f7873d7c4c74b8883db207a08bf8a783ec8c26da6b3d660a0929048ce6422
├── model-00002-of-00004.safetensors -<span class="token operator">&gt;</span> <span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/.cache/huggingface/hub/models--Qwen--Qwen1.5-7B-Chat/blobs/e573fdaf3eba785c4b31b8858288f762f3541f09d75b53dfb1ae4d8ee5011d65
├── model-00003-of-00004.safetensors -<span class="token operator">&gt;</span> <span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/.cache/huggingface/hub/models--Qwen--Qwen1.5-7B-Chat/blobs/1d7cd36508251baa069a894f6ee98da5929e8b0788ff9c9faa7934ad102f845a
├── model-00004-of-00004.safetensors -<span class="token operator">&gt;</span> <span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/.cache/huggingface/hub/models--Qwen--Qwen1.5-7B-Chat/blobs/189e7a297229937ec93912c940ef05019738c56ff0720c811fd083f5dd400dce
├── model.safetensors.index.json
├── tokenizer.json -<span class="token operator">&gt;</span> <span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/.cache/huggingface/hub/models--Qwen--Qwen1.5-7B-Chat/blobs/33ea6c72ebb92a237fa2bdf26c5ff16592efcdae
├── tokenizer_config.json
└── vocab.json

<span class="token number">0</span> directories, <span class="token number">13</span> files
</code></pre> 
<p>model-00001-of-00004.safetensors、model-00002-of-00004.safetensors 等都是连接到 …/.cache/huggingface/hub 地址的。可能作者考虑到模型的复用问题吧，因此没有将模型直接下载到本地指定目录，而是将软连接建在这个目录底下，这个就不做深究了。</p> 
<h4><a id="2__49"></a>2. 代码部署使用</h4> 
<p>在真正使用前，按照 Qwen 1.5 的官方说明，我们还需要将 transformer 升级到 4.37.0 版本及其以上。如下图：<br> <img src="https://images2.imgbox.com/55/a8/xvBU5g8K_o.png" alt="image.png"><br> 既然这样，那就直接 pip install --upgrade 吧，如下图：</p> 
<pre><code class="prism language-bash"><span class="token punctuation">(</span>base<span class="token punctuation">)</span> yuanzhenhui@MacBook-Pro qwen % pip <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> transformers
Requirement already satisfied: transformers <span class="token keyword">in</span> /Users/yuanzhenhui/anaconda3/lib/python3.11/site-packages <span class="token punctuation">(</span><span class="token number">4.37</span>.0<span class="token punctuation">)</span>
Collecting transformers
  Downloading transformers-4.39.3-py3-none-any.whl.metadata <span class="token punctuation">(</span><span class="token number">134</span> kB<span class="token punctuation">)</span>
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class="token number">134.8</span>/134.8 kB <span class="token number">358.9</span> kB/s eta <span class="token number">0</span>:00:00
Requirement already satisfied: filelock <span class="token keyword">in</span> /Users/yuanzhenhui/anaconda3/lib/python3.11/site-packages <span class="token punctuation">(</span>from transformers<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">3.13</span>.1<span class="token punctuation">)</span>
Requirement already satisfied: huggingface-hub<span class="token operator">&lt;</span><span class="token number">1.0</span>,<span class="token operator">&gt;=</span><span class="token number">0.19</span>.3 <span class="token keyword">in</span> /Users/yuanzhenhui/anaconda3/lib/python3.11/site-packages <span class="token punctuation">(</span>from transformers<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">0.20</span>.3<span class="token punctuation">)</span>
Requirement already satisfied: numpy<span class="token operator">&gt;=</span><span class="token number">1.17</span> <span class="token keyword">in</span> /Users/yuanzhenhui/anaconda3/lib/python3.11/site-packages <span class="token punctuation">(</span>from transformers<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">1.26</span>.4<span class="token punctuation">)</span>
Requirement already satisfied: packaging<span class="token operator">&gt;=</span><span class="token number">20.0</span> <span class="token keyword">in</span> /Users/yuanzhenhui/anaconda3/lib/python3.11/site-packages <span class="token punctuation">(</span>from transformers<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">23.2</span><span class="token punctuation">)</span>
Requirement already satisfied: pyyaml<span class="token operator">&gt;=</span><span class="token number">5.1</span> <span class="token keyword">in</span> /Users/yuanzhenhui/anaconda3/lib/python3.11/site-packages <span class="token punctuation">(</span>from transformers<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">6.0</span>.1<span class="token punctuation">)</span>
Requirement already satisfied: regex<span class="token operator">!=</span><span class="token number">2019.12</span>.17 <span class="token keyword">in</span> /Users/yuanzhenhui/anaconda3/lib/python3.11/site-packages <span class="token punctuation">(</span>from transformers<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">2023.10</span>.3<span class="token punctuation">)</span>
Requirement already satisfied: requests <span class="token keyword">in</span> /Users/yuanzhenhui/anaconda3/lib/python3.11/site-packages <span class="token punctuation">(</span>from transformers<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">2.31</span>.0<span class="token punctuation">)</span>
Requirement already satisfied: tokenizers<span class="token operator">&lt;</span><span class="token number">0.19</span>,<span class="token operator">&gt;=</span><span class="token number">0.14</span> <span class="token keyword">in</span> /Users/yuanzhenhui/anaconda3/lib/python3.11/site-packages <span class="token punctuation">(</span>from transformers<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">0.15</span>.2<span class="token punctuation">)</span>
Requirement already satisfied: safetensors<span class="token operator">&gt;=</span><span class="token number">0.4</span>.1 <span class="token keyword">in</span> /Users/yuanzhenhui/anaconda3/lib/python3.11/site-packages <span class="token punctuation">(</span>from transformers<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">0.4</span>.2<span class="token punctuation">)</span>
Requirement already satisfied: tqdm<span class="token operator">&gt;=</span><span class="token number">4.27</span> <span class="token keyword">in</span> /Users/yuanzhenhui/anaconda3/lib/python3.11/site-packages <span class="token punctuation">(</span>from transformers<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">4.65</span>.0<span class="token punctuation">)</span>
Requirement already satisfied: fsspec<span class="token operator">&gt;=</span><span class="token number">2023.5</span>.0 <span class="token keyword">in</span> /Users/yuanzhenhui/anaconda3/lib/python3.11/site-packages <span class="token punctuation">(</span>from huggingface-hub<span class="token operator">&lt;</span><span class="token number">1.0</span>,<span class="token operator">&gt;=</span><span class="token number">0.19</span>.3-<span class="token operator">&gt;</span>transformers<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">2023.10</span>.0<span class="token punctuation">)</span>
Requirement already satisfied: typing-extensions<span class="token operator">&gt;=</span><span class="token number">3.7</span>.4.3 <span class="token keyword">in</span> /Users/yuanzhenhui/anaconda3/lib/python3.11/site-packages <span class="token punctuation">(</span>from huggingface-hub<span class="token operator">&lt;</span><span class="token number">1.0</span>,<span class="token operator">&gt;=</span><span class="token number">0.19</span>.3-<span class="token operator">&gt;</span>transformers<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">4.9</span>.0<span class="token punctuation">)</span>
Requirement already satisfied: charset-normalizer<span class="token operator">&lt;</span><span class="token number">4</span>,<span class="token operator">&gt;=</span><span class="token number">2</span> <span class="token keyword">in</span> /Users/yuanzhenhui/anaconda3/lib/python3.11/site-packages <span class="token punctuation">(</span>from requests-<span class="token operator">&gt;</span>transformers<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">2.0</span>.4<span class="token punctuation">)</span>
Requirement already satisfied: idna<span class="token operator">&lt;</span><span class="token number">4</span>,<span class="token operator">&gt;=</span><span class="token number">2.5</span> <span class="token keyword">in</span> /Users/yuanzhenhui/anaconda3/lib/python3.11/site-packages <span class="token punctuation">(</span>from requests-<span class="token operator">&gt;</span>transformers<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">3.4</span><span class="token punctuation">)</span>
Requirement already satisfied: urllib<span class="token operator"><span class="token file-descriptor important">3</span>&lt;</span><span class="token number">3</span>,<span class="token operator">&gt;=</span><span class="token number">1.21</span>.1 <span class="token keyword">in</span> /Users/yuanzhenhui/anaconda3/lib/python3.11/site-packages <span class="token punctuation">(</span>from requests-<span class="token operator">&gt;</span>transformers<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">2.0</span>.7<span class="token punctuation">)</span>
Requirement already satisfied: certifi<span class="token operator">&gt;=</span><span class="token number">2017.4</span>.17 <span class="token keyword">in</span> /Users/yuanzhenhui/anaconda3/lib/python3.11/site-packages <span class="token punctuation">(</span>from requests-<span class="token operator">&gt;</span>transformers<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">2024.2</span>.2<span class="token punctuation">)</span>
Downloading transformers-4.39.3-py3-none-any.whl <span class="token punctuation">(</span><span class="token number">8.8</span> MB<span class="token punctuation">)</span>
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class="token number">8.8</span>/8.8 MB <span class="token number">2.6</span> MB/s eta <span class="token number">0</span>:00:00
Installing collected packages: transformers
  Attempting uninstall: transformers
    Found existing installation: transformers <span class="token number">4.37</span>.0
    Uninstalling transformers-4.37.0:
      Successfully uninstalled transformers-4.37.0
Successfully installed transformers-4.39.3
</code></pre> 
<p>轻轻松松就能够升级到 4.39.3 版本了。接下来就可以编写部署代码，如下图：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer

<span class="token triple-quoted-string string">"""
由于 Qwen 1.5 已经整合到 transformers 里面了，因此我这边使用的正是 transformers 的调用方式
"""</span>

<span class="token comment"># 大模型名称和模型定义</span>
model_name <span class="token operator">=</span> <span class="token string">"Qwen/Qwen1.5-7B-Chat"</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">,</span> torch_dtype<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span> device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">)</span>

<span class="token comment"># 这里对大模型角色进行定义</span>
sys_content <span class="token operator">=</span> <span class="token string">"You are a helpful assistant"</span>

<span class="token comment"># 获取千问 token 实例</span>
<span class="token keyword">def</span> <span class="token function">setup_qwen_tokenizer</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span>

<span class="token comment"># 设置问答输入信息</span>
<span class="token keyword">def</span> <span class="token function">setup_model_input</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> prompt<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 判断硬件使用情况，有 cuda 用 cuda 没有 cuda 用 cpu</span>
    <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  
        device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>  
    <span class="token keyword">else</span><span class="token punctuation">:</span>  
        device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cpu"</span><span class="token punctuation">)</span>
    <span class="token comment"># 需要提问的内容的 json 格式    </span>
    messages <span class="token operator">=</span> <span class="token punctuation">[</span>
        <span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> sys_content<span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> prompt<span class="token punctuation">}</span>
    <span class="token punctuation">]</span>
    <span class="token comment"># 该函数将使用提供了标记化器来生成输入文本，然后对其进行标记化并将其转换为PyTorch张量。</span>
    text <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>apply_chat_template<span class="token punctuation">(</span>messages<span class="token punctuation">,</span> tokenize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> add_generation_prompt<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> 
    <span class="token keyword">return</span> tokenizer<span class="token punctuation">(</span><span class="token punctuation">[</span>text<span class="token punctuation">]</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

<span class="token comment"># 提交问题并获取回复</span>
<span class="token keyword">def</span> <span class="token function">msg_generate</span><span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">:</span>
    tokenizer <span class="token operator">=</span> setup_qwen_tokenizer<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 整理模型所需的输入信息</span>
    model_inputs <span class="token operator">=</span> setup_model_input<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> prompt<span class="token punctuation">)</span>
    <span class="token comment"># 根据模型生成id集合</span>
    generated_ids <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>model_inputs<span class="token punctuation">.</span>input_ids<span class="token punctuation">,</span> max_new_tokens<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">)</span>
    <span class="token comment"># 删除没有响应的id</span>
    generated_ids <span class="token operator">=</span> <span class="token punctuation">[</span>output_ids<span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token keyword">for</span> input_ids<span class="token punctuation">,</span> output_ids <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>
        model_inputs<span class="token punctuation">.</span>input_ids<span class="token punctuation">,</span> generated_ids<span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token comment"># 根据id集合对返回信息进行解码获得返回结果</span>
    <span class="token keyword">return</span> tokenizer<span class="token punctuation">.</span>batch_decode<span class="token punctuation">(</span>generated_ids<span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token comment"># 这里提供一个调用实例</span>
    prompt <span class="token operator">=</span> <span class="token string">"中医药理论是否能解释并解决全身乏力伴随心跳过速的症状？"</span>
    response <span class="token operator">=</span> msg_generate<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"&gt;&gt;&gt; "</span><span class="token operator">+</span>response<span class="token punctuation">)</span>
</code></pre> 
<p>将 msg_generate 封装之后就能够根据需要提供对外服务了（在外层加一层 flask 不就可以直接提供接口了么，再加个精美一点的 UI 界面，老板又可以割一波韭菜了，技术人员保住饭碗不是梦…开个玩笑）。具体的执行效果如下图：</p> 
<pre><code class="prism language-bash"><span class="token punctuation">(</span>base<span class="token punctuation">)</span> yuanzhenhui@MacBook-Pro qwen % python qwen_model.py 
Loading checkpoint shards: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████<span class="token operator">|</span> <span class="token number">4</span>/4 <span class="token punctuation">[</span>00:0<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:00,  <span class="token number">8</span>.75it/s<span class="token punctuation">]</span>
WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.
Special tokens have been added <span class="token keyword">in</span> the vocabulary, <span class="token function">make</span> sure the associated word embeddings are fine-tuned or trained.
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> 中医药理论确实可以尝试解释和治疗全身乏力和心跳过速的综合症状，但需要具体辨证论治，因为中医认为人体的病症是内外环境、脏腑功能失衡的结果。以下是可能的解释和治疗方法：

<span class="token number">1</span>. 全身乏力：中医认为这是“气虚”、“血虚”或“阴阳两虚”的表现。气虚可能导致人体机能下降，运化无力；血虚则可能影响血液滋养全身，导致四肢乏力。心悸（心跳过速）可能是心气不宁，心神不安，或是心血不足，心脉瘀阻等问题。调理方法可能包括补气养血、益心安神等，比如黄芪、人参、当归、熟地、枣仁等中药。

<span class="token number">2</span>. 心跳过速：中医认为可能是心火上炎、心血瘀阻、心神不宁等引起。针对不同的病因，可能采用清热解毒、活血化瘀、镇静安神等方法，如黄连、丹参、麦冬、柏子仁等。

<span class="token number">3</span>. 实践中，中医会通过望、闻、问、切四诊合参，全面了解患者的整体情况，然后开具个性化的处方。如果心跳过速伴有其他症状，如胸闷、心慌、失眠、面色苍白等，可能还需要结合西医的心电图检查和其他检查结果。

<span class="token number">4</span>. 请注意，虽然中医药有其独特的疗效，但对于严重的或持续存在的症状，还是建议及时就医，以西医的诊断和治疗为主，中医辅助调养。

总的来说，中医药在调理全身状况，改善亚健康状态，特别是对于一些慢性病的治疗上有独特优势，但对于急性病和严重症状，应遵循医嘱，中西医结合为宜。
</code></pre> 
<p>由于我用的是 Mac 因此只能用 cpu 跑，就一个问题也跑了将近 20 分钟，所以建议大家本地部署还是用 gpu 吧。当然了千问团队也很贴心弄了GGUF 版本（专门为 cpu 用户提供的），但是我没有尝试过我就不多说什么了。</p> 
<p>但说真的能够代码部署那后面的可塑性就很强了，相信接下来会有更多的企业投身到“人工智能+”的行列吧，在这里真的要感谢阿里千问团队付出的努力和无私的分享。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ced819f908ca6c95cd9bc77905493641/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">AI绘画SD 教程 - 采样器详解及对比，建议收藏！</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6cc7407aea9ab466cc53d26f21706eb3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">ElasticSearch中使用bge-large-zh-v1.5进行向量检索（一）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>