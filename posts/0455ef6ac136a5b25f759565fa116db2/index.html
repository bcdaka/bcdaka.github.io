<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>大数据开发之Hadoop - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/0455ef6ac136a5b25f759565fa116db2/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="大数据开发之Hadoop">
  <meta property="og:description" content="大数据开发之Hadoop Hadoop的发展Hadoop的三个功能组件一、HDFS 分布式文件系统 1、HDFS的基础架构2、HDFS基础操作命令3、HDFS WEB浏览：4、Big Data Tools插件5、使用NFS网关功能将HDFS挂载到本地系统6、HDFS数据存储7、NameNode 元数据8、SecondaryNameNode的作用 二、MapReduce 分布式计算 1、大数据体系内的计算， 举例：2、分布式（数据）计算 的两种模式3、分布式计算框架 - MapReduce 三、Yarn 分布式资源调度 1、资源调度2、Yarn核心架构3、Yarn容器4、Yarn辅助架构 提交MapReduce程序至YARN运行 大数据的核心工作、软件生态
大数据的核心工作解释大数据软件生态存储妥善保存海量待处理数据Apache Hadoop HDFS、Apache HBase、Apache Kudu、云平台计算完成海量数据的价值挖掘Apache Hadoop MapReduce、Apache Spark、Apache Flink传输协助各个环节的数据传输fApache Kafka、Apache Pulsar、Apache Flume、Apache Sqoop Hadoop的发展 Hadoop创始人：Doug Cutting
Hadoop起源于Apache Lucene子项目：Nutch：Nutch的设计目标是构建一个大型的全网搜索引擎。遇到瓶颈：如何解决数十亿网页的存储和索引问题
Google三篇论文：
《The Google file system》：谷歌分布式文件系统GFS
《MapReduce: Simpliﬁed Data Processing on Large Clusters》：谷歌分布式计算框架MapReduce
《Bigtable: A Distributed Storage System for Structured Data》：谷歌结构化数据存储系统
Hadoop商业发行版本：
CDH（Cloudera’s Distribution, including Apache Hadoop） Cloudera公司出品，目前使用最多的商业版HDP（Hortonworks Data Platform），Hortonworks公司出品，目前被Cloudera收购星环，国产商业版，星环公司出品，在国内政企使用较多 Hadoop的三个功能组件 HDFS组件：HDFS是Hadoop内的分布式存储组件。可以构建分布式文件系统用于数据存储MapReduce组件：MapReduce是Hadoop内分布式计算组件。提供编程接口供用户开发分布式计算程序YARN组件：YARN是Hadoop内分布式资源调度组件。可供用户整体调度大规模集群的资源使用。 一、HDFS 分布式文件系统 HDFS全称：Hadoop Distributed File System">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-18T20:58:14+08:00">
    <meta property="article:modified_time" content="2024-07-18T20:58:14+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大数据开发之Hadoop</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="Hadoop_0"></a>大数据开发之Hadoop</h2> 
<ul><li><a href="#t1" rel="nofollow">Hadoop的发展</a></li><li><a href="#t2" rel="nofollow">Hadoop的三个功能组件</a></li><li><a href="#t3" rel="nofollow">一、HDFS 分布式文件系统</a> 
  <ul><li><a href="#t4" rel="nofollow">1、HDFS的基础架构</a></li><li><a href="#t5" rel="nofollow">2、HDFS基础操作命令</a></li><li><a href="#t6" rel="nofollow">3、HDFS WEB浏览：</a></li><li><a href="#t7" rel="nofollow">4、Big Data Tools插件</a></li><li><a href="#t8" rel="nofollow">5、使用NFS网关功能将HDFS挂载到本地系统</a></li><li><a href="#t9" rel="nofollow">6、HDFS数据存储</a></li><li><a href="#t10" rel="nofollow">7、NameNode 元数据</a></li><li><a href="#t11" rel="nofollow">8、SecondaryNameNode的作用</a></li></ul> </li><li><a href="#t12" rel="nofollow">二、MapReduce 分布式计算</a> 
  <ul><li><a href="#t13" rel="nofollow">1、大数据体系内的计算， 举例：</a></li><li><a href="#t14" rel="nofollow">2、分布式（数据）计算 的两种模式</a></li><li><a href="#t15" rel="nofollow">3、分布式计算框架 - MapReduce</a></li></ul> </li><li><a href="#t16" rel="nofollow">三、Yarn 分布式资源调度</a> 
  <ul><li><a href="#t17" rel="nofollow">1、资源调度</a></li><li><a href="#t18" rel="nofollow">2、Yarn核心架构</a></li><li><a href="#t19" rel="nofollow">3、Yarn容器</a></li><li><a href="#t20" rel="nofollow">4、Yarn辅助架构</a></li></ul> </li><li><a href="#t21" rel="nofollow">提交MapReduce程序至YARN运行</a></li></ul> 
<p><strong>大数据的核心工作、软件生态</strong></p> 
<table><thead><tr><th>大数据的核心工作</th><th>解释</th><th>大数据软件生态</th></tr></thead><tbody><tr><td>存储</td><td>妥善保存海量待处理数据</td><td>Apache Hadoop HDFS、Apache HBase、Apache Kudu、云平台</td></tr><tr><td>计算</td><td>完成海量数据的价值挖掘</td><td>Apache Hadoop MapReduce、Apache Spark、Apache Flink</td></tr><tr><td>传输</td><td>协助各个环节的数据传输</td><td>fApache Kafka、Apache Pulsar、Apache Flume、Apache Sqoop</td></tr></tbody></table> 
<h3><a id="Hadoop_31"></a>Hadoop的发展</h3> 
<p><strong>Hadoop创始人</strong>：Doug Cutting<br> <strong>Hadoop起源于Apache Lucene子项目：Nutch</strong>：Nutch的设计目标是构建一个大型的全网搜索引擎。遇到瓶颈：如何解决数十亿网页的存储和索引问题<br> <strong>Google三篇论文</strong>：</p> 
<ul><li> <p>《The Google file system》：谷歌分布式文件系统GFS</p> </li><li> <p>《MapReduce: Simpliﬁed Data Processing on Large Clusters》：谷歌分布式计算框架MapReduce</p> </li><li> <p>《Bigtable: A Distributed Storage System for Structured Data》：谷歌结构化数据存储系统</p> </li></ul> 
<p><strong>Hadoop商业发行版本</strong>：</p> 
<ul><li>CDH（Cloudera’s Distribution, including Apache Hadoop） Cloudera公司出品，目前使用最多的商业版</li><li>HDP（Hortonworks Data Platform），Hortonworks公司出品，目前被Cloudera收购</li><li>星环，国产商业版，星环公司出品，在国内政企使用较多</li></ul> 
<h3><a id="Hadoop_49"></a>Hadoop的三个功能组件</h3> 
<ul><li><strong>HDFS组件</strong>：HDFS是Hadoop内的分布式存储组件。可以构建分布式文件系统用于数据存储</li><li><strong>MapReduce组件</strong>：MapReduce是Hadoop内分布式计算组件。提供编程接口供用户开发分布式计算程序</li><li><strong>YARN组件</strong>：YARN是Hadoop内分布式资源调度组件。可供用户整体调度大规模集群的资源使用。</li></ul> 
<h3><a id="HDFS__55"></a>一、HDFS 分布式文件系统</h3> 
<blockquote> 
 <p><code>HDFS</code>全称：Hadoop Distributed File System<br> 是<code>Hadoop</code>三大组件（<code>HDFS</code>、<code>MapReduce</code>、<code>YARN</code>）之一<br> 可在多台服务器上构建集群，提供分布式数据存储能力</p> 
 <p><strong>文件系统协议</strong><br> <code>file://</code> 表示Linux本地文件<br> <code>hdfs://namenode_server:port/</code> 表示HDFS文件系统<br> 比如当前集群表示为：hdfs://node1:8020/。一般可以省略file://和hdfs://协议头，不用写</p> 
</blockquote> 
<h4><a id="1HDFS_66"></a>1、HDFS的基础架构</h4> 
<img src="https://images2.imgbox.com/1d/cc/Y4BAj6Is_o.png" width="500"> 
<p><strong>NameNode</strong>：主角色，负责管理HDFS整个文件系统 和 DataNode</p> 
<p><strong>Datanode</strong>：从角色，主要负责数据的存储，即存入数据和取出数据</p> 
<p><strong>SecondaryNameNode</strong>: 辅助角色，主要帮助NameNode完成元数据整理工作(打杂)</p> 
<h4><a id="2HDFS_76"></a>2、HDFS基础操作命令</h4> 
<p>HDFS同Linux系统一样，均是以/作为根目录的组织形式</p> 
<p>官方文档：<a href="https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-common/FileSystemShell.html" rel="nofollow">Apache Hadoop 3.3.4 – Overview</a></p> 
<pre><code class="prism language-sh">hdfs dfs <span class="token punctuation">[</span>generic options<span class="token punctuation">]</span>
</code></pre> 
<pre><code class="prism language-bash"><span class="token comment"># 创建文件夹</span>
hdfs dfs <span class="token parameter variable">-mkdir</span> <span class="token punctuation">[</span>-p<span class="token punctuation">]</span> <span class="token operator">&lt;</span>path<span class="token operator">&gt;</span> <span class="token punctuation">..</span>.		<span class="token comment"># -p选项的行为与Linux mkdir -p一致，它会沿着路径创建父目录。</span>

<span class="token comment"># 查看指定目录下内容</span>
hdfs dfs <span class="token parameter variable">-ls</span> <span class="token punctuation">[</span>-h<span class="token punctuation">]</span> <span class="token punctuation">[</span>-R<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">&lt;</span>path<span class="token operator">&gt;</span> <span class="token punctuation">..</span>.<span class="token punctuation">]</span> 	<span class="token comment"># -h 人性化显示文件size、-R 递归查看指定目录及其子目录</span>

<span class="token comment"># 上传文件到HDFS指定目录下</span>
hdfs dfs <span class="token parameter variable">-put</span> <span class="token punctuation">[</span>-f<span class="token punctuation">]</span> <span class="token punctuation">[</span>-p<span class="token punctuation">]</span> <span class="token operator">&lt;</span>localsrc<span class="token operator">&gt;</span> <span class="token punctuation">..</span>. <span class="token operator">&lt;</span>dst<span class="token operator">&gt;</span>

<span class="token comment"># 查看HDFS文件内容</span>
hdfs dfs <span class="token parameter variable">-cat</span> <span class="token operator">&lt;</span>src<span class="token operator">&gt;</span> <span class="token punctuation">..</span>.
<span class="token comment"># 读取大文件可以使用管道符配合more</span>
hdfs dfs <span class="token parameter variable">-cat</span> <span class="token operator">&lt;</span>src<span class="token operator">&gt;</span> <span class="token operator">|</span> <span class="token function">more</span>

<span class="token comment"># 下载HDFS文件</span>
hdfs dfs <span class="token parameter variable">-get</span> <span class="token punctuation">[</span>-f<span class="token punctuation">]</span> <span class="token punctuation">[</span>-p<span class="token punctuation">]</span> <span class="token operator">&lt;</span>src<span class="token operator">&gt;</span> <span class="token punctuation">..</span>. <span class="token operator">&lt;</span>localdst<span class="token operator">&gt;</span>
<span class="token comment"># 下载文件到本地文件系统指定目录，localdst必须是目录</span>

<span class="token comment"># 拷贝HDFS文件</span>
hdfs dfs <span class="token parameter variable">-cp</span> <span class="token punctuation">[</span>-f<span class="token punctuation">]</span> <span class="token operator">&lt;</span>src<span class="token operator">&gt;</span> <span class="token punctuation">..</span>. <span class="token operator">&lt;</span>dst<span class="token operator">&gt;</span>

<span class="token comment"># 追加数据到HDFS文件中</span>
hdfs dfs <span class="token parameter variable">-appendToFile</span> <span class="token operator">&lt;</span>localsrc<span class="token operator">&gt;</span> <span class="token punctuation">..</span>. <span class="token operator">&lt;</span>dst<span class="token operator">&gt;</span>	<span class="token comment"># 如果&lt;localSrc&gt;为-，则输入为从标准输入中读取、 dst如果文件不存在，将创建该文件。 </span>

<span class="token comment"># HDFS数据移动操作</span>
hdfs dfs <span class="token parameter variable">-mv</span> <span class="token operator">&lt;</span>src<span class="token operator">&gt;</span> <span class="token punctuation">..</span>. <span class="token operator">&lt;</span>dst<span class="token operator">&gt;</span>

<span class="token comment"># HDFS数据删除操作</span>
hdfs dfs <span class="token parameter variable">-rm</span> <span class="token parameter variable">-r</span> <span class="token punctuation">[</span>-skipTrash<span class="token punctuation">]</span> URI <span class="token punctuation">[</span>URI <span class="token punctuation">..</span>.<span class="token punctuation">]</span>	<span class="token comment"># -skipTrash 跳过回收站，直接删除</span>
</code></pre> 
<ul><li> <p><code>path</code> 为待创建的目录</p> </li><li> <p><code>-f</code> 覆盖目标文件（已存在下）</p> </li><li> <p><code>-p</code> 保留访问和修改时间，所有权和权限。</p> </li><li> <p><code>localsrc</code> 本地文件系统（客户端所在机器）</p> </li><li> <p><code>dst</code> 目标文件系统（HDFS）</p> </li></ul> 
<h4><a id="3HDFS_WEB_127"></a>3、HDFS WEB浏览：</h4> 
<p>http://node1:9870</p> 
<img src="https://images2.imgbox.com/bf/09/ka7ykdTc_o.png" width="600"> 
<img src="https://images2.imgbox.com/ca/75/IXdltJQs_o.png" width="600"> 
<p>使用WEB浏览操作文件系统，一般会遇到权限问题</p> 
<p><img src="https://images2.imgbox.com/bc/b3/d5ZA2ryr_o.png" alt="在这里插入图片描述"></p> 
<p>这是因为WEB浏览器中是以匿名用户（dr.who）登陆的，其只有只读权限，多数操作是做不了的。如果需要以特权用户在浏览器中进行操作，需要配置如下内容到<code>core-site.xml</code>并重启集群</p> 
<pre><code class="prism language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>hadoop.http.staticuser.user<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>username<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
<p>不推荐这样做。HDFS WEBUI，只读权限挺好的，简单浏览即可，如果给与高权限，会有很大的安全问题，造成数据泄露或丢失</p> 
<h4><a id="4Big_Data_Tools_152"></a>4、Big Data Tools插件</h4> 
<blockquote> 
 <p>在Jetbrains的产品中，均可以安装插件，其中：Big Data Tools插件可以帮助我们方便的操作HDFS</p> 
</blockquote> 
<ol><li> <p>下载插件</p> <p>在设置-&gt;Plugins（插件）-&gt; Marketplace（市场），搜索Big Data Tools，点击Install安装即可</p> <img src="https://images2.imgbox.com/de/44/Pqa0Psna_o.png" width="700"> </li><li> <p>需要对Windows系统做一些基础设置，配合插件使用</p> 
  <ul><li>解压Hadoop安装包到Windows系统，如解压到：E:\hadoop-3.3.4</li><li>设置$HADOOP_HOME环境变量指向：E:\hadoop-3.3.4</li><li>下载 <a href="https://github.com/steveloughran/winutils/blob/master/hadoop-3.0.0/bin/hadoop.dll">hadoop.dll</a> <a href="https://github.com/steveloughran/winutils/blob/master/hadoop-3.0.0/bin/winutils.exe">winutils.exe</a></li><li>将hadoop.dll和winutils.exe放入E:\hadoop-3.3.4/bin中</li><li>重启IDEA</li></ul> </li><li> <p>插件使用</p> <p><img src="https://images2.imgbox.com/b5/6d/FV10H8nr_o.png" width="400"><img src="https://images2.imgbox.com/cb/6c/4BZbWAU0_o.png" width="380"></p> <p><img src="https://images2.imgbox.com/36/b2/U8Cc05e4_o.png" width="400"><img src="https://images2.imgbox.com/91/c5/7f6xgWyi_o.png" width="300"></p> <p>还可以插看和在修改文件内容</p> </li></ol> 
<h4><a id="5NFSHDFS_179"></a>5、使用NFS网关功能将HDFS挂载到本地系统</h4> 
<blockquote> 
 <p>HDFS提供了基于NFS（Network File System）的插件，可以对外提供NFS网关，供其它系统挂载使用。<br> NFS 网关支持 NFSv3，并允许将 HDFS 作为客户机本地文件系统的一部分挂载，现在支持：上传、下载、删除、追加内容<br> 如下图，将HDFS挂载为Windows文件管理器的网络位置，(<strong>NFS功能需要windows专业版</strong>)</p> 
 <p><img src="https://images2.imgbox.com/0c/76/z3fzeZUG_o.png" alt="在这里插入图片描述"></p> 
</blockquote> 
<ol><li> <p>在<code>core-site.xml</code>内新增如下两项</p> <pre><code class="prism language-xml"><span class="token comment">&lt;!-- 允许hadoop用户代理任何其它用户组 --&gt;</span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>hadoop.proxyuser.[username].groups<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>	
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>

<span class="token comment">&lt;!-- 允许代理任意服务器的请求 --&gt;</span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>hadoop.proxyuser.[username].hosts<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> </li><li> <p>在<code>hdfs-site.xml</code>中新增如下项</p> <pre><code class="prism language-xml"><span class="token comment">&lt;!-- NFS操作HDFS系统，所使用的超级用户（hdfs的启动用户为超级用户） --&gt;</span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>nfs.superuser<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>[username]<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>

<span class="token comment">&lt;!-- NFS接收数据上传时使用的临时目录 --&gt;</span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>nfs.dump.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>/tmp/.hdfs-nfs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>

<span class="token comment">&lt;!-- NFS允许连接的客户端IP和权限，rw表示读写，IP整体或部分可以以*代替 --&gt;</span>
<span class="token comment">&lt;!-- 192.168.88.1这个IP是电脑虚拟网卡VMnet8的IP，连接虚拟机就走这个网卡 --&gt;</span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>nfs.exports.allowed.hosts<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>192.168.88.1 rw<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>	
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> </li><li> <p>将配置好的<code>core-site.xml</code>和<code>hdfs-site.xml</code>分发到其他节点、重启Hadoop HDFS集群（先stop-dfs.sh，后start-dfs.sh）</p> </li><li> <p>停止系统的NFS相关进程</p> <pre><code class="prism language-sh">systemctl stop nfs
systemctl disable nfs	<span class="token comment"># 关闭系统nfs并关闭其开机自启</span>
yum remove <span class="token parameter variable">-y</span> rpcbind    <span class="token comment"># 卸载系统自带rpcbind</span>
</code></pre> </li><li> <p>启动portmap（HDFS自带的rpcbind功能）（必须以root执行）</p> <pre><code class="prism language-sh">hdfs <span class="token parameter variable">--daemon</span> start portmap
</code></pre> </li><li> <p>启动nfs（HDFS自带的nfs功能）（必须以hadoop用户执行）</p> <pre><code class="prism language-sh">hdfs <span class="token parameter variable">--daemon</span> start nfs3
</code></pre> </li><li> <p>开启Windows的NFS功能<br> <img src="https://images2.imgbox.com/59/d4/5Lh64y2r_o.png" width="400"><img src="https://images2.imgbox.com/06/11/ZxSqhyrg_o.png" width="280"></p> <p><strong>此功能需要专业版，如果是家庭版Windows需要升级为专业版</strong></p> </li><li> <p>在Windows命令提示符（CMD）内输入</p> <pre><code>net use G: \\192.168.88.101\!	
</code></pre> <p><code>192.168.88.101</code> 为 NameNode主机IP地址</p> <p><code>G:</code> 盘符名称（任意，与已有不重复即可)</p> </li><li> <p>完成后即可在文件管理器中看到盘符为G的网络位置（见上图）</p> </li></ol> 
<p>至此，就将HDFS挂载到Windows文件管理器内了可以进行上传、下载、改名、删除、追加文本等操作。</p> 
<ol start="10"><li>点击右键客户断开连接</li></ol> 
<h4><a id="6HDFS_270"></a>6、HDFS数据存储</h4> 
<blockquote> 
 <p><strong>当客户端向 HDFS 写入文件时</strong>：</p> 
 <ol><li> <p>客户端向 NameNode 请求写操作。</p> </li><li> <p>NameNode审核权限、剩余空间后，满足条件允许写入，并告知客户端写入的DataNode地址</p> </li><li> <p>客户端将数据分成块，并将每个块写入到第一个 DataNode。</p> </li><li> <p>第一个 DataNode 将块复制到第二个 DataNode，第二个 DataNode 再复制到第三个 DataNode，依此类推，直到达到副本因子。</p> </li><li> <p>每个 DataNode 在成功存储块后，向 NameNode 发送报告。</p> </li></ol> 
 <p><img src="https://images2.imgbox.com/d3/7a/RaI1uUJO_o.png" alt="在这里插入图片描述"></p> 
</blockquote> 
<blockquote> 
 <p><strong>当客户端从 HDFS 读取文件时</strong>：</p> 
 <ol><li> <p>客户端向 NameNode 请求读取操作。</p> </li><li> <p>NameNode判断客户端权限等细节后，允许读取，并返回此文件的block列表</p> </li><li> <p>客户端拿到block列表后自行寻找DataNode读取即可</p> </li></ol> 
 <p><img src="https://images2.imgbox.com/02/ac/VLr4k2U5_o.png" alt="在这里插入图片描述"></p> 
</blockquote> 
<p><strong>数据存储</strong></p> 
<p>HDFS 将文件拆分成多个块（默认块大小为 128MB），每个块被存储在不同的 DataNode 上：</p> 
<ul><li><strong>块（Block）</strong>：文件被分割成大小相等的数据块。块大小可以在 HDFS 配置中设置（通常为 128MB 或 256MB）。</li><li><strong>副本（Replication）</strong>：每个数据块会被复制到多个 DataNode 上（默认副本数为 3）。这种数据冗余提供了容错性和高可用性。</li></ul> 
<p><strong>副本（Replication）</strong></p> 
<pre><code class="prism language-xml"><span class="token comment">&lt;!-- hdfs-site.xml中配置 设置默认文件上传到HDFS中拥有的副本数量 --&gt;</span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.replication<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
<pre><code class="prism language-sh"><span class="token comment"># 可以在上传文件的时候，临时决定被上传文件以多少个副本存储</span>
hdfs dfs <span class="token parameter variable">-D</span> <span class="token assign-left variable">dfs.replication</span><span class="token operator">=</span><span class="token number">2</span> <span class="token parameter variable">-put</span> test.txt /tmp/

<span class="token comment"># 对于已经存在HDFS的文件，修改dfs.replication属性不会生效, 如果要修改已存在文件可以通过命令</span>
<span class="token comment"># 如下命令，指定path的内容将会被修改为2个副本存储</span>
hdfs dfs <span class="token parameter variable">-setrep</span> <span class="token punctuation">[</span>-R<span class="token punctuation">]</span> <span class="token number">2</span> path	<span class="token comment"># -R选项可选，使用-R表示对子目录也生效</span>

<span class="token comment"># 使用hdfs提供的fsck命令来检查文件的副本数</span>
hdfs <span class="token function">fsck</span> path <span class="token punctuation">[</span>-files <span class="token punctuation">[</span>-blocks <span class="token punctuation">[</span>-locations<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
</code></pre> 
<ul><li><code>-files</code>可以列出路径内的文件状态</li><li><code>-files -blocks</code> 输出文件块报告（有几个块，多少副本）</li><li><code>-files -blocks -locations</code> 输出每一个block的详情</li></ul> 
<p><strong>块（Block）</strong></p> 
<p>对于块（block），hdfs默认设置为256MB一个，也就是1GB文件会被划分为4个block存储。</p> 
<pre><code class="prism language-xml"><span class="token comment">&lt;!-- hdfs-site.xml中配置 块大小可以通过参数 --&gt;</span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.blocksize<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>268435456<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">&gt;</span></span>设置HDFS块大小，单位是b<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
<h4><a id="7NameNode__344"></a>7、NameNode 元数据</h4> 
<p>NameNode基于edits和FSImage的配合，完成整个文件系统文件的管理。</p> 
<ol><li>每次对HDFS的操作，均被edits文件记录</li><li>edits达到大小上线后，开启新的edits记录</li><li>定期进行edits的合并操作<br> 如当前没有fsimage文件， 将全部edits合并为第一个fsimage<br> 如当前已存在fsimage文件，将全部edits和已存在的fsimage进行合并，形成新的fsimage</li></ol> 
<p><img src="https://images2.imgbox.com/35/f0/mtQTCm69_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-xml"><span class="token comment">&lt;!-- hdfs-site.xml中配置 元数据位置如下/data/nn/current --&gt;</span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.namenode.name.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>/data/nn<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
<p>元数据合并控制参数</p> 
<p>对于元数据的合并，是一个定时过程，基于：</p> 
<ul><li><code>dfs.namenode.checkpoint.period</code>，默认3600（秒）即1小时</li><li><code>dfs.namenode.checkpoint.txns</code>，默认1000000，即100W次事务</li></ul> 
<p>即每1小时或发生100W次事务合并(有一个达到条件就执行)<br> 检查是否达到条件，默认60秒检查一次，基于：</p> 
<ul><li><code>dfs.namenode.checkpoint.check.period</code>，默认60（秒），来决定</li></ul> 
<h4><a id="8SecondaryNameNode_377"></a>8、SecondaryNameNode的作用</h4> 
<p>NameNode的元数据并不是有NameNode本身完成，NameNode不负责数据写入，只负责元数据记录和权限审批<br> SecondaryNameNode会通过http从NameNode拉取数据（edits和fsimage）<br> 然后合并完成后提供给NameNode使用。</p> 
<h3><a id="MapReduce__383"></a>二、MapReduce 分布式计算</h3> 
<h4><a id="1__385"></a>1、大数据体系内的计算， 举例：</h4> 
<p>销售额统计、区域销售占比、季度销售占比<br> 利润率走势、客单价走势、成本走势<br> 品类分析、消费者分析、店铺分析</p> 
<p><strong>数据太大，一台计算机无法独立处理、靠数量来取胜</strong></p> 
<p><strong>计算</strong>：对数据进行处理，使用统计分析等手段得到需要的结果<br> <strong>分布式计算</strong>：多台服务器协同工作，共同完成一个计算任务</p> 
<h4><a id="2__396"></a>2、分布式（数据）计算 的两种模式</h4> 
<p><strong>分散-&gt;汇总模式：（MapReduce就是这种模式）</strong></p> 
<ol><li> <p>将数据分片，多台服务器各自负责一部分数据处理</p> </li><li> <p>然后将各自的结果，进行汇总处理</p> </li><li> <p>最终得到想要的计算结果</p> </li></ol> 
<p><strong>中心调度-&gt;步骤执行模式：（大数据体系的Spark、Flink等是这种模式）</strong></p> 
<ol><li> <p>由一个节点作为中心调度管理者</p> </li><li> <p>将任务划分为几个具体步骤</p> </li><li> <p>管理者安排每个机器执行任务</p> </li><li> <p>最终得到结果数据</p> </li></ol> 
<h4><a id="3__MapReduce_413"></a>3、分布式计算框架 - MapReduce</h4> 
<p>MapReduce是“分散-&gt;汇总”模式的分布式计算框架，可供开发人员开发相关程序进行分布式数据计算。<br> 提供了2个编程接口：Map 和 Reduce</p> 
<ul><li>Map功能接口提供了“分散”的功能， 由服务器分布式对数据进行处理</li><li>Reduce功能接口提供了“汇总（聚合）”的功能，将分布式的处理结果汇总统计</li></ul> 
<p><strong>MapReduce执行原理 (单词统计样例)</strong></p> 
<p>假定有4台服务器用以执行MapReduce任务可以3台服务器执行Map，1台服务器执行Reduce</p> 
<p><img src="https://images2.imgbox.com/c3/a0/0TrkD6zE_o.png" alt="在这里插入图片描述"></p> 
<p>​ 文件 将任务分解为：3个Map(分散) Task(任务) 1个Reduce(汇总) Task</p> 
<h3><a id="Yarn__430"></a>三、Yarn 分布式资源调度</h3> 
<h4><a id="1_432"></a>1、资源调度</h4> 
<ul><li><strong>资源</strong>：服务器硬件资源，如：CPU、内存、硬盘、网络等</li><li><strong>资源调度</strong>：管控服务器硬件资源，提供更好的利用率</li><li><strong>分布式资源调度</strong>：管控整个分布式服务器集群的全部资源，整合进行统一调度</li></ul> 
<p><strong>对于资源的利用，有规划、有管理的调度资源使用，是效率最高的方式</strong></p> 
<p>YARN 管控整个集群的资源进行调度， 那么应用程序在运行时，就是在YARN的监管（管理）下去运行的。<br> 这就像：全部资源都是公司（YARN）的，由公司分配给个人（具体的程序）去使用。</p> 
<p>YARN用来调度资源给MapReduce分配和管理运行资源，所以，MapReduce需要YARN才能执行（普遍情况）</p> 
<h4><a id="2Yarn_445"></a>2、Yarn核心架构</h4> 
<img src="https://images2.imgbox.com/16/96/EXyXU9Tw_o.png" width="500"> 
<p><strong>主(Master)角色</strong>：ResourceManager：整个集群的资源调度者， 负责协调调度各个程序所需的资源。</p> 
<p><strong>从(Slave)角色</strong>：NodeManager：单个服务器的资源调度者，负责调度单个服务器上的资源提供给应用程序使用。</p> 
<h4><a id="3Yarn_453"></a>3、Yarn容器</h4> 
<p>如何实现服务器上精准分配如下的硬件资源呢？</p> 
<ul><li>容器（Container）是YARN的NodeManager在所属服务器上分配资源的手段</li><li>创建一个资源容器，即由NodeManager占用这部分资源</li><li>然后应用程序运行在NodeManager创建的这个容器内</li><li>应用程序无法突破容器的资源限制</li></ul> 
<h4><a id="4Yarn_462"></a>4、Yarn辅助架构</h4> 
<p>YARN的架构中除了核心角色还可以搭配2个辅助角色使得YARN集群运行更加稳定</p> 
<p><strong>代理服务器(ProxyServer)</strong>：Web Application Proxy Web应用程序代理</p> 
<p><strong>历史服务器(JobHistoryServer)</strong>： 应用程序历史信息记录服务</p> 
<p><strong>1、代理服务器</strong></p> 
<ul><li> <p>代理服务器，即Web应用代理是 YARN 的一部分。默认情况下，它将作为资源管理器(RM)的一部分运行，但是可以配置为在独立模式下运行。使用代理的原因是为了减少通过 YARN 进行基于网络的攻击的可能性。</p> </li><li> <p>这是因为， YARN在运行时会提供一个WEB UI站点（同HDFS的WEB UI站点一样）可供用户在浏览器内查看YARN的运行信息</p> </li><li> <p>开启代理服务器，可以提高YARN在开放网络中的安全性 （但不是绝对安全只能是辅助提高一些）</p> </li><li> <p>统一收集到HDFS，由历史服务器托管为WEB UI供用户在浏览器统一查看</p> </li></ul> 
<p>代理服务器默认集成在了ResourceManager中，也可以将其分离出来单独启动，<br> 如果要分离代理服务器，在<code>yarn-site.xml</code>中配置<code> yarn.web-proxy.address</code>参数<br> 并通过命令启动它即可 <code>$HADOOP_YARN_HOME/sbin/yarn-daemon.sh start proxyserver</code></p> 
<pre><code class="prism language-xml"><span class="token comment">&lt;!-- yarn-site.xml --&gt;</span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>yarn.web-proxy.address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>node01:8089<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">&gt;</span></span>代理服务器主机和端口<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
<p><strong>2、JobHistoryServer历史服务器</strong></p> 
<ul><li>提供WEB UI站点，供用户在浏览器上查看程序日志</li><li>可以保留历史数据，随时查看历史运行程序信息</li></ul> 
<p>运行日志，产生在多个容器中，太零散了难以查看，所以需要历史服务器</p> 
<p>JobHistoryServer需要配置：</p> 
<ul><li> <p>开启日志聚合，即从容器中抓取日志到HDFS集中存储</p> <pre><code class="prism language-xml"><span class="token comment">&lt;!-- yarn-site.xml --&gt;</span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>yarn.log-aggregation-enable<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">&gt;</span></span>开启日志聚合<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>yarn.nodemanager.remote-app-log-dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>/tmp/logs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">&gt;</span></span>程序日常HDFS的存储路径<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> </li><li> <p>配置历史服务器端口和主机</p> <pre><code class="prism language-xml"><span class="token comment">&lt;!-- mapred-site.xml --&gt;</span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>mapreduce.jobhistory.webapp.address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>node01:19888<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">&gt;</span></span>配置历史服务器web端口为node0的19888端口<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>mapreduce.jobhistory.address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>node01:10020<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>description</span><span class="token punctuation">&gt;</span></span>历史服务器通讯端口为node01:10020<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>description</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre> </li><li> <p>启动和停止：<code>$HADOOP_HOME/bin/mapred --daemon start|stop historyserver</code></p> </li></ul> 
<h3><a id="MapReduceYARN_536"></a>提交MapReduce程序至YARN运行</h3> 
<ul><li> <p>YARN作为资源调度管控框架，其本身提供资源供许多程序运行，常见的有：MapReduce程序、Spark程序、Flink程序</p> </li><li> <p>Hadoop官方内置了一些预置的MapReduce程序代码，无需编程，只需要通过命令即可使用，例如：</p> 
  <ol><li>wordcount：单词计数程序。</li><li>pi：求圆周率 (通过蒙特卡罗算法（统计模拟法）求圆周率)</li></ol> </li><li> <p>这些内置的示例MapReduce程序代码，都在 <code>$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar</code> 这个文件内。</p> </li><li> <p>可以通过 hadoop jar 命令来运行它，提交MapReduce程序到YARN中。</p> <pre><code class="prism language-sh">hadoop jar 程序文件 java类名 <span class="token punctuation">[</span>程序参数<span class="token punctuation">]</span> <span class="token punctuation">..</span>. <span class="token punctuation">[</span>程序参数<span class="token punctuation">]</span>
<span class="token comment"># 参数1是数据输入路径</span>
<span class="token comment"># 参数2是结果输出路径</span>
hadoop <span class="token variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar wordcount /input/word.txt /output/wc
</code></pre> <p>可以通过 http://node01:8088 查看程序运行状态日志，历史信息</p> </li></ul> 
<p>oop官方内置了一些预置的MapReduce程序代码，无需编程，只需要通过命令即可使用，例如：</p> 
<ol><li>wordcount：单词计数程序。</li><li>pi：求圆周率 (通过蒙特卡罗算法（统计模拟法）求圆周率)</li></ol> 
<ul><li> <p>这些内置的示例MapReduce程序代码，都在 <code>$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar</code> 这个文件内。</p> </li><li> <p>可以通过 hadoop jar 命令来运行它，提交MapReduce程序到YARN中。</p> <pre><code class="prism language-sh">hadoop jar 程序文件 java类名 <span class="token punctuation">[</span>程序参数<span class="token punctuation">]</span> <span class="token punctuation">..</span>. <span class="token punctuation">[</span>程序参数<span class="token punctuation">]</span>
<span class="token comment"># 参数1是数据输入路径</span>
<span class="token comment"># 参数2是结果输出路径</span>
hadoop <span class="token variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar wordcount /input/word.txt /output/wc
</code></pre> <p>可以通过 http://node01:8088 查看程序运行状态日志，历史信息</p> <p><img src="https://images2.imgbox.com/02/e2/hyie0TN3_o.png" alt="在这里插入图片描述"></p> </li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e1f37fc6eb4ef53b96a1fd6e1aa7ce72/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">python—正则表达式</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1cc9d1e319edc5c44e95f640f0234b7e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">离散型随机变量为何不是左连续？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>