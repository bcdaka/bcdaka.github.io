<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>动手实践生成式人工智能GAI - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/5559cb7935c7b9d12530831005abf174/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="动手实践生成式人工智能GAI">
  <meta property="og:description" content="基于台湾大学李宏毅教授的Introduction to Generative AI 2024 Spring课程，总结
生成式人工智能GAI实践任务。参考资源包括课程的课件、视频和实践任务的代码。
Introduction to Generative AI 2024 Spring
也感谢B站Up主搬运的视频
李宏毅2024春《生成式人工智能导论》_哔哩哔哩_bilibili
本文介绍LLM微调、学习人类偏好和微调稳定扩散模型三个关键实践内容，每个实践内容包括主要目标、理论知识及参考文献、训练数据、测试数据、调整超参数的方法、模型评价方法。
1 LLM微调 目标 基于预训练模型训练出完成某一特定任务的模型。
理论基础 温度调节、Top-K截断和Nucleus采样（也称为Top-p截断）是大型语言模型（LLM）的常用生成策略技术，用于调整生成文本的多样性和连贯性。
温度调节（Temperature Scaling）：通过一个温度参数来平滑或锐化概率分布。温度较高时，概率分布更加平坦，生成的文本更加多样；温度较低时，概率分布更加尖锐，生成的文本更加确定。Top-K截断（Top-K Truncation）：在生成下一个token时，只考虑概率最高的K个tokens，其他的tokens被设置为0概率，不会被选中。Nucleus采样（Top-p Sampling）：选择累积概率超过某个阈值p的最小集合，然后从这个集合中采样下一个token。这样可以在保持多样性的同时，避免选择极低概率的token。 训练数据 instruction
input
output
数据条数：1040
测试数据 instruction
input
调整超参数 Temperature：设定生产回复的随机度，值越小生成的回复越稳定Top-k：增加生成回复的多样性和避免生成重复的词Top-p：抽样的概率阈值，用于控制生成回复的多样性max_length：生成回复的最大长度 评价方法 可以用大语言模型助教来评价格式和内容。需要准备包含评价标准和步骤的提示语。
2 学习人类偏好 目标 根据人类期望调整LLM的输出。
理论基础 RLHF Reinforcement Learning from Human Feedback参考https://openai.com/index/instruction-following/
Huggingface introduction to RLHF: https://huggingface.co/blog/rlhf https://huggingface.co/blog/trl-peft RLHF需要训练一个Reward Model评估 LLM 输出的答案，学习哪种反应更好（更类似于人类的偏好），然后用强化学习微调LLM。RLHF的弱点是需要训练额外的Reward Model，强化学习训练非常不稳定，很难调整超参数。
DPO Direct Preference Optimization 直接提供两种不同的回答，一种是首选回答，另一种是非首选回答
LLM 直接从回答中学习偏好，而不需要明确的奖励模型。
参考文档 Direct Preference Optimization: Your Language Model is Secretly a Reward Model">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-09T23:11:52+08:00">
    <meta property="article:modified_time" content="2024-08-09T23:11:52+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">动手实践生成式人工智能GAI</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="margin-left:0;"><span style="background-color:#FFFFFF;"><span style="color:#212529;">基于台湾大学李宏毅教授的</span></span><span style="background-color:#FFFFFF;"><span style="color:#212529;">Introduction to Generative AI 2024 Spring</span></span><span style="background-color:#FFFFFF;"><span style="color:#212529;">课程，总结</span></span></p> 
<p style="margin-left:0;"><span style="background-color:#FFFFFF;"><span style="color:#212529;">生成式人工智能</span></span><span style="background-color:#FFFFFF;"><span style="color:#212529;">GAI</span></span><span style="background-color:#FFFFFF;"><span style="color:#212529;">实践任务。</span></span>参考资源包括课程的课件、视频和实践任务的代码。</p> 
<p style="margin-left:0;"><a href="https://speech.ee.ntu.edu.tw/~hylee/genai/2024-spring.php" rel="nofollow" title="Introduction to Generative AI 2024 Spring">Introduction to Generative AI 2024 Spring</a></p> 
<p style="margin-left:0;">也感谢B站Up主搬运的视频</p> 
<p style="margin-left:0;"><a href="https://www.bilibili.com/video/BV1BJ4m1e7g8" rel="nofollow" title="李宏毅2024春《生成式人工智能导论》_哔哩哔哩_bilibili">李宏毅2024春《生成式人工智能导论》_哔哩哔哩_bilibili</a></p> 
<p style="margin-left:0;"><span style="background-color:#FFFFFF;"><span style="color:#212529;">本文介绍</span></span><span style="background-color:#FFFFFF;"><span style="color:#212529;">LLM</span></span><span style="background-color:#FFFFFF;"><span style="color:#212529;">微调、学习人类偏好和微调稳定扩散模型三个关键实践内容，</span></span><span style="background-color:#FFFFFF;"><span style="color:#212529;">每个实践内容包括主要目标、理论知识及参考文献、</span></span><span style="background-color:#FFFFFF;"><span style="color:#212529;">训练数据、测试数据、</span></span><span style="background-color:#FFFFFF;"><span style="color:#212529;">调整超参数的方法、模型评价方法。</span></span></p> 
<p style="margin-left:0;"></p> 
<h2 style="margin-left:0;"><span style="color:#0f4761;">1 LLM微调</span></h2> 
<h3 style="margin-left:0;"><span style="color:#0f4761;">目标</span></h3> 
<p style="margin-left:0;">基于预训练模型训练出完成某一特定任务的模型。</p> 
<h3 style="margin-left:0;"><span style="color:#0f4761;">理论基础</span></h3> 
<p style="margin-left:0cm;"><span style="color:#333333;">温度调节、</span><span style="color:#333333;">Top-K</span><span style="color:#333333;">截断和</span><span style="color:#333333;">Nucleus</span><span style="color:#333333;">采样（也称为</span><span style="color:#333333;">Top-p</span><span style="color:#333333;">截断）是大型语言模型（</span><span style="color:#333333;">LLM</span><span style="color:#333333;">）的常用生成策略技术，用于调整生成文本的多样性和连贯性。</span></p> 
<ol><li><span style="color:#333333;">温度调节（</span><span style="color:#333333;">Temperature Scaling</span><span style="color:#333333;">）</span>：通过一个温度参数来平滑或锐化概率分布。温度较高时，概率分布更加平坦，生成的文本更加多样；温度较低时，概率分布更加尖锐，生成的文本更加确定。</li><li><span style="color:#333333;">Top-K</span><span style="color:#333333;">截断（</span><span style="color:#333333;">Top-K Truncation</span><span style="color:#333333;">）</span>：在生成下一个<span style="color:#333333;">token</span><span style="color:#333333;">时，只考虑概率最高的</span><span style="color:#333333;">K</span><span style="color:#333333;">个</span><span style="color:#333333;">tokens</span><span style="color:#333333;">，其他的</span><span style="color:#333333;">tokens</span><span style="color:#333333;">被设置为</span><span style="color:#333333;">0</span><span style="color:#333333;">概率，不会被选中。</span></li><li><span style="color:#333333;">Nucleus</span><span style="color:#333333;">采样（</span><span style="color:#333333;">Top-p Sampling</span><span style="color:#333333;">）</span>：选择累积概率超过某个阈值<span style="color:#333333;">p</span><span style="color:#333333;">的最小集合，然后从这个集合中采样下一个</span><span style="color:#333333;">token</span><span style="color:#333333;">。这样可以在保持多样性的同时，避免选择极低概率的</span><span style="color:#333333;">token</span><span style="color:#333333;">。</span></li></ol> 
<h3 style="margin-left:0;"><span style="color:#0f4761;">训练数据</span></h3> 
<p style="margin-left:0;">instruction</p> 
<p style="margin-left:0;">input</p> 
<p style="margin-left:0;">output</p> 
<p style="margin-left:0;">数据条数：1040</p> 
<h3 style="margin-left:0;"><span style="color:#0f4761;">测试数据</span></h3> 
<p style="margin-left:0;">instruction</p> 
<p style="margin-left:0;">input</p> 
<h3 style="margin-left:0;"><span style="color:#0f4761;">调整超参数</span></h3> 
<ul><li> 
  <ul><li>Temperature：设定生产回复的随机度，值越小生成的回复越稳定</li><li>Top-k：增加生成回复的多样性和避免生成重复的词</li><li>Top-p：抽样的概率阈值，用于控制生成回复的多样性</li><li>max_length：生成回复的最大长度</li></ul></li></ul> 
<h3 style="margin-left:0;"><span style="color:#0f4761;">评价方法</span></h3> 
<p style="margin-left:0;">可以用大语言模型助教来评价格式和内容。需要准备包含评价标准和步骤的提示语。</p> 
<p style="margin-left:0;"></p> 
<h2 style="margin-left:0;"><span style="color:#0f4761;">2 学习人类偏好</span></h2> 
<h3 style="margin-left:0;"><span style="color:#0f4761;">目标</span></h3> 
<p style="margin-left:0;">根据人类期望调整LLM的输出。</p> 
<p style="margin-left:0;"></p> 
<h3 style="margin-left:0;"><span style="color:#0f4761;">理论基础</span></h3> 
<p style="margin-left:0;"></p> 
<p style="margin-left:0;">RLHF Reinforcement Learning from Human Feedback参考<a href="https://openai.com/index/instruction-following/" rel="nofollow" title="https://openai.com/index/instruction-following/">https://openai.com/index/instruction-following/</a></p> 
<p style="margin-left:0cm;"><span style="background-color:#FFFFFF;"><span style="color:#000000;">Huggingface introduction to RLHF: https://huggingface.co/blog/rlhf https://huggingface.co/blog/trl-peft </span></span></p> 
<p style="margin-left:0;"></p> 
<p style="margin-left:0;"></p> 
<p style="margin-left:0;">RLHF需要训练一个Reward Model评估 LLM 输出的答案，学习哪种反应更好（更类似于人类的偏好），然后用强化学习微调LLM。RLHF的弱点是需要训练额外的Reward Model，强化学习训练非常不稳定，很难调整超参数。</p> 
<p style="margin-left:0;"></p> 
<p style="margin-left:0;">DPO Direct Preference Optimization 直接提供两种不同的回答，一种是首选回答，另一种是非首选回答</p> 
<p style="margin-left:0;">LLM 直接从回答中学习偏好，而不需要明确的奖励模型。</p> 
<p style="margin-left:0;">参考文档 Direct Preference Optimization: Your Language Model is Secretly a Reward Model</p> 
<p style="margin-left:0;"><a href="https://arxiv.org/abs/2305.18290" rel="nofollow" title="https://arxiv.org/abs/2305.18290">https://arxiv.org/abs/2305.18290</a></p> 
<p style="margin-left:0;"></p> 
<p style="margin-left:0;"></p> 
<h3 style="margin-left:0;"><span style="color:#0f4761;">训练数据</span></h3> 
<p style="margin-left:0;">可以使用大语言模型生成，使用50条数据，每一条记录包括以下字段。</p> 
<p style="margin-left:0;">id编号：</p> 
<p style="margin-left:0;">prompt提示：输入问题</p> 
<p style="margin-left:0;">support支持：用支持的立场回答</p> 
<p style="margin-left:0;">oppose反对：用反对立场回答</p> 
<h3 style="margin-left:0;"><span style="color:#0f4761;">测试数据</span></h3> 
<p style="margin-left:0;">可以使用大语言模型生成，使用10条数据，每一条记录包括以下字段</p> 
<p style="margin-left:0;">id编号：</p> 
<p style="margin-left:0;">prompt提示：输入问题</p> 
<h3 style="margin-left:0;"></h3> 
<h3 style="margin-left:0;"><span style="color:#0f4761;">调整超参数</span></h3> 
<p style="margin-left:0;">support_ratio: 支持某个观点的数据比例</p> 
<p style="margin-left:0;">data_size: 决定 10~50 个训练数据的数量</p> 
<p style="margin-left:0;">num_epoch: 选择 1~3 以选择训练历元数</p> 
<p style="margin-left:0;"></p> 
<h3 style="margin-left:0;"><span style="color:#0f4761;">评价方法</span></h3> 
<p style="margin-left:0;">查看针对测试数据的测试结果，比较原始模型和训练模型的回答。</p> 
<p style="margin-left:0;"></p> 
<h2 style="margin-left:0;"><span style="color:#0f4761;">3 微调稳定扩散模型</span></h2> 
<h3 style="margin-left:0;"><span style="color:#0f4761;">目的</span></h3> 
<p style="margin-left:0;">通过对相同的面部图片进行微调，使稳定扩散Stable Diffusion模型生成一致的面部结果。</p> 
<p style="margin-left:0;"></p> 
<h3 style="margin-left:0;"><span style="color:#0f4761;">理论基础</span></h3> 
<p style="margin-left:0;">LoRA: Low-Rank Adaptation of Large Language Models</p> 
<p style="margin-left:0;"><a href="https://arxiv.org/abs/2106.09685" rel="nofollow" title="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a></p> 
<p style="margin-left:0;">LoRA，一种对已有训练好的模型进行微小改动的技术</p> 
<p style="margin-left:0;"></p> 
<h3 style="margin-left:0;"><span style="color:#0f4761;">调整超参数</span></h3> 
<p style="margin-left:0;">learning_rate（推荐）：模型的学习率，增加该值会使模型更关注训练数据而不是文本提示。(脸部距离↓（好）和 CLIP 分数↓（坏）</p> 
<p style="margin-left:0;">lora_rank：LoRA 模型的维度</p> 
<p style="margin-left:0;">lora_alpha（推荐）：LoRA 模型的权重</p> 
<p style="margin-left:0;">lora_rank ⬆️=》Face Distance ↓CLIP 分数↓</p> 
<p style="margin-left:0;">learning_rate⬆️=》Face Distance ↓CLIP 分数↓</p> 
<p style="margin-left:0;">max_train_steps:总训练步骤</p> 
<p style="margin-left:0;">validation_prompt_num：验证图像的数量</p> 
<p style="margin-left:0;">validation_step_ratio：验证步骤与 max_train_steps 之比</p> 
<h3 style="margin-left:0;"><span style="color:#0f4761;">评价方法</span></h3> 
<ul><li>Face Distance Score  <a href="https://github.com/HamadYA/GhostFaceNets" title="GitHub - HamadYA/GhostFaceNets: This repository contains the official implementation of GhostFaceNets, State-Of-The-Art lightweight face recognition models.">GitHub - HamadYA/GhostFaceNets: This repository contains the official implementation of GhostFaceNets, State-Of-The-Art lightweight face recognition models.</a> <a href="https://ieeexplore.ieee.org/document/10098610" rel="nofollow" title="GhostFaceNets: Lightweight Face Recognition Model From Cheap Operations | IEEE Journals &amp; Magazine | IEEE Xplore">GhostFaceNets: Lightweight Face Recognition Model From Cheap Operations | IEEE Journals &amp; Magazine | IEEE Xplore</a></li><li>CLIP Score  <a href="https://github.com/openai/CLIP" title="GitHub - openai/CLIP: CLIP (Contrastive Language-Image Pretraining),  Predict the most relevant text snippet given an image">GitHub - openai/CLIP: CLIP (Contrastive Language-Image Pretraining), Predict the most relevant text snippet given an image</a></li><li>The number of Faceless Images 无脸图像的数量</li></ul> 
<p style="margin-left:0;"></p> 
<p style="margin-left:0;"></p> 
<p style="margin-left:0;"></p> 
<h2 style="margin-left:0;"><span style="color:#0f4761;">4 GAI的安全问题</span></h2> 
<p style="margin-left:0;"></p> 
<p style="margin-left:0;">任务 1： LLM 是否会遵循有害的上下文示例？</p> 
<p style="margin-left:0;">任务 2：LLM 会回答带有刻板印象的问题吗？</p> 
<p style="margin-left:0;"></p> 
<p style="margin-left:0;">此外，李宏毅教授的课程还包括了了解人工智能在想什么和讲座视频快速摘要<span style="background-color:#FFFFFF;"><span style="color:#212529;">等实践任务，也值得动手探索。</span></span></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/45757765c45557b881d0f44b0f38f349/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">C语言之“ 分支和循环 ” （2）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0dc1ba7e76ccd49e7dae93661d29a319/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">二叉树的遍历问题—广度优先实现(从代码理解算法)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>