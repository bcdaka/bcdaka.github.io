<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>在本地部署Ollama服务接口附加OpenWebUI做测试 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/9ff2782c5ec8513f501f02644d0f5864/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="在本地部署Ollama服务接口附加OpenWebUI做测试">
  <meta property="og:description" content="使用Ollama在本地部署一个类似openai的API做开发和测试 1、准备一个旧电脑 因为配置要求不高，五年前的电脑都能使用，装一块旧显卡，显存大一点的最好有8G。实在没显卡也没关系，电脑内存大也能运行，无非运行的慢一些不影响开发测试。在电脑上安装centos stream 9 服务器带界面版，装上显卡驱动，没有显卡的只装系统就行了。配置好能上网（本篇不用科学上网）。
2、安装ollama 随便一个目录下，执行如下命令：
# curl -fsSL https://ollama.com/install.sh | sh
显示如下：表示安装成功
&gt;&gt;&gt; Downloading ollama...
######################################################################## 100.0%##O#-# &gt;&gt;&gt; Installing ollama to /usr/local/bin...
&gt;&gt;&gt; Creating ollama user...
&gt;&gt;&gt; Adding ollama user to render group...
&gt;&gt;&gt; Adding ollama user to video group...
&gt;&gt;&gt; Adding current user to ollama group...
&gt;&gt;&gt; Creating ollama systemd service...
&gt;&gt;&gt; Enabling and starting ollama service...
Created symlink /etc/systemd/system/default.target.wants/ollama.service → /etc/systemd/system/ollama.service.
&gt;&gt;&gt; NVIDIA GPU installed.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-26T14:39:59+08:00">
    <meta property="article:modified_time" content="2024-07-26T14:39:59+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">在本地部署Ollama服务接口附加OpenWebUI做测试</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2><strong>使用Ollama</strong>在本地部署一个类似openai的API做开发和测试</h2> 
<h3>1、准备一个旧电脑</h3> 
<p>因为配置要求不高，五年前的电脑都能使用，装一块旧显卡，显存大一点的最好有8G。实在没显卡也没关系，电脑内存大也能运行，无非运行的慢一些不影响开发测试。在电脑上安装centos stream 9 服务器带界面版，装上显卡驱动，没有显卡的只装系统就行了。配置好能上网（本篇不用科学上网）。</p> 
<h3>2、安装ollama</h3> 
<p>随便一个目录下，执行如下命令：</p> 
<p> # curl -fsSL https://ollama.com/install.sh | sh</p> 
<p>显示如下：表示安装成功<br> &gt;&gt;&gt; Downloading ollama...<br> ######################################################################## 100.0%##O#-#                                                                        <br> &gt;&gt;&gt; Installing ollama to /usr/local/bin...<br> &gt;&gt;&gt; Creating ollama user...<br> &gt;&gt;&gt; Adding ollama user to render group...<br> &gt;&gt;&gt; Adding ollama user to video group...<br> &gt;&gt;&gt; Adding current user to ollama group...<br> &gt;&gt;&gt; Creating ollama systemd service...<br> &gt;&gt;&gt; Enabling and starting ollama service...<br> Created symlink /etc/systemd/system/default.target.wants/ollama.service → /etc/systemd/system/ollama.service.<br> &gt;&gt;&gt; NVIDIA GPU installed.</p> 
<p>查看ollama的状态<br> # systemctl status ollama<br> 安装成功后执行ollama -v命令，查看版本信息，如果可以显示则代表已经安装好<br> # ollama -v<br> ollama version is 0.2.20</p> 
<p>ollama安装完成后只能本机访问，如需外网访问，还要配置一下，或者使用代理转发来解决跨域访问问题。</p> 
<p>cd到目录/etc/systemd/system下:vim ollama.service，在[Service]标签下添加如下两行并保存：<br> Environment="OLLAMA_HOST=:11434"<br> Environment="OLLAMA_ORIGINS=*"</p> 
<p>这两行是解决其它电脑访问和跨域问题。注意版本号低于0.2.20的不支持IPv6，端口前面要加IP：0.0.0.0</p> 
<p>重启服务：</p> 
<p># systemctl daemon-reload</p> 
<p># systemctl restart ollama.service</p> 
<p>如不想修改服务器文件可设置代理。</p> 
<p></p> 
<h3>3、加一个Qwen2模型测试</h3> 
<p>如果显卡大可以选个大模型。</p> 
<p># ollama pull qwen2:7b //拉取一个模型，7b是中型的需要6G显存，也可以拉1.5b或0.5b的2G显存够了。</p> 
<p><br> 测试运行：</p> 
<p>$ ollama run qwen2   </p> 
<p>启动完毕，到此其实我们已经有了一个控制台对话界面，已可以与Qwen2-7B对话了，/bye 退出。</p> 
<p>至此就部署完毕，下面测试。</p> 
<h3>4、测试接口：可以用POST工具</h3> 
<p>这里推荐apipost软件。</p> 
<p>ollama提供openai一样的API接口：<br> http://192.168.0.1:11434/v1/chat/completions<br> 调用方式也是一样的：POST，认证选Bearer可以填ollama或其它，其实并不验证，只是为了与接口格式相兼容。<br>  {"Content-Type": "application/json", Authorization: "Bearer ollama" }<br> {<!-- --><br>     "model": "Qwen2-7B",<br>     "messages": [<br>         {<!-- --><br>             "role": "system",<br>             "content": "你是一个智能机器人助手，请帮助回答问题。"<br>         },<br>         {<!-- --><br>             "role": "user",<br>             "content":"你好"<br>         }<br>     ],<br>     "temperature": 0.8,<br>     "top_p": 0.8,<br>     "max_tokens": 1024,<br>     "echo": false,<br>     "stream": false, //如果是true会有吐字效果；否则所有字都生成才返回。<br>     "repetition_penalty": 1.1</p> 
<p>}<br> 返回内容：跟据是否stream为true或false，格式有所不同，在开发时注意choices下面是message还是delta。</p> 
<h3>5、使用OpenWebUI测试</h3> 
<p>安装Open WebUI，可用三种方法：</p> 
<p>第一种是使用docker，我本不喜欢docker这个命令就不推荐这个方法了。这种方式网上一搜有一堆都是这种方法安装的。</p> 
<p>第二种是使用git：环境要求：Node.js &gt;= 20.10 和 Bun &gt;= 1.0.21  并且 Python &gt;= 3.11，不会装环境的参考后面第6条。</p> 
<p>使用如下命令安装：没git命令可以yum install git安装一个。<br> $ git clone https://github.com/open-webui/open-webui.git<br> $ cd open-webui/</p> 
<p># 复制一份 .env 目录和文件<br> $ cp -RPp .env.example .env</p> 
<p># 用node编绎前端<br> $ npm i<br> $ npm run build</p> 
<p># 安装后端所需包<br> $ cd ./backend<br> 下面这步要注意了：！！！如果不使用国内源，一天可能都在装包，不是吓唬，是真的包很多很慢。<br> 如果网速不好，一定换国内源，如下：<br> $ pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</p> 
<p>安装包：</p> 
<p>$ pip install -r requirements.txt -U</p> 
<p>启动：<br> $ bash start.sh</p> 
<p>运行不出错的话 http://localhost:8080/ ，但是不用科学上网的一定会出错😄<br> 第一次启动会上huggingface网下载一点东西all-MiniLM-L6-v2：所以先指定一个镜像网站，再运行（也可以先魔搭上下载好，再把backend/apps/rag/utils.py里的第318行改成这样：embedding_model_repo_path = r'\下载目录\all-MiniLM-L6-v2'来运行。）：</p> 
<p> $ HF_ENDPOINT=https://hf-mirror.com bash start.sh 这样就不会出错了。</p> 
<p>浏览器输入http://localhost:8080/去访问吧。</p> 
<p>第三种最简单：新版的openwebui已经做到python的包里面了，至少需要python3.11版本。</p> 
<p>使用pip install open-webui 安装，装之前最好设置国内源。</p> 
<p>安装完成使用： open-webui serve启动，同样第一次运行要加变量指定镜像网站：</p> 
<p>$ HF_ENDPOINT=https://hf-mirror.com open-webui serve</p> 
<p>以后直接运行$ open-webui serve就行了。</p> 
<p>启动后到http://localhost:8080/访问，先注册一个用户，进去后设置中文，设置外网链接：关掉openai接口，只留ollama接口，修改ollama接口IP或在本机上不用修改。选一下模型Qwen2-7B:latest就可以愉快的通过web界面进行聊天对话了。</p> 
<h3>6、附：安装环境</h3> 
<h4>安装miniconda3</h4> 
<p>使用conda管理python虚环境，python的版本很多很乱，conda是一个很好的虚拟环境管理工具，使用miniconda管理python版本已经够用了：</p> 
<p># cd /usr/local/src<br> # wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</p> 
<p>sh Miniconda3-latest-Linux-x86_64.sh -p /usr/local/miniconda3</p> 
<p>回车后一堆license的信息，这个按enter建后必须yes才能继续，按空格键一次一页快一些。</p> 
<p>验证安装</p> 
<p>You can undo this by running `conda init --reverse $SHELL`? [yes|no]<br> [no] &gt;&gt;&gt;</p> 
<p>You have chosen to not have conda modify your shell scripts at all.<br> To activate conda's base environment in your current shell session:<br> eval "$(/usr/local/miniconda3/bin/conda shell.YOUR_SHELL_NAME hook)"<br> To install conda's shell functions for easier access, first activate, then:<br> conda init</p> 
<p>跟据 提示：执行命令eval "$(/usr/local/miniconda3/bin/conda shell.bash hook)"创建环境<br> 然后执行conda init<br> 显示：<br> no change     /usr/local/miniconda3/condabin/conda<br> no change     /usr/local/miniconda3/bin/conda<br> no change     /usr/local/miniconda3/bin/conda-env<br> no change     /usr/local/miniconda3/bin/activate<br> no change     /usr/local/miniconda3/bin/deactivate<br> no change     /usr/local/miniconda3/etc/profile.d/conda.sh<br> no change     /usr/local/miniconda3/etc/fish/conf.d/conda.fish<br> no change     /usr/local/miniconda3/shell/condabin/Conda.psm1<br> no change     /usr/local/miniconda3/shell/condabin/conda-hook.ps1<br> no change     /usr/local/miniconda3/lib/python3.12/site-packages/xontrib/conda.xsh<br> no change     /usr/local/miniconda3/etc/profile.d/conda.csh<br> modified      /root/.bashrc</p> 
<p>==&gt; For changes to take effect, close and re-open your current shell. &lt;==<br> 可以查看/root/.bashrc文件备更改增加了环境参数如下：<br> # &gt;&gt;&gt; conda initialize &gt;&gt;&gt;<br> # !! Contents within this block are managed by 'conda init' !!<br> __conda_setup="$('/usr/local/miniconda3/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)"<br> if [ $? -eq 0 ]; then<br>     eval "$__conda_setup"<br> else<br>     if [ -f "/usr/local/miniconda3/etc/profile.d/conda.sh" ]; then<br>         . "/usr/local/miniconda3/etc/profile.d/conda.sh"<br>     else<br>         export PATH="/usr/local/miniconda3/bin:$PATH"<br>     fi<br> fi<br> unset __conda_setup<br> # &lt;&lt;&lt; conda initialize &lt;&lt;&lt;<br> 个人不太喜欢进入系统的时候自动激活conda的base环境，因为会占用时间，切换节点会延迟，所以关掉自动激活base环境：<br> conda config --set auto_activate_base false<br> 重启：# reboot<br><br> #设置conda镜像源<br> # conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/<br> # conda config --set show_channel_urls yes</p> 
<p>#最好安装完重启一下系统</p> 
<h4>创建python虚环境：</h4> 
<p># conda create -n python311 python=3.11（如果精确到子版本可用两个等号如：==3.10.6，否则是大版本的最新子版本。）</p> 
<p># conda env list 显示创建的环境。</p> 
<p>进入和退出python虚环境，进入后命令提示符前面显示虚环境名称：</p> 
<p># conda activate python311</p> 
<p>如果退出使用：# conda deactivate </p> 
<p>#在py311环境下设置pip镜像源<br> # pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</p> 
<h4>安装node ：</h4> 
<p>linux自带的node.js版本有些老了，不够用的，网上查了一下node也象python一样版本很多情况复杂，也有象conda 这样的版本管理的软件nvm来管理版本。<br> 先装nvm<br> $ curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash</p> 
<p>安装node.js 20的最新子版本</p> 
<p>$ nvm install 20</p> 
<p>node -v # 显示版本号：`v20.15.1`</p> 
<p>npm -v # 显示版本号： `10.7.0`，npm是node带的包管理器命令，就象python里的pip一样。</p> 
<h4>安装Bun：</h4> 
<p>Open WebUI用到了bun命令，Bun是和node差不多的框架。<br> # 下载安装脚本并开始安装<br> curl -fsSL https://bun.sh/install | bash</p> 
<p># 刷新环境变量<br> source /home/username/.bashrc</p> 
<p># 查看是否安装成功<br> bun --help<br> bun -v 显示1.1.1</p> 
<h3>7、ollama命令说明，更新和清除</h3> 
<p>ollama的操作命令跟docker操作命令非常相似：<br> ollama serve    # 启动ollama<br> ollama create    # 从模型文件创建模型<br> ollama show        # 显示模型信息<br> ollama run        # 运行模型<br> ollama pull        # 从注册仓库中拉取模型<br> ollama push        # 将模型推送到注册仓库<br> ollama list        # 列出已下载模型<br> ollama cp        # 复制模型<br> ollama rm        # 删除模型<br> ollama help        # 获取有关任何命令的帮助信息</p> 
<p>更新和安装一样再执行一遍命令：</p> 
<p># curl -fsSL https://ollama.com/install.sh | sh</p> 
<p>卸载Ollama：<br> 停止并禁用服务<br> systemctl stop ollama<br> systemctl disable ollama</p> 
<p>删除服务文件和Ollama二进制文件<br> rm /etc/systemd/system/ollama.service <br> rm $(which ollama)</p> 
<p>清理Ollama用户和组<br> rm -r /usr/share/ollama<br> userdel ollama<br> groupdel ollama<br>  </p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b1cf8cfd18cb37292ca0e1ed7a29f184/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【算法/训练】：前缀和&amp;&amp;差分</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a95f1188829a0b406cc7dc52da7fb640/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">2024河北省研究生数学建模竞赛C题室外三维点云数据分割识别思路代码分析</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>