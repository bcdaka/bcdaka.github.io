<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>即插即用的涨点模块之注意力机制（SKAttention）详解及代码，可应用于检测、分割、分类等各种算法领域 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/5a082cf092fd38b334c81516716d66a6/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="即插即用的涨点模块之注意力机制（SKAttention）详解及代码，可应用于检测、分割、分类等各种算法领域">
  <meta property="og:description" content="目录
前言
一、SKNet结构
二、SKNet计算流程
三、SKNet参数
四、代码详解
前言 Selective Kernel Networks（SKNet）
来源：CVPR2019
官方代码：https://github.com/implus/SKNet
什么是感受野？感受野（receptive field）是指在网络的前向传播过程中，每个神经元对输入数据的区域大小。换句话说，它表示了神经元在输入空间中接收信息的范围。在图像处理任务中，神经元的感受野大小通常与输入图像的像素大小有关。较小的感受野可以捕获局部细节，而较大的感受野则可以捕获更大范围的整体结构和语境信息。因此，设计合适大小的感受野对于不同的任务和网络架构至关重要。SKAttention能够根据输入动态选择不同大小的卷积核。这种设计使得网络可以根据输入自适应地调整其感受野，从而更有效地捕获不同尺度的特征。这在处理诸如图像分类和对象检测等任务中特别有用，这些任务中输入特征的尺度和大小可能有很大的变化。
一、SKNet结构 SKNet结构如图一所示。SK卷积由Split，Select和Split三个操作来实现。Split操作：使用多个不同大小的卷积核对输入特征进行卷积操作（每次卷积操作即一组CBR），得到多个尺度的特征表示，再将这些特征表示拼接起来；Fuse操作：由两个全连接层、一个全局平均池化及Relu激活函数组成，首先对多个分支元素求和，即相同形状的张量中的对应元素进行相加。然后进行全局平均池化，压缩为具有相同通道数的特征向量，捕捉全局信息。接着先降维再升维，得到 K 个尺度对应的通道描述符，并将升维后的特征向量重塑为与输入的大小相同，重塑后的特征向量按照第 0 维度（K 维度）进行堆叠，形成一个新的张量，通过 softmax 函数将每个尺度对应的权重进行归一化处理，使得它们的总和为 1。Selecte操作：将每个尺度的权重与对应的之前卷积之后的结果加权求和，得到不同的分支权重组合，影响融合后的层级V的有效感受野大小。
精读：Split操作：目的：为了捕获多尺度的特征信息，Split操作首先将输入特征通过不同大小的卷积核处理。常见的配置可能包括使用3x3、5x5等不同尺寸的卷积核。
实现：每个卷积后接批归一化（Batch Normalization）和ReLU激活函数，形成一组卷积-批归一化-激活（CBR）单元。这些不同尺度的特征图接着被拼接在一起，形成一个更丰富的特征表示。
Fuse 操作：目的：为了综合多尺度的信息并生成每个尺度的重要性权重，Fuse操作处理拼接后的特征，通过全局信息来指导选择操作。
实现：求和：首先将所有分支的特征图进行逐元素相加。全局平均池化：接着对求和后的结果进行全局平均池化，从而压缩特征至一个全局描述符。降维与升维：通过两个全连接层（通常先降维后升维），处理池化后的特征，生成每个尺度的通道描述符。重塑与归一化：将通道描述符重塑成原始输入的尺寸，并通过softmax进行归一化，生成每个尺度的权重。
Select 操作：目的：根据Fuse操作生成的尺度权重，动态选择并融合不同尺度的特征。
实现：将每个尺度的权重应用于对应的卷积输出（从Split操作得到），通过加权求和的方式，结合这些特征。这样，网络可以侧重于当前最有效的特征尺度，从而优化处理结果。
二、SKNet计算流程 对于任意给定的特征映射，默认情况下我们首先进行两个变换：X→∈和：X→∈，核大小分别为3×3和5×5，将其分为和，并将和按元素求和，得到U:
然后通过全局平均池化将（B,C,H,W）压缩到（B,C），单个特征向量:
然后经过全连接层进行降维和升维:
接下来通过Softmax得到各个特征尺度的权重，并在Select中将其与卷积后的结果加权求和。
三、SKNet参数 利用thop库的profile函数计算FLOPs和Param。Input:(512,7,7)。
Module
FLOPs
Param
SKAttention
1079555584
22192192
四、代码详解 import torch from torch import nn from collections import OrderedDict class SKAttention(nn.Module): #通道数channel, 卷积核尺度kernels, 降维系数reduction, 分组数group, 降维后的通道数L def __init__(self, channel=512, kernels=[1, 3, 5, 7], reduction=16, group=1, L=32): super().">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-20T16:10:07+08:00">
    <meta property="article:modified_time" content="2024-04-20T16:10:07+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">即插即用的涨点模块之注意力机制（SKAttention）详解及代码，可应用于检测、分割、分类等各种算法领域</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E5%89%8D%E8%A8%80-toc" style="margin-left:0px;"><a href="#%E5%89%8D%E8%A8%80" rel="nofollow">前言</a></p> 
<p id="%E4%B8%80%E3%80%81SKNet%E7%BB%93%E6%9E%84-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81SKNet%E7%BB%93%E6%9E%84" rel="nofollow">一、SKNet结构</a></p> 
<p id="%E4%BA%8C%E3%80%81SKNet%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81SKNet%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B" rel="nofollow">二、SKNet计算流程</a></p> 
<p id="%E4%B8%89%E3%80%81SKNet%E5%8F%82%E6%95%B0-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81SKNet%E5%8F%82%E6%95%B0" rel="nofollow">三、SKNet参数</a></p> 
<p id="%E5%9B%9B%E3%80%81%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3" rel="nofollow">四、代码详解</a></p> 
<hr> 
<h2 id="%E5%89%8D%E8%A8%80"><a id="_7"></a>前言</h2> 
<p style="margin-left:0;text-align:center;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><strong>Selective Kernel Networks</strong><strong>（</strong><strong>SK</strong><strong>Net</strong><strong>）</strong></span></span></p> 
<p style="margin-left:0;text-align:left;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><strong>来源：</strong><strong>CVPR</strong><strong>2019</strong></span></span></p> 
<p style="margin-left:0;text-align:left;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><strong>官方代码：</strong><strong>https://github.com/implus/SKNet</strong></span></span></p> 
<p style="margin-left:0;text-align:left;"><span style="background-color:#FFFFFF;"><span style="color:#000000;">        什么是感受野？感受野（receptive field）是指在网络的前向传播过程中，每个神经元对输入数据的区域大小。换句话说，它表示了神经元在输入空间中接收信息的范围。在图像处理任务中，神经元的感受野大小通常与输入图像的像素大小有关。较小的感受野可以捕获局部细节，而较大的感受野则可以</span></span><span style="background-color:#FFFFFF;"><span style="color:#000000;">捕获更大范围的整体结构和语境信息。因此，设计合适大小的感受野对于不同的任务和网络架构至关重要。SKAttention能够根据输入动态选择不同大小的卷积核。这种设计使得网络可以根据输入自适应地调整其感受野，从而更有效地捕获不同尺度的特征。这在处理诸如图像分类和对象检测等任务中特别有用，这些任务中输入特征的尺度和大小可能有很大的变化。</span></span></p> 
<hr> 
<h2 id="%E4%B8%80%E3%80%81SKNet%E7%BB%93%E6%9E%84"><a id="pandas_16"></a>一、SKNet结构</h2> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">        SKNet结构如图一所示。SK卷积由Split，Select和Split三个操作来实现。<strong>Split操作：</strong>使用多个不同大小的卷积核对输入特征进行卷积操作（每次卷积操作即一组CBR），得到多个尺度的特征表示，再将这些特征表示拼接起来；<strong>Fuse操作：</strong>由两个全连接层、一个全局平均池化及Relu激活函数组成，首先对多个分支元素求和，即相同形状的张量中的对应元素进行相加。然后进行全局平均池化，压缩为具有相同通道数的特征向量，捕捉全局信息。接着先降维再升维，得到 K 个尺度对应的通道描述符，并将升维后的特征向量重塑为与输入的大小相同，重塑后的特征向量按照第 0 维度（K 维度）进行堆叠，形成一个新的张量，通过 softmax 函数将每个尺度对应的权重进行归一化处理，使得它们的总和为 1。<strong>Selecte操作：</strong>将每个尺度的权重与对应的之前卷积之后的结果加权求和，得到不同的分支权重组合，影响融合后的层级V的有效感受野大小。</span></span></span></p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="370" src="https://images2.imgbox.com/4a/91/p3EXk5eR_o.png" width="1067"></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#4472c4;">精读：</span><strong><span style="color:#FF0000;">Split</span></strong><strong><span style="color:#FF0000;">操作：</span></strong><strong><span style="color:#000000;">目的：</span></strong><span style="color:#000000;">为了捕获多尺度的特征信息，Split操作首先将输入特征通过不同大小的卷积核处理。常见的配置可能包括使用3x3、5x5等不同尺寸的卷积核。</span></span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><strong><span style="color:#000000;">实现：</span></strong><span style="color:#000000;">每个卷积后接批归一化（Batch Normalization）和ReLU激活函数，形成一组卷积-批归一化-激活（CBR）单元。这些不同尺度的特征图接着被拼接在一起，形成一个更丰富的特征表示。</span></span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><strong><span style="color:#FF0000;">Fuse </span></strong><strong><span style="color:#FF0000;">操作：</span></strong><strong><span style="color:#000000;">目的：</span></strong><span style="color:#000000;">为了综合多尺度的信息并生成每个尺度的重要性权重，Fuse操作处理拼接后的特征，通过全局信息来指导选择操作。</span></span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><strong><span style="color:#000000;">实现：</span></strong><span style="color:#000000;">求和：首先将所有分支的特征图进行逐元素相加。全局平均池化：接着对求和后的结果进行全局平均池化，从而压缩特征至一个全局描述符。降维与升维：通过两个全连接层（通常先降维后升维），处理池化后的特征，生成每个尺度的通道描述符。重塑与归一化：将通道描述符重塑成原始输入的尺寸，并通过softmax进行归一化，生成每个尺度的权重。</span></span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><strong><span style="color:#FF0000;">Select </span></strong><strong><span style="color:#FF0000;">操作：</span></strong><strong><span style="color:#000000;">目的</span></strong><span style="color:#000000;">：根据Fuse操作生成的尺度权重，动态选择并融合不同尺度的特征。</span></span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><strong><span style="color:#000000;">实现：</span></strong><span style="color:#000000;">将每个尺度的权重应用于对应的卷积输出（从Split操作得到），通过加权求和的方式，结合这些特征。这样，网络可以侧重于当前最有效的特征尺度，从而优化处理结果。</span></span></span></p> 
<h2 id="%E4%BA%8C%E3%80%81SKNet%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B"><a id="_19"></a>二、SKNet计算流程</h2> 
<h3><a id="1_20"></a></h3> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">对于任意给定的特征映射</span></span></span><img alt="" height="29" src="https://images2.imgbox.com/b9/af/fWbqdwQj_o.png" width="83"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">，默认情况下我们首先进行两个变换</span></span></span><img alt="" height="29" src="https://images2.imgbox.com/21/e8/NdAWRez2_o.png" width="16"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">：X→</span></span></span><img alt="" height="30" src="https://images2.imgbox.com/d6/d6/Ezc9qS4Y_o.png" width="17"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">∈</span></span></span><img alt="" height="29" src="https://images2.imgbox.com/1a/90/lKDZag49_o.png" width="83"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">和</span></span></span><img alt="" height="30" src="https://images2.imgbox.com/36/9a/20E4gDdK_o.png" width="16"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">：X→</span></span></span><img alt="" height="30" src="https://images2.imgbox.com/c6/56/EpAtVX2E_o.png" width="17"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">∈</span></span></span><img alt="" height="29" src="https://images2.imgbox.com/a0/b1/Zriu1FUt_o.png" width="83"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">，核大小分别为3×3和5×5，将其分为</span></span></span><img alt="" height="30" src="https://images2.imgbox.com/e1/d8/annAXlez_o.png" width="17"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">和</span></span></span><img alt="" height="30" src="https://images2.imgbox.com/6a/5f/IgCmyjEu_o.png" width="17"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">，并将</span></span></span><img alt="" height="30" src="https://images2.imgbox.com/fa/34/VgrmRrhU_o.png" width="17"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">和</span></span></span><img alt="" height="30" src="https://images2.imgbox.com/da/b5/0zGC6TO6_o.png" width="17"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">按元素求和，得到U:</span></span></span></p> 
<p class="img-center"><img alt="" height="30" src="https://images2.imgbox.com/9e/9f/8y1IahNf_o.png" width="111"></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">然后通过全局平均池化</span></span></span><img alt="" height="31" src="https://images2.imgbox.com/5d/d7/ggAueXOv_o.png" width="32"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">将（B,C,H,W）压缩到（B,C），单个特征向量</span></span></span><img alt="" height="28" src="https://images2.imgbox.com/3f/59/25T4H4A8_o.png" width="20"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">:</span></span></span></p> 
<p class="img-center"><img alt="" height="84" src="https://images2.imgbox.com/31/b7/Zai6ykvm_o.png" width="378"></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">然后经过全连接层</span></span></span><img alt="" height="31" src="https://images2.imgbox.com/9d/c4/iAiFWTKR_o.png" width="31"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">进行降维和升维:</span></span></span></p> 
<p class="img-center"><img alt="" height="31" src="https://images2.imgbox.com/20/ec/glpJVnrr_o.png" width="241"></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">接下来通过Softmax得到各个特征尺度的权重，并在Select中将其与卷积后的结果加权求和。</span></span></span></p> 
<h2 id="%E4%B8%89%E3%80%81SKNet%E5%8F%82%E6%95%B0">三、SKNet参数</h2> 
<p><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">利用thop库的profile函数计算FLOPs和Param。Input</span><span style="color:#000000;">:(512,7,7)</span><span style="color:#000000;">。</span></span></span></p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:142pt;"> <p style="margin-left:0;text-align:left;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">Module</span></span></span></p> </td><td style="border-color:#000000;vertical-align:top;width:142.05pt;"> <p style="margin-left:0;text-align:left;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">FLOPs</span></span></span></p> </td><td style="border-color:#000000;vertical-align:top;width:142.05pt;"> <p style="margin-left:0;text-align:left;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">Param</span></span></span></p> </td></tr><tr><td style="border-color:#000000;vertical-align:top;width:142pt;"> <p style="margin-left:0;text-align:left;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">SKAttention</span></span></span></p> </td><td style="vertical-align:top;width:142.05pt;"> <p style="margin-left:0;text-align:left;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">1079555584</span></span></span></p> </td><td style="vertical-align:top;width:142.05pt;"> <p style="margin-left:0;text-align:left;"><span style="background-color:#FFFFFF;"><span style="color:#0d0d0d;"><span style="color:#000000;">22192192</span></span></span></p> </td></tr></tbody></table> 
<h2 id="%E5%9B%9B%E3%80%81%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3" style="margin-left:0px;text-align:justify;">四、代码详解</h2> 
<pre><code class="language-python">import torch
from torch import nn
from collections import OrderedDict


class SKAttention(nn.Module):
    #通道数channel, 卷积核尺度kernels, 降维系数reduction, 分组数group, 降维后的通道数L
    def __init__(self, channel=512, kernels=[1, 3, 5, 7], reduction=16, group=1, L=32):
        super().__init__()
        self.d = max(L, channel // reduction)
        self.convs = nn.ModuleList([])
        #有几个kernels,就有几个尺度, 每个尺度对应的卷积层由Conv-bn-relu实现
        for k in kernels:
            self.convs.append(
                nn.Sequential(OrderedDict([
                    ('conv', nn.Conv2d(channel, channel, kernel_size=k, padding=k // 2, groups=group)),
                    ('bn', nn.BatchNorm2d(channel)),
                    ('relu', nn.ReLU())
                ]))
            )
        self.fc = nn.Linear(channel, self.d)
        self.fcs = nn.ModuleList([])
        # 将降维后的通道数L通过K个全连接层得到K个尺度对应的通道描述符表示, 然后基于K个通道描述符计算注意力权重
        for i in range(len(kernels)):
            self.fcs.append(nn.Linear(self.d, channel))
        self.softmax = nn.Softmax(dim=0)

    def forward(self, x):
        B, C, H, W = x.size()
        # 存放多尺度的输出
        conv_outs=[]
        ## Split: 将输入特征x通过K个卷积层得到K个尺度的特征
        for conv in self.convs:
            scale = conv(x)
            conv_outs.append(scale)
        feats=torch.stack(conv_outs,0) # torch.stack()函数用于在新创建的维度上对输入的张量序列进行拼接, (B,C,H,W)--&gt;(K,B,C,H,W), K为尺度数

        ## Fuse: 首先将多尺度的信息进行相加,sum()默认在第一个维度进行求和
        U=sum(conv_outs) # (K,B,C,H,W)--&gt;sum--&gt;(B,C,H,W)
        # 全局平均池化操作: (B,C,H,W)--&gt;mean--&gt;(B,C,H)--&gt;mean--&gt;(B,C)  【mean操作等价于全局平均池化的操作】
        S=U.mean(-1).mean(-1)
        # 降低通道数,提高计算效率: (B,C)--&gt;(B,d)
        Z=self.fc(S)

        # 将紧凑特征Z通过K个全连接层得到K个尺度对应的通道描述符表示, 然后基于K个通道描述符计算注意力权重
        weights=[]
        for fc in self.fcs:
            weight=fc(Z) #恢复预输入相同的通道数: (B,d)--&gt;(B,C)
            weights.append(weight.view(B,C,1,1)) # (B,C)--&gt;(B,C,1,1)
        scale_weight=torch.stack(weights,0) #将K个通道描述符在0个维度上拼接: (K,B,C,1,1)
        scale_weight=self.softmax(scale_weight) #在第0个维度上执行softmax,获得每个尺度的权重: (K,B,C,1,1)

        ##  Select
        V=(scale_weight*feats).sum(0) # 将每个尺度的权重与对应的特征进行加权求和,第一步是加权，第二步是求和：(K,B,C,1,1) * (K,B,C,H,W) = (K,B,C,H,W)--&gt;sum--&gt;(B,C,H,W)
        return V

if __name__ == '__main__':
    from  torchsummary import summary
    from thop import profile
    model = SKAttention(channel=512, reduction=8)
    # summary(model, (512, 7, 7), device='cpu', batch_size=1)
    flops, params = profile(model, inputs=(torch.randn(1, 512, 7, 7),))
    print(f"FLOPs: {flops}, Params: {params}")
</code></pre> 
<hr> 
<h2><a id="_45"></a></h2>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f3e2a20c2b9179add987582be2cc82e4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">在本地跑一个AI模型(5) - Stable Diffusion</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/84ca1991bf48322eb2ea47d87e5c0dc5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Docker容器搭建Hadoop集群(hadoop-3.1.3)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>