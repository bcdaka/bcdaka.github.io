<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>VGMShield：揭秘视频生成模型滥用的检测与追踪技术 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/e80999cef80de29592bc90fb8c1894d7/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="VGMShield：揭秘视频生成模型滥用的检测与追踪技术">
  <meta property="og:description" content="视频生成模型，如 Stable Video Diffusion 和 Videocrafter，已经能够生成合理且高分辨率的视频。但这些技术进步也带来了被恶意利用的风险，比如用于制造假新闻或进行政治宣传。因此，来自弗吉尼亚大学和亥姆霍兹信息安全中心（CISPA）的研究团队着手开发了 VGMShield，一套旨在降低视频生成模型滥用风险的综合解决方案：
现实世界中的参与者分为三个不同的实体：创建者（Creator）、修改者（Modifier）和消费者（Consumer）。创建者可能是摄影师或记者，他们上传信息供消费者使用。修改者可能会恶意使用这些图像生成误导消费者的视频。为了解决视频生成模型带来的安全隐患，研究者提出了一个包含假视频检测、假视频来源追踪和滥用预防的综合防御框架。
消费者 (Consumer)、创建者 (Creator) 和修改者 (Modifier) 在信息/内容生成和消费周期中的角色 假视频检测和追踪 潜在的威胁模型基于两个关键因素：视频来源和生成视频所使用的模型。检测任务分为四种场景：在目标检测中，检测器不仅知道可能生成视频的模型，还知道用于生成视频的数据源。这种场景虽然理想化，但为研究者提供了一个起点，以理解在最佳条件下模型的表现。第二，数据盲检测，其中检测器知道视频的来源，但对生成视频使用的数据分布一无所知。模型盲检测则相反，检测器知道数据分布，但不知道使用的是哪种模型。最后，开放检测是一个更为现实和具有挑战性的场景，检测器对数据源和模型都一无所知。
假视频检测和追踪的不同场景，包括数据和模型的知识情况 研究者们在构建假视频检测和追踪方法时，将这个问题转化为了分类任务。他们基于一个关键假设：假视频会在空间维度上展示出异常特征，同时在时间维度上表现出不一致性。为了识别这些异常，研究者们选择了几种预训练的视频识别模型，这些模型能够理解视频的时空动态。他们选用的模型包括 Inflated 3D ConvNet (I3D)、X-CLIP 和 Video Masked Autoencoders (MAE)，每种模型都以其独特的方式处理视频数据。
在检测部分，研究者们根据不同的应用场景调整了模型的训练方法。他们遵循两个基本原则：首先，为了应对开放检测设置，他们确保训练集尽可能多样化，包含了大量的真实和假视频；其次，当任务更加具体时，例如检测器需要识别特定模型或数据分布生成的视频，他们会缩小训练集的范围，使其更贴近实际任务的需求。
在追踪源模型的过程中，研究者们采取了一种假设所有视频均为假视频的方法，并使用多标签分类模型来确定视频可能来源于的九种不同模型之一。与检测任务不同，追踪任务的训练涉及到所有这些模型生成的假视频，这样做可以帮助模型学习到每种模型生成视频的独特特征。
为了验证所提出方法的有效性，研究者采用了两个广泛使用的数据集：WebVid-10M和InternVid，来进行评估。这些数据集拥有大量的视频和相应的文本描述，为训练和评估机器学习模型提供了丰富的资源。研究者进一步收集了所有可用的开源视频生成模型，并针对每一种模型生成了1000个视频，这些视频随后被用于检测和追踪任务。
假视频检测的实验流程 图2展示了假视频检测的实验流程。为了确保真实视频和假视频具有相似的分布特征，研究者们采用了一种特定的方法来生成假视频。这种方法利用了真实视频的第一帧（以及可能伴随的字幕）来生成对应的假视频。这样的处理旨在最小化真实视频和假视频之间的差异，从而使得检测模型不能依赖于其他特征（例如，真实视频总是关于动物的，而假视频总是关于汽车的）。
分类模型由两部分组成：一个作为主干的视频识别模型，以及一个全连接的模型。这个全连接的模型被训练来识别和区分真实视频和假视频。视频识别模型负责提取视频中的关键特征，而全连接层则用于学习如何根据这些特征来区分视频的真实性。通过在真实视频和假视频上训练这个复合模型，研究者们旨在提高检测系统的性能和准确性。这种训练方法有助于模型更好地理解真假视频之间的微妙差异，从而在实际应用中更有效地识别出假视频。
实验的详细参数，包括输入帧数、训练周期、学习率等 研究者展示了在不同数据集和多种生成任务中应用三种检测模型的结果。他们发现，Video Masked Autoencoders（MAE）模型在各种场景下都展现出了稳定性和高效性，尤其是在开放检测设置中，MAE模型的检测准确率显著高于其他模型。
使用不同的检测模型（X-CLIP, MAE, I3D）在 WebVid-10M 数据集上构造视频时的检测结果 MAE 基础的假视频检测模型在 InternVid 数据集上四种设置下的性能 研究者展示了在数据知情和数据不知情两种设置下，使用X-CLIP、I3D和MAE作为后端的追踪模型的性能。特别地，在数据不知情的设置中，即使面对未知数据源的视频，MAE模型仍然能够保持高达90%的准确率，显示出其在追踪任务上的鲁棒性。
在数据感知和数据不可知设置下，使用 Webvid 和 InternVid 数据集的源追踪结果 研究者们采用了Grad-CAM技术来深入分析模型的决策过程。Grad-CAM（梯度类激活映射）是一种可视化技术，它能够展示模型在做出分类决策时所关注的视频区域。通过这种方式，研究者们可以直观地理解检测模型是如何识别出假视频的。
使用 Grad-CAM 展示了 I3D 和 MAE 检测模型在 InternVid-SVD 生成任务中做出检测决定的具体方面 研究者们首先观察了基于I3D和MAE的检测模型，这两种模型在假视频检测上都取得了超过90%的准确率。通过Grad-CAM生成的热图，研究者们发现这些模型主要依赖于视频中的对象来进行判断。例如，I3D模型倾向于关注人脸、栅栏、手指和引擎电池等对象，而MAE模型则集中于识别如下水道盖、人体和手指等异常对象。
源追踪模型依赖于视频中不同位置的特征来确定生成模型 研究者们还注意到，MAE模型在检测时空异常方面表现得更为灵活和敏感。与I3D模型相比，MAE模型不仅能够识别出对象在空间维度上的扭曲，还能够察觉到对象在时间维度上的不连贯性。例如，如果视频中的排水管在连续帧之间发生了明显的形状变化，MAE模型能够捕捉到这种变化，并据此判断视频为假。
通过Grad-CAM的分析，研究者们得出结论，MAE模型之所以在假视频检测上表现更为出色，是因为其能够同时关注视频中的时空异常，而不仅仅是空间上的失真。这一发现对于进一步优化和改进假视频检测技术具有重要意义。
滥用预防 与文本到视频的生成任务相比，图像到视频的生成任务更容易被滥用，因为存在修改者可能恶意使用这些技术。为了防止这种情况，研究者们设计了一种基于对抗性样本的防御机制，专门针对图像到视频的生成任务。
对抗性样本最初是为了解决分类问题而引入的，通过向原始图像中添加对人类视觉不可见但能导致神经网络做出错误判断的小扰动。研究者们采用类似的方法，创建对抗性样本来干扰视频生成模型，使模型的编码器误解输入图像，从而产生不正确和异常的视频帧，保护原始图像不被滥用。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-15T16:08:00+08:00">
    <meta property="article:modified_time" content="2024-07-15T16:08:00+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">VGMShield：揭秘视频生成模型滥用的检测与追踪技术</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>视频生成模型，如 Stable Video Diffusion 和 Videocrafter，已经能够生成合理且高分辨率的视频。但这些技术进步也带来了被恶意利用的风险，比如用于制造假新闻或进行政治宣传。因此，来自弗吉尼亚大学和亥姆霍兹信息安全中心（CISPA）的研究团队着手开发了 VGMShield，一套旨在降低视频生成模型滥用风险的综合解决方案：</p> 
<p>现实世界中的参与者分为三个不同的实体：创建者（Creator）、修改者（Modifier）和消费者（Consumer）。创建者可能是摄影师或记者，他们上传信息供消费者使用。修改者可能会恶意使用这些图像生成误导消费者的视频。为了解决视频生成模型带来的安全隐患，研究者提出了一个包含假视频检测、假视频来源追踪和滥用预防的综合防御框架。</p> 
<figure class="image"> 
 <img alt="" height="537" src="https://images2.imgbox.com/e0/01/L0Qq0kXA_o.png" width="729"> 
 <figcaption>
   消费者 (Consumer)、创建者 (Creator) 和修改者 (Modifier) 在信息/内容生成和消费周期中的角色 
 </figcaption> 
</figure> 
<h4>假视频检测和追踪</h4> 
<p>潜在的威胁模型基于两个关键因素：视频来源和生成视频所使用的模型。检测任务分为四种场景：在目标检测中，检测器不仅知道可能生成视频的模型，还知道用于生成视频的数据源。这种场景虽然理想化，但为研究者提供了一个起点，以理解在最佳条件下模型的表现。第二，数据盲检测，其中检测器知道视频的来源，但对生成视频使用的数据分布一无所知。模型盲检测则相反，检测器知道数据分布，但不知道使用的是哪种模型。最后，开放检测是一个更为现实和具有挑战性的场景，检测器对数据源和模型都一无所知。</p> 
<figure class="image"> 
 <img alt="" height="466" src="https://images2.imgbox.com/e5/5a/FBHSORfi_o.png" width="767"> 
 <figcaption>
   假视频检测和追踪的不同场景，包括数据和模型的知识情况 
 </figcaption> 
</figure> 
<h5></h5> 
<p>研究者们在构建假视频检测和追踪方法时，将这个问题转化为了分类任务。他们基于一个关键假设：假视频会在空间维度上展示出异常特征，同时在时间维度上表现出不一致性。为了识别这些异常，研究者们选择了几种预训练的视频识别模型，这些模型能够理解视频的时空动态。他们选用的模型包括 Inflated 3D ConvNet (I3D)、X-CLIP 和 Video Masked Autoencoders (MAE)，每种模型都以其独特的方式处理视频数据。</p> 
<p>在检测部分，研究者们根据不同的应用场景调整了模型的训练方法。他们遵循两个基本原则：首先，为了应对开放检测设置，他们确保训练集尽可能多样化，包含了大量的真实和假视频；其次，当任务更加具体时，例如检测器需要识别特定模型或数据分布生成的视频，他们会缩小训练集的范围，使其更贴近实际任务的需求。</p> 
<p>在追踪源模型的过程中，研究者们采取了一种假设所有视频均为假视频的方法，并使用多标签分类模型来确定视频可能来源于的九种不同模型之一。与检测任务不同，追踪任务的训练涉及到所有这些模型生成的假视频，这样做可以帮助模型学习到每种模型生成视频的独特特征。</p> 
<p>为了验证所提出方法的有效性，研究者采用了两个广泛使用的数据集：WebVid-10M和InternVid，来进行评估。这些数据集拥有大量的视频和相应的文本描述，为训练和评估机器学习模型提供了丰富的资源。研究者进一步收集了所有可用的开源视频生成模型，并针对每一种模型生成了1000个视频，这些视频随后被用于检测和追踪任务。</p> 
<p></p> 
<figure class="image"> 
 <img alt="" height="634" src="https://images2.imgbox.com/6c/bf/QV01KLT5_o.png" width="752"> 
 <figcaption>
   假视频检测的实验流程 
 </figcaption> 
</figure> 
<p>图2展示了假视频检测的实验流程。为了确保真实视频和假视频具有相似的分布特征，研究者们采用了一种特定的方法来生成假视频。这种方法利用了真实视频的第一帧（以及可能伴随的字幕）来生成对应的假视频。这样的处理旨在最小化真实视频和假视频之间的差异，从而使得检测模型不能依赖于其他特征（例如，真实视频总是关于动物的，而假视频总是关于汽车的）。</p> 
<p>分类模型由两部分组成：一个作为主干的视频识别模型，以及一个全连接的模型。这个全连接的模型被训练来识别和区分真实视频和假视频。视频识别模型负责提取视频中的关键特征，而全连接层则用于学习如何根据这些特征来区分视频的真实性。通过在真实视频和假视频上训练这个复合模型，研究者们旨在提高检测系统的性能和准确性。这种训练方法有助于模型更好地理解真假视频之间的微妙差异，从而在实际应用中更有效地识别出假视频。</p> 
<figure class="image"> 
 <img alt="" height="363" src="https://images2.imgbox.com/ff/20/5GqLr8RT_o.png" width="722"> 
 <figcaption>
   实验的详细参数，包括输入帧数、训练周期、学习率等 
 </figcaption> 
</figure> 
<h5></h5> 
<p>研究者展示了在不同数据集和多种生成任务中应用三种检测模型的结果。他们发现，Video Masked Autoencoders（MAE）模型在各种场景下都展现出了稳定性和高效性，尤其是在开放检测设置中，MAE模型的检测准确率显著高于其他模型。</p> 
<figure class="image"> 
 <img alt="" height="795" src="https://images2.imgbox.com/06/d8/xyFYp54B_o.png" width="1200"> 
 <figcaption>
   使用不同的检测模型（X-CLIP, MAE, I3D）在 WebVid-10M 数据集上构造视频时的检测结果 
 </figcaption> 
</figure> 
<figure class="image"> 
 <img alt="" height="398" src="https://images2.imgbox.com/62/9c/zaosahak_o.png" width="737"> 
 <figcaption>
    MAE 基础的假视频检测模型在 InternVid 数据集上四种设置下的性能 
 </figcaption> 
</figure> 
<h5></h5> 
<p>研究者展示了在数据知情和数据不知情两种设置下，使用X-CLIP、I3D和MAE作为后端的追踪模型的性能。特别地，在数据不知情的设置中，即使面对未知数据源的视频，MAE模型仍然能够保持高达90%的准确率，显示出其在追踪任务上的鲁棒性。</p> 
<figure class="image"> 
 <img alt="" height="381" src="https://images2.imgbox.com/e5/37/Ywp28BCw_o.png" width="747"> 
 <figcaption>
   在数据感知和数据不可知设置下，使用 Webvid 和 InternVid 数据集的源追踪结果 
 </figcaption> 
</figure> 
<h5></h5> 
<p>研究者们采用了Grad-CAM技术来深入分析模型的决策过程。Grad-CAM（梯度类激活映射）是一种可视化技术，它能够展示模型在做出分类决策时所关注的视频区域。通过这种方式，研究者们可以直观地理解检测模型是如何识别出假视频的。</p> 
<figure class="image"> 
 <img alt="" height="744" src="https://images2.imgbox.com/7f/fc/F83FTJmi_o.png" width="745"> 
 <figcaption>
   使用 Grad-CAM 展示了 I3D 和 MAE 检测模型在 InternVid-SVD 生成任务中做出检测决定的具体方面 
 </figcaption> 
</figure> 
<p>研究者们首先观察了基于I3D和MAE的检测模型，这两种模型在假视频检测上都取得了超过90%的准确率。通过Grad-CAM生成的热图，研究者们发现这些模型主要依赖于视频中的对象来进行判断。例如，I3D模型倾向于关注人脸、栅栏、手指和引擎电池等对象，而MAE模型则集中于识别如下水道盖、人体和手指等异常对象。</p> 
<figure class="image"> 
 <img alt="" height="521" src="https://images2.imgbox.com/07/0b/dBYBmWw9_o.png" width="1200"> 
 <figcaption>
   源追踪模型依赖于视频中不同位置的特征来确定生成模型 
 </figcaption> 
</figure> 
<p>研究者们还注意到，MAE模型在检测时空异常方面表现得更为灵活和敏感。与I3D模型相比，MAE模型不仅能够识别出对象在空间维度上的扭曲，还能够察觉到对象在时间维度上的不连贯性。例如，如果视频中的排水管在连续帧之间发生了明显的形状变化，MAE模型能够捕捉到这种变化，并据此判断视频为假。</p> 
<p>通过Grad-CAM的分析，研究者们得出结论，MAE模型之所以在假视频检测上表现更为出色，是因为其能够同时关注视频中的时空异常，而不仅仅是空间上的失真。这一发现对于进一步优化和改进假视频检测技术具有重要意义。</p> 
<h4>滥用预防</h4> 
<p>与文本到视频的生成任务相比，图像到视频的生成任务更容易被滥用，因为存在修改者可能恶意使用这些技术。为了防止这种情况，研究者们设计了一种基于对抗性样本的防御机制，专门针对图像到视频的生成任务。</p> 
<p>对抗性样本最初是为了解决分类问题而引入的，通过向原始图像中添加对人类视觉不可见但能导致神经网络做出错误判断的小扰动。研究者们采用类似的方法，创建对抗性样本来干扰视频生成模型，使模型的编码器误解输入图像，从而产生不正确和异常的视频帧，保护原始图像不被滥用。</p> 
<p><img alt="" height="459" src="https://images2.imgbox.com/5d/e1/4lmw5JDf_o.png" width="685"></p> 
<figure class="image"> 
 <img alt="" height="364" src="https://images2.imgbox.com/1a/8c/y5tsD1iJ_o.png" width="1200"> 
 <figcaption>
   通过引入扰动来实施预防策略，包括有向防御和无向防御策略 
 </figcaption> 
</figure> 
<h5></h5> 
<p>研究者们在多个开源和商业视频生成模型上测试了他们的防御策略。他们采用了两种不同的防御方法：有向防御和无向防御。有向防御需要选择一个目标图像来指导添加的扰动，而无向防御则不需要目标图像，直接优化图像以增加损失。</p> 
<p>在实验中，研究者们使用了两种嵌入器，分别从Stable Video Diffusion模型中获取。他们设置了不同的参数，并测试了不同等级的扰动强度。实验结果表明，无论是有向防御还是无向防御，都能有效地防止Stable Video Diffusion生成正常的视频。然而，当面对未知的视频生成模型时，使用有向防御方法生成的对抗性样本可能效果较弱。</p> 
<figure class="image"> 
 <img alt="" height="435" src="https://images2.imgbox.com/21/ab/5pms22aE_o.png" width="1200"> 
 <figcaption>
   在不同 η 设置下，使用有向和无向防御方法生成的对抗性样本 
 </figcaption> 
</figure> 
<figure class="image"> 
 <img alt="" height="301" src="https://images2.imgbox.com/3d/fa/VQ3HZArG_o.png" width="716"> 
 <figcaption>
   不同扰动强度下对抗性样本的防御效果评估 
 </figcaption> 
</figure> 
<h5></h5> 
<p>虽然有向防御在某些情况下可以提供更有效的保护，但它可能需要精心选择目标图像，以确保扰动的不可见性。而无向防御虽然不需要目标图像，但可能需要更大的扰动强度来实现类似的防御效果。尽管存在这些挑战，研究者们提出的滥用预防策略为保护图像不被恶意用于视频生成提供了有价值的见解和工具。</p> 
<p></p> 
<p>论文链接：https://arxiv.org/abs/2402.13126</p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/cb357bb89c9a6a9e911cadf6bf0d4888/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【机器人学】2-1.六自由度机器人运动学逆解【附MATLAB机器人逆解代码】</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1c81feb8ee0838d81dd126fe399cb171/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【OrangePi AIpro】: 探索AI加成的开源硬件魅力</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>