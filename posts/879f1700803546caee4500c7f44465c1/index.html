<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>llama-factory SFT系列教程 (二)，大模型在自定义数据集 lora 训练与部署 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/879f1700803546caee4500c7f44465c1/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="llama-factory SFT系列教程 (二)，大模型在自定义数据集 lora 训练与部署">
  <meta property="og:description" content="文章目录 简介支持的模型列表2. 添加自定义数据集3. lora 微调4. 大模型 &#43; lora 权重，部署问题 参考资料 简介 文章列表：
llama-factory SFT系列教程 (一)，大模型 API 部署与使用llama-factory SFT系列教程 (二)，大模型在自定义数据集 lora 训练与部署
llama-factory SFT系列教程 (三)，chatglm3-6B 命名实体识别实战
llama-factory SFT 系列教程 (四)，lora sft 微调后，使用vllm加速推理 支持的模型列表 模型名模型大小默认模块TemplateBaichuan27B/13BW_packbaichuan2BLOOM560M/1.1B/1.7B/3B/7.1B/176Bquery_key_value-BLOOMZ560M/1.1B/1.7B/3B/7.1B/176Bquery_key_value-ChatGLM36Bquery_key_valuechatglm3DeepSeek (MoE)7B/16B/67Bq_proj,v_projdeepseekFalcon7B/40B/180Bquery_key_valuefalconGemma2B/7Bq_proj,v_projgemmaInternLM27B/20Bwqkvintern2LLaMA7B/13B/33B/65Bq_proj,v_proj-LLaMA-27B/13B/70Bq_proj,v_projllama2Mistral7Bq_proj,v_projmistralMixtral8x7Bq_proj,v_projmistralOLMo1B/7Batt_projolmoPhi-1.5/21.3B/2.7Bq_proj,v_proj-Qwen1.8B/7B/14B/72Bc_attnqwenQwen1.50.5B/1.8B/4B/7B/14B/72Bq_proj,v_projqwenStarCoder23B/7B/15Bq_proj,v_proj-XVERSE7B/13B/65Bq_proj,v_projxverseYi6B/9B/34Bq_proj,v_projyiYuan2B/51B/102Bq_proj,v_projyuan 参考自：https://zhuanlan.zhihu.com/p/689333581
默认模块 作为 --lora_target 参数的默认值，也可使用 --lora_target all 参数指定全部模块；
–template 参数可以是 default, alpaca, vicuna 等任意值。但“对话”（Chat）模型请务必使用对应的模板。
项目所支持模型的完整列表请参阅 constants.py。
2. 添加自定义数据集 LLaMA-Factory 数据集说明，参考该文件给出的说明，在dataset_info.json 文件中添加配置信息；
参考如下数据集格式，定义自定义数据集；
[ { &#34;instruction&#34;: &#34;用户指令（必填）&#34;, &#34;input&#34;: &#34;用户输入（选填）&#34;, &#34;output&#34;: &#34;模型回答（必填）&#34;, &#34;system&#34;: &#34;系统提示词（选填）&#34;, &#34;history&#34;: [ [&#34;第一轮指令（选填）&#34;, &#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-09T13:05:34+08:00">
    <meta property="article:modified_time" content="2024-05-09T13:05:34+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">llama-factory SFT系列教程 (二)，大模型在自定义数据集 lora 训练与部署</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><a href="#_1" rel="nofollow">简介</a></li><li><a href="#_9" rel="nofollow">支持的模型列表</a></li><li><a href="#2__41" rel="nofollow">2. 添加自定义数据集</a></li><li><a href="#3_lora__92" rel="nofollow">3. lora 微调</a></li><li><a href="#4___lora__134" rel="nofollow">4. 大模型 + lora 权重，部署</a></li><li><ul><li><a href="#_218" rel="nofollow">问题</a></li></ul> 
   </li><li><a href="#_222" rel="nofollow">参考资料</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h3><a id="_1"></a>简介</h3> 
<p>文章列表：</p> 
<ol><li><a href="https://blog.csdn.net/sjxgghg/article/details/137654018?spm=1001.2014.3001.5502">llama-factory SFT系列教程 (一)，大模型 API 部署与使用</a></li><li><a href="https://blog.csdn.net/sjxgghg/article/details/137656248?spm=1001.2014.3001.5502">llama-factory SFT系列教程 (二)，大模型在自定义数据集 lora 训练与部署<br> </a></li><li><a href="https://blog.csdn.net/sjxgghg/article/details/137698364">llama-factory SFT系列教程 (三)，chatglm3-6B 命名实体识别实战<br> </a></li><li><a href="https://blog.csdn.net/sjxgghg/article/details/137993809">llama-factory SFT 系列教程 (四)，lora sft 微调后，使用vllm加速推理</a></li></ol> 
<h3><a id="_9"></a>支持的模型列表</h3> 
<table><thead><tr><th>模型名</th><th>模型大小</th><th>默认模块</th><th>Template</th></tr></thead><tbody><tr><td>Baichuan2</td><td>7B/13B</td><td>W_pack</td><td>baichuan2</td></tr><tr><td>BLOOM</td><td>560M/1.1B/1.7B/3B/7.1B/176B</td><td>query_key_value</td><td>-</td></tr><tr><td>BLOOMZ</td><td>560M/1.1B/1.7B/3B/7.1B/176B</td><td>query_key_value</td><td>-</td></tr><tr><td>ChatGLM3</td><td>6B</td><td>query_key_value</td><td>chatglm3</td></tr><tr><td>DeepSeek (MoE)</td><td>7B/16B/67B</td><td>q_proj,v_proj</td><td>deepseek</td></tr><tr><td>Falcon</td><td>7B/40B/180B</td><td>query_key_value</td><td>falcon</td></tr><tr><td>Gemma</td><td>2B/7B</td><td>q_proj,v_proj</td><td>gemma</td></tr><tr><td>InternLM2</td><td>7B/20B</td><td>wqkv</td><td>intern2</td></tr><tr><td>LLaMA</td><td>7B/13B/33B/65B</td><td>q_proj,v_proj</td><td>-</td></tr><tr><td>LLaMA-2</td><td>7B/13B/70B</td><td>q_proj,v_proj</td><td>llama2</td></tr><tr><td>Mistral</td><td>7B</td><td>q_proj,v_proj</td><td>mistral</td></tr><tr><td>Mixtral</td><td>8x7B</td><td>q_proj,v_proj</td><td>mistral</td></tr><tr><td>OLMo</td><td>1B/7B</td><td>att_proj</td><td>olmo</td></tr><tr><td>Phi-1.5/2</td><td>1.3B/2.7B</td><td>q_proj,v_proj</td><td>-</td></tr><tr><td>Qwen</td><td>1.8B/7B/14B/72B</td><td>c_attn</td><td>qwen</td></tr><tr><td>Qwen1.5</td><td>0.5B/1.8B/4B/7B/14B/72B</td><td>q_proj,v_proj</td><td>qwen</td></tr><tr><td>StarCoder2</td><td>3B/7B/15B</td><td>q_proj,v_proj</td><td>-</td></tr><tr><td>XVERSE</td><td>7B/13B/65B</td><td>q_proj,v_proj</td><td>xverse</td></tr><tr><td>Yi</td><td>6B/9B/34B</td><td>q_proj,v_proj</td><td>yi</td></tr><tr><td>Yuan</td><td>2B/51B/102B</td><td>q_proj,v_proj</td><td>yuan</td></tr></tbody></table> 
<blockquote> 
 <p>参考自：https://zhuanlan.zhihu.com/p/689333581</p> 
</blockquote> 
<ul><li> <p><strong>默认模块</strong> 作为 --lora_target 参数的默认值，也可使用 --lora_target all 参数指定全部模块；</p> </li><li> <p>–template 参数可以是 default, alpaca, vicuna 等任意值。但“对话”（Chat）模型请务必使用对应的模板。</p> </li></ul> 
<p>项目所支持模型的完整列表请参阅 <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/src/llmtuner/extras/constants.py">constants.py</a>。</p> 
<h3><a id="2__41"></a>2. 添加自定义数据集</h3> 
<p><a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md">LLaMA-Factory 数据集说明</a>，参考该文件给出的说明，在<code>dataset_info.json</code> 文件中添加配置信息；</p> 
<p>参考如下数据集格式，定义自定义数据集；</p> 
<pre><code class="prism language-python"><span class="token punctuation">[</span>
  <span class="token punctuation">{<!-- --></span>
    <span class="token string">"instruction"</span><span class="token punctuation">:</span> <span class="token string">"用户指令（必填）"</span><span class="token punctuation">,</span>
    <span class="token string">"input"</span><span class="token punctuation">:</span> <span class="token string">"用户输入（选填）"</span><span class="token punctuation">,</span>
    <span class="token string">"output"</span><span class="token punctuation">:</span> <span class="token string">"模型回答（必填）"</span><span class="token punctuation">,</span>
    <span class="token string">"system"</span><span class="token punctuation">:</span> <span class="token string">"系统提示词（选填）"</span><span class="token punctuation">,</span>
    <span class="token string">"history"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>
      <span class="token punctuation">[</span><span class="token string">"第一轮指令（选填）"</span><span class="token punctuation">,</span> <span class="token string">"第一轮回答（选填）"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
      <span class="token punctuation">[</span><span class="token string">"第二轮指令（选填）"</span><span class="token punctuation">,</span> <span class="token string">"第二轮回答（选填）"</span><span class="token punctuation">]</span>
    <span class="token punctuation">]</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">]</span>
</code></pre> 
<p>新数据集内容如下：<br> <code>diy.json</code></p> 
<pre><code class="prism language-python"><span class="token punctuation">[</span>
  <span class="token punctuation">{<!-- --></span>
    <span class="token string">"instruction"</span><span class="token punctuation">:</span> <span class="token string">"你是谁？"</span><span class="token punctuation">,</span>
    <span class="token string">"input"</span><span class="token punctuation">:</span> <span class="token string">""</span><span class="token punctuation">,</span>
    <span class="token string">"output"</span><span class="token punctuation">:</span> <span class="token string">"我是Qwen，edit by JieShin."</span><span class="token punctuation">,</span>
    <span class="token string">"history"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token punctuation">{<!-- --></span>
    <span class="token string">"instruction"</span><span class="token punctuation">:</span> <span class="token string">"你能帮我干些什么？"</span><span class="token punctuation">,</span>
    <span class="token string">"input"</span><span class="token punctuation">:</span> <span class="token string">""</span><span class="token punctuation">,</span>
    <span class="token string">"output"</span><span class="token punctuation">:</span> <span class="token string">"我能和你互动问答，我的其他功能正在开发中。"</span><span class="token punctuation">,</span>
    <span class="token string">"history"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
   <span class="token punctuation">}</span>
<span class="token punctuation">]</span>
</code></pre> 
<p>添加自定义数据集的步骤如下：</p> 
<ol><li>将 <code>diy.json</code> 文件保存到 <code>LLaMA-Factory/data</code> 文件夹下；</li></ol> 
<p><img src="https://images2.imgbox.com/1f/e7/EBFYIX8B_o.png" alt="在这里插入图片描述"></p> 
<ol start="2"><li>在 dataset_info.json 文件中，配置数据集<br> 首先计算 <code>diy.json</code> 文件的sha1sum, <code>sha1sum diy.json</code><br> <img src="https://images2.imgbox.com/bf/b6/toLFu8Wc_o.png" alt="在这里插入图片描述"><br> <code>vim dataset_info.json</code> 添加自定义数据集的配置信息, 把 diy.json 文件的sha1 值添加到文件中，<code>"diy"</code> 为该数据集名；<br> <img src="https://images2.imgbox.com/a0/d5/RWg6WSH1_o.png" alt="在这里插入图片描述"></li></ol> 
<h3><a id="3_lora__92"></a>3. lora 微调</h3> 
<p>使用配置好的 <code>diy</code> 数据集进行模型训练；</p> 
<p><code>--model_name_or_path qwen/Qwen-7B</code>，只写模型名，不写绝对路径可运行成功，因为配置了<code>export USE_MODELSCOPE_HUB=1</code></p> 
<p>查看 配置是否生效，输出1 即为配置成功：<br> <code>echo $USE_MODELSCOPE_HUB</code></p> 
<p><img src="https://images2.imgbox.com/b3/e3/YpCTugpc_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-bash"><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> python src/train_bash.py <span class="token punctuation">\</span>
<span class="token parameter variable">--stage</span> sft <span class="token punctuation">\</span>
<span class="token parameter variable">--do_train</span> <span class="token punctuation">\</span>
<span class="token parameter variable">--model_name_or_path</span> qwen/Qwen-7B <span class="token punctuation">\</span>
<span class="token parameter variable">--dataset</span> diy <span class="token punctuation">\</span>
<span class="token parameter variable">--template</span> qwen <span class="token punctuation">\</span>
<span class="token parameter variable">--finetuning_type</span> lora <span class="token punctuation">\</span>
<span class="token parameter variable">--lora_target</span> c_attn <span class="token punctuation">\</span>
<span class="token parameter variable">--output_dir</span> /mnt/workspace/llama_factory_demo/qwen/lora/sft <span class="token punctuation">\</span>
<span class="token parameter variable">--overwrite_cache</span> <span class="token punctuation">\</span>
<span class="token parameter variable">--per_device_train_batch_size</span> <span class="token number">4</span> <span class="token punctuation">\</span>
<span class="token parameter variable">--gradient_accumulation_steps</span> <span class="token number">4</span> <span class="token punctuation">\</span>
<span class="token parameter variable">--lr_scheduler_type</span> cosine <span class="token punctuation">\</span>
<span class="token parameter variable">--logging_steps</span> <span class="token number">10</span> <span class="token punctuation">\</span>
<span class="token parameter variable">--save_strategy</span> epoch <span class="token punctuation">\</span>
<span class="token parameter variable">--learning_rate</span> 5e-5 <span class="token punctuation">\</span>
<span class="token parameter variable">--num_train_epochs</span> <span class="token number">50.0</span> <span class="token punctuation">\</span>
<span class="token parameter variable">--plot_loss</span> <span class="token punctuation">\</span>
<span class="token parameter variable">--fp16</span>
</code></pre> 
<p>训练完成的lora 权重，保存在下述文件夹中；<br> <code>--output_dir /mnt/workspace/llama_factory_demo/qwen/lora/sft </code></p> 
<p>模型的训练结果如下：<br> <img src="https://images2.imgbox.com/d0/52/wXrZx2oS_o.png" alt="在这里插入图片描述"></p> 
<p>lora 训练后的权重如下图所示：<br> <img src="https://images2.imgbox.com/66/a3/yArZR7N6_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="4___lora__134"></a>4. 大模型 + lora 权重，部署</h3> 
<p>由于llama-factory 不支持 qwen 结合 lora 进行推理，故需要把 lora 权重融合进大模型成一个全新的大模型权重；</p> 
<p>可查看如下链接，了解如何合并模型权重：<a href="https://github.com/hiyouga/LLaMA-Factory/issues/2998#issuecomment-2051345963">merge_lora GitHub issue</a></p> 
<p>下述是合并 lora 权重的脚本，全新大模型的权重保存到 export_dir 文件夹；</p> 
<pre><code class="prism language-bash"><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> python src/export_model.py <span class="token punctuation">\</span>
    <span class="token parameter variable">--model_name_or_path</span> qwen/Qwen-7B <span class="token punctuation">\</span>
    <span class="token parameter variable">--adapter_name_or_path</span> /mnt/workspace/llama_factory_demo/qwen/lora/sft/checkpoint-50 <span class="token punctuation">\</span>
    <span class="token parameter variable">--template</span> qwen <span class="token punctuation">\</span>
    <span class="token parameter variable">--finetuning_type</span> lora <span class="token punctuation">\</span>
    <span class="token parameter variable">--export_dir</span> /mnt/workspace/merge_w/qwen <span class="token punctuation">\</span>
    <span class="token parameter variable">--export_size</span> <span class="token number">2</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--export_legacy_format</span> False
</code></pre> 
<p>使用融合后到大模型进行推理，model_name_or_path 为融合后的新大模型路径</p> 
<pre><code class="prism language-bash"><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> <span class="token assign-left variable">API_PORT</span><span class="token operator">=</span><span class="token number">8000</span> python src/api_demo.py <span class="token punctuation">\</span>
    <span class="token parameter variable">--model_name_or_path</span> /mnt/workspace/merge_w/qwen <span class="token punctuation">\</span>
    <span class="token parameter variable">--template</span> qwen <span class="token punctuation">\</span>
    <span class="token parameter variable">--infer_backend</span> vllm <span class="token punctuation">\</span>
    <span class="token parameter variable">--vllm_enforce_eager</span> <span class="token punctuation">\</span>
~                             
</code></pre> 
<p>模型请求脚本</p> 
<pre><code class="prism language-bash"><span class="token function">curl</span> <span class="token parameter variable">-X</span> <span class="token string">'POST'</span> <span class="token punctuation">\</span>
  <span class="token string">'http://0.0.0.0:8000/v1/chat/completions'</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">-H</span> <span class="token string">'accept: application/json'</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">-H</span> <span class="token string">'Content-Type: application/json'</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">-d</span> <span class="token string">'{
  "model": "string",
  "messages": [
    {
      "role": "user",
      "content": "你能帮我做一些什么事情？",
      "tool_calls": [
        {
          "id": "call_default",
          "type": "function",
          "function": {
            "name": "string",
            "arguments": "string"
          }
        }
      ]
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "string",
        "description": "string",
        "parameters": {}
      }
    }
  ],
  "do_sample": true,
  "temperature": 0,
  "top_p": 0,
  "n": 1,
  "max_tokens": 128,
  "stream": false
}'</span>
</code></pre> 
<p>模型推理得到了和数据集中一样的结果，这说明 lora 微调生效了；<br> <img src="https://images2.imgbox.com/4f/61/XyqzG1dc_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>以为设置了 <code>"stop": "&lt;|endoftext|&gt;"</code>，模型会在遇到结束符自动结束，但模型依然推理到了最长的长度后结束，设置的 stop 并没有生效；</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/f1/20/S7JXtw5D_o.png" alt="在这里插入图片描述"></p> 
<p><code>llama-factory</code>的作者表示还没有支持stop，万一未来支持了stop功能，大家可以关注这个issue <a href="https://github.com/hiyouga/LLaMA-Factory/issues/3114">support “stop” in api chat/completions #3114</a></p> 
<p>Qwen 有些模型的stop还没支持，有一些大模型已经支持了 stop。</p> 
<h4><a id="_218"></a>问题</h4> 
<p>虽然设置了 <code>"temperature": 0</code> ， 但是模型的输出结果依然变动很大，运行3-4次后，才出现训练数据集中的结果；</p> 
<h3><a id="_222"></a>参考资料</h3> 
<ul><li><a href="https://blog.csdn.net/ckq707718837/article/details/135106348">api 参数列表</a></li><li><a href="https://zhuanlan.zhihu.com/p/689333581" rel="nofollow">使用LLaMa-Factory简单高效微调大模型</a><br> 展示了支持的大模型列表；</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7d83a3877a9117014f737b2f4159d62e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">mac上，『已损坏，无法打开。 您应该将它移到废纸篓』的恶心玩意怎么解决</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/691e219135dfe0b3528ff29ae6deee91/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">深入浅出，一文搞懂向量数据库工作原理和应用</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>