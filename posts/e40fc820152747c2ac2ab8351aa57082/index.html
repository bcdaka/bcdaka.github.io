<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AI大模型探索之路-训练篇21：Llama2微调实战-LoRA技术微调步骤详解 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/e40fc820152747c2ac2ab8351aa57082/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="AI大模型探索之路-训练篇21：Llama2微调实战-LoRA技术微调步骤详解">
  <meta property="og:description" content="系列篇章💥 AI大模型探索之路-训练篇1：大语言模型微调基础认知
AI大模型探索之路-训练篇2：大语言模型预训练基础认知
AI大模型探索之路-训练篇3：大语言模型全景解读
AI大模型探索之路-训练篇4：大语言模型训练数据集概览
AI大模型探索之路-训练篇5：大语言模型预训练数据准备-词元化
AI大模型探索之路-训练篇6：大语言模型预训练数据准备-预处理
AI大模型探索之路-训练篇7：大语言模型Transformer库之HuggingFace介绍
AI大模型探索之路-训练篇8：大语言模型Transformer库-预训练流程编码体验
AI大模型探索之路-训练篇9：大语言模型Transformer库-Pipeline组件实践
AI大模型探索之路-训练篇10：大语言模型Transformer库-Tokenizer组件实践
AI大模型探索之路-训练篇11：大语言模型Transformer库-Model组件实践
AI大模型探索之路-训练篇12：语言模型Transformer库-Datasets组件实践
AI大模型探索之路-训练篇13：大语言模型Transformer库-Evaluate组件实践
AI大模型探索之路-训练篇14：大语言模型Transformer库-Trainer组件实践
AI大模型探索之路-训练篇15：大语言模型预训练之全量参数微调
AI大模型探索之路-训练篇16：大语言模型预训练-微调技术之LoRA
AI大模型探索之路-训练篇17：大语言模型预训练-微调技术之QLoRA
AI大模型探索之路-训练篇18：大语言模型预训练-微调技术之Prompt Tuning
AI大模型探索之路-训练篇19：大语言模型预训练-微调技术之Prefix Tuning
AI大模型探索之路-训练篇20：大语言模型预训练-常见微调技术对比
目录 系列篇章💥前言一、Llama2总体概述二、Llama2功能特点三、Llama2微调准备四、Llama2微调实战学术资源加速步骤1 导入相关包步骤2 加载数据集步骤3 数据集预处理1）获取分词器2）对齐设置3）定义数据处理函数4）对数据进行预处理5）打印查看inpu_ids6）检查数据（是否包含结束符） 步骤4 创建模型1、PEFT 步骤1 配置文件2、PEFT 步骤2 创建模型 步骤5 配置训练参数步骤6 创建训练器步骤7 模型训练步骤8 模型推理 五、Llama2 调试过程问题整理1、MAX_LENGTH设置：2、对齐设置3、设置pad_token_id4、结束符设置5、设置 adam_epsilon 总结 前言 在人工智能领域，大型预训练语言模型（Large Language Models, LLMs）已经成为推动自然语言处理（NLP）任务发展的重要力量。Llama2作为其中的一个先进代表，通过其庞大的参数规模和深度学习机制，展现了在多种NLP任务上的卓越性能。然而，为了使Llama2更好地适应特定的应用场景，对其进行微调（Fine-tuning）成为了一个关键步骤。本文将从专业角度出发，详细介绍如何基于LoRA（Low-Rank Adaptation）技术对Llama2进行微调。
一、Llama2总体概述 Llama2是Meta AI的研究成果（最新版Llama3最近也现世了），这是一个致力于人工智能研究的团队，隶属于Meta公司（即原Facebook公司）。该模型包括7B（70亿参数）、13B（130亿参数）以及70B（700亿参数）三个版本，训练所用的数据集达到了惊人的2万亿tokens。这一大语言模型的开发体现了Meta在AI领域的深入研究和技术积累。
Llama2是继Llama之后的一个大型语言模型，它在原有模型的基础上进行了扩展和优化，以支持更复杂的语言理解和生成任务。Llama2是一个基于Transformer架构的自回归类型大语言模型，Llama2通常具有数十亿甚至数百亿的参数，这些参数通过大量文本数据进行预训练得到，使其具备了广泛的语言知识和强大的语言生成能力。Llama2采用了分层的训练策略，通过预训练、指令微调和任务微调等阶段，使得模型具有较强的泛化能力。
github地址：https://github.com/meta-llama
Llama2目前开源的有7B、13B、70B，但对中文支持不是特别友好；需要自己使用中文语料进行预训练；也可以选用经过其他大佬微调过的中文变体版；
二、Llama2功能特点 Llama2的功能特点主要体现在以下几个方面：
1）上下文理解：能够理解长篇文本中的复杂语境和细微差别。
2）多任务学习：在预训练阶段就接触了多种任务，使其具备了解决多种NLP问题的能力。
3）生成能力：可以生成连贯、逻辑性强的文本内容。
4）适应性：通过微调，Llama2能够快速适应特定的应用场景和任务需求
三、Llama2微调准备 在对Llama2进行微调之前，需要做好以下准备工作：
1）数据准备：收集并预处理用于微调的数据集，包括文本清洗、标注等。
数据集：https://huggingface.co/datasets/c-s-ale/alpaca-gpt4-data-zh
2）模型选择：根据任务需求选择合适的Llama2模型版本。
模型地址：https://www.modelscope.cn/models?name=llama2-7b-ms&amp;page=1
模型下载方式1
git clone https://www.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-12T19:44:51+08:00">
    <meta property="article:modified_time" content="2024-05-12T19:44:51+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AI大模型探索之路-训练篇21：Llama2微调实战-LoRA技术微调步骤详解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_1"></a>系列篇章💥</h2> 
<p><a href="https://xundaomalu.blog.csdn.net/article/details/138107946" rel="nofollow">AI大模型探索之路-训练篇1：大语言模型微调基础认知</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138143923" rel="nofollow">AI大模型探索之路-训练篇2：大语言模型预训练基础认知</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138161057" rel="nofollow">AI大模型探索之路-训练篇3：大语言模型全景解读</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138205204" rel="nofollow">AI大模型探索之路-训练篇4：大语言模型训练数据集概览</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138225299" rel="nofollow">AI大模型探索之路-训练篇5：大语言模型预训练数据准备-词元化</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138267915" rel="nofollow">AI大模型探索之路-训练篇6：大语言模型预训练数据准备-预处理</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138294519" rel="nofollow">AI大模型探索之路-训练篇7：大语言模型Transformer库之HuggingFace介绍</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138348834" rel="nofollow">AI大模型探索之路-训练篇8：大语言模型Transformer库-预训练流程编码体验</a><br> <a href="https://blog.csdn.net/xiaobing259/article/details/138373677">AI大模型探索之路-训练篇9：大语言模型Transformer库-Pipeline组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138391592" rel="nofollow">AI大模型探索之路-训练篇10：大语言模型Transformer库-Tokenizer组件实践</a><br> <a href="https://blog.csdn.net/xiaobing259/article/details/138424867">AI大模型探索之路-训练篇11：大语言模型Transformer库-Model组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138426216" rel="nofollow">AI大模型探索之路-训练篇12：语言模型Transformer库-Datasets组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138448172" rel="nofollow">AI大模型探索之路-训练篇13：大语言模型Transformer库-Evaluate组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138448511" rel="nofollow">AI大模型探索之路-训练篇14：大语言模型Transformer库-Trainer组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138472105" rel="nofollow">AI大模型探索之路-训练篇15：大语言模型预训练之全量参数微调</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138518728" rel="nofollow">AI大模型探索之路-训练篇16：大语言模型预训练-微调技术之LoRA</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138555530" rel="nofollow">AI大模型探索之路-训练篇17：大语言模型预训练-微调技术之QLoRA</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138595171" rel="nofollow">AI大模型探索之路-训练篇18：大语言模型预训练-微调技术之Prompt Tuning</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138631718" rel="nofollow">AI大模型探索之路-训练篇19：大语言模型预训练-微调技术之Prefix Tuning</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138604711" rel="nofollow">AI大模型探索之路-训练篇20：大语言模型预训练-常见微调技术对比</a></p> 
<hr> 
<p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#_1" rel="nofollow">系列篇章💥</a></li><li><a href="#_30" rel="nofollow">前言</a></li><li><a href="#Llama2_35" rel="nofollow">一、Llama2总体概述</a></li><li><a href="#Llama2_43" rel="nofollow">二、Llama2功能特点</a></li><li><a href="#Llama2_49" rel="nofollow">三、Llama2微调准备</a></li><li><a href="#Llama2_78" rel="nofollow">四、Llama2微调实战</a></li><li><ul><li><a href="#_84" rel="nofollow">学术资源加速</a></li><li><a href="#1__100" rel="nofollow">步骤1 导入相关包</a></li><li><a href="#2__108" rel="nofollow">步骤2 加载数据集</a></li><li><a href="#3__139" rel="nofollow">步骤3 数据集预处理</a></li><li><ul><li><a href="#1_141" rel="nofollow">1）获取分词器</a></li><li><a href="#2_159" rel="nofollow">2）对齐设置</a></li><li><a href="#3_172" rel="nofollow">3）定义数据处理函数</a></li><li><a href="#4_194" rel="nofollow">4）对数据进行预处理</a></li><li><a href="#5inpu_ids_210" rel="nofollow">5）打印查看inpu_ids</a></li><li><a href="#6_222" rel="nofollow">6）检查数据（是否包含结束符）</a></li></ul> 
   </li><li><a href="#4__234" rel="nofollow">步骤4 创建模型</a></li><li><ul><li><a href="#1PEFT_1__258" rel="nofollow">1、PEFT 步骤1 配置文件</a></li><li><a href="#2PEFT_2__276" rel="nofollow">2、PEFT 步骤2 创建模型</a></li></ul> 
   </li><li><a href="#5__731" rel="nofollow">步骤5 配置训练参数</a></li><li><a href="#6__748" rel="nofollow">步骤6 创建训练器</a></li><li><a href="#7__761" rel="nofollow">步骤7 模型训练</a></li><li><a href="#8__770" rel="nofollow">步骤8 模型推理</a></li></ul> 
  </li><li><a href="#Llama2__790" rel="nofollow">五、Llama2 调试过程问题整理</a></li><li><ul><li><a href="#1MAX_LENGTH_791" rel="nofollow">1、MAX_LENGTH设置：</a></li><li><a href="#2_794" rel="nofollow">2、对齐设置</a></li><li><a href="#3pad_token_id_798" rel="nofollow">3、设置pad_token_id</a></li><li><a href="#4_802" rel="nofollow">4、结束符设置</a></li><li><a href="#5_adam_epsilon_849" rel="nofollow">5、设置 adam_epsilon</a></li></ul> 
  </li><li><a href="#_869" rel="nofollow">总结</a></li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_30"></a>前言</h2> 
<p>在人工智能领域，大型预训练语言模型（Large Language Models, LLMs）已经成为推动自然语言处理（NLP）任务发展的重要力量。Llama2作为其中的一个先进代表，通过其庞大的参数规模和深度学习机制，展现了在多种NLP任务上的卓越性能。然而，为了使Llama2更好地适应特定的应用场景，对其进行微调（Fine-tuning）成为了一个关键步骤。本文将从专业角度出发，详细介绍如何基于LoRA（Low-Rank Adaptation）技术对Llama2进行微调。<br> <img src="https://images2.imgbox.com/cd/46/bwdRGm7E_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="Llama2_35"></a>一、Llama2总体概述</h2> 
<p>Llama2是Meta AI的研究成果（最新版Llama3最近也现世了），这是一个致力于人工智能研究的团队，隶属于Meta公司（即原Facebook公司）。该模型包括7B（70亿参数）、13B（130亿参数）以及70B（700亿参数）三个版本，训练所用的数据集达到了惊人的2万亿tokens。这一大语言模型的开发体现了Meta在AI领域的深入研究和技术积累。</p> 
<p>Llama2是继Llama之后的一个大型语言模型，它在原有模型的基础上进行了扩展和优化，以支持更复杂的语言理解和生成任务。Llama2是一个基于Transformer架构的自回归类型大语言模型，Llama2通常具有数十亿甚至数百亿的参数，这些参数通过大量文本数据进行预训练得到，使其具备了广泛的语言知识和强大的语言生成能力。Llama2采用了分层的训练策略，通过预训练、指令微调和任务微调等阶段，使得模型具有较强的泛化能力。<br> github地址：https://github.com/meta-llama<br> <img src="https://images2.imgbox.com/2b/ae/IEoBVb3u_o.png" alt="在这里插入图片描述"></p> 
<p><code>Llama2目前开源的有7B、13B、70B，但对中文支持不是特别友好；需要自己使用中文语料进行预训练；也可以选用经过其他大佬微调过的中文变体版；</code></p> 
<h2><a id="Llama2_43"></a>二、Llama2功能特点</h2> 
<p>Llama2的功能特点主要体现在以下几个方面：<br> 1）上下文理解：能够理解长篇文本中的复杂语境和细微差别。<br> 2）多任务学习：在预训练阶段就接触了多种任务，使其具备了解决多种NLP问题的能力。<br> 3）生成能力：可以生成连贯、逻辑性强的文本内容。<br> 4）适应性：通过微调，Llama2能够快速适应特定的应用场景和任务需求</p> 
<h2><a id="Llama2_49"></a>三、Llama2微调准备</h2> 
<p>在对Llama2进行微调之前，需要做好以下准备工作：<br> 1）数据准备：收集并预处理用于微调的数据集，包括文本清洗、标注等。<br> 数据集：https://huggingface.co/datasets/c-s-ale/alpaca-gpt4-data-zh<br> <img src="https://images2.imgbox.com/ad/dd/zb6fP3IL_o.png" alt="在这里插入图片描述"></p> 
<p>2）模型选择：根据任务需求选择合适的Llama2模型版本。<br> 模型地址：https://www.modelscope.cn/models?name=llama2-7b-ms&amp;page=1<br> <img src="https://images2.imgbox.com/b4/73/RH4KAdz5_o.png" alt="在这里插入图片描述"></p> 
<p><strong>模型下载方式1</strong></p> 
<pre><code class="prism language-bash"><span class="token function">git</span> clone https://www.modelscope.cn/Llama-2-7b-ms.git
</code></pre> 
<p><strong>模型下载方式2</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> modelscope <span class="token keyword">import</span> snapshot_download
snapshot_download<span class="token punctuation">(</span>mode_id<span class="token operator">=</span><span class="token string">'Llama-2-7b-ms'</span><span class="token punctuation">,</span>cache_dir<span class="token operator">=</span>”<span class="token punctuation">.</span><span class="token operator">/</span>”<span class="token punctuation">)</span>
</code></pre> 
<p><strong>ModelScope（魔搭）说明：</strong><br> <img src="https://images2.imgbox.com/66/82/ruUVl0ED_o.png" alt="在这里插入图片描述"></p> 
<p><a href="https://www.modelscope.cn/" rel="nofollow">ModelScope</a>是一个模型服务平台，为国内AI开发者提供了丰富的资源和便利的服务；<code>类似HuggingFace；如果网络受限无法使用HuggingFace，可以选择从ModelScope下载相关模型资源；</code><br> ModelScope平台涵盖了自然语言处理、图像、语音、多模态和科学计算等五大AI领域，并提供了一套支持模型高效推理、训练评估及导出的Python Library。ModelScope通过提供易用的工具和资源，使得开发者能够更加简单地应用AI模型技术于实际问题解决中。此外，ModelScope还汇聚了众多优秀的国产模型资源，为开发者提供了更多的选择，并且鼓励开发者贡献自己的模型，从而促进了国内AI模型的共享和交流。</p> 
<h2><a id="Llama2_78"></a>四、Llama2微调实战</h2> 
<p>模型：Llama-2<br> 数据集：alpaca-gpt4-data-zh<br> 微调技术：LoRA<br> 使用资源：半精度 fp16（这里是为了减少GPU资源；正常情况默认微调时是全精度32位浮点数）</p> 
<h3><a id="_84"></a>学术资源加速</h3> 
<p>方便从huggingface下载模型，这云平台<a href="https://www.autodl.com/" rel="nofollow">autodl</a>提供的，仅适用于autodl。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> subprocess
<span class="token keyword">import</span> os

result <span class="token operator">=</span> subprocess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token string">'bash -c "source /etc/network_turbo &amp;&amp; env | grep proxy"'</span><span class="token punctuation">,</span> shell<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> capture_output<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> result<span class="token punctuation">.</span>stdout
<span class="token keyword">for</span> line <span class="token keyword">in</span> output<span class="token punctuation">.</span>splitlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token string">'='</span> <span class="token keyword">in</span> line<span class="token punctuation">:</span>
        var<span class="token punctuation">,</span> value <span class="token operator">=</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'='</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span>var<span class="token punctuation">]</span> <span class="token operator">=</span> value

</code></pre> 
<h3><a id="1__100"></a>步骤1 导入相关包</h3> 
<p>开始之前，我们需要导入适用于模型训练和推理的必要库，如transformers。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> datasets <span class="token keyword">import</span> Dataset
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForCausalLM<span class="token punctuation">,</span> DataCollatorForSeq2Seq<span class="token punctuation">,</span> TrainingArguments<span class="token punctuation">,</span> Trainer
</code></pre> 
<h3><a id="2__108"></a>步骤2 加载数据集</h3> 
<p>使用适当的数据加载器，例如datasets库，来加载预处理过的指令遵循性任务数据集。</p> 
<pre><code class="prism language-python">ds <span class="token operator">=</span> Dataset<span class="token punctuation">.</span>load_from_disk<span class="token punctuation">(</span><span class="token string">"/root/PEFT代码/tuning/lesson01/data/alpaca_data_zh"</span><span class="token punctuation">)</span>
ds
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">Dataset<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>
    features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'output'</span><span class="token punctuation">,</span> <span class="token string">'input'</span><span class="token punctuation">,</span> <span class="token string">'instruction'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    num_rows<span class="token punctuation">:</span> <span class="token number">26858</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<p>查看数据</p> 
<pre><code class="prism language-python">ds<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python"><span class="token punctuation">{<!-- --></span><span class="token string">'output'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'以下是保持健康的三个提示：\n\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\n\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\n\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。'</span><span class="token punctuation">,</span>
  <span class="token string">'4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。'</span><span class="token punctuation">,</span>
  <span class="token string">'朱利叶斯·凯撒，又称尤利乌斯·恺撒（Julius Caesar）是古罗马的政治家、军事家和作家。他于公元前44年3月15日被刺杀。 \n\n根据历史记载，当时罗马元老院里一些参议员联合起来策划了对恺撒的刺杀行动，因为他们担心恺撒的统治将给罗马共和制带来威胁。在公元前44年3月15日（又称“3月的艾达之日”），恺撒去参加元老院会议时，被一群参议员包围并被攻击致死。据记载，他身中23刀，其中一刀最终致命。'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token string">'input'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">''</span><span class="token punctuation">,</span> <span class="token string">'输入：4/16'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token string">'instruction'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'保持健康的三个提示。'</span><span class="token punctuation">,</span> <span class="token string">'解释为什么以下分数等同于1/4'</span><span class="token punctuation">,</span> <span class="token string">'朱利叶斯·凯撒是如何死亡的？'</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
</code></pre> 
<h3><a id="3__139"></a>步骤3 数据集预处理</h3> 
<p>利用预训练模型的分词器（Tokenizer）对原始文本进行编码，并生成相应的输入ID、注意力掩码和标签。</p> 
<h4><a id="1_141"></a>1）获取分词器</h4> 
<pre><code class="prism language-python"><span class="token comment">#加载本地模型，提前下载到本地</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"/root/autodl-tmp/Llama-2-7b-ms"</span><span class="token punctuation">)</span>
tokenizer
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">LlamaTokenizerFast<span class="token punctuation">(</span>name_or_path<span class="token operator">=</span><span class="token string">'/root/autodl-tmp/Llama-2-7b-ms'</span><span class="token punctuation">,</span> vocab_size<span class="token operator">=</span><span class="token number">32000</span><span class="token punctuation">,</span> model_max_length<span class="token operator">=</span><span class="token number">1000000000000000019884624838656</span><span class="token punctuation">,</span> is_fast<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> padding_side<span class="token operator">=</span><span class="token string">'left'</span><span class="token punctuation">,</span> truncation_side<span class="token operator">=</span><span class="token string">'right'</span><span class="token punctuation">,</span> special_tokens<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">'bos_token'</span><span class="token punctuation">:</span> <span class="token string">'&lt;s&gt;'</span><span class="token punctuation">,</span> <span class="token string">'eos_token'</span><span class="token punctuation">:</span> <span class="token string">'&lt;/s&gt;'</span><span class="token punctuation">,</span> <span class="token string">'unk_token'</span><span class="token punctuation">:</span> <span class="token string">'&lt;unk&gt;'</span><span class="token punctuation">,</span> <span class="token string">'pad_token'</span><span class="token punctuation">:</span> <span class="token string">'&lt;unk&gt;'</span><span class="token punctuation">}</span><span class="token punctuation">,</span> clean_up_tokenization_spaces<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  added_tokens_decoder<span class="token operator">=</span><span class="token punctuation">{<!-- --></span>
	<span class="token number">0</span><span class="token punctuation">:</span> AddedToken<span class="token punctuation">(</span><span class="token string">"&lt;unk&gt;"</span><span class="token punctuation">,</span> rstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> lstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> single_word<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> normalized<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> special<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
	<span class="token number">1</span><span class="token punctuation">:</span> AddedToken<span class="token punctuation">(</span><span class="token string">"&lt;s&gt;"</span><span class="token punctuation">,</span> rstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> lstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> single_word<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> normalized<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> special<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
	<span class="token number">2</span><span class="token punctuation">:</span> AddedToken<span class="token punctuation">(</span><span class="token string">"&lt;/s&gt;"</span><span class="token punctuation">,</span> rstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> lstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> single_word<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> normalized<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> special<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
	<span class="token number">32000</span><span class="token punctuation">:</span> AddedToken<span class="token punctuation">(</span><span class="token string">"&lt;pad&gt;"</span><span class="token punctuation">,</span> rstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> lstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> single_word<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> normalized<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> special<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre> 
<h4><a id="2_159"></a>2）对齐设置</h4> 
<p>padding_side模式左对齐，需要修改改为右边对齐</p> 
<pre><code class="prism language-python">tokenizer<span class="token punctuation">.</span>padding_side <span class="token operator">=</span> <span class="token string">"right"</span>
</code></pre> 
<p>对齐填充设置为结束符的token (eos_token_id)</p> 
<pre><code class="prism language-python">tokenizer<span class="token punctuation">.</span>pad_token_id <span class="token operator">=</span> <span class="token number">2</span>
</code></pre> 
<h4><a id="3_172"></a>3）定义数据处理函数</h4> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">process_func</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span>
    MAX_LENGTH <span class="token operator">=</span> <span class="token number">400</span>
    input_ids<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    instruction <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"Human: "</span> <span class="token operator">+</span> example<span class="token punctuation">[</span><span class="token string">"instruction"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> example<span class="token punctuation">[</span><span class="token string">"input"</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"\n\nAssistant: "</span><span class="token punctuation">,</span> add_special_tokens<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    response <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>example<span class="token punctuation">[</span><span class="token string">"output"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> add_special_tokens<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    input_ids <span class="token operator">=</span> instruction<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>tokenizer<span class="token punctuation">.</span>eos_token_id<span class="token punctuation">]</span>
    attention_mask <span class="token operator">=</span> instruction<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
    labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>instruction<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>tokenizer<span class="token punctuation">.</span>eos_token_id<span class="token punctuation">]</span>
    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span> <span class="token operator">&gt;</span> MAX_LENGTH<span class="token punctuation">:</span>
        input_ids <span class="token operator">=</span> input_ids<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
        attention_mask <span class="token operator">=</span> attention_mask<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
        labels <span class="token operator">=</span> labels<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
    <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"input_ids"</span><span class="token punctuation">:</span> input_ids<span class="token punctuation">,</span>
        <span class="token string">"attention_mask"</span><span class="token punctuation">:</span> attention_mask<span class="token punctuation">,</span>
        <span class="token string">"labels"</span><span class="token punctuation">:</span> labels
    <span class="token punctuation">}</span>
</code></pre> 
<h4><a id="4_194"></a>4）对数据进行预处理</h4> 
<pre><code class="prism language-python">tokenized_ds <span class="token operator">=</span> ds<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>process_func<span class="token punctuation">,</span> remove_columns<span class="token operator">=</span>ds<span class="token punctuation">.</span>column_names<span class="token punctuation">)</span>
tokenized_ds
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">Dataset<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>
    features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">,</span> <span class="token string">'attention_mask'</span><span class="token punctuation">,</span> <span class="token string">'labels'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    num_rows<span class="token punctuation">:</span> <span class="token number">26858</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="5inpu_ids_210"></a>5）打印查看inpu_ids</h4> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>tokenized_ds<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python"><span class="token punctuation">[</span><span class="token number">12968</span><span class="token punctuation">,</span> <span class="token number">29901</span><span class="token punctuation">,</span> <span class="token number">29871</span><span class="token punctuation">,</span> <span class="token number">30982</span><span class="token punctuation">,</span> <span class="token number">31695</span><span class="token punctuation">,</span> <span class="token number">31863</span><span class="token punctuation">,</span> <span class="token number">31577</span><span class="token punctuation">,</span> <span class="token number">30210</span><span class="token punctuation">,</span> <span class="token number">30457</span><span class="token punctuation">,</span> <span class="token number">30502</span><span class="token punctuation">,</span> <span class="token number">31302</span><span class="token punctuation">,</span> <span class="token number">30858</span><span class="token punctuation">,</span> <span class="token number">30267</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">,</span> <span class="token number">7900</span><span class="token punctuation">,</span> <span class="token number">22137</span><span class="token punctuation">,</span> <span class="token number">29901</span><span class="token punctuation">,</span> <span class="token number">29871</span><span class="token punctuation">,</span> <span class="token number">29871</span><span class="token punctuation">,</span> <span class="token number">30651</span><span class="token punctuation">,</span> <span class="token number">30557</span><span class="token punctuation">,</span> <span class="token number">30392</span><span class="token punctuation">,</span> <span class="token number">30982</span><span class="token punctuation">,</span> <span class="token number">31695</span><span class="token punctuation">,</span> <span class="token number">31863</span><span class="token punctuation">,</span> <span class="token number">31577</span><span class="token punctuation">,</span> <span class="token number">30210</span><span class="token punctuation">,</span> <span class="token number">30457</span><span class="token punctuation">,</span> <span class="token number">30502</span><span class="token punctuation">,</span> <span class="token number">31302</span><span class="token punctuation">,</span> <span class="token number">30858</span><span class="token punctuation">,</span> <span class="token number">30383</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">,</span> <span class="token number">29896</span><span class="token punctuation">,</span> <span class="token number">29889</span><span class="token punctuation">,</span> <span class="token number">29871</span><span class="token punctuation">,</span> <span class="token number">30982</span><span class="token punctuation">,</span> <span class="token number">31695</span><span class="token punctuation">,</span> <span class="token number">31687</span><span class="token punctuation">,</span> <span class="token number">30988</span><span class="token punctuation">,</span> <span class="token number">31704</span><span class="token punctuation">,</span> <span class="token number">30846</span><span class="token punctuation">,</span> <span class="token number">30267</span><span class="token punctuation">,</span> <span class="token number">31951</span><span class="token punctuation">,</span> <span class="token number">30408</span><span class="token punctuation">,</span> <span class="token number">232</span><span class="token punctuation">,</span> <span class="token number">132</span><span class="token punctuation">,</span> <span class="token number">157</span><span class="token punctuation">,</span> <span class="token number">236</span><span class="token punctuation">,</span> <span class="token number">131</span><span class="token punctuation">,</span> <span class="token number">133</span><span class="token punctuation">,</span> <span class="token number">30948</span><span class="token punctuation">,</span> <span class="token number">30210</span><span class="token punctuation">,</span> <span class="token number">31687</span><span class="token punctuation">,</span> <span class="token number">30988</span><span class="token punctuation">,</span> <span class="token number">31894</span><span class="token punctuation">,</span> <span class="token number">30846</span><span class="token punctuation">,</span> <span class="token number">30214</span><span class="token punctuation">,</span> <span class="token number">30847</span><span class="token punctuation">,</span> <span class="token number">233</span><span class="token punctuation">,</span> <span class="token number">152</span><span class="token punctuation">,</span> <span class="token number">166</span><span class="token punctuation">,</span> <span class="token number">233</span><span class="token punctuation">,</span> <span class="token number">176</span><span class="token punctuation">,</span> <span class="token number">168</span><span class="token punctuation">,</span> <span class="token number">30330</span><span class="token punctuation">,</span> <span class="token number">235</span><span class="token punctuation">,</span> <span class="token number">186</span><span class="token punctuation">,</span> <span class="token number">148</span><span class="token punctuation">,</span> <span class="token number">233</span><span class="token punctuation">,</span> <span class="token number">176</span><span class="token punctuation">,</span> <span class="token number">168</span><span class="token punctuation">,</span> <span class="token number">31391</span><span class="token punctuation">,</span> <span class="token number">233</span><span class="token punctuation">,</span> <span class="token number">187</span><span class="token punctuation">,</span> <span class="token number">187</span><span class="token punctuation">,</span> <span class="token number">233</span><span class="token punctuation">,</span> <span class="token number">182</span><span class="token punctuation">,</span> <span class="token number">182</span><span class="token punctuation">,</span> <span class="token number">30214</span><span class="token punctuation">,</span> <span class="token number">30815</span><span class="token punctuation">,</span> <span class="token number">231</span><span class="token punctuation">,</span> <span class="token number">194</span><span class="token punctuation">,</span> <span class="token number">134</span><span class="token punctuation">,</span> <span class="token number">31174</span><span class="token punctuation">,</span> <span class="token number">30869</span><span class="token punctuation">,</span> <span class="token number">235</span><span class="token punctuation">,</span> <span class="token number">164</span><span class="token punctuation">,</span> <span class="token number">131</span><span class="token punctuation">,</span> <span class="token number">31624</span><span class="token punctuation">,</span> <span class="token number">31863</span><span class="token punctuation">,</span> <span class="token number">31577</span><span class="token punctuation">,</span> <span class="token number">30214</span><span class="token punctuation">,</span> <span class="token number">232</span><span class="token punctuation">,</span> <span class="token number">165</span><span class="token punctuation">,</span> <span class="token number">161</span><span class="token punctuation">,</span> <span class="token number">232</span><span class="token punctuation">,</span> <span class="token number">191</span><span class="token punctuation">,</span> <span class="token number">189</span><span class="token punctuation">,</span> <span class="token number">235</span><span class="token punctuation">,</span> <span class="token number">133</span><span class="token punctuation">,</span> <span class="token number">143</span><span class="token punctuation">,</span> <span class="token number">235</span><span class="token punctuation">,</span> <span class="token number">133</span><span class="token punctuation">,</span> <span class="token number">140</span><span class="token punctuation">,</span> <span class="token number">31074</span><span class="token punctuation">,</span> <span class="token number">31180</span><span class="token punctuation">,</span> <span class="token number">30214</span><span class="token punctuation">,</span> <span class="token number">31666</span><span class="token punctuation">,</span> <span class="token number">30417</span><span class="token punctuation">,</span> <span class="token number">31931</span><span class="token punctuation">,</span> <span class="token number">30909</span><span class="token punctuation">,</span> <span class="token number">232</span><span class="token punctuation">,</span> <span class="token number">138</span><span class="token punctuation">,</span> <span class="token number">146</span><span class="token punctuation">,</span> <span class="token number">31022</span><span class="token punctuation">,</span> <span class="token number">30988</span><span class="token punctuation">,</span> <span class="token number">30908</span><span class="token punctuation">,</span> <span class="token number">30267</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">,</span> <span class="token number">29906</span><span class="token punctuation">,</span> <span class="token number">29889</span><span class="token punctuation">,</span> <span class="token number">29871</span><span class="token punctuation">,</span> <span class="token number">232</span><span class="token punctuation">,</span> <span class="token number">160</span><span class="token punctuation">,</span> <span class="token number">138</span><span class="token punctuation">,</span> <span class="token number">235</span><span class="token punctuation">,</span> <span class="token number">164</span><span class="token punctuation">,</span> <span class="token number">164</span><span class="token punctuation">,</span> <span class="token number">236</span><span class="token punctuation">,</span> <span class="token number">168</span><span class="token punctuation">,</span> <span class="token number">177</span><span class="token punctuation">,</span> <span class="token number">31855</span><span class="token punctuation">,</span> <span class="token number">30267</span><span class="token punctuation">,</span> <span class="token number">31951</span><span class="token punctuation">,</span> <span class="token number">30408</span><span class="token punctuation">,</span> <span class="token number">31855</span><span class="token punctuation">,</span> <span class="token number">30406</span><span class="token punctuation">,</span> <span class="token number">30374</span><span class="token punctuation">,</span> <span class="token number">236</span><span class="token punctuation">,</span> <span class="token number">181</span><span class="token punctuation">,</span> <span class="token number">159</span><span class="token punctuation">,</span> <span class="token number">30210</span><span class="token punctuation">,</span> <span class="token number">235</span><span class="token punctuation">,</span> <span class="token number">151</span><span class="token punctuation">,</span> <span class="token number">175</span><span class="token punctuation">,</span> <span class="token number">31854</span><span class="token punctuation">,</span> <span class="token number">30330</span><span class="token punctuation">,</span> <span class="token number">30716</span><span class="token punctuation">,</span> <span class="token number">30801</span><span class="token punctuation">,</span> <span class="token number">30330</span><span class="token punctuation">,</span> <span class="token number">30753</span><span class="token punctuation">,</span> <span class="token number">31112</span><span class="token punctuation">,</span> <span class="token number">30834</span><span class="token punctuation">,</span> <span class="token number">30503</span><span class="token punctuation">,</span> <span class="token number">235</span><span class="token punctuation">,</span> <span class="token number">135</span><span class="token punctuation">,</span> <span class="token number">133</span><span class="token punctuation">,</span> <span class="token number">235</span><span class="token punctuation">,</span> <span class="token number">133</span><span class="token punctuation">,</span> <span class="token number">173</span><span class="token punctuation">,</span> <span class="token number">232</span><span class="token punctuation">,</span> <span class="token number">147</span><span class="token punctuation">,</span> <span class="token number">174</span><span class="token punctuation">,</span> <span class="token number">31180</span><span class="token punctuation">,</span> <span class="token number">231</span><span class="token punctuation">,</span> <span class="token number">192</span><span class="token punctuation">,</span> <span class="token number">145</span><span class="token punctuation">,</span> <span class="token number">30210</span><span class="token punctuation">,</span> <span class="token number">235</span><span class="token punctuation">,</span> <span class="token number">158</span><span class="token punctuation">,</span> <span class="token number">142</span><span class="token punctuation">,</span> <span class="token number">30868</span><span class="token punctuation">,</span> <span class="token number">235</span><span class="token punctuation">,</span> <span class="token number">183</span><span class="token punctuation">,</span> <span class="token number">171</span><span class="token punctuation">,</span> <span class="token number">31855</span><span class="token punctuation">,</span> <span class="token number">30834</span><span class="token punctuation">,</span> <span class="token number">30214</span><span class="token punctuation">,</span> <span class="token number">236</span><span class="token punctuation">,</span> <span class="token number">132</span><span class="token punctuation">,</span> <span class="token number">194</span><span class="token punctuation">,</span> <span class="token number">232</span><span class="token punctuation">,</span> <span class="token number">136</span><span class="token punctuation">,</span> <span class="token number">144</span><span class="token punctuation">,</span> <span class="token number">30528</span><span class="token punctuation">,</span> <span class="token number">234</span><span class="token punctuation">,</span> <span class="token number">182</span><span class="token punctuation">,</span> <span class="token number">153</span><span class="token punctuation">,</span> <span class="token number">30330</span><span class="token punctuation">,</span> <span class="token number">30528</span><span class="token punctuation">,</span> <span class="token number">235</span><span class="token punctuation">,</span> <span class="token number">135</span><span class="token punctuation">,</span> <span class="token number">133</span><span class="token punctuation">,</span> <span class="token number">235</span><span class="token punctuation">,</span> <span class="token number">133</span><span class="token punctuation">,</span> <span class="token number">173</span><span class="token punctuation">,</span> <span class="token number">30503</span><span class="token punctuation">,</span> <span class="token number">30666</span><span class="token punctuation">,</span> <span class="token number">31041</span><span class="token punctuation">,</span> <span class="token number">31855</span><span class="token punctuation">,</span> <span class="token number">31399</span><span class="token punctuation">,</span> <span class="token number">30214</span><span class="token punctuation">,</span> <span class="token number">30651</span><span class="token punctuation">,</span> <span class="token number">30982</span><span class="token punctuation">,</span> <span class="token number">31695</span><span class="token punctuation">,</span> <span class="token number">31863</span><span class="token punctuation">,</span> <span class="token number">31577</span><span class="token punctuation">,</span> <span class="token number">30210</span><span class="token punctuation">,</span> <span class="token number">236</span><span class="token punctuation">,</span> <span class="token number">168</span><span class="token punctuation">,</span> <span class="token number">177</span><span class="token punctuation">,</span> <span class="token number">31855</span><span class="token punctuation">,</span> <span class="token number">231</span><span class="token punctuation">,</span> <span class="token number">188</span><span class="token punctuation">,</span> <span class="token number">163</span><span class="token punctuation">,</span> <span class="token number">233</span><span class="token punctuation">,</span> <span class="token number">134</span><span class="token punctuation">,</span> <span class="token number">178</span><span class="token punctuation">,</span> <span class="token number">30267</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">,</span> <span class="token number">29941</span><span class="token punctuation">,</span> <span class="token number">29889</span><span class="token punctuation">,</span> <span class="token number">29871</span><span class="token punctuation">,</span> <span class="token number">234</span><span class="token punctuation">,</span> <span class="token number">160</span><span class="token punctuation">,</span> <span class="token number">164</span><span class="token punctuation">,</span> <span class="token number">234</span><span class="token punctuation">,</span> <span class="token number">159</span><span class="token punctuation">,</span> <span class="token number">163</span><span class="token punctuation">,</span> <span class="token number">232</span><span class="token punctuation">,</span> <span class="token number">136</span><span class="token punctuation">,</span> <span class="token number">136</span><span class="token punctuation">,</span> <span class="token number">31722</span><span class="token punctuation">,</span> <span class="token number">30267</span><span class="token punctuation">,</span> <span class="token number">234</span><span class="token punctuation">,</span> <span class="token number">160</span><span class="token punctuation">,</span> <span class="token number">164</span><span class="token punctuation">,</span> <span class="token number">234</span><span class="token punctuation">,</span> <span class="token number">159</span><span class="token punctuation">,</span> <span class="token number">163</span><span class="token punctuation">,</span> <span class="token number">30783</span><span class="token punctuation">,</span> <span class="token number">30313</span><span class="token punctuation">,</span> <span class="token number">30988</span><span class="token punctuation">,</span> <span class="token number">31863</span><span class="token punctuation">,</span> <span class="token number">31577</span><span class="token punctuation">,</span> <span class="token number">235</span><span class="token punctuation">,</span> <span class="token number">138</span><span class="token punctuation">,</span> <span class="token number">182</span><span class="token punctuation">,</span> <span class="token number">31057</span><span class="token punctuation">,</span> <span class="token number">30908</span><span class="token punctuation">,</span> <span class="token number">30698</span><span class="token punctuation">,</span> <span class="token number">30214</span><span class="token punctuation">,</span> <span class="token number">30494</span><span class="token punctuation">,</span> <span class="token number">30470</span><span class="token punctuation">,</span> <span class="token number">30313</span><span class="token punctuation">,</span> <span class="token number">31951</span><span class="token punctuation">,</span> <span class="token number">30408</span><span class="token punctuation">,</span> <span class="token number">31370</span><span class="token punctuation">,</span> <span class="token number">30982</span><span class="token punctuation">,</span> <span class="token number">235</span><span class="token punctuation">,</span> <span class="token number">178</span><span class="token punctuation">,</span> <span class="token number">132</span><span class="token punctuation">,</span> <span class="token number">29871</span><span class="token punctuation">,</span> <span class="token number">29955</span><span class="token punctuation">,</span> <span class="token number">29899</span><span class="token punctuation">,</span> <span class="token number">29947</span><span class="token punctuation">,</span> <span class="token number">29871</span><span class="token punctuation">,</span> <span class="token number">30446</span><span class="token punctuation">,</span> <span class="token number">30594</span><span class="token punctuation">,</span> <span class="token number">30210</span><span class="token punctuation">,</span> <span class="token number">234</span><span class="token punctuation">,</span> <span class="token number">160</span><span class="token punctuation">,</span> <span class="token number">164</span><span class="token punctuation">,</span> <span class="token number">234</span><span class="token punctuation">,</span> <span class="token number">159</span><span class="token punctuation">,</span> <span class="token number">163</span><span class="token punctuation">,</span> <span class="token number">30267</span><span class="token punctuation">,</span> <span class="token number">31400</span><span class="token punctuation">,</span> <span class="token number">31076</span><span class="token punctuation">,</span> <span class="token number">30210</span><span class="token punctuation">,</span> <span class="token number">234</span><span class="token punctuation">,</span> <span class="token number">160</span><span class="token punctuation">,</span> <span class="token number">164</span><span class="token punctuation">,</span> <span class="token number">234</span><span class="token punctuation">,</span> <span class="token number">159</span><span class="token punctuation">,</span> <span class="token number">163</span><span class="token punctuation">,</span> <span class="token number">30417</span><span class="token punctuation">,</span> <span class="token number">31931</span><span class="token punctuation">,</span> <span class="token number">30909</span><span class="token punctuation">,</span> <span class="token number">232</span><span class="token punctuation">,</span> <span class="token number">138</span><span class="token punctuation">,</span> <span class="token number">146</span><span class="token punctuation">,</span> <span class="token number">235</span><span class="token punctuation">,</span> <span class="token number">192</span><span class="token punctuation">,</span> <span class="token number">190</span><span class="token punctuation">,</span> <span class="token number">232</span><span class="token punctuation">,</span> <span class="token number">145</span><span class="token punctuation">,</span> <span class="token number">142</span><span class="token punctuation">,</span> <span class="token number">31074</span><span class="token punctuation">,</span> <span class="token number">30214</span><span class="token punctuation">,</span> <span class="token number">231</span><span class="token punctuation">,</span> <span class="token number">194</span><span class="token punctuation">,</span> <span class="token number">134</span><span class="token punctuation">,</span> <span class="token number">31174</span><span class="token punctuation">,</span> <span class="token number">31687</span><span class="token punctuation">,</span> <span class="token number">30988</span><span class="token punctuation">,</span> <span class="token number">233</span><span class="token punctuation">,</span> <span class="token number">132</span><span class="token punctuation">,</span> <span class="token number">165</span><span class="token punctuation">,</span> <span class="token number">31810</span><span class="token punctuation">,</span> <span class="token number">30214</span><span class="token punctuation">,</span> <span class="token number">31666</span><span class="token punctuation">,</span> <span class="token number">31302</span><span class="token punctuation">,</span> <span class="token number">30528</span><span class="token punctuation">,</span> <span class="token number">31368</span><span class="token punctuation">,</span> <span class="token number">31474</span><span class="token punctuation">,</span> <span class="token number">31074</span><span class="token punctuation">,</span> <span class="token number">30503</span><span class="token punctuation">,</span> <span class="token number">31410</span><span class="token punctuation">,</span> <span class="token number">232</span><span class="token punctuation">,</span> <span class="token number">194</span><span class="token punctuation">,</span> <span class="token number">137</span><span class="token punctuation">,</span> <span class="token number">31074</span><span class="token punctuation">,</span> <span class="token number">30267</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span>
</code></pre> 
<h4><a id="6_222"></a>6）检查数据（是否包含结束符）</h4> 
<pre><code class="prism language-python">tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">filter</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">,</span> tokenized_ds<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python"><span class="token string">'4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。&lt;/s&gt;'</span>
</code></pre> 
<h3><a id="4__234"></a>步骤4 创建模型</h3> 
<p>然后，我们实例化一个预训练模型，这个模型将作为微调的基础。对于大型模型，我们可能还需要进行一些特定的配置，以适应可用的计算资源。（这里设置为半精度torch_dtype=torch.half,）</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"/root/autodl-tmp/Llama-2-7b-ms"</span><span class="token punctuation">,</span> 
                                             low_cpu_mem_usage<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                             torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>half<span class="token punctuation">,</span>
                                             device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">)</span>
</code></pre> 
<p>精度查看</p> 
<pre><code class="prism language-python">model<span class="token punctuation">.</span>dtype
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>float16
</code></pre> 
<p><code>下面2个部分是LoRA相关的配置。</code></p> 
<h4><a id="1PEFT_1__258"></a>1、PEFT 步骤1 配置文件</h4> 
<p>在使用PEFT进行微调时，我们首先需要创建一个配置文件，该文件定义了微调过程中的各种设置，如学习率调度、优化器选择等。<br> 提前安装peft：pip install peft</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> peft <span class="token keyword">import</span> LoraConfig<span class="token punctuation">,</span> TaskType<span class="token punctuation">,</span> get_peft_model

config <span class="token operator">=</span> LoraConfig<span class="token punctuation">(</span>task_type<span class="token operator">=</span>TaskType<span class="token punctuation">.</span>CAUSAL_LM<span class="token punctuation">,</span><span class="token punctuation">)</span>
config
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">LoraConfig<span class="token punctuation">(</span>peft_type<span class="token operator">=</span><span class="token operator">&lt;</span>PeftType<span class="token punctuation">.</span>LORA<span class="token punctuation">:</span> <span class="token string">'LORA'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> auto_mapping<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> base_model_name_or_path<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> revision<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> task_type<span class="token operator">=</span><span class="token operator">&lt;</span>TaskType<span class="token punctuation">.</span>CAUSAL_LM<span class="token punctuation">:</span> <span class="token string">'CAUSAL_LM'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> inference_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> r<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> target_modules<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> lora_alpha<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> lora_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> fan_in_fan_out<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">,</span> use_rslora<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> modules_to_save<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> init_lora_weights<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> layers_to_transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> layers_pattern<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> rank_pattern<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">,</span> alpha_pattern<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">,</span> megatron_config<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> megatron_core<span class="token operator">=</span><span class="token string">'megatron.core'</span><span class="token punctuation">,</span> loftq_config<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">,</span> use_dora<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> layer_replication<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>

</code></pre> 
<h4><a id="2PEFT_2__276"></a>2、PEFT 步骤2 创建模型</h4> 
<p>接下来，我们使用PEFT和预训练模型来创建一个微调模型。这个模型将包含原始的预训练模型以及由PEFT引入的低秩参数。</p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> get_peft_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> config<span class="token punctuation">)</span>
config
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">LoraConfig<span class="token punctuation">(</span>peft_type<span class="token operator">=</span><span class="token operator">&lt;</span>PeftType<span class="token punctuation">.</span>LORA<span class="token punctuation">:</span> <span class="token string">'LORA'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> auto_mapping<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> base_model_name_or_path<span class="token operator">=</span><span class="token string">'/root/autodl-tmp/Llama-2-7b-ms'</span><span class="token punctuation">,</span> revision<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> task_type<span class="token operator">=</span><span class="token operator">&lt;</span>TaskType<span class="token punctuation">.</span>CAUSAL_LM<span class="token punctuation">:</span> <span class="token string">'CAUSAL_LM'</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> inference_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> r<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> target_modules<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">'q_proj'</span><span class="token punctuation">,</span> <span class="token string">'v_proj'</span><span class="token punctuation">}</span><span class="token punctuation">,</span> lora_alpha<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> lora_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> fan_in_fan_out<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">,</span> use_rslora<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> modules_to_save<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> init_lora_weights<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> layers_to_transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> layers_pattern<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> rank_pattern<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">,</span> alpha_pattern<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">,</span> megatron_config<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> megatron_core<span class="token operator">=</span><span class="token string">'megatron.core'</span><span class="token punctuation">,</span> loftq_config<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span><span class="token punctuation">,</span> use_dora<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> layer_replication<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>查看模型中可训练参数的数量</strong></p> 
<pre><code class="prism language-python">model<span class="token punctuation">.</span>print_trainable_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#打印出模型中可训练参数的数量</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">trainable params<span class="token punctuation">:</span> <span class="token number">4</span><span class="token punctuation">,</span><span class="token number">194</span><span class="token punctuation">,</span><span class="token number">304</span> <span class="token operator">|</span><span class="token operator">|</span> <span class="token builtin">all</span> params<span class="token punctuation">:</span> <span class="token number">6</span><span class="token punctuation">,</span><span class="token number">742</span><span class="token punctuation">,</span><span class="token number">609</span><span class="token punctuation">,</span><span class="token number">920</span> <span class="token operator">|</span><span class="token operator">|</span> trainable<span class="token operator">%</span><span class="token punctuation">:</span> <span class="token number">0.06220594176090199</span>
</code></pre> 
<p><strong>查看模型参数，查看LoRA层添加到哪</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> param<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>embed_tokens<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">32000</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">5</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">6</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">7</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">11</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">12</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">13</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">14</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">16</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">17</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">18</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">19</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">20</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">21</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">22</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">23</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">24</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">25</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">26</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">27</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">28</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">28</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">28</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">28</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">28</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">28</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">28</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">28</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">28</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">28</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">28</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">28</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">28</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">29</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">29</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">29</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">29</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">29</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">29</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">29</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">29</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">29</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">29</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">29</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">29</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">29</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">30</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">30</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">30</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">30</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">30</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">30</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">30</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">30</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">30</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">30</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">30</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">30</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">30</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">31</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">31</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">31</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>q_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">31</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>k_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">31</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>base_layer<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">31</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">31</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>v_proj<span class="token punctuation">.</span>lora_B<span class="token punctuation">.</span>default<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">31</span><span class="token punctuation">.</span>self_attn<span class="token punctuation">.</span>o_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">31</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">31</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">11008</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">31</span><span class="token punctuation">.</span>mlp<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">11008</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">31</span><span class="token punctuation">.</span>input_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>layers<span class="token punctuation">.</span><span class="token number">31</span><span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>norm<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>lm_head<span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">32000</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float16
</code></pre> 
<h3><a id="5__731"></a>步骤5 配置训练参数</h3> 
<p>在这一步，我们定义训练参数，这些参数包括输出目录、学习率、权重衰减、梯度累积步数、训练周期数等。这些参数将被用来配置训练过程。(设置adam_epsilon=1e-4避免精度溢出）</p> 
<pre><code class="prism language-python"><span class="token comment">##   adam_epsilon=1e-4</span>
args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span><span class="token string">"/root/autodl-tmp/llama2output"</span><span class="token punctuation">,</span> <span class="token comment"># 指定模型训练结果的输出目录</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token comment"># 设置每个设备（如GPU）在训练过程中的批次大小为2，越大需要资源也越多</span>
    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token comment"># 指定梯度累积步数为8，即将多个批次的梯度累加后再进行一次参数更新</span>
    logging_steps<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token comment"># 每10个步骤记录一次日志信息</span>
    num_train_epochs<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token comment"># 指定训练的总轮数为1</span>
    adam_epsilon<span class="token operator">=</span><span class="token number">1e-4</span> <span class="token comment">#避免精度溢出</span>
<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="6__748"></a>步骤6 创建训练器</h3> 
<p>最后，我们创建一个训练器实例，它封装了训练循环。训练器将负责运行训练过程，并根据我们之前定义的参数进行优化。</p> 
<pre><code class="prism language-python">trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
    model<span class="token operator">=</span>model<span class="token punctuation">,</span>
    args<span class="token operator">=</span>args<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>tokenized_ds<span class="token punctuation">,</span>
    data_collator<span class="token operator">=</span>DataCollatorForSeq2Seq<span class="token punctuation">(</span>tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="7__761"></a>步骤7 模型训练</h3> 
<p>通过调用训练器的<code>train()</code>方法，我们启动模型的训练过程。这将根据之前定义的参数执行模型的训练。</p> 
<pre><code class="prism language-python">trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出：<br> <img src="https://images2.imgbox.com/3f/ab/qRaa1Hzj_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="8__770"></a>步骤8 模型推理</h3> 
<p>训练完成后，我们可以使用训练好的模型进行推理。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

pipe <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"text-generation"</span><span class="token punctuation">,</span> model<span class="token operator">=</span>model<span class="token punctuation">,</span> tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">)</span>

ipt <span class="token operator">=</span> <span class="token string">"Human: {}\n{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token string">"如何写简历？"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"\n\nAssistant: "</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>pipe<span class="token punctuation">(</span>ipt<span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> do_sample<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python"><span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span><span class="token string">'generated_text'</span><span class="token punctuation">:</span> <span class="token string">'Human: 如何写简历？\n\nAssistant:  写简历的方法主要有两个步骤：\n\n1. 首先，您需要确定您想要投递的职位和企业。根据您的职业目标和职位要求，准备您想要展示的职业经历和技能。\n\n2. 然后，根据您的职业目标和职位要求，您需要撰写简历的内容。您的简历应该包含您的个人信息、职业经历、技能和职能、成就和荣誉、职业目标和职业信念。您的简历应该简明'</span><span class="token punctuation">}</span><span class="token punctuation">]</span>
</code></pre> 
<h2><a id="Llama2__790"></a>五、Llama2 调试过程问题整理</h2> 
<h3><a id="1MAX_LENGTH_791"></a>1、MAX_LENGTH设置：</h3> 
<p>对数据集预处理时，为了批处理，我们会设置一个MAX_LENGTH； llama对中文支持不是很好，一个中文会用4个token表示，所以MAX_LENGTH要条长一点,原来256现在改为400，否则会缺很多数据</p> 
<h3><a id="2_794"></a>2、对齐设置</h3> 
<p>对齐设置：tokenizer.padding_side = “right”<br> 需要调整模型向右补齐（默认向左补齐），<strong>否则会导致训练时，无法收敛；</strong></p> 
<h3><a id="3pad_token_id_798"></a>3、设置pad_token_id</h3> 
<p>设置pad_token_id为eos_token_id（即Llama批处理时，根据MAX_LENGTH对齐不足时，补充结束符进行对齐）<br> tokenizer.pad_token_id = 2 #2代表结束符的token，可以查看tokenizer(tokenizer.eos_token)，<strong>不设置也会导致无法收敛</strong></p> 
<h3><a id="4_802"></a>4、结束符设置</h3> 
<p>分词器处理时，不添加结束符；因为这里会添加不成功，可以对比查看tokenizer(tokenizer.eos_token) 和 tokenizer(“abc” + tokenizer.eos_token) 会发现没有添加成功，因此就需要手工添加结束符；手动对input_id，attention_mask、labels 设置结束符 tokenizer.eos_token_id （代表）；<br> <strong>不处理会导致回答问题时，无法识别停止符，一直不会结束</strong><br> 修改前</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">process_func</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span>
    MAX_LENGTH <span class="token operator">=</span> <span class="token number">256</span>
    input_ids<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    instruction <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"Human: "</span> <span class="token operator">+</span> example<span class="token punctuation">[</span><span class="token string">"instruction"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> example<span class="token punctuation">[</span><span class="token string">"input"</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"\n\nAssistant: "</span><span class="token punctuation">)</span>
    response <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>example<span class="token punctuation">[</span><span class="token string">"output"</span><span class="token punctuation">]</span> <span class="token operator">+</span> tokenizer<span class="token punctuation">.</span>eos_token<span class="token punctuation">)</span>
    input_ids <span class="token operator">=</span> instruction<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span>
    attention_mask <span class="token operator">=</span> instruction<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span>
    labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>instruction<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span>
    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span> <span class="token operator">&gt;</span> MAX_LENGTH<span class="token punctuation">:</span>
        input_ids <span class="token operator">=</span> input_ids<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
        attention_mask <span class="token operator">=</span> attention_mask<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
        labels <span class="token operator">=</span> labels<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
    <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"input_ids"</span><span class="token punctuation">:</span> input_ids<span class="token punctuation">,</span>
        <span class="token string">"attention_mask"</span><span class="token punctuation">:</span> attention_mask<span class="token punctuation">,</span>
        <span class="token string">"labels"</span><span class="token punctuation">:</span> labels
    <span class="token punctuation">}</span>
</code></pre> 
<p>修改后</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">process_func</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span>
    MAX_LENGTH <span class="token operator">=</span> <span class="token number">400</span>
    input_ids<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    instruction <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"Human: "</span> <span class="token operator">+</span> example<span class="token punctuation">[</span><span class="token string">"instruction"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> example<span class="token punctuation">[</span><span class="token string">"input"</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"\n\nAssistant: "</span><span class="token punctuation">,</span> add_special_tokens<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    response <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>example<span class="token punctuation">[</span><span class="token string">"output"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> add_special_tokens<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    input_ids <span class="token operator">=</span> instruction<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>tokenizer<span class="token punctuation">.</span>eos_token_id<span class="token punctuation">]</span>
    attention_mask <span class="token operator">=</span> instruction<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
    labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>instruction<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>tokenizer<span class="token punctuation">.</span>eos_token_id<span class="token punctuation">]</span>
    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span> <span class="token operator">&gt;</span> MAX_LENGTH<span class="token punctuation">:</span>
        input_ids <span class="token operator">=</span> input_ids<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
        attention_mask <span class="token operator">=</span> attention_mask<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
        labels <span class="token operator">=</span> labels<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
    <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"input_ids"</span><span class="token punctuation">:</span> input_ids<span class="token punctuation">,</span>
        <span class="token string">"attention_mask"</span><span class="token punctuation">:</span> attention_mask<span class="token punctuation">,</span>
        <span class="token string">"labels"</span><span class="token punctuation">:</span> labels
    <span class="token punctuation">}</span>
</code></pre> 
<h3><a id="5_adam_epsilon_849"></a>5、设置 adam_epsilon</h3> 
<p>训练参数设置时，设置 adam_epsilon=1e-4 （默认全精度不需要设置，它的值为1e-8；采用半精度时，如果不设置，会出现精度溢出，导致adam_epsilon被四舍五入成0）；<strong>如果不设置，会导致训练时无法正常收敛</strong></p> 
<pre><code class="prism language-python"><span class="token comment">#默认全精度</span>
<span class="token comment">#import torch</span>
<span class="token comment">#torch.tensor(1e-8)</span>
<span class="token comment">#输出：tensor(1.0000e-08)</span>
<span class="token comment">#使用半精度后：torch.tensor(1e-8).half()</span>
<span class="token comment">#输出：tensor(0., dtype=torch.float16) #此时会被四舍五入成0</span>
args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span><span class="token string">"/root/autodl-tmp/llama2output"</span><span class="token punctuation">,</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
    logging_steps<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>
    num_train_epochs<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    adam_epsilon<span class="token operator">=</span><span class="token number">1e-4</span>
<span class="token punctuation">)</span>
</code></pre> 
<h2><a id="_869"></a>总结</h2> 
<p>本文详细阐述了如何通过LoRA技术微调Llama2大型预训练语言模型，使其更好地适应特定的自然语言处理任务。我们首先介绍了Llama2模型及其在AI领域的重要性，然后讨论了微调前的准备工作，如数据收集和模型选择。接着，我们逐步解释了微调的实战步骤，包括数据处理、模型创建、参数配置、训练以及推理。</p> 
<p>在调试过程中，我们强调了针对中文数据处理的MAX_LENGTH设置、对齐方式、结束符设置和训练参数优化等关键问题。这些调整确保了模型的有效收敛和性能提升。</p> 
<p>展望未来，随着Llama模型和LoRA技术的不断进步，我们可以期待更多高效、灵活的微调方法出现，特别是对非英语语言的优化将是研究的重点，以支持全球化的NLP应用发展。本文提供的指导和最佳实践，将助力读者为特定应用场景定制高性能的NLP模型，推动学术研究和工业应用的进步。</p> 
<p><img src="https://images2.imgbox.com/d1/47/zu2TmxKf_o.png" alt="在这里插入图片描述"></p> 
<p>🎯🔖更多专栏系列文章：<a href="https://blog.csdn.net/xiaobing259/category_12628007.html?spm=1001.2014.3001.5482"><strong>AIGC-AI大模型探索之路</strong></a></p> 
<blockquote> 
 <p>如果文章内容对您有所触动，别忘了<font color="red"><strong>点赞、⭐关注，收藏</strong></font>！加入我，让我们携手同行AI的探索之旅，一起开启智能时代的大门！</p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f5254c3aad5f717117ecf042a7c8cd95/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【LeetCode LCR 022】【C语言】环形链表 II</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2c3786c117f50ad3e885a46e28e1f047/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Go最新Google BigQuery 创始工程师：大数据已“死”(2)，最新Golang面试题整理</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>