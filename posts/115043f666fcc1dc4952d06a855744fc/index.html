<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>分享Python7个爬虫小案例（附源码） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/115043f666fcc1dc4952d06a855744fc/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="分享Python7个爬虫小案例（附源码）">
  <meta property="og:description" content="在这篇文章中，我们将分享7个Python爬虫的小案例，帮助大家更好地学习和了解Python爬虫的基础知识。以下是每个案例的简介和源代码：
1. 爬取豆瓣电影Top250 这个案例使用BeautifulSoup库爬取豆瓣电影Top250的电影名称、评分和评价人数等信息，并将这些信息保存到CSV文件中。
import requests from bs4 import BeautifulSoup import csv # 请求URL url = &#39;&lt;https://movie.douban.com/top250&gt;&#39; # 请求头部 headers = { &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&#39; } # 解析页面函数 def parse_html(html): soup = BeautifulSoup(html, &#39;lxml&#39;) movie_list = soup.find(&#39;ol&#39;, class_=&#39;grid_view&#39;).find_all(&#39;li&#39;) for movie in movie_list: title = movie.find(&#39;div&#39;, class_=&#39;hd&#39;).find(&#39;span&#39;, class_=&#39;title&#39;).get_text() rating_num = movie.find(&#39;div&#39;, class_=&#39;star&#39;).find(&#39;span&#39;, class_=&#39;rating_num&#39;).get_text() comment_num = movie.find(&#39;div&#39;, class_=&#39;star&#39;).find_all(&#39;span&#39;)[-1].get_text() writer.writerow([title, rating_num, comment_num]) # 保存数据函数 def save_data(): f = open(&#39;douban_movie_top250.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-03-17T14:13:51+08:00">
    <meta property="article:modified_time" content="2023-03-17T14:13:51+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">分享Python7个爬虫小案例（附源码）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>在这篇文章中，我们将分享7个Python爬虫的小案例，帮助大家更好地学习和了解Python爬虫的基础知识。以下是每个案例的简介和源代码：</p> 
<h3>1. 爬取豆瓣电影Top250</h3> 
<p>这个案例使用BeautifulSoup库爬取豆瓣电影Top250的电影名称、评分和评价人数等信息，并将这些信息保存到CSV文件中。</p> 
<pre><code>import requests
from bs4 import BeautifulSoup
import csv

# 请求URL
url = '&lt;https://movie.douban.com/top250&gt;'
# 请求头部
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
}

# 解析页面函数
def parse_html(html):
    soup = BeautifulSoup(html, 'lxml')
    movie_list = soup.find('ol', class_='grid_view').find_all('li')
    for movie in movie_list:
        title = movie.find('div', class_='hd').find('span', class_='title').get_text()
        rating_num = movie.find('div', class_='star').find('span', class_='rating_num').get_text()
        comment_num = movie.find('div', class_='star').find_all('span')[-1].get_text()
        writer.writerow([title, rating_num, comment_num])

# 保存数据函数
def save_data():
    f = open('douban_movie_top250.csv', 'a', newline='', encoding='utf-8-sig')
    global writer
    writer = csv.writer(f)
    writer.writerow(['电影名称', '评分', '评价人数'])
    for i in range(10):
        url = '&lt;https://movie.douban.com/top250?start=&gt;' + str(i*25) + '&amp;filter='
        response = requests.get(url, headers=headers)
        parse_html(response.text)
    f.close()

if __name__ == '__main__':
    save_data()

</code></pre> 
<h3>2. 爬取猫眼电影Top100</h3> 
<p>这个案例使用正则表达式和requests库爬取猫眼电影Top100的电影名称、主演和上映时间等信息，并将这些信息保存到TXT文件中。</p> 
<pre><code>import requests
import re

# 请求URL
url = '&lt;https://maoyan.com/board/4&gt;'
# 请求头部
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
}

# 解析页面函数
def parse_html(html):
    pattern = re.compile('&lt;p class="name"&gt;&lt;a href=".*?" title="(.*?)" data-act="boarditem-click" data-val="{movieId:\\\\d+}"&gt;(.*?)&lt;/a&gt;&lt;/p&gt;.*?&lt;p class="star"&gt;(.*?)&lt;/p&gt;.*?&lt;p class="releasetime"&gt;(.*?)&lt;/p&gt;', re.S)
    items = re.findall(pattern, html)
    for item in items:
        yield {
            '电影名称': item[1],
            '主演': item[2].strip(),
            '上映时间': item[3]
        }

# 保存数据函数
def save_data():
    f = open('maoyan_top100.txt', 'w', encoding='utf-8')
    for i in range(10):
        url = '&lt;https://maoyan.com/board/4?offset=&gt;' + str(i*10)
        response = requests.get(url, headers=headers)
        for item in parse_html(response.text):
            f.write(str(item) + '\\\\n')
    f.close()

if __name__ == '__main__':
    save_data()

</code></pre> 
<h3>3. 爬取全国高校名单</h3> 
<p>这个案例使用正则表达式和requests库爬取全国高校名单，并将这些信息保存到TXT文件中。</p> 
<pre><code>import requests
import re

# 请求URL
url = '&lt;http://www.zuihaodaxue.com/zuihaodaxuepaiming2019.html&gt;'
# 请求头部
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
}

# 解析页面函数
def parse_html(html):
    pattern = re.compile('&lt;tr class="alt"&gt;.*?&lt;td&gt;(.*?)&lt;/td&gt;.*?&lt;td&gt;&lt;div align="left"&gt;.*?&lt;a href="(.*?)" target="_blank"&gt;(.*?)&lt;/a&gt;&lt;/div&gt;&lt;/td&gt;.*?&lt;td&gt;(.*?)&lt;/td&gt;.*?&lt;td&gt;(.*?)&lt;/td&gt;.*?&lt;/tr&gt;', re.S)
    items = re.findall(pattern, html)
    for item in items:
        yield {
            '排名': item[0],
            '学校名称': item[2],
            '省市': item[3],
            '总分': item[4]
        }

# 保存数据函数
def save_data():
    f = open('university_top100.txt', 'w', encoding='utf-8')
    response = requests.get(url, headers=headers)
    for item in parse_html(response.text):
        f.write(str(item) + '\\\\n')
    f.close()

if __name__ == '__main__':
    save_data()

</code></pre> 
<h3>4. 爬取中国天气网城市天气</h3> 
<p>这个案例使用xpath和requests库爬取中国天气网的城市天气，并将这些信息保存到CSV文件中。</p> 
<pre><code>import requests
from lxml import etree
import csv

# 请求URL
url = '&lt;http://www.weather.com.cn/weather1d/101010100.shtml&gt;'
# 请求头部
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
}

# 解析页面函数
def parse_html(html):
    selector = etree.HTML(html)
    city = selector.xpath('//*[@id="around"]/div/div[1]/div[1]/h1/text()')[0]
    temperature = selector.xpath('//*[@id="around"]/div/div[1]/div[1]/p/i/text()')[0]
    weather = selector.xpath('//*[@id="around"]/div/div[1]/div[1]/p/@title')[0]
    wind = selector.xpath('//*[@id="around"]/div/div[1]/div[1]/p/span/text()')[0]
    return city, temperature, weather, wind

# 保存数据函数
def save_data():
    f = open('beijing_weather.csv', 'w', newline='', encoding='utf-8-sig')
    writer = csv.writer(f)
    writer.writerow(['城市', '温度', '天气', '风力'])
    for i in range(10):
        response = requests.get(url, headers=headers)
        city, temperature, weather, wind = parse_html(response.text)
        writer.writerow([city, temperature, weather, wind])
    f.close()

if __name__ == '__main__':
    save_data()

</code></pre> 
<h3>5. 爬取当当网图书信息</h3> 
<p>这个案例使用xpath和requests库爬取当当网图书信息，并将这些信息保存到CSV文件中。</p> 
<pre><code>import requests
from lxml import etree
import csv

# 请求URL
url = '&lt;http://search.dangdang.com/?key=Python&amp;act=input&gt;'
# 请求头部
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
}

# 解析页面函数
def parse_html(html):
    selector = etree.HTML(html)
    book_list = selector.xpath('//*[@id="search_nature_rg"]/ul/li')
    for book in book_list:
        title = book.xpath('a/@title')[0]
        link = book.xpath('a/@href')[0]
        price = book.xpath('p[@class="price"]/span[@class="search_now_price"]/text()')[0]
        author = book.xpath('p[@class="search_book_author"]/span[1]/a/@title')[0]
        publish_date = book.xpath('p[@class="search_book_author"]/span[2]/text()')[0]
        publisher = book.xpath('p[@class="search_book_author"]/span[3]/a/@title')[0]
        yield {
            '书名': title,
            '链接': link,
            '价格': price,
            '作者': author,
            '出版日期': publish_date,
            '出版社': publisher
        }

# 保存数据函数
def save_data():
    f = open('dangdang_books.csv', 'w', newline='', encoding='utf-8-sig')
    writer = csv.writer(f)
    writer.writerow(['书名', '链接', '价格', '作者', '出版日期', '出版社'])
    response = requests.get(url, headers=headers)
    for item in parse_html(response.text):
        writer.writerow(item.values())
    f.close()

if __name__ == '__main__':
    save_data()

</code></pre> 
<h3>6. 爬取糗事百科段子</h3> 
<p>这个案例使用xpath和requests库爬取糗事百科的段子，并将这些信息保存到TXT文件中。</p> 
<pre><code>import requests
from lxml import etree

# 请求URL
url = '&lt;https://www.qiushibaike.com/text/&gt;'
# 请求头部
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
}

# 解析页面函数
def parse_html(html):
    selector = etree.HTML(html)
    content_list = selector.xpath('//div[@class="content"]/span/text()')
    for content in content_list:
        yield content

# 保存数据函数
def save_data():
    f = open('qiushibaike_jokes.txt', 'w', encoding='utf-8')
    for i in range(3):
        url = '&lt;https://www.qiushibaike.com/text/page/&gt;' + str(i+1) + '/'
        response = requests.get(url, headers=headers)
        for content in parse_html(response.text):
            f.write(content + '\\\\n')
    f.close()

if __name__ == '__main__':
    save_data()

</code></pre> 
<h3>7. 爬取新浪微博</h3> 
<p>这个案例使用selenium和requests库爬取新浪微博，并将这些信息保存到TXT文件中。</p> 
<pre><code>import time
from selenium import webdriver
import requests

# 请求URL
url = '&lt;https://weibo.com/&gt;'
# 请求头部
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
}

# 解析页面函数
def parse_html(html):
    print(html)

# 保存数据函数
def save_data():
    f = open('weibo.txt', 'w', encoding='utf-8')
    browser = webdriver.Chrome()
    browser.get(url)
    time.sleep(10)
    browser.find_element_by_name('username').send_keys('username')
    browser.find_element_by_name('password').send_keys('password')
    browser.find_element_by_class_name('W_btn_a').click()
    time.sleep(10)
    response = requests.get(url, headers=headers, cookies=browser.get_cookies())
    parse_html(response.text)
    browser.close()
    f.close()

if __name__ == '__main__':
    save_data()

</code></pre> 
<p>希望这7个小案例能够帮助大家更好地掌握Python爬虫的基础知识！</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9058c071f891cdacda47fa31bb2c56c4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【java】HashMap底层实现原理</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0edce334cd2d4e7470415e1be8fe9f0c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">IntelliJ IDEA安装教程（超详细）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>