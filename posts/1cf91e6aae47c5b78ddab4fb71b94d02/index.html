<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LLama Factory微调模型全流程，与peft库调用训练的adapter - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/1cf91e6aae47c5b78ddab4fb71b94d02/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="LLama Factory微调模型全流程，与peft库调用训练的adapter">
  <meta property="og:description" content="提示：文章写完后，目录可以自动生成，如何生成可参考右边的帮助文档
文章目录 LLama Factory微调流程一、准备微调的数据集和模型二、读取模型和数据进行训练1.使用web ui2.修改官方脚本（推荐） 第三，加载与推理 LLama Factory微调流程 官方GitHub链接
官方数据集说明
官方微调命令
安装LLama factory
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git cd LLaMA-Factory pip install -e &#34;.[torch,metrics]&#34; 总体上你可以把整个流程分为下面几个步骤：
1.准备微调的数据集和模型
2.读取模型和数据进行训练
3.使用命令或者脚本进行微调训练
4.加载微调的训练模型
一、准备微调的数据集和模型 1.下载模型
这里可以使用HuggingFace或者ModelScope提供的方法进行模型下载，这里给出ModelScope的下载方式, 如果要使用Qwen2请看3：
from modelscope import AutoModelForCausalLM, AutoTokenizer, snapshot_download from modelscope import GenerationConfig ##cache_dir表示模型存储的目录 models_dir = &#34;app/models&#34; model_dir = snapshot_download(&#39;qwen/Qwen-7B-Chat&#39;,cache_dir=&#34;app/models&#34;) tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True, cache_dir=models_dir) model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=&#34;cuda:0&#34;,trust_remote_code=True, cache_dir=models_dir).eval() model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True, cache_dir=models_dir) response, history = model.chat(tokenizer, &#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-22T09:01:42+08:00">
    <meta property="article:modified_time" content="2024-07-22T09:01:42+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">LLama Factory微调模型全流程，与peft库调用训练的adapter</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>提示：文章写完后，目录可以自动生成，如何生成可参考右边的帮助文档</p> 
</blockquote> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#LLama_Factory_7" rel="nofollow">LLama Factory微调流程</a></li><li><a href="#_25" rel="nofollow">一、准备微调的数据集和模型</a></li><li><a href="#_108" rel="nofollow">二、读取模型和数据进行训练</a></li><li><ul><li><a href="#1web_ui_109" rel="nofollow">1.使用web ui</a></li><li><a href="#2_117" rel="nofollow">2.修改官方脚本（推荐）</a></li></ul> 
  </li><li><a href="#_175" rel="nofollow">第三，加载与推理</a></li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="LLama_Factory_7"></a>LLama Factory微调流程</h2> 
<p><a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md">官方GitHub链接</a><br> <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md">官方数据集说明</a><br> <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README_zh.md">官方微调命令</a><br> 安装LLama factory</p> 
<pre><code class="prism language-bash"><span class="token function">git</span> clone <span class="token parameter variable">--depth</span> <span class="token number">1</span> https://github.com/hiyouga/LLaMA-Factory.git
<span class="token builtin class-name">cd</span> LLaMA-Factory
pip <span class="token function">install</span> <span class="token parameter variable">-e</span> <span class="token string">".[torch,metrics]"</span>
</code></pre> 
<p>总体上你可以把整个流程分为下面几个步骤：<br> <strong>1.准备微调的数据集和模型</strong><br> <strong>2.读取模型和数据进行训练</strong><br> <strong>3.使用命令或者脚本进行微调训练</strong><br> <strong>4.加载微调的训练模型</strong></p> 
<hr> 
<h2><a id="_25"></a>一、准备微调的数据集和模型</h2> 
<p><strong>1.下载模型</strong><br> 这里可以使用HuggingFace或者ModelScope提供的方法进行模型下载，这里给出ModelScope的下载方式, 如果要使用Qwen2请看3：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> modelscope <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer<span class="token punctuation">,</span> snapshot_download
<span class="token keyword">from</span> modelscope <span class="token keyword">import</span> GenerationConfig
<span class="token comment">##cache_dir表示模型存储的目录</span>
models_dir <span class="token operator">=</span> <span class="token string">"app/models"</span>
model_dir <span class="token operator">=</span> snapshot_download<span class="token punctuation">(</span><span class="token string">'qwen/Qwen-7B-Chat'</span><span class="token punctuation">,</span>cache_dir<span class="token operator">=</span><span class="token string">"app/models"</span><span class="token punctuation">)</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_dir<span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> cache_dir<span class="token operator">=</span>models_dir<span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_dir<span class="token punctuation">,</span> device_map<span class="token operator">=</span><span class="token string">"cuda:0"</span><span class="token punctuation">,</span>trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> cache_dir<span class="token operator">=</span>models_dir<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>generation_config <span class="token operator">=</span> GenerationConfig<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_dir<span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> cache_dir<span class="token operator">=</span>models_dir<span class="token punctuation">)</span>
response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> <span class="token string">"你好！可以介绍一下大语言模型吗"</span><span class="token punctuation">,</span>history<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
</code></pre> 
<p><strong>2.读取本地模型</strong><br> 如果已经下载好模型也可以通过下面的代码测试一下是否正确读入</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> modelscope <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer<span class="token punctuation">,</span> snapshot_download
<span class="token keyword">from</span> modelscope <span class="token keyword">import</span> GenerationConfig
<span class="token comment">##model_dir表示模型存储的目录,是linux系统记得~/开头</span>
model_dir <span class="token operator">=</span> <span class="token string">"绝对路径"</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_dir<span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> cache_dir<span class="token operator">=</span>models_dir<span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_dir<span class="token punctuation">,</span> device_map<span class="token operator">=</span><span class="token string">"cuda:0"</span><span class="token punctuation">,</span>trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> cache_dir<span class="token operator">=</span>models_dir<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>generation_config <span class="token operator">=</span> GenerationConfig<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_dir<span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> cache_dir<span class="token operator">=</span>models_dir<span class="token punctuation">)</span>
response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> <span class="token string">"你好！可以介绍一下大语言模型吗"</span><span class="token punctuation">,</span>history<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
</code></pre> 
<p><strong>3. Qwen2的模型下载和调用</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> modelscope <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer<span class="token punctuation">,</span> snapshot_download
<span class="token keyword">from</span> modelscope <span class="token keyword">import</span> GenerationConfig
<span class="token keyword">from</span> peft <span class="token keyword">import</span> PeftModel
<span class="token comment">##本地路径，也可以使用modelscope的模型id，自动下载</span>
model_name <span class="token operator">=</span> <span class="token string">"app/models/Qwen/Qwen2-7B-Instruct"</span>
device <span class="token operator">=</span> <span class="token string">"cuda"</span> <span class="token comment"># the device to load the model onto</span>

model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
   model_name<span class="token punctuation">,</span>
   torch_dtype<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span>
   device_map<span class="token operator">=</span><span class="token string">"auto"</span>
<span class="token punctuation">)</span>
lora_adapter_path <span class="token operator">=</span> <span class="token string">"LLaMA-Factory/saves/qwen2/lora/sft"</span>
model <span class="token operator">=</span> PeftModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model<span class="token punctuation">,</span> lora_adapter_path<span class="token punctuation">)</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span>

prompt <span class="token operator">=</span> <span class="token string">"你好!"</span>
messages <span class="token operator">=</span> <span class="token punctuation">[</span>
   <span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"You are a helpful assistant."</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
   <span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> prompt<span class="token punctuation">}</span>
<span class="token punctuation">]</span>
text <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>apply_chat_template<span class="token punctuation">(</span>
   messages<span class="token punctuation">,</span>
   tokenize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
   add_generation_prompt<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span>
model_inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token punctuation">[</span>text<span class="token punctuation">]</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

generated_ids <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>
   <span class="token operator">**</span>model_inputs<span class="token punctuation">,</span>
   max_new_tokens<span class="token operator">=</span><span class="token number">512</span>
<span class="token punctuation">)</span>
generated_ids <span class="token operator">=</span> <span class="token punctuation">[</span>
   output_ids<span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token keyword">for</span> input_ids<span class="token punctuation">,</span> output_ids <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>model_inputs<span class="token punctuation">.</span>input_ids<span class="token punctuation">,</span> generated_ids<span class="token punctuation">)</span>
<span class="token punctuation">]</span>

response <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>batch_decode<span class="token punctuation">(</span>generated_ids<span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>

</code></pre> 
<p><strong>3.准备数据集</strong><br> 详细见<a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md">官方数据集说明</a>这里有你需要准备的数据集格式，准备好后将文件复制到LLaMA-Factory/data目录下，并且修改data_info.json加入你的数据集描述</p> 
<pre><code class="prism language-json"><span class="token string-property property">"数据集名称"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
  <span class="token string-property property">"file_name"</span><span class="token operator">:</span> <span class="token string">"data.json"</span><span class="token punctuation">,</span>
  <span class="token string-property property">"columns"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"prompt"</span><span class="token operator">:</span> <span class="token string">"text"</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<h2><a id="_108"></a>二、读取模型和数据进行训练</h2> 
<h3><a id="1web_ui_109"></a>1.使用web ui</h3> 
<p>在LLama Factory目录下打开终端输入</p> 
<pre><code>llamafactory-cli webui
</code></pre> 
<p>进入可视化微调界面：<br> <code>注意:如果是本地模型此处一定是绝对路径</code><br> <img src="https://images2.imgbox.com/c4/90/RgpnqlpS_o.png" alt="在这里插入图片描述">设置参数完成之后，可以通过预览命令获取训练命令输入到命令行执行，也可以在线执行<img src="https://images2.imgbox.com/fe/59/vQPLjsxf_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="2_117"></a>2.修改官方脚本（推荐）</h3> 
<p>在examples/train_qlora目录下，官方提供了许多的.yaml文件用于微调，我们只需要修改其中参数就可以使用。<br> 我修改的一个版本</p> 
<pre><code class="prism language-json">### model
###模型相对LLama<span class="token operator">-</span>Factory的路径或者模型的Hub Id
<span class="token literal-property property">model_name_or_path</span><span class="token operator">:</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token operator">/</span>app<span class="token operator">/</span>models<span class="token operator">/</span>Qwen<span class="token operator">/</span>Qwen2<span class="token operator">-</span>7B<span class="token operator">-</span>Instruct
<span class="token literal-property property">quantization_bit</span><span class="token operator">:</span> <span class="token number">4</span>
<span class="token literal-property property">quantization_method</span><span class="token operator">:</span> bitsandbytes  # choices<span class="token operator">:</span> <span class="token punctuation">[</span><span class="token function">bitsandbytes</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token operator">/</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">hqq</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">/</span><span class="token number">3</span><span class="token operator">/</span><span class="token number">4</span><span class="token operator">/</span><span class="token number">5</span><span class="token operator">/</span><span class="token number">6</span><span class="token operator">/</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">eetq</span> <span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

### method
<span class="token literal-property property">stage</span><span class="token operator">:</span> sft
<span class="token literal-property property">do_train</span><span class="token operator">:</span> <span class="token boolean">true</span>
<span class="token literal-property property">finetuning_type</span><span class="token operator">:</span> lora
<span class="token literal-property property">lora_target</span><span class="token operator">:</span> all

### dataset
### 你在data_info中给你数据集起的名字
<span class="token literal-property property">dataset</span><span class="token operator">:</span> sql_sft
<span class="token literal-property property">template</span><span class="token operator">:</span> qwen
<span class="token literal-property property">cutoff_len</span><span class="token operator">:</span> <span class="token number">1024</span>
<span class="token literal-property property">max_samples</span><span class="token operator">:</span> <span class="token number">1000</span>
<span class="token literal-property property">overwrite_cache</span><span class="token operator">:</span> <span class="token boolean">true</span>
<span class="token literal-property property">preprocessing_num_workers</span><span class="token operator">:</span> <span class="token number">16</span>

### output
### 训练完成之后的adaptor存储路径
<span class="token literal-property property">output_dir</span><span class="token operator">:</span> saves<span class="token operator">/</span>qwen2<span class="token operator">/</span>lora<span class="token operator">/</span>sft
<span class="token literal-property property">logging_steps</span><span class="token operator">:</span> <span class="token number">10</span>
<span class="token literal-property property">save_steps</span><span class="token operator">:</span> <span class="token number">500</span>
<span class="token literal-property property">plot_loss</span><span class="token operator">:</span> <span class="token boolean">true</span>
<span class="token literal-property property">overwrite_output_dir</span><span class="token operator">:</span> <span class="token boolean">true</span>

### train
<span class="token literal-property property">per_device_train_batch_size</span><span class="token operator">:</span> <span class="token number">1</span>
<span class="token literal-property property">gradient_accumulation_steps</span><span class="token operator">:</span> <span class="token number">8</span>
<span class="token literal-property property">learning_rate</span><span class="token operator">:</span> <span class="token number">1.0e-4</span>
<span class="token literal-property property">num_train_epochs</span><span class="token operator">:</span> <span class="token number">10.0</span>
<span class="token literal-property property">lr_scheduler_type</span><span class="token operator">:</span> cosine
<span class="token literal-property property">warmup_ratio</span><span class="token operator">:</span> <span class="token number">0.1</span>
<span class="token literal-property property">bf16</span><span class="token operator">:</span> <span class="token boolean">true</span>
<span class="token literal-property property">ddp_timeout</span><span class="token operator">:</span> <span class="token number">180000000</span>

### eval
<span class="token literal-property property">val_size</span><span class="token operator">:</span> <span class="token number">0.1</span>
<span class="token literal-property property">per_device_eval_batch_size</span><span class="token operator">:</span> <span class="token number">1</span>
<span class="token literal-property property">eval_strategy</span><span class="token operator">:</span> steps
<span class="token literal-property property">eval_steps</span><span class="token operator">:</span> <span class="token number">500</span>

</code></pre> 
<p>之后通过</p> 
<pre><code class="prism language-json">llamafactory<span class="token operator">-</span>cli train examples\train_qlora\<span class="token operator">**</span><span class="token punctuation">.</span>yaml
</code></pre> 
<p>即可训练，具体见<a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README_zh.md">官方微调命令</a></p> 
<h2><a id="_175"></a>第三，加载与推理</h2> 
<p>类似训练过程，我们可以修改examples\inferance\中的examples进行推理</p> 
<pre><code class="prism language-json">#模型相对LLama<span class="token operator">-</span>Factory的路径
<span class="token literal-property property">model_name_or_path</span><span class="token operator">:</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token operator">/</span>app<span class="token operator">/</span>models<span class="token operator">/</span>Qwen<span class="token operator">/</span>Qwen2<span class="token operator">-</span>7B<span class="token operator">-</span>Instruct
#训练的输出
<span class="token literal-property property">adapter_name_or_path</span><span class="token operator">:</span> saves<span class="token operator">/</span>qwen2<span class="token operator">/</span>lora<span class="token operator">/</span>sft
#模型系列
<span class="token literal-property property">template</span><span class="token operator">:</span> qwen
<span class="token literal-property property">finetuning_type</span><span class="token operator">:</span> lora
</code></pre> 
<p>修改完成之后类似的运行<br> llamafactory-cli chat examples\inferance\*.yaml<br> 成功读取adapter<br> <img src="https://images2.imgbox.com/cc/6c/z59DtWOn_o.png" alt="在这里插入图片描述"></p> 
<p>之后可以与模型进行对话了<br> <img src="https://images2.imgbox.com/24/e0/dl0YLRzY_o.png" alt="在这里插入图片描述"><br> <strong>Peft库应该可以直接读取在python中读取训练的adapter数据，但是暂时还没弄</strong><br> 下面给个简单的加载案例</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> modelscope <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer<span class="token punctuation">,</span> snapshot_download
<span class="token keyword">from</span> modelscope <span class="token keyword">import</span> GenerationConfig
<span class="token keyword">from</span> peft <span class="token keyword">import</span> PeftModel
model_name <span class="token operator">=</span> <span class="token string">"app/models/Qwen/Qwen2-7B-Instruct"</span>
device <span class="token operator">=</span> <span class="token string">"cuda"</span> <span class="token comment"># the device to load the model onto</span>

model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
   model_name<span class="token punctuation">,</span>
   torch_dtype<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span>
   device_map<span class="token operator">=</span><span class="token string">"auto"</span>
<span class="token punctuation">)</span>
lora_adapter_path <span class="token operator">=</span> <span class="token string">"LLaMA-Factory/saves/qwen2/lora/sft"</span>
model <span class="token operator">=</span> PeftModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model<span class="token punctuation">,</span> lora_adapter_path<span class="token punctuation">)</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span>

prompt <span class="token operator">=</span> <span class="token string">"你好!"</span>
messages <span class="token operator">=</span> <span class="token punctuation">[</span>
   <span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"You are a helpful assistant."</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
   <span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> prompt<span class="token punctuation">}</span>
<span class="token punctuation">]</span>
text <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>apply_chat_template<span class="token punctuation">(</span>
   messages<span class="token punctuation">,</span>
   tokenize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
   add_generation_prompt<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span>
model_inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token punctuation">[</span>text<span class="token punctuation">]</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

generated_ids <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>
   <span class="token operator">**</span>model_inputs<span class="token punctuation">,</span>
   max_new_tokens<span class="token operator">=</span><span class="token number">512</span>
<span class="token punctuation">)</span>
generated_ids <span class="token operator">=</span> <span class="token punctuation">[</span>
   output_ids<span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token keyword">for</span> input_ids<span class="token punctuation">,</span> output_ids <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>model_inputs<span class="token punctuation">.</span>input_ids<span class="token punctuation">,</span> generated_ids<span class="token punctuation">)</span>
<span class="token punctuation">]</span>

response <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>batch_decode<span class="token punctuation">(</span>generated_ids<span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b14dd0d3e8d4042f7172a066cfdf9781/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">web前端之关闭浏览器标签页后自动退出登录、踩坑之浏览器关闭或刷新前发送可靠的请求、页面卸载的生命周期、监听的不同写法、页面可见性、可视区域、事件、beforeunload、unmounted</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/addd746381c2b456c60d33b23b32e3b4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【OAuth2系列】集成微信小程序登录到 Spring Security OAuth 2.0</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>