<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>快速理解AIGC图像控制利器ControlNet和Lora的架构原理 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/541d8288c582140668b8ac109b0080c5/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="快速理解AIGC图像控制利器ControlNet和Lora的架构原理">
  <meta property="og:description" content="作者公众号 大数据与AI杂谈 （TalkCheap），转载请标明出处
ControlNet以及Lora是什么，玩过stable diffusion AI图像生成的同学应该都不陌生。
一般来说，如果你用以SD 或 SDXL为基础的模型来生成图像，产出的图像往往非常随机，很难对图像的内容做相对精确的控制。尤其是原始的SD和SDXL的底模，拥有很好的图像泛化能力（也就是说能根据提示词输出各种类型的图像），但也使得图像的效果通常不是最佳的，对内容的定向精确化控制的能力往往也不足。
比如你想让生成的图像更接近真人摄影风格一些（肤质，环境，灯光，胶片感等等）
又或者你想精确控制图像生成的内容领域，比如3D建筑模型或者游戏图标LOGO等
也可能你想指定生成人物的容貌特征，比如生成特定人物，或者你自己的头像，又或者你想精确控制人物的姿势，高低胖瘦，画面的布局构造等等。
这时候，你可以使用一些经过Finetune微调训练过的专有模型（比如专门针对摄影图片进行二次训练的模型）来对图像的内容做一些相对确定性的定制化输出。一定程度上，这种在特定图像集内进行二次训练的模型，实际上也就是一种对特定内容或风格的图像进行过拟合的过程，使得输出的内容能够尽可能靠近指定领域的图像，但这么做的结果，往往也导致其图像通用泛化输出能力的丧失，也就是牺牲了宽度换取了深度（比如很多人物画像模型就丧失了绘制风景图片的能力）
所以，你往往需要有各种不同的模型来输出不同的内容（至于Mid Journey等商业产品如何做到泛化内容的高质量输出，我猜大体要不然是模型规模大得多，能容纳更多的知识，要不然就是内部做了不同领域内容的预分流）。比如civitai上就有各种风格的模型可以下载
但是，类似SD这样的开源基础模型的体积基本都比较大（通常在2GB~8GB之间，相对普通消费者用户来说），如果要大量使用的话，存储和加载的代价都比较高，另外，如果你需要生成多种不同的风格或者内容的混合图像呢？那可能得组合就更多了。
那么，有没有更低成本和更灵活的方式呢，这时候就轮到Lora登场了
LORA LoRA全称Low-Rank Adaptation（低秩适配器) ，实际上最早是微软的同学在大语言模型的训练中发明并使用的一种低成本的模型微调技术 （论文的名字就是 LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS https://arxiv.org/pdf/2106.09685.pdf ）
其根本出发点也是为了降低大语言模型finetune的代价和大量Finetune模型加载的代价问题，毕竟商业化的语言模型通常体积要远大于图像模型，如果有大量的比如175B尺寸的大语言模型需要定制，其训练，存储和加载的代价即使是大公司也是难以承受的（主要是商业盈利成本方面的可操作性）。只是后来发现LoRA相关原理技术，应用在图像生成领域的模型和场景中，也是特别的适合。
LoRA的基本思想，是大部分的模型微调（或者定制，过拟合等等）过程，可以把微调后的模型等价的看作是一个原有的网络模型上旁路叠加上一个同样尺寸的针对增量微调内容的网络模型。微调训练后的模型的输出，相当于在原有网络模型的输出结果里混合上这个增量微调的网络模型的输出结果。
与此同时，既然是模型的定制化微调，那么我们有理由可以假定，差异的内容应该不会特别的大，也就是说，这个旁路模型的有效信息密度可能相比原先的模型要小的多。那么用同样结构和尺寸的网络模型来存储有效信息密度小得多的数据，是不是就有点浪费呢？
我们知道，大部分的网络模型，其核心数据就是各种网络参数，而这些参数大体上都是以各种大小的矩阵向量的形式存在的。比如我们在前面“通俗深入的理解Sora的架构原理”一文中介绍的SD图像扩散模型中的UNet网络结构，就是由数十个卷积网络层组成，Transformer模型中的各种Cross Attention层也类似
从数学的角度来说，一个矩阵的信息密度，大体可以用矩阵的“秩”（Rank）来粗略衡量（这也是LoRA中Rank的语义来源）
什么是秩呢，学过线性代数的同学应该都懂，忘了也没关系，它等价于一个矩阵中互不相关的行或列的数量。也就是不能通过四则运算从别的行或列的数据组合计算出来的行列有多少。简单来说，就是有多少行列的数据是不重复冗余的，冗余的行列数据越多，可想而知有效信息密度就越低，矩阵的秩也就越小。
而这个秩的上限，就是矩阵行列数的最小值，比如1000x1000的矩阵，秩的上限就是1000。而1000x2或2x1000的矩阵，秩的上限也就只有2（毕竟最多也就两行或两列数据互不相关）。
所以如果矩阵中的有效数据密度很低（或者数据很稀疏），可能它的秩就很低，那我们就没必要用高秩的矩阵来存储这些数据，用低秩的矩阵来近似替代就好了。
那怎么控制参数矩阵的秩呢? 那些参数不都是随机生成然后通过训练得到的嘛，而且，原始网络模型的输入输出的维度也是固定的，你怎么替换呢？
LoRA的方式是把旁路的矩阵替换成两个低秩矩阵的乘积。比如1000x1000的矩阵，用1000x2 和 2x1000的矩阵来替换，后者两个矩阵乘积运算的结果也是一个1000x1000的矩阵，也就是输入和输出可以和原矩阵的维度相匹配，但秩的上限就只有2了（因为按秩的定义，结果矩阵可以用相乘的两个矩阵的两行或两列数据的四则运算得到所有行列的数据）
所以，LoRA实际上并不精确控制参数矩阵的秩，只是限制了秩的上限，至于最终的实际数值还是训练出来的，理论上训练完的模型参数，能充分利用参数矩阵的信息密度上限当然更好，如果不能，那也没办法，但反过来，无论如何，你突破不了理论上限。
这么做的目的，当然是为了减少这部分旁路模型的参数数量，进而也就降低了训练的代价（时间和空间成本），比如上面的例子中，参数的数量就从1000x1000=100万 降低到了 1000x2x2=4000，降低了250倍。
实际训练过程中，原先的基础模型的参数是固定的，在Finetune迭代过程中只需要训练旁路网络的参数。而在使用过程中，只需要替换不同的旁路网络模型（甚至旁路叠加多个不同目的的网络模型），就能实现以较小的代价（训练，存储，推理）支持不同的目标场景输出的目的了。
比如在典型的SD图像生成场景中，一个基础模型的典型大小可能是2GB到8GB之间，而一个Finetune的LoRA的典型大小可能只有16MB到128MB之间（代价就是一个LoRA模型往往只能表达少数的特定语义），而训练的过程，简单的LoRA往往也可能在消费级的显卡上用几十分钟到几十小时的时间范围就能完成。实际使用中，可能也是一个基础模型叠加多个LoRA使用，比如一个LoRA控制画面风格，一个LoRA生成指定人物的外貌特征。
喜欢刨根到底的同学可能还会问，既然没有改变最终的网络结构，为啥叠加一个网络模型以后，就可以控制图像的特征了呢，你不是说AIGC图像生成随机性很大，没法精确控制吗？
确实，的确是没法精确控制，所以本质上Finetune的过程，相对于原模型的泛化能力来说，还是一个类似过拟合的过程，通过减少模型的输出可能性，降模型的输出限制到特定的范围内，来降低这个随机性，从而实现输出特定内容的目的。
实际训练LoRA模型的时候，大体都是选择一些特定方向内容的图片进行反复训练（特定人物图像，特定画风图像），使得LoRA模型学习到特定内容的表达方式。比如，下面这个Ink Painting的风格就是通过LoRA来控制实现的。
当然，有些Silder滑轨调节类的LoRA（比如填充不同数值控制从瘦到胖，皮肤白到黑，画面细节增强）等等，则是通过训练相应特征不同阶段性差异的对比图像来实现的，这个就是具体应用层面的技巧了。
而SD图像扩散模型，如我们之前文章所述，其生成过程是通过去除噪声来实现的，按不同比例混合特定内容的噪声数据，也就能实现对特定内容输出控制的强度大小。在使用的时候，也就可以在原模型的泛化能力和LoRA的过拟合控制之间寻找一个合适的平衡点，来实现对特定内容的输出干预。
ControlNet 那么，有了LoRA为什么还需要ControlNet呢，这两者有啥区别不？
实际上，ControlNet的基本核心思路和LoRA非常类似，也是通过混合旁路网络的输出结果，来实现对原网络模型输出结果的干预和调整。因为是模型叠加，所以和LoRA类似，它也能实现基础模型和一个或多个ControlNet模型按不同比例混合叠加使用之类的效果。
但它和LoRA模型最大的不同，还是在于网络结构和与原模型参数混合方式的设计上。LoRA的原理相对简单粗暴，Low Rank的压缩方式减少了模型体积，但一定程度上也限制了它的能力上限。
而ControlNet对原模型的参数矩阵并没有进行压缩，而是进行了1比1的精确复刻。当然他并没有复刻所有的模型层次，而是只复刻了部分编码层的模型参数结构。实际应用中，是将每一层编码层的输出结果通过一些线性变换再混合回原模型的解码层中来实现对最终输出效果的干预和控制。
因为没有压缩模型参数规模，所以其能力上限很容易理解，比LoRA是要高不少的，也具备更好的泛化能力。当然，ControlNet模型的尺寸相应的也就要大不少（典型的大小比如从500MB到2GB之间）
当然，说它和LoRA的原理类似，只是从对基础模型干预的基本流程和原理的角度来说的。实际上，ControlNet最大的卖点还是在他各种不同的应用模型的具体实现上。
不同于大部分LoRA模型是通过文本的激活和微调控制图像的内容，大部分的ControlNet模型都是以图像作参考来干预内容输出的（能这么做，根本的原因，当然还是因为能力上限空间更高，所以具体的应用模式就有了较大的区别）
一个典型的ControlNet模型的应用方式，大体分为两个步骤
第一个步骤是通过一幅参考图像生成一个引导图像，这个过程其实并没有用到ControlNet的模型，往往是单独训练的各种特定用途的图像算法的模型，用来抽取这个参考图像的某方面的特征。这个抽取特征的模型，可能是各种经典的图像处理算法（比如边缘轮廓检测，模糊，反向，灰度梯度），也可能是其它深度学习算法（比如图像切割，姿态检测）。多数情况下也需要和第二个步骤真正使用到的ControlNet模型配套使用。
第二个步骤才是把这个表达了参考图像指定特征的引导图像给到ControlNet的模型作为输入控制条件，用这个引导图像结合用户的其它文本或图像提示条件，再去干预最终的图像生成结果。比如这个人物姿态和脸部特征检测及控制生成的例子。就是通过ControlNet OpenPose模型来实现的。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-03-15T10:23:43+08:00">
    <meta property="article:modified_time" content="2024-03-15T10:23:43+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">快速理解AIGC图像控制利器ControlNet和Lora的架构原理</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><em>作者公众号 大数据与AI杂谈 （TalkCheap），转载请标明出处</em></p> 
<p>ControlNet以及Lora是什么，玩过stable diffusion AI图像生成的同学应该都不陌生。</p> 
<p>一般来说，如果你用以SD 或 SDXL为基础的模型来生成图像，产出的图像往往非常随机，很难对图像的内容做相对精确的控制。尤其是原始的SD和SDXL的底模，拥有很好的图像泛化能力（也就是说能根据提示词输出各种类型的图像），但也使得图像的效果通常不是最佳的，对内容的定向精确化控制的能力往往也不足。</p> 
<p>比如你想让生成的图像更接近真人摄影风格一些（肤质，环境，灯光，胶片感等等）</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="944" src="https://images2.imgbox.com/70/9c/uZBTujon_o.png" width="734"></p> 
<p>又或者你想精确控制图像生成的内容领域，比如3D建筑模型或者游戏图标LOGO等</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="689" src="https://images2.imgbox.com/10/9f/5TBDeEpJ_o.png" width="514"></p> 
<p>也可能你想指定生成人物的容貌特征，比如生成特定人物，或者你自己的头像，又或者你想精确控制人物的姿势，高低胖瘦，画面的布局构造等等。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="948" src="https://images2.imgbox.com/05/13/qZ3PsjZD_o.png" width="741"></p> 
<p>这时候，你可以使用一些经过Finetune微调训练过的专有模型（比如专门针对摄影图片进行二次训练的模型）来对图像的内容做一些相对确定性的定制化输出。一定程度上，这种在特定图像集内进行二次训练的模型，实际上也就是一种对特定内容或风格的图像进行过拟合的过程，使得输出的内容能够尽可能靠近指定领域的图像，但这么做的结果，往往也导致其图像通用泛化输出能力的丧失，也就是牺牲了宽度换取了深度（比如很多人物画像模型就丧失了绘制风景图片的能力）</p> 
<p>所以，你往往需要有各种不同的模型来输出不同的内容（至于Mid Journey等商业产品如何做到泛化内容的高质量输出，我猜大体要不然是模型规模大得多，能容纳更多的知识，要不然就是内部做了不同领域内容的预分流）。比如civitai上就有各种风格的模型可以下载</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="542" src="https://images2.imgbox.com/8b/72/QmVfs75L_o.png" width="1080"></p> 
<p>但是，类似SD这样的开源基础模型的体积基本都比较大（通常在2GB~8GB之间，相对普通消费者用户来说），如果要大量使用的话，存储和加载的代价都比较高，另外，如果你需要生成多种不同的风格或者内容的混合图像呢？那可能得组合就更多了。</p> 
<p>那么，有没有更低成本和更灵活的方式呢，这时候就轮到Lora登场了</p> 
<p></p> 
<h4 style="background-color:transparent;text-align:center;"><span style="color:#4da8ee;">LORA</span></h4> 
<p>LoRA全称Low-Rank Adaptation（低秩适配器) ，实际上最早是微软的同学在大语言模型的训练中发明并使用的一种低成本的模型微调技术 （论文的名字就是  LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS https://arxiv.org/pdf/2106.09685.pdf ）</p> 
<p>其根本出发点也是为了降低大语言模型finetune的代价和大量Finetune模型加载的代价问题，毕竟商业化的语言模型通常体积要远大于图像模型，如果有大量的比如175B尺寸的大语言模型需要定制，其训练，存储和加载的代价即使是大公司也是难以承受的（主要是商业盈利成本方面的可操作性）。只是后来发现LoRA相关原理技术，应用在图像生成领域的模型和场景中，也是特别的适合。</p> 
<p>LoRA的基本思想，是大部分的模型微调（或者定制，过拟合等等）过程，可以把微调后的模型等价的看作是一个原有的网络模型上旁路叠加上一个同样尺寸的针对增量微调内容的网络模型。微调训练后的模型的输出，相当于在原有网络模型的输出结果里混合上这个增量微调的网络模型的输出结果。</p> 
<p>与此同时，既然是模型的定制化微调，那么我们有理由可以假定，差异的内容应该不会特别的大，也就是说，这个旁路模型的有效信息密度可能相比原先的模型要小的多。那么用同样结构和尺寸的网络模型来存储有效信息密度小得多的数据，是不是就有点浪费呢？</p> 
<p>我们知道，大部分的网络模型，其核心数据就是各种网络参数，而这些参数大体上都是以各种大小的矩阵向量的形式存在的。比如我们在前面“通俗深入的理解Sora的架构原理”一文中介绍的SD图像扩散模型中的UNet网络结构，就是由数十个卷积网络层组成，Transformer模型中的各种Cross Attention层也类似</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="414" src="https://images2.imgbox.com/99/98/ucuUoEPg_o.jpg" width="750"></p> 
<p>从数学的角度来说，一个矩阵的信息密度，大体可以用矩阵的“秩”（Rank）来粗略衡量（这也是LoRA中Rank的语义来源）</p> 
<p>什么是秩呢，学过线性代数的同学应该都懂，忘了也没关系，它等价于一个矩阵中互不相关的行或列的数量。也就是不能通过四则运算从别的行或列的数据组合计算出来的行列有多少。简单来说，就是有多少行列的数据是不重复冗余的，冗余的行列数据越多，可想而知有效信息密度就越低，矩阵的秩也就越小。</p> 
<p>而这个秩的上限，就是矩阵行列数的最小值，比如1000x1000的矩阵，秩的上限就是1000。而1000x2或2x1000的矩阵，秩的上限也就只有2（毕竟最多也就两行或两列数据互不相关）。</p> 
<p>所以如果矩阵中的有效数据密度很低（或者数据很稀疏），可能它的秩就很低，那我们就没必要用高秩的矩阵来存储这些数据，用低秩的矩阵来近似替代就好了。</p> 
<p>那怎么控制参数矩阵的秩呢? 那些参数不都是随机生成然后通过训练得到的嘛，而且，原始网络模型的输入输出的维度也是固定的，你怎么替换呢？</p> 
<p>LoRA的方式是把旁路的矩阵替换成两个低秩矩阵的乘积。比如1000x1000的矩阵，用1000x2 和 2x1000的矩阵来替换，后者两个矩阵乘积运算的结果也是一个1000x1000的矩阵，也就是输入和输出可以和原矩阵的维度相匹配，但秩的上限就只有2了（因为按秩的定义，结果矩阵可以用相乘的两个矩阵的两行或两列数据的四则运算得到所有行列的数据）</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="476" src="https://images2.imgbox.com/e1/7f/ir7bCT1X_o.png" width="1080"></p> 
<p>所以，LoRA实际上并不精确控制参数矩阵的秩，只是限制了秩的上限，至于最终的实际数值还是训练出来的，理论上训练完的模型参数，能充分利用参数矩阵的信息密度上限当然更好，如果不能，那也没办法，但反过来，无论如何，你突破不了理论上限。</p> 
<p>这么做的目的，当然是为了减少这部分旁路模型的参数数量，进而也就降低了训练的代价（时间和空间成本），比如上面的例子中，参数的数量就从1000x1000=100万 降低到了 1000x2x2=4000，降低了250倍。</p> 
<p>实际训练过程中，原先的基础模型的参数是固定的，在Finetune迭代过程中只需要训练旁路网络的参数。而在使用过程中，只需要替换不同的旁路网络模型（甚至旁路叠加多个不同目的的网络模型），就能实现以较小的代价（训练，存储，推理）支持不同的目标场景输出的目的了。</p> 
<p>比如在典型的SD图像生成场景中，一个基础模型的典型大小可能是2GB到8GB之间，而一个Finetune的LoRA的典型大小可能只有16MB到128MB之间（代价就是一个LoRA模型往往只能表达少数的特定语义），而训练的过程，简单的LoRA往往也可能在消费级的显卡上用几十分钟到几十小时的时间范围就能完成。实际使用中，可能也是一个基础模型叠加多个LoRA使用，比如一个LoRA控制画面风格，一个LoRA生成指定人物的外貌特征。</p> 
<p>喜欢刨根到底的同学可能还会问，既然没有改变最终的网络结构，为啥叠加一个网络模型以后，就可以控制图像的特征了呢，你不是说AIGC图像生成随机性很大，没法精确控制吗？</p> 
<p>确实，的确是没法精确控制，所以本质上Finetune的过程，相对于原模型的泛化能力来说，还是一个类似过拟合的过程，通过减少模型的输出可能性，降模型的输出限制到特定的范围内，来降低这个随机性，从而实现输出特定内容的目的。</p> 
<p>实际训练LoRA模型的时候，大体都是选择一些特定方向内容的图片进行反复训练（特定人物图像，特定画风图像），使得LoRA模型学习到特定内容的表达方式。比如，下面这个Ink Painting的风格就是通过LoRA来控制实现的。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="631" src="https://images2.imgbox.com/f3/da/EFV77xGK_o.png" width="859"></p> 
<p>当然，有些Silder滑轨调节类的LoRA（比如填充不同数值控制从瘦到胖，皮肤白到黑，画面细节增强）等等，则是通过训练相应特征不同阶段性差异的对比图像来实现的，这个就是具体应用层面的技巧了。</p> 
<p>而SD图像扩散模型，如我们之前文章所述，其生成过程是通过去除噪声来实现的，按不同比例混合特定内容的噪声数据，也就能实现对特定内容输出控制的强度大小。在使用的时候，也就可以在原模型的泛化能力和LoRA的过拟合控制之间寻找一个合适的平衡点，来实现对特定内容的输出干预。</p> 
<p></p> 
<h4 style="background-color:transparent;text-align:center;"><span style="color:#4da8ee;">ControlNet</span></h4> 
<p>那么，有了LoRA为什么还需要ControlNet呢，这两者有啥区别不？</p> 
<p>实际上，ControlNet的基本核心思路和LoRA非常类似，也是通过混合旁路网络的输出结果，来实现对原网络模型输出结果的干预和调整。因为是模型叠加，所以和LoRA类似，它也能实现基础模型和一个或多个ControlNet模型按不同比例混合叠加使用之类的效果。</p> 
<p>但它和LoRA模型最大的不同，还是在于网络结构和与原模型参数混合方式的设计上。LoRA的原理相对简单粗暴，Low Rank的压缩方式减少了模型体积，但一定程度上也限制了它的能力上限。</p> 
<p>而ControlNet对原模型的参数矩阵并没有进行压缩，而是进行了1比1的精确复刻。当然他并没有复刻所有的模型层次，而是只复刻了部分编码层的模型参数结构。实际应用中，是将每一层编码层的输出结果通过一些线性变换再混合回原模型的解码层中来实现对最终输出效果的干预和控制。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="702" src="https://images2.imgbox.com/17/12/Ym50hEeT_o.png" width="795"></p> 
<p>因为没有压缩模型参数规模，所以其能力上限很容易理解，比LoRA是要高不少的，也具备更好的泛化能力。当然，ControlNet模型的尺寸相应的也就要大不少（典型的大小比如从500MB到2GB之间）</p> 
<p>当然，说它和LoRA的原理类似，只是从对基础模型干预的基本流程和原理的角度来说的。实际上，ControlNet最大的卖点还是在他各种不同的应用模型的具体实现上。</p> 
<p>不同于大部分LoRA模型是通过文本的激活和微调控制图像的内容，大部分的ControlNet模型都是以图像作参考来干预内容输出的（能这么做，根本的原因，当然还是因为能力上限空间更高，所以具体的应用模式就有了较大的区别）</p> 
<p>一个典型的ControlNet模型的应用方式，大体分为两个步骤</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="417" src="https://images2.imgbox.com/c4/28/VahyQ1Qq_o.png" width="1024"></p> 
<p>第一个步骤是通过一幅参考图像生成一个引导图像，这个过程其实并没有用到ControlNet的模型，往往是单独训练的各种特定用途的图像算法的模型，用来抽取这个参考图像的某方面的特征。这个抽取特征的模型，可能是各种经典的图像处理算法（比如边缘轮廓检测，模糊，反向，灰度梯度），也可能是其它深度学习算法（比如图像切割，姿态检测）。多数情况下也需要和第二个步骤真正使用到的ControlNet模型配套使用。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="624" src="https://images2.imgbox.com/b6/d0/37hkA5ex_o.png" width="846"></p> 
<p>第二个步骤才是把这个表达了参考图像指定特征的引导图像给到ControlNet的模型作为输入控制条件，用这个引导图像结合用户的其它文本或图像提示条件，再去干预最终的图像生成结果。比如这个人物姿态和脸部特征检测及控制生成的例子。就是通过ControlNet OpenPose模型来实现的。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="626" src="https://images2.imgbox.com/28/39/ndcxbBUl_o.png" width="842"></p> 
<p>当然，如果你已经有了特定的ControlNet模型所需要的引导图像（比如通过别的方式生成，手绘，之前存储下来的等等），那也可以直接使用该图像，从而跳过第一个步骤。</p> 
<p>看到这，你可能会问，干嘛要搞那么麻烦呢？不能直接用原始图像作参考来训练ControlNet的模型吗？这样使用的时候也不要串联两个模型了。</p> 
<p>我个人的理解是：</p> 
<p>第一，前面图像特征抽取的算法，已经有很多成熟通用的算法模型了，把这部分拆开，可以最大限度利用既有的先进技术和成果，没必要重复造轮子。</p> 
<p>第二，基于抽取的特征做特定ControlNet模型的训练，可以让模型忽略与想要的特征不相关的内容，避免受到原始图像中与想要抽取的特征不相关的信息的干扰（比如人物的姿态，和人物衣服着装的颜色，款式乃至图像背景显然都没有任何关系），这样，一方面有利于增强模型的泛化能力，另一方面可以加快模型的迭代收敛速度。</p> 
<p>第三，便于灵活组合和替换两个步骤中使用到的模型，实现不同的端到端能力组合，或者在不同的应用场景中复用既有的模块能力。</p> 
<p>当然，理论归理论，实际能力和效果有时候还是挺让人惊讶的（比如火柴杆图可以在图像加噪扩散流程中控制人的外形姿态，这居然也是通过暴力的相关性训练能训练出来的）</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="1024" src="https://images2.imgbox.com/58/e5/eLL9C3NE_o.png" width="1024"></p> 
<p>从上述原理来看，具体的ControlNet模型的应用形式其实也是可以有各种可能的，未必一定要通过参考图像的方式来运转，也未必一定要抽取图像的特征来控制新生成的图像结构，完全也可以和LoRA类似比如用来生成特定人物特定画风的图像，但目前大多数的ControlNet模型还是用来控制图像的具体图像结构的。毕竟前者，使用成本更低的LoRA就能较好的完成，没必要用到ControlNet，而后者精确控制图像结构则是落在通常的LoRA模型能力之外的。</p> 
<p>因此，综上，目前的图像生成控制的标准实践，大多数都是一个基础模型或者特定Finetune过的基础模型用来控制图像的宏观能力和基调，配合LoRA来控制图像风格或指定特定内容生成，然后通过ControlNet来精确控制整体图像构图</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="750" src="https://images2.imgbox.com/08/a5/rdfouTwm_o.jpg" width="750"></p> 
<p>但这并不是绝对唯一的解法，本质上上述各种图像控制方式，不管其架构如何，都是通过相关文本或图像条件，引导生成特定内容的噪声扩散图像数据，然后在不同的阶段按需混合到图像去噪的流程环节中，最终诱导模型的输出往指定的方向靠近，只要能达成类似效果或目的，怎么实现都只是手段，上述流程只是一个当前大家普遍遵守的最佳实践而已。</p> 
<p>具体使用哪些模型，以什么样的形式去创造上层的不用应用场景形态，比如是控制图像布局还是指定人物，姿态控制，背景替换，画风控制，换脸，换衣服，图像自动扩边（out painting）等等，乃至各种商业应用的可能性，能否想到，如何实现，能否实现，需要配合哪些额外的附加工作流程和外围图像处理模块。很大程度上也是看你对相关模型的基本工作原理和能力边界是否理解，以及具体应用场景和流程可能性的想象力，乃至行业商业模式的了解了。</p> 
<p>比如像weshop这样用来做商家服饰模特和商品图生成的应用，就是一种很典型的应用</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="554" src="https://images2.imgbox.com/e6/6e/wd7oiMJS_o.png" width="577"></p> 
<p>当然，图像生成控制的模型和控制方式也不只有LoRA和ControlNet两种，实际上图像扩散模型的各个环节理论上都能够进行干预和修改，相同的环节可能也有一些差异或大或小的不同的方案实现，比如腾讯的同学们提出的T2I-Adapter，其思想和应用流程就和ControlNet很类似（在很多SD上层应用界面的封装中，也把它和ControlNet放在一起），有兴趣的同学可以自行了解</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="688" src="https://images2.imgbox.com/06/d4/TzPpk5Rj_o.png" width="1080"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6bad4327ff796bd7d80f9781636fcff3/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">mac安全干净卸载Anaconda3</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/42e2a3d484bae962589679193864d521/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Android 获取 usb 权限的两种方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>