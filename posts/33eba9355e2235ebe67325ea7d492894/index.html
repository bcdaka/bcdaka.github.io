<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AIGC专栏9——Scalable Diffusion Models with Transformers （DiT）结构解析 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/33eba9355e2235ebe67325ea7d492894/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="AIGC专栏9——Scalable Diffusion Models with Transformers （DiT）结构解析">
  <meta property="og:description" content="AIGC专栏9——Scalable Diffusion Models with Transformers （DiT）结构解析 学习前言源码下载地址网络构建一、什么是Diffusion Transformer (DiT)二、DiT的组成三、生成流程1、采样流程a、生成初始噪声b、对噪声进行N次采样c、单次采样解析I、预测噪声II、施加噪声 d、预测噪声过程中的网络结构解析i、adaLN-Zero结构解析ii、patch分块处理iii、Transformer特征提取iv、上采样 3、隐空间解码生成图片 类别到图像预测过程代码 学习前言 近期Sora大火，它底层是Diffusion Transformer，本质上是使用Transformer结构代替原本的Unet进行噪声预测，好处是统一了文本生成与视频生成的结构。这训练优化和预测优化而言是个好事，因为只需要优化一种结构就够了。虽然觉得OpenAI是大力出奇迹，但还是得学！
源码下载地址 https://github.com/bubbliiiing/DiT-pytorch
喜欢的可以点个star噢。
网络构建 一、什么是Diffusion Transformer (DiT) DiT基于扩散模型，所以不免包含不断去噪的过程，如果是图生图的话，还有不断加噪的过程，此时离不开DDPM那张老图，如下：
DiT相比于DDPM，使用了更快的采样器，也使用了更大的分辨率，与Stable Diffusion一样使用了隐空间的扩散，但可能更偏研究性质一些，没有使用非常大的数据集进行预训练，只使用了imagenet进行预训练。
与Stable Diffusion不同的是，DiT的网络结构完全由Transformer组成，没有Unet中大量的上下采样，结构更为简单清晰。
本文主要是解析一下整个DiT模型的结构组成，并简单一次扩散，多次扩散的流程。本文代码来自于Diffusers，Diffusers代码较为简单清晰，是一个非常好的仓库，学习起来也比较快。
二、DiT的组成 DiT由三大部分组成。
1、Sampler采样器。
2、Variational Autoencoder (VAE) 变分自编码器。
3、UNet 主网络，噪声预测器。
每一部分都很重要，由于DiT的官方版本并没有在 大规模文本图片 的 数据集上训练，只使用了imagenet进行预训练。所以它并没有文本输入，而是以标签作为输入。因此，DiT只能按照类别进行图片生成，可以生成imagenet中的1000类
三、生成流程 生成流程分为两个部分：
1、生成正态分布向量后进行若干次采样。
2、进行解码。
由于DiT只能按照类别进行图片生成，所以无需对文本进行编码，直接传入类别的对应的id（0-1000）即可指定类别。
# --------------------------------- # # 前处理 # --------------------------------- # # 生成latent latents = randn_tensor( shape=(batch_size, latent_channels, latent_size, latent_size), generator=generator, device=self._execution_device, dtype=self.transformer.dtype, ) latent_model_input = torch.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-02-25T18:30:10+08:00">
    <meta property="article:modified_time" content="2024-02-25T18:30:10+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AIGC专栏9——Scalable Diffusion Models with Transformers （DiT）结构解析</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night-eighties">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>AIGC专栏9——Scalable Diffusion Models with Transformers （DiT）结构解析</h4> 
 <ul><li><a href="#_2" rel="nofollow">学习前言</a></li><li><a href="#_5" rel="nofollow">源码下载地址</a></li><li><a href="#_10" rel="nofollow">网络构建</a></li><li><ul><li><a href="#Diffusion_Transformer_DiT_11" rel="nofollow">一、什么是Diffusion Transformer (DiT)</a></li><li><a href="#DiT_19" rel="nofollow">二、DiT的组成</a></li><li><a href="#_26" rel="nofollow">三、生成流程</a></li><li><ul><li><a href="#1_121" rel="nofollow">1、采样流程</a></li><li><ul><li><a href="#a_123" rel="nofollow">a、生成初始噪声</a></li><li><a href="#bN_136" rel="nofollow">b、对噪声进行N次采样</a></li><li><a href="#c_194" rel="nofollow">c、单次采样解析</a></li><li><ul><li><a href="#I_195" rel="nofollow">I、预测噪声</a></li><li><a href="#II_226" rel="nofollow">II、施加噪声</a></li></ul> 
     </li><li><a href="#d_234" rel="nofollow">d、预测噪声过程中的网络结构解析</a></li><li><ul><li><a href="#iadaLNZero_236" rel="nofollow">i、adaLN-Zero结构解析</a></li><li><a href="#iipatch_283" rel="nofollow">ii、patch分块处理</a></li><li><a href="#iiiTransformer_330" rel="nofollow">iii、Transformer特征提取</a></li><li><a href="#iv_442" rel="nofollow">iv、上采样</a></li></ul> 
    </li></ul> 
    </li><li><a href="#3_470" rel="nofollow">3、隐空间解码生成图片</a></li></ul> 
  </li></ul> 
  </li><li><a href="#_487" rel="nofollow">类别到图像预测过程代码</a></li></ul> 
</div> 
<p></p> 
<h2><a id="_2"></a>学习前言</h2> 
<p>近期Sora大火，它底层是Diffusion Transformer，本质上是使用Transformer结构代替原本的Unet进行噪声预测，好处是统一了文本生成与视频生成的结构。这训练优化和预测优化而言是个好事，因为只需要优化一种结构就够了。虽然觉得OpenAI是大力出奇迹，但还是得学！<br> <img src="https://images2.imgbox.com/e5/d2/EzTRsynY_o.jpg" alt="在这里插入图片描述"></p> 
<h2><a id="_5"></a>源码下载地址</h2> 
<p><a href="https://github.com/bubbliiiing/DiT-pytorch">https://github.com/bubbliiiing/DiT-pytorch</a></p> 
<p>喜欢的可以点个star噢。</p> 
<h2><a id="_10"></a>网络构建</h2> 
<h3><a id="Diffusion_Transformer_DiT_11"></a>一、什么是Diffusion Transformer (DiT)</h3> 
<p>DiT基于扩散模型，所以不免包含不断去噪的过程，如果是图生图的话，还有不断加噪的过程，此时离不开DDPM那张老图，如下：<br> <img src="https://images2.imgbox.com/5e/23/ZDKE9ofV_o.png" alt="在这里插入图片描述"><br> DiT相比于DDPM，使用了更快的采样器，也使用了更大的分辨率，与Stable Diffusion一样使用了隐空间的扩散，但可能更偏研究性质一些，没有使用非常大的数据集进行预训练，只使用了imagenet进行预训练。</p> 
<p>与Stable Diffusion不同的是，DiT的网络结构完全由Transformer组成，没有Unet中大量的上下采样，结构更为简单清晰。</p> 
<p>本文主要是解析一下整个DiT模型的结构组成，并简单一次扩散，多次扩散的流程。本文代码来自于Diffusers，Diffusers代码较为简单清晰，是一个非常好的仓库，学习起来也比较快。</p> 
<h3><a id="DiT_19"></a>二、DiT的组成</h3> 
<p>DiT由三大部分组成。<br> 1、Sampler采样器。<br> 2、Variational Autoencoder (VAE) 变分自编码器。<br> 3、UNet 主网络，噪声预测器。</p> 
<p>每一部分都很重要，由于DiT的官方版本并没有在 大规模文本图片 的 数据集上训练，只使用了imagenet进行预训练。所以它并没有文本输入，而是以标签作为输入。因此，DiT只能按照类别进行图片生成，可以生成imagenet中的1000类</p> 
<h3><a id="_26"></a>三、生成流程</h3> 
<p><img src="https://images2.imgbox.com/8d/9d/9k4zsUlG_o.png" alt="在这里插入图片描述" width="500"><br> 生成流程分为两个部分：<br> 1、生成正态分布向量后进行若干次采样。<br> 2、进行解码。</p> 
<p>由于DiT只能按照类别进行图片生成，所以无需对文本进行编码，直接传入类别的对应的id（0-1000）即可指定类别。</p> 
<pre><code class="prism language-python"><span class="token comment"># --------------------------------- #</span>
<span class="token comment">#   前处理</span>
<span class="token comment"># --------------------------------- #</span>
<span class="token comment"># 生成latent</span>
latents <span class="token operator">=</span> randn_tensor<span class="token punctuation">(</span>
    shape<span class="token operator">=</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> latent_channels<span class="token punctuation">,</span> latent_size<span class="token punctuation">,</span> latent_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
    generator<span class="token operator">=</span>generator<span class="token punctuation">,</span>
    device<span class="token operator">=</span>self<span class="token punctuation">.</span>_execution_device<span class="token punctuation">,</span>
    dtype<span class="token operator">=</span>self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
latent_model_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>latents<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">if</span> guidance_scale <span class="token operator">&gt;</span> <span class="token number">1</span> <span class="token keyword">else</span> latents

<span class="token comment"># 将输入的label 与 null label进行concat，null label是负向提示类。</span>
class_labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>class_labels<span class="token punctuation">,</span> device<span class="token operator">=</span>self<span class="token punctuation">.</span>_execution_device<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
class_null <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1000</span><span class="token punctuation">]</span> <span class="token operator">*</span> batch_size<span class="token punctuation">,</span> device<span class="token operator">=</span>self<span class="token punctuation">.</span>_execution_device<span class="token punctuation">)</span>
class_labels_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>class_labels<span class="token punctuation">,</span> class_null<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">if</span> guidance_scale <span class="token operator">&gt;</span> <span class="token number">1</span> <span class="token keyword">else</span> class_labels

<span class="token comment"># 设置生成的步数</span>
self<span class="token punctuation">.</span>scheduler<span class="token punctuation">.</span>set_timesteps<span class="token punctuation">(</span>num_inference_steps<span class="token punctuation">)</span>

<span class="token comment"># --------------------------------- #</span>
<span class="token comment">#   扩散生成</span>
<span class="token comment"># --------------------------------- #</span>
<span class="token comment"># 开始N步扩散的循环</span>
<span class="token keyword">for</span> t <span class="token keyword">in</span> self<span class="token punctuation">.</span>progress_bar<span class="token punctuation">(</span>self<span class="token punctuation">.</span>scheduler<span class="token punctuation">.</span>timesteps<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> guidance_scale <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
        half <span class="token operator">=</span> latent_model_input<span class="token punctuation">[</span><span class="token punctuation">:</span> <span class="token builtin">len</span><span class="token punctuation">(</span>latent_model_input<span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">]</span>
        latent_model_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>half<span class="token punctuation">,</span> half<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    latent_model_input <span class="token operator">=</span> self<span class="token punctuation">.</span>scheduler<span class="token punctuation">.</span>scale_model_input<span class="token punctuation">(</span>latent_model_input<span class="token punctuation">,</span> t<span class="token punctuation">)</span>
    
    <span class="token comment"># 处理timesteps</span>
    timesteps <span class="token operator">=</span> t
    <span class="token keyword">if</span> <span class="token keyword">not</span> torch<span class="token punctuation">.</span>is_tensor<span class="token punctuation">(</span>timesteps<span class="token punctuation">)</span><span class="token punctuation">:</span>
        is_mps <span class="token operator">=</span> latent_model_input<span class="token punctuation">.</span>device<span class="token punctuation">.</span><span class="token builtin">type</span> <span class="token operator">==</span> <span class="token string">"mps"</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>timesteps<span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>float32 <span class="token keyword">if</span> is_mps <span class="token keyword">else</span> torch<span class="token punctuation">.</span>float64
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>int32 <span class="token keyword">if</span> is_mps <span class="token keyword">else</span> torch<span class="token punctuation">.</span>int64
        timesteps <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>timesteps<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>latent_model_input<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
    <span class="token keyword">elif</span> <span class="token builtin">len</span><span class="token punctuation">(</span>timesteps<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        timesteps <span class="token operator">=</span> timesteps<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>latent_model_input<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
    <span class="token comment"># broadcast to batch dimension in a way that's compatible with ONNX/Core ML</span>
    timesteps <span class="token operator">=</span> timesteps<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>latent_model_input<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># 将隐含层特征、时间步和种类输入传入到transformers中</span>
    noise_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">(</span>
        latent_model_input<span class="token punctuation">,</span> timestep<span class="token operator">=</span>timesteps<span class="token punctuation">,</span> class_labels<span class="token operator">=</span>class_labels_input
    <span class="token punctuation">)</span><span class="token punctuation">.</span>sample

    <span class="token comment"># perform guidance</span>
    <span class="token keyword">if</span> guidance_scale <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
        <span class="token comment"># 在通道上做分割，取出生图部分的通道</span>
        eps<span class="token punctuation">,</span> rest <span class="token operator">=</span> noise_pred<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>latent_channels<span class="token punctuation">]</span><span class="token punctuation">,</span> noise_pred<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> latent_channels<span class="token punctuation">:</span><span class="token punctuation">]</span>
        cond_eps<span class="token punctuation">,</span> uncond_eps <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>eps<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>eps<span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

        half_eps <span class="token operator">=</span> uncond_eps <span class="token operator">+</span> guidance_scale <span class="token operator">*</span> <span class="token punctuation">(</span>cond_eps <span class="token operator">-</span> uncond_eps<span class="token punctuation">)</span>
        eps <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>half_eps<span class="token punctuation">,</span> half_eps<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

        noise_pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>eps<span class="token punctuation">,</span> rest<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token comment"># 对结果进行分割，取出生图部分的通道</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>config<span class="token punctuation">.</span>out_channels <span class="token operator">//</span> <span class="token number">2</span> <span class="token operator">==</span> latent_channels<span class="token punctuation">:</span>
        model_output<span class="token punctuation">,</span> _ <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>noise_pred<span class="token punctuation">,</span> latent_channels<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        model_output <span class="token operator">=</span> noise_pred

    <span class="token comment"># 通过采样器将这一步噪声施加到隐含层</span>
    latent_model_input <span class="token operator">=</span> self<span class="token punctuation">.</span>scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span>model_output<span class="token punctuation">,</span> t<span class="token punctuation">,</span> latent_model_input<span class="token punctuation">)</span><span class="token punctuation">.</span>prev_sample

<span class="token keyword">if</span> guidance_scale <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
    latents<span class="token punctuation">,</span> _ <span class="token operator">=</span> latent_model_input<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    latents <span class="token operator">=</span> latent_model_input

<span class="token comment"># --------------------------------- #</span>
<span class="token comment">#   后处理</span>
<span class="token comment"># --------------------------------- #</span>
<span class="token comment"># 通过vae进行解码</span>
latents <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>vae<span class="token punctuation">.</span>config<span class="token punctuation">.</span>scaling_factor <span class="token operator">*</span> latents
samples <span class="token operator">=</span> self<span class="token punctuation">.</span>vae<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>latents<span class="token punctuation">)</span><span class="token punctuation">.</span>sample

samples <span class="token operator">=</span> <span class="token punctuation">(</span>samples <span class="token operator">/</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># 转化为float32类别</span>
samples <span class="token operator">=</span> samples<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="1_121"></a>1、采样流程</h4> 
<h5><a id="a_123"></a>a、生成初始噪声</h5> 
<p><img src="https://images2.imgbox.com/26/93/fI5nVXEC_o.png" alt="在这里插入图片描述" width="500"></p> 
<p>在生成初始噪声前介绍一下VAE，VAE是变分自编码器，可以将输入图片进行编码，<strong>一个高宽原本为256x256x3的图片在使用VAE编码后会变成32x32x4</strong>，<strong>这个4是人为设定的，不必纠结为什么不是3</strong>。这个时候我们就使用一个<strong>相对简单的矩阵代替原有的256x256x3的图片</strong>了，传输与存储成本就很低。<strong>在实际要去看的时候，可以对32x32x4的矩阵进行解码，获得256x256x3的图片。</strong></p> 
<p>因此，如果 我们要生成一个256x256x3的图片，那么我们只需要初始化一个32x32x4的隐向量，在隐空间进行扩散即可。在隐空间扩散好后，<strong>再使用解码器就可以生成256x256x3的图像。</strong></p> 
<p>在代码中，我们确实是这么做的，初始噪声的生成函数为randn_tensor，是diffusers自带的一个函数，尽管它写的很长，但实际生成初始噪声的代码只有一行：<br> <img src="https://images2.imgbox.com/65/db/dA5SDV5V_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python">latents <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>shape<span class="token punctuation">,</span> generator<span class="token operator">=</span>generator<span class="token punctuation">,</span> device<span class="token operator">=</span>rand_device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> layout<span class="token operator">=</span>layout<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
</code></pre> 
<p>代码本来位于diffusers的工具文件中，为了方便查看，我将其复制到nets/pipeline.py中。</p> 
<h5><a id="bN_136"></a>b、对噪声进行N次采样</h5> 
<p><img src="https://images2.imgbox.com/54/aa/MZGFQ22Z_o.png" alt="在这里插入图片描述" width="500"></p> 
<p>既然Stable Diffusion是一个不断扩散的过程，那么少不了不断的去噪声，那么怎么去噪声便是一个问题。</p> 
<p>在上一步中，我们已经获得了一个latents，它是一个符合正态分布的向量，我们便从它开始去噪声。</p> 
<p>在代码中，我们有一个对时间步的循环，会不断的将隐含层向量输入到transformers中进行噪声预测，并且一步一步的去噪。</p> 
<pre><code class="prism language-python"><span class="token comment"># --------------------------------- #</span>
<span class="token comment">#   扩散生成</span>
<span class="token comment"># --------------------------------- #</span>
<span class="token comment"># 开始N步扩散的循环</span>
<span class="token keyword">for</span> t <span class="token keyword">in</span> self<span class="token punctuation">.</span>progress_bar<span class="token punctuation">(</span>self<span class="token punctuation">.</span>scheduler<span class="token punctuation">.</span>timesteps<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> guidance_scale <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
        half <span class="token operator">=</span> latent_model_input<span class="token punctuation">[</span><span class="token punctuation">:</span> <span class="token builtin">len</span><span class="token punctuation">(</span>latent_model_input<span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">]</span>
        latent_model_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>half<span class="token punctuation">,</span> half<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    latent_model_input <span class="token operator">=</span> self<span class="token punctuation">.</span>scheduler<span class="token punctuation">.</span>scale_model_input<span class="token punctuation">(</span>latent_model_input<span class="token punctuation">,</span> t<span class="token punctuation">)</span>
    
    <span class="token comment"># 处理timesteps</span>
    timesteps <span class="token operator">=</span> t
    <span class="token keyword">if</span> <span class="token keyword">not</span> torch<span class="token punctuation">.</span>is_tensor<span class="token punctuation">(</span>timesteps<span class="token punctuation">)</span><span class="token punctuation">:</span>
        is_mps <span class="token operator">=</span> latent_model_input<span class="token punctuation">.</span>device<span class="token punctuation">.</span><span class="token builtin">type</span> <span class="token operator">==</span> <span class="token string">"mps"</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>timesteps<span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>float32 <span class="token keyword">if</span> is_mps <span class="token keyword">else</span> torch<span class="token punctuation">.</span>float64
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>int32 <span class="token keyword">if</span> is_mps <span class="token keyword">else</span> torch<span class="token punctuation">.</span>int64
        timesteps <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>timesteps<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>latent_model_input<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
    <span class="token keyword">elif</span> <span class="token builtin">len</span><span class="token punctuation">(</span>timesteps<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        timesteps <span class="token operator">=</span> timesteps<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>latent_model_input<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
    <span class="token comment"># broadcast to batch dimension in a way that's compatible with ONNX/Core ML</span>
    timesteps <span class="token operator">=</span> timesteps<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>latent_model_input<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># 将隐含层特征、时间步和种类输入传入到transformers中</span>
    noise_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">(</span>
        latent_model_input<span class="token punctuation">,</span> timestep<span class="token operator">=</span>timesteps<span class="token punctuation">,</span> class_labels<span class="token operator">=</span>class_labels_input
    <span class="token punctuation">)</span><span class="token punctuation">.</span>sample

    <span class="token comment"># perform guidance</span>
    <span class="token keyword">if</span> guidance_scale <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
        <span class="token comment"># 在通道上做分割，取出生图部分的通道</span>
        eps<span class="token punctuation">,</span> rest <span class="token operator">=</span> noise_pred<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>latent_channels<span class="token punctuation">]</span><span class="token punctuation">,</span> noise_pred<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> latent_channels<span class="token punctuation">:</span><span class="token punctuation">]</span>
        cond_eps<span class="token punctuation">,</span> uncond_eps <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>eps<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>eps<span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

        half_eps <span class="token operator">=</span> uncond_eps <span class="token operator">+</span> guidance_scale <span class="token operator">*</span> <span class="token punctuation">(</span>cond_eps <span class="token operator">-</span> uncond_eps<span class="token punctuation">)</span>
        eps <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>half_eps<span class="token punctuation">,</span> half_eps<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

        noise_pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>eps<span class="token punctuation">,</span> rest<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token comment"># 对结果进行分割，取出生图部分的通道</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>config<span class="token punctuation">.</span>out_channels <span class="token operator">//</span> <span class="token number">2</span> <span class="token operator">==</span> latent_channels<span class="token punctuation">:</span>
        model_output<span class="token punctuation">,</span> _ <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>noise_pred<span class="token punctuation">,</span> latent_channels<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        model_output <span class="token operator">=</span> noise_pred

    <span class="token comment"># 通过采样器将这一步噪声施加到隐含层</span>
    latent_model_input <span class="token operator">=</span> self<span class="token punctuation">.</span>scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span>model_output<span class="token punctuation">,</span> t<span class="token punctuation">,</span> latent_model_input<span class="token punctuation">)</span><span class="token punctuation">.</span>prev_sample
</code></pre> 
<h5><a id="c_194"></a>c、单次采样解析</h5> 
<h6><a id="I_195"></a>I、预测噪声</h6> 
<p>在进行单次采样前，需要首先判断是否有<strong>负向提示类</strong>，如果有，我们需要同时处理<strong>负向提示类</strong>，否则仅仅需要处理<strong>正向提示类</strong>。实际使用的时候一般都有<strong>负向提示类</strong>（效果会好一些），所以默认进入对应的处理过程。</p> 
<p>在处理<strong>负向提示类</strong>时，我们对输入进来的隐向量进行复制，一个属于<strong>正向提示类（0-999）</strong>，一个属于<strong>负向提示类（1000）</strong>。它们是在batch_size维度进行堆叠，二者不会互相影响。然后我们将<strong>正向提示类</strong>和<strong>负向提示类（1000）<strong>也在batch_size维度堆叠。代码中，如果guidance_scale＞1则代表需要</strong>负向提示类</strong>。</p> 
<pre><code class="prism language-python"><span class="token comment"># --------------------------------- #</span>
<span class="token comment">#   前处理</span>
<span class="token comment"># --------------------------------- #</span>
<span class="token comment"># 生成latent</span>
latents <span class="token operator">=</span> randn_tensor<span class="token punctuation">(</span>
    shape<span class="token operator">=</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> latent_channels<span class="token punctuation">,</span> latent_size<span class="token punctuation">,</span> latent_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
    generator<span class="token operator">=</span>generator<span class="token punctuation">,</span>
    device<span class="token operator">=</span>self<span class="token punctuation">.</span>_execution_device<span class="token punctuation">,</span>
    dtype<span class="token operator">=</span>self<span class="token punctuation">.</span>transformer<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
latent_model_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>latents<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">if</span> guidance_scale <span class="token operator">&gt;</span> <span class="token number">1</span> <span class="token keyword">else</span> latents

<span class="token comment"># 将输入的label 与 null label进行concat，null label是负向提示类。</span>
class_labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>class_labels<span class="token punctuation">,</span> device<span class="token operator">=</span>self<span class="token punctuation">.</span>_execution_device<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
class_null <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1000</span><span class="token punctuation">]</span> <span class="token operator">*</span> batch_size<span class="token punctuation">,</span> device<span class="token operator">=</span>self<span class="token punctuation">.</span>_execution_device<span class="token punctuation">)</span>
class_labels_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>class_labels<span class="token punctuation">,</span> class_null<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">if</span> guidance_scale <span class="token operator">&gt;</span> <span class="token number">1</span> <span class="token keyword">else</span> class_labels
</code></pre> 
<p>堆叠完后，我们将隐向量、步数和类别条件一起传入网络中，将结果在bs维度进行使用chunk进行分割。</p> 
<p>因为我们在堆叠时，<strong>正向提示类</strong>放在了前面。因此分割好后，前半部分<code>cond_eps</code>属于利用<strong>正向提示类</strong>得到的，后半部分<code>uncond_eps</code>属于利用<strong>负向提示类</strong>得到的，我们本质上应该<strong>扩大正向提示类的影响，远离负向提示类的影响</strong>。因此，我们使用<code>cond_eps-uncond_eps</code>计算二者的距离，使用scale扩大二者的距离。在uncond_eps基础上，得到最后的隐向量。</p> 
<pre><code class="prism language-python"><span class="token comment"># 堆叠完后，隐向量、步数和prompt条件一起传入网络中，将结果在bs维度进行使用chunk进行分割</span>
e_t_uncond<span class="token punctuation">,</span> e_t <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>apply_model<span class="token punctuation">(</span>x_in<span class="token punctuation">,</span> t_in<span class="token punctuation">,</span> c_in<span class="token punctuation">)</span><span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
e_t <span class="token operator">=</span> e_t_uncond <span class="token operator">+</span> unconditional_guidance_scale <span class="token operator">*</span> <span class="token punctuation">(</span>e_t <span class="token operator">-</span> e_t_uncond<span class="token punctuation">)</span>
</code></pre> 
<p>此时获得的eps就是通过隐向量和提示类共同获得的预测噪声啦。</p> 
<h6><a id="II_226"></a>II、施加噪声</h6> 
<p>在获得噪声后，我们还要将获得的新噪声，按照一定的比例添加到原来的原始噪声上。</p> 
<p>diffusers的代码并没有将施加噪声的代码写在明面上，而是使用采样器的step方法替代，采样流程与DDIM一致，因此直接参考DDIM公式即可，此前，在<a href="https://blog.csdn.net/weixin_44791964/article/details/130588215#II_223">Stable Diffusion相关博文</a>中写到过DDIM公式，可以参考对应博文了解一下。</p> 
<pre><code class="prism language-python">latent_model_input <span class="token operator">=</span> self<span class="token punctuation">.</span>scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span>model_output<span class="token punctuation">,</span> t<span class="token punctuation">,</span> latent_model_input<span class="token punctuation">)</span><span class="token punctuation">.</span>prev_sample
</code></pre> 
<h5><a id="d_234"></a>d、预测噪声过程中的网络结构解析</h5> 
<p>这个部分是DiT与Stable Diffusion最大的不同，DiT将网络结构从Unet转换成了Transformers，</p> 
<h6><a id="iadaLNZero_236"></a>i、adaLN-Zero结构解析</h6> 
<p><strong>Transformers主要做的工作是结合 时间步t 和 类别 计算这一时刻的噪声</strong>。此处的Transformers结构与VIT中的Transformers基本一致，但为了融合时间步t和类别，新增了一个Embed层和adaLN-Zero结构。</p> 
<ul><li>Embed层主要是将输入进来的timestep和label进行向量化。</li><li>adaLN-Zero则是通过全连接对向量化后的timestep和label进行映射，然后分为6个部分，分别作用于DiT的不同阶段用于缩放（scale）、偏置（shift、bias）与门函数（gate）。</li></ul> 
<p>如下是Embed层和adaLN-Zero结构的代码与示意图：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">CombinedTimestepLabelEmbeddings</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_classes<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> class_dropout_prob<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>time_proj <span class="token operator">=</span> Timesteps<span class="token punctuation">(</span>num_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> flip_sin_to_cos<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> downscale_freq_shift<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>timestep_embedder <span class="token operator">=</span> TimestepEmbedding<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> time_embed_dim<span class="token operator">=</span>embedding_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>class_embedder <span class="token operator">=</span> LabelEmbedding<span class="token punctuation">(</span>num_classes<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> class_dropout_prob<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> timestep<span class="token punctuation">,</span> class_labels<span class="token punctuation">,</span> hidden_dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        timesteps_proj <span class="token operator">=</span> self<span class="token punctuation">.</span>time_proj<span class="token punctuation">(</span>timestep<span class="token punctuation">)</span>
        timesteps_emb <span class="token operator">=</span> self<span class="token punctuation">.</span>timestep_embedder<span class="token punctuation">(</span>timesteps_proj<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>hidden_dtype<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># (N, D)</span>

        class_labels <span class="token operator">=</span> self<span class="token punctuation">.</span>class_embedder<span class="token punctuation">(</span>class_labels<span class="token punctuation">)</span>  <span class="token comment"># (N, D)</span>

        conditioning <span class="token operator">=</span> timesteps_emb <span class="token operator">+</span> class_labels  <span class="token comment"># (N, D)</span>

        <span class="token keyword">return</span> conditioning

<span class="token keyword">class</span> <span class="token class-name">AdaLayerNormZero</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Norm layer adaptive layer norm zero (adaLN-Zero).
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> num_embeddings<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>emb <span class="token operator">=</span> CombinedTimestepLabelEmbeddings<span class="token punctuation">(</span>num_embeddings<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>silu <span class="token operator">=</span> nn<span class="token punctuation">.</span>SiLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> <span class="token number">6</span> <span class="token operator">*</span> embedding_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-6</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> timestep<span class="token punctuation">,</span> class_labels<span class="token punctuation">,</span> hidden_dtype<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        emb <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>silu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>emb<span class="token punctuation">(</span>timestep<span class="token punctuation">,</span> class_labels<span class="token punctuation">,</span> hidden_dtype<span class="token operator">=</span>hidden_dtype<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        shift_msa<span class="token punctuation">,</span> scale_msa<span class="token punctuation">,</span> gate_msa<span class="token punctuation">,</span> shift_mlp<span class="token punctuation">,</span> scale_mlp<span class="token punctuation">,</span> gate_mlp <span class="token operator">=</span> emb<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> scale_msa<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> shift_msa<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>
        <span class="token keyword">return</span> x<span class="token punctuation">,</span> gate_msa<span class="token punctuation">,</span> shift_mlp<span class="token punctuation">,</span> scale_mlp<span class="token punctuation">,</span> gate_mlp
</code></pre> 
<p><img src="https://images2.imgbox.com/2f/96/ZwiqkBtx_o.png" alt="在这里插入图片描述" width="500"></p> 
<h6><a id="iipatch_283"></a>ii、patch分块处理</h6> 
<p>在代码中，我们使用一个PatchEmbed类对输入的隐含层向量进行分块，该操作便是VIT中的patchc操作，通过卷积进行类似于下采样的操作，可以减少计算量。<br> <img src="https://images2.imgbox.com/e2/22/WtFFRhe6_o.png" alt="在这里插入图片描述" width="500"><br> 如下为patch分块处理的代码，核心是使用<strong>步长和卷积核大小</strong>一样的Conv2d模块进行处理，由于步长和卷积核大小一致，<strong>每个图片区域的特征提取过程就不会有重叠</strong>。</p> 
<p>我们初始化生成的隐含层向量为32x32x4。在DiT-XL-2中，patch处理的<strong>步长和卷积核大小</strong>为2，通道为1152，在处理完成后，<strong>特征的通道上升，高宽被压缩</strong>，此时我们获得一个16x16x1152的新特征，然后我们将其在长宽上进行平铺，获得一个256x1152的向量，并且加上位置信息。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">PatchEmbed</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""2D Image to Patch Embedding"""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        height<span class="token operator">=</span><span class="token number">224</span><span class="token punctuation">,</span>
        width<span class="token operator">=</span><span class="token number">224</span><span class="token punctuation">,</span>
        patch_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>
        in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
        embed_dim<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">,</span>
        layer_norm<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        flatten<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
        bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        num_patches <span class="token operator">=</span> <span class="token punctuation">(</span>height <span class="token operator">//</span> patch_size<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>width <span class="token operator">//</span> patch_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> flatten
        self<span class="token punctuation">.</span>layer_norm <span class="token operator">=</span> layer_norm

        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>
            in_channels<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span>patch_size<span class="token punctuation">,</span> patch_size<span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>patch_size<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias
        <span class="token punctuation">)</span>
        <span class="token keyword">if</span> layer_norm<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embed_dim<span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-6</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>norm <span class="token operator">=</span> <span class="token boolean">None</span>

        pos_embed <span class="token operator">=</span> get_2d_sincos_pos_embed<span class="token punctuation">(</span>embed_dim<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>num_patches<span class="token operator">**</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"pos_embed"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>pos_embed<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> latent<span class="token punctuation">)</span><span class="token punctuation">:</span>
        latent <span class="token operator">=</span> self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>latent<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">:</span>
            latent <span class="token operator">=</span> latent<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># BCHW -&gt; BNC</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>layer_norm<span class="token punctuation">:</span>
            latent <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>latent<span class="token punctuation">)</span>
        <span class="token keyword">return</span> latent <span class="token operator">+</span> self<span class="token punctuation">.</span>pos_embed
</code></pre> 
<h6><a id="iiiTransformer_330"></a>iii、Transformer特征提取</h6> 
<p>此后，我们将向量传入Transformer中进行特征提取，对应图中的DiT Block。</p> 
<p><strong>256x1152的特征会通过图中红框的部分，而时间步t 和 类别会通过途中绿框的部分。</strong></p> 
<p>红框部分的结构，<strong>除了缩放（scale）、偏置（shift、bias）与门函数（gate，对应图中的α，代码中是gate但图中写scale）外</strong>，其它部分与VIT一模一样，可参考博文<a href="https://blog.csdn.net/weixin_44791964/article/details/122637701">VIT结构解析</a>进行了解，主要工作的模块是Self-Attention和Pointwise Feedforward（MLP）。这两个模块的输入和输出均为256x1152的特征。</p> 
<p>而缩放（scale）、偏置（shift、bias）与门函数（gate）分别对应了图中的γ、β和α。通过adaLN-Zero结构获得，γ、β分别在 Self-Attention和Pointwise Feedforward 的处理前 进行特征的 <strong>缩放与偏置</strong> ，而Pointwise Feedforward则在 Self-Attention和Pointwise Feedforward 的处理后 进行特征的 <strong>缩放</strong>。在代码中我添加了中文注释，方便读者<strong>区分添加缩放、偏置和门函数的位置</strong>。</p> 
<p>DiT Block的输入和输出特征均为256x1152。<br> <img src="https://images2.imgbox.com/8b/9c/bLESNMfI_o.png" alt="在这里插入图片描述" width="500"></p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">BasicTransformerBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
        num_attention_heads<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
        attention_head_dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
        dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span>
        cross_attention_dim<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        activation_fn<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">"geglu"</span><span class="token punctuation">,</span>
        num_embeds_ada_norm<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        attention_bias<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        only_cross_attention<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        double_self_attention<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        upcast_attention<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        norm_elementwise_affine<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
        norm_type<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">"layer_norm"</span><span class="token punctuation">,</span>
        final_dropout<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">,</span>
        attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        encoder_hidden_states<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        encoder_attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        timestep<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        cross_attention_kwargs<span class="token punctuation">:</span> Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Any<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        class_labels<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Notice that normalization is always applied before the real computation in the following blocks.</span>
        <span class="token comment"># 1. Self-Attention</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_ada_layer_norm<span class="token punctuation">:</span>
            norm_hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span> timestep<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>use_ada_layer_norm_zero<span class="token punctuation">:</span>
            <span class="token comment"># 在norm1中，已经进行了输入特征的缩放与偏置</span>
            norm_hidden_states<span class="token punctuation">,</span> gate_msa<span class="token punctuation">,</span> shift_mlp<span class="token punctuation">,</span> scale_mlp<span class="token punctuation">,</span> gate_mlp <span class="token operator">=</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>
                hidden_states<span class="token punctuation">,</span> timestep<span class="token punctuation">,</span> class_labels<span class="token punctuation">,</span> hidden_dtype<span class="token operator">=</span>hidden_states<span class="token punctuation">.</span>dtype
            <span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            norm_hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>

        cross_attention_kwargs <span class="token operator">=</span> cross_attention_kwargs <span class="token keyword">if</span> cross_attention_kwargs <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>

        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>attn1<span class="token punctuation">(</span>
            norm_hidden_states<span class="token punctuation">,</span>
            encoder_hidden_states<span class="token operator">=</span>encoder_hidden_states <span class="token keyword">if</span> self<span class="token punctuation">.</span>only_cross_attention <span class="token keyword">else</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
            attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
            <span class="token operator">**</span>cross_attention_kwargs<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        <span class="token comment"># 在self attention后，再次进行了特征的缩放（gate）</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_ada_layer_norm_zero<span class="token punctuation">:</span>
            attn_output <span class="token operator">=</span> gate_msa<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> attn_output
        hidden_states <span class="token operator">=</span> attn_output <span class="token operator">+</span> hidden_states

        <span class="token comment"># 2. Cross-Attention</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>attn2 <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            norm_hidden_states <span class="token operator">=</span> <span class="token punctuation">(</span>
                self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span> timestep<span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_ada_layer_norm <span class="token keyword">else</span> self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
            <span class="token punctuation">)</span>

            attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>attn2<span class="token punctuation">(</span>
                norm_hidden_states<span class="token punctuation">,</span>
                encoder_hidden_states<span class="token operator">=</span>encoder_hidden_states<span class="token punctuation">,</span>
                attention_mask<span class="token operator">=</span>encoder_attention_mask<span class="token punctuation">,</span>
                <span class="token operator">**</span>cross_attention_kwargs<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
            hidden_states <span class="token operator">=</span> attn_output <span class="token operator">+</span> hidden_states

        <span class="token comment"># 3. Feed-forward</span>
        norm_hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>norm3<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>

        <span class="token comment"># 在mlp前，进行了输入特征的缩放与偏置</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_ada_layer_norm_zero<span class="token punctuation">:</span>
            norm_hidden_states <span class="token operator">=</span> norm_hidden_states <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> scale_mlp<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> shift_mlp<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>_chunk_size <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token comment"># "feed_forward_chunk_size" can be used to save memory</span>
            <span class="token keyword">if</span> norm_hidden_states<span class="token punctuation">.</span>shape<span class="token punctuation">[</span>self<span class="token punctuation">.</span>_chunk_dim<span class="token punctuation">]</span> <span class="token operator">%</span> self<span class="token punctuation">.</span>_chunk_size <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
                <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
                    <span class="token string-interpolation"><span class="token string">f"`hidden_states` dimension to be chunked: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>norm_hidden_states<span class="token punctuation">.</span>shape<span class="token punctuation">[</span>self<span class="token punctuation">.</span>_chunk_dim<span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string"> has to be divisible by chunk size: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>self<span class="token punctuation">.</span>_chunk_size<span class="token punctuation">}</span></span><span class="token string">. Make sure to set an appropriate `chunk_size` when calling `unet.enable_forward_chunking`."</span></span>
                <span class="token punctuation">)</span>

            num_chunks <span class="token operator">=</span> norm_hidden_states<span class="token punctuation">.</span>shape<span class="token punctuation">[</span>self<span class="token punctuation">.</span>_chunk_dim<span class="token punctuation">]</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>_chunk_size
            ff_output <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>
                <span class="token punctuation">[</span>self<span class="token punctuation">.</span>ff<span class="token punctuation">(</span>hid_slice<span class="token punctuation">)</span> <span class="token keyword">for</span> hid_slice <span class="token keyword">in</span> norm_hidden_states<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span>num_chunks<span class="token punctuation">,</span> dim<span class="token operator">=</span>self<span class="token punctuation">.</span>_chunk_dim<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                dim<span class="token operator">=</span>self<span class="token punctuation">.</span>_chunk_dim<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            ff_output <span class="token operator">=</span> self<span class="token punctuation">.</span>ff<span class="token punctuation">(</span>norm_hidden_states<span class="token punctuation">)</span>

        <span class="token comment"># 在mlp后，再次进行了特征的缩放（gate）</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_ada_layer_norm_zero<span class="token punctuation">:</span>
            ff_output <span class="token operator">=</span> gate_mlp<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> ff_output

        hidden_states <span class="token operator">=</span> ff_output <span class="token operator">+</span> hidden_states

        <span class="token keyword">return</span> hidden_states
</code></pre> 
<h6><a id="iv_442"></a>iv、上采样</h6> 
<p>虽然这个部分学名可能不叫上采样，但是我觉得用上采样来描述它还是比较合适的，因为我们前面做过patch分块处理，所以隐含层的高宽被压缩，而这一步，则是将隐含层的高宽再还原回去。</p> 
<p>在这里我们会对256x1152进行两次全连接+一次LayerNorm，两次全连接的神经元个数分别为2304和<code>patch_size * patch_size * out_channels</code>。第一次全连接目的是扩宽通道数，第二次全链接则是还原高宽。两次全连接后，在DiT-XL-2中，out_channels为8（8可拆分为4 + 4，前面的4用于直接预测噪声，后面的4用于根据<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
         
         
           t 
          
         
           − 
          
         
           1 
          
         
        
       
      
        x_{t-1} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6389em; vertical-align: -0.2083em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span></span></span></span></span>均值和方差计算KL散度），特征层的shape从256x1152变为256x32。</p> 
<p>然后我们会进行一系列shape变换，首先将256x1152变为16x16x2x2x8，然后进行转置变为8x16x2x16x2，然后还原高宽变为8x32x32。此时上采样结束。该部分对应了图中的Linear And Reshape。<br> <img src="https://images2.imgbox.com/9c/9e/4xcONMOK_o.png" alt="在这里插入图片描述" width="500"></p> 
<p>上采样代码如下所示：</p> 
<pre><code class="prism language-python"><span class="token comment"># 3. Output</span>
conditioning <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer_blocks<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>norm1<span class="token punctuation">.</span>emb<span class="token punctuation">(</span>
    timestep<span class="token punctuation">,</span> class_labels<span class="token punctuation">,</span> hidden_dtype<span class="token operator">=</span>hidden_states<span class="token punctuation">.</span>dtype
<span class="token punctuation">)</span>
shift<span class="token punctuation">,</span> scale <span class="token operator">=</span> self<span class="token punctuation">.</span>proj_out_1<span class="token punctuation">(</span>F<span class="token punctuation">.</span>silu<span class="token punctuation">(</span>conditioning<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>norm_out<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> scale<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> shift<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>
hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>proj_out_2<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>

<span class="token comment"># unpatchify</span>
height <span class="token operator">=</span> width <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>hidden_states<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">**</span> <span class="token number">0.5</span><span class="token punctuation">)</span>
hidden_states <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>
    shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> height<span class="token punctuation">,</span> width<span class="token punctuation">,</span> self<span class="token punctuation">.</span>patch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>patch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>out_channels<span class="token punctuation">)</span>
<span class="token punctuation">)</span>
hidden_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"nhwpqc-&gt;nchpwq"</span><span class="token punctuation">,</span> hidden_states<span class="token punctuation">)</span>
output <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>
    shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>out_channels<span class="token punctuation">,</span> height <span class="token operator">*</span> self<span class="token punctuation">.</span>patch_size<span class="token punctuation">,</span> width <span class="token operator">*</span> self<span class="token punctuation">.</span>patch_size<span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="3_470"></a>3、隐空间解码生成图片</h4> 
<p>通过上述步骤，已经可以多次采样获得结果，然后我们便可以通过隐空间解码生成图片。</p> 
<p>隐空间解码生成图片的过程非常简单，将上文多次采样后的结果，使用vae的decode方法即可生成图片。</p> 
<pre><code class="prism language-python"><span class="token comment"># --------------------------------- #</span>
<span class="token comment">#   后处理</span>
<span class="token comment"># --------------------------------- #</span>
<span class="token comment"># 通过vae进行解码</span>
latents <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>vae<span class="token punctuation">.</span>config<span class="token punctuation">.</span>scaling_factor <span class="token operator">*</span> latents
samples <span class="token operator">=</span> self<span class="token punctuation">.</span>vae<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>latents<span class="token punctuation">)</span><span class="token punctuation">.</span>sample

samples <span class="token operator">=</span> <span class="token punctuation">(</span>samples <span class="token operator">/</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># 转化为float32类别</span>
samples <span class="token operator">=</span> samples<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h2><a id="_487"></a>类别到图像预测过程代码</h2> 
<p>整体预测代码如下：</p> 
<pre><code class="prism language-python">
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> json
<span class="token keyword">import</span> os
<span class="token keyword">from</span> diffusers <span class="token keyword">import</span> DPMSolverMultistepScheduler<span class="token punctuation">,</span> AutoencoderKL

<span class="token keyword">from</span> nets<span class="token punctuation">.</span>transformer_2d <span class="token keyword">import</span> Transformer2DModel
<span class="token keyword">from</span> nets<span class="token punctuation">.</span>pipeline <span class="token keyword">import</span> DiTPipeline

<span class="token comment"># 模型路径</span>
model_path <span class="token operator">=</span> <span class="token string">"model_data/DiT-XL-2-256"</span>

<span class="token comment"># 初始化DiT的各个组件</span>
scheduler <span class="token operator">=</span> DPMSolverMultistepScheduler<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_path<span class="token punctuation">,</span> subfolder<span class="token operator">=</span><span class="token string">"scheduler"</span><span class="token punctuation">)</span>
transformer <span class="token operator">=</span> Transformer2DModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_path<span class="token punctuation">,</span> subfolder<span class="token operator">=</span><span class="token string">"transformer"</span><span class="token punctuation">)</span>
vae <span class="token operator">=</span> AutoencoderKL<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_path<span class="token punctuation">,</span> subfolder<span class="token operator">=</span><span class="token string">"vae"</span><span class="token punctuation">)</span>
id2label <span class="token operator">=</span> json<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token builtin">open</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>model_path<span class="token punctuation">,</span> <span class="token string">"model_index.json"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'id2label'</span><span class="token punctuation">]</span>

<span class="token comment"># 初始化DiT的Pipeline</span>
pipe <span class="token operator">=</span> DiTPipeline<span class="token punctuation">(</span>scheduler<span class="token operator">=</span>scheduler<span class="token punctuation">,</span> transformer<span class="token operator">=</span>transformer<span class="token punctuation">,</span> vae<span class="token operator">=</span>vae<span class="token punctuation">,</span> id2label<span class="token operator">=</span>id2label<span class="token punctuation">)</span>
pipe <span class="token operator">=</span> pipe<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>

<span class="token comment"># imagenet种类 对应的 名称</span>
words <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"white shark"</span><span class="token punctuation">,</span> <span class="token string">"umbrella"</span><span class="token punctuation">]</span>
<span class="token comment"># 获得imagenet对应的ids</span>
class_ids <span class="token operator">=</span> pipe<span class="token punctuation">.</span>get_label_ids<span class="token punctuation">(</span>words<span class="token punctuation">)</span>
<span class="token comment"># 设置seed</span>
generator <span class="token operator">=</span> torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">42</span><span class="token punctuation">)</span>

<span class="token comment"># pipeline前传</span>
output <span class="token operator">=</span> pipe<span class="token punctuation">(</span>class_labels<span class="token operator">=</span>class_ids<span class="token punctuation">,</span> num_inference_steps<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>generator<span class="token punctuation">)</span>

<span class="token comment"># 保存图片</span>
<span class="token keyword">for</span> index<span class="token punctuation">,</span> image <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>output<span class="token punctuation">.</span>images<span class="token punctuation">)</span><span class="token punctuation">:</span>
    image<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"output-</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>index<span class="token punctuation">}</span></span><span class="token string">.png"</span></span><span class="token punctuation">)</span>

</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ede4b1a4d8c37ad930ee29ca5f6f3651/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">java hutool工具类处理json的常用方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c981badc2e1072c3c797d0b5e9bea14f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">numpy，matplotlib，python的版本对应（implement_array_function method already has a docstring报错）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>