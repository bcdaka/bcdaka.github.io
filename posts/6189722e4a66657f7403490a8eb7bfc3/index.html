<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【AIGC调研系列】GPT-4O比GPT-4强在哪 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/6189722e4a66657f7403490a8eb7bfc3/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【AIGC调研系列】GPT-4O比GPT-4强在哪">
  <meta property="og:description" content="GPT-4O与GPT-4在多个方面有所不同，主要体现在性能、响应速度、成本效益以及多模态处理能力上。
性能提升：GPT-4O在文本分析、推理和编程能力上相较于GPT-4有显著提升。特别是在视觉和音频理解能力上，GPT-4O表现出更优越的性能[3][8][10]。响应速度：GPT-4O的响应速度是GPT-4 Turbo的两倍，具体到音频输入的响应时间，最短可达232毫秒，平均320毫秒，这使得它在实时交互方面具有明显优势[1][2][5]。成本效益：GPT-4O的使用成本比GPT-4低50%，这对于开发者来说是一个重大的吸引力，因为它降低了实施成本，同时提高了使用率限制[2][3][15]。多模态处理能力：GPT-4O支持文本、音频、图像任意组合的输入，并能以同样的方式输出。这表明GPT-4O不仅能够处理传统的文本数据，还能理解和生成与之相关的音频和视觉内容[4][12][16]。 GPT-4O在保持与GPT-4相当的智能水平的同时，通过技术改进和优化，在响应速度、成本效益以及多模态处理能力上都有显著的提升。这些改进使得GPT-4O在实际应用中更具竞争力，尤其是在需要快速响应和高效处理多种数据类型的场景中[7][18][27]。
GPT-4O在视觉和音频理解能力上的具体表现和技术细节是什么？ GPT-4O在视觉和音频理解能力上的具体表现和技术细节如下：
视觉理解能力： GPT-4O能够理解和处理图像及其内容，包括图像中的文本信息。这意味着它可以同时处理和理解图像以及图像中包含的文本内容[30]。它具备原生多模态能力，不仅能处理文本、音频和图像任意组合的输入，还能对这些输入进行深入理解[31]。GPT-4O的视觉功能还包括阅读网页并转录图像和视频中的内容。其训练数据包括渲染的LaTeX/文本、网页截图、YouTube视频采样帧等[34]。此外，GPT-4O还能分析和理解图像内容，提供描述、识别对象，甚至解释场景，为图像分类、对象检测和视觉内容审核提供了可能性[37]。音频理解能力： GPT-4O在响应速度上有显著提升，最快232毫秒响应音频输入，平均响应时间为320毫秒，与人类在对话中的响应速度相当[32][33]。它能够整合处理视觉和文本信息，这暗示了其在音频方面也具有高度的整合和理解能力[30]。在训练数据方面，GPT-4O使用了YouTube视频采样帧，并运行Whisper（OpenAI的语音识别大模型）来获得transcript，这表明其在音频理解方面采用了高级的技术手段[34]。 GPT-4O在视觉和音频理解能力上的表现体现在其强大的多模态推理能力、快速的响应速度以及对复杂数据集的处理能力上。
GPT-4O的响应速度提升是如何实现的，与GPT-4 Turbo相比有哪些关键技术或方法？ GPT-4o的响应速度提升主要通过以下几个关键技术或方法实现：
多模式支持：GPT-4o支持多种输入和输出模式，包括文本、图像等，这使得其在处理非英语语言文本方面具有显著优势[41]。API性能提升：与GPT-4 Turbo相比，GPT-4o在API中提供了更快的速度和更低的成本，同时还能处理速率限制高出5倍的情况[40]。视觉和音频表现：GPT-4o在视频和音频方面的表现尤为出色，这表明它在处理这些特定类型的内容时具有较强的能力[42]。类人响应速度：GPT-4o能够在短至0.23秒（平均为0.32秒）的时间内响应音频输入，其响应速度与人类相似，这一点在与GPT-3.5对话时尤为明显[43][46]。成本效益：GPT-4o不仅在速度上有所提升，还在成本上实现了50%的降低，这使得其在经济性方面也具有优势[41]。多语言支持：GPT-4o能够处理50种不同的语言，这进一步证明了其在全球范围内的应用潜力[47]。 GPT-4O使用成本降低50%的具体原因是什么，与GPT-4在性能和功能上有哪些显著差异？ GPT-4O使用成本降低50%的具体原因主要包括以下几点：
技术优化和规模经济：通过在一个模型中集成所有模态，GPT-4O实现了更精细的多模态整合，这不仅提高了效率，还降低了成本[50][51][54]。此外，流式传输的神经网络也为进一步控制成本提供了可能[53]。性能提升与速率限制提高：与GPT-4 Turbo相比，GPT-4O的速度提高了2倍，同时速率限制提高了5倍，这直接导致了成本的大幅降低[52]。 在性能和功能上，GPT-4O与GPT-4的显著差异包括：
多模态支持：GPT-4O能够识别物体并根据视觉做出快速响应和回答，这表明它具有更强的逻辑推理能力[49]。此外，它还能感知情绪、语气、表情，实现更自然的人机交互[50][51]。跨文本、音频和视频的实时推理能力：GPT-4O可以跨文本、音频和视频进行实时推理，这使得它在非英语文本上的性能显著提高[55]。更快的生成速度：GPT-4O的生成速度比GPT-4 Turbo快2倍，这意味着用户可以更快地获得回答或内容[49]。 GPT-4O在成本和功能上的优势主要来源于其技术优化、多模态整合以及对大规模数据处理的能力提升。
GPT-4O如何处理文本、音频、图像的多模态输入，其技术原理和实现方式是什么？ GPT-4O处理文本、音频、图像的多模态输入主要依赖于其作为一个原生多模态模型的特性。这种模型能够直接理解和处理这些不同类型的数据，而无需将它们转换为同一种格式，如先将音频转录为文本再进行处理[61][63]。GPT-4O通过端到端的方式处理所有输入和输出，这意味着所有的文本、视觉和音频数据都由同一个神经网络处理[62][66]。
具体来说，GPT-4O利用其强大的语言模型能力，对文本数据进行处理。对于音频输入，它能够直接映射音频到音频，实现低延迟的实时交互[68]。而对于图像输入，虽然具体的技术细节未在证据中明确描述，但可以推测，GPT-4O同样能够直接理解图像内容，并与文本和音频数据一起被模型处理。
此外，GPT-4O在处理多模态输入时展现出了天然的多模态特性，能够处理文本、音频、图像任何组合的输入和输出，这一点是向更自然人机交互迈进的重大步骤[67]。这种能力不仅提升了人机交互的效率和自然度，也使得GPT-4O在速度和成本方面相比前代产品有了显著的提升[59]。
总结来说，GPT-4O通过其作为原生多模态模型的设计，以及端到端的处理方式，实现了对文本、音频、图像等多种模态输入的高效处理。
在实际应用场景中，GPT-4O的多模态处理能力对用户体验有何影响，是否有具体的案例研究或反馈？ GPT-4O的多模态处理能力对用户体验产生了显著的正面影响。首先，GPT-4O能够处理文本、音频和图像等多种模态的输入，这使得它能够提供更自然、流畅的交互体验[77]。例如，它可以在最快232毫秒内响应音频输入，与人类对话的反应速度基本一致，这相比之前的模型有了明显的提升[78]。
此外，GPT-4O的多模态能力还被应用于特定的实际场景中，如帮助盲人通过实时视觉和语音能力了解周围环境并做出决策[76]。这种应用不仅展示了GPT-4O技术的先进性，也极大地改善了目标用户群体的生活质量。
从商业角度来看，GPT-4O的多模态处理能力也为企业带来了便利，例如在智能终端Agent和机器人等领域的应用，有望提升用户体验[69]。此外，GPT-4O的易用性大幅提升，语音响应延迟大幅降低，这些都有助于增强其在各行各业中的应用潜力[75]。
总结来说，GPT-4O的多模态处理能力不仅提高了用户交互的自然度和流畅性，还通过具体的应用案例，如辅助盲人“看见”世界，直接改善了用户的生活质量，同时也为企业提供了更多的商业机会和效率提升。
参考资料 1. GPT-4o API 全新版本发布：性能更高，价格更优惠
2. We tried out GPT-4o, and it&#39;s so much faster than GPT-4 - XDA Developers
3. 如何评价 OpenAI 2024 Spring 发布的支持实时语音对话的模型 GPT-4O? - 知乎
4. 基本就是AGI？GPT-4o“它”来了
5. 一文总结OpenAI新发布的GPT-4o的能力和信息，免费ChatGPT用户也可以使用的GPT-4模型，开发者接口价格下降一半，数学推理评测 ...
6. GPT-4o大模型凭性能翻倍，揭示AI未来新篇章">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-15T11:48:57+08:00">
    <meta property="article:modified_time" content="2024-05-15T11:48:57+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【AIGC调研系列】GPT-4O比GPT-4强在哪</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>GPT-4O与GPT-4在多个方面有所不同，主要体现在性能、响应速度、成本效益以及多模态处理能力上。</p> 
<ol><li><strong>性能提升</strong>：GPT-4O在文本分析、推理和编程能力上相较于GPT-4有显著提升。特别是在视觉和音频理解能力上，GPT-4O表现出更优越的性能[3][8][10]。</li><li><strong>响应速度</strong>：GPT-4O的响应速度是GPT-4 Turbo的两倍，具体到音频输入的响应时间，最短可达232毫秒，平均320毫秒，这使得它在实时交互方面具有明显优势[1][2][5]。</li><li><strong>成本效益</strong>：GPT-4O的使用成本比GPT-4低50%，这对于开发者来说是一个重大的吸引力，因为它降低了实施成本，同时提高了使用率限制[2][3][15]。</li><li><strong>多模态处理能力</strong>：GPT-4O支持文本、音频、图像任意组合的输入，并能以同样的方式输出。这表明GPT-4O不仅能够处理传统的文本数据，还能理解和生成与之相关的音频和视觉内容[4][12][16]。</li></ol> 
<p>GPT-4O在保持与GPT-4相当的智能水平的同时，通过技术改进和优化，在响应速度、成本效益以及多模态处理能力上都有显著的提升。这些改进使得GPT-4O在实际应用中更具竞争力，尤其是在需要快速响应和高效处理多种数据类型的场景中[7][18][27]。</p> 
<h3>GPT-4O在视觉和音频理解能力上的具体表现和技术细节是什么？</h3> 
<p>GPT-4O在视觉和音频理解能力上的具体表现和技术细节如下：</p> 
<ol><li><strong>视觉理解能力</strong>： 
  <ol><li>GPT-4O能够理解和处理图像及其内容，包括图像中的文本信息。这意味着它可以同时处理和理解图像以及图像中包含的文本内容[30]。</li><li>它具备原生多模态能力，不仅能处理文本、音频和图像任意组合的输入，还能对这些输入进行深入理解[31]。</li><li>GPT-4O的视觉功能还包括阅读网页并转录图像和视频中的内容。其训练数据包括渲染的LaTeX/文本、网页截图、YouTube视频采样帧等[34]。</li><li>此外，GPT-4O还能分析和理解图像内容，提供描述、识别对象，甚至解释场景，为图像分类、对象检测和视觉内容审核提供了可能性[37]。</li></ol></li><li><strong>音频理解能力</strong>： 
  <ol><li>GPT-4O在响应速度上有显著提升，最快232毫秒响应音频输入，平均响应时间为320毫秒，与人类在对话中的响应速度相当[32][33]。</li><li>它能够整合处理视觉和文本信息，这暗示了其在音频方面也具有高度的整合和理解能力[30]。</li><li>在训练数据方面，GPT-4O使用了YouTube视频采样帧，并运行Whisper（OpenAI的语音识别大模型）来获得transcript，这表明其在音频理解方面采用了高级的技术手段[34]。</li></ol></li></ol> 
<p>GPT-4O在视觉和音频理解能力上的表现体现在其强大的多模态推理能力、快速的响应速度以及对复杂数据集的处理能力上。</p> 
<h3>GPT-4O的响应速度提升是如何实现的，与GPT-4 Turbo相比有哪些关键技术或方法？</h3> 
<p>GPT-4o的响应速度提升主要通过以下几个关键技术或方法实现：</p> 
<ol><li><strong>多模式支持</strong>：GPT-4o支持多种输入和输出模式，包括文本、图像等，这使得其在处理非英语语言文本方面具有显著优势[41]。</li><li><strong>API性能提升</strong>：与GPT-4 Turbo相比，GPT-4o在API中提供了更快的速度和更低的成本，同时还能处理速率限制高出5倍的情况[40]。</li><li><strong>视觉和音频表现</strong>：GPT-4o在视频和音频方面的表现尤为出色，这表明它在处理这些特定类型的内容时具有较强的能力[42]。</li><li><strong>类人响应速度</strong>：GPT-4o能够在短至0.23秒（平均为0.32秒）的时间内响应音频输入，其响应速度与人类相似，这一点在与GPT-3.5对话时尤为明显[43][46]。</li><li><strong>成本效益</strong>：GPT-4o不仅在速度上有所提升，还在成本上实现了50%的降低，这使得其在经济性方面也具有优势[41]。</li><li><strong>多语言支持</strong>：GPT-4o能够处理50种不同的语言，这进一步证明了其在全球范围内的应用潜力[47]。</li></ol> 
<h3>GPT-4O使用成本降低50%的具体原因是什么，与GPT-4在性能和功能上有哪些显著差异？</h3> 
<p>GPT-4O使用成本降低50%的具体原因主要包括以下几点：</p> 
<ol><li><strong>技术优化和规模经济</strong>：通过在一个模型中集成所有模态，GPT-4O实现了更精细的多模态整合，这不仅提高了效率，还降低了成本[50][51][54]。此外，流式传输的神经网络也为进一步控制成本提供了可能[53]。</li><li><strong>性能提升与速率限制提高</strong>：与GPT-4 Turbo相比，GPT-4O的速度提高了2倍，同时速率限制提高了5倍，这直接导致了成本的大幅降低[52]。</li></ol> 
<p>在性能和功能上，GPT-4O与GPT-4的显著差异包括：</p> 
<ol><li><strong>多模态支持</strong>：GPT-4O能够识别物体并根据视觉做出快速响应和回答，这表明它具有更强的逻辑推理能力[49]。此外，它还能感知情绪、语气、表情，实现更自然的人机交互[50][51]。</li><li><strong>跨文本、音频和视频的实时推理能力</strong>：GPT-4O可以跨文本、音频和视频进行实时推理，这使得它在非英语文本上的性能显著提高[55]。</li><li><strong>更快的生成速度</strong>：GPT-4O的生成速度比GPT-4 Turbo快2倍，这意味着用户可以更快地获得回答或内容[49]。</li></ol> 
<p>GPT-4O在成本和功能上的优势主要来源于其技术优化、多模态整合以及对大规模数据处理的能力提升。</p> 
<h3>GPT-4O如何处理文本、音频、图像的多模态输入，其技术原理和实现方式是什么？</h3> 
<p>GPT-4O处理文本、音频、图像的多模态输入主要依赖于其作为一个原生多模态模型的特性。这种模型能够直接理解和处理这些不同类型的数据，而无需将它们转换为同一种格式，如先将音频转录为文本再进行处理[61][63]。GPT-4O通过端到端的方式处理所有输入和输出，这意味着所有的文本、视觉和音频数据都由同一个神经网络处理[62][66]。</p> 
<p>具体来说，GPT-4O利用其强大的语言模型能力，对文本数据进行处理。对于音频输入，它能够直接映射音频到音频，实现低延迟的实时交互[68]。而对于图像输入，虽然具体的技术细节未在证据中明确描述，但可以推测，GPT-4O同样能够直接理解图像内容，并与文本和音频数据一起被模型处理。</p> 
<p>此外，GPT-4O在处理多模态输入时展现出了天然的多模态特性，能够处理文本、音频、图像任何组合的输入和输出，这一点是向更自然人机交互迈进的重大步骤[67]。这种能力不仅提升了人机交互的效率和自然度，也使得GPT-4O在速度和成本方面相比前代产品有了显著的提升[59]。</p> 
<p>总结来说，GPT-4O通过其作为原生多模态模型的设计，以及端到端的处理方式，实现了对文本、音频、图像等多种模态输入的高效处理。</p> 
<h3>在实际应用场景中，GPT-4O的多模态处理能力对用户体验有何影响，是否有具体的案例研究或反馈？</h3> 
<p>GPT-4O的多模态处理能力对用户体验产生了显著的正面影响。首先，GPT-4O能够处理文本、音频和图像等多种模态的输入，这使得它能够提供更自然、流畅的交互体验[77]。例如，它可以在最快232毫秒内响应音频输入，与人类对话的反应速度基本一致，这相比之前的模型有了明显的提升[78]。</p> 
<p>此外，GPT-4O的多模态能力还被应用于特定的实际场景中，如帮助盲人通过实时视觉和语音能力了解周围环境并做出决策[76]。这种应用不仅展示了GPT-4O技术的先进性，也极大地改善了目标用户群体的生活质量。</p> 
<p>从商业角度来看，GPT-4O的多模态处理能力也为企业带来了便利，例如在智能终端Agent和机器人等领域的应用，有望提升用户体验[69]。此外，GPT-4O的易用性大幅提升，语音响应延迟大幅降低，这些都有助于增强其在各行各业中的应用潜力[75]。</p> 
<p>总结来说，GPT-4O的多模态处理能力不仅提高了用户交互的自然度和流畅性，还通过具体的应用案例，如辅助盲人“看见”世界，直接改善了用户的生活质量，同时也为企业提供了更多的商业机会和效率提升。</p> 
<h3>参考资料</h3> 
<p><a href="https://m.nowcoder.com/discuss/620209721458302976?urlSource=home-api" rel="nofollow" title="1. GPT-4o API 全新版本发布：性能更高，价格更优惠">1. GPT-4o API 全新版本发布：性能更高，价格更优惠</a></p> 
<p><a href="https://www.xda-developers.com/gpt-4o-tested-faster-than-gpt-4/" rel="nofollow" title="2. We tried out GPT-4o, and it's so much faster than GPT-4 - XDA Developers">2. We tried out GPT-4o, and it's so much faster than GPT-4 - XDA Developers</a></p> 
<p><a href="https://www.zhihu.com/question/655916303" rel="nofollow" title="3. 如何评价 OpenAI 2024 Spring 发布的支持实时语音对话的模型 GPT-4O? - 知乎">3. 如何评价 OpenAI 2024 Spring 发布的支持实时语音对话的模型 GPT-4O? - 知乎</a></p> 
<p><a href="https://www.51cto.com/article/788381.html" rel="nofollow" title="4. 基本就是AGI？GPT-4o“它”来了">4. 基本就是AGI？GPT-4o“它”来了</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/697591062" rel="nofollow" title="5. 一文总结OpenAI新发布的GPT-4o的能力和信息，免费ChatGPT用户也可以使用的GPT-4模型，开发者接口价格下降一半，数学推理评测 ...">5. 一文总结OpenAI新发布的GPT-4o的能力和信息，免费ChatGPT用户也可以使用的GPT-4模型，开发者接口价格下降一半，数学推理评测 ...</a></p> 
<p><a href="https://m.163.com/dy/article/J25IQ24H0519QIKK.html?spss=adap_pc" rel="nofollow" title="6. GPT-4o大模型凭性能翻倍，揭示AI未来新篇章">6. GPT-4o大模型凭性能翻倍，揭示AI未来新篇章</a></p> 
<p><a href="https://www.zhihu.com/question/655952351" rel="nofollow" title="7. GPT-4o和GPT-4有什么区别？ - 知乎">7. GPT-4o和GPT-4有什么区别？ - 知乎</a></p> 
<p><a href="https://www.53ai.com/news/qianyanjishu/2139.html" rel="nofollow" title="8. Hello GPT-4o - 大模型知识库">8. Hello GPT-4o - 大模型知识库</a></p> 
<p><a href="https://blog.csdn.net/DeepAIedu/article/details/138864442" title="9. 全面解析OpenAI的新作——GPT-4o 原创">9. 全面解析OpenAI的新作——GPT-4o 原创</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/697587954" rel="nofollow" title="10. gpt4o与gpt4.0的实测对比 - 知乎 - 知乎专栏">10. gpt4o与gpt4.0的实测对比 - 知乎 - 知乎专栏</a></p> 
<p><a href="https://openai.com/index/gpt-4o-and-more-tools-to-chatgpt-free/" rel="nofollow" title="11. Introducing GPT-4o and more tools to ChatGPT free users">11. Introducing GPT-4o and more tools to ChatGPT free users</a></p> 
<p><a href="https://m.mydrivers.com/newsview/979842.html" rel="nofollow" title="12. GPT-4o各种刷屏上手试了试：感觉目前也就那样">12. GPT-4o各种刷屏上手试了试：感觉目前也就那样</a></p> 
<p><a href="https://www.thepaper.cn/newsDetail_forward_27368886" rel="nofollow" title="13. OpenAI发布免费新品GPT-4o：可对音频、视觉和文本实时推理，与人自然对话，功能秒杀Siri">13. OpenAI发布免费新品GPT-4o：可对音频、视觉和文本实时推理，与人自然对话，功能秒杀Siri</a></p> 
<p><a href="https://www.jiqizhixin.com/articles/2024-05-14-5" rel="nofollow" title="14. OpenAI颠覆世界：GPT-4o完全免费，实时语音视频交互震撼 ...">14. OpenAI颠覆世界：GPT-4o完全免费，实时语音视频交互震撼 ...</a></p> 
<p><a href="https://www.163.com/dy/article/J25532TK0519DDQ2.html" rel="nofollow" title="15. 更快！更自然！OpenAI推出GPT-4o，记者实测">15. 更快！更自然！OpenAI推出GPT-4o，记者实测</a></p> 
<p><a href="https://www.36kr.com/p/2776550729057160" rel="nofollow" title="16. 上手了刷屏一天的GPT-4o，我感觉目前也就那样">16. 上手了刷屏一天的GPT-4o，我感觉目前也就那样</a></p> 
<p><a href="https://juejin.cn/post/7368316813711097856" rel="nofollow" title="17. 糟了，OpenAI再推旗舰GPT-4o，又有行业即将被颠覆">17. 糟了，OpenAI再推旗舰GPT-4o，又有行业即将被颠覆</a></p> 
<p><a href="https://www.zhihu.com/question/655918105?write" rel="nofollow" title="18. OpenAI 发布会发布了GPT4o,有哪些重大更新？">18. OpenAI 发布会发布了GPT4o,有哪些重大更新？</a></p> 
<p><a href="https://www.unite.ai/zh-CN/%E6%8F%AD%E6%99%93-chatgpt-4o-%E4%B8%8B%E4%B8%80%E4%BB%A3%E5%8A%9F%E8%83%BD%E5%8F%8A%E5%85%B6%E5%8F%98%E9%9D%A9%E6%80%A7%E5%BD%B1%E5%93%8D/" rel="nofollow" title="19. 揭晓 ChatGPT-4o：下一代功能及其变革性影响 - Unite.AI [2024-05-13]">19. 揭晓 ChatGPT-4o：下一代功能及其变革性影响 - Unite.AI [2024-05-13]</a></p> 
<p><a href="https://www.businessinsider.jp/post-287097" rel="nofollow" title="20. OpenAIの｢GPT-4o｣はまるで｢目を手に入れたAI｣。ChatGPT無料版でも利用可能に [2024-05-14]">20. OpenAIの｢GPT-4o｣はまるで｢目を手に入れたAI｣。ChatGPT無料版でも利用可能に [2024-05-14]</a></p> 
<p><a href="https://www.iplaysoft.com/gpt-4o.html" rel="nofollow" title="21. 全新ChatGPT-4o 模型重大更新- 能力惊人！支持API，所有人 ...">21. 全新ChatGPT-4o 模型重大更新- 能力惊人！支持API，所有人 ...</a></p> 
<p><a href="https://m.huxiu.com/article/3023512.html" rel="nofollow" title="22. GPT-4o是什么？是你想要的一切">22. GPT-4o是什么？是你想要的一切</a></p> 
<p><a href="https://openai.com/index/hello-gpt-4o/" rel="nofollow" title="23. Hello GPT-4o | OpenAI">23. Hello GPT-4o | OpenAI</a></p> 
<p><a href="https://blog.csdn.net/Deng_Xian_Sheng/article/details/138851725" title="24. GPT-4o测评，智力弱于人类，还没有我女朋友聪明。 原创">24. GPT-4o测评，智力弱于人类，还没有我女朋友聪明。 原创</a></p> 
<p><a href="https://juejin.cn/post/7368375416928895028" rel="nofollow" title="25. OpenAI爆炸更新！新模型GPT-4o发布！超强的视觉和语音 ...">25. OpenAI爆炸更新！新模型GPT-4o发布！超强的视觉和语音 ...</a></p> 
<p><a href="https://community.openai.com/t/gpt-4-vs-gpt-4o-which-is-the-better/746991" rel="nofollow" title="26. GPT-4 vs GPT-4o? Which is the better? - API - OpenAI Developer Forum">26. GPT-4 vs GPT-4o? Which is the better? - API - OpenAI Developer Forum</a></p> 
<p><a href="https://www.zhihu.com/question/655916007" rel="nofollow" title="27. OpenAI发布了最新的旗舰模型GPT-4o,如何评价GPT-4o?">27. OpenAI发布了最新的旗舰模型GPT-4o,如何评价GPT-4o?</a></p> 
<p><a href="https://www.bilibili.com/video/BV1mm421u7jf/" rel="nofollow" title="28. GPT-4o全解析：特性、影响与未来期待 - AI Explained_哔哩哔哩_bilibili [2024-05-14]">28. GPT-4o全解析：特性、影响与未来期待 - AI Explained_哔哩哔哩_bilibili [2024-05-14]</a></p> 
<p><a href="https://interface.sina.cn/pc_to_wap.d.html?ref=https%3A%2F%2Ffinance.sina.com.cn%2Fstock%2Fusstock%2Fc%2F2024-05-14%2Fdoc-inavenmw3612369.shtml%3Fcre%3Dtianyi%26mod%3Dpchp%26loc%3D11%26r%3D0%26rfunc%3D82%26tj%3Dcxvertical_pc_hp%26tr%3D12" rel="nofollow" title="29. OpenAI发布旗舰AI模型GPT-4o：图文音频全搞定完全免费">29. OpenAI发布旗舰AI模型GPT-4o：图文音频全搞定完全免费</a></p> 
<p><a href="https://chatgptgogogo.com/gpt-4o/index.html" rel="nofollow" title="30. OpenAI爆炸更新！Plus功能免费可用！新模型GPT-4o发布！视觉">30. OpenAI爆炸更新！Plus功能免费可用！新模型GPT-4o发布！视觉</a></p> 
<p><a href="https://new.qq.com/rain/a/20240514A0ANYR00" rel="nofollow" title="31. GPT-4o初体验：视觉、听觉跨越式升级">31. GPT-4o初体验：视觉、听觉跨越式升级</a></p> 
<p><a href="https://openmagic.ai/154495.html" rel="nofollow" title="32. OpenAI发布GPT-4o：无与伦比的音频视频理解能力 - AI魔法助手">32. OpenAI发布GPT-4o：无与伦比的音频视频理解能力 - AI魔法助手</a></p> 
<p><a href="https://www.donews.com/news/detail/4/4229112.html" rel="nofollow" title="33. OpenAI推出新一代AI模型GPT-4o 音频视频理解能力尚无敌手">33. OpenAI推出新一代AI模型GPT-4o 音频视频理解能力尚无敌手</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/643923957" rel="nofollow" title="34. 号外!号外!Gpt-4技术细节大揭秘! - 知乎 - 知乎专栏">34. 号外!号外!Gpt-4技术细节大揭秘! - 知乎 - 知乎专栏</a></p> 
<p><a href="https://shemmytalk.com/openai-%E5%8F%91%E5%B8%83-gpt-4o%E8%B7%A8%E8%B6%8A%E8%A7%86%E8%A7%89%E3%80%81%E9%9F%B3%E9%A2%91%E5%92%8C%E6%96%87%E6%9C%AC%E7%9A%84%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/" rel="nofollow" title="35. OpenAI 发布GPT-4o:跨越视觉、音频和文本的智能模型">35. OpenAI 发布GPT-4o:跨越视觉、音频和文本的智能模型</a></p> 
<p><a href="https://knewsmart.com/archives/297807" rel="nofollow" title="36. OpenAI推出新一代AI模型GPT-4o 音频视频理解能力尚无敌手">36. OpenAI推出新一代AI模型GPT-4o 音频视频理解能力尚无敌手</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/688836029" rel="nofollow" title="37. GPT-4 Vision ：指北教程 - 知乎 - 知乎专栏">37. GPT-4 Vision ：指北教程 - 知乎 - 知乎专栏</a></p> 
<p><a href="https://www.ai569.com/ai-tutorial/318.html" rel="nofollow" title="38. GPT-4o是什么以及其核心功能详解！（内附免费体验网址）">38. GPT-4o是什么以及其核心功能详解！（内附免费体验网址）</a></p> 
<p><a href="https://blog.csdn.net/cilibili/article/details/138874789" title="39. （教程）gpt-4o如何使用，怎么体验？gpt-4o和gpt-4-turbo的区别">39. （教程）gpt-4o如何使用，怎么体验？gpt-4o和gpt-4-turbo的区别</a></p> 
<p><a href="https://m.thepaper.cn/newsDetail_forward_27370331" rel="nofollow" title="40. OpenAI新版GPT-4o三连炸：更快，更强，还免费">40. OpenAI新版GPT-4o三连炸：更快，更强，还免费</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/697658172" rel="nofollow" title="41. （教程）gpt-4o如何使用和免费体验？gpt-4o和gpt-4-turbo的区别以及gpt-4o怎么开通的问题 - 知乎">41. （教程）gpt-4o如何使用和免费体验？gpt-4o和gpt-4-turbo的区别以及gpt-4o怎么开通的问题 - 知乎</a></p> 
<p><a href="http://cen.ce.cn/more/202405/15/t20240515_39003741.shtml" rel="nofollow" title="42. 会提供情绪价值，OpenAI推“王炸”新模型国内AI巨头压力不小，“">42. 会提供情绪价值，OpenAI推“王炸”新模型国内AI巨头压力不小，“</a></p> 
<p><a href="https://m.huxiu.com/article/3026469.html" rel="nofollow" title="43. GPT-4o的“类人”响应速度刺痛Siri？">43. GPT-4o的“类人”响应速度刺痛Siri？</a></p> 
<p><a href="https://wap.eastmoney.com/a/202405143076122250.html" rel="nofollow" title="44. OpenAI新模型GPT-4o“炸裂登场” 响应速度堪比真人关键还 ...">44. OpenAI新模型GPT-4o“炸裂登场” 响应速度堪比真人关键还 ...</a></p> 
<p><a href="https://juejin.cn/post/7368692841298673718" rel="nofollow" title="45. gpt-4o如何使用，怎么看gpt-4o和gpt-4 turbo、gpt3的区别">45. gpt-4o如何使用，怎么看gpt-4o和gpt-4 turbo、gpt3的区别</a></p> 
<p><a href="https://m.thepaper.cn/newsDetail_forward_27381701" rel="nofollow" title="46. GPT-4o的“类人”响应速度刺痛Siri？">46. GPT-4o的“类人”响应速度刺痛Siri？</a></p> 
<p><a href="https://interface.sina.cn/pc_to_wap.d.html?ref=https%3A%2F%2Ffinance.sina.com.cn%2Fjjxw%2F2024-05-15%2Fdoc-inavheux0345321.shtml" rel="nofollow" title="47. “有史以来最好的模型”GPT-4o功能全部免费">47. “有史以来最好的模型”GPT-4o功能全部免费</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/665479979" rel="nofollow" title="48. GPT-4 Turbo 登场，有哪些功能值得关注？将带来哪些影响? ( OpenAI API 文档解读 ) - 知乎 [2023-11-07]">48. GPT-4 Turbo 登场，有哪些功能值得关注？将带来哪些影响? ( OpenAI API 文档解读 ) - 知乎 [2023-11-07]</a></p> 
<p><a href="https://www.yjpoo.com/article/3645.html" rel="nofollow" title="49. GPT-4o与GPT-4 Turbo 功能对比有什么变化和更新？">49. GPT-4o与GPT-4 Turbo 功能对比有什么变化和更新？</a></p> 
<p><a href="https://m.thepaper.cn/newsDetail_forward_27372297" rel="nofollow" title="50. 解读｜GPT-4o为OpenAI开启超级入口，对谷歌形成挑战？">50. 解读｜GPT-4o为OpenAI开启超级入口，对谷歌形成挑战？</a></p> 
<p><a href="https://interface.sina.cn/pc_to_wap.d.html?ref=https%3A%2F%2Ffinance.sina.com.cn%2Fjjxw%2F2024-05-14%2Fdoc-inavewzp1038288.shtml" rel="nofollow" title="51. 解读｜GPT-4o为OpenAI开启超级入口，对谷歌形成挑战？">51. 解读｜GPT-4o为OpenAI开启超级入口，对谷歌形成挑战？</a></p> 
<p><a href="https://m.163.com/news/article/J27ET4UO05199A0B.html" rel="nofollow" title="52. 遥遥领先的GPT-4o，为什么要免费开放？">52. 遥遥领先的GPT-4o，为什么要免费开放？</a></p> 
<p><a href="https://finance.sina.cn/hkstock/gsxw/2024-05-15/detail-inavhmav0233350.d.html?vt=4&amp;cid=76526&amp;node_id=76526" rel="nofollow" title="53. 中信证券：OpenAI推出GPT新模型端到端加速边缘侧落地">53. 中信证券：OpenAI推出GPT新模型端到端加速边缘侧落地</a></p> 
<p><a href="https://interface.sina.cn/pc_to_wap.d.html?ref=https%3A%2F%2Ffinance.sina.com.cn%2Fstock%2Fhkstock%2Fggscyd%2F2024-05-15%2Fdoc-inavheuv3572661.shtml" rel="nofollow" title="54. 国泰君安：OpenAI发布GPT-4o 新型商业模式或将逐步推出">54. 国泰君安：OpenAI发布GPT-4o 新型商业模式或将逐步推出</a></p> 
<p><a href="https://www.163.com/dy/article/J25T0KL705199LJK.html" rel="nofollow" title="55. 性能更强还免费的GPT-4o发布国内外大模型差距拉大了吗？">55. 性能更强还免费的GPT-4o发布国内外大模型差距拉大了吗？</a></p> 
<p><a href="https://www.zhihu.com/question/589892876" rel="nofollow" title="56. Gpt4.0 与它的祖父辈们的差别是什么？ - 知乎">56. Gpt4.0 与它的祖父辈们的差别是什么？ - 知乎</a></p> 
<p><a href="https://www.ithome.com/0/767/693.htm" rel="nofollow" title="58. OpenAI 发布全新旗舰生成式 AI 模型 GPT-4o：语音对话更流畅，免费提供 - IT之家 [2024-05-14]">58. OpenAI 发布全新旗舰生成式 AI 模型 GPT-4o：语音对话更流畅，免费提供 - IT之家 [2024-05-14]</a></p> 
<p><a href="https://m.163.com/dy/article/J25H6LHL0519QIKK.html?spss=adap_pc" rel="nofollow" title="59. GPT-4o来了支持文本、音频和图像的多模态输入输出">59. GPT-4o来了支持文本、音频和图像的多模态输入输出</a></p> 
<p><a href="https://linux.do/t/topic/87465" rel="nofollow" title="60. GPT-4o 的实时音频对话是怎么实现的？">60. GPT-4o 的实时音频对话是怎么实现的？</a></p> 
<p><a href="https://blog.csdn.net/FrenzyTechAI/article/details/138860798" title="61. OpenAI 推出GPT-4o：实现多模态AI 交互原创">61. OpenAI 推出GPT-4o：实现多模态AI 交互原创</a></p> 
<p><a href="https://www.36kr.com/p/2774954379248518" rel="nofollow" title="62. OpenAI教谷歌做语音助手，新模型GPT-4o科幻级语音交互">62. OpenAI教谷歌做语音助手，新模型GPT-4o科幻级语音交互</a></p> 
<p><a href="https://news.sciencenet.cn/htmlnews/2024/5/522593.shtm" rel="nofollow" title="63. OpenAI发布新品GPT-4o，功能秒杀Siri - 新闻- 科学网">63. OpenAI发布新品GPT-4o，功能秒杀Siri - 新闻- 科学网</a></p> 
<p><a href="https://finance.sina.com.cn/stock/stockzmt/2024-05-14/doc-inavewzm4241930.shtml" rel="nofollow" title="64. 西部郑宏达| GPT-4o：人类在AI多模态大模型的进步">64. 西部郑宏达| GPT-4o：人类在AI多模态大模型的进步</a></p> 
<p><a href="https://www.aihub.cn/tools/chatbot/gpt-4o/" rel="nofollow" title="65. GPT-4o：OpenAI最新发布的多模态AI大模型，可实时推理音频">65. GPT-4o：OpenAI最新发布的多模态AI大模型，可实时推理音频</a></p> 
<p><a href="https://interface.sina.cn/pc_to_wap.d.html?ref=https%3A%2F%2Ffinance.sina.com.cn%2Fstock%2Fhkstock%2Fggscyd%2F2024-05-15%2Fdoc-inavhmav0233350.shtml" rel="nofollow" title="66. 中信证券：OpenAI推出GPT新模型端到端加速边缘侧落地">66. 中信证券：OpenAI推出GPT新模型端到端加速边缘侧落地</a></p> 
<p><a href="https://wap.stockstar.com/detail/IG2024051400027051" rel="nofollow" title="67. OpenAI发布GPT-4o，哪些多模态AI概念股或迎发展新机遇？">67. OpenAI发布GPT-4o，哪些多模态AI概念股或迎发展新机遇？</a></p> 
<p><a href="https://wallstreetcn.com/articles/3714832" rel="nofollow" title="68. Jim Fan锐评：GPT-4o低延迟的奥秘在这里">68. Jim Fan锐评：GPT-4o低延迟的奥秘在这里</a></p> 
<p><a href="https://www.163.com/dy/article/J25H6LHL0519QIKK.html" rel="nofollow" title="69. GPT-4o来了 支持文本、音频和图像的多模态输入输出|速度|gpt-4|视频生成模型_网易订阅 [2024-05-14]">69. GPT-4o来了 支持文本、音频和图像的多模态输入输出|速度|gpt-4|视频生成模型_网易订阅 [2024-05-14]</a></p> 
<p><a href="http://www.ce.cn/macro/more/202405/15/t20240515_39003097.shtml" rel="nofollow" title="70. OpenAI推“王炸”新模型聪明又快速还会提供情绪价值">70. OpenAI推“王炸”新模型聪明又快速还会提供情绪价值</a></p> 
<p><a href="https://www.stcn.com/article/detail/1205112.html" rel="nofollow" title="71. 中信证券：OpenAI推出GPT新模型端到端加速边缘侧落地">71. 中信证券：OpenAI推出GPT新模型端到端加速边缘侧落地</a></p> 
<p><a href="https://www.sohu.com/a/778808214_121332532" rel="nofollow" title="72. GPT-4o凌晨炸场：“AI伴侣”触手可及，谷歌、阿里、腾讯压力山大">72. GPT-4o凌晨炸场：“AI伴侣”触手可及，谷歌、阿里、腾讯压力山大</a></p> 
<p><a href="https://ai-bot.cn/openai-gpt-4o/" rel="nofollow" title="73. GPT-4o - OpenAI最新发布的多模态AI大模型 | AI工具集">73. GPT-4o - OpenAI最新发布的多模态AI大模型 | AI工具集</a></p> 
<p><a href="https://zhuanlan.zhihu.com/p/668156677" rel="nofollow" title="74. GPT4-Turbo专题研究：多模态能力提升，应用生态加速 - 知乎">74. GPT4-Turbo专题研究：多模态能力提升，应用生态加速 - 知乎</a></p> 
<p><a href="http://stock.10jqka.com.cn/20240515/c657848503.shtml" rel="nofollow" title="75. Open AI发布GPT 4o 关注与C端用户体验密切相关的行业">75. Open AI发布GPT 4o 关注与C端用户体验密切相关的行业</a></p> 
<p><a href="https://m.chinaz.com/2024/0514/1616357.shtml" rel="nofollow" title="76. GPT-4o实际应用案例：盲人可以更好地“看见”世界">76. GPT-4o实际应用案例：盲人可以更好地“看见”世界</a></p> 
<p><a href="https://tech.cnr.cn/techyw/kan/20240515/t20240515_526705550.shtml" rel="nofollow" title='77. "有史以来最好的模型"GPT-4o功能全部免费 - 央广网科技 [2024-05-15]'>77. "有史以来最好的模型"GPT-4o功能全部免费 - 央广网科技 [2024-05-15]</a></p> 
<p><a href="https://xueqiu.com/7183081776/290028913" rel="nofollow" title="78. 【光大海外】GPT-4o后续影响：推理端降本+多模态+低延迟带来AI应用转折点 【特别提示】本订阅号中所涉及的证券研究信息，均取自于 光大证券 ... [2024-05-15]">78. 【光大海外】GPT-4o后续影响：推理端降本+多模态+低延迟带来AI应用转折点 【特别提示】本订阅号中所涉及的证券研究信息，均取自于 光大证券 ... [2024-05-15]</a></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/39f33602c5de6302b8b4f9482a4afc44/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">AI绘画Stable Diffusion | 如何利用SD垫图实现照片风格转换，动漫真人互转教程</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/751ae993157c7f3ffbf4dd7c31c50d21/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">AI绘画进阶工具ComfyUI 傻瓜整合包安装教程！模型共享，一键安装！</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>