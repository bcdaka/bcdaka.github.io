<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Spark期末汇总 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/148b284d1509fa40819a466eafac9ba6/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Spark期末汇总">
  <meta property="og:description" content="目录
第二章 Scala基础
一.scala的特性
二.定义与使用数组
第三章 Spark编程基础
RDD简介
1. parallelize()
2. makeRDD()
3.使用 union()方法合并多个 RDD
4.使用 filter()方法进行过滤
5.使用 distinct()方法进行去重
6.使用map()方法转换数据
7.使用 sortBy)方法进行排序
8.使用 collect()方法查询数据
9.使用 flatMap)方法转换数据
第四章 Spark编程进阶
在集群环境中运行 Spark
spark-submit常用项配置
第五章 Spark SQL——结构化数据文件处理
一.DataFrame简介
二.DataFrame的创建
1.数据准备
2.RDD直接转换为DataFrame
3.DataFrame的常用操作
第二章 Scala基础 一.scala的特性 (1)面向对象
Seala是一种纯释的面向对象语言。一个对象的类型和行为是由类和特征描迷的。类通过子类化和灵活的混合类进行扩展，成为多重继承的可靠解快方案。
(2)函数式编程
Scala 提供了轻量级语法来定义匿名函数，支持高阶函数，允许函数嵌套，并支持函数柯里化。Scala 的样例类与模式匹配支持两数式编程语言中的代数类型。Scala 的单例对象提供了方便的方法来组合不属于类的函数。用户还可以使用 Seala 的模式匹配，编写类似正则表达式的代码处理可扩展标记语言(Extensible Markup Language, XML)格式的数据。
(3)静态类型
Scala 配备了表现型的系统，以静态的方式进行抽象，以安全和连贯的方式进行使用。
系统支持将通用类、内部类、抽象类和复合类作为对象成员，也支持隐式参数、转换和多态方法等，这为抽象编程的安全重用和软件类型的安全扩展提供了强大的支持。
(4)可扩展
在实践中，专用领域的应用程序开发往往需要特定的语言扩展。Scala 提供了许多独特
的语言机制，可以以库的形式无缝添加新的语言结构。
二.定义与使用数组 方式一
方式二
操作数组
# 查看数组z的长度
z.length">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-13T16:57:54+08:00">
    <meta property="article:modified_time" content="2024-05-13T16:57:54+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Spark期末汇总</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E7%AC%AC%E4%BA%8C%E7%AB%A0%C2%A0%20%C2%A0%20Scala%E5%9F%BA%E7%A1%80-toc" style="margin-left:40px;"><a href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0%C2%A0%20%C2%A0%20Scala%E5%9F%BA%E7%A1%80" rel="nofollow">第二章    Scala基础</a></p> 
<p id="%E4%B8%80.scala%E7%9A%84%E7%89%B9%E6%80%A7-toc" style="margin-left:80px;"><a href="#%E4%B8%80.scala%E7%9A%84%E7%89%B9%E6%80%A7" rel="nofollow">一.scala的特性</a></p> 
<p id="%E4%BA%8C.%E5%AE%9A%E4%B9%89%E4%B8%8E%E4%BD%BF%E7%94%A8%E6%95%B0%E7%BB%84-toc" style="margin-left:80px;"><a href="#%E4%BA%8C.%E5%AE%9A%E4%B9%89%E4%B8%8E%E4%BD%BF%E7%94%A8%E6%95%B0%E7%BB%84" rel="nofollow">二.定义与使用数组</a></p> 
<p id="%E7%AC%AC%E4%B8%89%E7%AB%A0%C2%A0%20%C2%A0%20Spark%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80-toc" style="margin-left:40px;"><a href="#%E7%AC%AC%E4%B8%89%E7%AB%A0%C2%A0%20%C2%A0%20Spark%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80" rel="nofollow">第三章    Spark编程基础</a></p> 
<p id="RDD%E7%AE%80%E4%BB%8B-toc" style="margin-left:80px;"><a href="#RDD%E7%AE%80%E4%BB%8B" rel="nofollow">RDD简介</a></p> 
<p id="1.%20parallelize()-toc" style="margin-left:80px;"><a href="#1.%20parallelize%28%29" rel="nofollow">1. parallelize()</a></p> 
<p id="2.%20makeRDD()-toc" style="margin-left:80px;"><a href="#2.%20makeRDD%28%29" rel="nofollow">2. makeRDD()</a></p> 
<p id="3.%E4%BD%BF%E7%94%A8%20union()%E6%96%B9%E6%B3%95%E5%90%88%E5%B9%B6%E5%A4%9A%E4%B8%AA%20RDD-toc" style="margin-left:80px;"><a href="#3.%E4%BD%BF%E7%94%A8%20union%28%29%E6%96%B9%E6%B3%95%E5%90%88%E5%B9%B6%E5%A4%9A%E4%B8%AA%20RDD" rel="nofollow">3.使用 union()方法合并多个 RDD</a></p> 
<p id="4.%E4%BD%BF%E7%94%A8%20filter()%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E8%BF%87%E6%BB%A4-toc" style="margin-left:80px;"><a href="#4.%E4%BD%BF%E7%94%A8%20filter%28%29%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E8%BF%87%E6%BB%A4" rel="nofollow">4.使用 filter()方法进行过滤</a></p> 
<p id="5.%E4%BD%BF%E7%94%A8%20distinct()%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E5%8E%BB%E9%87%8D-toc" style="margin-left:80px;"><a href="#5.%E4%BD%BF%E7%94%A8%20distinct%28%29%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E5%8E%BB%E9%87%8D" rel="nofollow">5.使用 distinct()方法进行去重</a></p> 
<p id="6.%E4%BD%BF%E7%94%A8map()%E6%96%B9%E6%B3%95%E8%BD%AC%E6%8D%A2%E6%95%B0%E6%8D%AE-toc" style="margin-left:80px;"><a href="#6.%E4%BD%BF%E7%94%A8map%28%29%E6%96%B9%E6%B3%95%E8%BD%AC%E6%8D%A2%E6%95%B0%E6%8D%AE" rel="nofollow">6.使用map()方法转换数据</a></p> 
<p id="7.%E4%BD%BF%E7%94%A8%20sortBy)%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E6%8E%92%E5%BA%8F-toc" style="margin-left:80px;"><a href="#7.%E4%BD%BF%E7%94%A8%20sortBy%29%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E6%8E%92%E5%BA%8F" rel="nofollow">7.使用 sortBy)方法进行排序</a></p> 
<p id="8.%E4%BD%BF%E7%94%A8%20collect()%E6%96%B9%E6%B3%95%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE-toc" style="margin-left:80px;"><a href="#8.%E4%BD%BF%E7%94%A8%20collect%28%29%E6%96%B9%E6%B3%95%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE" rel="nofollow">8.使用 collect()方法查询数据</a></p> 
<p id="9.%E4%BD%BF%E7%94%A8%20flatMap)%E6%96%B9%E6%B3%95%E8%BD%AC%E6%8D%A2%E6%95%B0%E6%8D%AE-toc" style="margin-left:80px;"><a href="#9.%E4%BD%BF%E7%94%A8%20flatMap%29%E6%96%B9%E6%B3%95%E8%BD%AC%E6%8D%A2%E6%95%B0%E6%8D%AE" rel="nofollow">9.使用 flatMap)方法转换数据</a></p> 
<p id="%E7%AC%AC%E5%9B%9B%E7%AB%A0%C2%A0%20%C2%A0%20Spark%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6-toc" style="margin-left:40px;"><a href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0%C2%A0%20%C2%A0%20Spark%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6" rel="nofollow">第四章    Spark编程进阶</a></p> 
<p id="%E5%9C%A8%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E4%B8%AD%E8%BF%90%E8%A1%8C%20Spark-toc" style="margin-left:80px;"><a href="#%E5%9C%A8%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E4%B8%AD%E8%BF%90%E8%A1%8C%20Spark" rel="nofollow">在集群环境中运行 Spark</a></p> 
<p id="spark-submit%E5%B8%B8%E7%94%A8%E9%A1%B9%E9%85%8D%E7%BD%AE-toc" style="margin-left:80px;"><a href="#spark-submit%E5%B8%B8%E7%94%A8%E9%A1%B9%E9%85%8D%E7%BD%AE" rel="nofollow">spark-submit常用项配置</a></p> 
<p id="%E7%AC%AC%E4%BA%94%E7%AB%A0%C2%A0%20%C2%A0%20Spark%20SQL%E2%80%94%E2%80%94%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86-toc" style="margin-left:40px;"><a href="#%E7%AC%AC%E4%BA%94%E7%AB%A0%C2%A0%20%C2%A0%20Spark%20SQL%E2%80%94%E2%80%94%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86" rel="nofollow">第五章    Spark SQL——结构化数据文件处理</a></p> 
<p id="%E4%B8%80.DataFrame%E7%AE%80%E4%BB%8B-toc" style="margin-left:80px;"><a href="#%E4%B8%80.DataFrame%E7%AE%80%E4%BB%8B" rel="nofollow">一.DataFrame简介</a></p> 
<p id="%E4%BA%8C.DataFrame%E7%9A%84%E5%88%9B%E5%BB%BA-toc" style="margin-left:80px;"><a href="#%E4%BA%8C.DataFrame%E7%9A%84%E5%88%9B%E5%BB%BA" rel="nofollow">二.DataFrame的创建</a></p> 
<p id="1.%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87-toc" style="margin-left:120px;"><a href="#1.%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87" rel="nofollow">1.数据准备</a></p> 
<p id="2.RDD%E7%9B%B4%E6%8E%A5%E8%BD%AC%E6%8D%A2%E4%B8%BADataFrame-toc" style="margin-left:120px;"><a href="#2.RDD%E7%9B%B4%E6%8E%A5%E8%BD%AC%E6%8D%A2%E4%B8%BADataFrame" rel="nofollow">2.RDD直接转换为DataFrame</a></p> 
<p id="3.DataFrame%E7%9A%84%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C-toc" style="margin-left:120px;"><a href="#3.DataFrame%E7%9A%84%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C" rel="nofollow">3.DataFrame的常用操作</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h3 style="margin-left:.31in;text-align:justify;"><span style="color:#6eaad7;">第二章    Scala基础</span></h3> 
<h4 id="%E4%B8%80.scala%E7%9A%84%E7%89%B9%E6%80%A7" style="margin-left:.31in;text-align:justify;"><span style="color:#ed7976;">一.scala的特性</span></h4> 
<p><span style="color:#faa572;">(1)面向对象</span><br> Seala是一种纯释的面向对象语言。一个对象的类型和行为是由类和特征描迷的。类通过子类化和灵活的混合类进行扩展，成为多重继承的可靠解快方案。<br><span style="color:#faa572;">(2)函数式编程</span><br> Scala 提供了轻量级语法来定义匿名函数，支持高阶函数，允许函数嵌套，并支持函数柯里化。Scala 的样例类与模式匹配支持两数式编程语言中的代数类型。Scala 的单例对象提供了方便的方法来组合不属于类的函数。用户还可以使用 Seala 的模式匹配，编写类似正则表达式的代码处理可扩展标记语言(Extensible Markup Language, XML)格式的数据。<br><span style="color:#faa572;">(3)静态类型</span><br> Scala 配备了表现型的系统，以静态的方式进行抽象，以安全和连贯的方式进行使用。<br> 系统支持将通用类、内部类、抽象类和复合类作为对象成员，也支持隐式参数、转换和多态方法等，这为抽象编程的安全重用和软件类型的安全扩展提供了强大的支持。<br><span style="color:#faa572;">(4)可扩展</span><br> 在实践中，专用领域的应用程序开发往往需要特定的语言扩展。Scala 提供了许多独特<br> 的语言机制，可以以库的形式无缝添加新的语言结构。</p> 
<h4 id="%E4%BA%8C.%E5%AE%9A%E4%B9%89%E4%B8%8E%E4%BD%BF%E7%94%A8%E6%95%B0%E7%BB%84"><span style="color:#ed7976;">二.定义与使用数组</span></h4> 
<p><span style="color:#faa572;">方式一</span></p> 
<p><img alt="" height="83" src="https://images2.imgbox.com/c9/b9/OUmbsAhm_o.png" width="471"></p> 
<p><span style="color:#faa572;">方式二</span></p> 
<p><img alt="" height="47" src="https://images2.imgbox.com/60/a9/jdzs9ukv_o.png" width="581"></p> 
<p><span style="color:#faa572;">操作数组</span></p> 
<p><span style="color:#0d0016;"># 查看数组z的长度<br> z.length<br> # 查看数组z的第一个元素<br> z.head<br> # 查看数组z中除了第一个元素外的其他元素<br> Z.tai1<br> # 判断数组z是否为空<br> z.isEmpty<br> # 判断数组z是否包含元素"baidu"<br> z.contains ("baidu")</span></p> 
<p><img alt="" height="238" src="https://images2.imgbox.com/b6/ae/g7F1ujsd_o.png" width="392"></p> 
<p>连接两个数组既可以使用操作符“+”、也可以使用 concat)方法。使用 cncal)方法需要先通过“import Array_”命令导入包。定义数组ar1 和ar2, 并分别便用操作符和 concato 方法连接两个数组，结果如图</p> 
<p><img alt="" height="95" src="https://images2.imgbox.com/99/0e/gIhzXnS6_o.png" width="354"></p> 
<p><img alt="" height="140" src="https://images2.imgbox.com/9f/75/aSDAWqxC_o.png" width="438"></p> 
<p>Scala 可以使用 range0方法创建区间数组。使用 range0方法前同样需要先通过命令“import Array_”导入包。创建区间为1~10且步长为2的数组，如图</p> 
<p><img alt="" height="48" src="https://images2.imgbox.com/ae/be/xAqcoLKJ_o.png" width="369"></p> 
<h3 id="%E7%AC%AC%E4%B8%89%E7%AB%A0%C2%A0%20%C2%A0%20Spark%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80" style="margin-left:.31in;text-align:justify;"><span style="color:#6eaad7;">第三章    Spark编程基础</span></h3> 
<h4 id="RDD%E7%AE%80%E4%BB%8B"><span style="color:#ed7976;">RDD简介</span></h4> 
<p>RDD 是一个容错的、只读的、可进行并行操作的数据结构，是一个分布在集群各个节点中的存放元素的集合。RDD有3种不同的创建方法。第一种是将程序中已存在的 Sea 集合（如集合、列表、数组）转换成RDD，第二种是对已有RDD 进行转换得到新的RDD，这两种方法都是通过内存中已有的数据创建RDD 的。第三种是直接读取外部存储系统的数据创建 RDD。</p> 
<p></p> 
<h4 id="1.%20parallelize()"><span style="color:#ed7976;">1. parallelize()</span></h4> 
<p><br> parallelize0方法有两个输人参数，说明如下。<br> (1)要转化的集合：必须是 Scg 集合。Seg 表示序列，指的是一类具有一定长度的、<br> 可送代访问的对象，其中每个数据元素均带有一个从0开始的、固定的索引。<br> (2)分区数。若不设分区数，则RDD的分区数默认为该程序分配到的资源的 CPU 核心数。通过 parallelize()方法用一个数组的数据创建 RDD，并设置分区数为4，创建后查看该RDD的分区数</p> 
<p><span style="color:#faa572;">使用 makeRDD()方法创建 RDD 并查看各分区的值</span><br> #定义一个数组<br> val data = Array(1, 2, 3,4, 5)<br> # 使用parallelize ()方法创建 RDD<br> val distData = sc, parallelize (data)<br> # 查看 RDD默认分区个数<br> distData.partitions.size<br> # 设置分区个数为4后创建 RDD<br> val distData = sc, parallelize (data, 4)<br> # 再次查看 RDD分区个数<br> distData.partitions, size</p> 
<h4 id="2.%20makeRDD()"><span style="color:#ed7976;">2. makeRDD()</span></h4> 
<p><br> makeRDDO方法有两种使用方式，第一种使用方式与 parallelize)方法一致;第二种方式是通过接收一个 Seq[(T, Seq[String])]参数类型创建 RDD。第二种方式生成的 RDD 中保存的是T的值，Seq[String]部分的数据会按照 Seg[(T, Seq[String)]的顺序存放到各个分区中，一个 Seq[String]对应存放至一个分区，并为数据提供位置信息，通过preferredLocations0方法可以根据位置信息查看每一个分区的值。调用 makeRDDO时不可以直接指定 RDD的分区个数，分区的个数与 Seq[String)参数的个数是保持一致的。使用 makeRDDO方法创建 RDD，并根据位置信息查看每一个分区的值。</p> 
<p><br><span style="color:#faa572;">使用 makeRDD()方法创建 RDD 并查看各分区的值</span><br> #定义一个序列 seg<br> val seq = Seg((1, Seq("iteblog.com", "sparkhostl.com"))，<br> (3, Seq("itebolg.com", "sparkhost2.com")), (2,Seq("iteblog.com", "sparkhost3.com")))# 使用 makeRDD()方法创建RDD<br> val iteblog = sc.makeRDD(seg)<br> # 查看 RDD的值iteblog.collect # 查看分区个数<br> iteblog.partitioner<br> iteblog.partitions.size<br> # 根据位置信息查看每一个分区的值<br> iteblog.preferredLocations (iteblog.partitions (0))iteblog.preferredLocations (iteblog.partitions (1))iteblog.preferredLocations(iteblog.partitions(2))</p> 
<p></p> 
<h4 id="3.%E4%BD%BF%E7%94%A8%20union()%E6%96%B9%E6%B3%95%E5%90%88%E5%B9%B6%E5%A4%9A%E4%B8%AA%20RDD"><span style="color:#ed7976;">3.使用 union()方法合并多个 RDD</span></h4> 
<p><br> union()方法是一种转换操作，用于将两个RDD合并成一个，不进行去重操作，而且两<br> 个RDD 中每个元素中的值的个数、数据类型需要保持一致。创建两个存放二元组的 RDD, 通过 umion)方法合并两个RDD，不处理重复数据，并且每个二元组的值的个数、数据类型都是一致的。</p> 
<p></p> 
<h4 id="4.%E4%BD%BF%E7%94%A8%20filter()%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E8%BF%87%E6%BB%A4"><span style="color:#ed7976;">4.使用 filter()方法进行过滤</span></h4> 
<p><br> fiter)方法是一种转换操作，用于过滤 RDD 中的元素。filter()方法需要一个参数，这个参数是一个用于过源的函数，该丽数的返回值为 Boolean 类型。filter()方法将返回值为 true 的元素保留，将返回值为 false的元素过滤掉，最后返回一个存储符合过滤条件的所有元素的新RDD。<br> 创建一个RDD，并且过滤掉每个元组第二个值小于等于1的元素，如代码 3-14所示。其中第一个 ilter0方法中使用了“.2”，第一个“”与第二个 filter)方法中的“x”一样，均表示RDD 的每一个元素。</p> 
<p></p> 
<h4 id="5.%E4%BD%BF%E7%94%A8%20distinct()%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E5%8E%BB%E9%87%8D"><span style="color:#ed7976;">5.使用 distinct()方法进行去重</span></h4> 
<p><br> distinct)方法是一种转换操作，用于RDD的数据去重，去除两个完全相同的元素，没<br> 有参数。创建一个带有重复数据的RDD,并使用 distinct)方法去重，如代码3-15 所示，通过 collect0方法查看结果。</p> 
<h4 id="6.%E4%BD%BF%E7%94%A8map()%E6%96%B9%E6%B3%95%E8%BD%AC%E6%8D%A2%E6%95%B0%E6%8D%AE"><span style="color:#ed7976;">6.使用map()方法转换数据</span></h4> 
<p><br> map()方法是一种基础的 RDD 转换操作，可以对RDD中的每一个数据元素通过某种丽数进行转换并返回新的RDD。map0方法是懒操作，不会立即进行计算。<br> 转换操作是创建RDD 的第二种方法，通过转换已有 RDD生成新的 RDD。因为 RDD -个不可变的集合，所以如果对RDD 数据进行了某种转换，那么会生成一个新的 RDD。<br> 例如，通过一个存放了5个 Int类型的数据元素的列表创建一个RDD，可通过map()方法对每一个元素进行平方运算，结果会生成一个新的RDD，如代码3-6所示。<br><span style="color:#faa572;">map()方法示例</span><br> 创建 RDD<br> val distData = sc.parallelize (List(I;，3, 45，3，76<br> # map)方法求平方值<br> val sq_dist = distData.nap(x=&gt;x*x)</p> 
<h4 id="7.%E4%BD%BF%E7%94%A8%20sortBy)%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E6%8E%92%E5%BA%8F"><span style="color:#ed7976;">7.使用 sortBy)方法进行排序</span></h4> 
<p><br> sortBy()方法用于对标准RDD进行排序，有3个可输人参数，说明如下。<br> (1)第1个参数是一个函数(T)=&gt;K,左边是要被排序对象中的每一个元素，右边返<br> 回的值是元素中要进行排序的值。<br> (2)第2个参数是ascending, 决定排序后 RDD中的元素是升序的还是降序的，默认<br> 是tue,即升序排序，如果需要降序排序则需要将参数的值设置为 falseo<br> (3)第3个参数是 wunPartitons, 块定排序后的RDD的分区个数，默认排序后的分区个数和排序之前的分区个数相等，即 this,partitions.size.<br> 第一个参数是必须输人的，而后面的两个参数可以不输人。例如，通过一个存放了3<br> 个二元组的列表创建一个 RDD，对元组的第二个值进行降序排序，分区个数设置为1。</p> 
<p>#.创建 RDD<br> val data = sc.parallelize(List((1, 3), (45，3), (7，6)))</p> 
<p>#使用SoxtBy()方法对元组的第二个值进行降序排序，分区个数没置为1</p> 
<p>val sort data - data.sortBy(x m&gt; x.2, false,1)</p> 
<p></p> 
<h4 id="8.%E4%BD%BF%E7%94%A8%20collect()%E6%96%B9%E6%B3%95%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE"><span style="color:#ed7976;">8.使用 collect()方法查询数据</span></h4> 
<p><br> collect(方法是一种行动操作，可以将RDD 中所有元萦转换成数组并返回到 Dive端，<br> 适用于返回处理后的少量数据。因为需要从集群各个节点收集数据到本地，经过网络传当并目加裁到 Driver内存中，所以如果数据量比较大，会给网络传输造成很大的压力。因此数据量较大时，尽量不使用collct0方法，否则可能导致Driver端出现内存益出问题。collegn 方法有以下两种操作方式。<br> (1)collect：直接调用collect 返回该RDD中的所有元索，返回类型是一个 Array[T]数组，这是较为常用的一种方式。<br> 使用 collect()方法查看在sq_dist 和 sort_data的结果，分别返回了经过平方运算后的 Int类型的数组和对元组第二个值进行降序排列后的数组。<br><span style="color:#faa572;">collect()方法示例</span><br> #查看 sq_dist 和 sort_data 的结果<br> sq_dist.collect<br> sort_data.collect</p> 
<p>(2) collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U。这种方式需要提供一个标准的偏函数，将元素保存至一个RDD中。首先定义一个函数 one,用于将 collect 方法得到的数组中数值为1的值替换为“one"，将其他值替换为“other”。创建一个只有3个 Iot 类型数据的RDD，在使用collec1()方法时将one 两数作为参数。<br><span style="color:#faa572;">collecl(PartialFunction)方法示例</span><br> 定义一个函数 one<br> val one:PartialrunctionlInt, stringl - (caso I &gt; "one":case - -&gt; "othe,<br> # 创建RDD<br> val data = sc, parallelize (List(2,3, 1))<br> #使用collect()方法，将 one 两数作为参数<br> data.collect (one).collect</p> 
<h4 id="9.%E4%BD%BF%E7%94%A8%20flatMap)%E6%96%B9%E6%B3%95%E8%BD%AC%E6%8D%A2%E6%95%B0%E6%8D%AE"><span style="color:#ed7976;">9.使用 flatMap)方法转换数据</span></h4> 
<p><br> fatMap()方法将函数参数应用于RDD之中的每一个元素，将返回的迭代器（如数组、列表等）中的所有元素构成新的 RDD。使用 flatMap0方法时先进行 map（映射）再进行 flat （扁平化）操作，数据会先经过跟 map()方法一样的操作，为每一条输人返回一个迭代器（可迭代的数据类型），然后将所得到的不同级别的送代器中的元素全部当成同级别的元素，返回一个元素级别全部相同的RDD。这个转换操作通常用来切分单词。<br> 例如，分别用 map()方法和 flatMap()方法分割字符串。用map()方法分割后，每个元素<br> 对应返回一个迭代器，即数组。flatMapO方法在进行同 map()方法一样的操作后，将3个迭代器的元素扁平化（压成同一级别），保存在新 RDD 中。<br><span style="color:#faa572;">flatMap()方法示例</span><br> # 创建 RDD<br> val. test = sc.parallelize (List ("How are you", "I am fine", "what about you"))<br> # 查看 RDD<br> test.collect<br> #使用map分割字符电后，再查看 RDD<br> test.map(x=&gt; x,split("")).collect<br> #使用latMap分割字符串后，再查看 RDD<br> test.flatMap(x =&gt; x.split("")).collect</p> 
<h3 id="%E7%AC%AC%E5%9B%9B%E7%AB%A0%C2%A0%20%C2%A0%20Spark%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6" style="margin-left:.31in;text-align:justify;"><span style="color:#6eaad7;">第四章    Spark编程进阶</span></h3> 
<p></p> 
<h4 id="%E5%9C%A8%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E4%B8%AD%E8%BF%90%E8%A1%8C%20Spark"><span style="color:#ed7976;">在集群环境中运行 Spark</span></h4> 
<p><br> 直接在开发环境中运行 Spark 程序时通常选择的是本地模式。如果数据的规模比较庞大，更常用的方式还是在 Spark 程序开发测试完成后编译打包为Java 归档(Java Archive, JAR)包，并通过 spark-submit 命令提交到 Spark集群环境中运行。</p> 
<p></p> 
<p>spark-submit的脚本在 Spark安装目录的bin 目录下,spark-submit 是 Spark 为所有支特<br> 的集群管理器提供的一个提交作业的工具。Spark 在/example 目录下有 Scala、Java、 Pyithon 和R的示例程序，都可以通过 spark-submit运行。<br> sporik-subomit 提交JAR 包到集群有一定的格式要求，需要设置一些参数，语法如下，</p> 
<p><br><span style="color:#faa572;">./bin/spark-submit --class &lt;main-class&gt;<br> --master &lt;master-url&gt;<br> --deploy-mode &lt;deploy-mode&gt;\<br> --conf &lt;"key=value"&gt;</span></p> 
<p><span style="color:#faa572;">...# other options </span></p> 
<p><span style="color:#faa572;">&lt;appli.cat.ion-jar&gt; )<br> [application-argaments」</span></p> 
<p><br> 如果除了设置运行的脚本名称之外不设置其他参数，那么 Spark 程序默认在本地<br> 运行。</p> 
<p><br> --class：应用程序的入口，指主程序。<br> --master：指定要连接的集群URL。<br> -deploy-mode：是否将驱动程序部署在工作节点(cluster)或本地作为外部客户端<br> (client)。<br> --conf：设置任意 Spark 配登属性，允许使用"key=value WordCount"的格式设置任意的SparkConf 配置属性。<br> application-jar：包含应用程序和所有依赖关系的JAR包的路径。<br> application-arguments：传递给main)方法的参数。<br> 将代码 4-3 所示的程序运行模式更改为打包到集样中运行。程序中无须设置 master 地址、Hadoop 安装包位置。输人、输出路径可通过 spark-submit 指定。</p> 
<h4 id="spark-submit%E5%B8%B8%E7%94%A8%E9%A1%B9%E9%85%8D%E7%BD%AE"><span style="color:#ed7976;">spark-submit常用项配置</span></h4> 
<p><span style="color:#faa572;">ndme Name</span>    设置程序名<br><span style="color:#faa572;">--jars JARS</span>   添加依赖包<br><span style="color:#faa572;">-driver-memory MEM</span>    Driver 程序使用的内存大小<br><span style="color:#faa572;">-executor-memory MEM</span>    Executor使用的内存大小<br><span style="color:#faa572;">-total-executor-cores NUM</span>    Executor使用的总内核数<br><span style="color:#faa572;">--executor-cores NUM   </span> 每个Bxecutor使用的内核数<br><span style="color:#faa572;">-num-executors NUM   </span> 启动的Executor数量<br><span style="color:#faa572;">spark.eventLog.dir</span><br> 保存日志相关信息的路径，可以是“hdfs://” 开头的 HDES 路径，也可以<br> 是“e:// 开头的本地路径，路径均需要提前创建<br><span style="color:#faa572;">spark.eventLog.enabled</span>    是否开启日志记录</p> 
<p><span style="color:#faa572;">spark.cores.max</span><br> 当应用程序运行在 Standalone 集群或粗粒度共享模式 Mesos 集群时，应用<br> 程序向集群（不是每台机器，而是整个集群）请求的最大 CPU内核总数。如果不设置，那么对于 Standalone 集群将使用 spark,deploy. defaultCores 指定的数值，而 Mesos 集群将使用集群中可用的内核</p> 
<h3 id="%E7%AC%AC%E4%BA%94%E7%AB%A0%C2%A0%20%C2%A0%20Spark%20SQL%E2%80%94%E2%80%94%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86" style="margin-left:.31in;text-align:justify;"><span style="color:#6eaad7;">第五章    Spark SQL——结构化数据文件处理</span></h3> 
<h4 id="%E4%B8%80.DataFrame%E7%AE%80%E4%BB%8B" style="margin-left:.31in;text-align:justify;"><span style="color:#ed7976;">一.DataFrame简介</span></h4> 
<div style="margin-left:.31in;text-align:justify;"> 
 <span style="color:#1369b2;">Spark SQL</span> 
 <span style="color:#000000;">使用的数据抽象</span> 
 <span style="color:#1369b2;">并非是</span> 
 <span style="color:#1369b2;">RDD</span> 
 <span style="color:#000000;">，而</span> 
 <span style="color:#1369b2;">是</span> 
 <span style="color:#1369b2;">DataFrame</span> 
 <span style="color:#000000;">。</span> 
</div> 
<div style="margin-left:.31in;text-align:justify;"> 
 <span style="color:#000000;">在</span> 
 <span style="color:#000000;">Spark 1.3.0</span> 
 <span style="color:#000000;">版本之前，</span> 
 <span style="color:#1369b2;">DataFrame</span> 
 <span style="color:#1369b2;">被称为</span> 
 <span style="color:#1369b2;">SchemaRDD</span> 
 <span style="color:#1369b2;">。</span> 
</div> 
<div style="margin-left:.31in;text-align:justify;"> 
 <span style="color:#000000;">DataFrame</span> 
 <span style="color:#000000;">使</span> 
 <span style="color:#000000;">Spark</span> 
 <span style="color:#000000;">具备处理大规模结构化数据的能力。</span> 
</div> 
<div style="margin-left:.31in;text-align:justify;"> 
 <span style="color:#000000;">在</span> 
 <span style="color:#000000;">Spark</span> 
 <span style="color:#000000;">中，</span> 
 <span style="color:#1369b2;">DataFrame</span> 
 <span style="color:#000000;">是一种以</span> 
 <span style="color:#1369b2;">RDD</span> 
 <span style="color:#000000;">为基础的</span> 
 <span style="color:#1369b2;">分布式数据集</span> 
 <span style="color:#000000;">。</span> 
</div> 
<div style="margin-left:.31in;text-align:justify;"> 
 <span style="color:#1369b2;">DataFrame</span> 
 <span style="color:#000000;">的</span> 
 <span style="color:#1369b2;">结构类似传统数据库</span> 
 <span style="color:#000000;">的</span> 
 <span style="color:#1369b2;">二维表格，</span> 
 <span style="color:#000000;">可以从很多数据源中创建，如结构化文件、外部数据库、</span> 
 <span style="color:#000000;">Hive</span> 
 <span style="color:#000000;">表等数据源。</span> 
</div> 
<div style="margin-left:.31in;text-align:justify;"></div> 
<div style="margin-left:.31in;text-align:justify;"></div> 
<p><span style="color:#1369b2;">        DataFrame</span><span style="color:#000000;">可以看作是分布式的</span><span style="color:#1369b2;">Row</span><span style="color:#1369b2;">对象</span><span style="color:#000000;">的</span><span style="color:#1369b2;">集合</span><span style="color:#000000;">，在二维表数据集的每一列都带有名称和类型，这就是Schema元信息，这使得</span><span style="color:#1369b2;">Spark</span><span style="color:#1369b2;">框架可获取</span><span style="color:#000000;">更多</span><span style="color:#1369b2;">数据结构信息</span><span style="color:#000000;">，从而对在</span><span style="color:#000000;">DataFrame</span><span style="color:#000000;">背后的数据源以及作用于</span><span style="color:#000000;">DataFrame</span><span style="color:#000000;">之上数据变换进行针对性的优化，最终达到</span><span style="color:#1369b2;">提升计算效率</span><span style="color:#000000;">。</span></p> 
<p><img alt="" height="327" src="https://images2.imgbox.com/3e/20/3qzqTWV7_o.png" width="680"></p> 
<h4 id="%E4%BA%8C.DataFrame%E7%9A%84%E5%88%9B%E5%BB%BA"><span style="color:#ed7976;">二.DataFrame的创建</span></h4> 
<h5 id="1.%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span style="color:#faa572;">1.数据准备</span></h5> 
<p style="text-align:left;"><span style="color:#000000;">在</span><span style="color:#000000;">HDFS</span><span style="color:#000000;">文件系统中的</span><span style="color:#000000;">/spark</span><span style="color:#000000;">目录中</span><span style="color:#000000;">有一个ff</span><span style="color:#000000;">.txt</span><span style="color:#000000;">文件</span><span style="color:#000000;">，内容如下：</span></p> 
<p style="text-align:left;">1 zhangsan 33<br> 2 lisi 44<br> 3 wangwu 44<br> 4 cc 28<br> 5 xiaoshuai 54<br> 6 xiaomei 88</p> 
<h5 id="2.RDD%E7%9B%B4%E6%8E%A5%E8%BD%AC%E6%8D%A2%E4%B8%BADataFrame" style="text-align:left;"><br><span style="color:#faa572;">2.RDD直接转换为DataFrame</span></h5> 
<p><img alt="" height="178" src="https://images2.imgbox.com/25/2b/KkOd6jJf_o.png" width="723"></p> 
<p><img alt="" height="281" src="https://images2.imgbox.com/aa/e7/kfPREOuY_o.png" width="466"></p> 
<h5 id="3.DataFrame%E7%9A%84%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C"><span style="color:#faa572;">3.DataFrame的常用操作</span></h5> 
<p><img alt="" height="251" src="https://images2.imgbox.com/c5/2c/pqjRqT2O_o.png" width="561"></p> 
<p>示例：</p> 
<p></p> 
<p>展示前三条</p> 
<p><img alt="" height="195" src="https://images2.imgbox.com/7a/f6/40x9xMxB_o.png" width="263"></p> 
<p>对列名进行重命名</p> 
<p><img alt="" height="265" src="https://images2.imgbox.com/d8/62/MnxssFit_o.png" width="724"></p> 
<p>过滤年龄大于或等于44的数据</p> 
<p><img alt="" height="196" src="https://images2.imgbox.com/a2/b1/KvtWYCv9_o.png" width="420"></p> 
<p>根据年龄进行升序或降序排列</p> 
<p><img alt="" height="477" src="https://images2.imgbox.com/55/95/sMOEY2Fe_o.png" width="499"></p> 
<p>按年龄进行分组并统计人数</p> 
<p><img alt="" height="215" src="https://images2.imgbox.com/c6/3f/uLKuV56z_o.png" width="446"></p> 
<p>创建DataFrame对象qsm</p> 
<p><img alt="" height="283" src="https://images2.imgbox.com/e8/9f/MmBhmt7S_o.png" width="759"></p> 
<p></p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/953592e49fda4a693158abb88e541ece/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">MacOS BurpSuite安装指南</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5a146b2c25633c040174bd7505955e8d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Linux文件管理（超详细讲解）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>