<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Spark SQL函数详解：案例解析(第8天) - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/dbffa28cffcb75ede73e7cf6b339b5f7/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Spark SQL函数详解：案例解析(第8天)">
  <meta property="og:description" content="系列文章目录 1- Spark SQL函数定义（掌握）
2- Spark 原生自定义UDF函数案例解析（掌握）
3- Pandas自定义函数案例解析（熟悉）
4- Apache Arrow框架案例解析（熟悉）
5- spark常见面试题
文章目录 系列文章目录前言一、Spark SQL函数定义（掌握）1. 窗口函数2. 自定义函数背景2.1 回顾函数分类标准2.2 自定义函数背景 二、Spark原生自定义UDF函数1. 自定义函数流程1.1 自定义演示一1.2 自定义演示二1.3 自定义演示三 三、Pandas的自定义函数1. Apache Arrow框架2. 基于Arrow完成Pandas和Spark的DataFrame互转3. 基于Pandas自定义函数3.1 自定义函数流程3.2 自定义UDF函数3.3 自定义UDAF函数 四、Spark常见面试题1. Spark client 和Spark cluster的区别？2. Spark常用端口号3. Repartitons和Coalesce区别 前言 本文主要通过案例解析工作中常用的Spark SQL函数，以及应用场景
一、Spark SQL函数定义（掌握） 1. 窗口函数 回顾之前学习过的窗口函数：
分析函数 over(partition by xxx order by xxx [asc|desc] [rows between xxx and xxx]) 分析函数可以大致分成如下3类： 1- 第一类: 聚合函数 sum() count() avg() max() min() 2- 第二类: 排序函数 row_number() rank() dense_rank() 3- 第三类: 其他函数 ntile() first_value() last_value() lead() lag() 三个排序函数的区别?">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-30T11:58:29+08:00">
    <meta property="article:modified_time" content="2024-06-30T11:58:29+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Spark SQL函数详解：案例解析(第8天)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>系列文章目录</h2> 
<p>1- Spark SQL函数定义（掌握）<br> 2- Spark 原生自定义UDF函数案例解析（掌握）<br> 3- Pandas自定义函数案例解析（熟悉）<br> 4- Apache Arrow框架案例解析（熟悉）<br> 5- spark常见面试题</p> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_0" rel="nofollow">系列文章目录</a></li><li><a href="#_12" rel="nofollow">前言</a></li><li><ul><li><a href="#Spark_SQL_17" rel="nofollow">一、Spark SQL函数定义（掌握）</a></li><li><ul><li><a href="#1__19" rel="nofollow">1. 窗口函数</a></li><li><a href="#2__119" rel="nofollow">2. 自定义函数背景</a></li><li><ul><li><a href="#21__121" rel="nofollow">2.1 回顾函数分类标准</a></li><li><a href="#22__137" rel="nofollow">2.2 自定义函数背景</a></li></ul> 
   </li></ul> 
   </li><li><a href="#SparkUDF_172" rel="nofollow">二、Spark原生自定义UDF函数</a></li><li><ul><li><a href="#1___174" rel="nofollow">1. 自定义函数流程</a></li><li><ul><li><a href="#11__205" rel="nofollow">1.1 自定义演示一</a></li><li><a href="#12__292" rel="nofollow">1.2 自定义演示二</a></li><li><a href="#13__373" rel="nofollow">1.3 自定义演示三</a></li></ul> 
   </li></ul> 
   </li><li><a href="#Pandas_461" rel="nofollow">三、Pandas的自定义函数</a></li><li><ul><li><a href="#1__Apache_Arrow_463" rel="nofollow">1. Apache Arrow框架</a></li><li><a href="#2__ArrowPandasSparkDataFrame_501" rel="nofollow">2. 基于Arrow完成Pandas和Spark的DataFrame互转</a></li><li><a href="#3_Pandas_582" rel="nofollow">3. 基于Pandas自定义函数</a></li><li><ul><li><a href="#31__592" rel="nofollow">3.1 自定义函数流程</a></li><li><a href="#32_UDF_621" rel="nofollow">3.2 自定义UDF函数</a></li><li><a href="#33_UDAF_699" rel="nofollow">3.3 自定义UDAF函数</a></li></ul> 
   </li></ul> 
   </li><li><a href="#Spark_777" rel="nofollow">四、Spark常见面试题</a></li><li><ul><li><a href="#1_Spark_client_Spark_cluster_778" rel="nofollow">1. Spark client 和Spark cluster的区别？</a></li><li><a href="#2_Spark_790" rel="nofollow">2. Spark常用端口号</a></li><li><a href="#3_RepartitonsCoalesce_798" rel="nofollow">3. Repartitons和Coalesce区别</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_12"></a>前言</h2> 
<p>本文主要通过案例解析工作中常用的Spark SQL函数，以及应用场景</p> 
<hr> 
<h3><a id="Spark_SQL_17"></a>一、Spark SQL函数定义（掌握）</h3> 
<h4><a id="1__19"></a>1. 窗口函数</h4> 
<p>回顾之前学习过的窗口函数：</p> 
<pre><code class="prism language-properties">分析函数 over(partition by xxx order by xxx [asc|desc] [rows between xxx and xxx])

分析函数可以大致分成如下3类：
1- 第一类: 聚合函数 sum() count() avg() max() min()
2- 第二类: 排序函数 row_number() rank() dense_rank() 
3- 第三类: 其他函数 ntile()  first_value() last_value() lead() lag() 

三个排序函数的区别?
row_number(): 巧记 1234  特点: 唯一且连续
rank(): 巧记 1224 特点: 并列不连续
dense_rank(): 巧记 1223  特点: 并列且连续
</code></pre> 
<p>在Spark SQL中使用窗口函数案例：</p> 
<p>已知数据如下:</p> 
<blockquote> 
 <pre><code>cookie1,2018-04-10,1
cookie1,2018-04-11,5
cookie1,2018-04-12,7
cookie1,2018-04-13,3
cookie1,2018-04-14,2
cookie1,2018-04-15,4
cookie1,2018-04-16,4
cookie2,2018-04-10,2
cookie2,2018-04-11,3
cookie2,2018-04-12,5
cookie2,2018-04-13,6
cookie2,2018-04-14,3
cookie2,2018-04-15,9
cookie2,2018-04-16,7
</code></pre> 
</blockquote> 
<p>需求: 要求找出每个cookie中pv排在前3位的数据，也就是分组取TOPN问题</p> 
<pre><code class="prism language-python"><span class="token comment"># 导包</span>
<span class="token keyword">import</span> os
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql <span class="token keyword">import</span> SparkSession<span class="token punctuation">,</span>functions <span class="token keyword">as</span> F<span class="token punctuation">,</span>Window <span class="token keyword">as</span> W

<span class="token comment"># 绑定指定的python解释器</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'SPARK_HOME'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/export/server/spark'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'PYSPARK_PYTHON'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/root/anaconda3/bin/python3'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'PYSPARK_DRIVER_PYTHON'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/root/anaconda3/bin/python3'</span>

<span class="token comment"># 创建main函数</span>
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token comment"># 1.创建SparkContext对象</span>
    spark <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">.</span>appName<span class="token punctuation">(</span><span class="token string">'pyspark_demo'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>master<span class="token punctuation">(</span><span class="token string">'local[*]'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 2.数据输入</span>
    df <span class="token operator">=</span> spark<span class="token punctuation">.</span>read<span class="token punctuation">.</span>csv<span class="token punctuation">(</span>
        path<span class="token operator">=</span><span class="token string">'file:///export/data/spark_project/spark_sql/data/cookie.txt'</span><span class="token punctuation">,</span>
        sep<span class="token operator">=</span><span class="token string">','</span><span class="token punctuation">,</span>
        schema<span class="token operator">=</span><span class="token string">'cookie string,datestr string,pv int'</span>
    <span class="token punctuation">)</span>
    <span class="token comment"># 3.数据处理(切分,转换,分组聚合)</span>
    <span class="token comment"># 4.数据输出</span>
    etldf <span class="token operator">=</span> df<span class="token punctuation">.</span>dropDuplicates<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>dropna<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># SQL方式</span>
    etldf<span class="token punctuation">.</span>createTempView<span class="token punctuation">(</span><span class="token string">'cookie_logs'</span><span class="token punctuation">)</span>
    spark<span class="token punctuation">.</span>sql<span class="token punctuation">(</span>
        <span class="token triple-quoted-string string">"""
        select cookie,datestr,pv
        from (
           select cookie,datestr,pv,
              dense_rank() over(partition by cookie order by pv desc) as rn
           from cookie_logs
        ) temp where rn &lt;=3 
        """</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># DSL方式</span>
    etldf<span class="token punctuation">.</span>select<span class="token punctuation">(</span>
        <span class="token string">'cookie'</span><span class="token punctuation">,</span> <span class="token string">'datestr'</span><span class="token punctuation">,</span> <span class="token string">'pv'</span><span class="token punctuation">,</span>
        F<span class="token punctuation">.</span>dense_rank<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>over<span class="token punctuation">(</span> W<span class="token punctuation">.</span>partitionBy<span class="token punctuation">(</span><span class="token string">'cookie'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>orderBy<span class="token punctuation">(</span>F<span class="token punctuation">.</span>desc<span class="token punctuation">(</span><span class="token string">'pv'</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'rn'</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>where<span class="token punctuation">(</span><span class="token string">'rn &lt;=3'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token string">'cookie'</span><span class="token punctuation">,</span> <span class="token string">'datestr'</span><span class="token punctuation">,</span> <span class="token string">'pv'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>


    <span class="token comment"># 5.关闭资源</span>
    spark<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<p>运行结果截图：</p> 
<p><img src="https://images2.imgbox.com/1f/d1/92x5h7sK_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="2__119"></a>2. 自定义函数背景</h4> 
<h5><a id="21__121"></a>2.1 回顾函数分类标准</h5> 
<p>SQL函数，主要分为以下三大类：</p> 
<ul><li>UDF函数：普通函数 
  <ul><li>特点：<strong>一对一</strong>，输入一个得到一个</li><li>例如：split() …</li></ul> </li><li>UDAF函数：聚合函数 
  <ul><li>特点：<strong>多对一</strong>，输入多个得到一个</li><li>例如：sum() avg() count() min() max() …</li></ul> </li><li>UDTF函数：表生成函数 
  <ul><li>特点：<strong>一对多</strong>，输入一个得到多个</li><li>例如：explode() …</li></ul> </li></ul> 
<p>在SQL中提供的所有的内置函数，都是属于以上三类中某一类函数</p> 
<h5><a id="22__137"></a>2.2 自定义函数背景</h5> 
<p>思考：有这么多的内置函数，为啥还需要自定义函数呢?</p> 
<pre><code class="prism language-properties">	为了扩充函数功能。在实际使用中，并不能保证所有的操作函数都已经提前的内置好了。很多基于业务处理的功能，其实并没有提供对应的函数，提供的函数更多是以公共功能函数。此时需要进行自定义，来扩充新的功能函数
</code></pre> 
<p>​ 在Spark SQL中，针对Python语言，对于自定义函数，原生支持的并不是特别好。目前原生仅支持自定义UDF函数，而无法自定义UDAF函数和UDTF函数。</p> 
<p>​ 在1.6版本后，Java 和scala语言支持自定义UDAF函数，但Python并不支持。</p> 
<pre><code class="prism language-properties">1- SparkSQL原生的时候，Python只能开发UDF函数
2- SparkSQL借助其他第三方组件(Arrow,pandas...)，Python可以开发UDF、UDAF函数,同时也提升效率
</code></pre> 
<p><img src="https://images2.imgbox.com/2a/48/fQfDCFry_o.png" alt="在这里插入图片描述"></p> 
<p>​ <strong>Spark SQL原生UDF函数存在的问题：大量的序列化和反序列</strong></p> 
<pre><code class="prism language-properties">	虽然Python支持自定义UDF函数，但是其效率并不是特别的高效。因为在使用的时候，传递一行处理一行，返回一行的方式。这样会带来非常大的序列化的开销的问题，导致原生UDF函数效率不好
	
早期解决方案: 基于Java/Scala来编写自定义UDF函数，然后基于python调用即可
	
目前主要的解决方案: 引入Arrow框架，可以基于内存来完成数据传输工作，可以大大的降低了序列化的开销，提供传输的效率，解决原生的问题。同时还可以基于pandas的自定义函数，利用pandas的函数优势完成各种处理操作
</code></pre> 
<p><img src="https://images2.imgbox.com/54/e5/y7YqB5AL_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="SparkUDF_172"></a>二、Spark原生自定义UDF函数</h3> 
<h4><a id="1___174"></a>1. 自定义函数流程</h4> 
<pre><code class="prism language-properties">第一步: 在PySpark中创建一个Python的函数，在这个函数中书写自定义的功能逻辑代码即可

第二步: 将Python函数注册到Spark SQL中
	注册方式一: udf对象 = sparkSession.udf.register(参数1,参数2,参数3)
		参数1: 【UDF函数名称】，此名称用于后续在SQL中使用，可以任意取值，但是要符合名称的规范
		参数2: 【自定义的Python函数】，表示将哪个Python的函数注册为Spark SQL的函数
		参数3: 【UDF函数的返回值类型】。用于表示当前这个Python的函数返回的类型
		udf对象: 返回值对象，是一个UDF对象，可以在DSL中使用
	
		说明: 如果通过方式一来注册函数, 【可以用在SQL和DSL】
	
	注册方式二:  udf对象 = F.udf(参数1,参数2)
		参数1: Python函数的名称，表示将那个Python的函数注册为Spark SQL的函数
		参数2: 返回值的类型。用于表示当前这个Python的函数返回的类型
		udf对象: 返回值对象，是一个UDF对象，可以在DSL中使用
		
		说明: 如果通过方式二来注册函数，【仅能用在DSL中】
		
	注册方式三:  语法糖写法  @F.udf(returnType=返回值类型)  放置到对应Python的函数上面
		说明: 实际是方式二的扩展。如果通过方式三来注册函数，【仅能用在DSL中】
	
		
第三步: 在Spark SQL的 DSL/ SQL 中进行使用即可

</code></pre> 
<h5><a id="11__205"></a>1.1 自定义演示一</h5> 
<p>需求1: 请自定义一个函数，完成对 数据 统一添加一个后缀名的操作 , 例如后缀名 ‘_itheima’</p> 
<p>效果如下:</p> 
<p><img src="https://images2.imgbox.com/b8/21/y34JbXA3_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token comment"># 导包</span>
<span class="token keyword">import</span> os
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql <span class="token keyword">import</span> SparkSession<span class="token punctuation">,</span>functions <span class="token keyword">as</span> F
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>types <span class="token keyword">import</span> StringType

<span class="token comment"># 绑定指定的python解释器</span>

os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'SPARK_HOME'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/export/server/spark'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'PYSPARK_PYTHON'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/root/anaconda3/bin/python3'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'PYSPARK_DRIVER_PYTHON'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/root/anaconda3/bin/python3'</span>

<span class="token comment"># 创建main函数</span>
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token comment"># 1.创建SparkContext对象</span>
    spark <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">.</span>appName<span class="token punctuation">(</span><span class="token string">'pyspark_demo'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>master<span class="token punctuation">(</span><span class="token string">'local[*]'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>


    <span class="token comment"># 2.数据输入</span>
    df <span class="token operator">=</span> spark<span class="token punctuation">.</span>createDataFrame<span class="token punctuation">(</span>
        data<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token string">'张三'</span><span class="token punctuation">,</span><span class="token string">'广州'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token string">'李四'</span><span class="token punctuation">,</span><span class="token string">'深圳'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        schema<span class="token operator">=</span><span class="token string">'id int,name string,address string'</span>
    <span class="token punctuation">)</span>
    df<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 3.SparkSQL自定义udf函数</span>
    <span class="token comment"># 第一步.自定义python函数</span>
    <span class="token keyword">def</span> <span class="token function">add_suffix</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> data<span class="token operator">+</span><span class="token string">'_itheima'</span>

    <span class="token comment"># 第二步.把python函数注册到SparkSQL</span>
    <span class="token comment"># ① spark.udf.register注册</span>
    dsl1_add_suffix <span class="token operator">=</span> spark<span class="token punctuation">.</span>udf<span class="token punctuation">.</span>register<span class="token punctuation">(</span><span class="token string">'sql_add_suffix'</span><span class="token punctuation">,</span>add_suffix<span class="token punctuation">,</span>StringType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># ②F.udf注册</span>
    dsl2_add_suffix <span class="token operator">=</span> F<span class="token punctuation">.</span>udf<span class="token punctuation">(</span>add_suffix<span class="token punctuation">,</span> StringType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># ③@F.udf注册</span>
    <span class="token decorator annotation punctuation">@F<span class="token punctuation">.</span>udf</span><span class="token punctuation">(</span> StringType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">candy_add_suffix</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> data<span class="token operator">+</span><span class="token string">'_itheima'</span>

    <span class="token comment"># 第三步.在SparkSQL中调用自定义函数</span>
    <span class="token comment"># SQL方式</span>
    df<span class="token punctuation">.</span>createTempView<span class="token punctuation">(</span><span class="token string">'temp'</span><span class="token punctuation">)</span>
    spark<span class="token punctuation">.</span>sql<span class="token punctuation">(</span>
        <span class="token triple-quoted-string string">"""select id,name,sql_add_suffix(address) as new_address from temp"""</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token comment"># DSL方式</span>
    <span class="token comment"># 调用dsl1_add_suffix</span>
    df<span class="token punctuation">.</span>select<span class="token punctuation">(</span>
        <span class="token string">'id'</span><span class="token punctuation">,</span> <span class="token string">'name'</span><span class="token punctuation">,</span> dsl1_add_suffix<span class="token punctuation">(</span><span class="token string">'address'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'new_address'</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 调用dsl2_add_suffix</span>
    df<span class="token punctuation">.</span>select<span class="token punctuation">(</span>
        <span class="token string">'id'</span><span class="token punctuation">,</span> <span class="token string">'name'</span><span class="token punctuation">,</span> dsl2_add_suffix<span class="token punctuation">(</span><span class="token string">'address'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'new_address'</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 调用candy_add_suffix</span>
    df<span class="token punctuation">.</span>select<span class="token punctuation">(</span>
        <span class="token string">'id'</span><span class="token punctuation">,</span> <span class="token string">'name'</span><span class="token punctuation">,</span> candy_add_suffix<span class="token punctuation">(</span><span class="token string">'address'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'new_address'</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 4.关闭资源</span>
    spark<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>博主友情提醒: 可能遇到的问题如下</p> 
<p><img src="https://images2.imgbox.com/94/e9/oMM1lSBG_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-properties">原因: 在错误的地方调用了错误的函数。spark.udf.register参数1取的函数名只能在SQL中使用，不能在DSL中用。
</code></pre> 
<h5><a id="12__292"></a>1.2 自定义演示二</h5> 
<p>需求2: 请自定义一个函数，返回值类型为复杂类型: 列表</p> 
<p>效果如下:</p> 
<p><img src="https://images2.imgbox.com/fd/44/eiUGMKnx_o.png" alt="在这里插入图片描述"></p> 
<p>参考代码:</p> 
<pre><code class="prism language-python"><span class="token comment"># 导包</span>
<span class="token keyword">import</span> os
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql <span class="token keyword">import</span> SparkSession<span class="token punctuation">,</span>functions <span class="token keyword">as</span> F
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>types <span class="token keyword">import</span> StringType<span class="token punctuation">,</span> ArrayType

<span class="token comment"># 绑定指定的python解释器</span>

os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'SPARK_HOME'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/export/server/spark'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'PYSPARK_PYTHON'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/root/anaconda3/bin/python3'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'PYSPARK_DRIVER_PYTHON'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/root/anaconda3/bin/python3'</span>

<span class="token comment"># 创建main函数</span>
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token comment"># 1.创建SparkContext对象</span>
    spark <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">.</span>appName<span class="token punctuation">(</span><span class="token string">'pyspark_demo'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>master<span class="token punctuation">(</span><span class="token string">'local[*]'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>


    <span class="token comment"># 2.数据输入</span>
    df <span class="token operator">=</span> spark<span class="token punctuation">.</span>createDataFrame<span class="token punctuation">(</span>
        data<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token string">'张三_广州'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token string">'李四_深圳'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        schema<span class="token operator">=</span><span class="token string">'id int,name_address string'</span>
    <span class="token punctuation">)</span>
    df<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 3.SparkSQL自定义udf函数</span>
    <span class="token comment"># 第一步.自定义python函数</span>
    <span class="token keyword">def</span> <span class="token function">my_split</span><span class="token punctuation">(</span>data<span class="token punctuation">:</span><span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        list1 <span class="token operator">=</span> data<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'_'</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> list1

    <span class="token comment"># 第二步.把python函数注册到SparkSQL</span>
    <span class="token comment"># ① spark.udf.register注册</span>
    dsl1_add_suffix <span class="token operator">=</span> spark<span class="token punctuation">.</span>udf<span class="token punctuation">.</span>register<span class="token punctuation">(</span><span class="token string">'sql_add_suffix'</span><span class="token punctuation">,</span>my_split<span class="token punctuation">,</span>ArrayType<span class="token punctuation">(</span>StringType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># ②F.udf注册</span>
    dsl2_add_suffix <span class="token operator">=</span> F<span class="token punctuation">.</span>udf<span class="token punctuation">(</span>my_split<span class="token punctuation">,</span> ArrayType<span class="token punctuation">(</span>StringType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># ③@F.udf注册</span>
    <span class="token decorator annotation punctuation">@F<span class="token punctuation">.</span>udf</span><span class="token punctuation">(</span>ArrayType<span class="token punctuation">(</span>StringType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">candy_add_suffix</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
        list1 <span class="token operator">=</span> data<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'_'</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> list1

    <span class="token comment"># 第三步.在SparkSQL中调用自定义函数</span>
    <span class="token comment"># SQL方式</span>
    df<span class="token punctuation">.</span>createTempView<span class="token punctuation">(</span><span class="token string">'temp'</span><span class="token punctuation">)</span>
    spark<span class="token punctuation">.</span>sql<span class="token punctuation">(</span>
        <span class="token triple-quoted-string string">"""select id,sql_add_suffix(name_address) as new_address from temp"""</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># DSL方式</span>
    <span class="token comment"># 调用dsl1_add_suffix</span>
    df<span class="token punctuation">.</span>select<span class="token punctuation">(</span>
        <span class="token string">'id'</span><span class="token punctuation">,</span>  dsl1_add_suffix<span class="token punctuation">(</span><span class="token string">'name_address'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'new_name_address'</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 调用dsl2_add_suffix</span>
    df<span class="token punctuation">.</span>select<span class="token punctuation">(</span>
        <span class="token string">'id'</span><span class="token punctuation">,</span>dsl2_add_suffix<span class="token punctuation">(</span><span class="token string">'name_address'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'new_name_address'</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 调用candy_add_suffix</span>
    df<span class="token punctuation">.</span>select<span class="token punctuation">(</span>
        <span class="token string">'id'</span><span class="token punctuation">,</span>candy_add_suffix<span class="token punctuation">(</span><span class="token string">'name_address'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'new_name_address'</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>


    <span class="token comment"># 4.关闭资源</span>
    spark<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="13__373"></a>1.3 自定义演示三</h5> 
<p>需求3: 请自定义一个函数，返回值类型为复杂类型: 字典</p> 
<p>效果如下:</p> 
<p><img src="https://images2.imgbox.com/1f/21/24cyNR1l_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-properties">注意: 注意: 如果是字典类型,StructType中列名需要和字典的key值一致,否则是null补充
</code></pre> 
<pre><code class="prism language-python"><span class="token comment"># 导包</span>
<span class="token keyword">import</span> os
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql <span class="token keyword">import</span> SparkSession<span class="token punctuation">,</span>functions <span class="token keyword">as</span> F
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>types <span class="token keyword">import</span> StringType<span class="token punctuation">,</span> ArrayType<span class="token punctuation">,</span> StructType

<span class="token comment"># 绑定指定的python解释器</span>

os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'SPARK_HOME'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/export/server/spark'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'PYSPARK_PYTHON'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/root/anaconda3/bin/python3'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'PYSPARK_DRIVER_PYTHON'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/root/anaconda3/bin/python3'</span>

<span class="token comment"># 创建main函数</span>
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token comment"># 1.创建SparkContext对象</span>
    spark <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">.</span>appName<span class="token punctuation">(</span><span class="token string">'pyspark_demo'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>master<span class="token punctuation">(</span><span class="token string">'local[*]'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>


    <span class="token comment"># 2.数据输入</span>
    df <span class="token operator">=</span> spark<span class="token punctuation">.</span>createDataFrame<span class="token punctuation">(</span>
        data<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token string">'张三_广州'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token string">'李四_深圳'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        schema<span class="token operator">=</span><span class="token string">'id int,name_address string'</span>
    <span class="token punctuation">)</span>
    df<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 3.SparkSQL自定义udf函数</span>
    <span class="token comment"># 第一步.自定义python函数</span>
    <span class="token keyword">def</span> <span class="token function">my_split</span><span class="token punctuation">(</span>data<span class="token punctuation">:</span><span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        list1 <span class="token operator">=</span> data<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'_'</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span><span class="token string">'name'</span><span class="token punctuation">:</span>list1<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">'address'</span><span class="token punctuation">:</span>list1<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">}</span>

    <span class="token comment"># 第二步.把python函数注册到SparkSQL</span>
    <span class="token comment"># 注意: 如果是字典类型,StructType中列名需要和字典的key值一致,否则是null</span>
    t <span class="token operator">=</span> StructType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token string">'name'</span><span class="token punctuation">,</span>StringType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token string">'address'</span><span class="token punctuation">,</span>StringType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># ① spark.udf.register注册</span>
    dsl1_add_suffix <span class="token operator">=</span> spark<span class="token punctuation">.</span>udf<span class="token punctuation">.</span>register<span class="token punctuation">(</span><span class="token string">'sql_add_suffix'</span><span class="token punctuation">,</span>my_split<span class="token punctuation">,</span>t<span class="token punctuation">)</span>
    <span class="token comment"># ②F.udf注册</span>
    dsl2_add_suffix <span class="token operator">=</span> F<span class="token punctuation">.</span>udf<span class="token punctuation">(</span>my_split<span class="token punctuation">,</span> t<span class="token punctuation">)</span>
    <span class="token comment"># ③@F.udf注册</span>
    <span class="token decorator annotation punctuation">@F<span class="token punctuation">.</span>udf</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">candy_add_suffix</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
        list1 <span class="token operator">=</span> data<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'_'</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span><span class="token string">'name'</span><span class="token punctuation">:</span>list1<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">'address'</span><span class="token punctuation">:</span>list1<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">}</span>

    <span class="token comment"># 第三步.在SparkSQL中调用自定义函数</span>
    <span class="token comment"># SQL方式</span>
    df<span class="token punctuation">.</span>createTempView<span class="token punctuation">(</span><span class="token string">'temp'</span><span class="token punctuation">)</span>
    spark<span class="token punctuation">.</span>sql<span class="token punctuation">(</span>
        <span class="token triple-quoted-string string">"""select id,sql_add_suffix(name_address) as new_name_address from temp"""</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># DSL方式</span>
    <span class="token comment"># 调用dsl1_add_suffix</span>
    df<span class="token punctuation">.</span>select<span class="token punctuation">(</span>
        <span class="token string">'id'</span><span class="token punctuation">,</span> dsl1_add_suffix<span class="token punctuation">(</span><span class="token string">'name_address'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'new_name_address'</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 调用dsl2_add_suffix</span>
    df<span class="token punctuation">.</span>select<span class="token punctuation">(</span>
        <span class="token string">'id'</span><span class="token punctuation">,</span>dsl2_add_suffix<span class="token punctuation">(</span><span class="token string">'name_address'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'new_name_address'</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 调用candy_add_suffix</span>
    df<span class="token punctuation">.</span>select<span class="token punctuation">(</span>
        <span class="token string">'id'</span><span class="token punctuation">,</span>candy_add_suffix<span class="token punctuation">(</span><span class="token string">'name_address'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'new_name_address'</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 4.关闭资源</span>
    spark<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="Pandas_461"></a>三、Pandas的自定义函数</h3> 
<h4><a id="1__Apache_Arrow_463"></a>1. Apache Arrow框架</h4> 
<p>​ Apache Arrow是Apache旗下的一款顶级的项目。是一个跨平台的在内存中以列式存储的数据层，它的设计目标就是作为一个跨平台的数据层，来加快<strong>大数据分析项目的运行效率</strong></p> 
<p>​ Pandas 与 Spark SQL 进行交互的时候，建立在Apache Arrow上，带来低开销 高性能的UDF函数</p> 
<p>​</p> 
<p>如何安装? 三个节点建议都安装（注：集群搭建后续会更新）</p> 
<pre><code class="prism language-properties">检查服务器上是否有安装pyspark
pip list | grep pyspark  或者 conda list | grep pyspark

如果服务器已经安装了pyspark的库，那么仅需要执行以下内容，即可安装。例如在 node1安装
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pyspark[sql]
	
如果服务器中python环境中没有安装pyspark，建议执行以下操作，即可安装。例如在 node2 和 node3安装
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pyarrow==16.1.0
</code></pre> 
<p><img src="https://images2.imgbox.com/eb/0e/mZAqAnl7_o.png" alt="在这里插入图片描述"></p> 
<p><strong>Arrow并不会自动使用，在某些情况下，需要配置 以及在代码中需要进行小的更改才可以使用</strong></p> 
<p>如何使用呢? 默认不会自动启动的, 一般建议手动配置</p> 
<pre><code class="prism language-python">spark<span class="token punctuation">.</span>conf<span class="token punctuation">.</span><span class="token builtin">set</span><span class="token punctuation">(</span><span class="token string">'spark.sql.execution.arrow.pyspark.enabled'</span><span class="token punctuation">,</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="2__ArrowPandasSparkDataFrame_501"></a>2. 基于Arrow完成Pandas和Spark的DataFrame互转</h4> 
<p><strong>Pandas中DataFrame：</strong></p> 
<p>DataFrame：表示一个二维表对象，就是表示整个表</p> 
<p>字段、列、索引；Series表示一列</p> 
<p><img src="https://images2.imgbox.com/0f/89/TNjfmAIF_o.png" alt="在这里插入图片描述"></p> 
<p><strong>Spark SQL中DataFrame：</strong></p> 
<p><img src="https://images2.imgbox.com/7b/56/YSaxv02L_o.png" alt="在这里插入图片描述"></p> 
<p>使用场景：</p> 
<p>1- Spark的DataFrame -&gt; Pandas的DataFrame：当大数据处理到后期的时候，可能数据量会越来越少，这样可以考虑使用单机版的Pandas来做后续数据的分析</p> 
<p>2- Pandas的DataFrame -&gt; Spark的DataFrame：当数据量达到单机无法高效处理的时候，或者需要和其他大数据框架集成的时候，可以转成Spark中的DataFrame</p> 
<pre><code class="prism language-properties">Pandas的DataFrame -&gt; Spark的DataFrame: spark.createDataFrame(data=pandas_df)
Spark的DataFrame -&gt; Pandas的DataFrame: init_df.toPandas()
</code></pre> 
<p>示例:</p> 
<pre><code class="prism language-python"><span class="token comment"># 导包</span>
<span class="token keyword">import</span> os
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql <span class="token keyword">import</span> SparkSession

<span class="token comment"># 绑定指定的python解释器</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'SPARK_HOME'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/export/server/spark'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'PYSPARK_PYTHON'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/root/anaconda3/bin/python3'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'PYSPARK_DRIVER_PYTHON'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/root/anaconda3/bin/python3'</span>

<span class="token comment"># 创建main函数</span>
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token comment"># 1.创建SparkContext对象</span>
    spark <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">.</span>appName<span class="token punctuation">(</span><span class="token string">'pyspark_demo'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>master<span class="token punctuation">(</span><span class="token string">'local[*]'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># TODO: 手动开启arrow框架</span>
    spark<span class="token punctuation">.</span>conf<span class="token punctuation">.</span><span class="token builtin">set</span><span class="token punctuation">(</span><span class="token string">'spark.sql.execution.arrow.pyspark.enabled'</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>

    <span class="token comment"># 2.数据输入</span>
    df <span class="token operator">=</span> spark<span class="token punctuation">.</span>createDataFrame<span class="token punctuation">(</span>
        data<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token string">'张三_广州'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token string">'李四_深圳'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        schema<span class="token operator">=</span><span class="token string">'id int ,name_address string'</span>
    <span class="token punctuation">)</span>
    df<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>df<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'------------------------'</span><span class="token punctuation">)</span>

    <span class="token comment"># 3.数据处理(切分,转换,分组聚合)</span>
    <span class="token comment"># 4.数据输出</span>
    <span class="token comment"># spark-&gt;pandas</span>
    pd_df <span class="token operator">=</span> df<span class="token punctuation">.</span>toPandas<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>pd_df<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>pd_df<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'------------------------'</span><span class="token punctuation">)</span>
    <span class="token comment"># pandas-&gt;spark</span>
    df2 <span class="token operator">=</span> spark<span class="token punctuation">.</span>createDataFrame<span class="token punctuation">(</span>pd_df<span class="token punctuation">)</span>
    df2<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>df2<span class="token punctuation">)</span><span class="token punctuation">)</span>
    

    <span class="token comment"># 5.关闭资源</span>
    spark<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<h4><a id="3_Pandas_582"></a>3. 基于Pandas自定义函数</h4> 
<p>​ 基于Pandas的UDF函数来转换为Spark SQL的UDF函数进行使用。底层是基于Arrow框架来完成数据传输，允许向量化（可以充分利用计算机CPU性能）操作。</p> 
<p>​ Pandas的UDF函数其实本质上就是Python的函数，只不过函数的传入数据类型为<strong>Pandas的类型</strong></p> 
<p>​ <strong>基于Pandas的UDF可以使用自定义UDF函数和自定义UDAF函数</strong></p> 
<h5><a id="31__592"></a>3.1 自定义函数流程</h5> 
<pre><code class="prism language-properties">第一步: 在PySpark中创建一个Python的函数，在这个函数中书写自定义的功能逻辑代码即可

第二步: 将Python函数包装成Spark SQL的函数
	注册方式一: udf对象 = spark.udf.register(参数1, 参数2)
		参数1: UDF函数名称。此名称用于后续在SQL中使用，可以任意取值，但是要符合名称的规范
		参数2: Python函数的名称。表示将哪个Python的函数注册为Spark SQL的函数
		使用: udf对象只能在DSL中使用。参数1指定的名称只能在SQL中使用
		
		
	注册方式二: udf对象 = F.pandas_udf(参数1, 参数2)
		参数1: 自定义的Python函数。表示将哪个Python的函数注册为Spark SQL的函数
		参数2: UDF函数的返回值类型。用于表示当前这个Python的函数返回的类型对应到Spark SQL的数据类型
		udf对象: 返回值对象，是一个UDF对象。仅能用在DSL中使用
	
	注册方式三: 语法糖写法  @F.pandas_udf(returnType)  放置到对应Python的函数上面
		说明: 实际是方式二的扩展。仅能用在DSL中使用
	
第三步: 在Spark SQL的 DSL/ SQL 中进行使用即可

基于pandas方式还支持自定义UDAF函数
注意: 如果要用于自定义UDAF函数,理论上只能用上述注册方式三语法糖方式,也就意味着理论只能DSL使用
注意: 如果还想同时用SQL方式和DSL方式,可以把加了语法糖的函数,再传入到方式register注册,就可以使用了!
</code></pre> 
<h5><a id="32_UDF_621"></a>3.2 自定义UDF函数</h5> 
<ul><li> <p>自定义Python函数的要求：SeriesToSeries</p> <p><img src="https://images2.imgbox.com/bd/52/9EVEHBHd_o.png" alt="在这里插入图片描述"></p> </li></ul> 
<pre><code class="prism language-python"><span class="token comment"># 导包</span>
<span class="token keyword">import</span> os
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql <span class="token keyword">import</span> SparkSession<span class="token punctuation">,</span>functions <span class="token keyword">as</span> F
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd

<span class="token comment"># 绑定指定的python解释器</span>
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>types <span class="token keyword">import</span> LongType<span class="token punctuation">,</span> IntegerType

os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'SPARK_HOME'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/export/server/spark'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'PYSPARK_PYTHON'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/root/anaconda3/bin/python3'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'PYSPARK_DRIVER_PYTHON'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/root/anaconda3/bin/python3'</span>

<span class="token comment"># 创建main函数</span>
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token comment"># 1.创建SparkContext对象</span>
    spark <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">.</span>appName<span class="token punctuation">(</span><span class="token string">'pyspark_demo'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>master<span class="token punctuation">(</span><span class="token string">'local[*]'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># TODO: 开启Arrow的使用</span>
    spark<span class="token punctuation">.</span>conf<span class="token punctuation">.</span><span class="token builtin">set</span><span class="token punctuation">(</span><span class="token string">'spark.sql.execution.arrow.pyspark.enabled'</span><span class="token punctuation">,</span> <span class="token string">'True'</span><span class="token punctuation">)</span>

    <span class="token comment"># 2.数据输入</span>
    df <span class="token operator">=</span> spark<span class="token punctuation">.</span>createDataFrame<span class="token punctuation">(</span>
        data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        schema<span class="token operator">=</span> <span class="token string">'num1 int,num2 int'</span>
    <span class="token punctuation">)</span>
    df<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 3.基于pandas自定义函数 :SeriesTOSeries</span>
    <span class="token comment"># 第一步: 自定义python函数</span>
    <span class="token keyword">def</span> <span class="token function">multiply</span><span class="token punctuation">(</span>num1<span class="token punctuation">:</span>pd<span class="token punctuation">.</span>Series<span class="token punctuation">,</span>num2<span class="token punctuation">:</span>pd<span class="token punctuation">.</span>Series<span class="token punctuation">)</span><span class="token operator">-</span><span class="token operator">&gt;</span>pd<span class="token punctuation">.</span>Series<span class="token punctuation">:</span>
        <span class="token keyword">return</span> num1<span class="token operator">*</span>num2

    <span class="token comment"># 第二步: 把python注册为SparkSQL函数</span>
    <span class="token comment"># ①spark.udf.register注册</span>
    dsl1_multiply <span class="token operator">=</span> spark<span class="token punctuation">.</span>udf<span class="token punctuation">.</span>register<span class="token punctuation">(</span><span class="token string">'sql_multiply'</span><span class="token punctuation">,</span>multiply<span class="token punctuation">)</span>
    <span class="token comment"># ②F.pandas_udf注册</span>
    dsl2_multiply <span class="token operator">=</span> F<span class="token punctuation">.</span>pandas_udf<span class="token punctuation">(</span>multiply<span class="token punctuation">,</span>IntegerType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># ③@F.pandas_udf注册</span>
    <span class="token decorator annotation punctuation">@F<span class="token punctuation">.</span>pandas_udf</span><span class="token punctuation">(</span>IntegerType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">candy_multiply</span><span class="token punctuation">(</span>num1<span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">,</span> num2<span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">:</span>
        <span class="token keyword">return</span> num1 <span class="token operator">*</span> num2

    <span class="token comment"># 第三步: 在SparkSQL中调用注册后函数</span>
    <span class="token comment"># SQL方式</span>
    df<span class="token punctuation">.</span>createTempView<span class="token punctuation">(</span><span class="token string">'temp'</span><span class="token punctuation">)</span>
    spark<span class="token punctuation">.</span>sql<span class="token punctuation">(</span>
        <span class="token triple-quoted-string string">"""select num1,num2,sql_multiply(num1,num2) as result from temp"""</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># DSL方式</span>
    <span class="token comment">#调用dsl1_multiply</span>
    df<span class="token punctuation">.</span>select<span class="token punctuation">(</span>
        <span class="token string">'num1'</span><span class="token punctuation">,</span><span class="token string">'num2'</span><span class="token punctuation">,</span>dsl1_multiply<span class="token punctuation">(</span><span class="token string">'num1'</span><span class="token punctuation">,</span><span class="token string">'num2'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'result'</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 调用dsl2_multiply</span>
    df<span class="token punctuation">.</span>select<span class="token punctuation">(</span>
        <span class="token string">'num1'</span><span class="token punctuation">,</span> <span class="token string">'num2'</span><span class="token punctuation">,</span> dsl2_multiply<span class="token punctuation">(</span><span class="token string">'num1'</span><span class="token punctuation">,</span> <span class="token string">'num2'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'result'</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 调用candy_multiply</span>
    df<span class="token punctuation">.</span>select<span class="token punctuation">(</span>
        <span class="token string">'num1'</span><span class="token punctuation">,</span> <span class="token string">'num2'</span><span class="token punctuation">,</span> candy_multiply<span class="token punctuation">(</span><span class="token string">'num1'</span><span class="token punctuation">,</span> <span class="token string">'num2'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'result'</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 4.关闭资源</span>
    spark<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<h5><a id="33_UDAF_699"></a>3.3 自定义UDAF函数</h5> 
<ul><li> <p>自定义Python函数的要求：Series To 标量</p> <p>表示：<strong>自定义函数的输入数据类型是Pandas中的Series对象，返回值数据类型是标量数据类型。也就是Python中的数据类型，例如：int、float、bool、list…</strong></p> <p><img src="https://images2.imgbox.com/e9/e6/cHyI85Mu_o.png" alt="在这里插入图片描述"></p> </li></ul> 
<pre><code class="prism language-properties">基于pandas方式还支持自定义UDAF函数
注意: 如果要用于自定义UDAF函数,理论上只能用上述注册方式三语法糖方式,也就意味着理论只能DSL使用
注意: 如果还想同时用SQL方式和DSL方式,可以把加了语法糖的函数,再传入到方式register注册,就可以使用了!
</code></pre> 
<pre><code class="prism language-python"><span class="token comment"># 导包</span>
<span class="token keyword">import</span> os
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql <span class="token keyword">import</span> SparkSession<span class="token punctuation">,</span> functions <span class="token keyword">as</span> F
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd

<span class="token comment"># 绑定指定的python解释器</span>
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>types <span class="token keyword">import</span> LongType<span class="token punctuation">,</span> IntegerType<span class="token punctuation">,</span> FloatType

os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'SPARK_HOME'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/export/server/spark'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'PYSPARK_PYTHON'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/root/anaconda3/bin/python3'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'PYSPARK_DRIVER_PYTHON'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'/root/anaconda3/bin/python3'</span>

<span class="token comment"># 创建main函数</span>
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token comment"># 1.创建SparkContext对象</span>
    spark <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">.</span>appName<span class="token punctuation">(</span><span class="token string">'pyspark_demo'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>master<span class="token punctuation">(</span><span class="token string">'local[*]'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># TODO: 开启Arrow的使用</span>
    spark<span class="token punctuation">.</span>conf<span class="token punctuation">.</span><span class="token builtin">set</span><span class="token punctuation">(</span><span class="token string">'spark.sql.execution.arrow.pyspark.enabled'</span><span class="token punctuation">,</span> <span class="token string">'True'</span><span class="token punctuation">)</span>

    <span class="token comment"># 2.数据输入</span>
    df <span class="token operator">=</span> spark<span class="token punctuation">.</span>createDataFrame<span class="token punctuation">(</span>
        data<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">10.0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        schema<span class="token operator">=</span><span class="token string">'id int,value float'</span>
    <span class="token punctuation">)</span>
    df<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>


    <span class="token comment"># 3.基于pandas自定义函数 :SeriesTOSeries</span>
    <span class="token comment"># 第一步: 自定义python函数</span>
    <span class="token comment"># ③@F.pandas_udf注册  注意: 理论上UDAF只能用注册方式三语法糖方式,也就意味着只能DSL使用</span>
    <span class="token decorator annotation punctuation">@F<span class="token punctuation">.</span>pandas_udf</span><span class="token punctuation">(</span>FloatType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">candy_mean_v</span><span class="token punctuation">(</span>value<span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">float</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> value<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>


    <span class="token comment"># 第二步: 注意: 如果还想同时用SQL方式和DSL方式,可以把加了语法糖的函数,再传入到方式一register注册</span>
    <span class="token comment"># ①spark.udf.register注册</span>
    dsl1_mean_v <span class="token operator">=</span> spark<span class="token punctuation">.</span>udf<span class="token punctuation">.</span>register<span class="token punctuation">(</span><span class="token string">'sql_mean_v'</span><span class="token punctuation">,</span> candy_mean_v<span class="token punctuation">)</span>

    <span class="token comment"># 第三步: 在SparkSQL中调用注册后函数</span>
    <span class="token comment"># DSL方式</span>
    <span class="token comment"># 调用candy_mean_v</span>
    df<span class="token punctuation">.</span>groupBy<span class="token punctuation">(</span><span class="token string">'id'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>agg<span class="token punctuation">(</span>
        candy_mean_v<span class="token punctuation">(</span><span class="token string">'value'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'result'</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 调用dsl1_mean_v</span>
    df<span class="token punctuation">.</span>groupBy<span class="token punctuation">(</span><span class="token string">'id'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>agg<span class="token punctuation">(</span>
        dsl1_mean_v<span class="token punctuation">(</span><span class="token string">'value'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">'result'</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># SQL方式</span>
    df<span class="token punctuation">.</span>createTempView<span class="token punctuation">(</span><span class="token string">'temp'</span><span class="token punctuation">)</span>
    spark<span class="token punctuation">.</span>sql<span class="token punctuation">(</span>
        <span class="token triple-quoted-string string">"""select id,sql_mean_v(value) as result from temp group by id"""</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 4.关闭资源</span>
    spark<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="Spark_777"></a>四、Spark常见面试题</h3> 
<h4><a id="1_Spark_client_Spark_cluster_778"></a>1. Spark client 和Spark cluster的区别？</h4> 
<pre><code>区别是driver 进程在哪运行，client模式driver运行在master节点上，不在worker节点上；cluster模式
driver运行在worker集群某节点上，不在master节点上。
一般来说，如果提交任务的节点（即Master）和Worker集群在同一个网络内，此时client mode比较合
适。
如果提交任务的节点和Worker集群相隔比较远，就会采用cluster mode来最小化Driver和Executor之间
的网络延迟。
yarn client模式：driverzai当前提交任务的节点上，可以打印任务运行的日志信息。
yarn cluster模式：driver在AppMaster所有节点上，分布式分配，不能再提交任务的本机打印日志信
息。
</code></pre> 
<h4><a id="2_Spark_790"></a>2. Spark常用端口号</h4> 
<pre><code>Spark-shell任务端口：4040
内部通讯端口：7077
查看任务执行情况端口：8080
历史服务器：18080
Oozie端口号：11000
</code></pre> 
<h4><a id="3_RepartitonsCoalesce_798"></a>3. Repartitons和Coalesce区别</h4> 
<ul><li>关系：两者都是用来改变 RDD 的 partition 数量的，repartition 底层调用的就是 coalesce 方 法：<br> coalesce(numPartitions, shuffle = true)</li><li>区别：repartition 一定会发生 shuffle，coalesce 根据传入的参数来判断是否发生 shuffle 一般情况<br> 下增大 rdd 的 partition 数量使用 repartition，减少 partition 数量时使用 coalesce。</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/70f1c6cbff7c05fe8c028f02a77d8572/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">hive架构详解：HQL案例解析(第15天)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/93aeafa18a31bdc6425d8f7265b6eecd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Spark Core内核调度机制详解(第5天）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>