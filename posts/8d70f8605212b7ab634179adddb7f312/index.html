<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ubuntu 部署spark - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/8d70f8605212b7ab634179adddb7f312/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="ubuntu 部署spark">
  <meta property="og:description" content="的方式似懂非懂目录
一、 conda环境
二、PySpark环境安装
三、Spark On YARN环境搭建
1. 先解压
2. 配置spark-env.sh
3. 修改hadoop的yarn-site.xml
4. 配置依赖spark jar包
5. 修改spark-defaults.conf
6. 修改spark-env.sh
7. 建议配上python
四、提交测试
首先下载spark安装包，Downloads | Apache Spark
选择对应hadoop版本的spark下载，其他版本链接：News | Apache Spark
-----------------------------------------------------------------------------------
一、 conda环境 为了隔离环境方便（ubuntu自带有python了，在虚拟环境单独再搞一个），根据ubuntu中安装miniconda3-py_ubuntu安装miniconda3-CSDN博客
安装好conda.
配置好源后，配置变量
vim ~/.bashrc
export ANACONDA_HOME=/home/peng/miniconda3 export PATH=$PATH:$ANACONDA_HOME/bin 根据自己的路径更改
使其生效 source ~/.bashrc
这时发现，终端会自动进入conda的base环境，可以关闭
conda config --set auto_activate_base false 创建pyspark_env环境
conda create -n pyspark_env python=3.8 期间需要下载各种依赖包，都 y 就行
安装完成后，可查看当前安装的env。
conda env list ----------------------
二、PySpark环境安装 摘自黑马：PySpark: 是Python的库, 由Spark官方提供.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-28T11:45:20+08:00">
    <meta property="article:modified_time" content="2024-05-28T11:45:20+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ubuntu 部署spark</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><strong>的方式似懂非懂目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%20conda%E7%8E%AF%E5%A2%83-toc" style="margin-left:80px;"><a href="#%E4%B8%80%E3%80%81%20conda%E7%8E%AF%E5%A2%83" rel="nofollow">一、 conda环境</a></p> 
<p id="%E4%BA%8C%E3%80%81PySpark%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85-toc" style="margin-left:80px;"><a href="#%E4%BA%8C%E3%80%81PySpark%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85" rel="nofollow">二、PySpark环境安装</a></p> 
<p id="%E4%B8%89%E3%80%81Spark%20On%20YARN%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA-toc" style="margin-left:80px;"><a href="#%E4%B8%89%E3%80%81Spark%20On%20YARN%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA" rel="nofollow">三、Spark On YARN环境搭建</a></p> 
<p id="1.%20%E5%85%88%E8%A7%A3%E5%8E%8B-toc" style="margin-left:160px;"><a href="#1.%20%E5%85%88%E8%A7%A3%E5%8E%8B" rel="nofollow">1. 先解压</a></p> 
<p id="2.%20%E9%85%8D%E7%BD%AEspark-env.sh-toc" style="margin-left:160px;"><a href="#2.%20%E9%85%8D%E7%BD%AEspark-env.sh" rel="nofollow">2. 配置spark-env.sh</a></p> 
<p id="3.%20%E4%BF%AE%E6%94%B9hadoop%E7%9A%84yarn-site.xml-toc" style="margin-left:160px;"><a href="#3.%20%E4%BF%AE%E6%94%B9hadoop%E7%9A%84yarn-site.xml" rel="nofollow">3. 修改hadoop的yarn-site.xml</a></p> 
<p id="4.%20%E9%85%8D%E7%BD%AE%E4%BE%9D%E8%B5%96spark%20jar%E5%8C%85-toc" style="margin-left:160px;"><a href="#4.%20%E9%85%8D%E7%BD%AE%E4%BE%9D%E8%B5%96spark%20jar%E5%8C%85" rel="nofollow">4. 配置依赖spark jar包</a></p> 
<p id="5.%20%E4%BF%AE%E6%94%B9spark-defaults.conf-toc" style="margin-left:160px;"><a href="#5.%20%E4%BF%AE%E6%94%B9spark-defaults.conf" rel="nofollow">5. 修改spark-defaults.conf</a></p> 
<p id="6.%20%E4%BF%AE%E6%94%B9spark-env.sh-toc" style="margin-left:160px;"><a href="#6.%20%E4%BF%AE%E6%94%B9spark-env.sh" rel="nofollow">6. 修改spark-env.sh</a></p> 
<p id="7.%20%E5%BB%BA%E8%AE%AE%E9%85%8D%E4%B8%8Apython-toc" style="margin-left:160px;"><a href="#7.%20%E5%BB%BA%E8%AE%AE%E9%85%8D%E4%B8%8Apython" rel="nofollow">7. 建议配上python</a></p> 
<p id="%E6%8F%90%E4%BA%A4%E6%B5%8B%E8%AF%95-toc" style="margin-left:80px;"><a href="#%E6%8F%90%E4%BA%A4%E6%B5%8B%E8%AF%95" rel="nofollow">四、提交测试</a></p> 
<hr id="hr-toc"> 
<p></p> 
<p>首先下载spark安装包，<a href="https://spark.apache.org/downloads.html" rel="nofollow" title="Downloads | Apache Spark">Downloads | Apache Spark</a></p> 
<p>选择对应hadoop版本的spark下载，其他版本链接：<a href="https://spark.apache.org/news/index.html" rel="nofollow" title="News | Apache Spark">News | Apache Spark</a></p> 
<p>-----------------------------------------------------------------------------------</p> 
<h4 id="%E4%B8%80%E3%80%81%20conda%E7%8E%AF%E5%A2%83">一、 conda环境</h4> 
<p>为了隔离环境方便（ubuntu自带有python了，在虚拟环境单独再搞一个），根据<a href="https://blog.csdn.net/pzy0668/article/details/137061665" title="ubuntu中安装miniconda3-py_ubuntu安装miniconda3-CSDN博客">ubuntu中安装miniconda3-py_ubuntu安装miniconda3-CSDN博客</a></p> 
<p>安装好conda.</p> 
<p>配置好源后，配置变量</p> 
<p>vim  ~/.bashrc</p> 
<pre><code>export ANACONDA_HOME=/home/peng/miniconda3
export PATH=$PATH:$ANACONDA_HOME/bin</code></pre> 
<p>根据自己的路径更改</p> 
<p>使其生效    source  ~/.bashrc</p> 
<p><img alt="" class="left" height="45" src="https://images2.imgbox.com/af/a5/ILFBoB8E_o.png" width="347"></p> 
<p>这时发现，终端会自动进入conda的base环境，可以关闭</p> 
<pre><code>conda config --set auto_activate_base false</code></pre> 
<p><img alt="" class="left" height="40" src="https://images2.imgbox.com/70/4d/Yp4uVj9X_o.png" width="615"></p> 
<p>创建pyspark_env环境</p> 
<pre><code> conda create -n pyspark_env python=3.8</code></pre> 
<p>期间需要下载各种依赖包，都  y   就行</p> 
<p>安装完成后，可查看当前安装的env。</p> 
<pre><code>conda env list</code></pre> 
<p><img alt="" class="left" height="128" src="https://images2.imgbox.com/74/99/oWhgywxq_o.png" width="604"></p> 
<p>----------------------</p> 
<h4 id="%E4%BA%8C%E3%80%81PySpark%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85">二、PySpark环境安装</h4> 
<p>摘自黑马：<strong><strong>PySpark: 是Python的库, 由Spark官方提供. 专供Python语言使用. 类似Pandas一样,是一个库</strong></strong></p> 
<p>先激活新创建的环境</p> 
<pre><code>conda activate pyspark_env</code></pre> 
<p><img alt="" class="left" height="60" src="https://images2.imgbox.com/f9/62/u678OaMa_o.png" width="475"></p> 
<p>然后，在pyspark_env环境中，可以用pip install pyspark  或者<span style="color:#fe2c24;"><strong> conda install pyspark==3.2.1 </strong></span>安装pyspark   <span style="color:#b95514;"> 为了后续flink安装依赖包一致，下图pyspark后面加上==3.2.1</span></p> 
<p><img alt="" height="484" src="https://images2.imgbox.com/21/bd/GfecjyXi_o.png" width="739"></p> 
<h4 id="%E4%B8%89%E3%80%81Spark%20On%20YARN%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">三、Spark On YARN环境搭建</h4> 
<p><span style="color:#fe2c24;">如果是分布式的集群，下面步骤均需同步到每个节点。该教程参考自黑马程序员，非常感谢！ </span></p> 
<p>直接用之前电脑的存档文件，各位可以从官网下载到合适目录，我放在  /home/peng/software  （硬盘换了，重新安装后，所有软件都放在这个目录了，先安装了hadoop3.2.4）下</p> 
<h6 id="1.%20%E5%85%88%E8%A7%A3%E5%8E%8B">1. 先解压</h6> 
<p>cd /home/peng/software</p> 
<p>tar -zxvf spark-3.2.2-bin-hadoop3.2.tgz </p> 
<p>创建spark的软连接：</p> 
<pre><code>ln  -s  spark-3.2.2-bin-hadoop3.2  spark</code></pre> 
<h6 id="2.%20%E9%85%8D%E7%BD%AEspark-env.sh">2. 配置spark-env.sh</h6> 
<p>先进入spark/conf目录</p> 
<p>cd   spark/conf/</p> 
<p>cp spark-env.sh.template spark-env.sh</p> 
<p>vim spark-env.sh</p> 
<p>在配置地方，加入下面两行</p> 
<pre><code>HADOOP_CONF_DIR=/home/peng/software/hadoop-3.2.4/etc/hadoop
YARN_CONF_DIR=/home/peng/software/hadoop-3.2.4/etc/hadoop</code></pre> 
<h6><span style="color:#fe2c24;">注意：如果用的是spark-without-hadoop的spark安装包，需要在上面多添加一句：</span></h6> 
<p><span style="color:#956fe7;">（如，用的是spark-3.2.1-bin-without-hadoop.tgz，而hadoop是3.1.3的话，加一句下面的，否则不需要添加）</span></p> 
<pre><code>export SPARK_DIST_CLASSPATH=$(/home/peng/software/hadoop-3.1.3/bin/hadoop classpath)</code></pre> 
<h6 id="3.%20%E4%BF%AE%E6%94%B9hadoop%E7%9A%84yarn-site.xml">3. 修改hadoop的yarn-site.xml</h6> 
<pre><code>vi /home/peng/software/hadoop-3.2.4/etc/hadoop/yarn-site.xml </code></pre> 
<p>内容如下（注意其中的master改为你节点的名称或者ip，且是hdfs的主机，<u><span style="color:#fe2c24;"><strong>之前hdfs配置的是localhost，那么下面spark配置里面的所有master都需要改为localhost</strong></span></u>）：</p> 
<pre><code>&lt;configuration&gt;
    &lt;!-- 配置yarn主节点的位置 --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
        &lt;value&gt;master&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 设置yarn集群的内存分配方案 --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
        &lt;value&gt;20480&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;
        &lt;value&gt;2048&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;
        &lt;value&gt;2.1&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 开启日志聚合功能 --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 设置聚合日志在hdfs上的保存时间 --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;
        &lt;value&gt;604800&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 设置yarn历史服务器地址 --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.log.server.url&lt;/name&gt;
        &lt;value&gt;http://master:19888/jobhistory/logs&lt;/value&gt;
    &lt;/property&gt;
    &lt;!-- 关闭yarn内存检查 --&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;
        &lt;value&gt;false&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;
        &lt;value&gt;false&lt;/value&gt;
    &lt;/property&gt;

&lt;/configuration&gt;
</code></pre> 
<p>设置spark的历史服务器地址</p> 
<pre><code>cd /home/peng/software/spark/conf
cp spark-defaults.conf.template spark-defaults.conf
vi spark-defaults.conf</code></pre> 
<p>末尾添加下面内容（注意：改好master( master--&gt;localhost)，并在hdfs根目录下创建sparklog目录）：</p> 
<pre><code>spark.eventLog.enabled                  true
spark.eventLog.dir                      hdfs://master:9000/sparklog/
spark.eventLog.compress                 true
spark.yarn.historyServer.address        master:18080</code></pre> 
<p>设置日志级别</p> 
<pre><code>hadoop fs -mkdir /sparklog

cd /home/peng/software/spark/conf
cp log4j.properties.template log4j.properties
vi log4j.properties</code></pre> 
<p>设置 INFO  改为  WARN</p> 
<p><img alt="" height="94" src="https://images2.imgbox.com/76/24/2EkUdjOu_o.png" width="617"></p> 
<h6 id="4.%20%E9%85%8D%E7%BD%AE%E4%BE%9D%E8%B5%96spark%20jar%E5%8C%85">4. 配置依赖spark jar包</h6> 
<p><strong><span style="color:#c00000;"><strong>当Spark Application应用提交运行在YARN上时，默认情况下，每次提交应用都需要将依赖</strong></span></strong><strong><span style="background-color:#ffff00;"><span style="color:#c00000;"><strong>Spark相关jar包上传到YARN 集群中</strong></span></span></strong><strong><span style="color:#c00000;"><strong>，为了节省提交时间和存储空间，将Spark相关jar包上传到HDFS目录中，</strong></span></strong>设置属性告知Spark Application应用</p> 
<p>先在hadoop创建存放路径（先启动hdfs：strat-dfs.sh），再上传相关jar文件</p> 
<pre><code>start-all.sh
hadoop fs -mkdir -p /spark/jars
hadoop fs -put /home/peng/software/spark/jars/*  /spark/jars</code></pre> 
<h6 id="5.%20%E4%BF%AE%E6%94%B9spark-defaults.conf">5. 修改spark-defaults.conf</h6> 
<pre><code>cd /home/peng/software/spark/conf/
vi spark-defaults.conf</code></pre> 
<p>末尾添加下面一行（master —&gt;localhost）</p> 
<pre><code>spark.yarn.jars  hdfs://master:9000/spark/jars/*</code></pre> 
<h6 id="6.%20%E4%BF%AE%E6%94%B9spark-env.sh">6. 修改spark-env.sh</h6> 
<p>添加下面一行，注意master( master -&gt; localhost)</p> 
<pre><code>export SPARK_HISTORY_OPTS="
-Dspark.history.ui.port=18080
-Dspark.history.fs.logDirectory=hdfs://master:9000/sparklog
-Dspark.history.retainedApplications=30"</code></pre> 
<h6 id="7.%20%E5%BB%BA%E8%AE%AE%E9%85%8D%E4%B8%8Apython">7. 建议配上python</h6> 
<pre><code>vi ~/.bashrc

# 添加下面两行
export PYSPARK_PYTHON=/home/peng/miniconda3/envs/pyspark_env/bin/python
export PYSPARK_DRIVER_PYTHON=/home/peng/miniconda3/envs/pyspark_env/bin/python</code></pre> 
<p>顺便配上SPARK_HOME</p> 
<pre><code>export SPARK_HOME=/home/peng/software/spark
export PATH=$PATH:$SPARK_HOME/bin</code></pre> 
<p>使生效，source  ~/.bashrc</p> 
<p><span style="color:#fe2c24;"><strong>-----如果是集群模式，上面内容需要同步到其他节点---------------------------</strong></span></p> 
<h4 id="%E6%8F%90%E4%BA%A4%E6%B5%8B%E8%AF%95">四、提交测试</h4> 
<p><img alt="" height="157" src="https://images2.imgbox.com/8e/a0/6fkil0rO_o.png" width="764"></p> 
<p>代码如下：</p> 
<pre><code>spark-submit \
--master yarn \
/home/peng/software/spark/examples/src/main/python/pi.py \
10</code></pre> 
<p>结果如下（确保sparklog已经被创建）：</p> 
<p><img alt="" height="294" src="https://images2.imgbox.com/cd/f8/NC7nc0YM_o.png" width="1200"></p> 
<p>-------------------------------------------------------------------</p> 
<p>至此，完成了spark on yarn 的配置，单机版不用这么麻烦，但为了以后集群使用，故采用该方式。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0b425022c226c3a5a897a0d2add4ab1c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">pycharm配置anaconda环境时找不到python.exe解决办法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7bac797a7b8cfcd5474d56318110a1fe/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【机器学习】探究DQN通过训练来解决AI序列决策问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>