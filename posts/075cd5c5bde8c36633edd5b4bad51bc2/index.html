<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习中卷积算子和dropout算子的作用 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/075cd5c5bde8c36633edd5b4bad51bc2/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="深度学习中卷积算子和dropout算子的作用">
  <meta property="og:description" content="笔者在调网络的时候，有时调细一些在想不同卷积核尺寸的卷积操作有啥区别，在哪些算子后用dropout会比较好呢？于是有了下面一段总结。
文章目录 一、卷积核尺寸1X1和3X3的区别1x1卷积核3x3卷积核 二、dropout的作用使用情况算子组合注意事项 一、卷积核尺寸1X1和3X3的区别 在卷积神经网络中，1x1和3x3卷积核有着不同的用途和特点，各自适用于不同的情况和目标。以下是它们的主要区别：
1x1卷积核 功能： 通道间信息整合：1x1卷积主要用于改变通道数，通过线性组合不同通道的信息，可以实现跨通道的信息融合。降维和升维：1x1卷积可以用来减少特征图的通道数，从而降低计算成本；也可以用来增加通道数，提升特征表达能力。非线性映射：虽然1x1卷积没有空间上的感受野，但它可以引入非线性激活函数，使得模型具备更强的表达能力。 优点： 计算效率高：1x1卷积的计算量很小，非常高效。参数少：参数量小，有助于减小模型的复杂度。 应用场景： 瓶颈层（Bottleneck）：在ResNet等网络中，通过1x1卷积先降维再升维，减少计算量。特征融合：在Inception网络中，用于融合不同尺度的特征。 3x3卷积核 功能： 局部特征提取：3x3卷积核有一个较小的感受野，但比1x1卷积能捕捉更多的空间信息，是最常用的卷积核尺寸。平滑和细化：通过3x3卷积，可以实现对图像局部区域的平滑和细化，提取更细致的特征。 优点： 较好的感受野：相比1x1卷积，3x3卷积有更大的感受野，可以捕捉更广泛的局部信息。计算复杂度适中：3x3卷积在计算效率和感受野之间取得了较好的平衡，是一种常见且有效的选择。 应用场景： 特征提取：广泛应用于各种卷积层，用于提取图像的局部特征。深度网络：在VGG、ResNet等深度网络中，大量使用3x3卷积层，形成深层次的特征表示。
总结 1x1卷积：主要用于通道间的信息整合和调整通道数，计算效率高，参数少。3x3卷积：用于提取局部空间特征，具有较好的感受野和适中的计算复杂度。 总结一下：增加通道的时候一般用大尺寸的卷积核，因为打的卷积核有更大的感受野，更能捕捉到更多的局部特征，增加通道正合适。反之就用1X1的卷积核。
二、dropout的作用 Dropout是一种常见的正则化技术，用于防止神经网络的过拟合问题。它通过在训练过程中随机地将一部分神经元的输出设置为零，从而减少模型对特定神经元的依赖，提高模型的泛化能力。Dropout在以下情况下和算子组合中使用较为常见：
使用情况 防止过拟合： 当训练数据较少或模型过于复杂时，容易发生过拟合现象。Dropout可以有效地防止过拟合，提升模型在测试数据上的表现。 大型神经网络： 在深层神经网络（如全连接层、卷积神经网络等）中，特别是当网络层数较多时，使用Dropout可以防止网络过度拟合训练数据。 训练阶段： Dropout通常只在训练阶段使用。在测试阶段，神经元的所有输出都会保留，并且会将训练时的Dropout概率考虑进去，以便于保持输出的一致性。 算子组合 全连接层（Fully Connected Layer）： Dropout在全连接层中使用非常普遍。在每个训练步骤中，随机将部分神经元的输出设置为零。例如：model.add(Dense(128, activation=&#39;relu&#39;)) model.add(Dropout(0.5)) # 50%的神经元输出被随机丢弃 卷积层（Convolutional Layer）： 在卷积层后也可以使用Dropout，虽然较少见，但在一些较复杂的网络结构中会使用。例如：model.add(Conv2D(64, (3, 3), activation=&#39;relu&#39;)) model.add(Dropout(0.25)) # 25%的神经元输出被随机丢弃 循环神经网络（RNN/LSTM/GRU）： 在循环神经网络中，Dropout也被广泛使用，通常称为“时间Dropout”或“变换Dropout”。例如：model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) 正则化组合： Dropout可以与其他正则化方法组合使用，如L2正则化、Batch Normalization等。例如：model.add(Dense(128, activation=&#39;relu&#39;, kernel_regularizer=l2(0.01))) model.add(Dropout(0.5)) model.add(BatchNormalization()) 在分类任务的最后一层之前： Dropout通常在输出层之前使用，确保最后一层的神经元不会被丢弃。例如：model.add(Dense(128, activation=&#39;relu&#39;)) model.add(Dropout(0.5)) model.add(Dense(num_classes, activation=&#39;softmax&#39;)) 注意事项 Dropout率选择：通常的Dropout率为0.2到0.5，具体取决于任务和模型复杂度。测试阶段：在测试阶段应关闭Dropout，通过使用训练时的Dropout率缩放神经元的输出。计算开销：Dropout增加了一些计算开销，尤其是在大型网络中。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-02T18:11:00+08:00">
    <meta property="article:modified_time" content="2024-08-02T18:11:00+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习中卷积算子和dropout算子的作用</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><img src="https://images2.imgbox.com/7f/ef/wWonRPlB_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>笔者在调网络的时候，有时调细一些在想不同卷积核尺寸的卷积操作有啥区别，在哪些算子后用dropout会比较好呢？于是有了下面一段总结。</p> 
</blockquote> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#1X13X3_7" rel="nofollow">一、卷积核尺寸1X1和3X3的区别</a></li><li><ul><li><a href="#1x1_9" rel="nofollow">1x1卷积核</a></li><li><a href="#3x3_20" rel="nofollow">3x3卷积核</a></li></ul> 
  </li><li><a href="#dropout_36" rel="nofollow">二、dropout的作用</a></li><li><ul><li><a href="#_38" rel="nofollow">使用情况</a></li><li><a href="#_45" rel="nofollow">算子组合</a></li><li><a href="#_78" rel="nofollow">注意事项</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="1X13X3_7"></a>一、卷积核尺寸1X1和3X3的区别</h2> 
<p>在卷积神经网络中，1x1和3x3卷积核有着不同的用途和特点，各自适用于不同的情况和目标。以下是它们的主要区别：</p> 
<h3><a id="1x1_9"></a>1x1卷积核</h3> 
<ul><li>功能： 
  <ul><li>通道间信息整合：1x1卷积主要用于改变通道数，通过线性组合不同通道的信息，可以实现跨通道的信息融合。</li><li>降维和升维：1x1卷积可以用来减少特征图的通道数，从而降低计算成本；也可以用来增加通道数，提升特征表达能力。</li><li>非线性映射：虽然1x1卷积没有空间上的感受野，但它可以引入非线性激活函数，使得模型具备更强的表达能力。</li></ul> </li><li>优点： 
  <ul><li>计算效率高：1x1卷积的计算量很小，非常高效。</li><li>参数少：参数量小，有助于减小模型的复杂度。</li></ul> </li><li>应用场景： 
  <ul><li>瓶颈层（Bottleneck）：在ResNet等网络中，通过1x1卷积先降维再升维，减少计算量。</li><li>特征融合：在Inception网络中，用于融合不同尺度的特征。</li></ul> </li></ul> 
<h3><a id="3x3_20"></a>3x3卷积核</h3> 
<ul><li>功能： 
  <ul><li>局部特征提取：3x3卷积核有一个较小的感受野，但比1x1卷积能捕捉更多的空间信息，是最常用的卷积核尺寸。</li><li>平滑和细化：通过3x3卷积，可以实现对图像局部区域的平滑和细化，提取更细致的特征。</li></ul> </li><li>优点： 
  <ul><li>较好的感受野：相比1x1卷积，3x3卷积有更大的感受野，可以捕捉更广泛的局部信息。</li><li>计算复杂度适中：3x3卷积在计算效率和感受野之间取得了较好的平衡，是一种常见且有效的选择。</li></ul> </li><li>应用场景： 
  <ul><li>特征提取：广泛应用于各种卷积层，用于提取图像的局部特征。</li><li>深度网络：在VGG、ResNet等深度网络中，大量使用3x3卷积层，形成深层次的特征表示。<br> 总结</li></ul> </li><li>1x1卷积：主要用于通道间的信息整合和调整通道数，计算效率高，参数少。</li><li>3x3卷积：用于提取局部空间特征，具有较好的感受野和适中的计算复杂度。</li></ul> 
<p><strong>总结一下</strong>：增加通道的时候一般用大尺寸的卷积核，因为打的卷积核有更大的感受野，更能捕捉到更多的局部特征，增加通道正合适。反之就用1X1的卷积核。</p> 
<h2><a id="dropout_36"></a>二、dropout的作用</h2> 
<p>Dropout是一种常见的正则化技术，用于防止神经网络的过拟合问题。它通过在训练过程中随机地将一部分神经元的输出设置为零，从而减少模型对特定神经元的依赖，提高模型的泛化能力。Dropout在以下情况下和算子组合中使用较为常见：</p> 
<h3><a id="_38"></a>使用情况</h3> 
<ol><li>防止过拟合： 
  <ul><li>当训练数据较少或模型过于复杂时，容易发生过拟合现象。Dropout可以有效地防止过拟合，提升模型在测试数据上的表现。</li></ul> </li><li>大型神经网络： 
  <ul><li>在深层神经网络（如全连接层、卷积神经网络等）中，特别是当网络层数较多时，使用Dropout可以防止网络过度拟合训练数据。</li></ul> </li><li>训练阶段： 
  <ul><li>Dropout通常只在训练阶段使用。在测试阶段，神经元的所有输出都会保留，并且会将训练时的Dropout概率考虑进去，以便于保持输出的一致性。</li></ul> </li></ol> 
<h3><a id="_45"></a>算子组合</h3> 
<ol><li>全连接层（Fully Connected Layer）： 
  <ul><li>Dropout在全连接层中使用非常普遍。在每个训练步骤中，随机将部分神经元的输出设置为零。例如：<pre><code class="prism language-py">model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 50%的神经元输出被随机丢弃</span>
</code></pre> </li></ul> </li><li>卷积层（Convolutional Layer）： 
  <ul><li>在卷积层后也可以使用Dropout，虽然较少见，但在一些较复杂的网络结构中会使用。例如：<pre><code class="prism language-py">model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.25</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 25%的神经元输出被随机丢弃</span>
</code></pre> </li></ul> </li><li>循环神经网络（RNN/LSTM/GRU）： 
  <ul><li>在循环神经网络中，Dropout也被广泛使用，通常称为“时间Dropout”或“变换Dropout”。例如：<pre><code class="prism language-py">model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>LSTM<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> recurrent_dropout<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> </li></ul> </li><li>正则化组合： 
  <ul><li>Dropout可以与其他正则化方法组合使用，如L2正则化、Batch Normalization等。例如：<pre><code class="prism language-py">model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">,</span> kernel_regularizer<span class="token operator">=</span>l2<span class="token punctuation">(</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> </li></ul> </li><li>在分类任务的最后一层之前： 
  <ul><li>Dropout通常在输出层之前使用，确保最后一层的神经元不会被丢弃。例如：<pre><code class="prism language-py">model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span>num_classes<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> </li></ul> </li></ol> 
<h3><a id="_78"></a>注意事项</h3> 
<ul><li>Dropout率选择：通常的Dropout率为0.2到0.5，具体取决于任务和模型复杂度。</li><li>测试阶段：在测试阶段应关闭Dropout，通过使用训练时的Dropout率缩放神经元的输出。</li><li>计算开销：Dropout增加了一些计算开销，尤其是在大型网络中。<br> 总之，Dropout是一种灵活且强大的正则化技术，适用于防止深度学习模型的过拟合。通过在不同的算子组合中使用Dropout，可以有效提升模型的泛化能力和稳健性。</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/256e56c18ff494a7624145c4946d2239/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Leetcode每日一题 20240802 3128.直角三角形</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/951ca0b0e723d5a3b88ce1c12ea150d8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">数据挖掘可以挖掘什么类型的模式？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>