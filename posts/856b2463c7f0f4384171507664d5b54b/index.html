<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python爬虫入门实战（详细步骤） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/856b2463c7f0f4384171507664d5b54b/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Python爬虫入门实战（详细步骤）">
  <meta property="og:description" content="1. 技术选型 爬虫这个功能，我个人理解是什么语言都能写的，只要能正常发送 HTTP 请求，将响应回来的静态页面模版 HTML 上把我们所需要的数据提取出来就可以了，原理很简单，这个东西当然可以手动去统计收集，但是网络平台毕竟还是很多的，还是画点时间，写个爬虫把数据爬取下来，存到数据库里，然后写一个统计报表的 SQL 语句比较方便，后续如果有时间的话，我会写一个简单的前后端分离的报表样例分享出来。
网上现在 Python 爬虫的课程非常的火爆，其实我心里也有点小九九，想玩点骚操作，不想用老本行去写这个爬虫，当然最后的事实是证明确实用 Python 写爬虫要比用 Java 来写爬虫要简单的多。
2. 环境准备 首先笔者的电脑是 Win10 的，Python 选用的是 3.7.4 ，貌似现在网上 Python3 的爬虫教程并不多，其中还是遇到不少的问题，下面也会分享给大家。
开发工具笔者选用的是 VSCode ，在这里推荐一下微软这个开源的产品，非常的轻量化，需要什么插件自己安装就好，不用的插件一律不要，自主性非常高，如果感觉搞不定的朋友可以选择 JetBrains 提供的 Pycharm ，分为社区版和付费版，一般而言，我们使用社区版足矣。
笔者这里直接新建了一个文件夹，创建了一个名为 spider-demo.py 的文件，这个就是我们一会要写的爬虫的文件了，可以给大家看下笔者的开发环境，如下：
这其实是一个调试成功的截图，从下面打印的日志中可以看到，笔者这里抓取了三个平台的数据。
3. 数据库 笔者使用的数据是 Mysql 5.7.19 版本，数据库的字符集是使用的 utf8mb4 ，至于为什么使用 utf8mb4 而不是 utf8 ，各位百度一下吧，很多人讲的都比我讲的好，我简单说一句就是 Mysql 的 utf8 其实是一个假的 utf8 ，而后面增加的字符集 utf8mb4 才是真正的 utf8 。
而 Python 连接 Mysql 也是需要驱动的，和在 Java 中连接数据库需要驱动一样，这里使用的是 pymysql ，安装命令：
pip install pymysql 有没有感觉很简单， pip 是 Python 的一个包管理工具，我的个人理解是类似于一个 Maven 的东西，所有的我们需要的第三方的包都能在这个上面下载到。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-06T16:15:06+08:00">
    <meta property="article:modified_time" content="2024-08-06T16:15:06+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python爬虫入门实战（详细步骤）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p></p> 
<p class="img-center"><img alt="" height="800" src="https://images2.imgbox.com/ac/a6/HFfsbxQd_o.jpg" width="1200"></p> 
<h3 id="2-技术选型">1. 技术选型</h3> 
<p>爬虫这个功能，我个人理解是什么语言都能写的，只要能正常发送 HTTP 请求，将响应回来的静态页面模版 HTML 上把我们所需要的数据提取出来就可以了，原理很简单，这个东西当然可以手动去统计收集，但是网络平台毕竟还是很多的，还是画点时间，写个爬虫把数据爬取下来，存到数据库里，然后写一个统计报表的 SQL 语句比较方便，后续如果有时间的话，我会写一个简单的前后端分离的报表样例分享出来。</p> 
<p>网上现在 Python 爬虫的课程非常的火爆，其实我心里也有点小九九，想玩点骚操作，不想用老本行去写这个爬虫，当然最后的事实是证明确实用 Python 写爬虫要比用 Java 来写爬虫要简单的多。</p> 
<h3 id="3-环境准备">2. 环境准备</h3> 
<p>首先笔者的电脑是 Win10 的，Python 选用的是 3.7.4 ，貌似现在网上 Python3 的爬虫教程并不多，其中还是遇到不少的问题，下面也会分享给大家。</p> 
<p>开发工具笔者选用的是 VSCode ，在这里推荐一下微软这个开源的产品，非常的轻量化，需要什么插件自己安装就好，不用的插件一律不要，自主性非常高，如果感觉搞不定的朋友可以选择 JetBrains 提供的 Pycharm ，分为社区版和付费版，一般而言，我们使用社区版足矣。</p> 
<p>笔者这里直接新建了一个文件夹，创建了一个名为 <code>spider-demo.py</code> 的文件，这个就是我们一会要写的爬虫的文件了，可以给大家看下笔者的开发环境，如下：</p> 
<p></p> 
<p class="img-center"><img alt="" height="1040" src="https://images2.imgbox.com/80/ca/dIk17j4i_o.png" width="1200"></p> 
<p>这其实是一个调试成功的截图，从下面打印的日志中可以看到，笔者这里抓取了三个平台的数据。</p> 
<h3 id="4-数据库">3. 数据库</h3> 
<p>笔者使用的数据是 Mysql 5.7.19 版本，数据库的字符集是使用的 utf8mb4 ，至于为什么使用 utf8mb4 而不是 utf8 ，各位百度一下吧，很多人讲的都比我讲的好，我简单说一句就是 Mysql 的 utf8 其实是一个假的 utf8 ，而后面增加的字符集 utf8mb4 才是真正的 utf8 。</p> 
<p>而 Python 连接 Mysql 也是需要驱动的，和在 Java 中连接数据库需要驱动一样，这里使用的是 pymysql ，安装命令：</p> 
<pre><code>pip install pymysql
</code></pre> 
<p>有没有感觉很简单， pip 是 Python 的一个包管理工具，我的个人理解是类似于一个 Maven 的东西，所有的我们需要的第三方的包都能在这个上面下载到。</p> 
<p>当然，这里可能会出现 <code>timeout</code> 的情况，视大家的网络情况而定，我在晚上执行这个命令的时候真的是各种 <code>timeout</code> ，当然 Maven 会有国内的镜像战， pip 显然肯定也会有么，这里都列给大家：</p> 
<ul><li>阿里云 <a href="https://mirrors.aliyun.com/pypi/simple/" rel="nofollow" title="Simple Index">Simple Index</a></li><li>中国科技大学 <a href="https://pypi.mirrors.ustc.edu.cn/simple/" rel="nofollow" title="Simple Index">Simple Index</a></li><li>豆瓣(douban) <a href="https://pypi.douban.com/simple/" rel="nofollow" title="https://pypi.douban.com/simple/">https://pypi.douban.com/simple/</a></li><li>清华大学 <a href="https://pypi.tuna.tsinghua.edu.cn/simple/" rel="nofollow" title="Simple Index">Simple Index</a></li><li>中国科学技术大学 <a href="http://pypi.mirrors.ustc.edu.cn/simple/" rel="nofollow" title="Simple Index">Simple Index</a></li></ul> 
<p>具体使用方式命令如下：</p> 
<pre><code>pip install -i https://mirrors.aliyun.com/pypi/simple/ 库名
</code></pre> 
<p>笔者这里仅仅尝试过阿里云和清华大学的镜像站，其余未做尝试，以上内容来自于网络。</p> 
<p>表结构，设计如下图，这里设计很粗糙的，简简单单的只做了一张表，多余话我也不说，大家看图吧，字段后面都有注释了：</p> 
<p></p> 
<p class="img-center"><img alt="" height="589" src="https://images2.imgbox.com/cc/b9/KArJRQ2n_o.png" width="1056"></p> 
<p>建表语句提交至 Github 仓库，有需要的同学可以去查看。</p> 
<h3 id="5-实战">4. 实战</h3> 
<p>整体思路分以下这么几步：</p> 
<ol><li>通过 GET 请求将整个页面的 HTML 静态资源请求回来</li><li>通过一些匹配规则匹配到我们需要的数据</li><li>存入数据库</li></ol> 
<h4 id="51-请求-html-静态资源">5.1 请求 HTML 静态资源</h4> 
<p>Python3 为我们提供了 urllib 这个标准库，无需我们额外的安装，使用的时候需要先引入：</p> 
<pre><code>from urllib import request
</code></pre> 
<p>接下来我们使用 urllib 发送 GET 请求，如下：</p> 
<pre><code>req_csdn = request.Request('https://blog.csdn.net/meteor_93')
req_csdn.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36')
html_csdn = request.urlopen(req_csdn).read().decode('utf-8')
</code></pre> 
<blockquote> 
 <p>User Agent中文名为用户代理，简称 UA，它是一个特殊字符串头，使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本、浏览器渲染引擎、浏览器语言、浏览器插件等。</p> 
</blockquote> 
<p>这里在请求头中添加这个是为了模拟浏览器正常请求，很多服务器都会做检测，发现不是正常浏览器的请求会直接拒绝，虽然后面实测笔者爬取的这几个平台都没有这项检测，但是能加就加一下么，当然真实的浏览器发送的请求头里面不仅仅只有一个 UA ，还会有一些其他的信息，如下图：</p> 
<p></p> 
<p class="img-center"><img alt="" height="936" src="https://images2.imgbox.com/19/b8/qkUpPEMy_o.png" width="1038"></p> 
<p>笔者这里的 UA 信息是直接从这里 Copy 出来的。代码写到这里，我们已经拿到了页面静态资源<br><code>html_csdn</code> ，接下来我们就是要解析这个资源，从中匹配出来我们需要的信息。</p> 
<h4 id="52-xpath-数据匹配">5.2 xpath 数据匹配</h4> 
<p><strong>xpath 是什么？</strong></p> 
<blockquote> 
 <p>XPath 是一门在 XML 文档中查找信息的语言。XPath 可用来在 XML 文档中对元素和属性进行遍历。 XPath 是 W3C XSLT 标准的主要元素，并且 XQuery 和 XPointer 都构建于 XPath 表达之上。</p> 
</blockquote> 
<p>从上面这句话我们可以看出来， xpath 是用来查找 XML ，而我们的 HTML 可以认为是语法不标准的 XML 文档，恰巧我们可以通过这种方式来解析 HTML 文档。</p> 
<p>我们在使用 xpath 之前，需要先安装 xpath 的依赖库，这个库并不是 Python 提供的标准库，安装语句如下：</p> 
<pre><code>pip install lxml
</code></pre> 
<p>如果网络不给力的同学可以使用上面的镜像站进行安装。</p> 
<p>而 xpath 的表达式非常简单，具体的语法大家可以参考 W3school 提供的教程（<a href="https://www.w3school.com.cn/xpath/xpath_syntax.asp" rel="nofollow" title="XPath 语法">XPath 语法</a> ），笔者这里不多介绍，具体使用方式如下：</p> 
<pre><code>read_num_csdn = etree.HTML(html_csdn).xpath('//*[@id="asideProfile"]/div[3]/dl[2]/dd/@title')[0]
fans_num_csdn = etree.HTML(html_csdn).xpath('//*[@id="fan"]/text()')[0]
rank_num_csdn = etree.HTML(html_csdn).xpath('//*[@id="asideProfile"]/div[3]/dl[4]/@title')[0]
like_num_csdn = etree.HTML(html_csdn).xpath('//*[@id="asideProfile"]/div[2]/dl[3]/dd/span/text()')[0]
</code></pre> 
<p>这里笔者主要获取了总阅读数、总粉丝数、排名和总点赞数。</p> 
<p>这里列举几个最基础的使用，这几个使用在本示例中已经完全够用：</p> 
<table><thead><tr><th>表达式</th><th>描述</th></tr></thead><tbody><tr><td><code>nodename</code></td><td>选取此节点的所有子节点。</td></tr><tr><td><code>/</code></td><td>从根节点选取。</td></tr><tr><td><code>//</code></td><td>从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。</td></tr><tr><td><code>.</code></td><td>选取当前节点。</td></tr><tr><td><code>..</code></td><td>选取当前节点的父节点。</td></tr><tr><td><code>@</code></td><td>选取属性。</td></tr><tr><td><code>text</code></td><td>选取当前节点内容。</td></tr></tbody></table> 
<p>还有一种简单的方式，我们可以通过 Chrome 浏览器获取 xpath 表达式，具体操作见截图：</p> 
<p></p> 
<p class="img-center"><img alt="" height="934" src="https://images2.imgbox.com/67/60/x8Bbyw6l_o.png" width="625"></p> 
<p>打开 F12 ，鼠标右键需要生成 xpath 表达式的内容，点击 Copy -&gt; Copy XPath 即可。</p> 
<p>这里有一点需要注意，我们直接通过 xpath 取出来的数据数据类型并不是基础数据类型，如果要做运算或者字符串拼接，需要做类型强转，否则会报错，如下：</p> 
<pre><code>req_cnblog = request.Request('https://www.cnblogs.com/babycomeon/default.html?page=2')
req_cnblog.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36')
html_cnblog = request.urlopen(req_cnblog).read().decode('utf-8')

max_page_num = etree.HTML(html_cnblog).xpath('//*[@id="homepage_top_pager"]/div/text()')

# 最大页数
max_page_num = re.findall(r"\d+\.?\d*", str(max_page_num))[0]
</code></pre> 
<p>这里需要获取 cnblog 的博客最大页数，首先取到了 <code>max_page_num</code> ，这里直接做 <code>print</code> 的话是可以正常打印一个字符串出来的，但是如果直接去做正则匹配，就会类型错误。</p> 
<h4 id="53-写入数据库">5.3 写入数据库</h4> 
<p>数据库的操作我就不多做介绍了，有写过 Java 的同学应该都很清楚 jdbc 是怎么写的，先使用 ip 、 port 、 用户名、密码、数据库名称、字符集等信息获取连接，然后开启连接，写一句 sql ，把 sql 拼好，执行 sql ，然后提交数据，然后关闭连接，代码如下：</p> 
<pre><code>def connect():
    conn = pymysql.connect(host='localhost',
                           port=3306,
                           user='root',
                           password='123456',
                           database='test',
                           charset='utf8mb4')

    # 获取操作游标
    cursor = conn.cursor()
    return {"conn": conn, "cursor": cursor}

connection = connect()
conn, cursor = connection['conn'], connection['cursor']

sql_insert = "insert into spider_data(id, plantform, read_num, fans_num, rank_num, like_num, create_date) values (UUID(), %(plantform)s, %(read_num)s, %(fans_num)s, %(rank_num)s, %(like_num)s, now())"

</code></pre> 
<p>在本示例中，爬虫只负责一个数据爬取工作，所以只需要一句 insert 语句就够了，然后在每个平台爬取完成后，将这句 sql 中的占位符替换掉，执行 sql 后 commit 操作即可，示例代码如下：</p> 
<pre><code>csdn_data = {
    "plantform": 'csdn',
    "read_num": read_num_csdn,
    "fans_num": fans_num_csdn,
    "rank_num": rank_num_csdn,
    "like_num": like_num_csdn
}

cursor.execute(sql_insert, csdn_data)
conn.commit()</code></pre> 
<p><strong>最后这里给大家免费分享一份Python学习资料，包含了视频、源码、课件，希望能够帮助到那些不满现状，想提示自己却又没用方向的朋友，也可以和我一起来交流呀！</strong></p> 
<p><strong>编辑资料、学习路线图、源代码、软件安装包等！</strong></p> 
<p style="text-align:center;"><strong><img alt="" src="https://images2.imgbox.com/6e/bc/vw9jfVsp_o.jpg"></strong></p> 
<h6>一、Python所有方向的学习路线</h6> 
<p>Python所有方向路线就是把Python常用的技术点做整理，形成各个领域的知识点汇总，它的用处就在于，你可以按照上面的知识点去找对应的学习资源，保证自己学得较为全面。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="578" src="https://images2.imgbox.com/61/69/LkU21WK3_o.png" width="761"></p> 
<p></p> 
<p></p> 
<p class="img-center"><img alt="图片" height="392" src="https://images2.imgbox.com/86/1f/2wKPQzqZ_o.png" width="543"></p> 
<p></p> 
<p>二、学习软件</p> 
<p>工欲善其事必先利其器。学习Python常用的开发软件都在这里了，还有环境配置的教程，给大家节省了很多时间。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="352" src="https://images2.imgbox.com/5e/ac/4SFIcxcP_o.png" width="550"></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<h6>三、全套PDF电子书</h6> 
<p>书籍的好处就在于权威和体系健全，刚开始学习的时候你可以只看视频或者听某个人讲课，但等你学完之后，你觉得你掌握了，这时候建议还是得去看一下书籍，看权威技术书籍也是每个程序员必经之路。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="409" src="https://images2.imgbox.com/e6/21/mRoW7gyk_o.png" width="674"></p> 
<h6>四、入门学习视频全套</h6> 
<p>我们在看视频学习的时候，不能光动眼动脑不动手，比较科学的学习方法是在理解之后运用它们，这时候练手项目就很适合了。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="584" src="https://images2.imgbox.com/41/65/HVkGSPTv_o.png" width="1027"></p> 
<p></p> 
<p class="img-center"><img alt="图片" height="519" src="https://images2.imgbox.com/e4/ec/S70uRf6P_o.png" width="1080"></p> 
<p></p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/86187cd5372e1ca5c049b1d8f67de5e6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">ASP.Net Core设置接口根路径的方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0354d983ad9298208a09c09c3d664f2a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">vue&#43;elementui 表格分页限制最大页码数</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>