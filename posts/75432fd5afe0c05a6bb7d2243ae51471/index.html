<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>使用llama.cpp实现LLM大模型的格式转换、量化、推理、部署 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/75432fd5afe0c05a6bb7d2243ae51471/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="使用llama.cpp实现LLM大模型的格式转换、量化、推理、部署">
  <meta property="og:description" content="使用llama.cpp实现LLM大模型的格式转换、量化、推理、部署 概述 llama.cpp的主要目标是能够在各种硬件上实现LLM推理，只需最少的设置，并提供最先进的性能。提供1.5位、2位、3位、4位、5位、6位和8位整数量化，以加快推理速度并减少内存使用。
GitHub：https://github.com/ggerganov/llama.cpp
克隆和编译 克隆最新版llama.cpp仓库代码
python 复制代码git clone https://github.com/ggerganov/llama.cpp 对llama.cpp项目进行编译，在目录下会生成一系列可执行文件
css复制代码main：使用模型进行推理 quantize：量化模型 server：提供模型API服务 1.编译构建CPU执行环境，安装简单，适用于没有GPU的操作系统
python复制代码cd llama.cpp mkdir 2.编译构建GPU执行环境，确保安装CUDA工具包，适用于有GPU的操作系统
如果CUDA设置正确，那么执行nvidia-smi、nvcc --version没有错误提示，则表示一切设置正确。
python 复制代码make clean &amp;&amp; make LLAMA_CUDA=1 3.如果编译失败或者需要重新编译，可尝试清理并重新编译，直至编译成功
python 复制代码make clean 😝有需要的小伙伴，可以V扫描下方二维码免费领取🆓
## 环境准备 1.下载受支持的模型
要使用llamma.cpp，首先需要准备它支持的模型。在官方文档中给出了说明，这里仅仅截取其中一部分
2.安装依赖
llama.cpp项目下带有requirements.txt 文件，直接安装依赖即可。
python 复制代码pip install -r requirements.txt 模型格式转换 根据模型架构，可以使用convert.py或convert-hf-to-gguf.py文件。
转换脚本读取模型配置、分词器、张量名称&#43;数据，并将它们转换为GGUF元数据和张量。
GGUF格式 Llama-3相比其前两代显著扩充了词表大小，由32K扩充至128K，并且改为BPE词表。因此需要使用--vocab-type参数指定分词算法，默认值是spm，如果是bpe，需要显示指定
注意：
官方文档说convert.py不支持LLaMA 3，喊使用convert-hf-to-gguf.py，但它不支持--vocab-type，且出现异常：error: unrecognized arguments: --vocab-type bpe，因此使用convert.py且没出问题
使用llama.cpp项目中的convert.py脚本转换模型为GGUF格式
python复制代码root@master:~/work/llama.cpp# python3 ./convert.py /root/work/models/Llama3-Chinese-8B-Instruct/ --outtype f16 --vocab-type bpe --outfile ./models/Llama3-FP16.gguf INFO:convert:Loading model file /root/work/models/Llama3-Chinese-8B-Instruct/model-00001-of-00004.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-15T09:00:00+08:00">
    <meta property="article:modified_time" content="2024-06-15T09:00:00+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">使用llama.cpp实现LLM大模型的格式转换、量化、推理、部署</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="llamacppLLM_0"></a>使用llama.cpp实现LLM大模型的格式转换、量化、推理、部署</h2> 
<h3><a id="_6"></a>概述</h3> 
<blockquote> 
 <p>llama.cpp的主要目标是能够在各种硬件上实现LLM推理，只需最少的设置，并提供最先进的性能。提供1.5位、2位、3位、4位、5位、6位和8位整数量化，以加快推理速度并减少内存使用。</p> 
</blockquote> 
<p>GitHub：<code>https://github.com/ggerganov/llama.cpp</code></p> 
<h3><a id="_12"></a>克隆和编译</h3> 
<p>克隆最新版llama.cpp仓库代码</p> 
<pre><code class="prism language-python">python

复制代码git clone https<span class="token punctuation">:</span><span class="token operator">//</span>github<span class="token punctuation">.</span>com<span class="token operator">/</span>ggerganov<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp
</code></pre> 
<p>对llama.cpp项目进行编译，在目录下会生成一系列可执行文件</p> 
<pre><code class="prism language-css">css复制代码main：使用模型进行推理

quantize：量化模型

server：提供模型API服务
</code></pre> 
<p>1.编译构建CPU执行环境，安装简单，适用于没有GPU的操作系统</p> 
<pre><code class="prism language-python">python复制代码cd llama<span class="token punctuation">.</span>cpp

mkdir 
</code></pre> 
<p>2.编译构建GPU执行环境，确保安装CUDA工具包，适用于有GPU的操作系统</p> 
<blockquote> 
 <p>如果CUDA设置正确，那么执行<code>nvidia-smi</code>、<code>nvcc --version</code>没有错误提示，则表示一切设置正确。</p> 
</blockquote> 
<pre><code class="prism language-python">python

复制代码make clean <span class="token operator">&amp;</span><span class="token operator">&amp;</span>  make LLAMA_CUDA<span class="token operator">=</span><span class="token number">1</span>
</code></pre> 
<p>3.如果编译失败或者需要重新编译，可尝试清理并重新编译，直至编译成功</p> 
<pre><code class="prism language-python">python

复制代码make clean
</code></pre> 
<p>😝有需要的小伙伴，可以V扫描下方二维码免费领取🆓</p> 
<img src="https://images2.imgbox.com/d6/3d/L6qf0qck_o.png"> ## 环境准备 
<p>1.下载受支持的模型</p> 
<blockquote> 
 <p>要使用llamma.cpp，首先需要准备它支持的模型。在官方文档中给出了说明，这里仅仅截取其中一部分</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/02/b8/KoihNacy_o.png" alt="在这里插入图片描述"> 2.安装依赖</p> 
<blockquote> 
 <p>llama.cpp项目下带有requirements.txt 文件，直接安装依赖即可。</p> 
</blockquote> 
<pre><code class="prism language-python">python

复制代码pip install <span class="token operator">-</span>r requirements<span class="token punctuation">.</span>txt
</code></pre> 
<h3><a id="_76"></a>模型格式转换</h3> 
<blockquote> 
 <p>根据模型架构，可以使用<code>convert.py</code>或<code>convert-hf-to-gguf.py</code>文件。</p> 
</blockquote> 
<blockquote> 
 <p>转换脚本读取模型配置、分词器、张量名称+数据，并将它们转换为GGUF元数据和张量。</p> 
</blockquote> 
<h4><a id="GGUF_82"></a>GGUF格式</h4> 
<blockquote> 
 <p>Llama-3相比其前两代显著扩充了词表大小，由32K扩充至128K，并且改为BPE词表。因此需要使用<code>--vocab-type</code>参数指定分词算法，默认值是spm，如果是bpe，需要显示指定</p> 
</blockquote> 
<p>注意：</p> 
<blockquote> 
 <p>官方文档说convert.py不支持LLaMA 3，喊使用convert-hf-to-gguf.py，但它不支持<code>--vocab-type</code>，且出现异常：<code>error: unrecognized arguments: --vocab-type bpe</code>，因此使用convert.py且没出问题</p> 
</blockquote> 
<p>使用llama.cpp项目中的convert.py脚本转换模型为GGUF格式</p> 
<pre><code class="prism language-python">python复制代码root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># python3 ./convert.py  /root/work/models/Llama3-Chinese-8B-Instruct/ --outtype f16 --vocab-type bpe --outfile ./models/Llama3-FP16.gguf</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00001</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00001</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00002</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00003</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00004</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>model parameters count <span class="token punctuation">:</span> <span class="token number">8030261248</span> <span class="token punctuation">(</span>8B<span class="token punctuation">)</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>params <span class="token operator">=</span> Params<span class="token punctuation">(</span>n_vocab<span class="token operator">=</span><span class="token number">128256</span><span class="token punctuation">,</span> n_embd<span class="token operator">=</span><span class="token number">4096</span><span class="token punctuation">,</span> n_layer<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> n_ctx<span class="token operator">=</span><span class="token number">8192</span><span class="token punctuation">,</span> n_ff<span class="token operator">=</span><span class="token number">14336</span><span class="token punctuation">,</span> n_head<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> n_head_kv<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> n_experts<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> n_experts_used<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> f_norm_eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> rope_scaling_type<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> f_rope_freq_base<span class="token operator">=</span><span class="token number">500000.0</span><span class="token punctuation">,</span> f_rope_scale<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> n_orig_ctx<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> rope_finetuned<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> ftype<span class="token operator">=</span><span class="token operator">&lt;</span>GGMLFileType<span class="token punctuation">.</span>MostlyF16<span class="token punctuation">:</span> <span class="token number">1</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> path_model<span class="token operator">=</span>PosixPath<span class="token punctuation">(</span><span class="token string">'/root/work/models/Llama3-Chinese-8B-Instruct'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loaded vocab <span class="token builtin">file</span> PosixPath<span class="token punctuation">(</span><span class="token string">'/root/work/models/Llama3-Chinese-8B-Instruct/tokenizer.json'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token string">'bpe'</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Vocab info<span class="token punctuation">:</span> <span class="token operator">&lt;</span>BpeVocab <span class="token keyword">with</span> <span class="token number">128000</span> base tokens <span class="token keyword">and</span> <span class="token number">256</span> added tokens<span class="token operator">&gt;</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Special vocab info<span class="token punctuation">:</span> <span class="token operator">&lt;</span>SpecialVocab <span class="token keyword">with</span> <span class="token number">280147</span> merges<span class="token punctuation">,</span> special tokens <span class="token punctuation">{<!-- --></span><span class="token string">'bos'</span><span class="token punctuation">:</span> <span class="token number">128000</span><span class="token punctuation">,</span> <span class="token string">'eos'</span><span class="token punctuation">:</span> <span class="token number">128001</span><span class="token punctuation">}</span><span class="token punctuation">,</span> add special tokens unset<span class="token operator">&gt;</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Writing models<span class="token operator">/</span>Llama3<span class="token operator">-</span>FP16<span class="token punctuation">.</span>gguf<span class="token punctuation">,</span> <span class="token builtin">format</span> <span class="token number">1</span>
WARNING<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Ignoring added_tokens<span class="token punctuation">.</span>json since model matches vocab size without it<span class="token punctuation">.</span>
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>gguf_writer<span class="token punctuation">:</span>gguf<span class="token punctuation">:</span> This GGUF <span class="token builtin">file</span> <span class="token keyword">is</span> <span class="token keyword">for</span> Little Endian only
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>Adding <span class="token number">280147</span> merge<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">.</span>
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>Setting special token <span class="token builtin">type</span> bos to <span class="token number">128000</span>
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>Setting special token <span class="token builtin">type</span> eos to <span class="token number">128001</span>
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>Setting chat_template to <span class="token punctuation">{<!-- --></span><span class="token operator">%</span> <span class="token builtin">set</span> loop_messages <span class="token operator">=</span> messages <span class="token operator">%</span><span class="token punctuation">}</span><span class="token punctuation">{<!-- --></span><span class="token operator">%</span> <span class="token keyword">for</span> message <span class="token keyword">in</span> loop_messages <span class="token operator">%</span><span class="token punctuation">}</span><span class="token punctuation">{<!-- --></span><span class="token operator">%</span> <span class="token builtin">set</span> content <span class="token operator">=</span> <span class="token string">'&lt;|start_header_id|&gt;'</span> <span class="token operator">+</span> message<span class="token punctuation">[</span><span class="token string">'role'</span><span class="token punctuation">]</span> <span class="token operator">+</span> '<span class="token operator">&lt;</span><span class="token operator">|</span>end_header_id<span class="token operator">|</span><span class="token operator">&gt;</span>

<span class="token string">'+ message['</span>content<span class="token string">'] | trim + '</span><span class="token operator">&lt;</span><span class="token operator">|</span>eot_id<span class="token operator">|</span><span class="token operator">&gt;</span><span class="token string">' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{<!-- -->{ content }}{% endfor %}{<!-- -->{ '</span><span class="token operator">&lt;</span><span class="token operator">|</span>start_header_id<span class="token operator">|</span><span class="token operator">&gt;</span>assistant<span class="token operator">&lt;</span><span class="token operator">|</span>end_header_id<span class="token operator">|</span><span class="token operator">&gt;</span>

' <span class="token punctuation">}</span><span class="token punctuation">}</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">1</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor token_embd<span class="token punctuation">.</span>weight                      <span class="token operator">|</span> size <span class="token number">128256</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">1</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">2</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_norm<span class="token punctuation">.</span>weight                 <span class="token operator">|</span> size   <span class="token number">4096</span>           <span class="token operator">|</span> <span class="token builtin">type</span> F32  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">2</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">3</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_down<span class="token punctuation">.</span>weight                  <span class="token operator">|</span> size   <span class="token number">4096</span> x  <span class="token number">14336</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">2</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">4</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_gate<span class="token punctuation">.</span>weight                  <span class="token operator">|</span> size  <span class="token number">14336</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">2</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">5</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_up<span class="token punctuation">.</span>weight                    <span class="token operator">|</span> size  <span class="token number">14336</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">2</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">6</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_norm<span class="token punctuation">.</span>weight                  <span class="token operator">|</span> size   <span class="token number">4096</span>           <span class="token operator">|</span> <span class="token builtin">type</span> F32  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">2</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">7</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_k<span class="token punctuation">.</span>weight                    <span class="token operator">|</span> size   <span class="token number">1024</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">2</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">8</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_output<span class="token punctuation">.</span>weight               <span class="token operator">|</span> size   <span class="token number">4096</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">2</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">9</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_q<span class="token punctuation">.</span>weight                    <span class="token operator">|</span> size   <span class="token number">4096</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">3</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span> <span class="token number">10</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_v<span class="token punctuation">.</span>weight                    <span class="token operator">|</span> size   <span class="token number">1024</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">3</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span> <span class="token number">11</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>attn_norm<span class="token punctuation">.</span>weight                 <span class="token operator">|</span> size   <span class="token number">4096</span>           <span class="token operator">|</span> <span class="token builtin">type</span> F32  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">3</span>
</code></pre> 
<p>转换为FP16的GGUF格式，模型体积大概15G。</p> 
<pre><code class="prism language-python">python复制代码root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ll models -h</span>
<span class="token operator">-</span>rw<span class="token operator">-</span>r<span class="token operator">-</span><span class="token operator">-</span>r<span class="token operator">-</span><span class="token operator">-</span>  <span class="token number">1</span> root root  15G May <span class="token number">17</span> <span class="token number">07</span><span class="token punctuation">:</span><span class="token number">47</span> Llama3<span class="token operator">-</span>FP16<span class="token punctuation">.</span>gguf
</code></pre> 
<h4><a id="bin_135"></a>bin格式</h4> 
<pre><code class="prism language-python">python复制代码root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># python3 ./convert.py  /root/work/models/Llama3-Chinese-8B-Instruct/ --outtype f16 --vocab-type bpe --outfile ./models/Llama3-FP16.bin</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00001</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00001</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00002</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00003</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00004</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>model parameters count <span class="token punctuation">:</span> <span class="token number">8030261248</span> <span class="token punctuation">(</span>8B<span class="token punctuation">)</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>params <span class="token operator">=</span> Params<span class="token punctuation">(</span>n_vocab<span class="token operator">=</span><span class="token number">128256</span><span class="token punctuation">,</span> n_embd<span class="token operator">=</span><span class="token number">4096</span><span class="token punctuation">,</span> n_layer<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> n_ctx<span class="token operator">=</span><span class="token number">8192</span><span class="token punctuation">,</span> n_ff<span class="token operator">=</span><span class="token number">14336</span><span class="token punctuation">,</span> n_head<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> n_head_kv<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> n_experts<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> n_experts_used<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> f_norm_eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> rope_scaling_type<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> f_rope_freq_base<span class="token operator">=</span><span class="token number">500000.0</span><span class="token punctuation">,</span> f_rope_scale<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> n_orig_ctx<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> rope_finetuned<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> ftype<span class="token operator">=</span><span class="token operator">&lt;</span>GGMLFileType<span class="token punctuation">.</span>MostlyF16<span class="token punctuation">:</span> <span class="token number">1</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> path_model<span class="token operator">=</span>PosixPath<span class="token punctuation">(</span><span class="token string">'/root/work/models/Llama3-Chinese-8B-Instruct'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loaded vocab <span class="token builtin">file</span> PosixPath<span class="token punctuation">(</span><span class="token string">'/root/work/models/Llama3-Chinese-8B-Instruct/tokenizer.json'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token string">'bpe'</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Vocab info<span class="token punctuation">:</span> <span class="token operator">&lt;</span>BpeVocab <span class="token keyword">with</span> <span class="token number">128000</span> base tokens <span class="token keyword">and</span> <span class="token number">256</span> added tokens<span class="token operator">&gt;</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Special vocab info<span class="token punctuation">:</span> <span class="token operator">&lt;</span>SpecialVocab <span class="token keyword">with</span> <span class="token number">280147</span> merges<span class="token punctuation">,</span> special tokens <span class="token punctuation">{<!-- --></span><span class="token string">'bos'</span><span class="token punctuation">:</span> <span class="token number">128000</span><span class="token punctuation">,</span> <span class="token string">'eos'</span><span class="token punctuation">:</span> <span class="token number">128001</span><span class="token punctuation">}</span><span class="token punctuation">,</span> add special tokens unset<span class="token operator">&gt;</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Writing models<span class="token operator">/</span>Llama3<span class="token operator">-</span>FP16<span class="token punctuation">.</span><span class="token builtin">bin</span><span class="token punctuation">,</span> <span class="token builtin">format</span> <span class="token number">1</span>
WARNING<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Ignoring added_tokens<span class="token punctuation">.</span>json since model matches vocab size without it<span class="token punctuation">.</span>
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>gguf_writer<span class="token punctuation">:</span>gguf<span class="token punctuation">:</span> This GGUF <span class="token builtin">file</span> <span class="token keyword">is</span> <span class="token keyword">for</span> Little Endian only
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>Adding <span class="token number">280147</span> merge<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">.</span>
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>Setting special token <span class="token builtin">type</span> bos to <span class="token number">128000</span>
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>Setting special token <span class="token builtin">type</span> eos to <span class="token number">128001</span>
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>Setting chat_template to <span class="token punctuation">{<!-- --></span><span class="token operator">%</span> <span class="token builtin">set</span> loop_messages <span class="token operator">=</span> messages <span class="token operator">%</span><span class="token punctuation">}</span><span class="token punctuation">{<!-- --></span><span class="token operator">%</span> <span class="token keyword">for</span> message <span class="token keyword">in</span> loop_messages <span class="token operator">%</span><span class="token punctuation">}</span><span class="token punctuation">{<!-- --></span><span class="token operator">%</span> <span class="token builtin">set</span> content <span class="token operator">=</span> <span class="token string">'&lt;|start_header_id|&gt;'</span> <span class="token operator">+</span> message<span class="token punctuation">[</span><span class="token string">'role'</span><span class="token punctuation">]</span> <span class="token operator">+</span> '<span class="token operator">&lt;</span><span class="token operator">|</span>end_header_id<span class="token operator">|</span><span class="token operator">&gt;</span>

<span class="token string">'+ message['</span>content<span class="token string">'] | trim + '</span><span class="token operator">&lt;</span><span class="token operator">|</span>eot_id<span class="token operator">|</span><span class="token operator">&gt;</span><span class="token string">' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{<!-- -->{ content }}{% endfor %}{<!-- -->{ '</span><span class="token operator">&lt;</span><span class="token operator">|</span>start_header_id<span class="token operator">|</span><span class="token operator">&gt;</span>assistant<span class="token operator">&lt;</span><span class="token operator">|</span>end_header_id<span class="token operator">|</span><span class="token operator">&gt;</span>

' <span class="token punctuation">}</span><span class="token punctuation">}</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">1</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor token_embd<span class="token punctuation">.</span>weight                      <span class="token operator">|</span> size <span class="token number">128256</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">4</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">2</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_norm<span class="token punctuation">.</span>weight                 <span class="token operator">|</span> size   <span class="token number">4096</span>           <span class="token operator">|</span> <span class="token builtin">type</span> F32  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">4</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">3</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_down<span class="token punctuation">.</span>weight                  <span class="token operator">|</span> size   <span class="token number">4096</span> x  <span class="token number">14336</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">4</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">4</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_gate<span class="token punctuation">.</span>weight                  <span class="token operator">|</span> size  <span class="token number">14336</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">5</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_up<span class="token punctuation">.</span>weight                    <span class="token operator">|</span> size  <span class="token number">14336</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">6</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_norm<span class="token punctuation">.</span>weight                  <span class="token operator">|</span> size   <span class="token number">4096</span>           <span class="token operator">|</span> <span class="token builtin">type</span> F32  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">7</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_k<span class="token punctuation">.</span>weight                    <span class="token operator">|</span> size   <span class="token number">1024</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">8</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_output<span class="token punctuation">.</span>weight               <span class="token operator">|</span> size   <span class="token number">4096</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">9</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_q<span class="token punctuation">.</span>weight                    <span class="token operator">|</span> size   <span class="token number">4096</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span> <span class="token number">10</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_v<span class="token punctuation">.</span>weight                    <span class="token operator">|</span> size   <span class="token number">1024</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span> <span class="token number">11</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>attn_norm<span class="token punctuation">.</span>weight                 <span class="token operator">|</span> size   <span class="token number">4096</span>           <span class="token operator">|</span> <span class="token builtin">type</span> F32  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span> <span class="token number">12</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>ffn_down<span class="token punctuation">.</span>weight                  <span class="token operator">|</span> size   <span class="token number">4096</span> x  <span class="token number">14336</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span> <span class="token number">13</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>ffn_gate<span class="token punctuation">.</span>weight                  <span class="token operator">|</span> size  <span class="token number">14336</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
python复制代码root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ll models -h</span>
<span class="token operator">-</span>rw<span class="token operator">-</span>r<span class="token operator">-</span><span class="token operator">-</span>r<span class="token operator">-</span><span class="token operator">-</span>  <span class="token number">1</span> root root  15G May <span class="token number">17</span> <span class="token number">07</span><span class="token punctuation">:</span><span class="token number">47</span> Llama3<span class="token operator">-</span>FP16<span class="token punctuation">.</span>gguf
<span class="token operator">-</span>rw<span class="token operator">-</span>r<span class="token operator">-</span><span class="token operator">-</span>r<span class="token operator">-</span><span class="token operator">-</span>  <span class="token number">1</span> root root  15G May <span class="token number">17</span> <span class="token number">08</span><span class="token punctuation">:</span><span class="token number">02</span> Llama3<span class="token operator">-</span>FP16<span class="token punctuation">.</span><span class="token builtin">bin</span>
</code></pre> 
<h3><a id="_178"></a>模型量化</h3> 
<p>模型量化使用quantize命令，其具体可用参数与允许量化的类型如下：</p> 
<pre><code class="prism language-python">python复制代码root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ./quantize</span>
usage<span class="token punctuation">:</span> <span class="token punctuation">.</span><span class="token operator">/</span>quantize <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span><span class="token builtin">help</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>allow<span class="token operator">-</span>requantize<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>leave<span class="token operator">-</span>output<span class="token operator">-</span>tensor<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>pure<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>imatrix<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>include<span class="token operator">-</span>weights<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>exclude<span class="token operator">-</span>weights<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>output<span class="token operator">-</span>tensor<span class="token operator">-</span><span class="token builtin">type</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>token<span class="token operator">-</span>embedding<span class="token operator">-</span><span class="token builtin">type</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>override<span class="token operator">-</span>kv<span class="token punctuation">]</span> model<span class="token operator">-</span>f32<span class="token punctuation">.</span>gguf <span class="token punctuation">[</span>model<span class="token operator">-</span>quant<span class="token punctuation">.</span>gguf<span class="token punctuation">]</span> <span class="token builtin">type</span> <span class="token punctuation">[</span>nthreads<span class="token punctuation">]</span>

  <span class="token operator">-</span><span class="token operator">-</span>allow<span class="token operator">-</span>requantize<span class="token punctuation">:</span> Allows requantizing tensors that have already been quantized<span class="token punctuation">.</span> Warning<span class="token punctuation">:</span> This can severely <span class="token builtin">reduce</span> quality compared to quantizing <span class="token keyword">from</span> 16bit <span class="token keyword">or</span> 32bit
  <span class="token operator">-</span><span class="token operator">-</span>leave<span class="token operator">-</span>output<span class="token operator">-</span>tensor<span class="token punctuation">:</span> Will leave output<span class="token punctuation">.</span>weight un<span class="token punctuation">(</span>re<span class="token punctuation">)</span>quantized<span class="token punctuation">.</span> Increases model size but may also increase quality<span class="token punctuation">,</span> especially when requantizing
  <span class="token operator">-</span><span class="token operator">-</span>pure<span class="token punctuation">:</span> Disable k<span class="token operator">-</span>quant mixtures <span class="token keyword">and</span> quantize <span class="token builtin">all</span> tensors to the same <span class="token builtin">type</span>
  <span class="token operator">-</span><span class="token operator">-</span>imatrix file_name<span class="token punctuation">:</span> use data <span class="token keyword">in</span> file_name <span class="token keyword">as</span> importance matrix <span class="token keyword">for</span> quant optimizations
  <span class="token operator">-</span><span class="token operator">-</span>include<span class="token operator">-</span>weights tensor_name<span class="token punctuation">:</span> use importance matrix <span class="token keyword">for</span> this<span class="token operator">/</span>these tensor<span class="token punctuation">(</span>s<span class="token punctuation">)</span>
  <span class="token operator">-</span><span class="token operator">-</span>exclude<span class="token operator">-</span>weights tensor_name<span class="token punctuation">:</span> use importance matrix <span class="token keyword">for</span> this<span class="token operator">/</span>these tensor<span class="token punctuation">(</span>s<span class="token punctuation">)</span>
  <span class="token operator">-</span><span class="token operator">-</span>output<span class="token operator">-</span>tensor<span class="token operator">-</span><span class="token builtin">type</span> ggml_type<span class="token punctuation">:</span> use this ggml_type <span class="token keyword">for</span> the output<span class="token punctuation">.</span>weight tensor
  <span class="token operator">-</span><span class="token operator">-</span>token<span class="token operator">-</span>embedding<span class="token operator">-</span><span class="token builtin">type</span> ggml_type<span class="token punctuation">:</span> use this ggml_type <span class="token keyword">for</span> the token embeddings tensor
  <span class="token operator">-</span><span class="token operator">-</span>keep<span class="token operator">-</span>split<span class="token punctuation">:</span> will generate quatized model <span class="token keyword">in</span> the same shards <span class="token keyword">as</span> <span class="token builtin">input</span>  <span class="token operator">-</span><span class="token operator">-</span>override<span class="token operator">-</span>kv KEY<span class="token operator">=</span>TYPE<span class="token punctuation">:</span>VALUE
      Advanced option to override model metadata by key <span class="token keyword">in</span> the quantized model<span class="token punctuation">.</span> May be specified multiple times<span class="token punctuation">.</span>
Note<span class="token punctuation">:</span> <span class="token operator">-</span><span class="token operator">-</span>include<span class="token operator">-</span>weights <span class="token keyword">and</span> <span class="token operator">-</span><span class="token operator">-</span>exclude<span class="token operator">-</span>weights cannot be used together

Allowed quantization types<span class="token punctuation">:</span>
   <span class="token number">2</span>  <span class="token keyword">or</span>  Q4_0    <span class="token punctuation">:</span>  <span class="token number">3</span><span class="token punctuation">.</span>56G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.2166</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
   <span class="token number">3</span>  <span class="token keyword">or</span>  Q4_1    <span class="token punctuation">:</span>  <span class="token number">3</span><span class="token punctuation">.</span>90G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.1585</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
   <span class="token number">8</span>  <span class="token keyword">or</span>  Q5_0    <span class="token punctuation">:</span>  <span class="token number">4</span><span class="token punctuation">.</span>33G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.0683</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
   <span class="token number">9</span>  <span class="token keyword">or</span>  Q5_1    <span class="token punctuation">:</span>  <span class="token number">4</span><span class="token punctuation">.</span>70G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.0349</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">19</span>  <span class="token keyword">or</span>  IQ2_XXS <span class="token punctuation">:</span>  <span class="token number">2.06</span> bpw quantization
  <span class="token number">20</span>  <span class="token keyword">or</span>  IQ2_XS  <span class="token punctuation">:</span>  <span class="token number">2.31</span> bpw quantization
  <span class="token number">28</span>  <span class="token keyword">or</span>  IQ2_S   <span class="token punctuation">:</span>  <span class="token number">2.5</span>  bpw quantization
  <span class="token number">29</span>  <span class="token keyword">or</span>  IQ2_M   <span class="token punctuation">:</span>  <span class="token number">2.7</span>  bpw quantization
  <span class="token number">24</span>  <span class="token keyword">or</span>  IQ1_S   <span class="token punctuation">:</span>  <span class="token number">1.56</span> bpw quantization
  <span class="token number">31</span>  <span class="token keyword">or</span>  IQ1_M   <span class="token punctuation">:</span>  <span class="token number">1.75</span> bpw quantization
  <span class="token number">10</span>  <span class="token keyword">or</span>  Q2_K    <span class="token punctuation">:</span>  <span class="token number">2</span><span class="token punctuation">.</span>63G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.6717</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">21</span>  <span class="token keyword">or</span>  Q2_K_S  <span class="token punctuation">:</span>  <span class="token number">2</span><span class="token punctuation">.</span>16G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">9.0634</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">23</span>  <span class="token keyword">or</span>  IQ3_XXS <span class="token punctuation">:</span>  <span class="token number">3.06</span> bpw quantization
  <span class="token number">26</span>  <span class="token keyword">or</span>  IQ3_S   <span class="token punctuation">:</span>  <span class="token number">3.44</span> bpw quantization
  <span class="token number">27</span>  <span class="token keyword">or</span>  IQ3_M   <span class="token punctuation">:</span>  <span class="token number">3.66</span> bpw quantization mix
  <span class="token number">12</span>  <span class="token keyword">or</span>  Q3_K    <span class="token punctuation">:</span> alias <span class="token keyword">for</span> Q3_K_M
  <span class="token number">22</span>  <span class="token keyword">or</span>  IQ3_XS  <span class="token punctuation">:</span>  <span class="token number">3.3</span> bpw quantization
  <span class="token number">11</span>  <span class="token keyword">or</span>  Q3_K_S  <span class="token punctuation">:</span>  <span class="token number">2</span><span class="token punctuation">.</span>75G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.5551</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">12</span>  <span class="token keyword">or</span>  Q3_K_M  <span class="token punctuation">:</span>  <span class="token number">3</span><span class="token punctuation">.</span>07G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.2496</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">13</span>  <span class="token keyword">or</span>  Q3_K_L  <span class="token punctuation">:</span>  <span class="token number">3</span><span class="token punctuation">.</span>35G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.1764</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">25</span>  <span class="token keyword">or</span>  IQ4_NL  <span class="token punctuation">:</span>  <span class="token number">4.50</span> bpw non<span class="token operator">-</span>linear quantization
  <span class="token number">30</span>  <span class="token keyword">or</span>  IQ4_XS  <span class="token punctuation">:</span>  <span class="token number">4.25</span> bpw non<span class="token operator">-</span>linear quantization
  <span class="token number">15</span>  <span class="token keyword">or</span>  Q4_K    <span class="token punctuation">:</span> alias <span class="token keyword">for</span> Q4_K_M
  <span class="token number">14</span>  <span class="token keyword">or</span>  Q4_K_S  <span class="token punctuation">:</span>  <span class="token number">3</span><span class="token punctuation">.</span>59G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.0992</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">15</span>  <span class="token keyword">or</span>  Q4_K_M  <span class="token punctuation">:</span>  <span class="token number">3</span><span class="token punctuation">.</span>80G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.0532</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">17</span>  <span class="token keyword">or</span>  Q5_K    <span class="token punctuation">:</span> alias <span class="token keyword">for</span> Q5_K_M
  <span class="token number">16</span>  <span class="token keyword">or</span>  Q5_K_S  <span class="token punctuation">:</span>  <span class="token number">4</span><span class="token punctuation">.</span>33G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.0400</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">17</span>  <span class="token keyword">or</span>  Q5_K_M  <span class="token punctuation">:</span>  <span class="token number">4</span><span class="token punctuation">.</span>45G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.0122</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">18</span>  <span class="token keyword">or</span>  Q6_K    <span class="token punctuation">:</span>  <span class="token number">5</span><span class="token punctuation">.</span>15G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.0008</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
   <span class="token number">7</span>  <span class="token keyword">or</span>  Q8_0    <span class="token punctuation">:</span>  <span class="token number">6</span><span class="token punctuation">.</span>70G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.0004</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
   <span class="token number">1</span>  <span class="token keyword">or</span>  F16     <span class="token punctuation">:</span> <span class="token number">14</span><span class="token punctuation">.</span>00G<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0020</span> ppl @ Mistral<span class="token operator">-</span>7B
  <span class="token number">32</span>  <span class="token keyword">or</span>  BF16    <span class="token punctuation">:</span> <span class="token number">14</span><span class="token punctuation">.</span>00G<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0050</span> ppl @ Mistral<span class="token operator">-</span>7B
   <span class="token number">0</span>  <span class="token keyword">or</span>  F32     <span class="token punctuation">:</span> <span class="token number">26</span><span class="token punctuation">.</span>00G              @ 7B
          COPY    <span class="token punctuation">:</span> only copy tensors<span class="token punctuation">,</span> no quantizing
</code></pre> 
<blockquote> 
 <p>使用quantize量化模型，它提供各种量化位数的模型：Q2、Q3、Q4、Q5、Q6、Q8、F16。</p> 
</blockquote> 
<blockquote> 
 <p>量化模型的命名方法遵循: Q + 量化比特位 + 变种。量化位数越少，对硬件资源的要求越低，但是模型的精度也越低。</p> 
</blockquote> 
<p>模型经过量化之后，可以发现模型的大小从15G降低到8G，但模型精度从16位浮点数降低到8位整数。</p> 
<pre><code class="prism language-python">python复制代码root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ./quantize ./models/Llama3-FP16.gguf  ./models/Llama3-q8.gguf q8_0</span>
main<span class="token punctuation">:</span> build <span class="token operator">=</span> <span class="token number">2908</span> <span class="token punctuation">(</span>359cbe3f<span class="token punctuation">)</span>
main<span class="token punctuation">:</span> built <span class="token keyword">with</span> cc <span class="token punctuation">(</span>Ubuntu <span class="token number">11.4</span><span class="token number">.0</span><span class="token operator">-</span>1ubuntu1<span class="token operator">~</span><span class="token number">22.04</span><span class="token punctuation">)</span> <span class="token number">11.4</span><span class="token number">.0</span> <span class="token keyword">for</span> x86_64<span class="token operator">-</span>linux<span class="token operator">-</span>gnu
main<span class="token punctuation">:</span> quantizing <span class="token string">'/root/work/models/Llama3-FP16.gguf'</span> to <span class="token string">'/root/work/models/Llama3-q8.gguf'</span> <span class="token keyword">as</span> Q8_0
llama_model_loader<span class="token punctuation">:</span> loaded meta data <span class="token keyword">with</span> <span class="token number">21</span> key<span class="token operator">-</span>value pairs <span class="token keyword">and</span> <span class="token number">291</span> tensors <span class="token keyword">from</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>FP16<span class="token punctuation">.</span>gguf <span class="token punctuation">(</span>version GGUF V3 <span class="token punctuation">(</span>latest<span class="token punctuation">)</span><span class="token punctuation">)</span>
llama_model_loader<span class="token punctuation">:</span> Dumping metadata keys<span class="token operator">/</span>values<span class="token punctuation">.</span> Note<span class="token punctuation">:</span> KV overrides do <span class="token keyword">not</span> <span class="token builtin">apply</span> <span class="token keyword">in</span> this output<span class="token punctuation">.</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">0</span><span class="token punctuation">:</span>                       general<span class="token punctuation">.</span>architecture <span class="token builtin">str</span>              <span class="token operator">=</span> llama
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">1</span><span class="token punctuation">:</span>                               general<span class="token punctuation">.</span>name <span class="token builtin">str</span>              <span class="token operator">=</span> Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">2</span><span class="token punctuation">:</span>                           llama<span class="token punctuation">.</span>vocab_size u32              <span class="token operator">=</span> <span class="token number">128256</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">3</span><span class="token punctuation">:</span>                       llama<span class="token punctuation">.</span>context_length u32              <span class="token operator">=</span> <span class="token number">8192</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">4</span><span class="token punctuation">:</span>                     llama<span class="token punctuation">.</span>embedding_length u32              <span class="token operator">=</span> <span class="token number">4096</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">5</span><span class="token punctuation">:</span>                          llama<span class="token punctuation">.</span>block_count u32              <span class="token operator">=</span> <span class="token number">32</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">6</span><span class="token punctuation">:</span>                  llama<span class="token punctuation">.</span>feed_forward_length u32              <span class="token operator">=</span> <span class="token number">14336</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">7</span><span class="token punctuation">:</span>                 llama<span class="token punctuation">.</span>rope<span class="token punctuation">.</span>dimension_count u32              <span class="token operator">=</span> <span class="token number">128</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">8</span><span class="token punctuation">:</span>                 llama<span class="token punctuation">.</span>attention<span class="token punctuation">.</span>head_count u32              <span class="token operator">=</span> <span class="token number">32</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">9</span><span class="token punctuation">:</span>              llama<span class="token punctuation">.</span>attention<span class="token punctuation">.</span>head_count_kv u32              <span class="token operator">=</span> <span class="token number">8</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">10</span><span class="token punctuation">:</span>     llama<span class="token punctuation">.</span>attention<span class="token punctuation">.</span>layer_norm_rms_epsilon f32              <span class="token operator">=</span> <span class="token number">0.000010</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">11</span><span class="token punctuation">:</span>                       llama<span class="token punctuation">.</span>rope<span class="token punctuation">.</span>freq_base f32              <span class="token operator">=</span> <span class="token number">500000.000000</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">12</span><span class="token punctuation">:</span>                          general<span class="token punctuation">.</span>file_type u32              <span class="token operator">=</span> <span class="token number">1</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">13</span><span class="token punctuation">:</span>                       tokenizer<span class="token punctuation">.</span>ggml<span class="token punctuation">.</span>model <span class="token builtin">str</span>              <span class="token operator">=</span> gpt2
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">14</span><span class="token punctuation">:</span>                      tokenizer<span class="token punctuation">.</span>ggml<span class="token punctuation">.</span>tokens arr<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span><span class="token number">128256</span><span class="token punctuation">]</span>  <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"!"</span><span class="token punctuation">,</span> <span class="token string">"\""</span><span class="token punctuation">,</span> <span class="token string">"#"</span><span class="token punctuation">,</span> <span class="token string">"$"</span><span class="token punctuation">,</span> <span class="token string">"%"</span><span class="token punctuation">,</span> <span class="token string">"&amp;"</span><span class="token punctuation">,</span> <span class="token string">"'"</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">15</span><span class="token punctuation">:</span>                      tokenizer<span class="token punctuation">.</span>ggml<span class="token punctuation">.</span>scores arr<span class="token punctuation">[</span>f32<span class="token punctuation">,</span><span class="token number">128256</span><span class="token punctuation">]</span>  <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.000000</span><span class="token punctuation">,</span> <span class="token number">0.000000</span><span class="token punctuation">,</span> <span class="token number">0.000000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">16</span><span class="token punctuation">:</span>                  tokenizer<span class="token punctuation">.</span>ggml<span class="token punctuation">.</span>token_type arr<span class="token punctuation">[</span>i32<span class="token punctuation">,</span><span class="token number">128256</span><span class="token punctuation">]</span>  <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">17</span><span class="token punctuation">:</span>                      tokenizer<span class="token punctuation">.</span>ggml<span class="token punctuation">.</span>merges arr<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span><span class="token number">280147</span><span class="token punctuation">]</span>  <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"Ġ Ġ"</span><span class="token punctuation">,</span> <span class="token string">"Ġ ĠĠĠ"</span><span class="token punctuation">,</span> <span class="token string">"ĠĠ ĠĠ"</span><span class="token punctuation">,</span> "<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">18</span><span class="token punctuation">:</span>                tokenizer<span class="token punctuation">.</span>ggml<span class="token punctuation">.</span>bos_token_id u32              <span class="token operator">=</span> <span class="token number">128000</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">19</span><span class="token punctuation">:</span>                tokenizer<span class="token punctuation">.</span>ggml<span class="token punctuation">.</span>eos_token_id u32              <span class="token operator">=</span> <span class="token number">128001</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">20</span><span class="token punctuation">:</span>                    tokenizer<span class="token punctuation">.</span>chat_template <span class="token builtin">str</span>              <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token operator">%</span> <span class="token builtin">set</span> loop_messages <span class="token operator">=</span> messages <span class="token operator">%</span><span class="token punctuation">}</span><span class="token punctuation">{<!-- --></span><span class="token operator">%</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> <span class="token builtin">type</span>  f32<span class="token punctuation">:</span>   <span class="token number">65</span> tensors
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> <span class="token builtin">type</span>  f16<span class="token punctuation">:</span>  <span class="token number">226</span> tensors
<span class="token punctuation">[</span>   <span class="token number">1</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                    token_embd<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">128256</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>  <span class="token number">1002.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>   <span class="token number">532.31</span> MiB
<span class="token punctuation">[</span>   <span class="token number">2</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>               blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_norm<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f32<span class="token punctuation">,</span> size <span class="token operator">=</span>    <span class="token number">0.016</span> MB
<span class="token punctuation">[</span>   <span class="token number">3</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_down<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span><span class="token number">14336</span><span class="token punctuation">,</span>  <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>   <span class="token number">112.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">59.50</span> MiB
<span class="token punctuation">[</span>   <span class="token number">4</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_gate<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">14336</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>   <span class="token number">112.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">59.50</span> MiB
<span class="token punctuation">[</span>   <span class="token number">5</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                  blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_up<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">14336</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>   <span class="token number">112.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">59.50</span> MiB
<span class="token punctuation">[</span>   <span class="token number">6</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_norm<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f32<span class="token punctuation">,</span> size <span class="token operator">=</span>    <span class="token number">0.016</span> MB
<span class="token punctuation">[</span>   <span class="token number">7</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                  blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_k<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>  <span class="token number">1024</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>     <span class="token number">8.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>     <span class="token number">4.25</span> MiB
<span class="token punctuation">[</span>   <span class="token number">8</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>             blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_output<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>  <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>    <span class="token number">32.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">17.00</span> MiB
<span class="token punctuation">[</span>   <span class="token number">9</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                  blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_q<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>  <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>    <span class="token number">32.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">17.00</span> MiB
<span class="token punctuation">[</span>  <span class="token number">10</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                  blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_v<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>  <span class="token number">1024</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>     <span class="token number">8.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>     <span class="token number">4.25</span> MiB
<span class="token punctuation">[</span>  <span class="token number">11</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>               blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>attn_norm<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f32<span class="token punctuation">,</span> size <span class="token operator">=</span>    <span class="token number">0.016</span> MB
<span class="token punctuation">[</span>  <span class="token number">12</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>ffn_down<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span><span class="token number">14336</span><span class="token punctuation">,</span>  <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>   <span class="token number">112.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">59.50</span> MiB
<span class="token punctuation">[</span>  <span class="token number">13</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>ffn_gate<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">14336</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>   <span class="token number">112.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">59.50</span> MiB
<span class="token punctuation">[</span>  <span class="token number">14</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                  blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>ffn_up<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">14336</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>   <span class="token number">112.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">59.50</span> MiB
<span class="token punctuation">[</span>  <span class="token number">15</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>ffn_norm<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f32<span class="token punctuation">,</span> size <span class="token operator">=</span>    <span class="token number">0.016</span> MB
<span class="token punctuation">[</span>  <span class="token number">16</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                  blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>attn_k<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>  <span class="token number">1024</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>     <span class="token number">8.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>     <span class="token number">4.25</span> MiB
<span class="token punctuation">[</span>  <span class="token number">17</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>             blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>attn_output<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>  <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>    <span class="token number">32.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">17.00</span> MiB
<span class="token punctuation">[</span>  <span class="token number">18</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                  blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>attn_q<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>  <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>    <span class="token number">32.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">17.00</span> MiB
<span class="token punctuation">[</span>  <span class="token number">19</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                  blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>attn_v<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>  <span class="token number">1024</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>     <span class="token number">8.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>     <span class="token number">4.25</span> MiB
python复制代码root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ll -h models/</span>
<span class="token operator">-</span>rw<span class="token operator">-</span>r<span class="token operator">-</span><span class="token operator">-</span>r<span class="token operator">-</span><span class="token operator">-</span>  <span class="token number">1</span> root root <span class="token number">8</span><span class="token punctuation">.</span>0G May <span class="token number">17</span> <span class="token number">07</span><span class="token punctuation">:</span><span class="token number">54</span> Llama3<span class="token operator">-</span>q8<span class="token punctuation">.</span>gguf
</code></pre> 
<h3><a id="_294"></a>模型加载与推理</h3> 
<p>模型加载与推理使用main命令，其支持如下可用参数：</p> 
<pre><code class="prism language-python">python复制代码root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ./main -h</span>

usage<span class="token punctuation">:</span> <span class="token punctuation">.</span><span class="token operator">/</span>main <span class="token punctuation">[</span>options<span class="token punctuation">]</span>

options<span class="token punctuation">:</span>
  <span class="token operator">-</span>h<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span><span class="token builtin">help</span>            show this <span class="token builtin">help</span> message <span class="token keyword">and</span> exit
  <span class="token operator">-</span><span class="token operator">-</span>version             show version <span class="token keyword">and</span> build info
  <span class="token operator">-</span>i<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>interactive     run <span class="token keyword">in</span> interactive mode
  <span class="token operator">-</span><span class="token operator">-</span>interactive<span class="token operator">-</span>specials allow special tokens <span class="token keyword">in</span> user text<span class="token punctuation">,</span> <span class="token keyword">in</span> interactive mode
  <span class="token operator">-</span><span class="token operator">-</span>interactive<span class="token operator">-</span>first   run <span class="token keyword">in</span> interactive mode <span class="token keyword">and</span> wait <span class="token keyword">for</span> <span class="token builtin">input</span> right away
  <span class="token operator">-</span>cnv<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>conversation  run <span class="token keyword">in</span> conversation mode <span class="token punctuation">(</span>does <span class="token keyword">not</span> <span class="token keyword">print</span> special tokens <span class="token keyword">and</span> suffix<span class="token operator">/</span>prefix<span class="token punctuation">)</span>
  <span class="token operator">-</span>ins<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>instruct      run <span class="token keyword">in</span> instruction mode <span class="token punctuation">(</span>use <span class="token keyword">with</span> Alpaca models<span class="token punctuation">)</span>
  <span class="token operator">-</span>cml<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>chatml        run <span class="token keyword">in</span> chatml mode <span class="token punctuation">(</span>use <span class="token keyword">with</span> ChatML<span class="token operator">-</span>compatible models<span class="token punctuation">)</span>
  <span class="token operator">-</span><span class="token operator">-</span>multiline<span class="token operator">-</span><span class="token builtin">input</span>     allows you to write <span class="token keyword">or</span> paste multiple lines without ending each <span class="token keyword">in</span> '\'
  <span class="token operator">-</span>r PROMPT<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>reverse<span class="token operator">-</span>prompt PROMPT
                        halt generation at PROMPT<span class="token punctuation">,</span> <span class="token keyword">return</span> control <span class="token keyword">in</span> interactive mode
                        <span class="token punctuation">(</span>can be specified more than once <span class="token keyword">for</span> multiple prompts<span class="token punctuation">)</span><span class="token punctuation">.</span>
  <span class="token operator">-</span><span class="token operator">-</span>color               colorise output to distinguish prompt <span class="token keyword">and</span> user <span class="token builtin">input</span> <span class="token keyword">from</span> generations
  <span class="token operator">-</span>s SEED<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>seed SEED  RNG seed <span class="token punctuation">(</span>default<span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> use random seed <span class="token keyword">for</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span>
  <span class="token operator">-</span>t N<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>threads N     number of threads to use during generation <span class="token punctuation">(</span>default<span class="token punctuation">:</span> <span class="token number">30</span><span class="token punctuation">)</span>
  <span class="token operator">-</span>tb N<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>threads<span class="token operator">-</span>batch N
                        number of threads to use during batch <span class="token keyword">and</span> prompt processing <span class="token punctuation">(</span>default<span class="token punctuation">:</span> same <span class="token keyword">as</span> <span class="token operator">-</span><span class="token operator">-</span>threads<span class="token punctuation">)</span>
  <span class="token operator">-</span>td N<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>threads<span class="token operator">-</span>draft N                        number of threads to use during generation <span class="token punctuation">(</span>default<span class="token punctuation">:</span> same <span class="token keyword">as</span> <span class="token operator">-</span><span class="token operator">-</span>threads<span class="token punctuation">)</span>
  <span class="token operator">-</span>tbd N<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>threads<span class="token operator">-</span>batch<span class="token operator">-</span>draft N
                        number of threads to use during batch <span class="token keyword">and</span> prompt processing <span class="token punctuation">(</span>default<span class="token punctuation">:</span> same <span class="token keyword">as</span> <span class="token operator">-</span><span class="token operator">-</span>threads<span class="token operator">-</span>draft<span class="token punctuation">)</span>
  <span class="token operator">-</span>p PROMPT<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>prompt PROMPT
                        prompt to start generation <span class="token keyword">with</span> <span class="token punctuation">(</span>default<span class="token punctuation">:</span> empty<span class="token punctuation">)</span>
</code></pre> 
<blockquote> 
 <p>可以加载预训练模型或者经过量化之后的模型，这里选择加载量化后的模型进行推理。</p> 
</blockquote> 
<p>在llama.cpp项目的根目录，执行如下命令，加载模型进行推理。</p> 
<pre><code class="prism language-python">python复制代码root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ./main -m models/Llama3-q8.gguf --color -f prompts/alpaca.txt -ins -c 2048 --temp 0.2 -n 256 --repeat_penalty 1.1</span>
Log start
main<span class="token punctuation">:</span> build <span class="token operator">=</span> <span class="token number">2908</span> <span class="token punctuation">(</span>359cbe3f<span class="token punctuation">)</span>
main<span class="token punctuation">:</span> built <span class="token keyword">with</span> cc <span class="token punctuation">(</span>Ubuntu <span class="token number">11.4</span><span class="token number">.0</span><span class="token operator">-</span>1ubuntu1<span class="token operator">~</span><span class="token number">22.04</span><span class="token punctuation">)</span> <span class="token number">11.4</span><span class="token number">.0</span> <span class="token keyword">for</span> x86_64<span class="token operator">-</span>linux<span class="token operator">-</span>gnu
main<span class="token punctuation">:</span> seed  <span class="token operator">=</span> <span class="token number">1715935175</span>
llama_model_loader<span class="token punctuation">:</span> loaded meta data <span class="token keyword">with</span> <span class="token number">22</span> key<span class="token operator">-</span>value pairs <span class="token keyword">and</span> <span class="token number">291</span> tensors <span class="token keyword">from</span> models<span class="token operator">/</span>Llama3<span class="token operator">-</span>q8<span class="token punctuation">.</span>gguf <span class="token punctuation">(</span>version GGUF V3 <span class="token punctuation">(</span>latest<span class="token punctuation">)</span><span class="token punctuation">)</span>
llama_model_loader<span class="token punctuation">:</span> Dumping metadata keys<span class="token operator">/</span>values<span class="token punctuation">.</span> Note<span class="token punctuation">:</span> KV overrides do <span class="token keyword">not</span> <span class="token builtin">apply</span> <span class="token keyword">in</span> this output<span class="token punctuation">.</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">0</span><span class="token punctuation">:</span>                       general<span class="token punctuation">.</span>architecture <span class="token builtin">str</span>              <span class="token operator">=</span> llama
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">1</span><span class="token punctuation">:</span>                               general<span class="token punctuation">.</span>name <span class="token builtin">str</span>              <span class="token operator">=</span> Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">2</span><span class="token punctuation">:</span>                           llama<span class="token punctuation">.</span>vocab_size u32              <span class="token operator">=</span> <span class="token number">128256</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">3</span><span class="token punctuation">:</span>                       llama<span class="token punctuation">.</span>context_length u32              <span class="token operator">=</span> <span class="token number">8192</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">4</span><span class="token punctuation">:</span>                     llama<span class="token punctuation">.</span>embedding_length u32              <span class="token operator">=</span> <span class="token number">4096</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">5</span><span class="token punctuation">:</span>                          llama<span class="token punctuation">.</span>block_count u32              <span class="token operator">=</span> <span class="token number">32</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">6</span><span class="token punctuation">:</span>                  llama<span class="token punctuation">.</span>feed_forward_length u32              <span class="token operator">=</span> <span class="token number">14336</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">7</span><span class="token punctuation">:</span>                 llama<span class="token punctuation">.</span>rope<span class="token punctuation">.</span>dimension_count u32              <span class="token operator">=</span> <span class="token number">128</span>

<span class="token operator">==</span> Running <span class="token keyword">in</span> interactive mode<span class="token punctuation">.</span> <span class="token operator">==</span>
 <span class="token operator">-</span> Press Ctrl<span class="token operator">+</span>C to interject at <span class="token builtin">any</span> time<span class="token punctuation">.</span>
 <span class="token operator">-</span> Press Return to <span class="token keyword">return</span> control to LLaMa<span class="token punctuation">.</span>
 <span class="token operator">-</span> To <span class="token keyword">return</span> control without starting a new line<span class="token punctuation">,</span> end your <span class="token builtin">input</span> <span class="token keyword">with</span> <span class="token string">'/'</span><span class="token punctuation">.</span>
 <span class="token operator">-</span> If you want to submit another line<span class="token punctuation">,</span> end your <span class="token builtin">input</span> <span class="token keyword">with</span> '\'<span class="token punctuation">.</span>

<span class="token operator">&lt;</span><span class="token operator">|</span>begin_of_text<span class="token operator">|</span><span class="token operator">&gt;</span>Below <span class="token keyword">is</span> an instruction that describes a task<span class="token punctuation">.</span> Write a response that appropriately completes the request<span class="token punctuation">.</span>
<span class="token operator">&gt;</span> hi
Hello! How can I <span class="token builtin">help</span> you today?<span class="token operator">&lt;</span><span class="token operator">|</span>eot_id<span class="token operator">|</span><span class="token operator">&gt;</span>

<span class="token operator">&gt;</span>
</code></pre> 
<p>在提示符<code>&gt;</code>之后输入prompt，使用<code>ctrl+c</code>中断输出，多行信息以<code>\</code>作为行尾。执行<code>./main -h</code>命令查看帮助和参数说明，以下是一些常用的参数： `</p> 
<table><thead><tr><th>命令</th><th>描述</th></tr></thead><tbody><tr><td>-m</td><td>指定 LLaMA 模型文件的路径</td></tr><tr><td>-mu</td><td>指定远程 http url 来下载文件</td></tr><tr><td>-i</td><td>以交互模式运行程序，允许直接提供输入并接收实时响应。</td></tr><tr><td>-ins</td><td>以指令模式运行程序，这在处理羊驼模型时特别有用。</td></tr><tr><td>-f</td><td>指定prompt模板，alpaca模型请加载prompts/alpaca.txt</td></tr><tr><td>-n</td><td>控制回复生成的最大长度（默认：128）</td></tr><tr><td>-c</td><td>设置提示上下文的大小，值越大越能参考更长的对话历史（默认：512）</td></tr><tr><td>-b</td><td>控制batch size（默认：8），可适当增加</td></tr><tr><td>-t</td><td>控制线程数量（默认：4），可适当增加</td></tr><tr><td><code>--</code>repeat_penalty</td><td>控制生成回复中对重复文本的惩罚力度</td></tr><tr><td><code>--</code>temp</td><td>温度系数，值越低回复的随机性越小，反之越大</td></tr><tr><td><code>--</code>top_p, top_k</td><td>控制解码采样的相关参数</td></tr><tr><td><code>--</code>color</td><td>区分用户输入和生成的文本</td></tr></tbody></table> 
<h3><a id="API_380"></a>模型API服务</h3> 
<p>llama.cpp提供了完全与OpenAI API兼容的API接口，使用经过编译生成的server可执行文件启动API服务。</p> 
<pre><code class="prism language-python">python复制代码root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ./server -m models/Llama3-q8.gguf --host 0.0.0.0 --port 8000</span>
<span class="token punctuation">{<!-- --></span><span class="token string">"tid"</span><span class="token punctuation">:</span><span class="token string">"140018656950080"</span><span class="token punctuation">,</span><span class="token string">"timestamp"</span><span class="token punctuation">:</span><span class="token number">1715936504</span><span class="token punctuation">,</span><span class="token string">"level"</span><span class="token punctuation">:</span><span class="token string">"INFO"</span><span class="token punctuation">,</span><span class="token string">"function"</span><span class="token punctuation">:</span><span class="token string">"main"</span><span class="token punctuation">,</span><span class="token string">"line"</span><span class="token punctuation">:</span><span class="token number">2942</span><span class="token punctuation">,</span><span class="token string">"msg"</span><span class="token punctuation">:</span><span class="token string">"build info"</span><span class="token punctuation">,</span><span class="token string">"build"</span><span class="token punctuation">:</span><span class="token number">2908</span><span class="token punctuation">,</span><span class="token string">"commit"</span><span class="token punctuation">:</span><span class="token string">"359cbe3f"</span><span class="token punctuation">}</span>
<span class="token punctuation">{<!-- --></span><span class="token string">"tid"</span><span class="token punctuation">:</span><span class="token string">"140018656950080"</span><span class="token punctuation">,</span><span class="token string">"timestamp"</span><span class="token punctuation">:</span><span class="token number">1715936504</span><span class="token punctuation">,</span><span class="token string">"level"</span><span class="token punctuation">:</span><span class="token string">"INFO"</span><span class="token punctuation">,</span><span class="token string">"function"</span><span class="token punctuation">:</span><span class="token string">"main"</span><span class="token punctuation">,</span><span class="token string">"line"</span><span class="token punctuation">:</span><span class="token number">2947</span><span class="token punctuation">,</span><span class="token string">"msg"</span><span class="token punctuation">:</span><span class="token string">"system info"</span><span class="token punctuation">,</span><span class="token string">"n_threads"</span><span class="token punctuation">:</span><span class="token number">30</span><span class="token punctuation">,</span><span class="token string">"n_threads_batch"</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token string">"total_threads"</span><span class="token punctuation">:</span><span class="token number">30</span><span class="token punctuation">,</span><span class="token string">"system_info"</span><span class="token punctuation">:</span><span class="token string">"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | "</span><span class="token punctuation">}</span>
llama_model_loader<span class="token punctuation">:</span> loaded meta data <span class="token keyword">with</span> <span class="token number">22</span> key<span class="token operator">-</span>value pairs <span class="token keyword">and</span> <span class="token number">291</span> tensors <span class="token keyword">from</span> models<span class="token operator">/</span>Llama3<span class="token operator">-</span>q8<span class="token punctuation">.</span>gguf <span class="token punctuation">(</span>version GGUF V3 <span class="token punctuation">(</span>latest<span class="token punctuation">)</span><span class="token punctuation">)</span>
llama_model_loader<span class="token punctuation">:</span> Dumping metadata keys<span class="token operator">/</span>values<span class="token punctuation">.</span> Note<span class="token punctuation">:</span> KV overrides do <span class="token keyword">not</span> <span class="token builtin">apply</span> <span class="token keyword">in</span> this output<span class="token punctuation">.</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">0</span><span class="token punctuation">:</span>                       general<span class="token punctuation">.</span>architecture <span class="token builtin">str</span>              <span class="token operator">=</span> llama
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">1</span><span class="token punctuation">:</span>                               general<span class="token punctuation">.</span>name <span class="token builtin">str</span>              <span class="token operator">=</span> Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">2</span><span class="token punctuation">:</span>                           llama<span class="token punctuation">.</span>vocab_size u32              <span class="token operator">=</span> <span class="token number">128256</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">3</span><span class="token punctuation">:</span>                       llama<span class="token punctuation">.</span>context_length u32              <span class="token operator">=</span> <span class="token number">8192</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">4</span><span class="token punctuation">:</span>                     llama<span class="token punctuation">.</span>embedding_length u32              <span class="token operator">=</span> <span class="token number">4096</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">5</span><span class="token punctuation">:</span>                          llama<span class="token punctuation">.</span>block_count u32              <span class="token operator">=</span> <span class="token number">32</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">6</span><span class="token punctuation">:</span>                  llama<span class="token punctuation">.</span>feed_forward_length u32              <span class="token operator">=</span> <span class="token number">14336</span>
</code></pre> 
<p>启动API服务后，可以使用curl命令进行测试</p> 
<pre><code class="prism language-python">python复制代码curl <span class="token operator">-</span><span class="token operator">-</span>request POST \
    <span class="token operator">-</span><span class="token operator">-</span>url http<span class="token punctuation">:</span><span class="token operator">//</span>localhost<span class="token punctuation">:</span><span class="token number">8000</span><span class="token operator">/</span>completion \
    <span class="token operator">-</span><span class="token operator">-</span>header <span class="token string">"Content-Type: application/json"</span> \
    <span class="token operator">-</span><span class="token operator">-</span>data <span class="token string">'{"prompt": "Hi"}'</span>
</code></pre> 
<h3><a id="API_408"></a>模型API服务(第三方)</h3> 
<blockquote> 
 <p>在llamm.cpp项目中有提到各种语言编写的第三方工具包，可以使用这些工具包提供API服务，这里以Python为例，使用<a href="https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Fabetlen%2Fllama-cpp-python" rel="nofollow">llama-cpp-python</a>提供API服务。</p> 
</blockquote> 
<p>安装依赖</p> 
<pre><code class="prism language-python">python复制代码pip install llama<span class="token operator">-</span>cpp<span class="token operator">-</span>python

pip install llama<span class="token operator">-</span>cpp<span class="token operator">-</span>python <span class="token operator">-</span>i https<span class="token punctuation">:</span><span class="token operator">//</span>mirrors<span class="token punctuation">.</span>aliyun<span class="token punctuation">.</span>com<span class="token operator">/</span>pypi<span class="token operator">/</span>simple<span class="token operator">/</span>
</code></pre> 
<p>注意：可能还需要安装以下缺失依赖，可根据启动时的异常提示分别安装。</p> 
<pre><code class="prism language-python">python

复制代码pip install sse_starlette starlette_context pydantic_settings
</code></pre> 
<p>启动API服务，默认运行在<code>http://localhost:8000</code></p> 
<pre><code class="prism language-python">python

复制代码python <span class="token operator">-</span>m llama_cpp<span class="token punctuation">.</span>server <span class="token operator">-</span><span class="token operator">-</span>model models<span class="token operator">/</span>Llama3<span class="token operator">-</span>q8<span class="token punctuation">.</span>gguf
</code></pre> 
<p>安装openai依赖</p> 
<pre><code class="prism language-python">python

复制代码pip install openai
</code></pre> 
<p>使用openai调用API服务</p> 
<pre><code class="prism language-python">python复制代码<span class="token keyword">import</span> os
<span class="token keyword">from</span> openai <span class="token keyword">import</span> OpenAI  <span class="token comment"># 导入OpenAI库</span>

<span class="token comment"># 设置OpenAI的BASE_URL</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">"OPENAI_BASE_URL"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">"http://localhost:8000/v1"</span>

client <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 创建OpenAI客户端对象</span>

<span class="token comment"># 调用模型</span>
completion <span class="token operator">=</span> client<span class="token punctuation">.</span>chat<span class="token punctuation">.</span>completions<span class="token punctuation">.</span>create<span class="token punctuation">(</span>
    model<span class="token operator">=</span><span class="token string">"llama3"</span><span class="token punctuation">,</span> <span class="token comment"># 任意填</span>
    messages<span class="token operator">=</span><span class="token punctuation">[</span>
        <span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"你是一个乐于助人的助手。"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"你好!"</span><span class="token punctuation">}</span>
    <span class="token punctuation">]</span>
<span class="token punctuation">)</span>

<span class="token comment"># 输出模型回复</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>completion<span class="token punctuation">.</span>choices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>message<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/0f/dd/5zSQMlIY_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="GPU_470"></a>GPU推理</h3> 
<blockquote> 
 <p>如果编译构建了GPU执行环境，可以使用<code>-ngl N</code>或<code> --n-gpu-layers N</code>参数，指定offload层数，让模型在GPU上运行推理</p> 
</blockquote> 
<blockquote> 
 <p>例如：<code>-ngl 40</code>表示offload 40层模型参数到GPU</p> 
</blockquote> 
<p>未使用<code>-ngl N</code>或<code> --n-gpu-layers N</code>参数，程序默认在CPU上运行</p> 
<pre><code class="prism language-python">python

复制代码root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ./server -m models/Llama3-FP16.gguf  --host 0.0.0.0 --port 8000</span>
</code></pre> 
<p>可从以下关键启动日志看出，模型并没有在GPU上执行</p> 
<pre><code class="prism language-python">python复制代码ggml_cuda_init<span class="token punctuation">:</span> GGML_CUDA_FORCE_MMQ<span class="token punctuation">:</span>   no
ggml_cuda_init<span class="token punctuation">:</span> CUDA_USE_TENSOR_CORES<span class="token punctuation">:</span> yes
ggml_cuda_init<span class="token punctuation">:</span> found <span class="token number">1</span> CUDA devices<span class="token punctuation">:</span>
  Device <span class="token number">0</span><span class="token punctuation">:</span> Tesla V100S<span class="token operator">-</span>PCIE<span class="token operator">-</span>32GB<span class="token punctuation">,</span> compute capability <span class="token number">7.0</span><span class="token punctuation">,</span> VMM<span class="token punctuation">:</span> yes
llm_load_tensors<span class="token punctuation">:</span> ggml ctx size <span class="token operator">=</span>    <span class="token number">0.15</span> MiB
llm_load_tensors<span class="token punctuation">:</span> offloading <span class="token number">0</span> repeating layers to GPU
llm_load_tensors<span class="token punctuation">:</span> offloaded <span class="token number">0</span><span class="token operator">/</span><span class="token number">33</span> layers to GPU
llm_load_tensors<span class="token punctuation">:</span>        CPU <span class="token builtin">buffer</span> size <span class="token operator">=</span>  <span class="token number">8137.64</span> MiB
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
llama_new_context_with_model<span class="token punctuation">:</span> n_ctx      <span class="token operator">=</span> <span class="token number">2048</span>
llama_new_context_with_model<span class="token punctuation">:</span> n_batch    <span class="token operator">=</span> <span class="token number">2048</span>
llama_new_context_with_model<span class="token punctuation">:</span> n_ubatch   <span class="token operator">=</span> <span class="token number">512</span>
</code></pre> 
<p>使用<code>-ngl N</code>或<code> --n-gpu-layers N</code>参数，程序默认在GPU上运行</p> 
<pre><code class="prism language-python">python

复制代码root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ./server -m models/Llama3-FP16.gguf  --host 0.0.0.0 --port 8000   --n-gpu-layers 1000</span>
</code></pre> 
<p>可从以下关键启动日志看出，模型在GPU上执行</p> 
<pre><code class="prism language-python">python复制代码ggml_cuda_init<span class="token punctuation">:</span> GGML_CUDA_FORCE_MMQ<span class="token punctuation">:</span>   no
ggml_cuda_init<span class="token punctuation">:</span> CUDA_USE_TENSOR_CORES<span class="token punctuation">:</span> yes
ggml_cuda_init<span class="token punctuation">:</span> found <span class="token number">1</span> CUDA devices<span class="token punctuation">:</span>
  Device <span class="token number">0</span><span class="token punctuation">:</span> Tesla V100S<span class="token operator">-</span>PCIE<span class="token operator">-</span>32GB<span class="token punctuation">,</span> compute capability <span class="token number">7.0</span><span class="token punctuation">,</span> VMM<span class="token punctuation">:</span> yes
llm_load_tensors<span class="token punctuation">:</span> ggml ctx size <span class="token operator">=</span>    <span class="token number">0.30</span> MiB
llm_load_tensors<span class="token punctuation">:</span> offloading <span class="token number">32</span> repeating layers to GPU
llm_load_tensors<span class="token punctuation">:</span> offloading non<span class="token operator">-</span>repeating layers to GPU
llm_load_tensors<span class="token punctuation">:</span> offloaded <span class="token number">33</span><span class="token operator">/</span><span class="token number">33</span> layers to GPU
llm_load_tensors<span class="token punctuation">:</span>        CPU <span class="token builtin">buffer</span> size <span class="token operator">=</span>  <span class="token number">1002.00</span> MiB
llm_load_tensors<span class="token punctuation">:</span>      CUDA0 <span class="token builtin">buffer</span> size <span class="token operator">=</span> <span class="token number">14315.02</span> MiB
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
llama_new_context_with_model<span class="token punctuation">:</span> n_ctx      <span class="token operator">=</span> <span class="token number">512</span>
llama_new_context_with_model<span class="token punctuation">:</span> n_batch    <span class="token operator">=</span> <span class="token number">512</span>
llama_new_context_with_model<span class="token punctuation">:</span> n_ubatch   <span class="token operator">=</span> <span class="token number">512</span>
llama_new_context_with_model<span class="token punctuation">:</span> flash_attn <span class="token operator">=</span> <span class="token number">0</span>
</code></pre> 
<p>执行<code>nvidia-smi</code>命令，可以进一步验证模型已在GPU上运行。 <img src="https://images2.imgbox.com/4c/f1/rqLRIm0K_o.png" alt="在这里插入图片描述"></p> 
<p>https://juejin.cn/theme/detail/7218019389664067621?contentType=1)</p> 
<h3><a id="_536"></a><strong>那么，我们该如何学习大模型？</strong></h3> 
<p>作为一名热心肠的互联网老兵，我决定把宝贵的AI知识分享给大家。 至于能学习到多少就看你的学习毅力和能力了 。我已将重要的AI大模型资料包括AI大模型入门学习思维导图、精品AI大模型学习书籍手册、视频教程、实战学习等录播视频免费分享出来。</p> 
<h4><a id="_540"></a>一、大模型全套的学习路线</h4> 
<p>学习大型人工智能模型，如GPT-3、BERT或任何其他先进的神经网络模型，需要系统的方法和持续的努力。既然要系统的学习大模型，那么学习路线是必不可少的，下面的这份路线能帮助你快速梳理知识，形成自己的体系。</p> 
<p><strong>L1级别:AI大模型时代的华丽登场</strong><br> <img src="https://images2.imgbox.com/d8/c3/3wYlaSyP_o.png" alt=""><br> <strong>L2级别：AI大模型API应用开发工程</strong><br> <img src="https://images2.imgbox.com/6a/96/AXprE9Gb_o.png" alt=""><br> <strong>L3级别：大模型应用架构进阶实践</strong><br> <img src="https://images2.imgbox.com/bf/b5/CMLPqnru_o.png" alt=""><br> <strong>L4级别：大模型微调与私有化部署</strong><br> <img src="https://images2.imgbox.com/84/db/qTtmuUgh_o.png" alt=""><br> 一般掌握到第四个级别，市场上大多数岗位都是可以胜任，但要还不是天花板，天花板级别要求更加严格，对于算法和实战是非常苛刻的。建议普通人掌握到L4级别即可。</p> 
<p><strong>以上的AI大模型学习路线，不知道为什么发出来就有点糊</strong>，高清版可以微信扫描下方CSDN官方认证二维码免费领取【<code>保证100%免费</code>】</p> 
<img src="https://images2.imgbox.com/ff/0e/gRJjktl3_o.png"> 
<h4><a id="640AI_560"></a>二、640套AI大模型报告合集</h4> 
<p>这套包含640份报告的合集，涵盖了AI大模型的理论研究、技术实现、行业应用等多个方面。无论您是科研人员、工程师，还是对AI大模型感兴趣的爱好者，这套报告合集都将为您提供宝贵的信息和启示。</p> 
<p><img src="https://images2.imgbox.com/8e/d5/R9THLHqP_o.png" alt="img"></p> 
<h4><a id="PDF_568"></a>三、大模型经典PDF籍</h4> 
<p>随着人工智能技术的飞速发展，AI大模型已经成为了当今科技领域的一大热点。这些大型预训练模型，如GPT-3、BERT、XLNet等，以其强大的语言理解和生成能力，正在改变我们对人工智能的认识。 那以下这些PDF籍就是非常不错的学习资源。</p> 
<p><img src="https://images2.imgbox.com/bc/c5/iWz47lGW_o.png" alt="img"></p> 
<h4><a id="AI_574"></a>四、AI大模型商业化落地方案</h4> 
<p><img src="https://images2.imgbox.com/a4/16/mYQz8PM2_o.png" alt="img"></p> 
<p>作为普通人，入局大模型时代需要持续学习和实践，不断提高自己的技能和认知水平，同时也需要有责任感和伦理意识，为人工智能的健康发展贡献力量。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e0290eb4369cae5175e04252474c63bf/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Leaflet集成wheelnav在WebGIS中的应用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0a4c0bc7e7efcaede00b253951a18197/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">敏捷＝996/007？现实是……</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>