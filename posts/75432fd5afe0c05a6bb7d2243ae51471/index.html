<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ä½¿ç”¨llama.cppå®ç°LLMå¤§æ¨¡å‹çš„æ ¼å¼è½¬æ¢ã€é‡åŒ–ã€æ¨ç†ã€éƒ¨ç½² - ç¼–ç¨‹å¤§å’–</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/75432fd5afe0c05a6bb7d2243ae51471/">
  <meta property="og:site_name" content="ç¼–ç¨‹å¤§å’–">
  <meta property="og:title" content="ä½¿ç”¨llama.cppå®ç°LLMå¤§æ¨¡å‹çš„æ ¼å¼è½¬æ¢ã€é‡åŒ–ã€æ¨ç†ã€éƒ¨ç½²">
  <meta property="og:description" content="ä½¿ç”¨llama.cppå®ç°LLMå¤§æ¨¡å‹çš„æ ¼å¼è½¬æ¢ã€é‡åŒ–ã€æ¨ç†ã€éƒ¨ç½² æ¦‚è¿° llama.cppçš„ä¸»è¦ç›®æ ‡æ˜¯èƒ½å¤Ÿåœ¨å„ç§ç¡¬ä»¶ä¸Šå®ç°LLMæ¨ç†ï¼Œåªéœ€æœ€å°‘çš„è®¾ç½®ï¼Œå¹¶æä¾›æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æä¾›1.5ä½ã€2ä½ã€3ä½ã€4ä½ã€5ä½ã€6ä½å’Œ8ä½æ•´æ•°é‡åŒ–ï¼Œä»¥åŠ å¿«æ¨ç†é€Ÿåº¦å¹¶å‡å°‘å†…å­˜ä½¿ç”¨ã€‚
GitHubï¼šhttps://github.com/ggerganov/llama.cpp
å…‹éš†å’Œç¼–è¯‘ å…‹éš†æœ€æ–°ç‰ˆllama.cppä»“åº“ä»£ç 
python å¤åˆ¶ä»£ç git clone https://github.com/ggerganov/llama.cpp å¯¹llama.cppé¡¹ç›®è¿›è¡Œç¼–è¯‘ï¼Œåœ¨ç›®å½•ä¸‹ä¼šç”Ÿæˆä¸€ç³»åˆ—å¯æ‰§è¡Œæ–‡ä»¶
csså¤åˆ¶ä»£ç mainï¼šä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç† quantizeï¼šé‡åŒ–æ¨¡å‹ serverï¼šæä¾›æ¨¡å‹APIæœåŠ¡ 1.ç¼–è¯‘æ„å»ºCPUæ‰§è¡Œç¯å¢ƒï¼Œå®‰è£…ç®€å•ï¼Œé€‚ç”¨äºæ²¡æœ‰GPUçš„æ“ä½œç³»ç»Ÿ
pythonå¤åˆ¶ä»£ç cd llama.cpp mkdir 2.ç¼–è¯‘æ„å»ºGPUæ‰§è¡Œç¯å¢ƒï¼Œç¡®ä¿å®‰è£…CUDAå·¥å…·åŒ…ï¼Œé€‚ç”¨äºæœ‰GPUçš„æ“ä½œç³»ç»Ÿ
å¦‚æœCUDAè®¾ç½®æ­£ç¡®ï¼Œé‚£ä¹ˆæ‰§è¡Œnvidia-smiã€nvcc --versionæ²¡æœ‰é”™è¯¯æç¤ºï¼Œåˆ™è¡¨ç¤ºä¸€åˆ‡è®¾ç½®æ­£ç¡®ã€‚
python å¤åˆ¶ä»£ç make clean &amp;&amp; make LLAMA_CUDA=1 3.å¦‚æœç¼–è¯‘å¤±è´¥æˆ–è€…éœ€è¦é‡æ–°ç¼–è¯‘ï¼Œå¯å°è¯•æ¸…ç†å¹¶é‡æ–°ç¼–è¯‘ï¼Œç›´è‡³ç¼–è¯‘æˆåŠŸ
python å¤åˆ¶ä»£ç make clean ğŸ˜æœ‰éœ€è¦çš„å°ä¼™ä¼´ï¼Œå¯ä»¥Væ‰«æä¸‹æ–¹äºŒç»´ç å…è´¹é¢†å–ğŸ†“
## ç¯å¢ƒå‡†å¤‡ 1.ä¸‹è½½å—æ”¯æŒçš„æ¨¡å‹
è¦ä½¿ç”¨llamma.cppï¼Œé¦–å…ˆéœ€è¦å‡†å¤‡å®ƒæ”¯æŒçš„æ¨¡å‹ã€‚åœ¨å®˜æ–¹æ–‡æ¡£ä¸­ç»™å‡ºäº†è¯´æ˜ï¼Œè¿™é‡Œä»…ä»…æˆªå–å…¶ä¸­ä¸€éƒ¨åˆ†
2.å®‰è£…ä¾èµ–
llama.cppé¡¹ç›®ä¸‹å¸¦æœ‰requirements.txt æ–‡ä»¶ï¼Œç›´æ¥å®‰è£…ä¾èµ–å³å¯ã€‚
python å¤åˆ¶ä»£ç pip install -r requirements.txt æ¨¡å‹æ ¼å¼è½¬æ¢ æ ¹æ®æ¨¡å‹æ¶æ„ï¼Œå¯ä»¥ä½¿ç”¨convert.pyæˆ–convert-hf-to-gguf.pyæ–‡ä»¶ã€‚
è½¬æ¢è„šæœ¬è¯»å–æ¨¡å‹é…ç½®ã€åˆ†è¯å™¨ã€å¼ é‡åç§°&#43;æ•°æ®ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºGGUFå…ƒæ•°æ®å’Œå¼ é‡ã€‚
GGUFæ ¼å¼ Llama-3ç›¸æ¯”å…¶å‰ä¸¤ä»£æ˜¾è‘—æ‰©å……äº†è¯è¡¨å¤§å°ï¼Œç”±32Kæ‰©å……è‡³128Kï¼Œå¹¶ä¸”æ”¹ä¸ºBPEè¯è¡¨ã€‚å› æ­¤éœ€è¦ä½¿ç”¨--vocab-typeå‚æ•°æŒ‡å®šåˆ†è¯ç®—æ³•ï¼Œé»˜è®¤å€¼æ˜¯spmï¼Œå¦‚æœæ˜¯bpeï¼Œéœ€è¦æ˜¾ç¤ºæŒ‡å®š
æ³¨æ„ï¼š
å®˜æ–¹æ–‡æ¡£è¯´convert.pyä¸æ”¯æŒLLaMA 3ï¼Œå–Šä½¿ç”¨convert-hf-to-gguf.pyï¼Œä½†å®ƒä¸æ”¯æŒ--vocab-typeï¼Œä¸”å‡ºç°å¼‚å¸¸ï¼šerror: unrecognized arguments: --vocab-type bpeï¼Œå› æ­¤ä½¿ç”¨convert.pyä¸”æ²¡å‡ºé—®é¢˜
ä½¿ç”¨llama.cppé¡¹ç›®ä¸­çš„convert.pyè„šæœ¬è½¬æ¢æ¨¡å‹ä¸ºGGUFæ ¼å¼
pythonå¤åˆ¶ä»£ç root@master:~/work/llama.cpp# python3 ./convert.py /root/work/models/Llama3-Chinese-8B-Instruct/ --outtype f16 --vocab-type bpe --outfile ./models/Llama3-FP16.gguf INFO:convert:Loading model file /root/work/models/Llama3-Chinese-8B-Instruct/model-00001-of-00004.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-15T09:00:00+08:00">
    <meta property="article:modified_time" content="2024-06-15T09:00:00+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="ç¼–ç¨‹å¤§å’–" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">ç¼–ç¨‹å¤§å’–</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ä½¿ç”¨llama.cppå®ç°LLMå¤§æ¨¡å‹çš„æ ¼å¼è½¬æ¢ã€é‡åŒ–ã€æ¨ç†ã€éƒ¨ç½²</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="llamacppLLM_0"></a>ä½¿ç”¨llama.cppå®ç°LLMå¤§æ¨¡å‹çš„æ ¼å¼è½¬æ¢ã€é‡åŒ–ã€æ¨ç†ã€éƒ¨ç½²</h2> 
<h3><a id="_6"></a>æ¦‚è¿°</h3> 
<blockquote> 
 <p>llama.cppçš„ä¸»è¦ç›®æ ‡æ˜¯èƒ½å¤Ÿåœ¨å„ç§ç¡¬ä»¶ä¸Šå®ç°LLMæ¨ç†ï¼Œåªéœ€æœ€å°‘çš„è®¾ç½®ï¼Œå¹¶æä¾›æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æä¾›1.5ä½ã€2ä½ã€3ä½ã€4ä½ã€5ä½ã€6ä½å’Œ8ä½æ•´æ•°é‡åŒ–ï¼Œä»¥åŠ å¿«æ¨ç†é€Ÿåº¦å¹¶å‡å°‘å†…å­˜ä½¿ç”¨ã€‚</p> 
</blockquote> 
<p>GitHubï¼š<code>https://github.com/ggerganov/llama.cpp</code></p> 
<h3><a id="_12"></a>å…‹éš†å’Œç¼–è¯‘</h3> 
<p>å…‹éš†æœ€æ–°ç‰ˆllama.cppä»“åº“ä»£ç </p> 
<pre><code class="prism language-python">python

å¤åˆ¶ä»£ç git clone https<span class="token punctuation">:</span><span class="token operator">//</span>github<span class="token punctuation">.</span>com<span class="token operator">/</span>ggerganov<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp
</code></pre> 
<p>å¯¹llama.cppé¡¹ç›®è¿›è¡Œç¼–è¯‘ï¼Œåœ¨ç›®å½•ä¸‹ä¼šç”Ÿæˆä¸€ç³»åˆ—å¯æ‰§è¡Œæ–‡ä»¶</p> 
<pre><code class="prism language-css">csså¤åˆ¶ä»£ç mainï¼šä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†

quantizeï¼šé‡åŒ–æ¨¡å‹

serverï¼šæä¾›æ¨¡å‹APIæœåŠ¡
</code></pre> 
<p>1.ç¼–è¯‘æ„å»ºCPUæ‰§è¡Œç¯å¢ƒï¼Œå®‰è£…ç®€å•ï¼Œé€‚ç”¨äºæ²¡æœ‰GPUçš„æ“ä½œç³»ç»Ÿ</p> 
<pre><code class="prism language-python">pythonå¤åˆ¶ä»£ç cd llama<span class="token punctuation">.</span>cpp

mkdir 
</code></pre> 
<p>2.ç¼–è¯‘æ„å»ºGPUæ‰§è¡Œç¯å¢ƒï¼Œç¡®ä¿å®‰è£…CUDAå·¥å…·åŒ…ï¼Œé€‚ç”¨äºæœ‰GPUçš„æ“ä½œç³»ç»Ÿ</p> 
<blockquote> 
 <p>å¦‚æœCUDAè®¾ç½®æ­£ç¡®ï¼Œé‚£ä¹ˆæ‰§è¡Œ<code>nvidia-smi</code>ã€<code>nvcc --version</code>æ²¡æœ‰é”™è¯¯æç¤ºï¼Œåˆ™è¡¨ç¤ºä¸€åˆ‡è®¾ç½®æ­£ç¡®ã€‚</p> 
</blockquote> 
<pre><code class="prism language-python">python

å¤åˆ¶ä»£ç make clean <span class="token operator">&amp;</span><span class="token operator">&amp;</span>  make LLAMA_CUDA<span class="token operator">=</span><span class="token number">1</span>
</code></pre> 
<p>3.å¦‚æœç¼–è¯‘å¤±è´¥æˆ–è€…éœ€è¦é‡æ–°ç¼–è¯‘ï¼Œå¯å°è¯•æ¸…ç†å¹¶é‡æ–°ç¼–è¯‘ï¼Œç›´è‡³ç¼–è¯‘æˆåŠŸ</p> 
<pre><code class="prism language-python">python

å¤åˆ¶ä»£ç make clean
</code></pre> 
<p>ğŸ˜æœ‰éœ€è¦çš„å°ä¼™ä¼´ï¼Œå¯ä»¥Væ‰«æä¸‹æ–¹äºŒç»´ç å…è´¹é¢†å–ğŸ†“</p> 
<img src="https://images2.imgbox.com/d6/3d/L6qf0qck_o.png"> ## ç¯å¢ƒå‡†å¤‡ 
<p>1.ä¸‹è½½å—æ”¯æŒçš„æ¨¡å‹</p> 
<blockquote> 
 <p>è¦ä½¿ç”¨llamma.cppï¼Œé¦–å…ˆéœ€è¦å‡†å¤‡å®ƒæ”¯æŒçš„æ¨¡å‹ã€‚åœ¨å®˜æ–¹æ–‡æ¡£ä¸­ç»™å‡ºäº†è¯´æ˜ï¼Œè¿™é‡Œä»…ä»…æˆªå–å…¶ä¸­ä¸€éƒ¨åˆ†</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/02/b8/KoihNacy_o.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> 2.å®‰è£…ä¾èµ–</p> 
<blockquote> 
 <p>llama.cppé¡¹ç›®ä¸‹å¸¦æœ‰requirements.txt æ–‡ä»¶ï¼Œç›´æ¥å®‰è£…ä¾èµ–å³å¯ã€‚</p> 
</blockquote> 
<pre><code class="prism language-python">python

å¤åˆ¶ä»£ç pip install <span class="token operator">-</span>r requirements<span class="token punctuation">.</span>txt
</code></pre> 
<h3><a id="_76"></a>æ¨¡å‹æ ¼å¼è½¬æ¢</h3> 
<blockquote> 
 <p>æ ¹æ®æ¨¡å‹æ¶æ„ï¼Œå¯ä»¥ä½¿ç”¨<code>convert.py</code>æˆ–<code>convert-hf-to-gguf.py</code>æ–‡ä»¶ã€‚</p> 
</blockquote> 
<blockquote> 
 <p>è½¬æ¢è„šæœ¬è¯»å–æ¨¡å‹é…ç½®ã€åˆ†è¯å™¨ã€å¼ é‡åç§°+æ•°æ®ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºGGUFå…ƒæ•°æ®å’Œå¼ é‡ã€‚</p> 
</blockquote> 
<h4><a id="GGUF_82"></a>GGUFæ ¼å¼</h4> 
<blockquote> 
 <p>Llama-3ç›¸æ¯”å…¶å‰ä¸¤ä»£æ˜¾è‘—æ‰©å……äº†è¯è¡¨å¤§å°ï¼Œç”±32Kæ‰©å……è‡³128Kï¼Œå¹¶ä¸”æ”¹ä¸ºBPEè¯è¡¨ã€‚å› æ­¤éœ€è¦ä½¿ç”¨<code>--vocab-type</code>å‚æ•°æŒ‡å®šåˆ†è¯ç®—æ³•ï¼Œé»˜è®¤å€¼æ˜¯spmï¼Œå¦‚æœæ˜¯bpeï¼Œéœ€è¦æ˜¾ç¤ºæŒ‡å®š</p> 
</blockquote> 
<p>æ³¨æ„ï¼š</p> 
<blockquote> 
 <p>å®˜æ–¹æ–‡æ¡£è¯´convert.pyä¸æ”¯æŒLLaMA 3ï¼Œå–Šä½¿ç”¨convert-hf-to-gguf.pyï¼Œä½†å®ƒä¸æ”¯æŒ<code>--vocab-type</code>ï¼Œä¸”å‡ºç°å¼‚å¸¸ï¼š<code>error: unrecognized arguments: --vocab-type bpe</code>ï¼Œå› æ­¤ä½¿ç”¨convert.pyä¸”æ²¡å‡ºé—®é¢˜</p> 
</blockquote> 
<p>ä½¿ç”¨llama.cppé¡¹ç›®ä¸­çš„convert.pyè„šæœ¬è½¬æ¢æ¨¡å‹ä¸ºGGUFæ ¼å¼</p> 
<pre><code class="prism language-python">pythonå¤åˆ¶ä»£ç root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># python3 ./convert.py  /root/work/models/Llama3-Chinese-8B-Instruct/ --outtype f16 --vocab-type bpe --outfile ./models/Llama3-FP16.gguf</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00001</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00001</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00002</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00003</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00004</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>model parameters count <span class="token punctuation">:</span> <span class="token number">8030261248</span> <span class="token punctuation">(</span>8B<span class="token punctuation">)</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>params <span class="token operator">=</span> Params<span class="token punctuation">(</span>n_vocab<span class="token operator">=</span><span class="token number">128256</span><span class="token punctuation">,</span> n_embd<span class="token operator">=</span><span class="token number">4096</span><span class="token punctuation">,</span> n_layer<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> n_ctx<span class="token operator">=</span><span class="token number">8192</span><span class="token punctuation">,</span> n_ff<span class="token operator">=</span><span class="token number">14336</span><span class="token punctuation">,</span> n_head<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> n_head_kv<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> n_experts<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> n_experts_used<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> f_norm_eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> rope_scaling_type<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> f_rope_freq_base<span class="token operator">=</span><span class="token number">500000.0</span><span class="token punctuation">,</span> f_rope_scale<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> n_orig_ctx<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> rope_finetuned<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> ftype<span class="token operator">=</span><span class="token operator">&lt;</span>GGMLFileType<span class="token punctuation">.</span>MostlyF16<span class="token punctuation">:</span> <span class="token number">1</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> path_model<span class="token operator">=</span>PosixPath<span class="token punctuation">(</span><span class="token string">'/root/work/models/Llama3-Chinese-8B-Instruct'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loaded vocab <span class="token builtin">file</span> PosixPath<span class="token punctuation">(</span><span class="token string">'/root/work/models/Llama3-Chinese-8B-Instruct/tokenizer.json'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token string">'bpe'</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Vocab info<span class="token punctuation">:</span> <span class="token operator">&lt;</span>BpeVocab <span class="token keyword">with</span> <span class="token number">128000</span> base tokens <span class="token keyword">and</span> <span class="token number">256</span> added tokens<span class="token operator">&gt;</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Special vocab info<span class="token punctuation">:</span> <span class="token operator">&lt;</span>SpecialVocab <span class="token keyword">with</span> <span class="token number">280147</span> merges<span class="token punctuation">,</span> special tokens <span class="token punctuation">{<!-- --></span><span class="token string">'bos'</span><span class="token punctuation">:</span> <span class="token number">128000</span><span class="token punctuation">,</span> <span class="token string">'eos'</span><span class="token punctuation">:</span> <span class="token number">128001</span><span class="token punctuation">}</span><span class="token punctuation">,</span> add special tokens unset<span class="token operator">&gt;</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Writing models<span class="token operator">/</span>Llama3<span class="token operator">-</span>FP16<span class="token punctuation">.</span>gguf<span class="token punctuation">,</span> <span class="token builtin">format</span> <span class="token number">1</span>
WARNING<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Ignoring added_tokens<span class="token punctuation">.</span>json since model matches vocab size without it<span class="token punctuation">.</span>
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>gguf_writer<span class="token punctuation">:</span>gguf<span class="token punctuation">:</span> This GGUF <span class="token builtin">file</span> <span class="token keyword">is</span> <span class="token keyword">for</span> Little Endian only
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>Adding <span class="token number">280147</span> merge<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">.</span>
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>Setting special token <span class="token builtin">type</span> bos to <span class="token number">128000</span>
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>Setting special token <span class="token builtin">type</span> eos to <span class="token number">128001</span>
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>Setting chat_template to <span class="token punctuation">{<!-- --></span><span class="token operator">%</span> <span class="token builtin">set</span> loop_messages <span class="token operator">=</span> messages <span class="token operator">%</span><span class="token punctuation">}</span><span class="token punctuation">{<!-- --></span><span class="token operator">%</span> <span class="token keyword">for</span> message <span class="token keyword">in</span> loop_messages <span class="token operator">%</span><span class="token punctuation">}</span><span class="token punctuation">{<!-- --></span><span class="token operator">%</span> <span class="token builtin">set</span> content <span class="token operator">=</span> <span class="token string">'&lt;|start_header_id|&gt;'</span> <span class="token operator">+</span> message<span class="token punctuation">[</span><span class="token string">'role'</span><span class="token punctuation">]</span> <span class="token operator">+</span> '<span class="token operator">&lt;</span><span class="token operator">|</span>end_header_id<span class="token operator">|</span><span class="token operator">&gt;</span>

<span class="token string">'+ message['</span>content<span class="token string">'] | trim + '</span><span class="token operator">&lt;</span><span class="token operator">|</span>eot_id<span class="token operator">|</span><span class="token operator">&gt;</span><span class="token string">' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{<!-- -->{ content }}{% endfor %}{<!-- -->{ '</span><span class="token operator">&lt;</span><span class="token operator">|</span>start_header_id<span class="token operator">|</span><span class="token operator">&gt;</span>assistant<span class="token operator">&lt;</span><span class="token operator">|</span>end_header_id<span class="token operator">|</span><span class="token operator">&gt;</span>

' <span class="token punctuation">}</span><span class="token punctuation">}</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">1</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor token_embd<span class="token punctuation">.</span>weight                      <span class="token operator">|</span> size <span class="token number">128256</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">1</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">2</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_norm<span class="token punctuation">.</span>weight                 <span class="token operator">|</span> size   <span class="token number">4096</span>           <span class="token operator">|</span> <span class="token builtin">type</span> F32  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">2</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">3</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_down<span class="token punctuation">.</span>weight                  <span class="token operator">|</span> size   <span class="token number">4096</span> x  <span class="token number">14336</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">2</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">4</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_gate<span class="token punctuation">.</span>weight                  <span class="token operator">|</span> size  <span class="token number">14336</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">2</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">5</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_up<span class="token punctuation">.</span>weight                    <span class="token operator">|</span> size  <span class="token number">14336</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">2</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">6</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_norm<span class="token punctuation">.</span>weight                  <span class="token operator">|</span> size   <span class="token number">4096</span>           <span class="token operator">|</span> <span class="token builtin">type</span> F32  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">2</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">7</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_k<span class="token punctuation">.</span>weight                    <span class="token operator">|</span> size   <span class="token number">1024</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">2</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">8</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_output<span class="token punctuation">.</span>weight               <span class="token operator">|</span> size   <span class="token number">4096</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">2</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">9</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_q<span class="token punctuation">.</span>weight                    <span class="token operator">|</span> size   <span class="token number">4096</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">3</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span> <span class="token number">10</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_v<span class="token punctuation">.</span>weight                    <span class="token operator">|</span> size   <span class="token number">1024</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">3</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span> <span class="token number">11</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>attn_norm<span class="token punctuation">.</span>weight                 <span class="token operator">|</span> size   <span class="token number">4096</span>           <span class="token operator">|</span> <span class="token builtin">type</span> F32  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">3</span>
</code></pre> 
<p>è½¬æ¢ä¸ºFP16çš„GGUFæ ¼å¼ï¼Œæ¨¡å‹ä½“ç§¯å¤§æ¦‚15Gã€‚</p> 
<pre><code class="prism language-python">pythonå¤åˆ¶ä»£ç root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ll models -h</span>
<span class="token operator">-</span>rw<span class="token operator">-</span>r<span class="token operator">-</span><span class="token operator">-</span>r<span class="token operator">-</span><span class="token operator">-</span>  <span class="token number">1</span> root root  15G May <span class="token number">17</span> <span class="token number">07</span><span class="token punctuation">:</span><span class="token number">47</span> Llama3<span class="token operator">-</span>FP16<span class="token punctuation">.</span>gguf
</code></pre> 
<h4><a id="bin_135"></a>binæ ¼å¼</h4> 
<pre><code class="prism language-python">pythonå¤åˆ¶ä»£ç root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># python3 ./convert.py  /root/work/models/Llama3-Chinese-8B-Instruct/ --outtype f16 --vocab-type bpe --outfile ./models/Llama3-FP16.bin</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00001</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00001</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00002</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00003</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loading model <span class="token builtin">file</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct<span class="token operator">/</span>model<span class="token operator">-</span><span class="token number">00004</span><span class="token operator">-</span>of<span class="token operator">-</span><span class="token number">00004</span><span class="token punctuation">.</span>safetensors
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>model parameters count <span class="token punctuation">:</span> <span class="token number">8030261248</span> <span class="token punctuation">(</span>8B<span class="token punctuation">)</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>params <span class="token operator">=</span> Params<span class="token punctuation">(</span>n_vocab<span class="token operator">=</span><span class="token number">128256</span><span class="token punctuation">,</span> n_embd<span class="token operator">=</span><span class="token number">4096</span><span class="token punctuation">,</span> n_layer<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> n_ctx<span class="token operator">=</span><span class="token number">8192</span><span class="token punctuation">,</span> n_ff<span class="token operator">=</span><span class="token number">14336</span><span class="token punctuation">,</span> n_head<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> n_head_kv<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> n_experts<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> n_experts_used<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> f_norm_eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> rope_scaling_type<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> f_rope_freq_base<span class="token operator">=</span><span class="token number">500000.0</span><span class="token punctuation">,</span> f_rope_scale<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> n_orig_ctx<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> rope_finetuned<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> ftype<span class="token operator">=</span><span class="token operator">&lt;</span>GGMLFileType<span class="token punctuation">.</span>MostlyF16<span class="token punctuation">:</span> <span class="token number">1</span><span class="token operator">&gt;</span><span class="token punctuation">,</span> path_model<span class="token operator">=</span>PosixPath<span class="token punctuation">(</span><span class="token string">'/root/work/models/Llama3-Chinese-8B-Instruct'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Loaded vocab <span class="token builtin">file</span> PosixPath<span class="token punctuation">(</span><span class="token string">'/root/work/models/Llama3-Chinese-8B-Instruct/tokenizer.json'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token string">'bpe'</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Vocab info<span class="token punctuation">:</span> <span class="token operator">&lt;</span>BpeVocab <span class="token keyword">with</span> <span class="token number">128000</span> base tokens <span class="token keyword">and</span> <span class="token number">256</span> added tokens<span class="token operator">&gt;</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Special vocab info<span class="token punctuation">:</span> <span class="token operator">&lt;</span>SpecialVocab <span class="token keyword">with</span> <span class="token number">280147</span> merges<span class="token punctuation">,</span> special tokens <span class="token punctuation">{<!-- --></span><span class="token string">'bos'</span><span class="token punctuation">:</span> <span class="token number">128000</span><span class="token punctuation">,</span> <span class="token string">'eos'</span><span class="token punctuation">:</span> <span class="token number">128001</span><span class="token punctuation">}</span><span class="token punctuation">,</span> add special tokens unset<span class="token operator">&gt;</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Writing models<span class="token operator">/</span>Llama3<span class="token operator">-</span>FP16<span class="token punctuation">.</span><span class="token builtin">bin</span><span class="token punctuation">,</span> <span class="token builtin">format</span> <span class="token number">1</span>
WARNING<span class="token punctuation">:</span>convert<span class="token punctuation">:</span>Ignoring added_tokens<span class="token punctuation">.</span>json since model matches vocab size without it<span class="token punctuation">.</span>
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>gguf_writer<span class="token punctuation">:</span>gguf<span class="token punctuation">:</span> This GGUF <span class="token builtin">file</span> <span class="token keyword">is</span> <span class="token keyword">for</span> Little Endian only
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>Adding <span class="token number">280147</span> merge<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">.</span>
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>Setting special token <span class="token builtin">type</span> bos to <span class="token number">128000</span>
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>Setting special token <span class="token builtin">type</span> eos to <span class="token number">128001</span>
INFO<span class="token punctuation">:</span>gguf<span class="token punctuation">.</span>vocab<span class="token punctuation">:</span>Setting chat_template to <span class="token punctuation">{<!-- --></span><span class="token operator">%</span> <span class="token builtin">set</span> loop_messages <span class="token operator">=</span> messages <span class="token operator">%</span><span class="token punctuation">}</span><span class="token punctuation">{<!-- --></span><span class="token operator">%</span> <span class="token keyword">for</span> message <span class="token keyword">in</span> loop_messages <span class="token operator">%</span><span class="token punctuation">}</span><span class="token punctuation">{<!-- --></span><span class="token operator">%</span> <span class="token builtin">set</span> content <span class="token operator">=</span> <span class="token string">'&lt;|start_header_id|&gt;'</span> <span class="token operator">+</span> message<span class="token punctuation">[</span><span class="token string">'role'</span><span class="token punctuation">]</span> <span class="token operator">+</span> '<span class="token operator">&lt;</span><span class="token operator">|</span>end_header_id<span class="token operator">|</span><span class="token operator">&gt;</span>

<span class="token string">'+ message['</span>content<span class="token string">'] | trim + '</span><span class="token operator">&lt;</span><span class="token operator">|</span>eot_id<span class="token operator">|</span><span class="token operator">&gt;</span><span class="token string">' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{<!-- -->{ content }}{% endfor %}{<!-- -->{ '</span><span class="token operator">&lt;</span><span class="token operator">|</span>start_header_id<span class="token operator">|</span><span class="token operator">&gt;</span>assistant<span class="token operator">&lt;</span><span class="token operator">|</span>end_header_id<span class="token operator">|</span><span class="token operator">&gt;</span>

' <span class="token punctuation">}</span><span class="token punctuation">}</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">1</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor token_embd<span class="token punctuation">.</span>weight                      <span class="token operator">|</span> size <span class="token number">128256</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">4</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">2</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_norm<span class="token punctuation">.</span>weight                 <span class="token operator">|</span> size   <span class="token number">4096</span>           <span class="token operator">|</span> <span class="token builtin">type</span> F32  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">4</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">3</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_down<span class="token punctuation">.</span>weight                  <span class="token operator">|</span> size   <span class="token number">4096</span> x  <span class="token number">14336</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">4</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">4</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_gate<span class="token punctuation">.</span>weight                  <span class="token operator">|</span> size  <span class="token number">14336</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">5</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_up<span class="token punctuation">.</span>weight                    <span class="token operator">|</span> size  <span class="token number">14336</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">6</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_norm<span class="token punctuation">.</span>weight                  <span class="token operator">|</span> size   <span class="token number">4096</span>           <span class="token operator">|</span> <span class="token builtin">type</span> F32  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">7</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_k<span class="token punctuation">.</span>weight                    <span class="token operator">|</span> size   <span class="token number">1024</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">8</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_output<span class="token punctuation">.</span>weight               <span class="token operator">|</span> size   <span class="token number">4096</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span>  <span class="token number">9</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_q<span class="token punctuation">.</span>weight                    <span class="token operator">|</span> size   <span class="token number">4096</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span> <span class="token number">10</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_v<span class="token punctuation">.</span>weight                    <span class="token operator">|</span> size   <span class="token number">1024</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span> <span class="token number">11</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>attn_norm<span class="token punctuation">.</span>weight                 <span class="token operator">|</span> size   <span class="token number">4096</span>           <span class="token operator">|</span> <span class="token builtin">type</span> F32  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span> <span class="token number">12</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>ffn_down<span class="token punctuation">.</span>weight                  <span class="token operator">|</span> size   <span class="token number">4096</span> x  <span class="token number">14336</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
INFO<span class="token punctuation">:</span>convert<span class="token punctuation">:</span><span class="token punctuation">[</span> <span class="token number">13</span><span class="token operator">/</span><span class="token number">291</span><span class="token punctuation">]</span> Writing tensor blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>ffn_gate<span class="token punctuation">.</span>weight                  <span class="token operator">|</span> size  <span class="token number">14336</span> x   <span class="token number">4096</span>  <span class="token operator">|</span> <span class="token builtin">type</span> F16  <span class="token operator">|</span> T<span class="token operator">+</span>   <span class="token number">5</span>
pythonå¤åˆ¶ä»£ç root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ll models -h</span>
<span class="token operator">-</span>rw<span class="token operator">-</span>r<span class="token operator">-</span><span class="token operator">-</span>r<span class="token operator">-</span><span class="token operator">-</span>  <span class="token number">1</span> root root  15G May <span class="token number">17</span> <span class="token number">07</span><span class="token punctuation">:</span><span class="token number">47</span> Llama3<span class="token operator">-</span>FP16<span class="token punctuation">.</span>gguf
<span class="token operator">-</span>rw<span class="token operator">-</span>r<span class="token operator">-</span><span class="token operator">-</span>r<span class="token operator">-</span><span class="token operator">-</span>  <span class="token number">1</span> root root  15G May <span class="token number">17</span> <span class="token number">08</span><span class="token punctuation">:</span><span class="token number">02</span> Llama3<span class="token operator">-</span>FP16<span class="token punctuation">.</span><span class="token builtin">bin</span>
</code></pre> 
<h3><a id="_178"></a>æ¨¡å‹é‡åŒ–</h3> 
<p>æ¨¡å‹é‡åŒ–ä½¿ç”¨quantizeå‘½ä»¤ï¼Œå…¶å…·ä½“å¯ç”¨å‚æ•°ä¸å…è®¸é‡åŒ–çš„ç±»å‹å¦‚ä¸‹ï¼š</p> 
<pre><code class="prism language-python">pythonå¤åˆ¶ä»£ç root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ./quantize</span>
usage<span class="token punctuation">:</span> <span class="token punctuation">.</span><span class="token operator">/</span>quantize <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span><span class="token builtin">help</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>allow<span class="token operator">-</span>requantize<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>leave<span class="token operator">-</span>output<span class="token operator">-</span>tensor<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>pure<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>imatrix<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>include<span class="token operator">-</span>weights<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>exclude<span class="token operator">-</span>weights<span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>output<span class="token operator">-</span>tensor<span class="token operator">-</span><span class="token builtin">type</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>token<span class="token operator">-</span>embedding<span class="token operator">-</span><span class="token builtin">type</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token operator">-</span>override<span class="token operator">-</span>kv<span class="token punctuation">]</span> model<span class="token operator">-</span>f32<span class="token punctuation">.</span>gguf <span class="token punctuation">[</span>model<span class="token operator">-</span>quant<span class="token punctuation">.</span>gguf<span class="token punctuation">]</span> <span class="token builtin">type</span> <span class="token punctuation">[</span>nthreads<span class="token punctuation">]</span>

  <span class="token operator">-</span><span class="token operator">-</span>allow<span class="token operator">-</span>requantize<span class="token punctuation">:</span> Allows requantizing tensors that have already been quantized<span class="token punctuation">.</span> Warning<span class="token punctuation">:</span> This can severely <span class="token builtin">reduce</span> quality compared to quantizing <span class="token keyword">from</span> 16bit <span class="token keyword">or</span> 32bit
  <span class="token operator">-</span><span class="token operator">-</span>leave<span class="token operator">-</span>output<span class="token operator">-</span>tensor<span class="token punctuation">:</span> Will leave output<span class="token punctuation">.</span>weight un<span class="token punctuation">(</span>re<span class="token punctuation">)</span>quantized<span class="token punctuation">.</span> Increases model size but may also increase quality<span class="token punctuation">,</span> especially when requantizing
  <span class="token operator">-</span><span class="token operator">-</span>pure<span class="token punctuation">:</span> Disable k<span class="token operator">-</span>quant mixtures <span class="token keyword">and</span> quantize <span class="token builtin">all</span> tensors to the same <span class="token builtin">type</span>
  <span class="token operator">-</span><span class="token operator">-</span>imatrix file_name<span class="token punctuation">:</span> use data <span class="token keyword">in</span> file_name <span class="token keyword">as</span> importance matrix <span class="token keyword">for</span> quant optimizations
  <span class="token operator">-</span><span class="token operator">-</span>include<span class="token operator">-</span>weights tensor_name<span class="token punctuation">:</span> use importance matrix <span class="token keyword">for</span> this<span class="token operator">/</span>these tensor<span class="token punctuation">(</span>s<span class="token punctuation">)</span>
  <span class="token operator">-</span><span class="token operator">-</span>exclude<span class="token operator">-</span>weights tensor_name<span class="token punctuation">:</span> use importance matrix <span class="token keyword">for</span> this<span class="token operator">/</span>these tensor<span class="token punctuation">(</span>s<span class="token punctuation">)</span>
  <span class="token operator">-</span><span class="token operator">-</span>output<span class="token operator">-</span>tensor<span class="token operator">-</span><span class="token builtin">type</span> ggml_type<span class="token punctuation">:</span> use this ggml_type <span class="token keyword">for</span> the output<span class="token punctuation">.</span>weight tensor
  <span class="token operator">-</span><span class="token operator">-</span>token<span class="token operator">-</span>embedding<span class="token operator">-</span><span class="token builtin">type</span> ggml_type<span class="token punctuation">:</span> use this ggml_type <span class="token keyword">for</span> the token embeddings tensor
  <span class="token operator">-</span><span class="token operator">-</span>keep<span class="token operator">-</span>split<span class="token punctuation">:</span> will generate quatized model <span class="token keyword">in</span> the same shards <span class="token keyword">as</span> <span class="token builtin">input</span>  <span class="token operator">-</span><span class="token operator">-</span>override<span class="token operator">-</span>kv KEY<span class="token operator">=</span>TYPE<span class="token punctuation">:</span>VALUE
      Advanced option to override model metadata by key <span class="token keyword">in</span> the quantized model<span class="token punctuation">.</span> May be specified multiple times<span class="token punctuation">.</span>
Note<span class="token punctuation">:</span> <span class="token operator">-</span><span class="token operator">-</span>include<span class="token operator">-</span>weights <span class="token keyword">and</span> <span class="token operator">-</span><span class="token operator">-</span>exclude<span class="token operator">-</span>weights cannot be used together

Allowed quantization types<span class="token punctuation">:</span>
   <span class="token number">2</span>  <span class="token keyword">or</span>  Q4_0    <span class="token punctuation">:</span>  <span class="token number">3</span><span class="token punctuation">.</span>56G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.2166</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
   <span class="token number">3</span>  <span class="token keyword">or</span>  Q4_1    <span class="token punctuation">:</span>  <span class="token number">3</span><span class="token punctuation">.</span>90G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.1585</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
   <span class="token number">8</span>  <span class="token keyword">or</span>  Q5_0    <span class="token punctuation">:</span>  <span class="token number">4</span><span class="token punctuation">.</span>33G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.0683</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
   <span class="token number">9</span>  <span class="token keyword">or</span>  Q5_1    <span class="token punctuation">:</span>  <span class="token number">4</span><span class="token punctuation">.</span>70G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.0349</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">19</span>  <span class="token keyword">or</span>  IQ2_XXS <span class="token punctuation">:</span>  <span class="token number">2.06</span> bpw quantization
  <span class="token number">20</span>  <span class="token keyword">or</span>  IQ2_XS  <span class="token punctuation">:</span>  <span class="token number">2.31</span> bpw quantization
  <span class="token number">28</span>  <span class="token keyword">or</span>  IQ2_S   <span class="token punctuation">:</span>  <span class="token number">2.5</span>  bpw quantization
  <span class="token number">29</span>  <span class="token keyword">or</span>  IQ2_M   <span class="token punctuation">:</span>  <span class="token number">2.7</span>  bpw quantization
  <span class="token number">24</span>  <span class="token keyword">or</span>  IQ1_S   <span class="token punctuation">:</span>  <span class="token number">1.56</span> bpw quantization
  <span class="token number">31</span>  <span class="token keyword">or</span>  IQ1_M   <span class="token punctuation">:</span>  <span class="token number">1.75</span> bpw quantization
  <span class="token number">10</span>  <span class="token keyword">or</span>  Q2_K    <span class="token punctuation">:</span>  <span class="token number">2</span><span class="token punctuation">.</span>63G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.6717</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">21</span>  <span class="token keyword">or</span>  Q2_K_S  <span class="token punctuation">:</span>  <span class="token number">2</span><span class="token punctuation">.</span>16G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">9.0634</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">23</span>  <span class="token keyword">or</span>  IQ3_XXS <span class="token punctuation">:</span>  <span class="token number">3.06</span> bpw quantization
  <span class="token number">26</span>  <span class="token keyword">or</span>  IQ3_S   <span class="token punctuation">:</span>  <span class="token number">3.44</span> bpw quantization
  <span class="token number">27</span>  <span class="token keyword">or</span>  IQ3_M   <span class="token punctuation">:</span>  <span class="token number">3.66</span> bpw quantization mix
  <span class="token number">12</span>  <span class="token keyword">or</span>  Q3_K    <span class="token punctuation">:</span> alias <span class="token keyword">for</span> Q3_K_M
  <span class="token number">22</span>  <span class="token keyword">or</span>  IQ3_XS  <span class="token punctuation">:</span>  <span class="token number">3.3</span> bpw quantization
  <span class="token number">11</span>  <span class="token keyword">or</span>  Q3_K_S  <span class="token punctuation">:</span>  <span class="token number">2</span><span class="token punctuation">.</span>75G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.5551</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">12</span>  <span class="token keyword">or</span>  Q3_K_M  <span class="token punctuation">:</span>  <span class="token number">3</span><span class="token punctuation">.</span>07G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.2496</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">13</span>  <span class="token keyword">or</span>  Q3_K_L  <span class="token punctuation">:</span>  <span class="token number">3</span><span class="token punctuation">.</span>35G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.1764</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">25</span>  <span class="token keyword">or</span>  IQ4_NL  <span class="token punctuation">:</span>  <span class="token number">4.50</span> bpw non<span class="token operator">-</span>linear quantization
  <span class="token number">30</span>  <span class="token keyword">or</span>  IQ4_XS  <span class="token punctuation">:</span>  <span class="token number">4.25</span> bpw non<span class="token operator">-</span>linear quantization
  <span class="token number">15</span>  <span class="token keyword">or</span>  Q4_K    <span class="token punctuation">:</span> alias <span class="token keyword">for</span> Q4_K_M
  <span class="token number">14</span>  <span class="token keyword">or</span>  Q4_K_S  <span class="token punctuation">:</span>  <span class="token number">3</span><span class="token punctuation">.</span>59G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.0992</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">15</span>  <span class="token keyword">or</span>  Q4_K_M  <span class="token punctuation">:</span>  <span class="token number">3</span><span class="token punctuation">.</span>80G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.0532</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">17</span>  <span class="token keyword">or</span>  Q5_K    <span class="token punctuation">:</span> alias <span class="token keyword">for</span> Q5_K_M
  <span class="token number">16</span>  <span class="token keyword">or</span>  Q5_K_S  <span class="token punctuation">:</span>  <span class="token number">4</span><span class="token punctuation">.</span>33G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.0400</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">17</span>  <span class="token keyword">or</span>  Q5_K_M  <span class="token punctuation">:</span>  <span class="token number">4</span><span class="token punctuation">.</span>45G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.0122</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
  <span class="token number">18</span>  <span class="token keyword">or</span>  Q6_K    <span class="token punctuation">:</span>  <span class="token number">5</span><span class="token punctuation">.</span>15G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.0008</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
   <span class="token number">7</span>  <span class="token keyword">or</span>  Q8_0    <span class="token punctuation">:</span>  <span class="token number">6</span><span class="token punctuation">.</span>70G<span class="token punctuation">,</span> <span class="token operator">+</span><span class="token number">0.0004</span> ppl @ LLaMA<span class="token operator">-</span>v1<span class="token operator">-</span>7B
   <span class="token number">1</span>  <span class="token keyword">or</span>  F16     <span class="token punctuation">:</span> <span class="token number">14</span><span class="token punctuation">.</span>00G<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0020</span> ppl @ Mistral<span class="token operator">-</span>7B
  <span class="token number">32</span>  <span class="token keyword">or</span>  BF16    <span class="token punctuation">:</span> <span class="token number">14</span><span class="token punctuation">.</span>00G<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0050</span> ppl @ Mistral<span class="token operator">-</span>7B
   <span class="token number">0</span>  <span class="token keyword">or</span>  F32     <span class="token punctuation">:</span> <span class="token number">26</span><span class="token punctuation">.</span>00G              @ 7B
          COPY    <span class="token punctuation">:</span> only copy tensors<span class="token punctuation">,</span> no quantizing
</code></pre> 
<blockquote> 
 <p>ä½¿ç”¨quantizeé‡åŒ–æ¨¡å‹ï¼Œå®ƒæä¾›å„ç§é‡åŒ–ä½æ•°çš„æ¨¡å‹ï¼šQ2ã€Q3ã€Q4ã€Q5ã€Q6ã€Q8ã€F16ã€‚</p> 
</blockquote> 
<blockquote> 
 <p>é‡åŒ–æ¨¡å‹çš„å‘½åæ–¹æ³•éµå¾ª: Q + é‡åŒ–æ¯”ç‰¹ä½ + å˜ç§ã€‚é‡åŒ–ä½æ•°è¶Šå°‘ï¼Œå¯¹ç¡¬ä»¶èµ„æºçš„è¦æ±‚è¶Šä½ï¼Œä½†æ˜¯æ¨¡å‹çš„ç²¾åº¦ä¹Ÿè¶Šä½ã€‚</p> 
</blockquote> 
<p>æ¨¡å‹ç»è¿‡é‡åŒ–ä¹‹åï¼Œå¯ä»¥å‘ç°æ¨¡å‹çš„å¤§å°ä»15Gé™ä½åˆ°8Gï¼Œä½†æ¨¡å‹ç²¾åº¦ä»16ä½æµ®ç‚¹æ•°é™ä½åˆ°8ä½æ•´æ•°ã€‚</p> 
<pre><code class="prism language-python">pythonå¤åˆ¶ä»£ç root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ./quantize ./models/Llama3-FP16.gguf  ./models/Llama3-q8.gguf q8_0</span>
main<span class="token punctuation">:</span> build <span class="token operator">=</span> <span class="token number">2908</span> <span class="token punctuation">(</span>359cbe3f<span class="token punctuation">)</span>
main<span class="token punctuation">:</span> built <span class="token keyword">with</span> cc <span class="token punctuation">(</span>Ubuntu <span class="token number">11.4</span><span class="token number">.0</span><span class="token operator">-</span>1ubuntu1<span class="token operator">~</span><span class="token number">22.04</span><span class="token punctuation">)</span> <span class="token number">11.4</span><span class="token number">.0</span> <span class="token keyword">for</span> x86_64<span class="token operator">-</span>linux<span class="token operator">-</span>gnu
main<span class="token punctuation">:</span> quantizing <span class="token string">'/root/work/models/Llama3-FP16.gguf'</span> to <span class="token string">'/root/work/models/Llama3-q8.gguf'</span> <span class="token keyword">as</span> Q8_0
llama_model_loader<span class="token punctuation">:</span> loaded meta data <span class="token keyword">with</span> <span class="token number">21</span> key<span class="token operator">-</span>value pairs <span class="token keyword">and</span> <span class="token number">291</span> tensors <span class="token keyword">from</span> <span class="token operator">/</span>root<span class="token operator">/</span>work<span class="token operator">/</span>models<span class="token operator">/</span>Llama3<span class="token operator">-</span>FP16<span class="token punctuation">.</span>gguf <span class="token punctuation">(</span>version GGUF V3 <span class="token punctuation">(</span>latest<span class="token punctuation">)</span><span class="token punctuation">)</span>
llama_model_loader<span class="token punctuation">:</span> Dumping metadata keys<span class="token operator">/</span>values<span class="token punctuation">.</span> Note<span class="token punctuation">:</span> KV overrides do <span class="token keyword">not</span> <span class="token builtin">apply</span> <span class="token keyword">in</span> this output<span class="token punctuation">.</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">0</span><span class="token punctuation">:</span>                       general<span class="token punctuation">.</span>architecture <span class="token builtin">str</span>              <span class="token operator">=</span> llama
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">1</span><span class="token punctuation">:</span>                               general<span class="token punctuation">.</span>name <span class="token builtin">str</span>              <span class="token operator">=</span> Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">2</span><span class="token punctuation">:</span>                           llama<span class="token punctuation">.</span>vocab_size u32              <span class="token operator">=</span> <span class="token number">128256</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">3</span><span class="token punctuation">:</span>                       llama<span class="token punctuation">.</span>context_length u32              <span class="token operator">=</span> <span class="token number">8192</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">4</span><span class="token punctuation">:</span>                     llama<span class="token punctuation">.</span>embedding_length u32              <span class="token operator">=</span> <span class="token number">4096</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">5</span><span class="token punctuation">:</span>                          llama<span class="token punctuation">.</span>block_count u32              <span class="token operator">=</span> <span class="token number">32</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">6</span><span class="token punctuation">:</span>                  llama<span class="token punctuation">.</span>feed_forward_length u32              <span class="token operator">=</span> <span class="token number">14336</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">7</span><span class="token punctuation">:</span>                 llama<span class="token punctuation">.</span>rope<span class="token punctuation">.</span>dimension_count u32              <span class="token operator">=</span> <span class="token number">128</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">8</span><span class="token punctuation">:</span>                 llama<span class="token punctuation">.</span>attention<span class="token punctuation">.</span>head_count u32              <span class="token operator">=</span> <span class="token number">32</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">9</span><span class="token punctuation">:</span>              llama<span class="token punctuation">.</span>attention<span class="token punctuation">.</span>head_count_kv u32              <span class="token operator">=</span> <span class="token number">8</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">10</span><span class="token punctuation">:</span>     llama<span class="token punctuation">.</span>attention<span class="token punctuation">.</span>layer_norm_rms_epsilon f32              <span class="token operator">=</span> <span class="token number">0.000010</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">11</span><span class="token punctuation">:</span>                       llama<span class="token punctuation">.</span>rope<span class="token punctuation">.</span>freq_base f32              <span class="token operator">=</span> <span class="token number">500000.000000</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">12</span><span class="token punctuation">:</span>                          general<span class="token punctuation">.</span>file_type u32              <span class="token operator">=</span> <span class="token number">1</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">13</span><span class="token punctuation">:</span>                       tokenizer<span class="token punctuation">.</span>ggml<span class="token punctuation">.</span>model <span class="token builtin">str</span>              <span class="token operator">=</span> gpt2
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">14</span><span class="token punctuation">:</span>                      tokenizer<span class="token punctuation">.</span>ggml<span class="token punctuation">.</span>tokens arr<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span><span class="token number">128256</span><span class="token punctuation">]</span>  <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"!"</span><span class="token punctuation">,</span> <span class="token string">"\""</span><span class="token punctuation">,</span> <span class="token string">"#"</span><span class="token punctuation">,</span> <span class="token string">"$"</span><span class="token punctuation">,</span> <span class="token string">"%"</span><span class="token punctuation">,</span> <span class="token string">"&amp;"</span><span class="token punctuation">,</span> <span class="token string">"'"</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">15</span><span class="token punctuation">:</span>                      tokenizer<span class="token punctuation">.</span>ggml<span class="token punctuation">.</span>scores arr<span class="token punctuation">[</span>f32<span class="token punctuation">,</span><span class="token number">128256</span><span class="token punctuation">]</span>  <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.000000</span><span class="token punctuation">,</span> <span class="token number">0.000000</span><span class="token punctuation">,</span> <span class="token number">0.000000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">16</span><span class="token punctuation">:</span>                  tokenizer<span class="token punctuation">.</span>ggml<span class="token punctuation">.</span>token_type arr<span class="token punctuation">[</span>i32<span class="token punctuation">,</span><span class="token number">128256</span><span class="token punctuation">]</span>  <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">17</span><span class="token punctuation">:</span>                      tokenizer<span class="token punctuation">.</span>ggml<span class="token punctuation">.</span>merges arr<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span><span class="token number">280147</span><span class="token punctuation">]</span>  <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"Ä  Ä "</span><span class="token punctuation">,</span> <span class="token string">"Ä  Ä Ä Ä "</span><span class="token punctuation">,</span> <span class="token string">"Ä Ä  Ä Ä "</span><span class="token punctuation">,</span> "<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">18</span><span class="token punctuation">:</span>                tokenizer<span class="token punctuation">.</span>ggml<span class="token punctuation">.</span>bos_token_id u32              <span class="token operator">=</span> <span class="token number">128000</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">19</span><span class="token punctuation">:</span>                tokenizer<span class="token punctuation">.</span>ggml<span class="token punctuation">.</span>eos_token_id u32              <span class="token operator">=</span> <span class="token number">128001</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv  <span class="token number">20</span><span class="token punctuation">:</span>                    tokenizer<span class="token punctuation">.</span>chat_template <span class="token builtin">str</span>              <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token operator">%</span> <span class="token builtin">set</span> loop_messages <span class="token operator">=</span> messages <span class="token operator">%</span><span class="token punctuation">}</span><span class="token punctuation">{<!-- --></span><span class="token operator">%</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> <span class="token builtin">type</span>  f32<span class="token punctuation">:</span>   <span class="token number">65</span> tensors
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> <span class="token builtin">type</span>  f16<span class="token punctuation">:</span>  <span class="token number">226</span> tensors
<span class="token punctuation">[</span>   <span class="token number">1</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                    token_embd<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">128256</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>  <span class="token number">1002.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>   <span class="token number">532.31</span> MiB
<span class="token punctuation">[</span>   <span class="token number">2</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>               blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_norm<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f32<span class="token punctuation">,</span> size <span class="token operator">=</span>    <span class="token number">0.016</span> MB
<span class="token punctuation">[</span>   <span class="token number">3</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_down<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span><span class="token number">14336</span><span class="token punctuation">,</span>  <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>   <span class="token number">112.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">59.50</span> MiB
<span class="token punctuation">[</span>   <span class="token number">4</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_gate<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">14336</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>   <span class="token number">112.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">59.50</span> MiB
<span class="token punctuation">[</span>   <span class="token number">5</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                  blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_up<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">14336</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>   <span class="token number">112.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">59.50</span> MiB
<span class="token punctuation">[</span>   <span class="token number">6</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>ffn_norm<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f32<span class="token punctuation">,</span> size <span class="token operator">=</span>    <span class="token number">0.016</span> MB
<span class="token punctuation">[</span>   <span class="token number">7</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                  blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_k<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>  <span class="token number">1024</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>     <span class="token number">8.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>     <span class="token number">4.25</span> MiB
<span class="token punctuation">[</span>   <span class="token number">8</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>             blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_output<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>  <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>    <span class="token number">32.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">17.00</span> MiB
<span class="token punctuation">[</span>   <span class="token number">9</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                  blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_q<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>  <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>    <span class="token number">32.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">17.00</span> MiB
<span class="token punctuation">[</span>  <span class="token number">10</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                  blk<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>attn_v<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>  <span class="token number">1024</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>     <span class="token number">8.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>     <span class="token number">4.25</span> MiB
<span class="token punctuation">[</span>  <span class="token number">11</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>               blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>attn_norm<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f32<span class="token punctuation">,</span> size <span class="token operator">=</span>    <span class="token number">0.016</span> MB
<span class="token punctuation">[</span>  <span class="token number">12</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>ffn_down<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span><span class="token number">14336</span><span class="token punctuation">,</span>  <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>   <span class="token number">112.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">59.50</span> MiB
<span class="token punctuation">[</span>  <span class="token number">13</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>ffn_gate<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">14336</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>   <span class="token number">112.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">59.50</span> MiB
<span class="token punctuation">[</span>  <span class="token number">14</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                  blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>ffn_up<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">14336</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>   <span class="token number">112.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">59.50</span> MiB
<span class="token punctuation">[</span>  <span class="token number">15</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>ffn_norm<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f32<span class="token punctuation">,</span> size <span class="token operator">=</span>    <span class="token number">0.016</span> MB
<span class="token punctuation">[</span>  <span class="token number">16</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                  blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>attn_k<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>  <span class="token number">1024</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>     <span class="token number">8.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>     <span class="token number">4.25</span> MiB
<span class="token punctuation">[</span>  <span class="token number">17</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>             blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>attn_output<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>  <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>    <span class="token number">32.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">17.00</span> MiB
<span class="token punctuation">[</span>  <span class="token number">18</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                  blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>attn_q<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>  <span class="token number">4096</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>    <span class="token number">32.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>    <span class="token number">17.00</span> MiB
<span class="token punctuation">[</span>  <span class="token number">19</span><span class="token operator">/</span> <span class="token number">291</span><span class="token punctuation">]</span>                  blk<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">.</span>attn_v<span class="token punctuation">.</span>weight <span class="token operator">-</span> <span class="token punctuation">[</span> <span class="token number">4096</span><span class="token punctuation">,</span>  <span class="token number">1024</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">,</span>     <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">type</span> <span class="token operator">=</span>    f16<span class="token punctuation">,</span> converting to q8_0 <span class="token punctuation">.</span><span class="token punctuation">.</span> size <span class="token operator">=</span>     <span class="token number">8.00</span> MiB <span class="token operator">-</span><span class="token operator">&gt;</span>     <span class="token number">4.25</span> MiB
pythonå¤åˆ¶ä»£ç root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ll -h models/</span>
<span class="token operator">-</span>rw<span class="token operator">-</span>r<span class="token operator">-</span><span class="token operator">-</span>r<span class="token operator">-</span><span class="token operator">-</span>  <span class="token number">1</span> root root <span class="token number">8</span><span class="token punctuation">.</span>0G May <span class="token number">17</span> <span class="token number">07</span><span class="token punctuation">:</span><span class="token number">54</span> Llama3<span class="token operator">-</span>q8<span class="token punctuation">.</span>gguf
</code></pre> 
<h3><a id="_294"></a>æ¨¡å‹åŠ è½½ä¸æ¨ç†</h3> 
<p>æ¨¡å‹åŠ è½½ä¸æ¨ç†ä½¿ç”¨mainå‘½ä»¤ï¼Œå…¶æ”¯æŒå¦‚ä¸‹å¯ç”¨å‚æ•°ï¼š</p> 
<pre><code class="prism language-python">pythonå¤åˆ¶ä»£ç root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ./main -h</span>

usage<span class="token punctuation">:</span> <span class="token punctuation">.</span><span class="token operator">/</span>main <span class="token punctuation">[</span>options<span class="token punctuation">]</span>

options<span class="token punctuation">:</span>
  <span class="token operator">-</span>h<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span><span class="token builtin">help</span>            show this <span class="token builtin">help</span> message <span class="token keyword">and</span> exit
  <span class="token operator">-</span><span class="token operator">-</span>version             show version <span class="token keyword">and</span> build info
  <span class="token operator">-</span>i<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>interactive     run <span class="token keyword">in</span> interactive mode
  <span class="token operator">-</span><span class="token operator">-</span>interactive<span class="token operator">-</span>specials allow special tokens <span class="token keyword">in</span> user text<span class="token punctuation">,</span> <span class="token keyword">in</span> interactive mode
  <span class="token operator">-</span><span class="token operator">-</span>interactive<span class="token operator">-</span>first   run <span class="token keyword">in</span> interactive mode <span class="token keyword">and</span> wait <span class="token keyword">for</span> <span class="token builtin">input</span> right away
  <span class="token operator">-</span>cnv<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>conversation  run <span class="token keyword">in</span> conversation mode <span class="token punctuation">(</span>does <span class="token keyword">not</span> <span class="token keyword">print</span> special tokens <span class="token keyword">and</span> suffix<span class="token operator">/</span>prefix<span class="token punctuation">)</span>
  <span class="token operator">-</span>ins<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>instruct      run <span class="token keyword">in</span> instruction mode <span class="token punctuation">(</span>use <span class="token keyword">with</span> Alpaca models<span class="token punctuation">)</span>
  <span class="token operator">-</span>cml<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>chatml        run <span class="token keyword">in</span> chatml mode <span class="token punctuation">(</span>use <span class="token keyword">with</span> ChatML<span class="token operator">-</span>compatible models<span class="token punctuation">)</span>
  <span class="token operator">-</span><span class="token operator">-</span>multiline<span class="token operator">-</span><span class="token builtin">input</span>     allows you to write <span class="token keyword">or</span> paste multiple lines without ending each <span class="token keyword">in</span> '\'
  <span class="token operator">-</span>r PROMPT<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>reverse<span class="token operator">-</span>prompt PROMPT
                        halt generation at PROMPT<span class="token punctuation">,</span> <span class="token keyword">return</span> control <span class="token keyword">in</span> interactive mode
                        <span class="token punctuation">(</span>can be specified more than once <span class="token keyword">for</span> multiple prompts<span class="token punctuation">)</span><span class="token punctuation">.</span>
  <span class="token operator">-</span><span class="token operator">-</span>color               colorise output to distinguish prompt <span class="token keyword">and</span> user <span class="token builtin">input</span> <span class="token keyword">from</span> generations
  <span class="token operator">-</span>s SEED<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>seed SEED  RNG seed <span class="token punctuation">(</span>default<span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> use random seed <span class="token keyword">for</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span>
  <span class="token operator">-</span>t N<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>threads N     number of threads to use during generation <span class="token punctuation">(</span>default<span class="token punctuation">:</span> <span class="token number">30</span><span class="token punctuation">)</span>
  <span class="token operator">-</span>tb N<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>threads<span class="token operator">-</span>batch N
                        number of threads to use during batch <span class="token keyword">and</span> prompt processing <span class="token punctuation">(</span>default<span class="token punctuation">:</span> same <span class="token keyword">as</span> <span class="token operator">-</span><span class="token operator">-</span>threads<span class="token punctuation">)</span>
  <span class="token operator">-</span>td N<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>threads<span class="token operator">-</span>draft N                        number of threads to use during generation <span class="token punctuation">(</span>default<span class="token punctuation">:</span> same <span class="token keyword">as</span> <span class="token operator">-</span><span class="token operator">-</span>threads<span class="token punctuation">)</span>
  <span class="token operator">-</span>tbd N<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>threads<span class="token operator">-</span>batch<span class="token operator">-</span>draft N
                        number of threads to use during batch <span class="token keyword">and</span> prompt processing <span class="token punctuation">(</span>default<span class="token punctuation">:</span> same <span class="token keyword">as</span> <span class="token operator">-</span><span class="token operator">-</span>threads<span class="token operator">-</span>draft<span class="token punctuation">)</span>
  <span class="token operator">-</span>p PROMPT<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token operator">-</span>prompt PROMPT
                        prompt to start generation <span class="token keyword">with</span> <span class="token punctuation">(</span>default<span class="token punctuation">:</span> empty<span class="token punctuation">)</span>
</code></pre> 
<blockquote> 
 <p>å¯ä»¥åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æˆ–è€…ç»è¿‡é‡åŒ–ä¹‹åçš„æ¨¡å‹ï¼Œè¿™é‡Œé€‰æ‹©åŠ è½½é‡åŒ–åçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚</p> 
</blockquote> 
<p>åœ¨llama.cppé¡¹ç›®çš„æ ¹ç›®å½•ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼ŒåŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚</p> 
<pre><code class="prism language-python">pythonå¤åˆ¶ä»£ç root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ./main -m models/Llama3-q8.gguf --color -f prompts/alpaca.txt -ins -c 2048 --temp 0.2 -n 256 --repeat_penalty 1.1</span>
Log start
main<span class="token punctuation">:</span> build <span class="token operator">=</span> <span class="token number">2908</span> <span class="token punctuation">(</span>359cbe3f<span class="token punctuation">)</span>
main<span class="token punctuation">:</span> built <span class="token keyword">with</span> cc <span class="token punctuation">(</span>Ubuntu <span class="token number">11.4</span><span class="token number">.0</span><span class="token operator">-</span>1ubuntu1<span class="token operator">~</span><span class="token number">22.04</span><span class="token punctuation">)</span> <span class="token number">11.4</span><span class="token number">.0</span> <span class="token keyword">for</span> x86_64<span class="token operator">-</span>linux<span class="token operator">-</span>gnu
main<span class="token punctuation">:</span> seed  <span class="token operator">=</span> <span class="token number">1715935175</span>
llama_model_loader<span class="token punctuation">:</span> loaded meta data <span class="token keyword">with</span> <span class="token number">22</span> key<span class="token operator">-</span>value pairs <span class="token keyword">and</span> <span class="token number">291</span> tensors <span class="token keyword">from</span> models<span class="token operator">/</span>Llama3<span class="token operator">-</span>q8<span class="token punctuation">.</span>gguf <span class="token punctuation">(</span>version GGUF V3 <span class="token punctuation">(</span>latest<span class="token punctuation">)</span><span class="token punctuation">)</span>
llama_model_loader<span class="token punctuation">:</span> Dumping metadata keys<span class="token operator">/</span>values<span class="token punctuation">.</span> Note<span class="token punctuation">:</span> KV overrides do <span class="token keyword">not</span> <span class="token builtin">apply</span> <span class="token keyword">in</span> this output<span class="token punctuation">.</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">0</span><span class="token punctuation">:</span>                       general<span class="token punctuation">.</span>architecture <span class="token builtin">str</span>              <span class="token operator">=</span> llama
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">1</span><span class="token punctuation">:</span>                               general<span class="token punctuation">.</span>name <span class="token builtin">str</span>              <span class="token operator">=</span> Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">2</span><span class="token punctuation">:</span>                           llama<span class="token punctuation">.</span>vocab_size u32              <span class="token operator">=</span> <span class="token number">128256</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">3</span><span class="token punctuation">:</span>                       llama<span class="token punctuation">.</span>context_length u32              <span class="token operator">=</span> <span class="token number">8192</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">4</span><span class="token punctuation">:</span>                     llama<span class="token punctuation">.</span>embedding_length u32              <span class="token operator">=</span> <span class="token number">4096</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">5</span><span class="token punctuation">:</span>                          llama<span class="token punctuation">.</span>block_count u32              <span class="token operator">=</span> <span class="token number">32</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">6</span><span class="token punctuation">:</span>                  llama<span class="token punctuation">.</span>feed_forward_length u32              <span class="token operator">=</span> <span class="token number">14336</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">7</span><span class="token punctuation">:</span>                 llama<span class="token punctuation">.</span>rope<span class="token punctuation">.</span>dimension_count u32              <span class="token operator">=</span> <span class="token number">128</span>

<span class="token operator">==</span> Running <span class="token keyword">in</span> interactive mode<span class="token punctuation">.</span> <span class="token operator">==</span>
 <span class="token operator">-</span> Press Ctrl<span class="token operator">+</span>C to interject at <span class="token builtin">any</span> time<span class="token punctuation">.</span>
 <span class="token operator">-</span> Press Return to <span class="token keyword">return</span> control to LLaMa<span class="token punctuation">.</span>
 <span class="token operator">-</span> To <span class="token keyword">return</span> control without starting a new line<span class="token punctuation">,</span> end your <span class="token builtin">input</span> <span class="token keyword">with</span> <span class="token string">'/'</span><span class="token punctuation">.</span>
 <span class="token operator">-</span> If you want to submit another line<span class="token punctuation">,</span> end your <span class="token builtin">input</span> <span class="token keyword">with</span> '\'<span class="token punctuation">.</span>

<span class="token operator">&lt;</span><span class="token operator">|</span>begin_of_text<span class="token operator">|</span><span class="token operator">&gt;</span>Below <span class="token keyword">is</span> an instruction that describes a task<span class="token punctuation">.</span> Write a response that appropriately completes the request<span class="token punctuation">.</span>
<span class="token operator">&gt;</span> hi
Hello! How can I <span class="token builtin">help</span> you today?<span class="token operator">&lt;</span><span class="token operator">|</span>eot_id<span class="token operator">|</span><span class="token operator">&gt;</span>

<span class="token operator">&gt;</span>
</code></pre> 
<p>åœ¨æç¤ºç¬¦<code>&gt;</code>ä¹‹åè¾“å…¥promptï¼Œä½¿ç”¨<code>ctrl+c</code>ä¸­æ–­è¾“å‡ºï¼Œå¤šè¡Œä¿¡æ¯ä»¥<code>\</code>ä½œä¸ºè¡Œå°¾ã€‚æ‰§è¡Œ<code>./main -h</code>å‘½ä»¤æŸ¥çœ‹å¸®åŠ©å’Œå‚æ•°è¯´æ˜ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å¸¸ç”¨çš„å‚æ•°ï¼š `</p> 
<table><thead><tr><th>å‘½ä»¤</th><th>æè¿°</th></tr></thead><tbody><tr><td>-m</td><td>æŒ‡å®š LLaMA æ¨¡å‹æ–‡ä»¶çš„è·¯å¾„</td></tr><tr><td>-mu</td><td>æŒ‡å®šè¿œç¨‹ http url æ¥ä¸‹è½½æ–‡ä»¶</td></tr><tr><td>-i</td><td>ä»¥äº¤äº’æ¨¡å¼è¿è¡Œç¨‹åºï¼Œå…è®¸ç›´æ¥æä¾›è¾“å…¥å¹¶æ¥æ”¶å®æ—¶å“åº”ã€‚</td></tr><tr><td>-ins</td><td>ä»¥æŒ‡ä»¤æ¨¡å¼è¿è¡Œç¨‹åºï¼Œè¿™åœ¨å¤„ç†ç¾Šé©¼æ¨¡å‹æ—¶ç‰¹åˆ«æœ‰ç”¨ã€‚</td></tr><tr><td>-f</td><td>æŒ‡å®špromptæ¨¡æ¿ï¼Œalpacaæ¨¡å‹è¯·åŠ è½½prompts/alpaca.txt</td></tr><tr><td>-n</td><td>æ§åˆ¶å›å¤ç”Ÿæˆçš„æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤ï¼š128ï¼‰</td></tr><tr><td>-c</td><td>è®¾ç½®æç¤ºä¸Šä¸‹æ–‡çš„å¤§å°ï¼Œå€¼è¶Šå¤§è¶Šèƒ½å‚è€ƒæ›´é•¿çš„å¯¹è¯å†å²ï¼ˆé»˜è®¤ï¼š512ï¼‰</td></tr><tr><td>-b</td><td>æ§åˆ¶batch sizeï¼ˆé»˜è®¤ï¼š8ï¼‰ï¼Œå¯é€‚å½“å¢åŠ </td></tr><tr><td>-t</td><td>æ§åˆ¶çº¿ç¨‹æ•°é‡ï¼ˆé»˜è®¤ï¼š4ï¼‰ï¼Œå¯é€‚å½“å¢åŠ </td></tr><tr><td><code>--</code>repeat_penalty</td><td>æ§åˆ¶ç”Ÿæˆå›å¤ä¸­å¯¹é‡å¤æ–‡æœ¬çš„æƒ©ç½šåŠ›åº¦</td></tr><tr><td><code>--</code>temp</td><td>æ¸©åº¦ç³»æ•°ï¼Œå€¼è¶Šä½å›å¤çš„éšæœºæ€§è¶Šå°ï¼Œåä¹‹è¶Šå¤§</td></tr><tr><td><code>--</code>top_p, top_k</td><td>æ§åˆ¶è§£ç é‡‡æ ·çš„ç›¸å…³å‚æ•°</td></tr><tr><td><code>--</code>color</td><td>åŒºåˆ†ç”¨æˆ·è¾“å…¥å’Œç”Ÿæˆçš„æ–‡æœ¬</td></tr></tbody></table> 
<h3><a id="API_380"></a>æ¨¡å‹APIæœåŠ¡</h3> 
<p>llama.cppæä¾›äº†å®Œå…¨ä¸OpenAI APIå…¼å®¹çš„APIæ¥å£ï¼Œä½¿ç”¨ç»è¿‡ç¼–è¯‘ç”Ÿæˆçš„serverå¯æ‰§è¡Œæ–‡ä»¶å¯åŠ¨APIæœåŠ¡ã€‚</p> 
<pre><code class="prism language-python">pythonå¤åˆ¶ä»£ç root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ./server -m models/Llama3-q8.gguf --host 0.0.0.0 --port 8000</span>
<span class="token punctuation">{<!-- --></span><span class="token string">"tid"</span><span class="token punctuation">:</span><span class="token string">"140018656950080"</span><span class="token punctuation">,</span><span class="token string">"timestamp"</span><span class="token punctuation">:</span><span class="token number">1715936504</span><span class="token punctuation">,</span><span class="token string">"level"</span><span class="token punctuation">:</span><span class="token string">"INFO"</span><span class="token punctuation">,</span><span class="token string">"function"</span><span class="token punctuation">:</span><span class="token string">"main"</span><span class="token punctuation">,</span><span class="token string">"line"</span><span class="token punctuation">:</span><span class="token number">2942</span><span class="token punctuation">,</span><span class="token string">"msg"</span><span class="token punctuation">:</span><span class="token string">"build info"</span><span class="token punctuation">,</span><span class="token string">"build"</span><span class="token punctuation">:</span><span class="token number">2908</span><span class="token punctuation">,</span><span class="token string">"commit"</span><span class="token punctuation">:</span><span class="token string">"359cbe3f"</span><span class="token punctuation">}</span>
<span class="token punctuation">{<!-- --></span><span class="token string">"tid"</span><span class="token punctuation">:</span><span class="token string">"140018656950080"</span><span class="token punctuation">,</span><span class="token string">"timestamp"</span><span class="token punctuation">:</span><span class="token number">1715936504</span><span class="token punctuation">,</span><span class="token string">"level"</span><span class="token punctuation">:</span><span class="token string">"INFO"</span><span class="token punctuation">,</span><span class="token string">"function"</span><span class="token punctuation">:</span><span class="token string">"main"</span><span class="token punctuation">,</span><span class="token string">"line"</span><span class="token punctuation">:</span><span class="token number">2947</span><span class="token punctuation">,</span><span class="token string">"msg"</span><span class="token punctuation">:</span><span class="token string">"system info"</span><span class="token punctuation">,</span><span class="token string">"n_threads"</span><span class="token punctuation">:</span><span class="token number">30</span><span class="token punctuation">,</span><span class="token string">"n_threads_batch"</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token string">"total_threads"</span><span class="token punctuation">:</span><span class="token number">30</span><span class="token punctuation">,</span><span class="token string">"system_info"</span><span class="token punctuation">:</span><span class="token string">"AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | "</span><span class="token punctuation">}</span>
llama_model_loader<span class="token punctuation">:</span> loaded meta data <span class="token keyword">with</span> <span class="token number">22</span> key<span class="token operator">-</span>value pairs <span class="token keyword">and</span> <span class="token number">291</span> tensors <span class="token keyword">from</span> models<span class="token operator">/</span>Llama3<span class="token operator">-</span>q8<span class="token punctuation">.</span>gguf <span class="token punctuation">(</span>version GGUF V3 <span class="token punctuation">(</span>latest<span class="token punctuation">)</span><span class="token punctuation">)</span>
llama_model_loader<span class="token punctuation">:</span> Dumping metadata keys<span class="token operator">/</span>values<span class="token punctuation">.</span> Note<span class="token punctuation">:</span> KV overrides do <span class="token keyword">not</span> <span class="token builtin">apply</span> <span class="token keyword">in</span> this output<span class="token punctuation">.</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">0</span><span class="token punctuation">:</span>                       general<span class="token punctuation">.</span>architecture <span class="token builtin">str</span>              <span class="token operator">=</span> llama
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">1</span><span class="token punctuation">:</span>                               general<span class="token punctuation">.</span>name <span class="token builtin">str</span>              <span class="token operator">=</span> Llama3<span class="token operator">-</span>Chinese<span class="token operator">-</span>8B<span class="token operator">-</span>Instruct
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">2</span><span class="token punctuation">:</span>                           llama<span class="token punctuation">.</span>vocab_size u32              <span class="token operator">=</span> <span class="token number">128256</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">3</span><span class="token punctuation">:</span>                       llama<span class="token punctuation">.</span>context_length u32              <span class="token operator">=</span> <span class="token number">8192</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">4</span><span class="token punctuation">:</span>                     llama<span class="token punctuation">.</span>embedding_length u32              <span class="token operator">=</span> <span class="token number">4096</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">5</span><span class="token punctuation">:</span>                          llama<span class="token punctuation">.</span>block_count u32              <span class="token operator">=</span> <span class="token number">32</span>
llama_model_loader<span class="token punctuation">:</span> <span class="token operator">-</span> kv   <span class="token number">6</span><span class="token punctuation">:</span>                  llama<span class="token punctuation">.</span>feed_forward_length u32              <span class="token operator">=</span> <span class="token number">14336</span>
</code></pre> 
<p>å¯åŠ¨APIæœåŠ¡åï¼Œå¯ä»¥ä½¿ç”¨curlå‘½ä»¤è¿›è¡Œæµ‹è¯•</p> 
<pre><code class="prism language-python">pythonå¤åˆ¶ä»£ç curl <span class="token operator">-</span><span class="token operator">-</span>request POST \
    <span class="token operator">-</span><span class="token operator">-</span>url http<span class="token punctuation">:</span><span class="token operator">//</span>localhost<span class="token punctuation">:</span><span class="token number">8000</span><span class="token operator">/</span>completion \
    <span class="token operator">-</span><span class="token operator">-</span>header <span class="token string">"Content-Type: application/json"</span> \
    <span class="token operator">-</span><span class="token operator">-</span>data <span class="token string">'{"prompt": "Hi"}'</span>
</code></pre> 
<h3><a id="API_408"></a>æ¨¡å‹APIæœåŠ¡(ç¬¬ä¸‰æ–¹)</h3> 
<blockquote> 
 <p>åœ¨llamm.cppé¡¹ç›®ä¸­æœ‰æåˆ°å„ç§è¯­è¨€ç¼–å†™çš„ç¬¬ä¸‰æ–¹å·¥å…·åŒ…ï¼Œå¯ä»¥ä½¿ç”¨è¿™äº›å·¥å…·åŒ…æä¾›APIæœåŠ¡ï¼Œè¿™é‡Œä»¥Pythonä¸ºä¾‹ï¼Œä½¿ç”¨<a href="https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Fabetlen%2Fllama-cpp-python" rel="nofollow">llama-cpp-python</a>æä¾›APIæœåŠ¡ã€‚</p> 
</blockquote> 
<p>å®‰è£…ä¾èµ–</p> 
<pre><code class="prism language-python">pythonå¤åˆ¶ä»£ç pip install llama<span class="token operator">-</span>cpp<span class="token operator">-</span>python

pip install llama<span class="token operator">-</span>cpp<span class="token operator">-</span>python <span class="token operator">-</span>i https<span class="token punctuation">:</span><span class="token operator">//</span>mirrors<span class="token punctuation">.</span>aliyun<span class="token punctuation">.</span>com<span class="token operator">/</span>pypi<span class="token operator">/</span>simple<span class="token operator">/</span>
</code></pre> 
<p>æ³¨æ„ï¼šå¯èƒ½è¿˜éœ€è¦å®‰è£…ä»¥ä¸‹ç¼ºå¤±ä¾èµ–ï¼Œå¯æ ¹æ®å¯åŠ¨æ—¶çš„å¼‚å¸¸æç¤ºåˆ†åˆ«å®‰è£…ã€‚</p> 
<pre><code class="prism language-python">python

å¤åˆ¶ä»£ç pip install sse_starlette starlette_context pydantic_settings
</code></pre> 
<p>å¯åŠ¨APIæœåŠ¡ï¼Œé»˜è®¤è¿è¡Œåœ¨<code>http://localhost:8000</code></p> 
<pre><code class="prism language-python">python

å¤åˆ¶ä»£ç python <span class="token operator">-</span>m llama_cpp<span class="token punctuation">.</span>server <span class="token operator">-</span><span class="token operator">-</span>model models<span class="token operator">/</span>Llama3<span class="token operator">-</span>q8<span class="token punctuation">.</span>gguf
</code></pre> 
<p>å®‰è£…openaiä¾èµ–</p> 
<pre><code class="prism language-python">python

å¤åˆ¶ä»£ç pip install openai
</code></pre> 
<p>ä½¿ç”¨openaiè°ƒç”¨APIæœåŠ¡</p> 
<pre><code class="prism language-python">pythonå¤åˆ¶ä»£ç <span class="token keyword">import</span> os
<span class="token keyword">from</span> openai <span class="token keyword">import</span> OpenAI  <span class="token comment"># å¯¼å…¥OpenAIåº“</span>

<span class="token comment"># è®¾ç½®OpenAIçš„BASE_URL</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">"OPENAI_BASE_URL"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">"http://localhost:8000/v1"</span>

client <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># åˆ›å»ºOpenAIå®¢æˆ·ç«¯å¯¹è±¡</span>

<span class="token comment"># è°ƒç”¨æ¨¡å‹</span>
completion <span class="token operator">=</span> client<span class="token punctuation">.</span>chat<span class="token punctuation">.</span>completions<span class="token punctuation">.</span>create<span class="token punctuation">(</span>
    model<span class="token operator">=</span><span class="token string">"llama3"</span><span class="token punctuation">,</span> <span class="token comment"># ä»»æ„å¡«</span>
    messages<span class="token operator">=</span><span class="token punctuation">[</span>
        <span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"ä½ å¥½!"</span><span class="token punctuation">}</span>
    <span class="token punctuation">]</span>
<span class="token punctuation">)</span>

<span class="token comment"># è¾“å‡ºæ¨¡å‹å›å¤</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>completion<span class="token punctuation">.</span>choices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>message<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/0f/dd/5zSQMlIY_o.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p> 
<h3><a id="GPU_470"></a>GPUæ¨ç†</h3> 
<blockquote> 
 <p>å¦‚æœç¼–è¯‘æ„å»ºäº†GPUæ‰§è¡Œç¯å¢ƒï¼Œå¯ä»¥ä½¿ç”¨<code>-ngl N</code>æˆ–<code> --n-gpu-layers N</code>å‚æ•°ï¼ŒæŒ‡å®šoffloadå±‚æ•°ï¼Œè®©æ¨¡å‹åœ¨GPUä¸Šè¿è¡Œæ¨ç†</p> 
</blockquote> 
<blockquote> 
 <p>ä¾‹å¦‚ï¼š<code>-ngl 40</code>è¡¨ç¤ºoffload 40å±‚æ¨¡å‹å‚æ•°åˆ°GPU</p> 
</blockquote> 
<p>æœªä½¿ç”¨<code>-ngl N</code>æˆ–<code> --n-gpu-layers N</code>å‚æ•°ï¼Œç¨‹åºé»˜è®¤åœ¨CPUä¸Šè¿è¡Œ</p> 
<pre><code class="prism language-python">python

å¤åˆ¶ä»£ç root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ./server -m models/Llama3-FP16.gguf  --host 0.0.0.0 --port 8000</span>
</code></pre> 
<p>å¯ä»ä»¥ä¸‹å…³é”®å¯åŠ¨æ—¥å¿—çœ‹å‡ºï¼Œæ¨¡å‹å¹¶æ²¡æœ‰åœ¨GPUä¸Šæ‰§è¡Œ</p> 
<pre><code class="prism language-python">pythonå¤åˆ¶ä»£ç ggml_cuda_init<span class="token punctuation">:</span> GGML_CUDA_FORCE_MMQ<span class="token punctuation">:</span>   no
ggml_cuda_init<span class="token punctuation">:</span> CUDA_USE_TENSOR_CORES<span class="token punctuation">:</span> yes
ggml_cuda_init<span class="token punctuation">:</span> found <span class="token number">1</span> CUDA devices<span class="token punctuation">:</span>
  Device <span class="token number">0</span><span class="token punctuation">:</span> Tesla V100S<span class="token operator">-</span>PCIE<span class="token operator">-</span>32GB<span class="token punctuation">,</span> compute capability <span class="token number">7.0</span><span class="token punctuation">,</span> VMM<span class="token punctuation">:</span> yes
llm_load_tensors<span class="token punctuation">:</span> ggml ctx size <span class="token operator">=</span>    <span class="token number">0.15</span> MiB
llm_load_tensors<span class="token punctuation">:</span> offloading <span class="token number">0</span> repeating layers to GPU
llm_load_tensors<span class="token punctuation">:</span> offloaded <span class="token number">0</span><span class="token operator">/</span><span class="token number">33</span> layers to GPU
llm_load_tensors<span class="token punctuation">:</span>        CPU <span class="token builtin">buffer</span> size <span class="token operator">=</span>  <span class="token number">8137.64</span> MiB
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
llama_new_context_with_model<span class="token punctuation">:</span> n_ctx      <span class="token operator">=</span> <span class="token number">2048</span>
llama_new_context_with_model<span class="token punctuation">:</span> n_batch    <span class="token operator">=</span> <span class="token number">2048</span>
llama_new_context_with_model<span class="token punctuation">:</span> n_ubatch   <span class="token operator">=</span> <span class="token number">512</span>
</code></pre> 
<p>ä½¿ç”¨<code>-ngl N</code>æˆ–<code> --n-gpu-layers N</code>å‚æ•°ï¼Œç¨‹åºé»˜è®¤åœ¨GPUä¸Šè¿è¡Œ</p> 
<pre><code class="prism language-python">python

å¤åˆ¶ä»£ç root@master<span class="token punctuation">:</span><span class="token operator">~</span><span class="token operator">/</span>work<span class="token operator">/</span>llama<span class="token punctuation">.</span>cpp<span class="token comment"># ./server -m models/Llama3-FP16.gguf  --host 0.0.0.0 --port 8000   --n-gpu-layers 1000</span>
</code></pre> 
<p>å¯ä»ä»¥ä¸‹å…³é”®å¯åŠ¨æ—¥å¿—çœ‹å‡ºï¼Œæ¨¡å‹åœ¨GPUä¸Šæ‰§è¡Œ</p> 
<pre><code class="prism language-python">pythonå¤åˆ¶ä»£ç ggml_cuda_init<span class="token punctuation">:</span> GGML_CUDA_FORCE_MMQ<span class="token punctuation">:</span>   no
ggml_cuda_init<span class="token punctuation">:</span> CUDA_USE_TENSOR_CORES<span class="token punctuation">:</span> yes
ggml_cuda_init<span class="token punctuation">:</span> found <span class="token number">1</span> CUDA devices<span class="token punctuation">:</span>
  Device <span class="token number">0</span><span class="token punctuation">:</span> Tesla V100S<span class="token operator">-</span>PCIE<span class="token operator">-</span>32GB<span class="token punctuation">,</span> compute capability <span class="token number">7.0</span><span class="token punctuation">,</span> VMM<span class="token punctuation">:</span> yes
llm_load_tensors<span class="token punctuation">:</span> ggml ctx size <span class="token operator">=</span>    <span class="token number">0.30</span> MiB
llm_load_tensors<span class="token punctuation">:</span> offloading <span class="token number">32</span> repeating layers to GPU
llm_load_tensors<span class="token punctuation">:</span> offloading non<span class="token operator">-</span>repeating layers to GPU
llm_load_tensors<span class="token punctuation">:</span> offloaded <span class="token number">33</span><span class="token operator">/</span><span class="token number">33</span> layers to GPU
llm_load_tensors<span class="token punctuation">:</span>        CPU <span class="token builtin">buffer</span> size <span class="token operator">=</span>  <span class="token number">1002.00</span> MiB
llm_load_tensors<span class="token punctuation">:</span>      CUDA0 <span class="token builtin">buffer</span> size <span class="token operator">=</span> <span class="token number">14315.02</span> MiB
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
llama_new_context_with_model<span class="token punctuation">:</span> n_ctx      <span class="token operator">=</span> <span class="token number">512</span>
llama_new_context_with_model<span class="token punctuation">:</span> n_batch    <span class="token operator">=</span> <span class="token number">512</span>
llama_new_context_with_model<span class="token punctuation">:</span> n_ubatch   <span class="token operator">=</span> <span class="token number">512</span>
llama_new_context_with_model<span class="token punctuation">:</span> flash_attn <span class="token operator">=</span> <span class="token number">0</span>
</code></pre> 
<p>æ‰§è¡Œ<code>nvidia-smi</code>å‘½ä»¤ï¼Œå¯ä»¥è¿›ä¸€æ­¥éªŒè¯æ¨¡å‹å·²åœ¨GPUä¸Šè¿è¡Œã€‚ <img src="https://images2.imgbox.com/4c/f1/rqLRIm0K_o.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p> 
<p>https://juejin.cn/theme/detail/7218019389664067621?contentType=1)</p> 
<h3><a id="_536"></a><strong>é‚£ä¹ˆï¼Œæˆ‘ä»¬è¯¥å¦‚ä½•å­¦ä¹ å¤§æ¨¡å‹ï¼Ÿ</strong></h3> 
<p>ä½œä¸ºä¸€åçƒ­å¿ƒè‚ çš„äº’è”ç½‘è€å…µï¼Œæˆ‘å†³å®šæŠŠå®è´µçš„AIçŸ¥è¯†åˆ†äº«ç»™å¤§å®¶ã€‚ è‡³äºèƒ½å­¦ä¹ åˆ°å¤šå°‘å°±çœ‹ä½ çš„å­¦ä¹ æ¯…åŠ›å’Œèƒ½åŠ›äº† ã€‚æˆ‘å·²å°†é‡è¦çš„AIå¤§æ¨¡å‹èµ„æ–™åŒ…æ‹¬AIå¤§æ¨¡å‹å…¥é—¨å­¦ä¹ æ€ç»´å¯¼å›¾ã€ç²¾å“AIå¤§æ¨¡å‹å­¦ä¹ ä¹¦ç±æ‰‹å†Œã€è§†é¢‘æ•™ç¨‹ã€å®æˆ˜å­¦ä¹ ç­‰å½•æ’­è§†é¢‘å…è´¹åˆ†äº«å‡ºæ¥ã€‚</p> 
<h4><a id="_540"></a>ä¸€ã€å¤§æ¨¡å‹å…¨å¥—çš„å­¦ä¹ è·¯çº¿</h4> 
<p>å­¦ä¹ å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œå¦‚GPT-3ã€BERTæˆ–ä»»ä½•å…¶ä»–å…ˆè¿›çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œéœ€è¦ç³»ç»Ÿçš„æ–¹æ³•å’ŒæŒç»­çš„åŠªåŠ›ã€‚æ—¢ç„¶è¦ç³»ç»Ÿçš„å­¦ä¹ å¤§æ¨¡å‹ï¼Œé‚£ä¹ˆå­¦ä¹ è·¯çº¿æ˜¯å¿…ä¸å¯å°‘çš„ï¼Œä¸‹é¢çš„è¿™ä»½è·¯çº¿èƒ½å¸®åŠ©ä½ å¿«é€Ÿæ¢³ç†çŸ¥è¯†ï¼Œå½¢æˆè‡ªå·±çš„ä½“ç³»ã€‚</p> 
<p><strong>L1çº§åˆ«:AIå¤§æ¨¡å‹æ—¶ä»£çš„åä¸½ç™»åœº</strong><br> <img src="https://images2.imgbox.com/d8/c3/3wYlaSyP_o.png" alt=""><br> <strong>L2çº§åˆ«ï¼šAIå¤§æ¨¡å‹APIåº”ç”¨å¼€å‘å·¥ç¨‹</strong><br> <img src="https://images2.imgbox.com/6a/96/AXprE9Gb_o.png" alt=""><br> <strong>L3çº§åˆ«ï¼šå¤§æ¨¡å‹åº”ç”¨æ¶æ„è¿›é˜¶å®è·µ</strong><br> <img src="https://images2.imgbox.com/bf/b5/CMLPqnru_o.png" alt=""><br> <strong>L4çº§åˆ«ï¼šå¤§æ¨¡å‹å¾®è°ƒä¸ç§æœ‰åŒ–éƒ¨ç½²</strong><br> <img src="https://images2.imgbox.com/84/db/qTtmuUgh_o.png" alt=""><br> ä¸€èˆ¬æŒæ¡åˆ°ç¬¬å››ä¸ªçº§åˆ«ï¼Œå¸‚åœºä¸Šå¤§å¤šæ•°å²—ä½éƒ½æ˜¯å¯ä»¥èƒœä»»ï¼Œä½†è¦è¿˜ä¸æ˜¯å¤©èŠ±æ¿ï¼Œå¤©èŠ±æ¿çº§åˆ«è¦æ±‚æ›´åŠ ä¸¥æ ¼ï¼Œå¯¹äºç®—æ³•å’Œå®æˆ˜æ˜¯éå¸¸è‹›åˆ»çš„ã€‚å»ºè®®æ™®é€šäººæŒæ¡åˆ°L4çº§åˆ«å³å¯ã€‚</p> 
<p><strong>ä»¥ä¸Šçš„AIå¤§æ¨¡å‹å­¦ä¹ è·¯çº¿ï¼Œä¸çŸ¥é“ä¸ºä»€ä¹ˆå‘å‡ºæ¥å°±æœ‰ç‚¹ç³Š</strong>ï¼Œé«˜æ¸…ç‰ˆå¯ä»¥å¾®ä¿¡æ‰«æä¸‹æ–¹CSDNå®˜æ–¹è®¤è¯äºŒç»´ç å…è´¹é¢†å–ã€<code>ä¿è¯100%å…è´¹</code>ã€‘</p> 
<img src="https://images2.imgbox.com/ff/0e/gRJjktl3_o.png"> 
<h4><a id="640AI_560"></a>äºŒã€640å¥—AIå¤§æ¨¡å‹æŠ¥å‘Šåˆé›†</h4> 
<p>è¿™å¥—åŒ…å«640ä»½æŠ¥å‘Šçš„åˆé›†ï¼Œæ¶µç›–äº†AIå¤§æ¨¡å‹çš„ç†è®ºç ”ç©¶ã€æŠ€æœ¯å®ç°ã€è¡Œä¸šåº”ç”¨ç­‰å¤šä¸ªæ–¹é¢ã€‚æ— è®ºæ‚¨æ˜¯ç§‘ç ”äººå‘˜ã€å·¥ç¨‹å¸ˆï¼Œè¿˜æ˜¯å¯¹AIå¤§æ¨¡å‹æ„Ÿå…´è¶£çš„çˆ±å¥½è€…ï¼Œè¿™å¥—æŠ¥å‘Šåˆé›†éƒ½å°†ä¸ºæ‚¨æä¾›å®è´µçš„ä¿¡æ¯å’Œå¯ç¤ºã€‚</p> 
<p><img src="https://images2.imgbox.com/8e/d5/R9THLHqP_o.png" alt="img"></p> 
<h4><a id="PDF_568"></a>ä¸‰ã€å¤§æ¨¡å‹ç»å…¸PDFç±</h4> 
<p>éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„é£é€Ÿå‘å±•ï¼ŒAIå¤§æ¨¡å‹å·²ç»æˆä¸ºäº†å½“ä»Šç§‘æŠ€é¢†åŸŸçš„ä¸€å¤§çƒ­ç‚¹ã€‚è¿™äº›å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚GPT-3ã€BERTã€XLNetç­‰ï¼Œä»¥å…¶å¼ºå¤§çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œæ­£åœ¨æ”¹å˜æˆ‘ä»¬å¯¹äººå·¥æ™ºèƒ½çš„è®¤è¯†ã€‚ é‚£ä»¥ä¸‹è¿™äº›PDFç±å°±æ˜¯éå¸¸ä¸é”™çš„å­¦ä¹ èµ„æºã€‚</p> 
<p><img src="https://images2.imgbox.com/bc/c5/iWz47lGW_o.png" alt="img"></p> 
<h4><a id="AI_574"></a>å››ã€AIå¤§æ¨¡å‹å•†ä¸šåŒ–è½åœ°æ–¹æ¡ˆ</h4> 
<p><img src="https://images2.imgbox.com/a4/16/mYQz8PM2_o.png" alt="img"></p> 
<p>ä½œä¸ºæ™®é€šäººï¼Œå…¥å±€å¤§æ¨¡å‹æ—¶ä»£éœ€è¦æŒç»­å­¦ä¹ å’Œå®è·µï¼Œä¸æ–­æé«˜è‡ªå·±çš„æŠ€èƒ½å’Œè®¤çŸ¥æ°´å¹³ï¼ŒåŒæ—¶ä¹Ÿéœ€è¦æœ‰è´£ä»»æ„Ÿå’Œä¼¦ç†æ„è¯†ï¼Œä¸ºäººå·¥æ™ºèƒ½çš„å¥åº·å‘å±•è´¡çŒ®åŠ›é‡ã€‚</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e0290eb4369cae5175e04252474c63bf/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">Leafleté›†æˆwheelnavåœ¨WebGISä¸­çš„åº”ç”¨</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0a4c0bc7e7efcaede00b253951a18197/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">æ•æ·ï¼996/007ï¼Ÿç°å®æ˜¯â€¦â€¦</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 ç¼–ç¨‹å¤§å’–.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>