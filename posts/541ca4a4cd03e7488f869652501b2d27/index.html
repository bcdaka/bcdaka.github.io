<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【深度学习】最强算法之：残差网络（ResNet） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/541ca4a4cd03e7488f869652501b2d27/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【深度学习】最强算法之：残差网络（ResNet）">
  <meta property="og:description" content="残差网络 1、引言2、残差网络2.1 定义2.2 原理2.3 实现方式2.4 算法公式2.5 代码示例 3、总结 1、引言 小屌丝：鱼哥，深度神经网络是不是少了一篇没写
小鱼：少了哪篇？
小屌丝：额… 就是 内个…
小鱼：你倒是说啊， 哪个啊？
小屌丝：就…就是
小鱼：赶紧说，别墨迹
小屌丝：ResNet
小鱼：咳， 我还以为哪篇了
小屌丝：那…是不是要更新啊
小鱼：好说好说。
小屌丝： 哇塞。
2、残差网络 2.1 定义 残差网络（Residual Network，简称ResNet）是由Kaiming He等人在2015年提出的一种深度神经网络架构。
ResNet的核心思想是引入了“残差学习”的概念来解决深度神经网络训练中的梯度消失/梯度爆炸问题，使得网络能够通过简单地增加层数来提高准确率，而不会导致训练困难。
2.2 原理 残差网络的核心原理是通过残差模块来构建深层网络。
在传统的神经网络中，每一层的输出是下一层的输入。
而在残差网络中，每一层的输入不仅会传递给下一层，还会通过跳跃连接（skip connection）直接传递给更深的层次。
这种设计允许梯度直接流过这些跳跃连接，从而缓解了深层网络中的梯度消失问题。
2.3 实现方式 残差网络的实现主要包括以下步骤：
构建残差块： 每个残差块包含两个或三个卷积层，以及一个跨层的直接连接。对于两层残差块（称为basic block），输入首先经过一个卷积层（通常为3x3卷积），然后通过ReLU激活函数，再经过另一个卷积层，最后与原始输入（通过恒等映射或1x1卷积进行维度匹配）相加。对于三层残差块（称为bottleneck block），中间还包含一个1x1卷积层用于减少通道数，从而减少计算量。 堆叠残差块：将多个残差块堆叠在一起，形成完整的残差网络。根据任务需求和网络深度，可以选择使用不同类型的残差块和堆叠方式。添加其他组件：在残差网络的开头和结尾，可以添加其他组件，如卷积层、池化层、全连接层等，以满足特定的任务需求。 2.4 算法公式 残差模块的基本形式可以用以下公式表示：
[ y = F ( x , W i ) &#43; x ] [ \mathbf{y} = \mathcal{F}(\mathbf{x}, {W_i}) &#43; \mathbf{x} ] [y=F(x,Wi​)&#43;x]">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-06T12:48:48+08:00">
    <meta property="article:modified_time" content="2024-05-06T12:48:48+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【深度学习】最强算法之：残差网络（ResNet）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>残差网络</h4> 
 <ul><li><a href="#1_1" rel="nofollow">1、引言</a></li><li><a href="#2_19" rel="nofollow">2、残差网络</a></li><li><ul><li><a href="#21__20" rel="nofollow">2.1 定义</a></li><li><a href="#22__25" rel="nofollow">2.2 原理</a></li><li><a href="#23__33" rel="nofollow">2.3 实现方式</a></li><li><a href="#24__42" rel="nofollow">2.4 算法公式</a></li><li><a href="#25__55" rel="nofollow">2.5 代码示例</a></li></ul> 
  </li><li><a href="#3_172" rel="nofollow">3、总结</a></li></ul> 
</div> 
<p></p> 
<h2><a id="1_1"></a>1、引言</h2> 
<p><strong>小屌丝</strong>：鱼哥，深度神经网络是不是少了一篇没写<br> <strong>小鱼</strong>：少了哪篇？<br> <img src="https://images2.imgbox.com/cd/20/xFSQHmv9_o.jpg" alt="在这里插入图片描述"></p> 
<p><strong>小屌丝</strong>：额… 就是 内个…<br> <strong>小鱼</strong>：你倒是说啊， 哪个啊？<br> <strong>小屌丝</strong>：就…就是<br> <strong>小鱼</strong>：赶紧说，别墨迹<br> <strong>小屌丝</strong>：ResNet<br> <strong>小鱼</strong>：咳， 我还以为哪篇了<br> <strong>小屌丝</strong>：那…是不是要更新啊<br> <strong>小鱼</strong>：好说好说。<br> <strong>小屌丝</strong>： 哇塞。<br> <img src="https://images2.imgbox.com/4e/c8/LKZZByO2_o.gif" alt="在这里插入图片描述"></p> 
<h2><a id="2_19"></a>2、残差网络</h2> 
<h3><a id="21__20"></a>2.1 定义</h3> 
<p>残差网络（Residual Network，简称ResNet）是由Kaiming He等人在2015年提出的一种深度神经网络架构。</p> 
<p>ResNet的核心思想是引入了“残差学习”的概念来解决深度神经网络训练中的梯度消失/梯度爆炸问题，使得网络能够通过简单地增加层数来提高准确率，而不会导致训练困难。</p> 
<h3><a id="22__25"></a>2.2 原理</h3> 
<p>残差网络的核心原理是通过残差模块来构建深层网络。</p> 
<p>在传统的神经网络中，每一层的输出是下一层的输入。</p> 
<p>而在残差网络中，每一层的输入不仅会传递给下一层，还会通过跳跃连接（skip connection）直接传递给更深的层次。</p> 
<p>这种设计允许梯度直接流过这些跳跃连接，从而缓解了深层网络中的梯度消失问题。</p> 
<h3><a id="23__33"></a>2.3 实现方式</h3> 
<p>残差网络的实现主要包括以下步骤：</p> 
<ul><li><strong>构建残差块</strong>： 
  <ul><li>每个残差块包含两个或三个卷积层，以及一个跨层的直接连接。</li><li>对于两层残差块（称为basic block），输入首先经过一个卷积层（通常为3x3卷积），然后通过ReLU激活函数，再经过另一个卷积层，最后与原始输入（通过恒等映射或1x1卷积进行维度匹配）相加。</li><li>对于三层残差块（称为bottleneck block），中间还包含一个1x1卷积层用于减少通道数，从而减少计算量。</li></ul> </li><li><strong>堆叠残差块</strong>：将多个残差块堆叠在一起，形成完整的残差网络。根据任务需求和网络深度，可以选择使用不同类型的残差块和堆叠方式。</li><li><strong>添加其他组件</strong>：在残差网络的开头和结尾，可以添加其他组件，如卷积层、池化层、全连接层等，以满足特定的任务需求。</li></ul> 
<h3><a id="24__42"></a>2.4 算法公式</h3> 
<p>残差模块的基本形式可以用以下公式表示：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          [ 
         
        
          y 
         
        
          = 
         
        
          F 
         
        
          ( 
         
        
          x 
         
        
          , 
         
         
         
           W 
          
         
           i 
          
         
        
          ) 
         
        
          + 
         
        
          x 
         
        
          ] 
         
        
       
         [ \mathbf{y} = \mathcal{F}(\mathbf{x}, {W_i}) + \mathbf{x} ] 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord mathbf" style="margin-right: 0.016em;">y</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathcal" style="margin-right: 0.0993em;">F</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathbf">x</span><span class="mclose">]</span></span></span></span></span></span></p> 
<p>其中，</p> 
<ul><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          ( 
         
        
          x 
         
        
          ) 
         
        
       
         (\mathbf{x}) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mclose">)</span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          ( 
         
        
          y 
         
        
          ) 
         
        
       
         (\mathbf{y}) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathbf" style="margin-right: 0.016em;">y</span><span class="mclose">)</span></span></span></span></span>分别是模块的输入和输出，</li><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          ( 
         
        
          F 
         
        
          ( 
         
        
          x 
         
        
       
         (\mathcal{F}(\mathbf{x} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathcal" style="margin-right: 0.0993em;">F</span><span class="mopen">(</span><span class="mord mathbf">x</span></span></span></span></span>,<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           W 
          
         
           i 
          
         
        
          ) 
         
        
          ) 
         
        
       
         {W_i})) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span><span class="mclose">))</span></span></span></span></span>表示卷积层或全连接层的堆叠所进行的操作，</li><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          ( 
         
         
         
           W 
          
         
           i 
          
         
        
          ) 
         
        
       
         ({W_i}) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>是这些层的权重，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          ( 
         
        
          x 
         
        
          ) 
         
        
       
         (\mathbf{x}) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mclose">)</span></span></span></span></span>表示通过跳跃连接直接传递的输入。</li></ul> 
<p><img src="https://images2.imgbox.com/68/17/MUj9iNkd_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="25__55"></a>2.5 代码示例</h3> 
<pre><code class="prism language-python"><span class="token comment"># -*- coding:utf-8 -*-</span>
<span class="token comment"># @Time   : 2024-05-02</span>
<span class="token comment"># @Author : Carl_DJ</span>

<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F

<span class="token comment"># 定义残差块</span>
<span class="token keyword">class</span> <span class="token class-name">BasicBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    expansion <span class="token operator">=</span> <span class="token number">1</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> downsample<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>BasicBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 第一个卷积层</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span>
        <span class="token comment"># 第二个卷积层</span>
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels <span class="token operator">*</span> self<span class="token punctuation">.</span>expansion<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels <span class="token operator">*</span> self<span class="token punctuation">.</span>expansion<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>downsample <span class="token operator">=</span> downsample

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        identity <span class="token operator">=</span> x

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bn1<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bn2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>downsample <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            identity <span class="token operator">=</span> self<span class="token punctuation">.</span>downsample<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        out <span class="token operator">+=</span> identity
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        <span class="token keyword">return</span> out

<span class="token comment"># 构建ResNet模型</span>
<span class="token keyword">class</span> <span class="token class-name">ResNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> block<span class="token punctuation">,</span> layers<span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ResNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>in_channels <span class="token operator">=</span> <span class="token number">64</span>
        <span class="token comment"># 初始卷积层</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>in_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>in_channels<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>maxpool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 残差层</span>
        self<span class="token punctuation">.</span>layer1 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span>block<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> layers<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer2 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span>block<span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> layers<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer3 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span>block<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> layers<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer4 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span>block<span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> layers<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>avgpool <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span> <span class="token operator">*</span> block<span class="token punctuation">.</span>expansion<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_make_layer</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> block<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> blocks<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        downsample <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token keyword">if</span> stride <span class="token operator">!=</span> <span class="token number">1</span> <span class="token keyword">or</span> self<span class="token punctuation">.</span>in_channels <span class="token operator">!=</span> out_channels <span class="token operator">*</span> block<span class="token punctuation">.</span>expansion<span class="token punctuation">:</span>
            downsample <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>in_channels<span class="token punctuation">,</span> out_channels <span class="token operator">*</span> block<span class="token punctuation">.</span>expansion<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels <span class="token operator">*</span> block<span class="token punctuation">.</span>expansion<span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">)</span>

        layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>block<span class="token punctuation">(</span>self<span class="token punctuation">.</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> stride<span class="token punctuation">,</span> downsample<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>in_channels <span class="token operator">=</span> out_channels <span class="token operator">*</span> block<span class="token punctuation">.</span>expansion
        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> blocks<span class="token punctuation">)</span><span class="token punctuation">:</span>
            layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>block<span class="token punctuation">(</span>self<span class="token punctuation">.</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>bn1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>maxpool<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>avgpool<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">return</span> x

<span class="token comment"># 实例化ResNet模型并应用于输入数据</span>
<span class="token keyword">def</span> <span class="token function">resnet18</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> ResNet<span class="token punctuation">(</span>BasicBlock<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

model <span class="token operator">=</span> resnet18<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span>

<span class="token comment"># 假设我们有一批大小为4的输入数据</span>
input_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> model<span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>


</code></pre> 
<p><strong>代码解析：</strong></p> 
<ul><li>首先，定义了BasicBlock类，它是构成ResNet的基本残差块。</li><li>然后，定义了ResNet类，它通过重复堆叠BasicBlock来构建整个网络。 
  <ul><li>在ResNet类中，_make_layer方法用于生成每个残差层。</li><li>resnet18函数通过特定的配置来实例化一个18层深的ResNet模型。</li></ul> </li><li>最后，代码实例化了一个模型，并对一个随机生成的输入张量进行了一次前向传播，以展示模型的使用方式。</li></ul> 
<h2><a id="3_172"></a>3、总结</h2> 
<p>残差网络（ResNet）通过引入跳跃连接解决了深度神经网络训练中的关键问题，使得网络能够在不增加额外参数和计算复杂度的情况下增加深度，显著提高了深度学习模型的性能。</p> 
<p>ResNet的成功推动了深度学习在诸多领域的应用，如图像分类、物体检测和语义分割等。</p> 
<p>我是<a href="https://blog.csdn.net/wuyoudeyuer?type=blog"><strong>小鱼</strong></a>：</p> 
<ul><li><strong>CSDN 博客专家</strong>；</li><li><strong>阿里云 专家博主</strong>；</li><li><strong>51CTO博客专家</strong>；</li><li><strong>企业认证金牌面试官</strong>；</li><li><strong>多个名企认证&amp;特邀讲师等</strong>；</li><li><strong>名企签约职场面试培训、职场规划师</strong>；</li><li><strong>多个国内主流技术社区的认证专家博主</strong>；</li><li><strong>多款主流产品(阿里云等)测评一、二等奖获得者</strong>；</li></ul> 
<p>关注<strong>小鱼</strong>，学习<a href="https://blog.csdn.net/wuyoudeyuer/category_12468165.html"><strong>【机器学习】&amp;【深度学习】</strong></a>领域的知识。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/08bfea7f133a924d064394b94de2fddf/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Hadoop学习心得</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c123dca5db70a55f25dc66561b191aff/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">下一代自动化，国外厂商如何通过生成性AI重塑RPA？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>