<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Stable Diffusion 老照片修复&#43;高清化&#43;一键抠图教程 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/30261f23c9bdbab1b6ecdc500398cdc3/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Stable Diffusion 老照片修复&#43;高清化&#43;一键抠图教程">
  <meta property="og:description" content="大家有没有一些老旧，珍藏的照片因为岁月的侵蚀变得模糊而感到惋惜？有没有在做SD的时候生成的图片清晰度不够，过于模糊？但是不知道如何把它变得更清晰呢？
如果有，那你就来对了，本期课程就是教大家如何用SD的功能把照片放大变得超清，同时还教大家如何把破旧的老照片变得不再模糊。
好，马上开干！
我们进入到stable页面之后，今天就要讲高清化，我今天讲的分别是extra高清化和stable SR脚本。我们先讲extra高清化吧！点击图生图右边的高清化extras按钮，有的版本也把高清化翻译成附加功能或者后期处理，其实功能都一样，我们先看这一行，有单张图像，批量处理，批量处理目录下图像。是不是跟上节课讲的图生图的非常像？单张图像只能处理一张照片，我们不能框选好多图片。我们只能一张一张点，假如说我点CTRL多选呢？不行，只能选择一张图片。然后批量处理就是可以一次处理多张图片，就可以框选多张图片了，这就是批量处理。批量处理目录下图像，就是直接处理你文件夹内的所有图片。假如说我想把测试图文件夹全部复制到里边去，shift&#43;鼠标右建测试图文件夹，点击复制文件地址粘贴进去，之后把双报号删掉，输出目录路径，就是你在电脑里边新建一个新的空文件夹，必须是空文件夹！一定是以英文作为名字，把这个新文件夹路径复制到输出栏就可以批量处理了。我们今天先以单张图像去作为例子，下面这个缩放比例，原图是512×512，如果缩放比例调成2的话，输出图片的像素就会变成1024 1024的尺寸如果调到4的话是多少？4096×4096对不对？这是缩放比例。那么指定分辨率缩放是干嘛的？就是说本来我原图是512×512的，我不想把它变成等比例缩放，我想让它变成比如说1328*1104，原图是512×512正方形，做完之后是长方形。就在这里调整你要值，当你输入的图片的指定分辨率时，一定要勾选裁剪，不勾选整体的图片就是会成为一个拉伸的效果，就是硬拉伸的图片，所以一定要勾选裁剪以适应宽高比。今天我们就以等比缩放作为例子给大家展示。在讲解下面的obscu放大算法模型之前呢，我想先让大家下载一个新的模型算法，叫做4x-UltraSharp.pth,这个就是一个万能的放大模型，用它就对了！
后边我再给大家讲怎么安装。首先我们要先到这篇图文底部点开我的下载网盘地址，点击这个文件4x-UltraSharp.pth，点击下载就OK。下载完成之后呢，放到我们的sty v u根目录之后呢，我们点击models，然后点开ESRGAN里边，然后右键粘贴，或者直接把它拉进来，就完成了，
好，下面就是Upscaler1,Upscaler2，这两个就是放大算法的模型一打开之后发现这么多模型？我应该选哪个？根本不用担心啊，因为这里边有一个模型可以战胜所有其他模型，无脑用那一个4x-UltraSharp.pth就行，你只要知道在什么情况下用哪种算法就好。简单讲一下，前两个Lanczos和Nearest都是很传统的数学运算缩放算法，这两种算法几乎不会把你的图片变高净化，感觉像变了，但是没有完全变，我们以Lanczos举个例子，我们看一下原图，打开现在是这个样子的，然后再看一下对比图，两张的效果其实差不多，
我一会给大家解释这个Upscaler2是干嘛的，下一个EST差不多都是一样的，我就不给大家测试了，下一个4x-UltraSharp，这个就是我说的无脑用它就行，在放大图片的时候，你选它准没错，它放大任何图片都是又快，细节保留的又多，而且能让人变得更真实，不会出现那种磨皮过度的效果，我点击放大比例4倍，大家看一下效果，在这里放大看是不是非常的清晰？然后整体的皮肤的一些表面组织啊，其实它还是有保留的，虽然和真实人类的皮肤表层并不是那么像，但是它把原本应该留在皮肤上的一些噪点给保留下来了，这就是sharp的一个效果，是不是非常清晰，
其实我们自己可以做对比的。不知道他们有什么功能，我们就一个一个试就好了，大家也可以用之前课程讲过的X y z plot这个脚本来试，这比一个一个给演示要直观，下面这几个我就不讲了，我只讲这个ANEIME6B Anime就是动漫的意思，当我们想要放大动漫图片的时候，就用这个，其他一律都不要用，动漫就用它就好了。
所以当我们在对比所有的模型之后，哪个最好用？是不是就是这个呀？4x-UltraSharp做真人放大无脑用就行。动漫用ANEIME6BOK，大家知道这个就好。下面这Upscaler2什么时候去运用呢？因为在al sharp出现之前呢，好多其他的算法会出现磨皮过重的感觉，我们就要想办法给它做一个权衡，就是当它磨皮过重的时候，给它加一个去噪能力很差的一个算法，比如说第一和第二这两个其中一个，然后把它权衡一下，然后这个UPSCULE2可见度就是使用UPSCULE2模型在渲染时候的总占比，假如这里是0.3，就是30%的leg，然后剩下70%是alltra sharpop的模型运算，但是现在你有UltraSharp了，你就不需要去用它了，因为UltraSharp已经把好多的缺点给过滤掉了。下面讲一下这个GFPGAN面部修复程度，这个是专门修复面部损失的，当我们在修改面部模糊的时候，我们就把这2个UPSCULE全部改成none，然后缩放比例，这个调整固定4就OK。下面给大家看一下它的强大之处，这张图片。模不模糊我们放大看一下这张图片?放大之后这个样子的是不是很模糊?
当我们只有头像的这个图片很模糊的时候，我们就可以用到GFPGAN了，我们给大家看一下效果啊，点击生成，我们看一下放大过后是不是整体的图片变得很清晰了，面部的一些细节整体的样子是不是表现出来了，这就是GFPGAN面部修复的一个效果，但是它只可以把面部的细节变清晰，我们举个例子啊，假如说这张图片整体也是很模糊啊，但是它出来图片只有面部会变清晰，因为它会自动识别你的面部信晰，下面该模糊还是模糊，它是不会管你的，只会把你的面部细节保留变得更清晰。下面这个CodeFormer面部重建程度还是用那张照片吧，它跟GFPGAN都是修复面部呢，但是你在用CodeFormer的时候，它会把你面部的一些特征做改变，我们看一下把它调成1，我们看一下这边，左边这个眼睛是和原图会有一些差别的，我们看一下对比，这个是GFPGAN做出来的眼睛是这个样子，这个是Co former做出来的，大家觉得哪个更像原图啊？我是觉得第一个GFPGAN是更像原图的，OK,这两个可以同时开启，然后这个也是一个权重，面部修复权重为0的时候，它的效果是最大，1的时候它的效果最小，当你这两个同时用的时候，就可以用这个，
我们看一下这个面部修复程度，其实它就是在用我们这个面部修复用的一个算法，那么在哪里看呢？到设置里边有一个面部修复，你可以选择Co former或者GFPGAN，那这里它用的这个former weight是0.5，当你在做面部修复的时候其实建议大家还是选GFPGAN，因为如果你调成这个Co form模式，你会发现本来原图的面部细节会有所更改，这就是为什么它翻译的时候叫做面部重建程度，下面我讲一下这个remove background，大家看看自己的里面有没有这个插件?如果没有我教大家怎么安装，我把课程上所有插件的下载地址都放到本课程的最后面，大家自己去下载。这个插件叫做future web UI re BG,就叫做remove background,我们把网址复制到从网址安装，然后点击安装就OK了，然后安装好后点击应用并重启web UI完成后我们回到高清化，这个REMBG是干嘛的？这个是非常强大的抠图功能，打开这里边有这么多算法，我们看一下官方怎么解释这些模型呢？OK,第一个U2NET，它就是一个预训练模型，For general use cases,就是大部分的图片都是可以用这个算法。第二个U2NETp,我试了几次，它在抠图的时候精细程度没有第一个好。第三个*u2net_human_seg就是在我们做人类分割的时候用这个模型比较好，这个U2net cloth seg是非常强大的，当我们在做模特换装的时候用的比较多，它就会自动把人物主体的上衣和下衣直接给抠出来。我一会儿给大家看一个例子，然后这个salutta c这个就和U2NET很像啊，那下面这个is enemy,这个是专门给动漫抠图的，我们在想抠动漫图的时候就用它就OK了，Sam这个是我们模型里面没有的，因为它呢是另外一个插件，后面给大家讲，那个也是很强大的，我们先做例子，就先拿这个U2NET做例子，GFPGAN这些我们可以不调，先做个测试，我们假如说用这张用U2NET做测试，大概也就两秒钟的时间，他就把整张图片给抠好了，我们看下效果整体还是相当不错的，我们把它换成白底看一下，就会发现边缘有黑边，黑边怎么调整？勾选用Alpha ma调整就可以了，我们再看一下return mask return MAS就是把它变成蒙版，是不是就是我上节课讲的局部重绘的那样子？做的蒙版就直接给抠出来了，如果你不会用PS把图片抠出来，就用这个。我们看一下这个Alpha里面的数值啊，ErodeSize翻译就叫做腐蚀尺寸，就是保留主体的边缘留下的一些像素，它这个可以避免边缘太生硬的一个效果啊，因为有的时候我们在抠图的时候，边缘是直的，非常生硬，在我们做重绘的时候就效果不是很好，就是类似于边缘模糊，数值越高，图片的边缘范围保留的细节就会越多，但是不建议开太高啊，越低越好，因为我们不想让图片保留细节太多，假如说我们把它调成40，出来的图周围的一些背景保留的非常多，整体效果就不是非常好，但是它把你的黑边给去掉了，虽然是没有黑边，但是这个抠图整体还需要调整，怎么去做调整呢？下面Foreground 和background这两个干嘛的？Foreground 就是保留主体的前景预值，前景预值越高，去除前景的元素就越多，背景预值越高，去除背景的元素就越多，但是调太高时主体的元素也会被抠掉。在我反复试验几个小时左右后，终于研究出了一个相当完美的预设，就是模型就用这个，然后e size调成6，然后Foreground 调成143 background调成187。我试了几张图片啊，整体是没有黑边，并且抠图效果比较不错，但是每张图片的测试都是不一样的，当大家在抠图的时候发现效果不好，或者黑边很严重，试一下我这个预设，这可是我花很长时间试验出来的一个不错的预设，我们看一下整体的人物是不是没有黑边，并且背景也几乎是没有保留的，整体是一个很完美，整体的效果是最不错了，所以大家可以试一下我这个预设啊，下面还有一个UN cloth c,看一下效果，它能把衣服给抠掉。我们看一下是不是把上衣还有裤子全部给抠掉了，这个什么时候用比较多呢？一般是用这些mask直接变成蒙版，只不过这个下边边缘是有一点瑕疵，还有一个缺点就是不知道为什么衣服这两张图片隔的那么远啊，当使用这种方式抠衣服蒙版的时候，我们还是需要去后期PS或者其他软件里边调整一下才行。有一点非常重要的事情要跟大家讲一下，如果你是用启动器打开了SD，安装了re BG这个插件，在你第一次生成的时候大概率会报错，报什么错呢？会报一个类似于这样runtime error,然后是provider library get on n n X runtime error,我用的时候也是报这个错了，我研究了一晚上，最后终于知道原因了，必须是在你的web UI这个文件夹，然后后边再输入一个斜杠，输入PY310，然后回车，回车之后进入到这个文件夹，然后你输入CMD，打开终端口，把这串命令输入上去，然后点击回车，回车之后如果你看到这个页面，就代表你安装成功。那完成之后，你把web UI重启一下，然后再用这个remove background就可以了，这个真的是研究了一晚上才研究明白，如果你也遇见了同样的问题，同时在我的帮助下解决了，请一定点赞转发。最后再顺便给大家讲一下图像信息，就是在我们做文声图或者图生图的时候，当我们只保留了图片，但是忘记了这些参数和关键词的时候，我们用这个图像信息比较不错，怎么用呢？假如说我们用一个图生图的作为例子啊，到图生图文件夹用一张照片，回到图像信息图片，我们把它拖过来，然后右边就会直接生成这张图的一些关键词，像正向提示词，反向提示词，然后你的采样部署，采样器，C fg skill，还有一些种子等细节都会保留在这里。OK,这就是我今天给大家讲的内容，如何通过SD把你的模糊图片变得高清。如果觉得我的教程对你非常有帮助，希望给我点赞转发，大家关注我，轻松掌控AI！
写在最后 AIGC技术的未来发展前景广阔，随着人工智能技术的不断发展，AIGC技术也将不断提高。未来，AIGC技术将在游戏和计算领域得到更广泛的应用，使游戏和计算系统具有更高效、更智能、更灵活的特性。同时，AIGC技术也将与人工智能技术紧密结合，在更多的领域得到广泛应用，对程序员来说影响至关重要。未来，AIGC技术将继续得到提高，同时也将与人工智能技术紧密结合，在更多的领域得到广泛应用。
感兴趣的小伙伴，赠送全套AIGC学习资料和安装工具，包含AI绘画、AI人工智能等前沿科技教程，模型插件，具体看下方。
一、AIGC所有方向的学习路线">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-09T09:50:52+08:00">
    <meta property="article:modified_time" content="2024-05-09T09:50:52+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Stable Diffusion 老照片修复&#43;高清化&#43;一键抠图教程</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>大家有没有一些老旧，珍藏的照片因为岁月的侵蚀变得模糊而感到惋惜？有没有在做SD的时候生成的图片清晰度不够，过于模糊？但是不知道如何把它变得更清晰呢？</p> 
<p>如果有，那你就来对了，本期课程就是教大家如何用SD的功能把照片放大变得超清，同时还教大家如何把破旧的老照片变得不再模糊。</p> 
<p>好，马上开干！</p> 
<p>我们进入到stable页面之后，今天就要讲高清化，我今天讲的分别是extra高清化和stable SR脚本。我们先讲extra高清化吧！<img src="https://images2.imgbox.com/cb/98/bKAXvLTW_o.png" alt="">点击图生图右边的高清化extras按钮，有的版本也把高清化翻译成附加功能或者后期处理，其实功能都一样，我们先看这一行，有单张图像，批量处理，批量处理目录下图像。是不是跟上节课讲的图生图的非常像？单张图像只能处理一张照片，我们不能框选好多图片。我们只能一张一张点，假如说我点CTRL多选呢？不行，只能选择一张图片。然后批量处理就是可以一次处理多张图片，就可以框选多张图片了，这就是批量处理。<img src="https://images2.imgbox.com/56/41/jW3dKRty_o.png" alt="">批量处理目录下图像，就是直接处理你文件夹内的所有图片。<img src="https://images2.imgbox.com/b0/6f/p6J3UCFO_o.png" alt="">假如说我想把测试图文件夹全部复制到里边去，shift+鼠标右建测试图文件夹，点击复制文件地址粘贴进去，之后把双报号删掉，输出目录路径，就是你在电脑里边新建一个新的空文件夹，必须是空文件夹！一定是以英文作为名字，把这个新文件夹路径复制到输出栏就可以批量处理了。<img src="https://images2.imgbox.com/2c/1e/AjpCt6Ik_o.png" alt="">我们今天先以单张图像去作为例子，下面这个缩放比例，原图是512×512，如果缩放比例调成2的话，输出图片的像素就会变成1024 1024的尺寸如果调到4的话是多少？4096×4096对不对？这是缩放比例。<img src="https://images2.imgbox.com/a4/e7/IVXYlbN9_o.png" alt="">那么指定分辨率缩放是干嘛的？就是说本来我原图是512×512的，我不想把它变成等比例缩放，我想让它变成比如说1328*1104，原图是512×512正方形，做完之后是长方形。就在这里调整你要值，当你输入的图片的指定分辨率时，一定要勾选裁剪，不勾选整体的图片就是会成为一个拉伸的效果，就是硬拉伸的图片，所以一定要勾选裁剪以适应宽高比。<img src="https://images2.imgbox.com/b4/0a/8TPQ1xD4_o.png" alt="">今天我们就以等比缩放作为例子给大家展示。在讲解下面的obscu放大算法模型之前呢，我想先让大家下载一个新的模型算法，叫做4x-UltraSharp.pth,这个就是一个万能的放大模型，用它就对了！<img src="https://images2.imgbox.com/6d/0d/9hKbv4GG_o.png" alt=""></p> 
<p>后边我再给大家讲怎么安装。首先我们要先到这篇图文底部点开我的下载网盘地址，点击这个文件4x-UltraSharp.pth，点击下载就OK。<img src="https://images2.imgbox.com/f8/c0/AbR4hLbd_o.png" alt="">下载完成之后呢，放到我们的sty v u根目录之后呢，我们点击models，然后点开ESRGAN里边，然后右键粘贴，或者直接把它拉进来，就完成了，</p> 
<p><img src="https://images2.imgbox.com/3c/44/yaSYKx7p_o.png" alt=""></p> 
<p>好，下面就是Upscaler1,Upscaler2，这两个就是放大算法的模型一打开之后发现这么多模型？我应该选哪个？根本不用担心啊，因为这里边有一个模型可以战胜所有其他模型，无脑用那一个4x-UltraSharp.pth就行，你只要知道在什么情况下用哪种算法就好。<img src="https://images2.imgbox.com/c3/74/sEzfoo91_o.png" alt="">简单讲一下，前两个Lanczos和Nearest都是很传统的数学运算缩放算法，这两种算法几乎不会把你的图片变高净化，感觉像变了，但是没有完全变，我们以Lanczos举个例子，我们看一下原图，打开现在是这个样子的，然后再看一下对比图，两张的效果其实差不多，<img src="https://images2.imgbox.com/6e/d6/U0UgKqU7_o.png" alt=""></p> 
<p>我一会给大家解释这个Upscaler2是干嘛的，下一个EST差不多都是一样的，我就不给大家测试了，下一个4x-UltraSharp，这个就是我说的无脑用它就行，在放大图片的时候，你选它准没错，它放大任何图片都是又快，细节保留的又多，而且能让人变得更真实，不会出现那种磨皮过度的效果，我点击放大比例4倍，大家看一下效果，在这里放大看是不是非常的清晰？然后整体的皮肤的一些表面组织啊，其实它还是有保留的，虽然和真实人类的皮肤表层并不是那么像，但是它把原本应该留在皮肤上的一些噪点给保留下来了，这就是sharp的一个效果，是不是非常清晰，</p> 
<p><img src="https://images2.imgbox.com/f0/8b/Oldd2JGc_o.png" alt="">其实我们自己可以做对比的。不知道他们有什么功能，我们就一个一个试就好了，大家也可以用之前课程讲过的X y z plot这个脚本来试，这比一个一个给演示要直观，下面这几个我就不讲了，我只讲这个ANEIME6B Anime就是动漫的意思，当我们想要放大动漫图片的时候，就用这个，其他一律都不要用，动漫就用它就好了。<img src="https://images2.imgbox.com/e1/42/vdUSWbdK_o.png" alt=""></p> 
<p>所以当我们在对比所有的模型之后，哪个最好用？是不是就是这个呀？4x-UltraSharp做真人放大无脑用就行。动漫用ANEIME6BOK，大家知道这个就好。下面这Upscaler2什么时候去运用呢？因为在al sharp出现之前呢，好多其他的算法会出现磨皮过重的感觉，我们就要想办法给它做一个权衡，就是当它磨皮过重的时候，给它加一个去噪能力很差的一个算法，比如说第一和第二这两个其中一个，然后把它权衡一下，然后这个UPSCULE2可见度就是使用UPSCULE2模型在渲染时候的总占比，假如这里是0.3，就是30%的leg，然后剩下70%是alltra sharpop的模型运算，<img src="https://images2.imgbox.com/67/23/aGnsLadg_o.png" alt="">但是现在你有UltraSharp了，你就不需要去用它了，因为UltraSharp已经把好多的缺点给过滤掉了。下面讲一下这个GFPGAN面部修复程度，<img src="https://images2.imgbox.com/32/50/GR1tsC3L_o.png" alt="">这个是专门修复面部损失的，当我们在修改面部模糊的时候，我们就把这2个UPSCULE全部改成none，然后缩放比例，这个调整固定4就OK。下面给大家看一下它的强大之处，这张图片。模不模糊我们放大看一下这张图片?放大之后这个样子的是不是很模糊?</p> 
<p><img src="https://images2.imgbox.com/15/02/RFEF5cJc_o.png" alt=""></p> 
<p>当我们只有头像的这个图片很模糊的时候，我们就可以用到GFPGAN了，我们给大家看一下效果啊，点击生成，我们看一下放大过后是不是整体的图片变得很清晰了，面部的一些细节整体的样子是不是表现出来了，这就是GFPGAN面部修复的一个效果，<img src="https://images2.imgbox.com/e8/b7/B2fJx6IN_o.png" alt="">但是它只可以把面部的细节变清晰，我们举个例子啊，假如说这张图片整体也是很模糊啊，但是它出来图片只有面部会变清晰，因为它会自动识别你的面部信晰，下面该模糊还是模糊，它是不会管你的，只会把你的面部细节保留变得更清晰。<img src="https://images2.imgbox.com/f8/d2/pMC2lmru_o.png" alt="">下面这个CodeFormer面部重建程度还是用那张照片吧，它跟GFPGAN都是修复面部呢，但是你在用CodeFormer的时候，它会把你面部的一些特征做改变，我们看一下把它调成1，我们看一下这边，左边这个眼睛是和原图会有一些差别的，我们看一下对比，这个是GFPGAN做出来的眼睛是这个样子，这个是Co former做出来的，大家觉得哪个更像原图啊？我是觉得第一个GFPGAN是更像原图的，OK,这两个可以同时开启，然后这个也是一个权重，面部修复权重为0的时候，它的效果是最大，1的时候它的效果最小，当你这两个同时用的时候，就可以用这个，<img src="https://images2.imgbox.com/bf/cc/MFV2qqGh_o.png" alt=""></p> 
<p><img src="https://images2.imgbox.com/54/75/uDxhPNA1_o.png" alt="">我们看一下这个面部修复程度，其实它就是在用我们这个面部修复用的一个算法，那么在哪里看呢？到设置里边有一个面部修复，你可以选择Co former或者GFPGAN，那这里它用的这个former weight是0.5，当你在做面部修复的时候其实建议大家还是选GFPGAN，因为如果你调成这个Co form模式，你会发现本来原图的面部细节会有所更改，这就是为什么它翻译的时候叫做面部重建程度，<img src="https://images2.imgbox.com/85/d8/UQMHi96G_o.png" alt="">下面我讲一下这个remove background，大家看看自己的里面有没有这个插件?如果没有我教大家怎么安装，我把课程上所有插件的下载地址都放到本课程的最后面，大家自己去下载。这个插件叫做future web UI re BG,就叫做remove background,我们把网址复制到从网址安装，然后点击安装就OK了，<img src="https://images2.imgbox.com/30/66/jL87Uzo7_o.png" alt=""><img src="https://images2.imgbox.com/a6/a8/9X0ejJf2_o.png" alt="">然后安装好后点击应用并重启web UI完成后我们回到高清化，这个REMBG是干嘛的？这个是非常强大的抠图功能，<img src="https://images2.imgbox.com/4e/4a/GucUoxAF_o.png" alt="">打开这里边有这么多算法，我们看一下官方怎么解释这些模型呢？OK,第一个U2NET，它就是一个预训练模型，For general use cases,就是大部分的图片都是可以用这个算法。第二个U2NETp,我试了几次，它在抠图的时候精细程度没有第一个好。第三个*u2net_human_seg就是在我们做人类分割的时候用这个模型比较好，这个U2net cloth seg是非常强大的，当我们在做模特换装的时候用的比较多，它就会自动把人物主体的上衣和下衣直接给抠出来。我一会儿给大家看一个例子，然后这个salutta c这个就和U2NET很像啊，那下面这个is enemy,这个是专门给动漫抠图的，我们在想抠动漫图的时候就用它就OK了，<img src="https://images2.imgbox.com/58/8a/OUeoHD2b_o.png" alt="">Sam这个是我们模型里面没有的，因为它呢是另外一个插件，后面给大家讲，那个也是很强大的，我们先做例子，就先拿这个U2NET做例子，GFPGAN这些我们可以不调，先做个测试，我们假如说用这张用U2NET做测试，大概也就两秒钟的时间，他就把整张图片给抠好了，<img src="https://images2.imgbox.com/e3/c9/1pxUkGYd_o.png" alt="">我们看下效果整体还是相当不错的，我们把它换成白底看一下，就会发现边缘有黑边，黑边怎么调整？勾选用Alpha ma调整就可以了，<img src="https://images2.imgbox.com/4f/57/DI3J3ApS_o.png" alt="">我们再看一下return mask return MAS就是把它变成蒙版，<img src="https://images2.imgbox.com/5f/86/oUTs3ViW_o.png" alt="">是不是就是我上节课讲的局部重绘的那样子？做的蒙版就直接给抠出来了，如果你不会用PS把图片抠出来，就用这个。我们看一下这个Alpha里面的数值啊，ErodeSize翻译就叫做腐蚀尺寸，就是保留主体的边缘留下的一些像素，它这个可以避免边缘太生硬的一个效果啊，因为有的时候我们在抠图的时候，边缘是直的，非常生硬，在我们做重绘的时候就效果不是很好，就是类似于边缘模糊，数值越高，图片的边缘范围保留的细节就会越多，但是不建议开太高啊，越低越好，因为我们不想让图片保留细节太多，假如说我们把它调成40，出来的图周围的一些背景保留的非常多，整体效果就不是非常好，但是它把你的黑边给去掉了，虽然是没有黑边，但是这个抠图整体还需要调整，<img src="https://images2.imgbox.com/a6/b2/TjaeO2yY_o.png" alt="">怎么去做调整呢？下面Foreground 和background这两个干嘛的？Foreground 就是保留主体的前景预值，前景预值越高，去除前景的元素就越多，背景预值越高，去除背景的元素就越多，但是调太高时主体的元素也会被抠掉。在我反复试验几个小时左右后，终于研究出了一个相当完美的预设，就是模型就用这个，然后e size调成6，然后Foreground 调成143 background调成187。我试了几张图片啊，整体是没有黑边，并且抠图效果比较不错，但是每张图片的测试都是不一样的，当大家在抠图的时候发现效果不好，或者黑边很严重，试一下我这个预设，这可是我花很长时间试验出来的一个不错的预设，我们看一下整体的人物是不是没有黑边，并且背景也几乎是没有保留的，整体是一个很完美，整体的效果是最不错了，所以大家可以试一下我这个预设啊，<img src="https://images2.imgbox.com/7d/65/6IzENSr3_o.png" alt=""><img src="https://images2.imgbox.com/56/eb/hixWO3QC_o.png" alt="">下面还有一个UN cloth c,看一下效果，它能把衣服给抠掉。我们看一下是不是把上衣还有裤子全部给抠掉了，这个什么时候用比较多呢？一般是用这些mask直接变成蒙版，只不过这个下边边缘是有一点瑕疵，还有一个缺点就是不知道为什么衣服这两张图片隔的那么远啊，当使用这种方式抠衣服蒙版的时候，我们还是需要去后期PS或者其他软件里边调整一下才行。<img src="https://images2.imgbox.com/9d/5d/TwoLYCFA_o.png" alt="">有一点非常重要的事情要跟大家讲一下，如果你是用启动器打开了SD，安装了re BG这个插件，在你第一次生成的时候大概率会报错，报什么错呢？会报一个类似于这样runtime error,然后是provider library get on n n X runtime error,我用的时候也是报这个错了，<img src="https://images2.imgbox.com/70/19/B1H343KB_o.png" alt="">我研究了一晚上，最后终于知道原因了，必须是在你的web UI这个文件夹，然后后边再输入一个斜杠，输入PY310，然后回车，<img src="https://images2.imgbox.com/27/ce/nDyaGQM3_o.png" alt="">回车之后进入到这个文件夹，然后你输入CMD，打开终端口，把这串命令输入上去，然后点击回车，<img src="https://images2.imgbox.com/e6/22/I1QDumIa_o.png" alt="">回车之后如果你看到这个页面，就代表你安装成功。<img src="https://images2.imgbox.com/7b/0c/t98oxQiv_o.png" alt="">那完成之后，你把web UI重启一下，然后再用这个remove background就可以了，这个真的是研究了一晚上才研究明白，如果你也遇见了同样的问题，同时在我的帮助下解决了，请一定点赞转发。最后再顺便给大家讲一下图像信息，就是在我们做文声图或者图生图的时候，当我们只保留了图片，但是忘记了这些参数和关键词的时候，我们用这个图像信息比较不错，怎么用呢？假如说我们用一个图生图的作为例子啊，到图生图文件夹用一张照片，回到图像信息图片，我们把它拖过来，然后右边就会直接生成这张图的一些关键词，像正向提示词，反向提示词，然后你的采样部署，采样器，C fg skill，还有一些种子等细节都会保留在这里。<img src="https://images2.imgbox.com/50/19/bRvQAjSh_o.png" alt="">OK,这就是我今天给大家讲的内容，如何通过SD把你的模糊图片变得高清。如果觉得我的教程对你非常有帮助，希望给我点赞转发，大家关注我，轻松掌控AI！</p> 
<h3><a id="_29"></a>写在最后</h3> 
<p>AIGC技术的未来发展前景广阔，随着人工智能技术的不断发展，AIGC技术也将不断提高。未来，AIGC技术将在游戏和计算领域得到更广泛的应用，使游戏和计算系统具有更高效、更智能、更灵活的特性。同时，AIGC技术也将与人工智能技术紧密结合，在更多的领域得到广泛应用，对程序员来说影响至关重要。未来，AIGC技术将继续得到提高，同时也将与人工智能技术紧密结合，在更多的领域得到广泛应用。</p> 
<p><font face="幼圆" size="4" color="red">感兴趣的小伙伴，赠送全套AIGC学习资料和安装工具，包含AI绘画、AI人工智能等前沿科技教程，模型插件，具体看下方。<br> </font><br> <img src="https://images2.imgbox.com/c4/6e/qbIDCnYz_o.jpg"></p> 
<p><strong>一、AIGC所有方向的学习路线</strong></p> 
<p>AIGC所有方向的技术点做的整理，形成各个领域的知识点汇总，它的用处就在于，你可以按照下面的知识点去找对应的学习资源，保证自己学得较为全面。</p> 
<p><img src="https://images2.imgbox.com/98/bd/9qOqxgAo_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/66/f9/UNnNjqtZ_o.png" alt="在这里插入图片描述"></p> 
<p><strong>二、AIGC必备工具</strong></p> 
<p>工具都帮大家整理好了，安装就可直接上手！<br> <img src="https://images2.imgbox.com/1f/7d/8gnTosh2_o.png" alt="在这里插入图片描述"></p> 
<p><strong>三、最新AIGC学习笔记</strong></p> 
<p>当我学到一定基础，有自己的理解能力的时候，会去阅读一些前辈整理的书籍或者手写的笔记资料，这些笔记详细记载了他们对一些技术点的理解，这些理解是比较独到，可以学到不一样的思路。<br> <img src="https://images2.imgbox.com/82/ab/NeKGhkrR_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2c/2f/3wS7ifhc_o.png" alt="在这里插入图片描述"></p> 
<p><strong>四、AIGC视频教程合集</strong></p> 
<p>观看全面零基础学习视频，看视频学习是最快捷也是最有效果的方式，跟着视频中老师的思路，从基础到深入，还是很容易入门的。</p> 
<p><img src="https://images2.imgbox.com/ba/a8/dHP3hLj9_o.png" alt="在这里插入图片描述"></p> 
<p><strong>五、实战案例</strong></p> 
<p>纸上得来终觉浅，要学会跟着视频一起敲，要动手实操，才能将自己的所学运用到实际当中去，这时候可以搞点实战案例来学习。<br> <img src="https://images2.imgbox.com/da/6b/yl2wfWBv_o.png" alt="在这里插入图片描述"></p> 
<img src="https://images2.imgbox.com/8b/89/M8Fmac4Z_o.jpg"> 若有侵权，请联系删除
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/74470efb56eb7afecba7806c33203606/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">深度技术解读AlphaFold3: 谷歌第三代AI工具精准预测生物大分子四级结构</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ee08128e087eefbf4cd4eeddbd1860b3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">无法启动docker | “Job for docker.service failed because the control process exited with error code“</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>