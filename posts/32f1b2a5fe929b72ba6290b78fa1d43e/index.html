<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>《Spark 编程基础（Scala 版）》第 6 章 Spark SQL 实验 5 Spark SQL 编程初级实践 （超级详细版） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/32f1b2a5fe929b72ba6290b78fa1d43e/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="《Spark 编程基础（Scala 版）》第 6 章 Spark SQL 实验 5 Spark SQL 编程初级实践 （超级详细版）">
  <meta property="og:description" content="一、实验目的
（1）通过实验掌握 Spark SQL 的基本编程方法；
（2）熟悉 RDD 到 DataFrame 的转化方法；
（3）熟悉利用 Spark SQL 管理来自不同数据源的数据。
二、实验平台 操作系统： Ubuntu16.04 Spark 版本：2.1.0 数据库：MySQL
三、实验内容和要求
1．Spark SQL 基本操作 将下列 JSON 格式数据复制到 Linux 系统中，并保存命名为 employee.json。 { &#34;id&#34;:1 , &#34;name&#34;:&#34; Ella&#34; , &#34;age&#34;:36 } { &#34;id&#34;:2, &#34;name&#34;:&#34;Bob&#34;,&#34;age&#34;:29 } { &#34;id&#34;:3 , &#34;name&#34;:&#34;Jack&#34;,&#34;age&#34;:29 } { &#34;id&#34;:4 , &#34;name&#34;:&#34;Jim&#34;,&#34;age&#34;:28 } { &#34;id&#34;:4 , &#34;name&#34;:&#34;Jim&#34;,&#34;age&#34;:28 } { &#34;id&#34;:5 , &#34;name&#34;:&#34;Damon&#34; } { &#34;id&#34;:5 , &#34;name&#34;:&#34;Damon&#34; } 为 employee.json 创建 DataFrame，并写出 Scala 语句完成下列操作：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-22T10:07:16+08:00">
    <meta property="article:modified_time" content="2024-05-22T10:07:16+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">《Spark 编程基础（Scala 版）》第 6 章 Spark SQL 实验 5 Spark SQL 编程初级实践 （超级详细版）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>一、实验目的</p> 
<p>（1）通过实验掌握 Spark SQL 的基本编程方法；</p> 
<p>（2）熟悉 RDD 到 DataFrame 的转化方法；</p> 
<p>（3）熟悉利用 Spark SQL 管理来自不同数据源的数据。</p> 
<p>二、实验平台 操作系统： Ubuntu16.04 Spark 版本：2.1.0 数据库：MySQL</p> 
<p>三、实验内容和要求</p> 
<p>1．Spark SQL 基本操作 将下列 JSON 格式数据复制到 Linux 系统中，并保存命名为 employee.json。 { "id":1 , "name":" Ella" , "age":36 } { "id":2, "name":"Bob","age":29 } { "id":3 , "name":"Jack","age":29 } { "id":4 , "name":"Jim","age":28 } { "id":4 , "name":"Jim","age":28 } { "id":5 , "name":"Damon" } { "id":5 , "name":"Damon" } 为 employee.json 创建 DataFrame，并写出 Scala 语句完成下列操作：</p> 
<p>(1) 查询所有数据；</p> 
<p> (2) 查询所有数据，并去除重复的数据；</p> 
<p>(3) 查询所有数据，打印时去除 id 字段；</p> 
<p>(4) 筛选出 age&gt;30 的记录；</p> 
<p>(5) 将数据按 age 分组；</p> 
<p>(6) 将数据按 name 升序排列；</p> 
<p>(7) 取出前 3 行数据；</p> 
<p>(8) 查询所有记录的 name 列，并为其取别名为 username；</p> 
<p>(9) 查询年龄 age 的平均值；</p> 
<p>(10) 查询年龄 age 第 2 页 的最小值。</p> 
<p>1.再MobaXter里</p> 
<p>cd`/home/hadoop</p> 
<p>创建</p> 
<p>vim employee.json`</p> 
<p>把数据粘上去</p> 
<p><img alt="" height="182" src="https://images2.imgbox.com/4b/a3/ZTYWt1nm_o.png" width="737"></p> 
<p><img alt="" height="241" src="https://images2.imgbox.com/ab/c1/eQT73quC_o.png" width="280"></p> 
<p>2.再IDEA上</p> 
<p>创建</p> 
<pre>case object SparkSQLjibencaozuo</pre> 
<p><img alt="" height="214" src="https://images2.imgbox.com/5d/ec/sB7wpF5v_o.png" width="385"></p> 
<p>里面代码</p> 
<pre>package edu.hnuahe.wanqing.spark_6_3_CreateDataFrame
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
case object SparkSQLjibencaozuo{
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder.appName("EmployeeData").getOrCreate()
    import spark.implicits._

    // 加载 JSON 数据到 DataFrame
    //val employeeDF = spark.read.json("employee.json")

    val employeeDF = spark.read.json("file:///home/hadoop/employee.json")

    // (1) 查询所有数据
    employeeDF.show()

    // (2) 查询所有数据，并去除重复的数据
    val distinctEmployeeDF = employeeDF.distinct()
    distinctEmployeeDF.show()

    // (3) 查询所有数据，打印时去除 id 字段
    employeeDF.select("name", "age").show()

    // (4) 筛选出 age&gt;30 的记录
    employeeDF.filter($"age" &gt; 30).show()

    // (5) 将数据按 age 分组
    employeeDF.groupBy("age").count().show()

    // (6) 将数据按 name 升序排列
    employeeDF.orderBy($"name".asc).show()

    // (7) 取出前 3 行数据
    employeeDF.limit(3).show()

    // (8) 查询所有记录的 name 列，并为其取别名为 username
    employeeDF.select($"name".as("username")).show()

    // (9) 查询年龄 age 的平均值
    employeeDF.select(avg($"age")).show()

    // (10) 查询年龄 age 的最小值
    employeeDF.select(min($"age")).show()

    // 停止 Spark 会话
    spark.stop()

  }
}
</pre> 
<pre><code class="language-Scala">package edu.hnuahe.wanqing.spark_6_3_CreateDataFrame
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
case object SparkSQLjibencaozuo{
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder.appName("EmployeeData").getOrCreate()
    import spark.implicits._

    // 加载 JSON 数据到 DataFrame
    //val employeeDF = spark.read.json("employee.json")

    val employeeDF = spark.read.json("file:///home/hadoop/employee.json")

    // (1) 查询所有数据
    employeeDF.show()

    // (2) 查询所有数据，并去除重复的数据
    val distinctEmployeeDF = employeeDF.distinct()
    distinctEmployeeDF.show()

    // (3) 查询所有数据，打印时去除 id 字段
    employeeDF.select("name", "age").show()

    // (4) 筛选出 age&gt;30 的记录
    employeeDF.filter($"age" &gt; 30).show()

    // (5) 将数据按 age 分组
    employeeDF.groupBy("age").count().show()

    // (6) 将数据按 name 升序排列
    employeeDF.orderBy($"name".asc).show()

    // (7) 取出前 3 行数据
    employeeDF.limit(3).show()

    // (8) 查询所有记录的 name 列，并为其取别名为 username
    employeeDF.select($"name".as("username")).show()

    // (9) 查询年龄 age 的平均值
    employeeDF.select(avg($"age")).show()

    // (10) 查询年龄 age 的最小值
    employeeDF.select(min($"age")).show()

    // 停止 Spark 会话
    spark.stop()

  }
}
</code></pre> 
<p>注意一下</p> 
<p>```<br>     // 加载 JSON 数据到 DataFrame<br>     //val employeeDF = spark.read.json("employee.json")</p> 
<p>    val employeeDF = spark.read.json("file:///home/hadoop/employee.json")<br> ```</p> 
<p>记好employee.json文件在哪个目录下</p> 
<p>1.运行代码</p> 
<p>有这个结果就行了，不需要在意红色<br> Process finished with exit code 1</p> 
<p>2.打包项目 build-&gt;package-&gt;git add jar-&gt;commit-&gt;push，在虚拟机中 git pull origin master 上传的 jar 包</p> 
<p><img alt="" height="141" src="https://images2.imgbox.com/0a/3c/stGzD9XJ_o.png" width="1065"></p> 
<p>出现这个结果就正确的</p> 
<p>3.以 spark-local 模式提交 spark 任务<br> 先  cd /opt/module/spark-local/<br> 再运行<br> bin/spark-submit --master local[*] --jars /opt/module/spark-local/jars/mysql-connector-java-5.1.27-bin.jar --class IDEA自己文件的地址 ~/gitdata/target/scala_demo-1.0-SNAPSHOT.jar</p> 
<p>4.10结果就出来了</p> 
<p><img alt="" height="374" src="https://images2.imgbox.com/bc/a1/8greHWCb_o.png" width="516"></p> 
<p>2．编程实现将 RDD 转换为 DataFrame 源文件内容如下（包含 id,name,age）：</p> 
<p>1,Ella,36 2,Bob,29 3,Jack,29</p> 
<p>请将数据复制保存到 Linux 系统中，命名为 employee.txt，实现从 RDD 转换得到 DataFrame，并按“id:1,name:Ella,age:36”的格式打印出 DataFrame 的所有数据。请写出程序代 码。</p> 
<p>employee.txt 如上面一样</p> 
<p>再IDEA上建立RDDToDataFrameExample</p> 
<pre>package edu.hnuahe.wanqing.spark_6_3_CreateDataFrame
import org.apache.spark.sql.{SparkSession, Row}
import org.apache.spark.sql.types._
case object RDDToDataFrameExample{
  def main(args: Array[String]): Unit = {
    // 创建SparkSession
    val spark = SparkSession.builder()
      .appName("RDD to DataFrame Example")
      .master("local[*]") // 使用本地模式，如果连接到集群请更改这里
      .getOrCreate()

    import spark.implicits._

    // 指定employee.txt文件的位置
    val inputFilePath = "file:///home/hadoop/employee.txt"

    // 从文本文件读取数据创建RDD
    val rdd = spark.sparkContext.textFile(inputFilePath)

    // 定义DataFrame的schema
    val schema = StructType(Array(
      StructField("id", IntegerType, nullable = false),
      StructField("name", StringType, nullable = false),
      StructField("age", IntegerType, nullable = false)
    ))

    // 将RDD转换为DataFrame
    val dataFrame = spark.createDataFrame(rdd.map { line =&gt;
      val parts = line.split(",")
      Row(parts(0).toInt, parts(1), parts(2).toInt)
    }, schema)

    // 显示DataFrame内容
    dataFrame.show(false)

    // 按照指定格式打印所有数据
    dataFrame.collect().foreach { row =&gt;
      println(s"id:${row.getAs[Int]("id")},name:${row.getAs[String]("name")},age:${row.getAs[Int]("age")}")
    }

    // 停止SparkSession
    spark.stop()
  }

}
</pre> 
<pre><code class="language-Scala">package edu.hnuahe.wanqing.spark_6_3_CreateDataFrame
import org.apache.spark.sql.{SparkSession, Row}
import org.apache.spark.sql.types._
case object RDDToDataFrameExample{
  def main(args: Array[String]): Unit = {
    // 创建SparkSession
    val spark = SparkSession.builder()
      .appName("RDD to DataFrame Example")
      .master("local[*]") // 使用本地模式，如果连接到集群请更改这里
      .getOrCreate()

    import spark.implicits._

    // 指定employee.txt文件的位置
    val inputFilePath = "file:///home/hadoop/employee.txt"

    // 从文本文件读取数据创建RDD
    val rdd = spark.sparkContext.textFile(inputFilePath)

    // 定义DataFrame的schema
    val schema = StructType(Array(
      StructField("id", IntegerType, nullable = false),
      StructField("name", StringType, nullable = false),
      StructField("age", IntegerType, nullable = false)
    ))

    // 将RDD转换为DataFrame
    val dataFrame = spark.createDataFrame(rdd.map { line =&gt;
      val parts = line.split(",")
      Row(parts(0).toInt, parts(1), parts(2).toInt)
    }, schema)

    // 显示DataFrame内容
    dataFrame.show(false)

    // 按照指定格式打印所有数据
    dataFrame.collect().foreach { row =&gt;
      println(s"id:${row.getAs[Int]("id")},name:${row.getAs[String]("name")},age:${row.getAs[Int]("age")}")
    }

    // 停止SparkSession
    spark.stop()
  }



}
</code></pre> 
<p><img alt="" height="436" src="https://images2.imgbox.com/0e/30/3JYZv1f1_o.png" width="774"></p> 
<p>1.建表</p> 
<p>CREATE DATABASE sparktest;  <br> USE sparktest;  <br>   <br> CREATE TABLE employee (  <br>   id INT PRIMARY KEY,  <br>   name VARCHAR(50),  <br>   gender CHAR(1),  <br>   age INT  <br> );  <br>   <br> INSERT INTO employee (id, name, gender, age) VALUES (1, 'Alice', 'F', 22);  <br> INSERT INTO employee (id, name, gender, age) VALUES (2, 'John', 'M', 25);<br> ————————————————</p> 
<p></p> 
<p>.<img alt="" height="462" src="https://images2.imgbox.com/1e/b3/mncJSk1m_o.png" width="394"></p> 
<p><img alt="" height="559" src="https://images2.imgbox.com/0b/f8/th0zEwTp_o.png" width="949"></p> 
<p>再IDEA上建立MySQLDataFrameExample</p> 
<pre>package edu.hnuahe.wanqing.spark_6_3_CreateDataFrame
import org.apache.spark.sql.{SparkSession, Row}
import java.util.Properties
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions.max
import org.apache.spark.sql.functions.sum

case object MySQLDataFrameExample{
  def main(args: Array[String]): Unit = {
    // 创建SparkSession
    val spark = SparkSession.builder()
      .appName("MySQL DataFrame Example MySQL写入与读取")
      .master("local[*]") // 使用本地模式，如果连接到集群请更改这里
      .getOrCreate()

    import spark.implicits._

    // 配置MySQL JDBC连接
    val jdbcProperties = new Properties()
    jdbcProperties.setProperty("user", "root")
<span style="color:#0d0016;"><span style="background-color:#fe2c24;">    jdbcProperties.setProperty("password", "自己的密码")</span></span>
    jdbcProperties.setProperty("driver", "com.mysql.jdbc.Driver")
    //jdbcProperties.setProperty("driver", "com.mysql.cj.jdbc.Driver")

    // 定义MySQL的JDBC连接URL
    <span style="color:#0d0016;"><span style="background-color:#fe2c24;">val jdbcUrl = "jdbc:mysql://自己的主机:3306/sparktest"</span></span>

    // 创建DataFrame以插入数据
    val newEmployeeData = Seq(
      (3, "Mary", "F", 26),
      (4, "Tom", "M", 23)
    ).toDF("id", "name", "gender", "age")

    // 将DataFrame数据插入到MySQL的employee表中
    newEmployeeData.write
      .mode("append") // 使用append模式来添加数据，而不是覆盖
      .jdbc(jdbcUrl, "employee", jdbcProperties)

    // 从MySQL读取employee表的数据
    val employeeDF = spark.read
      .jdbc(jdbcUrl, "employee", jdbcProperties)

    // 打印age的最大值
    val maxAge = employeeDF.agg(max("age")).collect()(0).getAs[Int](0)
    println(s"Max age: $maxAge")

    // 打印age的总和
    val sumAge = employeeDF.agg(sum("age")).collect()(0).getAs[Long](0)
    println(s"Sum of ages: $sumAge")

    // 停止SparkSession
    spark.stop()
  }
}
</pre> 
<pre><code class="language-Scala">package edu.hnuahe.wanqing.spark_6_3_CreateDataFrame
import org.apache.spark.sql.{SparkSession, Row}
import java.util.Properties
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions.max
import org.apache.spark.sql.functions.sum

case object MySQLDataFrameExample{
  def main(args: Array[String]): Unit = {
    // 创建SparkSession
    val spark = SparkSession.builder()
      .appName("MySQL DataFrame Example MySQL写入与读取")
      .master("local[*]") // 使用本地模式，如果连接到集群请更改这里
      .getOrCreate()

    import spark.implicits._

    // 配置MySQL JDBC连接
    val jdbcProperties = new Properties()
    jdbcProperties.setProperty("user", "root")
    jdbcProperties.setProperty("password", "自己的")
    jdbcProperties.setProperty("driver", "com.mysql.jdbc.Driver")
    //jdbcProperties.setProperty("driver", "com.mysql.cj.jdbc.Driver")

    // 定义MySQL的JDBC连接URL
    val jdbcUrl = "jdbc:mysql://自己的主机:3306/sparktest"

    // 创建DataFrame以插入数据
    val newEmployeeData = Seq(
      (3, "Mary", "F", 26),
      (4, "Tom", "M", 23)
    ).toDF("id", "name", "gender", "age")

    // 将DataFrame数据插入到MySQL的employee表中
    newEmployeeData.write
      .mode("append") // 使用append模式来添加数据，而不是覆盖
      .jdbc(jdbcUrl, "employee", jdbcProperties)

    // 从MySQL读取employee表的数据
    val employeeDF = spark.read
      .jdbc(jdbcUrl, "employee", jdbcProperties)

    // 打印age的最大值
    val maxAge = employeeDF.agg(max("age")).collect()(0).getAs[Int](0)
    println(s"Max age: $maxAge")

    // 打印age的总和
    val sumAge = employeeDF.agg(sum("age")).collect()(0).getAs[Long](0)
    println(s"Sum of ages: $sumAge")

    // 停止SparkSession
    spark.stop()
  }
}
</code></pre> 
<p>再如实验一，上创到虚拟机上查看结果</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/eebda85262362a7eb482fccbbe70cf31/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">开源的在线JSON数据可视化编辑器jsoncrack本地部署与远程访问</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/514b9f0b49912e2bc96fc3d5a59189c5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">X-D-Lab/MindChat-Qwen-7B-v2模型向量化出现llama runner process has terminated: signal: aborted (core dumpe问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>