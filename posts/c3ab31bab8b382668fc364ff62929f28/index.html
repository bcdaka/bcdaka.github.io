<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【LLM】二、python调用本地的ollama部署的大模型 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/c3ab31bab8b382668fc364ff62929f28/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【LLM】二、python调用本地的ollama部署的大模型">
  <meta property="og:description" content="系列文章目录 往期文章：
【LLM】一、利用ollama本地部署大模型
目录
文章目录
前言
一、ollama库调用
二、langchain调用 三、requests调用
四、相关参数说明：
总结
前言 本地部署了大模型，下一步任务便是如何调用的问题，实际场景中个人感觉用http请求的方式较为合理，本篇文章也将通过http请求的方式来调用我们本地部署的大模型，正文开始。
一、ollama库调用 参考文档：ollama的python库调用
注意，这里的ollama不是我们第一篇安装的那个Ollama！！！！不要搞混
1、环境准备：
pip install ollama 2、调用示例：
如果你都是按照默认设置安装的Ollama，即host和port等均未设置，那执行以下代码即可 import ollama res=ollama.chat(model=&#34;phi3&#34;,stream=False,messages=[{&#34;role&#34;: &#34;user&#34;,&#34;content&#34;: &#34;你是谁&#34;}],options={&#34;temperature&#34;:0}) print(res) 返回结果如：
如果你更改了Ollama的配置，比如更改了监听端口，则执行下边代码： import ollama host=&#34;xxx&#34; port=&#34;xxx&#34; client= ollama.Client(host=f&#34;http://{host}:{port}&#34;) res=client.chat(model=&#34;qwen2:1.5b&#34;,messages=[{&#34;role&#34;: &#34;user&#34;,&#34;content&#34;: &#34;你是谁&#34;}],options={&#34;temperature&#34;:0}) print(res) 返回结果如：
其中，host和port改为你自己的即可
二、langchain调用 参考链接：langchain调用ollama
1、安装依赖：
pip install langchain pip install langchain_community 2、调用示例
from langchain_community.llms import Ollama host=&#34;xxx&#34; port=&#34;xxx&#34; #默认的端口号为11434 llm=Ollama(base_url=f&#34;http://{host}:{port}&#34;, model=&#34;qwen2:1.5b&#34;,temperature=0) res=llm.invoke(&#34;你是谁&#34;) print(res) 其中，host和port改为你自己的即可
结果如：
三、requests调用 1、安装依赖
pip install requests 2、调用示例">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-08T22:34:42+08:00">
    <meta property="article:modified_time" content="2024-07-08T22:34:42+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【LLM】二、python调用本地的ollama部署的大模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 id="%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95"><a id="_0"></a>系列文章目录</h2> 
<p>往期文章：</p> 
<p><a class="link-info" href="https://blog.csdn.net/qq_55068938/article/details/140212908" title="【LLM】一、利用ollama本地部署大模型">【LLM】一、利用ollama本地部署大模型</a></p> 
<hr> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95-toc" style="margin-left:0px;"><a href="#%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95" rel="nofollow">文章目录</a></p> 
<p></p> 
<p id="%E5%89%8D%E8%A8%80-toc" style="margin-left:0px;"><a href="#%E5%89%8D%E8%A8%80" rel="nofollow">前言</a></p> 
<p id="%E4%B8%80%E3%80%81ollama%E5%BA%93%E8%B0%83%E7%94%A8-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81ollama%E5%BA%93%E8%B0%83%E7%94%A8" rel="nofollow">一、ollama库调用</a></p> 
<p id="%E4%BA%8C%E3%80%81langchain%E8%B0%83%E7%94%A8%C2%A0-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81langchain%E8%B0%83%E7%94%A8%C2%A0" rel="nofollow">二、langchain调用 </a></p> 
<p id="%E4%B8%89%E3%80%81requests%E8%B0%83%E7%94%A8-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81requests%E8%B0%83%E7%94%A8" rel="nofollow">三、requests调用</a></p> 
<p id="%E5%9B%9B%E3%80%81%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E%EF%BC%9A-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E%EF%BC%9A" rel="nofollow">四、相关参数说明：</a></p> 
<p id="%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#%E6%80%BB%E7%BB%93" rel="nofollow">总结</a></p> 
<p></p> 
<hr> 
<h2 id="%E5%89%8D%E8%A8%80"><a id="_12"></a>前言</h2> 
<p>        本地部署了大模型，下一步任务便是如何调用的问题，实际场景中个人感觉用http请求的方式较为合理，本篇文章也将通过http请求的方式来调用我们本地部署的大模型，正文开始。</p> 
<p></p> 
<hr> 
<p></p> 
<h2 id="%E4%B8%80%E3%80%81ollama%E5%BA%93%E8%B0%83%E7%94%A8"><a id="pandas_22"></a>一、ollama库调用</h2> 
<p>参考文档：<a class="link-info" href="https://github.com/ollama/ollama-python" title="ollama的python库调用">ollama的python库调用</a></p> 
<p>注意，这里的ollama不是我们第一篇安装的那个Ollama！！！！不要搞混</p> 
<p>1、环境准备：</p> 
<pre><code class="language-bash">pip install ollama</code></pre> 
<p>2、调用示例：</p> 
<ul><li>     如果你都是按照默认设置安装的Ollama，即host和port等均未设置，那执行以下代码即可</li></ul> 
<pre><code class="language-python">import ollama
res=ollama.chat(model="phi3",stream=False,messages=[{"role": "user","content": "你是谁"}],options={"temperature":0})
print(res)</code></pre> 
<p>        返回结果如：</p> 
<p>         <img alt="" height="26" src="https://images2.imgbox.com/9e/b5/eKiTUjfe_o.png" width="642"></p> 
<ul><li>如果你更改了Ollama的配置，比如更改了监听端口，则执行下边代码：</li></ul> 
<pre><code class="language-python">import ollama

host="xxx"
port="xxx"
client= ollama.Client(host=f"http://{host}:{port}")
res=client.chat(model="qwen2:1.5b",messages=[{"role": "user","content": "你是谁"}],options={"temperature":0})

print(res)</code></pre> 
<p>返回结果如：</p> 
<p><img alt="" height="30" src="https://images2.imgbox.com/0f/b0/cvZMv0Ez_o.png" width="686"></p> 
<p><span style="color:#fe2c24;">其中，host和port改为你自己的即可</span></p> 
<p></p> 
<h2 id="%E4%BA%8C%E3%80%81langchain%E8%B0%83%E7%94%A8%C2%A0">二、langchain调用 </h2> 
<p>参考链接：<a class="link-info" href="https://blog.csdn.net/lovechris00/article/details/136869964" title="langchain调用ollama">langchain调用ollama</a></p> 
<p>1、安装依赖：</p> 
<pre><code class="language-bash">pip install langchain
pip install langchain_community</code></pre> 
<p>2、调用示例</p> 
<pre><code class="language-python">from langchain_community.llms import Ollama
host="xxx"
port="xxx" #默认的端口号为11434
llm=Ollama(base_url=f"http://{host}:{port}", model="qwen2:1.5b",temperature=0)
res=llm.invoke("你是谁")
print(res)</code></pre> 
<p><span style="color:#fe2c24;">       其中，host和port改为你自己的即可</span></p> 
<p>       结果如：</p> 
<p>     <img alt="" height="24" src="https://images2.imgbox.com/65/00/ZjfijA6F_o.png" width="611">   </p> 
<p></p> 
<h2 id="%E4%B8%89%E3%80%81requests%E8%B0%83%E7%94%A8">三、requests调用</h2> 
<p>1、安装依赖</p> 
<pre><code class="language-python">pip install requests</code></pre> 
<p></p> 
<p>2、调用示例</p> 
<pre><code class="language-python">host="xxx"
port="xxx"
url = f"http://{host}:{port}/api/chat"
model = "qwen2:1.5b"
headers = {"Content-Type": "application/json"}
data = {
        "model": model, #模型选择
        "options": {
            "temperature": 0.  #为0表示不让模型自由发挥，输出结果相对较固定，&gt;0的话，输出的结果会比较放飞自我
         },
        "stream": False, #流式输出
        "messages": [{
            "role": "system",
            "content":"你是谁？"
        }] #对话列表
    }
response=requests.post(url,json=data,headers=headers,timeout=60)
res=response.json()
print(res)</code></pre> 
<p><span style="color:#fe2c24;">其中，host和port改为你自己的即可，结果同上</span></p> 
<h2 id="%E5%9B%9B%E3%80%81%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E%EF%BC%9A">四、相关参数说明：</h2> 
<p>上述几个调用方式中所涉及到的比较重要的参数介绍如下：</p> 
<ul><li>temperature：用于调整生成结果的创造性程度，设置越高，生成的文本越新颖、越独特，设置越低，结果更集中</li><li>stream：默认false,是否流式传输回部分进度。</li><li>format: 转录输出的格式，可选项包括json、str等。</li></ul> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<hr> 
<h2 id="%E6%80%BB%E7%BB%93"><a id="_55"></a>总结</h2> 
<p>以上就是本篇的全部内容，如有问题，环境评论区交流，或+企鹅群：995760755交流；如觉得有用，欢迎三连</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c3238e49c6dfe583e27785a2a1c26c3b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">初识Java（二）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/193e649f760983346652ed3bc169f541/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Java]认识泛型</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>