<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>使用LLaMA-Factory微调Llama3大模型 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/c494e7ccdcc472ed3e6c92e67bf2e315/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="使用LLaMA-Factory微调Llama3大模型">
  <meta property="og:description" content="一、基础模型下载 本文的背景是微调一个基于Llama3的中文版模型Llama3-8B-Chinese-Chat，用于中文指定领域的问答下游任务 1、HuggingFace官网直接下载 官网地址：https://huggingface.co/models
镜像网址：https://hf-mirror.com/
按照如下图所示搜索需要的基础模型
点击Files and versions
依次点击下载，将模型文件下载至指定的文件夹内
此方法官方网站下载速度可能很慢，建议时间紧的同学采用镜像网站下载
2、使用HuggingFace的transformers库下载 安装transformers工具库 pip install transformers 创建环境变量HF_HOME，配置模型下载地址
验证配置是否生效 huggingface-cli scan-cache 执行如下代码 from transformers import AutoModel model_name = &#34;shenzhi-wang/Llama3-8B-Chinese-Chat&#34; model = AutoModel.from_pretrained(model_name) 3、HuggingFace国内镜像下载（旧版现已经弃用） # 下载所需的工具包 pip install -U huggingface_hub pip install huggingface-cli # 指定国内镜像源 # Linux export HF_ENDPOINT=https://hf-mirror.com # Windows set HUGGINGFACE_HUB_ENDPOINT=https://hf-mirror.com # 执行下载命令 # --resume-download 是下载中断恢复下载后继续从上次下载中断点继续下载 # shenzhi-wang/Llama3-8B-Chinese-Chat 是要下载模型的名称 # E:\LLM 是下载到本地的路径 huggingface-cli download --resume-download shenzhi-wang/Llama3-8B-Chinese-Chat --local-dir E:\LLM 4、下载成功后最好校验一下下载的模型是否有问题 二、微调工具LLaMA-Factory 框架 1、LLaMA-Factory安装 git clone --depth 1 https://github.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-22T21:23:16+08:00">
    <meta property="article:modified_time" content="2024-06-22T21:23:16+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">使用LLaMA-Factory微调Llama3大模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-dracula">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>一、基础模型下载</h2> 
<ul><li>本文的背景是微调一个基于Llama3的中文版模型<code>Llama3-8B-Chinese-Chat</code>，用于中文指定领域的问答下游任务</li></ul> 
<h3><a id="1HuggingFace_2"></a>1、HuggingFace官网直接下载</h3> 
<ul><li> <p>官网地址：<code>https://huggingface.co/models</code></p> </li><li> <p>镜像网址：<code>https://hf-mirror.com/</code></p> </li><li> <p>按照如下图所示搜索需要的基础模型<br> <img src="https://images2.imgbox.com/47/c4/KoYfmguP_o.png" alt="在这里插入图片描述"></p> </li><li> <p>点击<code>Files and versions</code><br> <img src="https://images2.imgbox.com/03/08/lPbF4x6p_o.png" alt="在这里插入图片描述"></p> </li><li> <p>依次点击下载，将模型文件下载至指定的文件夹内<img src="https://images2.imgbox.com/3f/7c/6qiDpG8N_o.png" alt="在这里插入图片描述"></p> </li><li> <p>此方法官方网站下载速度可能很慢，建议时间紧的同学采用镜像网站下载</p> </li></ul> 
<h3><a id="2HuggingFacetransformers_15"></a>2、使用HuggingFace的transformers库下载</h3> 
<ul><li>安装transformers工具库</li></ul> 
<pre><code class="prism language-powershell">pip install transformers
</code></pre> 
<ul><li>创建环境变量<code>HF_HOME</code>，配置模型下载地址<br> <img src="https://images2.imgbox.com/0c/73/tNvpQL5g_o.png" alt="在这里插入图片描述"></li><li>验证配置是否生效</li></ul> 
<pre><code class="prism language-powershell">huggingface-<span class="token function">cli</span> scan-cache
</code></pre> 
<p><img src="https://images2.imgbox.com/11/fb/n54xFowB_o.png" alt="在这里插入图片描述"></p> 
<ul><li>执行如下代码</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModel
model_name <span class="token operator">=</span> <span class="token string">"shenzhi-wang/Llama3-8B-Chinese-Chat"</span>
model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="3HuggingFace_36"></a>3、HuggingFace国内镜像下载（旧版现已经弃用）</h3> 
<pre><code class="prism language-powershell"><span class="token comment"># 下载所需的工具包</span>
pip install <span class="token operator">-</span>U huggingface_hub
pip install huggingface-<span class="token function">cli</span>

<span class="token comment"># 指定国内镜像源</span>
<span class="token comment"># Linux</span>
export HF_ENDPOINT=https:<span class="token operator">/</span><span class="token operator">/</span>hf-mirror<span class="token punctuation">.</span>com
<span class="token comment"># Windows</span>
<span class="token function">set</span> HUGGINGFACE_HUB_ENDPOINT=https:<span class="token operator">/</span><span class="token operator">/</span>hf-mirror<span class="token punctuation">.</span>com

<span class="token comment"># 执行下载命令 </span>
<span class="token comment"># --resume-download 是下载中断恢复下载后继续从上次下载中断点继续下载</span>
<span class="token comment"># shenzhi-wang/Llama3-8B-Chinese-Chat 是要下载模型的名称</span>
<span class="token comment"># E:\LLM 是下载到本地的路径</span>
huggingface-<span class="token function">cli</span> download <span class="token operator">--</span><span class="token function">resume-download</span> shenzhi-wang/Llama3-8B-Chinese-Chat <span class="token operator">--</span>local-<span class="token function">dir</span> E:\LLM
</code></pre> 
<h3><a id="4_55"></a>4、下载成功后最好校验一下下载的模型是否有问题</h3> 
<p><img src="https://images2.imgbox.com/76/4b/1ZyvNaRu_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="LLaMAFactory__57"></a>二、微调工具LLaMA-Factory 框架</h2> 
<h3><a id="1LLaMAFactory_58"></a>1、LLaMA-Factory安装</h3> 
<pre><code class="prism language-powershell">git clone <span class="token operator">--</span>depth 1 https:<span class="token operator">/</span><span class="token operator">/</span>github<span class="token punctuation">.</span>com/hiyouga/LLaMA-Factory<span class="token punctuation">.</span>git

cd LLaMA-Factory

<span class="token comment"># "[torch,metrics]"可以删除直接执行pip install -e .</span>
pip install <span class="token operator">-</span>e <span class="token string">".[torch,metrics]"</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/a7/23/Yx36u4Yh_o.png" alt="在这里插入图片描述"></p> 
<ul><li>下载成功后，入下图所示</li></ul> 
<p><img src="https://images2.imgbox.com/2c/bc/6V4CbOY8_o.png" alt="在这里插入图片描述"></p> 
<ul><li>LLaMA-Factory详细介绍可移步GitHub：<code>https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md</code></li></ul> 
<h2><a id="_73"></a>三、构建微调数据</h2> 
<h3><a id="1LLaMAFactory_74"></a>1、打开LLaMA-Factory项目</h3> 
<ul><li>目录结构如下图所示<br> <img src="https://images2.imgbox.com/05/1c/PclqWa5c_o.png" alt="在这里插入图片描述"></li></ul> 
<h3><a id="2json_77"></a>2、构建微调json数据</h3> 
<ul><li>在<code>data</code>目录下新建<code>fine_tuning_data</code>文件夹，用来存放微调所需的数据<br> <img src="https://images2.imgbox.com/a8/14/QLmfc5qo_o.png" alt="在这里插入图片描述"></li><li>由于业务数据公司要求保密，这里就随便构造一些数据用于实验，大家可以根据自己的业务需求进行预训练数据的构造<br> <img src="https://images2.imgbox.com/23/05/H1DTtwla_o.png" alt="在这里插入图片描述"></li><li>在<code>dataset_info.json</code>文件中将刚刚构建的微调数据进行路径配置，注意配置格式要与构建的数据格式一一对应</li></ul> 
<pre><code class="prism language-python"><span class="token comment"># 新构建的节点</span>
<span class="token string">"fintech"</span><span class="token punctuation">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"file_name"</span><span class="token punctuation">:</span> <span class="token string">"fine_tuning_data/fintech.json"</span><span class="token punctuation">,</span>
    <span class="token string">"columns"</span><span class="token punctuation">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"prompt"</span><span class="token punctuation">:</span> <span class="token string">"instruction"</span><span class="token punctuation">,</span>
      <span class="token string">"query"</span><span class="token punctuation">:</span> <span class="token string">"input"</span><span class="token punctuation">,</span>
      <span class="token string">"response"</span><span class="token punctuation">:</span> <span class="token string">"output"</span><span class="token punctuation">,</span>
      <span class="token string">"history"</span><span class="token punctuation">:</span> <span class="token string">"history"</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token comment"># 默认自带的节点，注意修改路径</span>
  <span class="token string">"identity"</span><span class="token punctuation">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"file_name"</span><span class="token punctuation">:</span> <span class="token string">"fine_tuning_data/identity.json"</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/5d/b9/m4ZkjYsp_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="LLaMAFactory_Web_UI_102"></a>四、启动LLaMA-Factory Web UI进行模型微调</h2> 
<h3><a id="1LLaMAFactory_103"></a>1、启动LLaMA-Factory并设置微调参数</h3> 
<ul><li>在LLaMA-Factory项目的根目录下输入已下指令启动LLaMa-Factory Web UI</li></ul> 
<pre><code class="prism language-powershell">llamafactory-<span class="token function">cli</span> webui
</code></pre> 
<ul><li>启动成功后命令框显示如下<br> <img src="https://images2.imgbox.com/14/ad/PEGL2YlA_o.png" alt="在这里插入图片描述"></li><li>Web UI界面显示如下<br> <img src="https://images2.imgbox.com/7b/56/nBLY5YyB_o.png" alt="在这里插入图片描述"></li><li>设置微调参数<br> <img src="https://images2.imgbox.com/31/ff/kWeT2OqF_o.png" alt="在这里插入图片描述"></li><li>点击预览命令，生产微调指令</li></ul> 
<pre><code class="prism language-powershell">llamafactory-<span class="token function">cli</span> train \
    <span class="token operator">--</span>stage sft \
    <span class="token operator">--</span>do_train True \
    <span class="token operator">--</span>model_name_or_path E:\LLM\Llama3-8B-Chinese-Chat \
    <span class="token operator">--</span>preprocessing_num_workers 16 \
    <span class="token operator">--</span>finetuning_type lora \
    <span class="token operator">--</span>template llama3 \
    <span class="token operator">--</span>flash_attn auto \
    <span class="token operator">--</span>dataset_dir <span class="token keyword">data</span> \
    <span class="token operator">--</span>dataset fintech<span class="token punctuation">,</span>identity \
    <span class="token operator">--</span>cutoff_len 1024 \
    <span class="token operator">--</span>learning_rate 0<span class="token punctuation">.</span>0002 \
    <span class="token operator">--</span>num_train_epochs 10<span class="token punctuation">.</span>0 \
    <span class="token operator">--</span>max_samples 1000 \
    <span class="token operator">--</span>per_device_train_batch_size 2 \
    <span class="token operator">--</span>gradient_accumulation_steps 8 \
    <span class="token operator">--</span>lr_scheduler_type cosine \
    <span class="token operator">--</span>max_grad_norm 1<span class="token punctuation">.</span>0 \
    <span class="token operator">--</span>logging_steps 5 \
    <span class="token operator">--</span>save_steps 100 \
    <span class="token operator">--</span>warmup_steps 0 \
    <span class="token operator">--</span>optim adamw_torch \
    <span class="token operator">--</span>packing False \
    <span class="token operator">--</span>report_to none \
    <span class="token operator">--</span>output_dir saves\LLaMA3-8B-Chinese-Chat\lora\train_2024-06-20-19-55-32 \
    <span class="token operator">--</span>fp16 True \
    <span class="token operator">--</span>plot_loss True \
    <span class="token operator">--</span>ddp_timeout 180000000 \
    <span class="token operator">--</span>include_num_input_tokens_seen True \
    <span class="token operator">--</span>lora_rank 8 \
    <span class="token operator">--</span>lora_alpha 16 \
    <span class="token operator">--</span>lora_dropout 0 \
    <span class="token operator">--</span>lora_target all
</code></pre> 
<p><img src="https://images2.imgbox.com/b6/fa/Y69MjY2O_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="2_153"></a>2、开始微调</h3> 
<ul><li>第一种方法启动微调</li><li>在Web UI中点击<code>开始</code></li></ul> 
<p><img src="https://images2.imgbox.com/83/81/opFq3CUn_o.png" alt="在这里插入图片描述"></p> 
<ul><li>第二种方法启动微调</li><li>将微调指令构建成<code>build/train_llama3_lora.yaml</code>配置文件，内容如下</li></ul> 
<pre><code class="prism language-yaml"><span class="token key atrule">stage</span><span class="token punctuation">:</span> sft
<span class="token key atrule">do_train</span><span class="token punctuation">:</span> <span class="token boolean important">True</span>
<span class="token key atrule">model_name_or_path</span><span class="token punctuation">:</span> E<span class="token punctuation">:</span>\LLM\LLMLlama3<span class="token punctuation">-</span>8B<span class="token punctuation">-</span>Chinese<span class="token punctuation">-</span>Chat
<span class="token key atrule">preprocessing_num_workers</span><span class="token punctuation">:</span> <span class="token number">16</span>
<span class="token key atrule">finetuning_type</span><span class="token punctuation">:</span> lora
<span class="token key atrule">template</span><span class="token punctuation">:</span> llama3
<span class="token key atrule">flash_attn</span><span class="token punctuation">:</span> auto
<span class="token key atrule">dataset_dir</span><span class="token punctuation">:</span> data
<span class="token key atrule">dataset</span><span class="token punctuation">:</span> fintech<span class="token punctuation">,</span>identity
<span class="token key atrule">cutoff_len</span><span class="token punctuation">:</span> <span class="token number">1024</span>
<span class="token key atrule">learning_rate</span><span class="token punctuation">:</span> <span class="token number">0.0002</span>
<span class="token key atrule">num_train_epochs</span><span class="token punctuation">:</span> <span class="token number">10.0</span>
<span class="token key atrule">max_samples</span><span class="token punctuation">:</span> <span class="token number">1000</span>
<span class="token key atrule">per_device_train_batch_size</span><span class="token punctuation">:</span> <span class="token number">2</span>
<span class="token key atrule">gradient_accumulation_steps</span><span class="token punctuation">:</span> <span class="token number">8</span>
<span class="token key atrule">lr_scheduler_type</span><span class="token punctuation">:</span> cosine
<span class="token key atrule">max_grad_norm</span><span class="token punctuation">:</span> <span class="token number">1.0</span>
<span class="token key atrule">logging_steps</span><span class="token punctuation">:</span> <span class="token number">5</span>
<span class="token key atrule">save_steps</span><span class="token punctuation">:</span> <span class="token number">100</span>
<span class="token key atrule">warmup_steps</span><span class="token punctuation">:</span> <span class="token number">0</span>
<span class="token key atrule">optim</span><span class="token punctuation">:</span> adamw_torch
<span class="token key atrule">packing</span><span class="token punctuation">:</span> <span class="token boolean important">False</span>
<span class="token key atrule">report_to</span><span class="token punctuation">:</span> none
<span class="token key atrule">output_dir</span><span class="token punctuation">:</span> saves\LLaMA3<span class="token punctuation">-</span>8B<span class="token punctuation">-</span>Chinese<span class="token punctuation">-</span>Chat\lora\train_2024<span class="token punctuation">-</span>06<span class="token punctuation">-</span>20<span class="token punctuation">-</span>19<span class="token punctuation">-</span>55<span class="token punctuation">-</span><span class="token number">32</span>
<span class="token key atrule">fp16</span><span class="token punctuation">:</span> <span class="token boolean important">True</span>
<span class="token key atrule">plot_loss</span><span class="token punctuation">:</span> <span class="token boolean important">True</span>
<span class="token key atrule">ddp_timeout</span><span class="token punctuation">:</span> <span class="token number">180000000</span>
<span class="token key atrule">include_num_input_tokens_seen</span><span class="token punctuation">:</span> <span class="token boolean important">True</span>
<span class="token key atrule">lora_rank</span><span class="token punctuation">:</span> <span class="token number">8</span>
<span class="token key atrule">lora_alpha</span><span class="token punctuation">:</span> <span class="token number">16</span>
<span class="token key atrule">lora_dropout</span><span class="token punctuation">:</span> <span class="token number">0</span>
<span class="token key atrule">lora_target</span><span class="token punctuation">:</span> all
</code></pre> 
<p><img src="https://images2.imgbox.com/4f/fc/cjb2lGXk_o.png" alt="在这里插入图片描述"></p> 
<ul><li>在LLaMA-Factory项目的根目录执行以下命令开始微调</li></ul> 
<pre><code class="prism language-powershell">llamafactory-<span class="token function">cli</span> train build/train_llama3_lora<span class="token punctuation">.</span>yaml
</code></pre> 
<ul><li>开始微调，需要耐心等待<br> <img src="https://images2.imgbox.com/03/59/uHxCLekx_o.png" alt="在这里插入图片描述"></li><li>微调过程中要经常检测一下显卡的运行情况，防止“爆显存”导致微调任务终止</li></ul> 
<pre><code class="prism language-powershell"><span class="token comment"># Linux</span>
watch <span class="token operator">-</span>n 1 nvidia-smi
<span class="token comment"># windows</span>
nvidia-smi<span class="token punctuation">.</span>exe <span class="token operator">-</span>l
</code></pre> 
<p><img src="https://images2.imgbox.com/a5/03/B0TWjyPW_o.png" alt="在这里插入图片描述"></p> 
<ul><li>开始微调后，可以观看终端进度条检测微调任务进程<br> <img src="https://images2.imgbox.com/bc/c6/vUwhmfVs_o.png" alt="在这里插入图片描述"></li><li>还可以在Web UI中观看微调过程中的loss下降图<br> <img src="https://images2.imgbox.com/f1/03/2bKEWUCK_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/77/e4/2jLqCIpZ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c1/c9/4lgW6OMS_o.png" alt="在这里插入图片描述"></li></ul> 
<h3><a id="3_218"></a>3、查看微调结果</h3> 
<ul><li>微调完成后，终端和Web UI会有相应的提示<br> <img src="https://images2.imgbox.com/f6/6c/TfSQEWo7_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/02/c5/Wq4JSBlM_o.png" alt="在这里插入图片描述"></li><li>微调完成后，在指定的文件夹下可以查看微调对应的模型参数<br> <img src="https://images2.imgbox.com/40/48/QuTxulLs_o.png" alt="在这里插入图片描述"></li></ul> 
<h2><a id="_224"></a>五、检验微调结果</h2> 
<h3><a id="1_225"></a>1、加载验证集进行模型验证</h3> 
<ul><li>在微调成功后，大家需要根据具体业务情况，将准备好的验证数据集加载进来，用于验证模型的微调情况<br> <img src="https://images2.imgbox.com/fb/55/1vegrkfh_o.png" alt="在这里插入图片描述"></li><li>这里在进行验证的过程中，需要用到<code>jieba、nltk、rouge-chinese</code>这三个NLP常用的工具包，大家如果没有安装，可以在执行验证前进行安装</li></ul> 
<pre><code class="prism language-powershell">pip install jieba nltk rouge-chinese
</code></pre> 
<h3><a id="2_233"></a>2、加载微调前后的模型进行推理，对比结果</h3> 
<ul><li>这里用到了vllm推理框架，没有安装的同学也需要安装一下</li></ul> 
<pre><code class="prism language-powershell"><span class="token comment"># Linux</span>
pip install vllm
</code></pre> 
<ul><li>加载预训练模型<code>Llama3-8B-Chinese-Chat</code><br> <img src="https://images2.imgbox.com/b0/61/lpZ1hji1_o.png" alt="在这里插入图片描述"></li><li>加载成功后，就可以进行对话了<br> <img src="https://images2.imgbox.com/da/d6/gJlQFnze_o.png" alt="在这里插入图片描述"></li><li>这是微调前的模型回答<br> <img src="https://images2.imgbox.com/52/19/2M3aPjXO_o.png" alt="在这里插入图片描述"></li><li>加载微调后的预训练模型<code>Llama3-8B-Chinese-Chat</code><br> <img src="https://images2.imgbox.com/e9/86/XHRZHaAV_o.png" alt="在这里插入图片描述"></li><li>这是微调后的模型回答，从结果可以看出，我们的微调是有效的<br> <img src="https://images2.imgbox.com/af/4a/YcBBA2Iy_o.png" alt="在这里插入图片描述"></li></ul> 
<h3><a id="3LoRA_Adapter_250"></a>3、将训练后的LoRA Adapter和基座模型合并</h3> 
<p>1、Web UI界面合并</p> 
<ul><li>根据业务指定好合并参数，点击导出，耐心等待即可<br> <img src="https://images2.imgbox.com/6f/b2/FbCgDDtl_o.png" alt="在这里插入图片描述"></li><li>合并导出后的模型目录如下<br> <img src="https://images2.imgbox.com/5c/39/IAs8u2OG_o.png" alt="在这里插入图片描述"></li><li>这样就可以直接使用微调后的模型了<br> <img src="https://images2.imgbox.com/0c/69/kxCpPHoJ_o.png" alt="在这里插入图片描述"></li></ul> 
<p>2、命令行合并</p> 
<ul><li>将模型合并参数构建成<code>build/merge_llama3_lora.yaml</code>配置文件，内容如下<br> <img src="https://images2.imgbox.com/50/23/7Q9w28v9_o.png" alt="在这里插入图片描述"></li></ul> 
<pre><code class="prism language-python"><span class="token comment">### Note: DO NOT use quantized model or quantization_bit when merging lora adapters</span>

<span class="token comment">### model</span>
model_name_or_path<span class="token punctuation">:</span> E<span class="token punctuation">:</span>\LLM\Llama3<span class="token operator">-</span>8B<span class="token operator">-</span>Chinese<span class="token operator">-</span>Chat
adapter_name_or_path<span class="token punctuation">:</span> E<span class="token punctuation">:</span>\LLM\LLaMA<span class="token operator">-</span>Factory\saves\LLaMA3<span class="token operator">-</span>8B<span class="token operator">-</span>Chinese<span class="token operator">-</span>Chat\lora\train_2024<span class="token operator">-</span><span class="token number">06</span><span class="token operator">-</span><span class="token number">20</span><span class="token operator">-</span><span class="token number">19</span><span class="token operator">-</span><span class="token number">55</span><span class="token operator">-</span><span class="token number">32</span>
template<span class="token punctuation">:</span> llama3
finetuning_type<span class="token punctuation">:</span> lora

<span class="token comment">### export</span>
export_dir<span class="token punctuation">:</span> E<span class="token punctuation">:</span>\LLM\LLaMA3<span class="token operator">-</span>8B<span class="token operator">-</span>Chinese<span class="token operator">-</span>Chat<span class="token operator">-</span>merged
export_size<span class="token punctuation">:</span> <span class="token number">4</span>
export_device<span class="token punctuation">:</span> cuda
export_legacy_format<span class="token punctuation">:</span> false
</code></pre> 
<ul><li>在LLaMA-Factory根目录下执行合并命令</li></ul> 
<pre><code class="prism language-powershell">llamafactory-<span class="token function">cli</span> export build/merge_llama3_lora<span class="token punctuation">.</span>yaml
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/050534cc0c4296ba990735a8d614234a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【八股文】算法岗位八股文、深度学习、AIGC八股文面试经验（一）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2abdbc56f4ea47a30a0168eb72741a63/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">vscode创建编辑markdown文件</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>