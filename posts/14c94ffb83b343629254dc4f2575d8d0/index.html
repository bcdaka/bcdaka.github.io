<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>开源 AI 模型实际部署以及后端搭建（一）--环境配置 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/14c94ffb83b343629254dc4f2575d8d0/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="开源 AI 模型实际部署以及后端搭建（一）--环境配置">
  <meta property="og:description" content="开源 AI 模型实际部署以及后端搭建（一）–环境配置 本文是博主zzzzz想要综合多个开源AI模型分布式部署,以及分布式后端反馈的搭建日记
技术栈 python 与 golang
Gin框架,grpc,Redis等等
基础环境搭建 算力平台 ( cuda , conda ) 基本需求显卡支持调用cuda且满足后续所提项目的显存(最好&gt;=12GB)
若搭建在虚拟机上需独占内存
对于一台新开的Linux机子,
一开始不少困扰大家的就是环境搭建,
我看过周围的人搭建环境卡了两天都没搭好(请不要让apt或者pip等工具自己匹配版本下载),
于此,我在这里写详细一些.
接下来,我以本人的Ubuntu Linux 22.04 &#43; NVIDIA RTX 5000 架构:Turing 举例 (T4可以照抄,其他更多型号要留意适配cuda版本)
以我的举例 ( PS:我们需要在英伟达官网找到对应卡支持的版本!!! )
wget https://developer.download.nvidia.com/compute/cuda/12.3.1/local_installers/cuda_12.3.1_545.23.08_linux.run 安装nvidia显卡驱动首先需要禁用nouveau，不然会碰到冲突的问题，导致无法安装nvidia显卡驱动。
编辑文件blacklist.conf
sudo vim /etc/modprobe.d/blacklist.conf
在文件最后部分插入以下两行内容
blacklist nouveau options nouveau modeset=0 更新系统
sudo update-initramfs -u 之后需要重新启动
sudo reboot 重启之后
我们回到下载cuda的位置
给予安装权限
chmod &#43;x cuda_12.3.1_545.23.08_linux.run 安装前保证之前安装过build-essential
sudo apt install build-essential 即可
sudo .">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-09-06T01:51:39+08:00">
    <meta property="article:modified_time" content="2024-09-06T01:51:39+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">开源 AI 模型实际部署以及后端搭建（一）--环境配置</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_AI__1"></a>开源 AI 模型实际部署以及后端搭建（一）–环境配置</h2> 
<blockquote> 
 <p>本文是博主zzzzz想要综合多个开源AI模型分布式部署,以及分布式后端反馈的搭建日记</p> 
</blockquote> 
<h3><a id="_5"></a>技术栈</h3> 
<p>python 与 golang</p> 
<p>Gin框架,grpc,Redis等等</p> 
<h3><a id="_11"></a>基础环境搭建</h3> 
<h4><a id="__cuda__conda__15"></a>算力平台 ( cuda , conda )</h4> 
<blockquote> 
 <p>基本需求显卡支持调用cuda且满足后续所提项目的显存(最好&gt;=12GB)</p> 
 <p>若搭建在虚拟机上需独占内存</p> 
</blockquote> 
<p>对于一台新开的Linux机子,</p> 
<p>一开始不少困扰大家的就是环境搭建,</p> 
<p>我看过周围的人搭建环境卡了两天都没搭好(请不要让apt或者pip等工具自己匹配版本下载),</p> 
<p>于此,我在这里写详细一些.</p> 
<p>接下来,我以本人的Ubuntu Linux 22.04 + NVIDIA RTX 5000 架构:Turing 举例 (T4可以照抄,其他更多型号要留意适配cuda版本)</p> 
<p>以我的举例 ( PS:我们需要在英伟达官网找到对应卡支持的版本!!! )</p> 
<pre><code>wget https://developer.download.nvidia.com/compute/cuda/12.3.1/local_installers/cuda_12.3.1_545.23.08_linux.run
</code></pre> 
<p>安装nvidia显卡驱动首先需要禁用nouveau，不然会碰到冲突的问题，导致无法安装nvidia显卡驱动。</p> 
<p>编辑文件blacklist.conf</p> 
<p><code>sudo vim /etc/modprobe.d/blacklist.conf</code><br> 在文件最后部分插入以下两行内容</p> 
<pre><code>blacklist nouveau
options nouveau modeset=0
</code></pre> 
<p>更新系统</p> 
<pre><code>sudo update-initramfs -u
</code></pre> 
<p>之后需要重新启动</p> 
<pre><code>sudo reboot
</code></pre> 
<p>重启之后</p> 
<p>我们回到下载cuda的位置</p> 
<p>给予安装权限</p> 
<pre><code>chmod +x cuda_12.3.1_545.23.08_linux.run
</code></pre> 
<p>安装前保证之前安装过build-essential</p> 
<pre><code>sudo apt install build-essential
</code></pre> 
<p>即可</p> 
<pre><code>sudo ./cuda_12.3.1_545.23.08_linux.run
</code></pre> 
<p>若出现Xrog报错直接kill</p> 
<pre><code>killall Xorg
</code></pre> 
<p>进入安装程序后我们可以直接全部默认也可选择性安装</p> 
<p>最后输入</p> 
<pre><code>nvidia-smi
</code></pre> 
<p>即有</p> 
<pre><code>Sat Aug 31 01:43:40 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro RTX 5000                Off | 00000000:0B:00.0 Off |                  Off |
| 33%   32C    P8               4W / 230W |      9MiB / 16384MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A    255944      G   /usr/lib/xorg/Xorg                            4MiB |
|    0   N/A  N/A    256103      G   ...libexec/gnome-remote-desktop-daemon        2MiB |
+---------------------------------------------------------------------------------------+
</code></pre> 
<p>我们的驱动就安装好了</p> 
<p>接下来我们安装conda</p> 
<p>对于隔离不同python环境此处很重要</p> 
<p>以我举例</p> 
<pre><code>wget -c https://repo.anaconda.com/archive/Anaconda3-2023.03-1-Linux-x86_64.sh
</code></pre> 
<p>下载好后</p> 
<pre><code>bash Anaconda3-2023.03-1-Linux-x86_64.sh
</code></pre> 
<p>安装中途记得填入yes否则他默认no就不会帮你配置默认环境</p> 
<p>安装完成后，建议添加 Anaconda 到系统环境变量中。</p> 
<p>在终端中运行以下命令：</p> 
<p><code>source ~/.bashrc</code></p> 
<p>或者手动编辑 .bashrc 文件，</p> 
<p>在文件末尾添加以下内容：</p> 
<p><code>export PATH="$HOME/anaconda3/bin:$PATH"</code></p> 
<p>保存文件后，运行：</p> 
<p><code>source ~/.bashrc</code></p> 
<p>当出现</p> 
<pre><code>(base) zzzzz@zzzzz-10L:~$
</code></pre> 
<p>即conda安装完毕</p> 
<h4><a id="___170"></a>后端平台 ( )</h4> 
<h5><a id="Golang_172"></a>Golang</h5> 
<p>下载好golang并查看版本</p> 
<pre><code>https://golang.google.cn/dl/go1.23.0.linux-amd64.tar.gz

sudo tar -C /usr/local -xzf go1.23.0.linux-amd64.tar.gz 
</code></pre> 
<p>设置环境变量：</p> 
<p>确保你的 <code>PATH</code> 环境变量中包含 Go 的安装路径。你可以在 <code>~/.bashrc</code> 文件中添加以下行：</p> 
<pre><code>export PATH=$PATH:/usr/local/go/bin
</code></pre> 
<p>然后重新加载配置文件：</p> 
<pre><code>source ~/.bashrc
</code></pre> 
<p>验证安装：</p> 
<p>确认你已经成功安装了新的 Go 版本：</p> 
<pre><code>go version
</code></pre> 
<p>Goproxy</p> 
<p>打开你的终端并执行</p> 
<pre><code>go env -w GO111MODULE=on
go env -w GOPROXY=https://goproxy.cn,direct
</code></pre> 
<p>然后我们如果是运行下载的项目既可以运行配置环境</p> 
<pre><code>go mod tidy
</code></pre> 
<h5><a id="Mysql_221"></a>Mysql</h5> 
<p>mysql准备</p> 
<pre><code>sudo apt install -y mysql-server-8.0
</code></pre> 
<p>如果不加<code>-y</code> 会在安装过程中，</p> 
<p>系统将提示你设置MySQL的root密码</p> 
<p>启动MySQL服务</p> 
<p>安装完成后，MySQL服务会自动启动，未启动则使用以下命令启动MySQL服务：</p> 
<p><code>sudo systemctl start mysql</code></p> 
<p>并将MySQL设置为开机自启动：</p> 
<p><code>sudo systemctl enable mysql</code></p> 
<p>检查MySQL状态</p> 
<p>你可以使用以下命令来检查MySQL是否正在运行：</p> 
<p><code>sudo systemctl status mysql</code></p> 
<p>默认安装是没有设置密码的，需要我们自己设置密码。</p> 
<p>登录mysql，在默认安装时如果没有让我们设置密码，则直接回车就能登录成功。</p> 
<p><code>sudo mysql</code></p> 
<p>配置8.0版本参考：</p> 
<p>注意: 在MySQL8.0以后的版本更改密码要使用Alter的方式, 而且要指定mysql_native_password</p> 
<pre><code>ALTER USER ‘root’@‘localhost’ IDENTIFIED WITH mysql_native_password BY ‘12345678’;

quit;
</code></pre> 
<p>我这里通过这种方式没有实现所有IP都能访问；我是通过直接修改配置文件才实现的，</p> 
<p>MySQL8.0版本把配置文件 my.cnf 拆分成mysql.cnf 和mysqld.cnf，</p> 
<p>我们需要修改的是mysqld.cnf文件：</p> 
<p><code>sudo vim /etc/mysql/mysql.conf.d/mysqld.cnf</code></p> 
<p>修改 bind-address，保存后重启MySQL即可。</p> 
<p><code>bind-address = 0.0.0.0</code></p> 
<p>重启MySQL重新加载一下配置：</p> 
<p><code>sudo systemctl restart mysql</code></p> 
<p>之后重启MySQL以root登录<code>mysql -u root -p</code>,</p> 
<p>密码就是我们刚刚设的</p> 
<p>详细后续参考其他mysql教程</p> 
<h3><a id="AI_287"></a>AI项目服务器部署</h3> 
<h4><a id="_AI_Llama_289"></a>自然语言模型 AI Llama</h4> 
<blockquote> 
 <p>这个模型有7B,13B,80B…版本</p> 
 <p>我的可以跑下7B 后续以7B为示例</p> 
 <p>参考汉化模型 star 比较多的模型 https://github.com/LlamaFamily/Llama-Chinese?tab=readme-ov-file</p> 
</blockquote> 
<h5><a id="_297"></a>环境配置</h5> 
<p>我们先创建一个环境</p> 
<p>要求3.10以上</p> 
<pre><code>conda create -n Llama python=3.10
conda activate Llama
</code></pre> 
<p>接着我们对着 cuda 驱动型号下载 pytorch 等必要工具 ，</p> 
<p>这里需注意！！！不然跑到 cpu 去了</p> 
<pre><code>conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia    
</code></pre> 
<p>或者使用</p> 
<pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
</code></pre> 
<p>接着</p> 
<pre><code>git clone https://github.com/LlamaFamily/Llama-Chinese.git
国内镜像下载: git clone https://mirror.ghproxy.com/https://github.com/LlamaFamily/Llama-Chinese.git
cd Llama-Chinese
pip install -r requirements.txt
</code></pre> 
<p>我们就下好本地 Llama 及其依赖项</p> 
<h5><a id="huggingface_335"></a>huggingface工具使用</h5> 
<blockquote> 
 <p>此网站有很多大模型和训练集, 是找模型和训练模型的好地方</p> 
</blockquote> 
<p>先去下载必要工具</p> 
<pre><code>pip install -U huggingface_hub
</code></pre> 
<p>处于国内则需要镜像</p> 
<p>否则无法使用</p> 
<pre><code>export HF_ENDPOINT=https://hf-mirror.com
</code></pre> 
<p><strong>huggingface-cli+hf_transfer</strong></p> 
<p><code>huggingface-cli</code> 和 <code>hf_transfer</code> 是 hugging face 官方提供的专门为下载而设计的工具链。</p> 
<p>前者是一个命令行工具，后者是下载加速模块。</p> 
<p><code>hf_transfer</code> 依附并兼容 <code>huggingface-cli</code>，</p> 
<p>是 hugging face 官方专门为提高下载速度基于 Rust 开发的一个模块，</p> 
<p>开启后在哪怕在拥有糟糕的网络 SZTU 机器上可以跑到最高 180MB/s ！！！</p> 
<pre><code>pip install -U hf-transfer
export HF_HUB_ENABLE_HF_TRANSFER=1
</code></pre> 
<p><strong>基本用法</strong></p> 
<pre><code class="prism language-bash">huggingface-cli download --resume-download FlagAlpha/Llama3-Chinese-8B-Instruct  --local-dir FlagAlpha/Llama3-Chinese-8B-Instruct
</code></pre> 
<p><strong>下载数据集</strong></p> 
<pre><code class="prism language-rust">huggingface<span class="token operator">-</span>cli download <span class="token operator">-</span><span class="token operator">-</span>resume<span class="token operator">-</span>download <span class="token operator">-</span><span class="token operator">-</span>repo<span class="token operator">-</span><span class="token keyword">type</span> <span class="token type-definition class-name">dataset</span> lavita<span class="token operator">/</span>medical<span class="token operator">-</span>qa<span class="token operator">-</span>shared<span class="token operator">-</span>task<span class="token operator">-</span>v1<span class="token operator">-</span>toy
</code></pre> 
<p>值得注意的是，有个<code>--local-dir-use-symlinks False</code> 参数可选，</p> 
<p>因为huggingface的工具链默认会使用符号链接来存储下载的文件，</p> 
<p>导致<code>--local-dir</code>指定的目录中都是一些“链接文件”，</p> 
<p>真实模型则存储在<code>~/.cache/huggingface</code>下，</p> 
<p>如果不喜欢这个可以用 <code>--local-dir-use-symlinks False</code>取消这个逻辑。</p> 
<p>删除模型要在真实模型处删除，不然自以为删除后，发现还是没有空间</p> 
<pre><code>| 0.00/826 [00:00&lt;?, ?B/s]
| 168M/9.87G [00:12&lt;11:58, 13.5MB/s]
| 220M/9.82G [00:13&lt;09:08, 17.5MB/s]
| 31.5M/9.96G [00:30&lt;2:29:19, 1.11MB/s]
| 577M/9.82G [00:36&lt;11:36, 13.3MB/s]
| 409M/9.90G [00:36&lt;08:43, 18.1MB/s]
| 199M/9.87G [00:35&lt;29:42, 5.43MB/s]
| 199M/5.69G [00:35&lt;17:37, 5.19MB/s]
| 189M/9.96G [00:34&lt;31:47, 5.12MB/s]
| 493M/9.87G [00:36&lt;11:01, 14.2MB/s]
</code></pre> 
<p>像这种70+G的大模型， 通过此工具仅需一顿饭时间即可下好。</p> 
<p>介绍完基本用法</p> 
<p>我们去到<code>zzzzz@zzzzz-10L:~/project/Llama-Chinese</code></p> 
<pre><code>huggingface-cli download --resume-download FlagAlpha/Llama3-Chinese-8B-Instruct  --local-dir FlagAlpha/Llama3-Chinese-8B-Instruct
</code></pre> 
<p>下载好后</p> 
<pre><code>python examples/chat_gradio.py --model_name_or_path FlagAlpha/Atom-7B-Chat
</code></pre> 
<p>即可以使用webui查看模型部署成功</p> 
<p>想了解更多可以访问 https://github.com/LlamaFamily/Llama-Chinese.git</p> 
<h4><a id="_LivePortrait_427"></a>视频生视频 LivePortrait</h4> 
<h5><a id="_429"></a>环境搭建</h5> 
<p>与前者同理</p> 
<pre><code>git clone https://github.com/KwaiVGI/LivePortrait
cd LivePortrait

# 使用conda创建环境
conda create -n LivePortrait python=3.9
conda activate LivePortrait
</code></pre> 
<p>对于pytorch同理</p> 
<p>这里需注意！！！不然跑到 cpu 去了</p> 
<pre><code>conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia    
</code></pre> 
<p>或者使用</p> 
<pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
</code></pre> 
<p>最后</p> 
<p>安装依赖项</p> 
<pre><code>pip install -r requirements.txt --no-deps 
#  防止自动更新pytorch导致前面环境配置崩溃
</code></pre> 
<h5><a id="huggingface_465"></a>同理在huggingface下载模型</h5> 
<pre><code class="prism language-bash"><span class="token comment"># !pip install -U "huggingface_hub[cli]"</span>
huggingface-cli download KwaiVGI/LivePortrait --local-dir pretrained_weights <span class="token parameter variable">--exclude</span> <span class="token string">"*.git*"</span> <span class="token string">"README.md"</span> <span class="token string">"docs"</span>
</code></pre> 
<p>最后到项目根目录下</p> 
<pre><code class="prism language-bash"><span class="token comment"># 源输入是图像</span>
python inference.py <span class="token parameter variable">-s</span> assets/examples/source/s9.jpg <span class="token parameter variable">-d</span> assets/examples/driving/d0.mp4

<span class="token comment"># 源输入是视频 ✨</span>
python inference.py <span class="token parameter variable">-s</span> assets/examples/source/s13.mp4 <span class="token parameter variable">-d</span> assets/examples/driving/d0.mp4

<span class="token comment"># 更多选项请见</span>
python inference.py <span class="token parameter variable">-h</span>
</code></pre> 
<p>若能生成即说明可以使用</p> 
<p>想了解更多可以访问 https://github.com/KwaiVGI/LivePortrait</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f3fb58c193c8897f92e26c671ab812fb/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">如何在SpringBoot中实现优雅关闭</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f10373e5b5e7f964326b2da5be07fde0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Docker 容器技术：简化 MySQL 主从复制部署与优化</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>