<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>flink的常见的任务提交方式 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/f7368c3b40031db603131e2dea0a83a4/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="flink的常见的任务提交方式">
  <meta property="og:description" content="1、以flinksql的方式直接提交任务 此方式使用起来相对比较简单，但是无法满足需要设置savepoint暂存点的流式任务需求。
使用此方式需要先创建Flink远方的执行环境，然后按序执行FlinkSql，流程如下：
java示例如下：
package com.xw.flink; import org.apache.flink.api.java.ExecutionEnvironment; import org.apache.flink.configuration.Configuration; import org.apache.flink.table.api.TableEnvironment; public class testSqlServer { public static void main(String[] args) throws Exception { ExecutionEnvironment env = ExecutionEnvironment.createRemoteEnvironment(&#34;192.168.1.88&#34;,18082); TableEnvironment tableEnv = TableEnvironment.create(env.getConfiguration()); Configuration configuration = tableEnv.getConfig().getConfiguration(); //任务名称设定 configuration.setString(&#34;pipeline.name&#34;,&#34;sqlserver&#34;); String sourceDDL = &#34;CREATE TABLE Orders (f1 STRING,f2 STRING,f3 STRING) WITH ( &#34; &#43; &#34; &#39;connector&#39; = &#39;jdbc&#39;, &#34; &#43; &#34; &#39;driver&#39;=&#39;com.microsoft.sqlserver.jdbc.SQLServerDriver&#39;, &#34; &#43; &#34; &#39;url&#39;=&#39;jdbc:sqlserver://192.168.1.40:1433;databaseName=test;useLOBs=false&#39;, &#34; &#43; &#34; &#39;table-name&#39;=&#39;test_czd1&#39;, &#34; &#43; &#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-01-26T10:39:50+08:00">
    <meta property="article:modified_time" content="2024-01-26T10:39:50+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">flink的常见的任务提交方式</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>1、以flinksql的方式直接提交任务</h2> 
<p>此方式使用起来相对比较简单，但是无法满足需要设置savepoint暂存点的流式任务需求。</p> 
<p>使用此方式需要先创建Flink远方的执行环境，然后按序执行FlinkSql，流程如下：</p> 
<p><img alt="" height="28" src="https://images2.imgbox.com/35/04/eB571Sy1_o.png" width="699"></p> 
<p>java示例如下：</p> 
<pre><code class="language-java">package com.xw.flink;

import org.apache.flink.api.java.ExecutionEnvironment;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.table.api.TableEnvironment;

public class testSqlServer {
    public static void main(String[] args) throws Exception {
        ExecutionEnvironment env = ExecutionEnvironment.createRemoteEnvironment("192.168.1.88",18082);
        TableEnvironment tableEnv = TableEnvironment.create(env.getConfiguration());
        Configuration configuration = tableEnv.getConfig().getConfiguration();
        //任务名称设定
        configuration.setString("pipeline.name","sqlserver");
        String sourceDDL = "CREATE TABLE Orders (f1 STRING,f2 STRING,f3 STRING) WITH ( " +
                " 'connector' = 'jdbc',  " +
                " 'driver'='com.microsoft.sqlserver.jdbc.SQLServerDriver',  " +
                " 'url'='jdbc:sqlserver://192.168.1.40:1433;databaseName=test;useLOBs=false',  " +
                " 'table-name'='test_czd1',  " +
                " 'username'='root',  " +
                " 'password'='root'" +
                ")";
        tableEnv.executeSql(sourceDDL);
        String rtLog = "CREATE TABLE logs (node_id STRING, send_count BIGINT,PRIMARY KEY(node_id) NOT ENFORCED) WITH ( " +
                " 'connector' = 'jdbc',  " +
                " 'driver'='com.mysql.cj.jdbc.Driver',  " +
                " 'url'='jdbc:mysql://192.168.0.68:3306/testDB?rewriteBatchedStatements=true&amp;useUnicode=true&amp;characterEncoding=utf8&amp;autoReconnect=true&amp;useSSL=false&amp;zeroDateTimeBehavior=convertToNull&amp;allowMultiQueries=true&amp;serverTimezone=GMT%2B8',  " +
                " 'table-name'='rt_log',  " +
                " 'username'='root',  " +
                " 'password'='root'," +
                " 'sink.buffer-flush.max-rows' = '20000'," +
                " 'sink.buffer-flush.interval' = '3000'" +
                ")";
        tableEnv.executeSql(rtLog);
        String sql = "insert into logs(node_id) select f1 from Orders limit 5";
        tableEnv.executeSql(sql);
    }
}</code></pre> 
<h2>2、以任务jar的方式上传任务</h2> 
<p>此方式主要通过用java编写一个任务，然后打成jar的形式上传到flink集群。此方式比较灵活，可以精确控制任务的算子。但是对于现场的运维来说是一个比较困难的问题，因为要求运维人员需要有代码开发的能力。</p> 
<p>java实现示例：</p> 
<pre><code class="language-java">public class testSqlServer {
    public static void main(String[] args) throws Exception {
        ExecutionEnvironment env = ExecutionEnvironment.createLocalEnvironment();
        TableEnvironment tableEnv = TableEnvironment.create(env.getConfiguration());
        Configuration configuration = tableEnv.getConfig().getConfiguration();
        //任务名称设定
        configuration.setString("pipeline.name","sqlserver");
        //todo 此部分可以是Flink-table  也可以是Flinksql
      
    }
}</code></pre> 
<p>然后将其打成jar包，然后上传到flink</p> 
<p><img alt="" height="506" src="https://images2.imgbox.com/15/f7/Y4rLpBkK_o.png" width="1200"></p> 
<p>填写class的全路径和并行度即可执行。</p> 
<h2>3、以Rest API方式进行提交</h2> 
<p>此方式综合了flinksql和flinkjar的两种形式，你也以在远方编写flinksql，然后通过调用API的形式将FlinkSql和参数发送到flink集群上可执行的jar。jar拿到参数组装flink任务并提交。</p> 
<p>Rest API官网可参考<a href="https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/" rel="nofollow" title="REST API | Apache Flink">REST API | Apache Flink</a></p> 
<p>java编写一个接受参数的jar，模版可参考</p> 
<pre><code class="language-java">public class SqlTemplate {
    public static void main(String[] args) throws Exception {
        ParameterTool parameters = ParameterTool.fromArgs(args);//获取传递的参数
        String arg = parameters.get("arg",null);
        if(arg == null){
            return ;
        }
        arg = URLDecoder.decode(arg, StandardCharsets.UTF_8.toString());//URLDecoder解码
        String[] programArgs =  arg.split("\\|\\|");
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        //设置重启策略，最多重启三次，每次间隔5秒钟
        env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,                    Time.seconds(5)
        ));
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);
        Configuration configuration = tableEnv.getConfig().getConfiguration();
        //任务名称设定
        configuration.setString("pipeline.name",programArgs[0]);
        // 任务并行度设定
        env.setParallelism(Integer.parseInt(programArgs[1]));
        //任务类型，流式任务强制开启checkpoint
        if("stream".equals(programArgs[2])){
            //检查点设定
            if(!StringUtils.isNullOrWhitespaceOnly(programArgs[3])){
                CheckPoint cp = JSON.parseObject(programArgs[3],CheckPoint.class);
                //开启检查点
                if(cp.getEnable()){
                    //开启检查点，1S一次
                    env.enableCheckpointing(cp.getCheckpointInterval());
                    //检查点策略 EXACTLY_ONCE 精准一次  AT_LEAST_ONCE至少一次
                    env.getCheckpointConfig().setCheckpointingMode(cp.getCheckPointingMode()==1?CheckpointingMode.EXACTLY_ONCE:CheckpointingMode.AT_LEAST_ONCE);
                     Checkpoint 必须在一分钟内完成，否则就会被抛弃
                    env.getCheckpointConfig().setCheckpointTimeout(cp.getCheckpointTimeout());
                     同一时间只允许一个 checkpoint 进行
                    env.getCheckpointConfig().setMaxConcurrentCheckpoints(cp.getMaxConcurrentCheckpoints());
                    //设置检查点保存位置
                    env.getCheckpointConfig().setCheckpointStorage(cp.getCheckpointDirectory());
                    //开启实验性的 unaligned checkpoints
                    if(cp.getUnalignedCheckpointsEnabled()){
                        env.getCheckpointConfig().enableUnalignedCheckpoints();
                    }
                }
            }else{//开启默认配置
                //开启检查点，5S一次
                env.enableCheckpointing(5000);
                //检查点策略 EXACTLY_ONCE 精准一次  AT_LEAST_ONCE至少一次
                env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
                 Checkpoint 必须在五分钟内完成，否则就会被抛弃
                env.getCheckpointConfig().setCheckpointTimeout(300000);
                 同一时间只允许一个 checkpoint 进行
                env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);
                //开启实验性的 unaligned checkpoints
                env.getCheckpointConfig().enableUnalignedCheckpoints();
            }
        }
        //可执行的SQL单节点执行
        String sql = programArgs[4];
        //特殊符号在连接器里都会被使用,采用双特殊符号进行分割
        String[] sqlExecu = sql.split(";;");
        List&lt;String&gt; create = new ArrayList&lt;&gt;();
        List&lt;String&gt; insert = new ArrayList&lt;&gt;();
        for (String script : sqlExecu) {
            if(!script.startsWith("insert") &amp;&amp; !script.startsWith("INSERT")){
                create.add(script);
            }else{
                insert.add(script);
            }
        }
        //可执行的SQL单节点执行
        create.forEach(tableEnv::executeSql);
        // 运行多条 INSERT 语句，将原表数据输出到多个结果表中
        StatementSet stmtSet = tableEnv.createStatementSet();
        insert.forEach(stmtSet::addInsertSql);
        //开始执行任务
        TableResult execute = stmtSet.execute();
     }</code></pre> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2dac9be5ebbef740dbeca0bb7fbd404a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">关于mac环境下的node版本管理工具n和nvm</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/cad1c6ca2c955020efb8019f460d93b4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Flink将数据流导入Doris</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>