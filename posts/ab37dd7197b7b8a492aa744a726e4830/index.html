<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Scrapy 分布式爬虫框架 Scrapy-Redis - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/ab37dd7197b7b8a492aa744a726e4830/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Scrapy 分布式爬虫框架 Scrapy-Redis">
  <meta property="og:description" content="github官网代码示例：https://github.com/rmax/scrapy-redis/blob/master/example-project/example/spiders/myspider_redis.py
什么是 Scrapy-Redis Scrapy-Redis 是一个基于 Scrapy 的扩展，用于实现分布式爬虫。它利用 Redis 作为分布式队列来共享待爬取的 URL 和去重数据，这样可以让多个爬虫实例（即多个爬虫节点）并行工作，从而实现大规模的分布式数据抓取。
把普通爬虫改造成分布式爬虫 使用普通爬虫，改造成分布式爬虫，更便于理解 1. 安装 scrapy_redis框架模块 pip install scrapy_redis 2. 爬虫类修改如下： import scrapy # --- 1. 导入分布式爬虫类 from scrapy_redis.spiders import RedisSpider # --- 2. 继承分布式爬虫类 class BaiduSpider(RedisSpider): name = &#34;baidu&#34; # --- 3. 注释原来普通爬虫的 allowed_domains，start_urls # allowed_domains = [&#34;baidu.com&#34;] # start_urls = [&#34;https://www.baidu.com&#34;] # --- 4. 设置redis的key，起始url就存在这个key里 redis_key = &#34;baidu&#34; # --- 5. 设置 __init__，固定写法如下 def __init__(self, *args, **kwargs): domain = kwargs.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-24T18:44:17+08:00">
    <meta property="article:modified_time" content="2024-08-24T18:44:17+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Scrapy 分布式爬虫框架 Scrapy-Redis</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>github官网代码示例：<a href="https://github.com/rmax/scrapy-redis/blob/master/example-project/example/spiders/myspider_redis.py" title="https://github.com/rmax/scrapy-redis/blob/master/example-project/example/spiders/myspider_redis.py">https://github.com/rmax/scrapy-redis/blob/master/example-project/example/spiders/myspider_redis.py</a></p> 
<h3>什么是 Scrapy-Redis</h3> 
<blockquote> 
 <p><code>Scrapy-Redis</code> 是一个基于 <code>Scrapy</code> 的扩展，用于实现分布式爬虫。它利用 Redis 作为分布式队列来共享待爬取的 URL 和去重数据，这样可以让多个爬虫实例（即多个爬虫节点）并行工作，从而实现大规模的分布式数据抓取。</p> 
</blockquote> 
<h3>把普通爬虫改造成分布式爬虫</h3> 
<h4>使用普通爬虫，改造成分布式爬虫，更便于理解</h4> 
<h4>1. 安装 scrapy_redis框架模块</h4> 
<pre><code class="language-python">pip install scrapy_redis</code></pre> 
<h4>2. 爬虫类修改如下：</h4> 
<pre><code class="language-python">import scrapy
# --- 1. 导入分布式爬虫类
from scrapy_redis.spiders import RedisSpider


# --- 2. 继承分布式爬虫类
class BaiduSpider(RedisSpider):
    name = "baidu"

    # --- 3. 注释原来普通爬虫的 allowed_domains，start_urls
    # allowed_domains = ["baidu.com"]
    # start_urls = ["https://www.baidu.com"]

    # --- 4. 设置redis的key，起始url就存在这个key里
    redis_key = "baidu"

    # --- 5. 设置 __init__，固定写法如下
    def __init__(self, *args, **kwargs):
        domain = kwargs.pop('domain', '')
        self.allowed_domains = list(filter(None, domain.split(',')))  # 获取启动爬虫命令时输入的域名
        super().__init__(*args, **kwargs)

    def parse(self, response):
        print("解析数据：", response.xpath('//title/text()').get())</code></pre> 
<h4>3. 配置 settings.py 文件</h4> 
<pre><code class="language-bash"># myproject2 是项目名称
SPIDER_MODULES = ["myproject2.spiders"]
NEWSPIDER_MODULE = "myproject2.spiders"

# USER_AGENT = "scrapy-redis (+https://github.com/rolando/scrapy-redis)"
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36"

# 重复过滤器使用的模块
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
# 设置调度器，scrapy_redis中的调度器具备与数据库交互的功能
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
# 保持任务队列，当程序结束时不清空 Redis 中的队列
SCHEDULER_PERSIST = True

# 设置任务队列使用的类型，可选的类型有：
# SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderPriorityQueue"  # 基于优先级的队列
# SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderQueue"          # 基于 FIFO 的队列
# SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderStack"          # 基于 LIFO 的队列

ITEM_PIPELINES = {
    # "myproject2.pipelines.ExamplePipeline": 300,
    "scrapy_redis.pipelines.RedisPipeline": 400,
}

# 配置 redis 连接信息
REDIS_HOST = "localhost"
REDIS_PORT = 6379
# REDIS_DB = 0
# REDIS_PASSWORD = "123456"

LOG_LEVEL = "DEBUG"

# Introduce an artifical delay to make use of parallelism. to speed up the
# crawl.
# 延迟时间，单位秒
DOWNLOAD_DELAY = 1</code></pre> 
<h4>4. 启动分布式爬虫</h4> 
<p>在爬虫类所在目录打开多个命令行窗口，使用如下命令启动爬虫</p> 
<pre><code class="language-bash"># scrapy runspider 爬虫类
scrapy runspider baidu.py</code></pre> 
<p>如下：表示启动了多个程序，一起用于对这个爬虫进行爬取</p> 
<p><img alt="" height="758" src="https://images2.imgbox.com/a7/33/Uaq30ij5_o.png" width="956"></p> 
<h4>5. 向redis中添加爬取的url</h4> 
<p>向redis中添加爬取的url，启动的爬虫会从redis中读取url进行爬取</p> 
<pre><code class="language-bash"># lpush key value1 value2 value3
lpush baidu "https://www.baidu.com"</code></pre> 
<h4>6. 效果</h4> 
<p><img alt="" height="639" src="https://images2.imgbox.com/5f/4e/5F0MqTlx_o.png" width="953"></p> 
<p>向 redis 中存入多个url后，可以看到有多个窗口同时爬取不同url的数据</p> 
<pre><code class="language-bash">lpush baidu "https://www.baidu.com" "https://tieba.baidu.com/f?kw=沙井"</code></pre> 
<p><img alt="" height="765" src="https://images2.imgbox.com/d4/50/ikbUq5wA_o.png" width="954"></p> 
<h3>实际使用场景</h3> 
<p>当一个网站的数据特别多的时候，有很多分页，或者下拉分页。通过对分页比较找出分页规则，通过分页规则计算拿到所有的分页地址。把所有的分页地址存到 redis 中</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ae988296a7b0f64c07d87f28eeb7e562/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">堆《数据结构》</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/30cee809c0fc1c84a49f7147b96126b3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">防患未然：构建AIGC时代下开发团队应对突发技术故障与危机的全面策略</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>