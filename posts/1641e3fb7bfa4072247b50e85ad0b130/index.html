<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python 爬虫项目实战（二）：爬取微博热搜榜 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/1641e3fb7bfa4072247b50e85ad0b130/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Python 爬虫项目实战（二）：爬取微博热搜榜">
  <meta property="og:description" content="前言 网络爬虫（Web Crawler），也称为网页蜘蛛（Web Spider）或网页机器人（Web Bot），是一种按照既定规则自动浏览网络并提取信息的程序。爬虫的主要用途包括数据采集、网络索引、内容抓取等。
爬虫的基本原理
种子 URL：爬虫从一个或多个种子 URL 开始，这些 URL 是起点。发送请求：爬虫向这些种子 URL 发送 HTTP 请求，通常是 GET 请求。获取响应：服务器返回网页的 HTML 内容作为响应。解析内容：爬虫解析 HTML 内容，提取所需的数据（如文本、链接、图片等）。提取链接：从网页中提取出所有链接，并将这些链接加入待访问队列。重复过程：爬虫重复上述步骤，直到达到某个停止条件，如爬取了一定数量的页面，或所有页面都被爬取完毕。 爬虫的分类
通用爬虫
设计用于抓取整个互联网的大量网页。搜索引擎（如 Google、Bing）的爬虫就是通用爬虫。 聚焦爬虫
专注于特定主题或领域，抓取相关网页。比如，一个新闻爬虫只抓取新闻网站的内容。 增量爬虫
仅抓取自上次爬取以来发生变化或更新的网页，适用于动态内容更新频繁的网站。 爬虫的合法性和道德
在编写和运行爬虫时，必须遵循以下原则：
遵守网站的 robots.txt：
大多数网站都有一个 robots.txt 文件，规定了哪些页面允许被爬取，哪些不允许。爬虫应当尊重这些规则。
避免过度抓取：
设置适当的抓取频率，避免对服务器造成过大负担。 尊重版权和隐私：
不应抓取或使用受版权保护的内容，或涉及用户隐私的数据。 获取许可：
在某些情况下，最好获得网站管理员的许可，特别是当你打算频繁地抓取大量数据时。 通过以上方法和原则，可以编写高效、可靠且合规的网络爬虫来满足数据采集的需求。 侦察 这个比较简单，直接抓包就可以看到数据来源
保存请求网址 检查数据在页面标签中的位置
标题在 td-02 中 热度在 span 中
源代码 import parsel import csv import requests # 创建 csv 文件 # &#39;a&#39; 表示以追加模式（append mode）打开文件 # newline=&#39;&#39;的作用是确保在写入文件时，所有的换行符都使用&#39;\n&#39; f = open(&#39;res.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-02T17:03:16+08:00">
    <meta property="article:modified_time" content="2024-08-02T17:03:16+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python 爬虫项目实战（二）：爬取微博热搜榜</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>前言</h2> 
<p>网络爬虫（Web Crawler），也称为网页蜘蛛（Web Spider）或网页机器人（Web Bot），是一种按照既定规则自动浏览网络并提取信息的程序。爬虫的主要用途包括数据采集、网络索引、内容抓取等。</p> 
<p><strong>爬虫的基本原理</strong></p> 
<ol><li><strong>种子 URL</strong>：爬虫从一个或多个种子 URL 开始，这些 URL 是起点。</li><li><strong>发送请求</strong>：爬虫向这些种子 URL 发送 HTTP 请求，通常是 GET 请求。</li><li><strong>获取响应</strong>：服务器返回网页的 HTML 内容作为响应。</li><li><strong>解析内容</strong>：爬虫解析 HTML 内容，提取所需的数据（如文本、链接、图片等）。</li><li><strong>提取链接</strong>：从网页中提取出所有链接，并将这些链接加入待访问队列。</li><li><strong>重复过程</strong>：爬虫重复上述步骤，直到达到某个停止条件，如爬取了一定数量的页面，或所有页面都被爬取完毕。</li></ol> 
<p><strong>爬虫的分类</strong></p> 
<ol><li> <p><strong>通用爬虫</strong></p> 
  <ul><li>设计用于抓取整个互联网的大量网页。搜索引擎（如 Google、Bing）的爬虫就是通用爬虫。</li></ul></li><li> <p><strong>聚焦爬虫</strong></p> 
  <ul><li>专注于特定主题或领域，抓取相关网页。比如，一个新闻爬虫只抓取新闻网站的内容。</li></ul></li><li> <p><strong>增量爬虫</strong></p> 
  <ul><li>仅抓取自上次爬取以来发生变化或更新的网页，适用于动态内容更新频繁的网站。</li></ul></li></ol> 
<p><strong>爬虫的合法性和道德</strong></p> 
<p>在编写和运行爬虫时，必须遵循以下原则：</p> 
<ol><li> <p><strong>遵守网站的 <code>robots.txt</code></strong>：</p> 
  <ul><li> <p>大多数网站都有一个 <code>robots.txt</code> 文件，规定了哪些页面允许被爬取，哪些不允许。爬虫应当尊重这些规则。</p> </li></ul></li><li> <p><strong>避免过度抓取</strong>：</p> 
  <ul><li>设置适当的抓取频率，避免对服务器造成过大负担。</li></ul></li><li> <p><strong>尊重版权和隐私</strong>：</p> 
  <ul><li>不应抓取或使用受版权保护的内容，或涉及用户隐私的数据。</li></ul></li><li> <p><strong>获取许可</strong>：</p> 
  <ul><li>在某些情况下，最好获得网站管理员的许可，特别是当你打算频繁地抓取大量数据时。</li></ul></li></ol> 
<p>通过以上方法和原则，可以编写高效、可靠且合规的网络爬虫来满足数据采集的需求。 </p> 
<h2>侦察</h2> 
<p>这个比较简单，直接抓包就可以看到数据来源</p> 
<p class="img-center"><img alt="" height="1041" src="https://images2.imgbox.com/4c/26/3i5NIU72_o.png" width="1200"></p> 
<p>保存请求网址 </p> 
<p class="img-center"><img alt="" height="385" src="https://images2.imgbox.com/21/48/vKD3ACyZ_o.png" width="853"></p> 
<p>检查数据在页面标签中的位置</p> 
<p class="img-center"><img alt="" height="905" src="https://images2.imgbox.com/50/f5/p1VF6wfa_o.png" width="1200"></p> 
<p>标题在 td-02 中 </p> 
<p class="img-center"><img alt="" height="210" src="https://images2.imgbox.com/f0/b5/7epVB5xX_o.png" width="1200"></p> 
<p>热度在 span 中</p> 
<p class="img-center"><img alt="" height="181" src="https://images2.imgbox.com/5b/28/h43I7ZW2_o.png" width="1200"></p> 
<h2>源代码</h2> 
<pre><code class="language-python">import parsel
import csv
import requests


# 创建 csv 文件
# 'a' 表示以追加模式（append mode）打开文件
# newline=''的作用是确保在写入文件时，所有的换行符都使用'\n'
f = open('res.csv', 'a', encoding='utf-8', newline='')

# csv.DictWriter 类用于将字典格式的数据写入 CSV 文件
# 每个字典表示一行，字典的键对应 CSV 文件的列名
csv_writer = csv.DictWriter(f, fieldnames=['排名', '标题', '热度'])

# 写入表头行，包含指定的字段名
csv_writer.writeheader()

# 请求地址
url = 'https://s.weibo.com/top/summary?cate=realtimehot'

# 伪造请求头及 cookie
headers = {
    'cookie': 'SUB=_2AkMR8Bzkf8NxqwFRmf0XzGvjb4x3zwHEieKnrO0_JRMxHRl-yT9kqmMHtRB6OnAyC3ZtjaT5q1jwM0_aHrCMEvlnAj-o; SUBP=0033WrSXqPxfM72-Ws9jqgMF55529P9D9WFTP2MxWPjMdfqH2lQ8Jx9_; _s_tentry=passport.weibo.com; Apache=9034410052178.598.1722586067660; SINAGLOBAL=9034410052178.598.1722586067660; ULV=1722586067671:1:1:1:9034410052178.598.1722586067660:',
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36'
}

response = requests.get(url, headers=headers)

# 使用 parsel.Selector 类将响应文本传递给 Selector 对象
# Selector 对象能够解析和处理 HTML 或 XML 文本
selector = parsel.Selector(response.text)

# 选择 &lt;div id='pl_top_realtimehot'&gt; 中的 tbody 中的 tr
trs = selector.css('#pl_top_realtimehot tbody tr')

# 定义排序
num = 1

for tr in trs:
    # .get() 方法从 SelectorList 中提取第一个匹配项的内容。如果没有匹配项，它将返回 None
    # 获取热搜标题
    title = tr.css('.td-02 a::text').get()

    # 获取热搜热度
    hot = tr.css('.td-02 span::text').get()

    # 创建字典保存数据
    dic = {
        '排名': num,
        '标题': title,
        '热度': hot,
    }

    print(dic)

    # writerow 写入数据
    csv_writer.writerow(dic)

    num += 1
</code></pre> 
<h2>项目效果</h2> 
<p class="img-center"><img alt="" height="388" src="https://images2.imgbox.com/13/c2/mQTYsfvg_o.png" width="666"></p> 
<p class="img-center"><img alt="" height="674" src="https://images2.imgbox.com/a5/b2/asdVOx4y_o.png" width="500"></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3170bc23d49a6b7ad84d994feaddf9ab/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">现在有什么赛道可以干到退休？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/41f06d37bd1be36f8538671d5ec08099/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【数据结构】——链式二叉树</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>