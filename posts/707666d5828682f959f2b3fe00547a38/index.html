<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AI/机器学习（计算机视觉/NLP）方向面试复习3 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/707666d5828682f959f2b3fe00547a38/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="AI/机器学习（计算机视觉/NLP）方向面试复习3">
  <meta property="og:description" content="1. Pooling 有哪些方式？pytorch的实现？ Pooling可以分成：最大池化，平均池化，全局平均池化，随机池化，空间金字塔池化。
1. 最大池化（Max Pooling） 最大池化是最常用的池化技术之一。它将输入图像划分为若干非重叠的矩形区域，然后对每个区域输出最大值。这种方法在实践中非常有效，能够很好地捕捉图像中的显著特征。
2. 平均池化（Average Pooling） 平均池化也将输入划分为多个区域，但它输出的是这些区域内的平均值。相较于最大池化，平均池化更平滑，但可能会丢失一些细节信息，因为它不像最大池化那样能突出显著特征。
3. 全局平均池化（Global Average Pooling） 全局平均池化是一种极端形式的平均池化，它计算整个特征图的平均值，通常用于卷积神经网络的最后阶段，直接输出用于分类的特征。这种方法能够显著减少模型的参数数量。
4. 随机池化（Stochastic Pooling） 随机池化是一种概率论的池化方法，不是简单地取最大值或平均值，而是根据预定义的概率分布（通常基于输入特征的大小）来选择池化区域内的元素。这种方法有助于增加模型的泛化能力，因为它引入了随机性。
6. 空间金字塔池化（Spatial Pyramid Pooling） 空间金字塔池化（SPP）是一种灵活的池化策略，它可以保持空间层次结构，允许网络接收任意大小的输入。SPP 通过在多个尺度上实现池化来捕获多层次的特征，这在一些特定的场景中非常有用，比如在需要处理不同分辨率的图像时。
2. attention的各种变形 self-attention里存在的问题是，当序列长度N非常大时，通过query(N,S) key(S,N) 相乘得到的Attention matrix(N,N)矩阵非常大。这里的计算过程非常复杂，就需要对self-attention进行简化。
而且这种简化经常会用在图像处理上，因为图像输入256×256的patch时，按像素为1个单位，N=256*256，过于大了。
一个方法是用感受野。把Attention matrix除了感受野以外的值设为0：（local attention）
但是这样设置感受野后，就和CNN没什么区别了。所以不太好。
Stride Attention：类似空洞卷积。每次看多几格的内容，例如空两格看三格以外的内容。
Local Attention：在原来的sequence里加上一个特殊的token。只有global token能获取所有的信息，其他token之间就不有attention了。Longformer用到了Global attention和striding attention， Big Bird用了global attention&#43;striding attention和random attention。
Reformer：如何在Attention Matrix里，判断哪些地方有比较大的attention value，哪些地方的attention value比较小？然后把value比较大的取出来，当成attention。
这样做的方式是对query和key进行clustering聚类。clustering的加速方法有很多，可以快速将相近的query和key放在一起。只计算同类的query和key的attention，不同类的query和key的attention位为0，可以加速。
但这样是人类判断方式，根据任务判断两者之间是否相近。同样，也可以实现一个神经网络来判断key，value之间是否相近：Sinkhorn Sorting Network。
Linformer：本质在说attention matrix是一个低秩矩阵，列之间相关性很大，根本不需要构建这么大的attention matrix。就对列进行压缩。具体做法是从key中找到representative keys。
具体的压缩方法有：（1）对key做卷积进行压缩，（2）key是N维的，直接乘一个(N×K)的矩阵
k,q first -&gt; v,k first 最后一个点是，当matrix相乘的顺序不同时，计算的效率也不一样。KQ先相乘再乘V比下面：先V乘K再乘Q的效率大很多。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-02T11:20:19+08:00">
    <meta property="article:modified_time" content="2024-08-02T11:20:19+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AI/机器学习（计算机视觉/NLP）方向面试复习3</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>1. Pooling 有哪些方式？pytorch的实现？</h3> 
<p>Pooling可以分成：最大池化，平均池化，全局平均池化，随机池化，空间金字塔池化。</p> 
<h5>1. 最大池化（Max Pooling）</h5> 
<p>最大池化是最常用的池化技术之一。它将输入图像划分为若干非重叠的矩形区域，然后对每个区域输出最大值。这种方法在实践中非常有效，能够很好地<strong>捕捉图像中的显著特征</strong>。</p> 
<h5>2. 平均池化（Average Pooling）</h5> 
<p>平均池化也将输入划分为多个区域，但它输出的是这些区域内的平均值。相较于最大池化，平均池化<strong>更平滑，但可能会丢失一些细节信息</strong>，因为它不像最大池化那样能突出显著特征。</p> 
<h5>3. 全局平均池化（Global Average Pooling）</h5> 
<p>全局平均池化是一种极端形式的平均池化，它计算整个特征图的平均值，<strong>通常用于卷积神经网络的最后阶段，直接输出用于分类的特征</strong>。这种方法能够显著减少模型的参数数量。</p> 
<h5>4. 随机池化（Stochastic Pooling）</h5> 
<p>随机池化是一种概率论的池化方法，不是简单地取最大值或平均值，而是根据预定义的概率分布（通常基于输入特征的大小）来选择池化区域内的元素。这种方法有助于<strong>增加模型的泛化能力</strong>，因为它引入了随机性。</p> 
<h5>6. 空间金字塔池化（Spatial Pyramid Pooling）</h5> 
<p>空间金字塔池化（SPP）是一种灵活的池化策略，它可以保持空间层次结构，允许网络接收任意大小的输入。SPP 通过在多个尺度上实现池化来捕获多层次的特征，这在一些特定的场景中非常有用，比如在需要处理不同分辨率的图像时。</p> 
<h3>2. attention的各种变形</h3> 
<p>self-attention里存在的问题是，当序列长度N非常大时，通过query(N,S) key(S,N) 相乘得到的Attention matrix(N,N)矩阵非常大。这里的计算过程非常复杂，就需要对self-attention进行简化。</p> 
<p>而且这种简化经常会用在图像处理上，因为图像输入256×256的patch时，按像素为1个单位，N=256*256，过于大了。</p> 
<p>一个方法是用感受野。把Attention matrix除了感受野以外的值设为0：（<span style="color:#ed7976;"><strong>local attention</strong></span>）</p> 
<p><img alt="" height="735" src="https://images2.imgbox.com/2a/b4/cT6SEUEb_o.png" width="1200"></p> 
<p>但是这样设置感受野后，就和CNN没什么区别了。所以不太好。</p> 
<p><span style="color:#be191c;"><strong>Stride Attention：</strong></span>类似空洞卷积。每次看多几格的内容，例如空两格看三格以外的内容。</p> 
<p><img alt="" height="581" src="https://images2.imgbox.com/bb/51/LpVzw9OB_o.png" width="841"></p> 
<p><span style="color:#be191c;"><strong>Local Attention：</strong></span>在原来的sequence里加上一个特殊的token。只有global token能获取所有的信息，其他token之间就不有attention了。Longformer用到了Global attention和striding attention， Big Bird用了global attention+striding attention和random attention。</p> 
<p><span style="color:#be191c;"><strong>Reformer：</strong></span>如何在Attention Matrix里，判断哪些地方有比较大的attention value，哪些地方的attention value比较小？然后把value比较大的取出来，当成attention。</p> 
<p>这样做的方式是对query和key进行clustering<strong>聚类</strong>。clustering的加速方法有很多，可以快速将相近的query和key放在一起。<strong>只计算同类的query和key的attention，</strong>不同类的query和key的attention位为0，可以加速。</p> 
<p>但这样是人类判断方式，根据任务判断两者之间是否相近。同样，也可以实现一个神经网络来判断key，value之间是否相近：<span style="color:#be191c;"><strong>Sinkhorn Sorting Network。</strong></span></p> 
<p><img alt="" height="756" src="https://images2.imgbox.com/5e/90/aGcQBdpT_o.png" width="960"></p> 
<p><span style="color:#be191c;"><strong>Linformer：</strong></span>本质在说attention matrix是一个低秩矩阵，列之间相关性很大，根本不需要构建这么大的attention matrix。就对列进行压缩。具体做法是从key中找到representative keys。</p> 
<p>具体的压缩方法有：（1）对key做卷积进行压缩，（2）key是N维的，直接乘一个(N×K)的矩阵</p> 
<p><img alt="" height="794" src="https://images2.imgbox.com/c1/64/8xxfdjHY_o.png" width="746"></p> 
<p><span style="color:#be191c;"><strong>k,q first -&gt; v,k first </strong></span>最后一个点是，当matrix相乘的顺序不同时，计算的效率也不一样。KQ先相乘再乘V比下面：先V乘K再乘Q的效率大很多。</p> 
<p><img alt="" height="749" src="https://images2.imgbox.com/51/84/9rNHATAI_o.png" width="1074"></p> 
<p></p> 
<h3>3. 如何输入数据同时包括float类型的数据和文本数据，如何将它们都输入到网络里？</h3> 
<p>首先分别处理：</p> 
<p>（1）对浮点数进行归一化和标准化处理</p> 
<p>（2）对文本数据进行向量化处理，如使用<strong>词袋模型（BOW）</strong>、TF-IDF、词向量（如Word2Vec、GloVe）或更高级的BERT等方式。</p> 
<p>然后进行特征合并:</p> 
<p>将预处理后的浮点数和文本数据连接起来</p> 
<p>可以用一个embedding层提取文本数据，再用一个dense层处理浮点型数据，然后用concatenate层连接起来。</p> 
<h3>4. 如何判断两个句子之间的相似性？</h3> 
<p>（1）余弦相似度。将两个向量之间的夹角。可以用来比较两个句子在向量之间的相似性。首先把句子转成TF-IDF向量，计算余弦相似度。</p> 
<p>（2）Jaccard相似度：比较两个句子的词的集合的相似性。它是度量两个集合交集和并集的比率。</p> 
<p>（3）基于词向量的方法：word2vec，转成向量后计算词向量的平均值。</p> 
<p>（4）基于深度学习的方法：BERT</p> 
<h3>5. 如何去除噪声或异常点？</h3> 
<p>（1）<strong>统计方法，可以用均值和标准差来计算。</strong></p> 
<p>均值去除方法：计算数据的均值，减去均值来中心化数据。</p> 
<p>标准化方法：将数据减去均值后除以标准差，使得数据具有零均值和单位标准差。</p> 
<p>Z-Score：利用zscore检测异常值，通常设定一个阈值，超出这个阈值的点被视为异常点。</p> 
<p><strong>四分位数计算法：</strong>（IQR）计算数据的四分位数和四分位距，低于Q1-1.5*IQR或者高于Q3+1.5*IQR的点被视为异常。</p> 
<p>（2）<strong>滤波方法：</strong></p> 
<p><strong>简单移动平均（SMA）</strong>：计算一个窗口内数据点的平均值，并且用这个平均值平滑数据。</p> 
<p>找一个滤波核，无论是一维二维都可以通过移动窗口进行平滑。</p> 
<p><strong>卡尔曼滤波：</strong>一种递归滤波器。</p> 
<p>（3）机器学习方法：</p> 
<p>聚类 K-means方法：检查哪些数据点不属于任何聚类中心。</p> 
<p>孤立森林：基于决策树的方法，构建树并且根据树的深度检测异常点。</p> 
<p>（4）小波变换：</p> 
<p>将数据转换到频域，根据频域计算</p> 
<h3>6. cross attention是什么，python实现</h3> 
<p>cross attention与selfattention的区别在于，cross attention是针对两个序列的。两个相同维度的序列，一个作为输入的Q，另一个提供K和V。</p> 
<p>算法基本上与self attention一致，就是输入的两个序列x1,x2,序列长度不一样，可是维度一样（embed size），所以输出的是x2的序列长度。</p> 
<pre><code class="language-python">class crossAttention(nn.Module):
    def __init__(self,input_size): # (Batchsize, sequencelength, embedsize)
        super(crossAttention,self).__init__()
        self.queryl = nn.Linear(input_size, input_size)
        self.keyl = nn.Linear(input_size, input_size)
        self.valuel = nn.Linear(input_size, input_size)

    def forward(self,x1, x2):
        batch_size, seq_len, embed_size = x1.shape # B,T,C
        batch_size, seq_len2, embed_size = x2.shape # B, T2,C
        Q = self.queryl(x1)
        K = self.keyl(x2)
        V = self.valuel(x2)
        mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)
        QK = Q@K.transpose(-2,-1)/torch.sqrt(seq_len)
        mask_QK = QK.masked_fill(mask, float("-inf"))
        attention_weights = F.softmax(mask_QK, dim = -1) # (B,T,C)@(B,C,T2) = (B,T,T2)
        output = attention_weights @V # (B,T,T2)@(B,T2,C) = (B,T,C) # 最后的序列长度是x1的T
        return output #(B,T,C)</code></pre> 
<h3>7. c++一些记不起来的用法（先写在这，后面汇总）</h3> 
<p>（1）unordered_map 和set 自定义比较函数</p> 
<p>优先队列是放一组数据并且根据某个值排序，unordered map是不能直接排序的，要取出来放在vector里再做排序：</p> 
<pre><code class="language-cpp">bool cmp(pair&lt;int, vector&lt;int&gt;&gt; a, pair&lt;int, vector&lt;int&gt;&gt; b) {
	return a.first &gt; b.first;
}
int main() {
	vector&lt;pair&lt;int, vector&lt;int&gt;&gt;&gt; a;
	sort(a.begin(), a.end(), cmp);
}
</code></pre> 
<p>（2）vector初始化</p> 
<pre><code class="language-cpp">	vector&lt;vector&lt;int&gt;&gt; n = vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(m, 0));</code></pre> 
<p>（3）int二维数组初始化</p> 
<pre><code class="language-cpp">int** array = new int*[rows];</code></pre> 
<p>（4）int怎么转long</p> 
<p>强制类型转换：</p> 
<pre><code class="language-cpp">// int -》 long
int a = 1000;
long b = a;
// long -》 int
long a = 1000;
int b = (int) a;</code></pre> 
<p>（5）vector判断一个元素是否在里面</p> 
<pre><code class="language-cpp">	// 查找是否有5这个元素
	vector&lt;int&gt;::iterator it;
	it = find(v.begin(), v.end(), 5);
    if (it == v.end()){
        return false;
    }else{
        return true;
    }</code></pre> 
<p>（6）unordered_map迭代</p> 
<pre><code class="language-cpp">unordered_map&lt;int,int&gt; mp;
for(auto::iterator it = mp.begin();it!=mp.end();it++){
    cout&lt;&lt;it-&gt;first&lt;&lt;it-&gt;second&lt;&lt;endl;
}</code></pre> 
<h3>8. python的ACM输入形式</h3> 
<p>一直没理解该怎么搞，当python输入是line，但是题目规定多个输入，第一行输入m表示样例个数，后面m行表示输入数据</p> 
<p>一般题目里给这些：</p> 
<pre><code class="language-python">import sys

for line in sys.stdin:
    a = line.split()
    print(int(a[0]) + int(a[1]))</code></pre> 
<p>但是这个输入是没法处理的，要自己写输入：用<strong>readline().strip()</strong></p> 
<pre><code class="language-python">import sys

# 读取第一行,获取case的数量
num_cases = int(sys.stdin.readline().strip())
print(num_cases)
# # 循环处理每个case
for _ in range(num_cases):
    # 读取一行输入并拆分
    line = sys.stdin.readline().strip()
    x = int(line[0])
    print(x)</code></pre> 
<h3>9. 焊点题（文远知行笔试）</h3> 
<p>题目大意是，给一个n*m的焊板，和两个焊头的起始位置(x1,y1),(x2,y2), 两个焊头只能朝着同一个方向移动，并且任意一个焊头都不能出界，求最后焊板上有几个位置焊不到。</p> 
<p>这题给我的感觉就是一定要想好了再动笔，我以为很简单就是个dfs，结果dfs超时，并且矩阵构建内存也不够，改了链表还是超时。</p> 
<p>理一下思路：</p> 
<p>根据两个点的坐标，找到它们离上下左右四个点的距离，然后求出两个正方形，分别是它们两个焊头能移动的范围。最后的结果是总面积-（两个矩阵的面积-两个矩阵重合的面积）</p> 
<p>重点是两个重合的面积怎么求。</p> 
<pre><code class="language-cpp">#include &lt;iostream&gt;
#include &lt;unordered_map&gt;
#include &lt;algorithm&gt;
using namespace std;

int area(int x1, int y1, int x2, int y2) {
	int width = abs(x1 - x2) +1;
	int height = abs(y1 - y2) +1;
	return width * height;
}
int main() {
	int n, m, x1, x2, y1, y2;
	//cin &gt;&gt; n &gt;&gt; m &gt;&gt; x1 &gt;&gt; y1 &gt;&gt; x2 &gt;&gt; y2;
	n = 4, m = 4, x1 = 0, y1 = 0, x2 = 2, y2 = 2;
	int minw1 = min(x1, x2);
	int minw2 = min(n-1 - x1, n-1 - x2);
	int minh1 = min(y1, y2);
	int minh2 = min(m - 1 - y1, m - 1 - y2);


	int A1 = area(x1-minw1, y1-minh1, x1+minw2, y1+minh2);
	int A2 = area(x2 - minw1, y2 - minh1, x2 + minw2, y2 + minh2);
	cout &lt;&lt; x1 - minw1 &lt;&lt; " " &lt;&lt; y1 - minh1 &lt;&lt; " " &lt;&lt; x1 + minw2 &lt;&lt; " " &lt;&lt; y1 + minh2 &lt;&lt; endl;
	cout &lt;&lt; x2 - minw1 &lt;&lt; " " &lt;&lt; y2 - minh1 &lt;&lt; " " &lt;&lt; x2 + minw2 &lt;&lt; " " &lt;&lt; y2 + minh2 &lt;&lt; endl;
	int x1_L = x1 - minw1,y1_L = y1 - minh1, x1_R = x1 + minw2, y1_R = y1 + minh2;
	int x2_L = x2 - minw1, y2_L = y2 - minh1, x2_R = x2 + minw2, y2_R = y2 + minh2;
	if ((x1_R &lt;= x2_L &amp;&amp; y1_R &lt;= y2_L)) {
		// 没重叠
		cout&lt;&lt; n * m - (A1 + A2) &lt;&lt; endl;
	}
	else {
		int overlap = area(max(x1 - minw1, x2 - minw1), max(y1 - minh1, y2 - minh1), min(x1 + minw2, x2 + minw2), min(y1 + minh2, y2 + minh2));
		cout &lt;&lt; overlap &lt;&lt; endl;
		cout &lt;&lt; n * m - (A1 + A2 - overlap) - 2 &lt;&lt; endl;

	}
}</code></pre> 
<h3 style="background-color:transparent;">10. 手写一个tensorflow的训练和测试</h3> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6b26dc84a8f497b75b58cca1fe4d4e90/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">数据结构与算法——矩阵</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/08cdec4e151e8e7d78eff3bae3237b9d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">校园课程助手【3】-使用枚举类封装异常优雅处理全局异常</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>