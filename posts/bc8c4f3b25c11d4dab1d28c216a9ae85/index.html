<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Spark编程基础（Python版）实验三RDD编程 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/bc8c4f3b25c11d4dab1d28c216a9ae85/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Spark编程基础（Python版）实验三RDD编程">
  <meta property="og:description" content="提示：文章写完后，目录可以自动生成，如何生成可参考右边的帮助文档
文章目录 一、实验环境二、实验流程 一、实验环境 Ubuntu18.04
Spark 2.4.0
Python 3.6.5
二、实验流程 1.PySpark交互式编程 在 spark下创建文件夹sparksqldata，将data01.txt上传到sparksqldata下：
cd /usr/local/spark mkdir sparksqldata cd /bin ./pyspark （1）统计学生人数（即文件的行数）
lines = sc.textFile(&#34;file:///usr/local/spark/sparksqldata/Data01.txt&#34;) res = lines.map(lambda x:x.split(&#34;,&#34;)).map(lambda x: x[0]) //获取每行数据的第1列 distinct_res = res.distinct() //去重操作 distinct_res.count()//取元素总个数 (2)统计开设课程总数
lines = sc.textFile(&#34;file:///usr/local/spark/sparksqldata/data01.txt&#34;) df = lines.map(lambda x:x.split(&#34;,&#34;)).map(lambda x:x[1]) df1 = df.distinct() df1.count() （3）计算Tom所有课程的平均分
lines = sc.textFile(&#34;file:///usr/local/spark/sparksqldata/data01.txt&#34;) res = lines.map(lambda x:x.split(&#34;,&#34;)).filter(lambda x:x[0]==&#34;Tom&#34;) res.foreach(print) score = res.map(lambda x:int(x[2])) num = res.count() sum_score = score.reduce(lambda x,y:x&#43;y) avg = sum_score/num print(avg) （4）计算每一个人的选课总数">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-05T01:10:41+08:00">
    <meta property="article:modified_time" content="2024-05-05T01:10:41+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Spark编程基础（Python版）实验三RDD编程</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <blockquote> 
 <p>提示：文章写完后，目录可以自动生成，如何生成可参考右边的帮助文档</p> 
</blockquote> 
<p></p> 
<div> 
 <h4>文章目录</h4> 
 <ul><li><a href="#pandas_16" rel="nofollow">一、</a><span style="color:#6eaad7;">实验环境</span></li><li><a href="#_19" rel="nofollow">二、</a><span style="color:#6eaad7;">实验流程</span></li></ul> 
</div> 
<h2>一、实验环境</h2> 
<p>           Ubuntu18.04</p> 
<p>           Spark 2.4.0</p> 
<p>           Python 3.6.5</p> 
<h2>二、实验流程</h2> 
<h4><strong>1.PySpark交互式编程</strong></h4> 
<p>在 spark下创建文件夹sparksqldata，将data01.txt上传到sparksqldata下：</p> 
<pre><code>cd /usr/local/spark
mkdir sparksqldata
cd /bin
./pyspark</code></pre> 
<p>（1）统计学生人数（即文件的行数）</p> 
<pre><code class="language-python">lines = sc.textFile("file:///usr/local/spark/sparksqldata/Data01.txt")
res = lines.map(lambda x:x.split(",")).map(lambda x: x[0]) //获取每行数据的第1列 
distinct_res = res.distinct()  //去重操作
distinct_res.count()//取元素总个数</code></pre> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/c2/39/WhNHmnbN_o.png"></p> 
<p>(2)统计开设课程总数</p> 
<pre><code class="language-python">lines = sc.textFile("file:///usr/local/spark/sparksqldata/data01.txt")
df = lines.map(lambda x:x.split(",")).map(lambda x:x[1]) 
df1 = df.distinct()
df1.count()</code></pre> 
<p style="text-align:center;"> <img alt="" src="https://images2.imgbox.com/11/0e/thNjBBrC_o.png"></p> 
<p>（3）计算Tom所有课程的平均分</p> 
<pre><code class="language-python">lines = sc.textFile("file:///usr/local/spark/sparksqldata/data01.txt")
res = lines.map(lambda x:x.split(",")).filter(lambda x:x[0]=="Tom")
res.foreach(print)
score = res.map(lambda x:int(x[2]))
num = res.count()
sum_score = score.reduce(lambda x,y:x+y)                                   
avg = sum_score/num
print(avg)</code></pre> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/7a/b8/bx2FFRu8_o.png"></p> 
<p>（4）计算每一个人的选课总数</p> 
<pre><code class="language-python">lines = sc.textFile("file:///usr/local/spark/sparksqldata/data01.txt")
res = lines.map(lambda x:x.split(",")).map(lambda x:(x[0],1))
each_res = res.reduceByKey(lambda x,y: x+y) 
each_res.foreach(print)</code></pre> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/5c/c1/gbs1N8iB_o.png"></p> 
<p>（5）计算DataBase的选修人数</p> 
<pre><code class="language-python">lines = sc.textFile("file:///usr/local/spark/sparksqldata/data01.txt")
res = lines.map(lambda x:x.split(",")).filter(lambda x:x[1]=="DataBase")
res.count()</code></pre> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/4b/fc/3IklWiu3_o.png"></p> 
<p>（6）计算每门课程的平均分</p> 
<pre><code class="language-python">lines = sc.textFile("file:///usr/local/spark/sparksqldata/data01.txt")
res = lines.map(lambda x:x.split(",")).map(lambda x:(x[1],(int(x[2]),1)))
temp = res.reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1])) 
avg = temp.map(lambda x:(x[0], round(x[1][0]/x[1][1],2)))
avg.foreach(print)</code></pre> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/fd/fc/HwqmCIcV_o.png"></p> 
<p>（7） 使用累加器计算共有多少人选了DataBase这门课。</p> 
<pre><code class="language-python">lines = sc.textFile("file:///usr/local/spark/sparksqldata/data01.txt")
res = lines.map(lambda x:x.split(",")).filter(lambda x:x[1]=="DataBase")
accum = sc.accumulator(0) 
res.foreach(lambda x:accum.add(1))
accum.value</code></pre> 
<p style="text-align:center;"> <img alt="" src="https://images2.imgbox.com/90/81/kZZOtvmK_o.png"></p> 
<h3>2.编写独立应用程序</h3> 
<p>退出Pyspark交互模式：exit()</p> 
<p>(1)在spark目录下创建文件A.txt,B.txt:</p> 
<pre><code class="language-python">cd /usr/local/spark
vim A.txt
A.txt写入以下内容（一定要是竖着的）：
20170101    x
20170102    y
20170103    x
20170104    y
20170105    z
20170106    z
vim B.txt
B.txt写入以下内容
20170101    y
20170102    y
20170103    x
20170104    z
20170105    y</code></pre> 
<p> 然后创建C.py文件：</p> 
<pre><code class="language-python">vim C.py
from pyspark import SparkContext
#初始化SparkContext
sc = SparkContext('local','remdup')
#加载两个文件A和B
lines1 = sc.textFile("file:///usr/local/spark/A.txt")
lines2 = sc.textFile("file:///usr/local/spark/B.txt")
#合并两个文件的内容
lines = lines1.union(lines2)
#去重操作
distinct_lines = lines.distinct() 
#排序操作
res = distinct_lines.sortBy(lambda x:x)
#将结果写入result文件中，repartition(1)的作用是让结果合并到一个文件中，不加的话会结果写入到两个文件
res.repartition(1).saveAsTextFile("file:///usr/local/spark/result")
Python3 C.py</code></pre> 
<p> 然后执行C.py文件：python3 C.py,运行结果如下：</p> 
<p><img alt="" src="https://images2.imgbox.com/c8/75/n3N4dA4C_o.png">然后我们要查看去重的结果：</p> 
<pre><code class="language-python">cd /usr/local/spark
cd result
ls
cat  part-00000  _SUCCESS</code></pre> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/97/ef/UvJgF8lG_o.png"></p> 
<p>(2)在spark目录下创建 Algorithm.txt、Database.txt、Python.txt</p> 
<pre><code class="language-python">cd /usr/local/spark
vim Algorithm.txt：
小明 92
小红 87
小新 82
小丽 90
vim Database.txt：
小明 95
小红 81
小新 89
小丽 85
vim Python.txt：
小明 82
小红 83
小新 94
小丽 91</code></pre> 
<p> 然后编写python程序：</p> 
<pre><code class="language-python">vim score.py
from pyspark import SparkContext
sc = SparkContext('local',' avgscore')
lines1 = sc.textFile("file:///usr/local/spark/Algorithm.txt")
lines2 = sc.textFile("file:///usr/local/spark/Database.txt")
lines3 = sc.textFile("file:///usr/local/spark/Python.txt")
lines = lines1.union(lines2).union(lines3)
data = lines.map(lambda x:x.split(" ")).map(lambda x:(x[0],(int(x[1]),1)))
res = data.reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1]))
result = res.map(lambda x:(x[0],round(x[1][0]/x[1][1],2)))
result.repartition(1).saveAsTextFile("file:///usr/local/spark/result1")</code></pre> 
<p> 然后执行Py文件：</p> 
<pre><code class="language-python">python3 score.py</code></pre> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/a0/bf/rQlVA7Ya_o.png"></p> 
<p>然后查看我们统计的结果：</p> 
<pre><code class="language-python">cd /usr/local/spark
cd result1
ls
cat part-00000  _SUCCESS</code></pre> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/09/97/uQCY3pNK_o.png"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5431d1017427302552ae2625026b758c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">AI智能体｜使用扣子Coze创建AI绘画工作流</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0b5e5fc1ce7056a5c235c04f92452bf1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">2024年大数据最全java程序员的AI之路-大数据篇 hadoop安装(1)，已有千人收藏</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>