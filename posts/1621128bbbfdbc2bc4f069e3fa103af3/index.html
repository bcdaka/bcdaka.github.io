<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>超详细！AI 绘画神器 Stable Diffusion 基础教程_stable diffusion ai怎么用 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/1621128bbbfdbc2bc4f069e3fa103af3/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="超详细！AI 绘画神器 Stable Diffusion 基础教程_stable diffusion ai怎么用">
  <meta property="og:description" content="HED 边缘检测
跟 Canny 类似，但自由发挥程度更高。HED 边界保留了输入图像中的细节，绘制的人物明暗对比明显，轮廓感更强，适合在保持原来构图的基础上对画面风格进行改变时使用。
Scribble 黑白稿提取
涂鸦成图，比 HED 和 Canny 的自由发挥程度更高，也可以用于对手绘线稿进行着色处理。
Mlsd 直线检测
通过分析图片的线条结构和几何形状来构建出建筑外框，适合建筑设计的使用。
Seg 区块标注
通过对原图内容进行语义分割，可以区分画面色块，适用于大场景的画风更改。
Normal Map 法线贴图
适用于三维立体图，通过提取用户输入图片中的 3D 物体的法线向量，以法线为参考绘制出一副新图，此图与原图的光影效果完全相同。
Depth 深度检测
通过提取原始图片中的深度信息，可以生成具有同样深度结构的图。还可以通过 3D 建模软件直接搭建出一个简单的场景，再用 Depth 模型渲染出图。
ControlNet 还有项关键技术是可以开启多个 ControlNet 的组合使用，对图像进行多条件控制。例如：你想对一张图像的背景和人物姿态分别进行控制，那我们可以配置 2 个 ControlNet，第 1 个 ControlNet 使用 Depth 模型对背景进行结构提取并重新风格化，第 2 个 ControlNet 使用 OpenPose 模型对人物进行姿态控制。此外在保持 Seed 种子数相同的情况下，固定出画面结构和风格，然后定义人物不同姿态，渲染后进行多帧图像拼接，就能生成一段动画。
以上通过 ControlNet 的 8 个主要模型，我们解决了图像结构的控制问题。接下来就是对图像风格进行控制。
3. 图像风格控制
Stable Diffusion 实现图像风格化的途径主要有以下几种：Artist 艺术家风格、Checkpoint 预训练大模型、LoRA 微调模型、Textual Inversion 文本反转模型。
Artist 艺术家风格">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-03T23:14:21+08:00">
    <meta property="article:modified_time" content="2024-05-03T23:14:21+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">超详细！AI 绘画神器 Stable Diffusion 基础教程_stable diffusion ai怎么用</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><img src="https://images2.imgbox.com/94/85/hxMLBo6f_o.jpg" alt="图片"></p> 
<p>HED 边缘检测</p> 
<p>跟 Canny 类似，但自由发挥程度更高。HED 边界保留了输入图像中的细节，绘制的人物明暗对比明显，轮廓感更强，适合在保持原来构图的基础上对画面风格进行改变时使用。</p> 
<p><img src="https://images2.imgbox.com/7e/98/cAnKOVZ1_o.jpg" alt="图片"></p> 
<p>Scribble 黑白稿提取</p> 
<p>涂鸦成图，比 HED 和 Canny 的自由发挥程度更高，也可以用于对手绘线稿进行着色处理。</p> 
<p><img src="https://images2.imgbox.com/95/e2/2NOefHMQ_o.jpg" alt="图片"></p> 
<p>Mlsd 直线检测</p> 
<p>通过分析图片的线条结构和几何形状来构建出建筑外框，适合建筑设计的使用。</p> 
<p><img src="https://images2.imgbox.com/b1/26/onH3gh5L_o.jpg" alt="图片"></p> 
<p>Seg 区块标注</p> 
<p>通过对原图内容进行语义分割，可以区分画面色块，适用于大场景的画风更改。</p> 
<p><img src="https://images2.imgbox.com/12/b1/RQVrB5Xh_o.jpg" alt="图片"></p> 
<p>Normal Map 法线贴图</p> 
<p>适用于三维立体图，通过提取用户输入图片中的 3D 物体的法线向量，以法线为参考绘制出一副新图，此图与原图的光影效果完全相同。</p> 
<p><img src="https://images2.imgbox.com/3b/fc/NESIcEUz_o.jpg" alt="图片"></p> 
<p>Depth 深度检测</p> 
<p>通过提取原始图片中的深度信息，可以生成具有同样深度结构的图。还可以通过 3D 建模软件直接搭建出一个简单的场景，再用 Depth 模型渲染出图。</p> 
<p><img src="https://images2.imgbox.com/4a/63/W21YcuWw_o.jpg" alt="图片"></p> 
<p>ControlNet 还有项关键技术是可以开启多个 ControlNet 的组合使用，对图像进行多条件控制。例如：你想对一张图像的背景和人物姿态分别进行控制，那我们可以配置 2 个 ControlNet，第 1 个 ControlNet 使用 Depth 模型对背景进行结构提取并重新风格化，第 2 个 ControlNet 使用 OpenPose 模型对人物进行姿态控制。此外在保持 Seed 种子数相同的情况下，固定出画面结构和风格，然后定义人物不同姿态，渲染后进行多帧图像拼接，就能生成一段动画。</p> 
<p>以上通过 ControlNet 的 8 个主要模型，我们解决了图像结构的控制问题。接下来就是对图像风格进行控制。</p> 
<p><strong>3. 图像风格控制</strong></p> 
<p>Stable Diffusion 实现图像风格化的途径主要有以下几种：Artist 艺术家风格、Checkpoint 预训练大模型、LoRA 微调模型、Textual Inversion 文本反转模型。</p> 
<p><img src="https://images2.imgbox.com/8a/f2/xvOW4SfO_o.jpg" alt="图片"></p> 
<p>Artist 艺术家风格</p> 
<p>主要通过画作种类 Tag（如：oil painting、ink painting、comic、illustration），画家/画风 Tag（如：Hayao Miyazaki、Cyberpunk）等控制图像风格。网上也有比较多的这类风格介绍，如：</p> 
<ol><li>https://promptomania.com</li><li>https://www.urania.ai/top-sd-artists</li></ol> 
<p>但需要注意的是，使用艺术家未经允许的风格进行商用，会存在侵权问题。</p> 
<p><img src="https://images2.imgbox.com/ff/ec/2DsVlSNj_o.jpg" alt="图片"></p> 
<p>Checkpoint 预训练大模型</p> 
<p>Checkpoint 是根据特定风格训练的大模型，模型风格强大，但体积也较大，一般 5-7GB。模型训练难度大，需要极高的显卡算力。目前网上已经有非常多的不同风格的成熟大模型可供下载使用。如：https://huggingface.co/models?pipeline_tag=text-to-image</p> 
<p><img src="https://images2.imgbox.com/51/4f/3ijzsATW_o.jpg" alt="图片"></p> 
<p>LoRA 微调模型</p> 
<p>LoRA 模型是通过截取大模型的某一特定部分生成的小模型，虽然不如大模型的能力完整，但短小精悍。因为训练方向明确，所以在生成特定内容的情况下，效果会更好。LoRA 模型也常用于训练自有风格模型，具有训练速度快，模型大小适中，配置要求低（8G 显存）的特点，能用少量图片训练出风格效果。常用 LoRA 模型下载地址：</p> 
<ol><li>https://stableres.info</li><li>https//civitai.com（友情提醒：不要在办公场所打开，不然会很尴尬）</li></ol> 
<p><img src="https://images2.imgbox.com/58/cb/z1Ko4I29_o.jpg" alt="图片"></p> 
<p>Textual Inversion 文本反转模型</p> 
<p>Textual Inversion 文本反转模型也是微调模型的一种，它是针对一个风格或一个主题训练的风格模型，一般用于提高人物还原度或优化画风，用这种方式生成的模型非常小，一般几十 KB，在生成画作时使用对应 Tag 在 prompt 中进行调用。</p> 
<p><img src="https://images2.imgbox.com/f2/7d/U8d7Cg2x_o.jpg" alt="图片"></p> 
<p>自有风格模型训练</p> 
<p>Stable Diffusion 的强大之处还在于能够自定义训练风格模型，如果现有风格无法满足要求，我们还可以自己训练特定风格模型。Stable Diffusion 支持训练大模型和微调模型。我比较推荐的是用 LoRA 模型训练方法，该方法训练速度快，模型大小适中（100MB 左右），配置要求低（8G 显存），能用极少量图片训练出风格效果。例如：下图中我用了 10 张工作中的素材图，大概花了 20 分钟时间训练出该风格的 LoRA 模型，然后使用该模型就可以生成风格类似的图片。如果将训练样本量增大，那么训练出来的风格样式会更加精确。</p> 
<p><img src="https://images2.imgbox.com/b1/40/71bHnMz2_o.jpg" alt="图片"></p> 
<p>了解了 Stable Diffusion 能干什么后，再来介绍下如何部署安装使用它。</p> 
<p>二、AI 绘画工具的部署安装</p> 
<p>以下主要介绍三种部署安装方式：云端部署、本地部署、本机安装，各有优缺点。当本机硬件条件支持的情况下，推荐本地部署，其它情况推荐云端方式。</p> 
<p><strong>1. 云端部署 Stable Diffusion</strong></p> 
<p>通过 Google Colab 进行云端部署，推荐将成熟的 Stable Diffusion Colab 项目复制到自己的 Google 云端硬盘运行，省去配置环境麻烦。这种部署方式的优点是：不吃本机硬件，在有限时间段内，可以免费使用 Google Colab 强大的硬件资源，通常能给到 15G 的 GPU 算力，出图速度非常快。缺点是：免费 GPU 使用时长不固定，通常情况下一天有几个小时的使用时长，如果需要更长时间使用，可以订阅 Colab 服务。</p> 
<p><img src="https://images2.imgbox.com/bc/ae/aYkVOPg2_o.jpg" alt="图片"></p> 
<p>推荐两个目前比较好用的 Stable Diffusion Colab，选择相应模型版本运行即可：</p> 
<ol><li>Stable Diffusion Colab：github.com/camenduru/stable-diffusion-webui-colab （不带 ControlNet）</li><li>ControlNet Colab：github.com/camenduru/controlnet-colab（带 ControlNet）</li></ol> 
<p><img src="https://images2.imgbox.com/b7/96/ZpUUgnSa_o.png" alt="图片"></p> 
<p>Colab 运行界面如下，点击连接虚拟机，连接成功后点击左侧运行代码，等待环境自动配置完成后，点击 WebUI URL 即可运行 Stable Diffusion。</p> 
<p><img src="https://images2.imgbox.com/28/58/3tAI9vku_o.png" alt="图片"></p> 
<p>Stable Diffusion WebUI 运行界面如下，在后面的操作方法里我会介绍下 Stable Diffusion 的基础操作。</p> 
<p><img src="https://images2.imgbox.com/19/c0/Gb7QukFc_o.png" alt="图片"></p> 
<p><strong>2. 本地部署 Stable Diffusion</strong></p> 
<p>**<br> **</p> 
<p>相较于 Google Colab 云端部署，本地部署 Stable Diffusion 的可扩展性更强，可自定义安装需要的模型和插件，隐私性和安全性更高，自由度也更高，而且完全免费。当然缺点是对本机硬件要求高，Windows 需要 NVIDIA 显卡，8G 以上显存，16G 以上内存。Mac 需要 M1/M2 芯片才可运行。</p> 
<p><img src="https://images2.imgbox.com/2d/09/9lPyc9No_o.jpg" alt="图片"></p> 
<p>本地部署方式主要分四步，以 Mac M1 为例：</p> 
<p>第 1 步：安装 Homebrew 和 Python3.10 环境</p> 
<p>Homebrew 是一个包管理工具，具体安装方法可参考：https://brew.idayer.com/</p> 
<p>Python3.10 安装：brew install cmake protobuf rust python@3.10 git wget</p> 
<p>第 2 步：克隆 Stable Diffusion WebUI 项目仓库</p> 
<p>推荐下载 AUTOMATIC1111 的 Stable Diffusion WebUI，能很好的支持 ControlNet 扩展。</p> 
<p>克隆项目仓库：git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui</p> 
<p>第 3 步：下载并存放 Stable Diffusion 模型</p> 
<p>Stable Diffusion 模型可以下载官方提供的 1.5 或 2.0 版本的 ckpt 文件，其它风格模型则根据自己需要下载。下载地址：huggingface.co/models?pipeline_tag=text-to-image</p> 
<p>下载完后找到 stable-diffusion-webui 文件夹，把下载的 ckpt 大模型文件存放到 stable-diffusion-webui/models/Stable-diffusion 目录下。</p> 
<p>如果下载了 LoRA 模型的 safetensors 文件，则存放到 stable-diffusion-webui/models/Lora 目录中。</p> 
<p>Textual Inversion 文本反转模型的 pt 文件，存放到 stable-diffusion-webui/embeddings 目录中。</p> 
<p>第 4 步：运行 Stable Diffusion WebUI</p> 
<p>模型文件存放完成后，运行 Stable Diffusion WebUI：</p> 
<p>先输入 cd stable-diffusion-webui 再输入 ./webui.sh，程序会自动完成下载安装。</p> 
<p>运行完毕后显示：Running on local URL: http://127.0.0.1:7860 To create a public link, set <code>share=True</code> in <code>launch()</code></p> 
<p>在浏览器中打开 http://127.0.0.1:7860 ，即可运行 Stable Diffusion WebUI</p> 
<p>需要用到的资源：</p> 
<ol><li>Homebrew 包管理工具：brew.idayer.com/guide/</li><li>Python 安装：www.python.org/downloads/</li><li>Stable Diffusion 项目仓库：github.com/AUTOMATIC1111/stable-diffusion-webui</li><li>Stable Diffusion 模型：huggingface.co/models?pipeline_tag=text-to-image</li></ol> 
<p><strong>ControlNet 的安装</strong></p> 
<p>安装完 Stable Diffusion WebUI 后，我们再安装 ControlNet 扩展，以便进行图像的精准控制。</p> 
<p>安装方法：</p> 
<p>第 1 步：安装 ControlNet 插件</p> 
<p>点击扩展，选择从 URL 安装，输入插件地址 https://github.com/Mikubill/sd-webui-controlnet ，点击 Install 后重启 WebUI。</p> 
<p>第 2 步：安装 ControlNet 模型</p> 
<p>打开模型下载页面 https://huggingface.co/lllyasviel/ControlNet/tree/main</p> 
<ol><li>将 annotator 目录中的人体检测预处理模型 body_pose_model.pth 和 hand_pose_model.pth 下载至本地 stable-diffusion-webui/extensions/sd-webui-controlnet/annotator/openpose 目录。</li><li>将深度图模型 dpt_hybrid-midas-501f0c75.pt 下载至本地 stable-diffusion-webui/extensions/sd-webui-controlnet/annotator/midas 目录<br> 将 models 目录中的文件下载至本地 stable-diffusion-webui/extensions/sd-webui-controlnet/models 目录</li><li>重启 WebUI 即可使用 ControlNet</li></ol> 
<p><img src="https://images2.imgbox.com/9b/79/CejUBqQy_o.png" alt="图片"></p> 
<p><strong>解决 ControlNet 在 Mac M1 上无法运行问题</strong></p> 
<p>对于 Mac M1 芯片的电脑来说，直接运行 ControlNet 会报错，导致无法使用 ControlNet。原因是 CUDA 是适用于 NVIDIA GPU 的计算框架，当前 Mac OS 无法使用此框架，因此脚本会尝试使用 CPU，但 M1 不支持半精度数字。因此我们需要跳过 CUDA 并使用 no-half。</p> 
<p>解决方法：</p> 
<ol><li>找到 webui-macos-env.sh 文件</li><li>添加 export COMMANDLINE_ARGS=“–precision full --no-half --skip-torch-cuda-test”</li></ol> 
<p><img src="https://images2.imgbox.com/90/72/KT9JNOk7_o.jpg" alt="图片"></p> 
<p><strong>3. 本机安装 DiffusionBee</strong></p> 
<p>**<br> **</p> 
<p>如果觉得云端部署和本地部署比较繁琐，或对使用要求没有那么高，那就试下最简单的一键安装方式。下载 Diffusionbee 应用：diffusionbee.com/download。优点是方便快捷，缺点是扩展能力差（可以安装大模型，无法进行插件扩展，如 ControlNet）。</p> 
<p><img src="https://images2.imgbox.com/bf/cb/vNJWv3PH_o.jpg" alt="图片"></p> 
<p><img src="https://images2.imgbox.com/a1/1f/hp2LIzqX_o.png" alt="图片"></p> 
<p>三、AI 绘画工具的操作技巧</p> 
<p><strong>1. Stable Diffusion 基础操作</strong></p> 
<p>**<br> **</p> 
<p>文生图</p> 
<p>如图所示 Stable Diffusion WebUI 的操作界面主要分为：模型区域、功能区域、参数区域、出图区域。</p> 
<ol><li>txt2img 为文生图功能，重点参数介绍：</li><li>正向提示词：描述图片中希望出现的内容</li><li>反向提示词：描述图片中不希望出现的内容</li><li>Sampling method：采样方法，推荐选择 Euler a 或 DPM++ 系列，采样速度快</li><li>Sampling steps：迭代步数，数值越大图像质量越好，生成时间也越长，一般控制在 30-50 就能出效果</li><li>Restore faces：可以优化脸部生成</li><li>Width/Height：生成图片的宽高，越大越消耗显存，生成时间也越长，一般方图 512x512，竖图 512x768，需要更大尺寸，可以到 Extras 功能里进行等比高清放大</li><li>CFG：提示词相关性，数值越大越相关，数值越小越不相关，一般建议 7-12 区间</li><li>Batch count/Batch size：生成批次和每批数量，如果需要多图，可以调整下每批数量</li><li>Seed：种子数，-1 表示随机，相同的种子数可以保持图像的一致性，如果觉得一张图的结构不错，但对风格不满意，可以将种子数固定，再调整 prompt 生成</li></ol> 
<p><img src="https://images2.imgbox.com/b3/17/yJUcpvEB_o.png" alt="图片"></p> 
<p>图生图</p> 
<p>img2img 功能可以生成与原图相似构图色彩的画像，或者指定一部分内容进行变换。可以重点使用 Inpaint 图像修补这个功能：</p> 
<ol><li>Resize mode：缩放模式，Just resize 只调整图片大小，如果输入与输出长宽比例不同，图片会被拉伸。Crop and resize 裁剪与调整大小，如果输入与输出长宽比例不同，会以图片中心向四周，将比例外的部分进行裁剪。Resize and fill 调整大小与填充，如果输入与输出分辨率不同，会以图片中心向四周，将比例内多余的部分进行填充</li><li>Mask blur：蒙版模糊度，值越大与原图边缘的过度越平滑，越小则边缘越锐利</li><li>Mask mode：蒙版模式，Inpaint masked 只重绘涂色部分，Inpaint not masked 重绘除了涂色的部分</li><li>Masked Content：蒙版内容，fill 用其他内容填充，original 在原来的基础上重绘</li><li>Inpaint area：重绘区域，Whole picture 整个图像区域，Only masked 只在蒙版区域</li><li>Denoising strength：重绘幅度，值越大越自由发挥，越小越和原图接近</li></ol> 
<p><img src="https://images2.imgbox.com/13/e8/HClfiMRr_o.png" alt="图片"></p> 
<p>ControlNet</p> 
<p>安装完 ControlNet 后，在 txt2img 和 img2img 参数面板中均可以调用 ControlNet。操作说明：</p> 
<ol><li>Enable：启用 ControlNet</li><li>Low VRAM：低显存模式优化，建议 8G 显存以下开启</li><li>Guess mode：猜测模式，可以不设置提示词，自动生成图片</li><li>Preprocessor：选择预处理器，主要有 OpenPose、Canny、HED、Scribble、Mlsd、Seg、Normal Map、Depth</li><li>Model：ControlNet 模型，模型选择要与预处理器对应</li><li>Weight：权重影响，使用 ControlNet 生成图片的权重占比影响</li><li>Guidance strength(T)：引导强度，值为 1 时，代表每迭代 1 步就会被 ControlNet 引导 1 次</li><li>Annotator resolution：数值越高，预处理图像越精细</li><li>Canny low/high threshold：控制最低和最高采样深度</li><li>Resize mode：图像大小模式，默认选择缩放至合适</li><li>Canvas width/height：画布宽高</li><li>Create blank canvas：创建空白画布</li><li>Preview annotator result：预览注释器结果，得到一张 ControlNet 模型提取的特征图片</li><li>Hide annotator result：隐藏预览图像窗口</li></ol> 
<p><img src="https://images2.imgbox.com/bd/90/C0MVGV0B_o.png" alt="图片"></p> 
<p>LoRA 模型训练说明</p> 
<p>前面提到 LoRA 模型具有训练速度快，模型大小适中（100MB 左右），配置要求低（8G 显存），能用少量图片训练出风格效果的优势。</p> 
<p>以下简要介绍该模型的训练方法：</p> 
<p>第 1 步：数据预处理</p> 
<h4><a id="_390"></a>最后</h4> 
<blockquote> 
 <p><strong>🍅 硬核资料</strong>：关注即可领取PPT模板、简历模板、行业经典书籍PDF。<br> <strong>🍅 技术互助</strong>：技术群大佬指点迷津，你的问题可能不是问题，求资源在群里喊一声。<br> <strong>🍅 面试题库</strong>：由技术群里的小伙伴们共同投稿，热乎的大厂面试真题，持续更新中。<br> <strong>🍅 知识体系</strong>：含编程语言、算法、大数据生态圈组件（Mysql、Hive、Spark、Flink）、数据仓库、Python、前端等等。</p> 
</blockquote> 
<p><strong>网上学习资料一大堆，但如果学到的知识不成体系，遇到问题时只是浅尝辄止，不再深入研究，那么很难做到真正的技术提升。</strong></p> 
<p><strong><a href="https://bbs.csdn.net/topics/618317507">需要这份系统化学习资料的朋友，可以戳这里无偿获取</a></strong></p> 
<p><strong>一个人可以走的很快，但一群人才能走的更远！不论你是正从事IT行业的老鸟或是对IT行业感兴趣的新人，都欢迎加入我们的的圈子（技术交流、学习资源、职场吐槽、大厂内推、面试辅导），让我们一起学习成长！</strong></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4e55d04900a97c24a0d75cd8f1b57786/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">《Mask2Former》算法详解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a3ee261187d4a7896740806962146e2c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Mac 环境变量配置</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>