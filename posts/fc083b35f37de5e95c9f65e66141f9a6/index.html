<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>一文搞懂策略梯度（Policy gradient）算法（一） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/fc083b35f37de5e95c9f65e66141f9a6/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="一文搞懂策略梯度（Policy gradient）算法（一）">
  <meta property="og:description" content="引言 在强化学习的过程中，从 Sarsa 到 Q-learning 再到 DQN，本质上都是值函数近似算法。
值函数近似算法都是先学习动作价值函数，然后根据估计的动作价值函数选择动作。如果没有动作价值函数的估计，策略也就不会存在。 例如，DQN的神经网络结构可以表示为如下图所示：
图中，输入是状态 s s s，输出是每个动作的 Q Q Q 值，即对每个动作的评分，分数越高意味着动作越好。通过对值函数的近似，我们可以知道回报最大的路径，从而指导智能体进行动作的选取。
但是，强化学习的目标，是学习最优策略。那么有没有一种可能，我们可以跳过动作价值的评估环节，直接从输入状态，到输出策略呢？
——策略梯度算法
在策略梯度算法中，策略函数的输入是状态 s s s 和动作 a a a，输出是一个0到1之间的概率值，当前最有效的方法是用神经网络近似策略函数。给出一个策略网络结构图：
如图，在策略网络结构中，输入是状态 s s s，输出是动作空间中每个动作的概率值。
两个关键 现在我们已经有了想法——直接从输入得到最优策略，那么随之而来
两个问题：
1、如何来衡量一个策略的好与坏？
2、如何搜索最优策略？
先来看看《强化学习》中关于策略梯度算法的定义：
策略梯度方法基于某种性能度量 J ( θ ) J(\theta) J(θ) 的梯度，这些梯度是标量 J ( θ ) J(\theta) J(θ) 对策略参数的梯度。这些方法的目标是最大化性能指标，所以它们的更新近似于 J J J 的梯度上升
梯度上升： θ t &#43; 1 = θ t &#43; α ∇ J ( θ t ) ^ \theta_{t&#43;1}=\theta_t&#43;\alpha\widehat{\nabla{J(\theta_t)}} θt&#43;1​=θt​&#43;α∇J(θt​) ​">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-22T10:16:45+08:00">
    <meta property="article:modified_time" content="2024-04-22T10:16:45+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">一文搞懂策略梯度（Policy gradient）算法（一）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>引言</h2> 
<p>在强化学习的过程中，从 Sarsa 到 Q-learning 再到 DQN，本质上都是值函数近似算法。<br> 值函数近似算法都是先学习动作价值函数，然后根据估计的动作价值函数选择动作。<strong>如果没有动作价值函数的估计，策略也就不会存在。</strong> 例如，DQN的神经网络结构可以表示为如下图所示：</p> 
<center> 
 <img src="https://images2.imgbox.com/7f/4e/VsIpLRNK_o.png" width="80%"> 
</center> 
<p>图中，输入是状态 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         s 
        
       
      
        s 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span></span>，输出是每个动作的 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         Q 
        
       
      
        Q 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8778em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span></span> 值，即对每个动作的评分，分数越高意味着动作越好。通过对值函数的近似，我们可以知道回报最大的路径，从而指导智能体进行动作的选取。</p> 
<p>但是，<strong>强化学习的目标，是学习最优策略</strong>。那么有没有一种可能，我们可以跳过动作价值的评估环节，直接从输入状态，到输出策略呢？<br> <strong>——策略梯度算法</strong></p> 
<p>在策略梯度算法中，策略函数的输入是状态 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         s 
        
       
      
        s 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span></span> 和动作 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         a 
        
       
      
        a 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">a</span></span></span></span></span>，输出是一个0到1之间的概率值，当前最有效的方法是用神经网络近似策略函数。给出一个策略网络结构图：</p> 
<center> 
 <img src="https://images2.imgbox.com/91/5c/a7j19x6m_o.png" width="90%"> 
</center> 
<p>如图，在策略网络结构中，输入是状态 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         s 
        
       
      
        s 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span></span>，输出是动作空间中每个动作的概率值。</p> 
<h2><a id="_14"></a>两个关键</h2> 
<p>现在我们已经有了想法——直接从输入得到最优策略，那么随之而来<br> <strong>两个问题：</strong><br> 1、如何来衡量一个策略的好与坏？<br> 2、如何搜索最优策略？</p> 
<p>先来看看《强化学习》中关于策略梯度算法的定义：</p> 
<blockquote> 
 <p>策略梯度方法基于某种性能度量 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          J 
         
        
          ( 
         
        
          θ 
         
        
          ) 
         
        
       
         J(\theta) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0962em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span></span></span></span></span> 的梯度，这些梯度是标量 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          J 
         
        
          ( 
         
        
          θ 
         
        
          ) 
         
        
       
         J(\theta) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0962em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span></span></span></span></span> 对策略参数的梯度。这些方法的目标是最大化性能指标，所以它们的更新近似于 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          J 
         
        
       
         J 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0962em;">J</span></span></span></span></span> 的梯度上升<br> 梯度上升：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           θ 
          
          
          
            t 
           
          
            + 
           
          
            1 
           
          
         
        
          = 
         
         
         
           θ 
          
         
           t 
          
         
        
          + 
         
        
          α 
         
         
          
          
            ∇ 
           
           
           
             J 
            
           
             ( 
            
            
            
              θ 
             
            
              t 
             
            
           
             ) 
            
           
          
         
           ^ 
          
         
        
       
         \theta_{t+1}=\theta_t+\alpha\widehat{\nabla{J(\theta_t)}} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9028em; vertical-align: -0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0278em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.8444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: -0.0278em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.3em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.05em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">∇</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0962em;">J</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: -0.0278em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="svg-align" style="top: -3.75em;"><span class="pstrut" style="height: 3em;"></span><span class="" style="height: 0.3em;"> 
            <svg width="100%" height="0.3em" viewbox="0 0 2364 300" preserveaspectratio="none"> 
             <path d="M1181 0h2l1171 176c6 0 10 5 10 11l-2 23c-1 6-5 10
-11 10h-1L1182 67 15 220h-1c-6 0-10-4-11-10l-2-23c-1-6 4-11 10-11z"></path> 
            </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.25em;"><span class=""></span></span></span></span></span></span></span></span></span><br> 其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
          
          
            ∇ 
           
           
           
             J 
            
           
             ( 
            
            
            
              θ 
             
            
              t 
             
            
           
             ) 
            
           
          
         
           ^ 
          
         
        
       
         \widehat{\nabla{J(\theta_t)}} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.3em; vertical-align: -0.25em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.05em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">∇</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0962em;">J</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: -0.0278em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="svg-align" style="top: -3.75em;"><span class="pstrut" style="height: 3em;"></span><span class="" style="height: 0.3em;"> 
            <svg width="100%" height="0.3em" viewbox="0 0 2364 300" preserveaspectratio="none"> 
             <path d="M1181 0h2l1171 176c6 0 10 5 10 11l-2 23c-1 6-5 10
-11 10h-1L1182 67 15 220h-1c-6 0-10-4-11-10l-2-23c-1-6 4-11 10-11z"></path> 
            </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.25em;"><span class=""></span></span></span></span></span></span></span></span></span> 是一个随机估计，它的期望是性能指标对它的参数 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           θ 
          
         
           t 
          
         
        
       
         \theta_t 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: -0.0278em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 的梯度的近似。我们将所有符合这个框架的方法都称为<strong>梯度策略法</strong>。</p> 
</blockquote> 
<p><strong>解决思路：</strong><br> 1、使用一个目标函数定义最优策略；<br> 2、基于梯度的优化算法；</p> 
<p>下面我们分别论述这两个关键点。</p> 
<h3><a id="_30"></a>目标函数</h3> 
<p>从上文中定义可以看出，策略梯度方法的目标函数即为<strong>某种性能度量 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          J 
         
        
          ( 
         
        
          θ 
         
        
          ) 
         
        
       
         J(\theta) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0962em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span></span></span></span></span></strong> ， 策略可以用任意的方式参数化，只要 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         π 
        
       
         ( 
        
       
         a 
        
       
         ∣ 
        
       
         s 
        
       
         , 
        
       
         θ 
        
       
         ) 
        
       
      
        \pi(a|s,\theta) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">π</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span></span></span></span></span> 对参数可导。</p> 
<p><strong>平均状态价值</strong></p> 
<p>由状态价值的定义</p> 
<center> 
 <img src="https://images2.imgbox.com/09/aa/5R5UEOkW_o.png" width="50%"> 
</center> 
<p>可以看到，状态价值即依赖当前状态 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          s 
         
        
          t 
         
        
       
      
        s_t 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，也依赖于策略网络 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         π 
        
       
      
        \pi 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">π</span></span></span></span></span> 的参数 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         θ 
        
       
      
        \theta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span></span></span></span></span>，进行说明：</p> 
<center> 
 <img src="https://images2.imgbox.com/f7/9c/vcP1f6Zw_o.png" width="80%"> 
</center> 
<p>如果一个策略很好，那么其状态价值 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          V 
         
        
          π 
         
        
       
         ( 
        
       
         S 
        
       
         ) 
        
       
      
        V_{\pi}(S) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.2222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">π</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mclose">)</span></span></span></span></span> 的均值应当很大。因此定义目标函数<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          J 
         
        
          ( 
         
        
          θ 
         
        
          ) 
         
        
          = 
         
         
         
           E 
          
         
           S 
          
         
        
          [ 
         
         
         
           V 
          
         
           π 
          
         
        
          ( 
         
        
          S 
         
        
          ) 
         
        
          ] 
         
        
       
         J(\theta)=E_S[V_\pi(S)] 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0962em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0576em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: -0.0576em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0576em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.2222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">π</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mclose">)]</span></span></span></span></span></span></p> 
<p>为啥这样定义呢？</p> 
<p>其实前面说了，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          V 
         
        
          π 
         
        
       
         ( 
        
       
         S 
        
       
         ) 
        
       
      
        V_{\pi}(S) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.2222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">π</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mclose">)</span></span></span></span></span> 依赖于当前状态和策略网络，因此对状态进行期望操作得到目标函数 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         J 
        
       
         ( 
        
       
         θ 
        
       
         ) 
        
       
      
        J(\theta) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0962em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span></span></span></span></span>，这样 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         J 
        
       
         ( 
        
       
         θ 
        
       
         ) 
        
       
      
        J(\theta) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0962em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span></span></span></span></span> 就只依赖于策略网络 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         π 
        
       
      
        \pi 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">π</span></span></span></span></span> 的参数 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         θ 
        
       
      
        \theta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span></span></span></span></span>——策略越好，则 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         J 
        
       
         ( 
        
       
         θ 
        
       
         ) 
        
       
      
        J(\theta) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0962em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span></span></span></span></span> 越大。</p> 
<p>即，策略学习可以描述为优化问题：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         m 
        
       
         a 
        
       
         x 
        
       
         J 
        
       
         ( 
        
       
         θ 
        
       
         ) 
        
       
      
        max J(\theta) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right: 0.0962em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span></span></span></span></span></p> 
<p>上述定义的目标函数中，设 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         d 
        
       
         ( 
        
       
         s 
        
       
         ) 
        
       
      
        d(s) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span></span> 为状态 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         s 
        
       
      
        s 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span></span> 的权重，有</p> 
<center> 
 <img src="https://images2.imgbox.com/a7/39/yVYUlAHM_o.png" width="35%"> 
</center> 
<p>则对于 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         d 
        
       
         ( 
        
       
         s 
        
       
         ) 
        
       
      
        d(s) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span></span> 而言，分为两种情况：<br> 1）<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         d 
        
       
      
        d 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span></span> 独立于策略 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         π 
        
       
      
        \pi 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">π</span></span></span></span></span></p> 
<center> 
 <img src="https://images2.imgbox.com/db/b8/9PJtSvJQ_o.png" width="80%"> 
</center> 
<p>2）<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         d 
        
       
      
        d 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span></span> 依赖于策略 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         π 
        
       
      
        \pi 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">π</span></span></span></span></span></p> 
<center> 
 <img src="https://images2.imgbox.com/1b/5e/cRtkgdhO_o.png" width="80%"> 
</center> 
<p>主要的区别，就在于求梯度的时候，计算有所不同。</p> 
<p><strong>平均奖励</strong></p> 
<p>第二种度量最优策略的目标函数为平均奖励，此处给出定义，不过多阐述。</p> 
<center> 
 <img src="https://images2.imgbox.com/5c/4a/qpXss3CB_o.png" width="80%"> 
</center> 
<p>结合以上，有策略梯度算法的基本思想：<br> 1、所有的目标函数都是关于策略 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         π 
        
       
      
        \pi 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">π</span></span></span></span></span> 的方程；<br> 2、策略 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         π 
        
       
      
        \pi 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">π</span></span></span></span></span> 由 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         θ 
        
       
      
        \theta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span></span></span></span></span> 参数化，因此所有的目标函数是关于 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         θ 
        
       
      
        \theta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span></span></span></span></span> 的方程；<br> 3、不同的 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         θ 
        
       
      
        \theta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span></span></span></span></span> 生成不同的目标函数值；<br> 4、通过搜索 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         θ 
        
       
      
        \theta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span></span></span></span></span> 的最优值最大化目标函数。</p> 
<h3><a id="_75"></a>策略梯度</h3> 
<p>此处以王树森 ——《深度强化学习》为基础，对策略梯度定理的简要证明进行阐述。</p> 
<p>首先，回顾以下状态价值函数的定义：</p> 
<center> 
 <img src="https://images2.imgbox.com/2c/a8/eN0ZZKgy_o.png" width="50%"> 
</center> 
<p>基于神经网络近似的策略函数中，将策略网络 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         π 
        
       
         ( 
        
       
         a 
        
       
         ∣ 
        
       
         s 
        
       
         ; 
        
       
         θ 
        
       
         ) 
        
       
      
        \pi(a|s;\theta) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">π</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mpunct">;</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span></span></span></span></span> 看作动作的概率质量函数（或概率密度函数）。则状态价值 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          V 
         
        
          π 
         
        
       
         ( 
        
       
         s 
        
       
         ) 
        
       
      
        V_\pi(s) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.2222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">π</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span></span> 可以写成：</p> 
<center> 
 <img src="https://images2.imgbox.com/86/da/y6PKfWzJ_o.png" width="50%"> 
</center> 
<p>状态价值函数关于 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         θ 
        
       
      
        \theta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span></span></span></span></span> 的梯度可以写作：</p> 
<center> 
 <img src="https://images2.imgbox.com/30/b0/K22GthDi_o.png" width="50%"> 
</center> 
<p>由链式法则，有</p> 
<center> 
 <img src="https://images2.imgbox.com/c5/b5/7d7Ufbid_o.png" width="80%"> 
</center> 
<p>上式最右面一项 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
      
        x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span> 的分析非常复杂，此处不具体进行分析。可得</p> 
<center> 
 <img src="https://images2.imgbox.com/d4/1f/gqgHvICH_o.png" width="80%"> 
</center> 
<p>进行替换，则有</p> 
<center> 
 <img src="https://images2.imgbox.com/ff/45/f4vvcyAz_o.png" width="80%"> 
</center> 
<p>由目标函数的定义，得</p> 
<center> 
 <img src="https://images2.imgbox.com/80/57/Tgk1fV98_o.png" width="80%"> 
</center> 
<p>以上。</p> 
<p><strong>简要证明到此结束，问题来了。</strong></p> 
<p>这个公式，好像不能直接用？<br> 为啥？</p> 
<p>策略梯度的求解过程中，需要知道两个东西，一个是状态 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         S 
        
       
      
        S 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span></span></span></span></span> 的概率密度函数 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         π 
        
       
         ( 
        
       
         A 
        
       
         ∣ 
        
       
         S 
        
       
         ; 
        
       
         θ 
        
       
         ) 
        
       
      
        \pi(A|S;\theta) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">π</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mpunct">;</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span></span></span></span></span>，另一个是动作价值函数 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          Q 
         
        
          π 
         
        
       
         ( 
        
       
         S 
        
       
         , 
        
       
         A 
        
       
         ) 
        
       
      
        Q_{\pi}(S,A) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">π</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">A</span><span class="mclose">)</span></span></span></span></span>。<br> 先来看 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         π 
        
       
         ( 
        
       
         A 
        
       
         ∣ 
        
       
         S 
        
       
         ; 
        
       
         θ 
        
       
         ) 
        
       
      
        \pi(A|S;\theta) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">π</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mpunct">;</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span></span></span></span></span>，如果我们知道所有状态的分布信息，那么求期望是可行的，但是很显然，我们并不知道状态 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         S 
        
       
      
        S 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span></span></span></span></span> 的概率密度函数；即使我们知道，能够通过连加或者定积分求出期望，我们也不愿意这么做，因为连加或者定积分的计算量非常大。</p> 
<p>怎么办呢？——这里用到了随机近似的概念。</p> 
<p>每次从环境中观测到一个状态 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         s 
        
       
      
        s 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span></span> ，相当于随机变量 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         S 
        
       
      
        S 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span></span></span></span></span> 的观测值。然后再根据当前的策略网络（策略网络的参数必须是最新的）随机抽样得出一个动作：</p> 
<center> 
 <img src="https://images2.imgbox.com/ec/d7/NFzBN5Qj_o.png" width="30%"> 
</center> 
<p>计算随机梯度：</p> 
<center> 
 <img src="https://images2.imgbox.com/59/5c/nrrFC6my_o.png" width="60%"> 
</center> 
<p>则有：</p> 
<center> 
 <img src="https://images2.imgbox.com/e5/c8/vVhFfHui_o.png" width="60%"> 
</center> 
<p><strong>但是，但是，但是</strong><br> 动作价值函数 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          Q 
         
        
          π 
         
        
       
         ( 
        
       
         S 
        
       
         , 
        
       
         A 
        
       
         ) 
        
       
      
        Q_{\pi}(S,A) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">π</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">A</span><span class="mclose">)</span></span></span></span></span>，我们也不晓得啊！~</p> 
<p>基于之前学到的值函数近似算法，我们有两种方法可以得到价值函数 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          Q 
         
        
          π 
         
        
       
         ( 
        
       
         S 
        
       
         , 
        
       
         A 
        
       
         ) 
        
       
      
        Q_{\pi}(S,A) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">π</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">A</span><span class="mclose">)</span></span></span></span></span>。</p> 
<p><strong>1、蒙特卡洛；</strong><br> 使用实际观测的回报 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         u 
        
       
      
        u 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">u</span></span></span></span></span> 近似 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          Q 
         
        
          π 
         
        
       
         ( 
        
       
         S 
        
       
         , 
        
       
         A 
        
       
         ) 
        
       
      
        Q_{\pi}(S,A) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">π</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">A</span><span class="mclose">)</span></span></span></span></span>，这种方法被称为——<strong>REINFORCE</strong>。</p> 
<p><strong>2、TD算法；</strong><br> 使用神经网络 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         q 
        
       
         ( 
        
       
         s 
        
       
         , 
        
       
         a 
        
       
         ; 
        
       
         w 
        
       
         ) 
        
       
      
        q(s,a;w) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">a</span><span class="mpunct">;</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="mclose">)</span></span></span></span></span> 近似 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          Q 
         
        
          π 
         
        
       
         ( 
        
       
         S 
        
       
         , 
        
       
         A 
        
       
         ) 
        
       
      
        Q_{\pi}(S,A) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">π</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0576em;">S</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">A</span><span class="mclose">)</span></span></span></span></span>，这种方法被称为——<strong>actor-critic</strong>。</p> 
<h2><a id="_131"></a>结束</h2> 
<p>关于 <strong>REINFORCE</strong> 和 <strong>actor-critic</strong>，后续有机会再介绍。</p> 
<p>到此为止，就算是完成了策略梯度（PG）算法的简要介绍，希望能帮到各位理清算法脉络。</p> 
<h2><a id="_137"></a>参考资料</h2> 
<p>王树森——深度强化学习<br> 赵世钰——强化学习的数学原理<br> Richard S. Sutton——强化学习</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a59cf7d2d286f69ce89950a2122aedd4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【Spring Security系列】Spring Security整合JWT：构建安全的Web应用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3b7d6d9d993107baa34d4b6c1bce2760/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">nginx偶发报错：【502】 upstream prematurely closed connection while reading response header from upstream</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>