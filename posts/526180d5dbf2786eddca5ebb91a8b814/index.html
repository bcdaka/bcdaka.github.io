<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>爬虫自己做的 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/526180d5dbf2786eddca5ebb91a8b814/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="爬虫自己做的">
  <meta property="og:description" content="1.urllib 1.1基本使用 1.2 下载（图片，页面，视频） 1.3 get 1.3.1 quote 中文变成对应uncode编码 当url 的wd=&#39;中文时&#39; quote是将中文变成对应uncode编码 然后拼接成完整的url
1.3.2urlencode方法 wd有多个参数 1.3.3ajas get实例 爬取豆瓣电影单页 1.因为是get所以才可以用get请求方式，找到含有所有信息的接口
总结: 1.导包
2.url 因为是get所以才可以用get请求方式，找到含有所有信息的接口
3.headers
4.请求对象的设置
5.获取请求相应 6.读要注意编码格式 7.书写，在书写时应该提前查看html是什么格式
1.3.4 多页 写代码需要什么就导入什么包 思路：
1.有十个url ，每个url形成，过程一样所以写入函数
2.，每个ur都要l获取相应，相应中步骤也一样
3.书写，每个url也都要创建一个文件来书写
post一样的思路，重点 url
1.4 post 如果html是josn格式，那么还要
1.5异常一般 try catch 1.5.1 http.error
1.5.2 url.error
1.6 爬取有登录的网站（cookie） 五步骤一样只有headers有改变
①：里面有自己登陆过的信息 重点 但每次登录这个值都会不一样
②：这是从哪个地址访问进来的，图片的反爬会用到
1.6 handler 防止ip禁止封住 1.7 代理 ip&#43;端口号 1.8 代理池 多个代理组成的数组，利用随机数
总结：用代理的时候首先写代理ip，然后才是爬虫的步骤，关键是request后面是代理的三步
2解析 2.1 xpath 不用安装插件">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-21T13:22:41+08:00">
    <meta property="article:modified_time" content="2024-07-21T13:22:41+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">爬虫自己做的</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p></p> 
<h2>1.urllib</h2> 
<h3>1.1基本使用</h3> 
<p><img alt="" height="424" src="https://images2.imgbox.com/41/1b/v3fiRiAU_o.png" width="515"></p> 
<h3>1.2 下载（图片，页面，视频）</h3> 
<h3>1.3 get</h3> 
<h4>1.3.1 quote 中文变成对应uncode编码</h4> 
<p>当url 的wd='中文时' quote是将中文变成对应uncode编码 然后拼接成完整的url</p> 
<p><img alt="" height="460" src="https://images2.imgbox.com/c6/4c/C47OYGGX_o.png" width="641"></p> 
<h4>1.3.2urlencode方法 wd有多个参数</h4> 
<p><img alt="" height="488" src="https://images2.imgbox.com/fb/6f/a5eDjaAT_o.png" width="592"></p> 
<h4>1.3.3ajas get实例 爬取豆瓣电影单页</h4> 
<p>1.因为是get所以才可以用get请求方式，找到含有所有信息的接口</p> 
<p><img alt="" height="514" src="https://images2.imgbox.com/79/17/MJOkxhxH_o.png" width="1200"></p> 
<h5>总结:</h5> 
<p><img alt="" height="496" src="https://images2.imgbox.com/81/c8/pWFWi5wJ_o.png" width="1115"></p> 
<p>1.导包</p> 
<p>2.url 因为是get所以才可以用get请求方式，找到含有所有信息的接口</p> 
<p>3.headers</p> 
<p>4.请求对象的设置</p> 
<p>5.获取请求相应 6.读要注意编码格式 7.书写，在书写时应该提前查看html是什么格式</p> 
<h4>1.3.4 多页 写代码需要什么就导入什么包</h4> 
<p>思路：</p> 
<p>1.有十个url ，每个url形成，过程一样所以写入函数<img alt="" height="228" src="https://images2.imgbox.com/b3/f1/W8Tjg41q_o.png" width="883"></p> 
<h3><img alt="" height="254" src="https://images2.imgbox.com/2e/41/W2i12TAf_o.png" width="1200"></h3> 
<h3><img alt="" height="491" src="https://images2.imgbox.com/4e/ef/2x2QLfUP_o.png" width="1200"></h3> 
<p>2.，每个ur都要l获取相应，相应中步骤也一样</p> 
<p><img alt="" height="217" src="https://images2.imgbox.com/87/85/fvrR5pL1_o.png" width="1038"></p> 
<h3><img alt="" height="279" src="https://images2.imgbox.com/02/a3/lyf4C4Ii_o.png" width="665"></h3> 
<p>3.书写，每个url也都要创建一个文件来书写</p> 
<p><img alt="" height="675" src="https://images2.imgbox.com/9b/67/oewXQDnq_o.png" width="1200"></p> 
<p>post一样的思路，重点 url</p> 
<h3>1.4 post</h3> 
<p></p> 
<p><img alt="" height="304" src="https://images2.imgbox.com/6e/d4/0d0IctNX_o.png" width="1072">如果html是josn格式，那么还要</p> 
<p><img alt="" height="87" src="https://images2.imgbox.com/db/4f/D4eGYhwv_o.png" width="405"></p> 
<h3>1.5异常一般 try catch</h3> 
<p>1.5.1 http.error</p> 
<p>1.5.2 url.error</p> 
<p><img alt="" height="665" src="https://images2.imgbox.com/02/e7/RSgvSgHP_o.png" width="590"></p> 
<h3>1.6 爬取有登录的网站（cookie）</h3> 
<p>五步骤一样只有headers有改变</p> 
<p><img alt="" height="669" src="https://images2.imgbox.com/aa/e3/omgzkt1t_o.png" width="1200">①：里面有自己登陆过的信息 重点 但每次登录这个值都会不一样</p> 
<p>②：这是从哪个地址访问进来的，图片的反爬会用到</p> 
<h3>1.6 handler</h3> 
<h3>防止ip禁止封住</h3> 
<h3><img alt="" height="821" src="https://images2.imgbox.com/ef/bd/mFEnyvKk_o.png" width="1200">1.7 代理 ip+端口号</h3> 
<p><img alt="" height="153" src="https://images2.imgbox.com/45/d2/5xR9BJF8_o.png" width="584"></p> 
<h3>1.8 代理池</h3> 
<p>多个代理组成的数组，利用随机数</p> 
<p>总结：用代理的时候首先写代理ip，然后才是爬虫的步骤，关键是request后面是代理的三步</p> 
<h2>2解析</h2> 
<h3>2.1 xpath</h3> 
<p>不用安装插件</p> 
<p>安装lxml库在终端安装即可</p> 
<p><img alt="" height="647" src="https://images2.imgbox.com/c2/2b/P8UKMcSA_o.png" width="658"></p> 
<p>运用xpath和requests时遇到的问题</p> 
<p>1.html中要注意与网页中network对比</p> 
<p><img alt="" height="303" src="https://images2.imgbox.com/b7/85/6di6KrWQ_o.png" width="1200"></p> 
<h4>2.2基本语法</h4> 
<p><img alt="" height="460" src="https://images2.imgbox.com/7f/ab/17Tobt67_o.png" width="463"></p> 
<p>总结</p> 
<p>urllib（获取网页html）+xpath（解析html）</p> 
<h3>2.2 josnpath</h3> 
<p><img alt="" height="268" src="https://images2.imgbox.com/eb/b0/6cHZnCCS_o.png" width="1200">只能解析本地文件</p> 
<p><img alt="" height="781" src="https://images2.imgbox.com/67/8b/6ww57Whc_o.png" width="1200"></p> 
<p><img alt="" height="555" src="https://images2.imgbox.com/d1/53/VMXPWfmk_o.png" width="1200"></p> 
<h2>3.bs4</h2> 
<h3>3.1基本语法</h3> 
<h4>3.1.1安装和能爬的对象</h4> 
<h4 style="background-color:transparent;"><img alt="" height="321" src="https://images2.imgbox.com/ce/04/KK3F6Fn5_o.png" width="759">3.1.2节点定位</h4> 
<p><img alt="" height="588" src="https://images2.imgbox.com/e8/8e/fFUqcQIL_o.png" width="584"><img alt="" height="255" src="https://images2.imgbox.com/92/ec/4BWbB0S1_o.png" width="501"></p> 
<h2 style="background-color:transparent;">4.selenium</h2> 
<p>本身是有界面，既然有界面速度有些慢</p> 
<p>Chrome 126-128 chromedriver</p> 
<p>https://googlechromelabs.github.io/chrome-for-testing/</p> 
<h3 style="background-color:transparent;">4.1元素定位</h3> 
<h4>1.id</h4> 
<h4>2.name</h4> 
<h4>3.xpath</h4> 
<h4>4.tag_name</h4> 
<h4>5.css_selector(bs4)</h4> 
<h4>6.link_text</h4> 
<h3><img alt="" height="690" src="https://images2.imgbox.com/bf/2c/JRaWkJma_o.png" width="1034">4.2 获取元素信息和交互</h3> 
<p><img alt="" height="534" src="https://images2.imgbox.com/9e/df/k2gLFqOL_o.png" width="652">无界无界面的hadless</p> 
<p><img alt="" height="690" src="https://images2.imgbox.com/92/ec/tpAusWOG_o.png" width="1200"></p> 
<h2>5.requests</h2> 
<h3>5.1基本使用</h3> 
<p><img alt="" height="473" src="https://images2.imgbox.com/fb/d8/jGNEKXJp_o.png" width="718"></p> 
<h3 style="background-color:transparent;">5.2get</h3> 
<p>可以与urllib中get对比 param参数不是必须的如果有像wd=的就用param。</p> 
<p><img alt="" height="507" src="https://images2.imgbox.com/6c/6f/SVgRt3ql_o.png" width="1200"></p> 
<p> Urllib</p> 
<p><img alt="" height="460" src="https://images2.imgbox.com/98/99/smenvbKv_o.png" width="641"><img alt="" height="488" src="https://images2.imgbox.com/c0/04/0GCs2XKU_o.png" width="592"></p> 
<h3>5.3post</h3> 
<h3>5.4 代理</h3> 
<p>快代理上有免费的ip，不一定好使，也可以买一个</p> 
<p>就是在ip上做手脚</p> 
<p><img alt="" height="679" src="https://images2.imgbox.com/61/19/j0FzaUPI_o.png" width="1181"></p> 
<h3>5.5cookie</h3> 
<h3>案例 爬登录后的古诗词</h3> 
<p>针对的是有验证码和登录</p> 
<p>需求:爬取 登录后的html</p> 
<p>攻克的难关:绕过登录（不想输入账户和密码）</p> 
<p>分析:验证码，其他的暂时未知</p> 
<h4>1.首先获取跳转时的url，</h4> 
<h5>思路1正常登录</h5> 
<p>1.清空日志，点击保留日志</p> 
<p><img alt="" height="498" src="https://images2.imgbox.com/88/9a/ruvkaOC7_o.png" width="1200">2.正常登录，点击负载可以看到自己登录的信息</p> 
<p><img alt="" height="318" src="https://images2.imgbox.com/29/12/WOwIJGe3_o.png" width="1200">2是登录后的1.跳转时的</p> 
<h5>2.错误登录</h5> 
<p>不要点击确定否则会跳转到其他页面</p> 
<p><img alt="" height="602" src="https://images2.imgbox.com/ea/21/NsuGooNb_o.png" width="1200">确认是什么请求</p> 
<h4 style="background-color:transparent;">2.爬虫获取请求（urllib，requests）</h4> 
<p>因为post请求参数有url，data，headers，</p> 
<p>url已经有了，headers也有了，就差设置data（下面的红框里面的除了登录，一般情况下除了账户密码，from，其他的都不知道）</p> 
<p><img alt="" height="257" src="https://images2.imgbox.com/63/35/u8iy0fNe_o.png" width="1200"></p> 
<p>所以接下来就要获取1，2，6</p> 
<p>通过多次错误登录查看1和2 的值不会变，看看最初登录url里面有木有 也要看是什么请求（get）</p> 
<p><img alt="" height="337" src="https://images2.imgbox.com/46/71/jASKZTVt_o.png" width="1200"><img alt="" height="314" src="https://images2.imgbox.com/a2/b8/DF9tOghe_o.png" width="1200"></p> 
<p>里面有</p> 
<p>所以先获取最初的html，然后获取1,2的值（xpth，bs4）</p> 
<p><img alt="" height="759" src="https://images2.imgbox.com/b4/fb/cXZLiNzb_o.png" width="1200"></p> 
<p><img alt="" height="236" src="https://images2.imgbox.com/8e/35/FXEZT50q_o.png" width="1200"></p> 
<p>通过bs4用选择器，现根据id找到该标签然后获取value的值</p> 
<p>因为select返回的是一个列表所以要用切片拿到列表里面数据</p> 
<p><img alt="" height="468" src="https://images2.imgbox.com/67/88/GZ4LqLEE_o.png" width="1200"></p> 
<p>解决1,2后再解决验证码</p> 
<p>因为验证码没登录或刷新都会变</p> 
<p>最好可以动态的拿取到比如url</p> 
<p><img alt="" height="235" src="https://images2.imgbox.com/12/ea/85BjUdtO_o.png" width="1200"></p> 
<p><img alt="" height="423" src="https://images2.imgbox.com/35/98/lAhA2RAu_o.png" width="1200">然后下载下来</p> 
<p><img alt="" height="166" src="https://images2.imgbox.com/70/4e/1Nl2bNq7_o.png" width="1200"></p> 
<p>运行程序时可以打开图片手动输入</p> 
<p>因为目标是一个html所以要写来了，并打开看看是否成功</p> 
<p><img alt="" height="180" src="https://images2.imgbox.com/4c/17/am9W6c0R_o.png" width="1019"><img alt="" height="507" src="https://images2.imgbox.com/12/f6/mqCNruD9_o.png" width="1200"></p> 
<p>因为 访问验证码时会发送一个请求，导致验证码刷新，之后再次访问又会刷新，导致验证码不对</p> 
<p><img alt="" height="180" src="https://images2.imgbox.com/c1/9a/XRszgeCD_o.png" width="1019"></p> 
<p><img alt="" height="166" src="https://images2.imgbox.com/2c/b4/iDUNBZbG_o.png" width="1200"></p> 
<p>解决办法</p> 
<p>reqests里面有一个session方法可以使返回变成对象，要把图片下载下来要用二进制</p> 
<p>查看验证码去文件夹里面去看因为运行后目录里面不一定刷新出来<img alt="" height="244" src="https://images2.imgbox.com/ce/bb/lZv2FeoE_o.png" width="1135">也用session.post()</p> 
<p><img alt="" height="180" src="https://images2.imgbox.com/54/97/p0rDmmLa_o.png" width="1019"></p> 
<p>最后成功</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/80/44/evNl7KAx_o.png" width="1128"></p> 
<p>企业中一般会用第三方软件识别认证码，提高效能 jiyingwang</p> 
<h2>6.scripts</h2> 
<p>当scripts，vrawlspider爬取多页时候 allowed改为域名</p> 
<p><img alt="" height="271" src="https://images2.imgbox.com/a6/90/7mU3KEk2_o.png" width="1187"></p> 
<h3>6.1安装报错时</h3> 
<h3><img alt="" height="690" src="https://images2.imgbox.com/fb/62/ZMVA7sfc_o.png" width="1200">6.2项目创建</h3> 
<p><img alt="" height="676" src="https://images2.imgbox.com/19/0a/crk8dZLM_o.png" width="1200"></p> 
<p>如果请求额url后面以html结尾不要加/</p> 
<h3>6.3response基本使用</h3> 
<p><img alt="" height="770" src="https://images2.imgbox.com/fa/0d/4EzJ1X6w_o.png" width="1200"></p> 
<p>工作原理</p> 
<p><img alt="" height="808" src="https://images2.imgbox.com/a0/90/T1dYqNR0_o.png" width="1200"></p> 
<h3>6.4scrapy shell</h3> 
<p>测试解析代码是否爬到数据（xpath b4）</p> 
<p>用的方法在 1.物理机终端安装ipython</p> 
<p>2.直接输入 scrapy shell 域名（www.baidu.com）</p> 
<p>3.解析</p> 
<p>4.查看内容</p> 
<p><img alt="" height="824" src="https://images2.imgbox.com/a9/ae/0DGsVbu1_o.png" width="930"></p> 
<h3>案例1.当当网爬社会书</h3> 
<h4>1.创建项目以及验证url是否正确</h4> 
<p><img alt="" height="736" src="https://images2.imgbox.com/5d/8f/axkxqDyF_o.png" width="1200"></p> 
<h4>2.在管道中定义有哪些数据需要爬取</h4> 
<p><img alt="" height="372" src="https://images2.imgbox.com/9a/ef/IfjeQusz_o.png" width="1200"></p> 
<h4>3.定位想要爬取东西</h4> 
<p><img alt="" height="837" src="https://images2.imgbox.com/48/08/o8RYZDOv_o.png" width="1200"></p> 
<h4>4.分析一下结构方便 解析</h4> 
<p>要爬取的东西都在这个ul下面</p> 
<p><img alt="" height="775" src="https://images2.imgbox.com/95/07/ohGMAhKQ_o.png" width="1118"></p> 
<h4>5.解析</h4> 
<p>比如</p> 
<p><img alt="" height="465" src="https://images2.imgbox.com/49/28/05u9DZhU_o.png" width="1200"></p> 
<p>通过观察 价格有两个xpath路径</p> 
<p><img alt="" height="696" src="https://images2.imgbox.com/c2/5b/BdvCMrvQ_o.png" width="1200"></p> 
<p><img alt="" height="105" src="https://images2.imgbox.com/09/34/u17WCAZ5_o.png" width="1008">Src</p> 
<p><img alt="" height="108" src="https://images2.imgbox.com/11/ce/iCEvhjt0_o.png" width="1200"></p> 
<p>通过观察又有问题，查看源代码</p> 
<p><img alt="" height="217" src="https://images2.imgbox.com/dc/49/6xsJ9TTt_o.png" width="1200"></p> 
<p>所以图片的xpath也有两个，后续写爬虫时要分情况，用if语句即可，</p> 
<p>条件分析</p> 
<p>都有src，但一个有data-original,一个没有，所以在data-original上做文章</p> 
<p>全部完成后</p> 
<p><img alt="" height="224" src="https://images2.imgbox.com/b9/89/mTAx2lZh_o.png" width="931"></p> 
<h4>6.书写爬虫</h4> 
<p>因为每一个都是包含多个所以想要实现一个名字一个price一个图片，所以最好是找到他们共同的部分，再共同的部分接着xpath</p> 
<p>共同的部分</p> 
<p><img alt="" height="202" src="https://images2.imgbox.com/86/c4/rP4UGnx4_o.png" width="451"></p> 
<p>所以爬虫可以这样写，并验证一下</p> 
<p><img alt="" height="693" src="https://images2.imgbox.com/92/fc/yvIePMAS_o.png" width="1200"></p> 
<p>因为价格有些是区间，如果也爬下来后面的数据清洗要处理，不爬下来也要处理所以不制造麻烦不爬下来</p> 
<p>对于src 都有src，但一个有data-original,一个没有，所以在data-original上做文章</p> 
<p><img alt="" height="381" src="https://images2.imgbox.com/bd/c4/3wHMq3Kn_o.png" width="939">想加序号在爬虫这里加</p> 
<p><img alt="" height="710" src="https://images2.imgbox.com/47/85/99WTqEPV_o.png" width="1200"></p> 
<h4>6.爬取完后就要存储下来pipelines</h4> 
<h5>1.setting中开启一个通道</h5> 
<h5><img alt="" height="173" src="https://images2.imgbox.com/08/f8/stu6dz2d_o.png" width="1147">2.在爬虫文件中导入items定义的类</h5> 
<p><img alt="" height="525" src="https://images2.imgbox.com/20/4b/zgCGm7Vj_o.png" width="1200">导入时那两个红色波浪线是表明书写规范，并不影响运行</p> 
<h5>3.就是items中自己定义的类名</h5> 
<h5><img alt="" height="512" src="https://images2.imgbox.com/3c/6b/p9vYsAWv_o.png" width="902">4.书写提交的代码</h5> 
<p><img alt="" height="284" src="https://images2.imgbox.com/33/49/IURDEjgE_o.png" width="996">socialbookitem中参数名称与items定义的参数名称一致</p> 
<h5>5.在piplines中书写存储代码</h5> 
<p><img alt="" height="472" src="https://images2.imgbox.com/b0/27/SLRQFhir_o.png" width="1200"></p> 
<h4>总结:</h4> 
<p>1.不推荐使用with open（）因为使用withopen每一次传递对象都会开启一次，影响读取，也会产生覆盖效果</p> 
<p>2.xpath不熟悉，不能很快的定位比如爬price时候</p> 
<h3>扩展1多条管道同时进行</h3> 
<h4>1.定义管道类</h4> 
<h5><img alt="" height="445" src="https://images2.imgbox.com/4e/02/91Jl3PgL_o.png" width="1200">2.书写内容</h5> 
<p><img alt="" height="514" src="https://images2.imgbox.com/66/88/Kgc6sKGT_o.png" width="1107"></p> 
<p>需要注意url是否完整</p> 
<p>这里就不完整需要，将他补充完整</p> 
<p>这里最好不要手动创建文件夹容易位置错误，最好自动创建文件夹，导下载图片失败致后面</p> 
<p>青蓝色是自己手动创建</p> 
<p>红色的是自动创建</p> 
<p><img alt="" height="696" src="https://images2.imgbox.com/7d/4a/zcjQxVro_o.png" width="1200"></p> 
<h4>3.再开启一条通道</h4> 
<p>模仿上面即可</p> 
<p>格式为路径+管道到类名</p> 
<p><img alt="" height="548" src="https://images2.imgbox.com/a8/03/3v1CLzLp_o.png" width="1200"></p> 
<h4>4.总结</h4> 
<p><img alt="" height="439" src="https://images2.imgbox.com/42/9b/OhBt545u_o.png" width="1096"><img alt="" height="567" src="https://images2.imgbox.com/b6/cd/rrg8Pk83_o.png" width="943"><span style="color:#fe2c24;"><strong>因此，每个管道类中的 </strong><strong><code>process_item</code></strong><strong> 方法名必须相同，</strong></span></p> 
<p><img alt="" height="672" src="https://images2.imgbox.com/54/bb/YhO1xvTI_o.png" width="1200"></p> 
<h3>扩展2多页下载</h3> 
<h4>1.针对allowed_domains改动</h4> 
<p>域名不要https，一般</p> 
<p><img alt="" height="187" src="https://images2.imgbox.com/98/6e/mC9Kbm9u_o.png" width="1197"></p> 
<h4>2.分析每页的URL有什么不同</h4> 
<p>#第二页</p> 
<p>#https: //category.dangdang.com/pg2-cp01.21.12.00.00.00.html</p> 
<p>#第三页</p> 
<p>#https://category.dangdang.com/pg3-cp01.21.12.00.00.00.html</p> 
<p>#第四页</p> 
<p>#https://category.dangdang.com/pg4-cp01.21.12.00.00.00.html</p> 
<h4>3.写代码</h4> 
<p><img alt="" height="484" src="https://images2.imgbox.com/b4/6a/PEqAjWpC_o.png" width="1171"><img alt="" height="640" src="https://images2.imgbox.com/a8/6c/jpcVK2TR_o.png" width="1200"></p> 
<p>utl设置完成后，关键的是如何调用</p> 
<p>yield scrapy.Request(url=url,callback=self.parse)</p> 
<p>url就是每页的url，callback=（调用的函数名）</p> 
<h3>扩展3当从一个页面跳转到另一个子页面时可以多次</h3> 
<h4>1.先找到跳转页面的url，要发出请求</h4> 
<p><img alt="" height="676" src="https://images2.imgbox.com/9b/d8/CUe5QL1j_o.png" width="1200"></p> 
<h4>2.拿到了，接着发出请求，并验证</h4> 
<p>跟当当网爬第二页同样的思路，只不过图片xpath在另一个URL后，再次解析</p> 
<p><img alt="" height="831" src="https://images2.imgbox.com/57/a3/pRkgVilK_o.png" width="1200"></p> 
<p>#src=//div[@id='Zoom']//img//@src</p> 
<p>https://img9.doubanio.com/view/photo/l_ratio_poster/public/p2910488597.jpg</p> 
<p>https://img9.doubanio.com/view/photo/l_ratio_poster/public/p2910208192.jpg</p> 
<p>排除一下，因为每次都是不同url，图片的xpath可能会不一样</p> 
<p>注意：当黑框能出来但爬不出来，大部分是xpath路经问题，可以对路径删，该</p> 
<p><img alt="" height="82" src="https://images2.imgbox.com/90/24/0ItN79os_o.png" width="1200"></p> 
<h4>3.如何把第一个的name给到第二个呢</h4> 
<p>scrapy.reqiest里面有meta可以强制转换成字典</p> 
<p>第二个方法接收一下</p> 
<h4><img alt="" height="639" src="https://images2.imgbox.com/22/8d/pZpHdiFi_o.png" width="1200">4.然后上传和导入items方法</h4> 
<p><img alt="" height="550" src="https://images2.imgbox.com/82/9e/vJnV96Fd_o.png" width="1200"></p> 
<h4>5.跟之前一样的在piplines中书写存储代码</h4> 
<p>如果多条管道也是同样的方法</p> 
<h3>扩展四链接提取器crawlspider（读书网）</h3> 
<p>跟scrapy很相似</p> 
<p>之前爬多页时是知道最后有多少页，但不知道的时候，可以用</p> 
<h4><img alt="" height="804" src="https://images2.imgbox.com/4e/73/OwmbzKJb_o.png" width="1097">1.创建项目</h4> 
<h4><img alt="" height="278" src="https://images2.imgbox.com/45/a1/M4Ud9eWv_o.png" width="832">2.写正则</h4> 
<p><img alt="" height="820" src="https://images2.imgbox.com/2e/8d/a9PldH4q_o.png" width="1200"></p> 
<p>例如</p> 
<p>\d代表数字，+</p> 
<p>.有时候不生效所以加个转义 \</p> 
<p><span style="color:#fe2c24;">观察是从2开始，因为是根据正则表达式，符合的爬，不符合不爬</span></p> 
<p><span style="color:#fe2c24;">https://www.dushu.com/book/1188.html这是第一页，并不符合，所以改成一样的 格式</span></p> 
<p><img alt="" height="798" src="https://images2.imgbox.com/96/3c/EZyv1fic_o.png" width="1200">爬多页的时候写域名</p> 
<p><img alt="" height="293" src="https://images2.imgbox.com/bb/f0/P5pkSMbo_o.png" width="989"><img alt="" height="403" src="https://images2.imgbox.com/93/ad/Cdqh3dA4_o.png" width="1200"></p> 
<h4>3.解析以及结果</h4> 
<p>图片打不开是正常的</p> 
<p><img alt="" height="675" src="https://images2.imgbox.com/bc/8e/k6S07m6s_o.png" width="1200"><img alt="" height="666" src="https://images2.imgbox.com/6d/17/rkQlGEfy_o.png" width="1200"></p> 
<h4>4.其他的一样的提交，存储（写代码），导入，开导管</h4> 
<h4>5.结果</h4> 
<p><img alt="" height="759" src="https://images2.imgbox.com/ac/1f/nw6EkcoU_o.png" width="849"></p> 
<h3>扩展五存入linux中数据库</h3> 
<p>首先在linux安装mysql</p> 
<p>老师给的mysql初始化操作</p> 
<h4>1.创建好数据库</h4> 
<p>格式一致</p> 
<p><img alt="" height="500" src="https://images2.imgbox.com/9e/11/dAiSXajD_o.png" width="507"></p> 
<h4>2.准备插入数据前的东西</h4> 
<p>1.linux虚拟机的ip ：192.168.186.129</p> 
<p>2.端口号:3306 (是一个整数)</p> 
<p>3.user:</p> 
<p>4.password:</p> 
<p>5.数据库名:crawlspider</p> 
<p>6.数据库的字符集：utf8</p> 
<p>想用其他暂时只需要该ip，password，数据库名</p> 
<p><img alt="" height="95" src="https://images2.imgbox.com/35/9b/yrHAVPRM_o.png" width="808"></p> 
<h4>3.在setting写下准备的东西</h4> 
<p><span style="color:#fe2c24;">utf-8 这里不写-，端口号必须为整数</span></p> 
<h4><img alt="" height="309" src="https://images2.imgbox.com/af/67/XozTSgwY_o.png" width="717">4.在管道中创建一个，并设置优先级</h4> 
<p><img alt="" height="529" src="https://images2.imgbox.com/85/d9/Dl5wfS2c_o.png" width="1200"></p> 
<h4>5.pymysql</h4> 
<p>如果没有就安装pymysql</p> 
<p>这里我本来就有</p> 
<h4><img alt="" height="252" src="https://images2.imgbox.com/18/c7/waRFng7a_o.png" width="1199">6.加载settings文件</h4> 
<h4><img alt="" height="565" src="https://images2.imgbox.com/2b/d4/d2ePaUhk_o.png" width="1200">7.书写连接setting文件</h4> 
<p><img alt="" height="669" src="https://images2.imgbox.com/5f/b9/P4WezJCo_o.png" width="807"><img alt="" height="476" src="https://images2.imgbox.com/e5/35/tbJemEBf_o.png" width="735"></p> 
<h4>8.写pymysql语句</h4> 
<p>先导入pymysql，写，最后关闭</p> 
<p><img alt="" height="533" src="https://images2.imgbox.com/86/59/s4tUb5Dz_o.png" width="1200"><img alt="" height="525" src="https://images2.imgbox.com/78/80/Uh5DM8hN_o.png" width="1200"></p> 
<h4>9.运行之后的问题</h4> 
<h5>1.权限问题</h5> 
<p><img alt="" height="649" src="https://images2.imgbox.com/c8/73/0uyvHM8M_o.png" width="1200"></p> 
<p>pymysql.err.OperationalError: (1045, "Access denied for user 'root'@'192.168.186.100' (using password: YES)")</p> 
<p>错误信息 <code>pymysql.err.OperationalError: (1045, "Access denied for user 'root'@'192.168.186.100' (using password: YES)")</code> 表示MySQL服务器拒绝了尝试使用用户名 <code>root</code> 和IP地址 <code>192.168.186.100</code> 进行登录的请求。这通常是因为以下几个原因：</p> 
<p><strong>1.检查用户权限</strong>：确保数据库用户 <code>user1</code> 有权限从 IP 地址 <code>192.168.186.100</code> 访问数据库。这通常在 MySQL 中通过 <code>GRANT</code> 语句完成。你可能需要联系数据库管理员或检查 MySQL 的用户表来确认这一点。</p> 
<p>你需要登录到 MySQL 数据库，检查 'user1' 的权限设置。可以使用以下 SQL 命令查看用户权限：</p> 
<pre></pre> 
<p><code>SELECT user, host FROM mysql.user WHERE user = 'user1'; </code></p> 
<p></p> 
<p>发现没有用户</p> 
<h5>2.mysql密码设置问题 创建用户显示</h5> 
<p><img alt="" height="73" src="https://images2.imgbox.com/7d/19/lb7w7TNv_o.png" width="1045"></p> 
<p>解释:</p> 
<p>你尝试设置的密码不符合MySQL的密码策略要求。MySQL有一组密码强度验证规则，以确保密码足够强大，难以被猜测或破解。</p> 
<p>通过以下命令查看密码规则 SHOW VARIABLES LIKE 'validate_password%';</p> 
<p>显示:</p> 
<p>Variable_name | Value |</p> 
<p>+--------------------------------------+--------+</p> 
<p>| validate_password_check_user_name | OFF | 是否包含用户名 on：不能包含用户名 off:可以包含</p> 
<p>| validate_password_dictionary_file | | 如果它是一个空值，表示没有使用外部字典文件来检查密码。</p> 
<p>| validate_password_length | 8 | 最小长度</p> 
<p>| validate_password_mixed_case_count | 1 | 必须包含的不同大小写字母的最小数量 设置为<code>1</code>意味着密码至少需要包含一个大写字母和一个小写字母。</p> 
<p>| validate_password_number_count | 1 | 必须包含的数字的最小数量</p> 
<p>| validate_password_policy | MEDIUM |</p> 
<ul><li> <p>这个变量设置密码的验证策略强度。可能的值包括：</p> 
  <ul><li> <p><code>LOW</code>：只检查密码长度。</p> </li><li> <p><code>MEDIUM</code>：检查密码长度、混合大小写字母、数字和特殊字符。</p> </li><li> <p><code>STRONG</code>：除了<code>MEDIUM</code>的所有检查外，还检查密码是否在字典文件中。</p> </li></ul></li></ul> 
<p>| validate_password_special_char_count | 1 | 这个变量设置密码中必须包含的特殊字符的最小数量。</p> 
<p>+--------------------------------------+--------</p> 
<p>我创的用户名密码:GRANT ALL PRIVILEGES ON crawlspider.* TO 'user1'@'192.168.186.100' IDENTIFIED BY ' 自己设';</p> 
<h6>3 检查MySQL用户权限</h6> 
<p>确保MySQL用户<code>user1</code>有权限从<code>192.168.186.100</code>访问数据库。可以通过以下SQL命令检查和修改用户权限：</p> 
<pre></pre> 
<p><code><em>-- 检查用户权限</em> SELECT User, Host FROM mysql.user WHERE User = 'user1'; </code></p> 
<p><code>检查用户具体权限 SHOW GRANTS FOR 'user1'@'localhost';</code></p> 
<p><code><em>-- 如果权限不正确，可以重新设置权限</em> GRANT ALL PRIVILEGES ON crawlspider.* TO 'user1'@'192.168.186.100' IDENTIFIED BY 'B@d2202daifulin';</code></p> 
<p><code>解释 ALL PRIVILEGES 表示授予所有可能的权限，这包括但不限于SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, REFERENCES, INDEX, ALTER, CREATE TEMPORARY TABLES, LOCK TABLES, EXECUTE, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, ALTER ROUTINE, EVENT, TRIGGER等。 </code></p> 
<p><code><strong>ON</strong><strong> crawlspider.</strong><strong>*</strong>: </code></p> 
<p><code>ON 关键字后面跟着指定权限应用的范围。</code></p> 
<p><code>crawlspider.* 表示这些权限将被授予crawlspider数据库中的所有表（* 是通配符，代表所有表）。</code></p> 
<p><code><strong>TO</strong><strong> ‘user1’@‘</strong><strong>192.168.186.100</strong><strong>’</strong>: TO 关键字后面跟着指定的用户和主机。 </code></p> 
<p><code>'user1' 是被授予权限的用户名。</code></p> 
<p><code>'192.168.186.100' 是允许该用户连接到MySQL服务器的主机IP地址。这意味着只有从IP地址192.168.186.100发起的连接才会使用这些权限。 <strong>IDENTIFIED </strong><strong>BY</strong><strong> ‘bd2202daifulin’</strong>: IDENTIFIED BY 关键字用于设置用户的密码。 </code></p> 
<p><code>' ' 是用户user1的新密码。 </code></p> 
<p><code>FLUSH PRIVILEGES; 解读: 在执行了任何修改权限的命令后，都应该执行 FLUSH PRIVILEGES; 命令，以确保所做的更改能够立即生效，而无需重启MySQL服务器</code></p> 
<p>权限解读:</p> 
<p><img alt="" height="201" src="https://images2.imgbox.com/4a/27/PpKVR6oy_o.png" width="791"></p> 
<p><strong>User</strong>: <code>user1</code></p> 
<p>这表示数据库中存在一个用户名为<code>user1</code>的用户账</p> 
<p></p> 
<p>请注意，<code>USAGE</code>权限本身并不授予对任何数据库或表的访问权限，它只是表示登录权限。实际的权限（如<code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>, <code>CREATE</code>, <code>DROP</code>等）必须明确地授予。如果你没有看到<code>SHOW GRANTS</code>命令的输出，那么可能意味着<code>user1</code>没有分配任何权限，或者仅有基本的登录权限。</p> 
<p></p> 
<ul><li> <p><strong>Host</strong>: <code>localhost</code></p> </li><li> <p>这表示<code>use</code><code>r1</code>账户只能从本地主机（即运行MySQL服务器的主机）进行连接。如果这里的值是<code>%</code>，则表示<code>user1</code>可以从任何主机连接到MySQL服务器</p> 自己的<img alt="" height="186" src="https://images2.imgbox.com/bf/0f/Vao5dLmL_o.png" width="663"></li><li> <p>更全的展示：</p> </li><li> <p><img alt="" height="305" src="https://images2.imgbox.com/97/7a/oiJEI8oH_o.png" width="930"></p> </li><li> <p><code>USAGE ON </code><em><code>.</code></em><code> TO 'user1'@'</code><code>localhost</code><code>'</code> 表示<code>user1</code>用户有权限登录到MySQL服务器，但没有指定数据库的任何操作权限。</p> </li><li> <p><code>GRANT SELECT, INSERT, UPDATE ON </code>mydatabase<code>.* TO 'user1'@'</code><code>localhost</code><code>'</code> 表示<code>user1</code>用户在<code>mydatabase</code>数据库上有<code>SELECT</code>, <code>INSERT</code>, 和 <code>UPDATE</code>权限。</p> </li></ul> 
<h6>4. 检查MySQL配置</h6> 
<p>确保MySQL服务器配置允许从<code>192.168.186.100</code>进行访问。检查MySQL的<code>my.cnf</code>或<code>my.ini</code>配置文件中的<code>bind-address</code>设置，确保它允许远程连接。 一般位置:</p> 
<p></p> 
<p><strong>基于</strong><strong>Debian</strong><strong>和Ubuntu的发行版</strong>：</p> 
<ol><li> <p><code>/etc/mysql/mysql.conf.d/mysqld.cnf</code></p> </li><li> <p>或者 <code>/etc/mysql/my.cnf</code></p> </li></ol> 
<p><strong>基于</strong><strong>Red Hat</strong><strong>、</strong><strong>CentOS</strong><strong>和</strong><strong>Fedora</strong><strong>的发行版</strong>：</p> 
<ol><li> <p><code>/etc/my.cnf</code></p> </li><li> <p><code>/etc/mysql/my.cnf</code></p> </li><li> <p>或者 <code>/etc/my.cnf.d/mysql-server.cnf</code></p> </li></ol> 
<p>进入后会看到</p> 
<p><img alt="" height="709" src="https://images2.imgbox.com/a3/eb/DIIjNpFU_o.png" width="942"></p> 
<p></p> 
<p>。bind-address默认情况下，它被设置为 <code>127.0.0.1</code>，这意味着MySQL服务器只接受来自本地主机的连接。如果你想允许远程连接，可以将这个值更改为服务器的公共IP地址或者设置为 <code>0.0.0.0</code> 以监听所有网络接口。</p> 
<p>设置完后要重启mysql服务:</p> 
<h5>3.mysql服务问题</h5> 
<p>systemctl restart mysql</p> 
<p>如果启动时显示:</p> 
<p>解决思路:</p> 
<p><strong>检查服务名称</strong></p> 
<p>查看所有已安装的服务：</p> 
<p>systemctl list-unit-files --type=service</p> 
<p>找到与mysql有关的:</p> 
<p><img alt="" height="69" src="https://images2.imgbox.com/8f/40/mvuovuLs_o.png" width="766"></p> 
<p>第二个是表示:它表示一个模板服务单元，用于创建实例化的服务单元。</p> 
<p>如，如果你想启动两个MySQL服务实例，每个实例监听不同的端口，你可以这样做：</p> 
<p>bash</p> 
<pre></pre> 
<p><code>systemctl start mysqld@instance1.service </code></p> 
<p><code>systemctl start mysqld@instance2.service</code></p> 
<p>在这里，<code>instance1</code> 和 <code>instance2</code> 是实例的名称，它们可以根据需要自定义。每个实例将根据 <code>mysqld@.service</code> 模板创建，但具有自己的特定配置，如不同的端口、数据目录或其他设置</p> 
<h6>5<strong>检查数据库服务状态</strong>：确保数据库服务正在运行，并且监听在正确的端口上。你可以使用命令 <code>mysqladmin ping</code> 或 <code>netstat -an | grep 3306</code> 来检查服务状态。</h6> 
<p><img alt="" height="75" src="https://images2.imgbox.com/97/e4/cATruRRj_o.png" width="938"></p> 
<p>解读:</p> 
<ul><li> <p><strong>tcp</strong>：这表示连接使用的是 TCP（传输控制协议）。</p> </li><li> <p><strong>0</strong>：这是发送队列中的字节数。由于这里显示为 0，这意味着当前没有数据在发送队列中等待发送。</p> </li><li> <p><strong>0</strong>：这是接收队列中的字节数。同样，0 表示接收队列中没有数据等待处理。</p> </li><li> <p><strong>0.0.0.0:3306</strong>：这表示 MySQL 服务器正在监听所有可用的网络接口上的端口 3306。<code>0.0.0.0</code> 是一个特殊的 IP 地址，代表所有可用的 IP 地址（即服务器上的所有网络接口）。</p> </li><li> <p><strong>0.0.0.0:</strong>*：这表示 MySQL 服务器准备好接受来自任何 IP 地址的连接。星号 <code>*</code> 表示对任何远程 IP 地址和端口的监听。</p> </li></ul> 
<p>所以: MySQL 服务器正在运行，并且正在监听所有网络接口上的端口 3306，准备接受来自任何远程主机的连接请求</p> 
<ul><li> <p><strong>LISTEN</strong>：这表示 MySQL 服务器正在监听状态，准备好接受进入的连接请求。</p> </li></ul> 
<h6>6<strong>检查网络连接</strong>：确保从你的 Scrapy 爬虫运行的机器到数据库服务器的网络连接没有问题。你可以使用 <code>ping</code> 或 <code>traceroute</code> 命令来测试网络连</h6> 
<p>ping &lt;数据库服务器IP&gt;,traceroute &lt;数据库服务器IP&gt;</p> 
<p>数据库服务ip就是bind-address 的值</p> 
<p>如果bind-address=0.0.0.0就不是这个值</p> 
<p>只能查看</p> 
<p>linux:</p> 
<p>如果你的数据库服务器和Scrapy爬虫在同一台机器上</p> 
<p>hostname -I</p> 
<p>当前机器的所有网络接口的IP地址</p> 
<p>ifconfig或ip addr</p> 
<p>展示:</p> 
<p><img alt="" height="360" src="https://images2.imgbox.com/f6/5d/XeD2O8yu_o.png" width="783"></p> 
<p><strong>3检查防火墙设置</strong>：确保没有防火墙规则阻止了从 Scrapy 爬虫运行机器到数据库服务器的连接</p> 
<p><img alt="" height="151" src="https://images2.imgbox.com/56/c3/RlzYRmCu_o.png" width="966">windows打开“控制面板”，然后选择“系统和安全”&gt;“Windows Defender 防火墙”,在Windows防火墙的高级设置中，你可以查看或编辑入站规则。确保3306端口有一个允许的规则。</p> 
<p><img alt="" height="207" src="https://images2.imgbox.com/b4/49/0miq9dqn_o.png" width="895"></p> 
<h5>4.字符集问题</h5> 
<p>运行后出现的问题:</p> 
<p>SQL error: (1366, "Incorrect string value: '\\xE6\\x9C\\x9D\\xE6\\x9E\\x9D...' for column 'name' at row 1") 这个错误是由于字符集或编码问题引起的。</p> 
<p>解决办法</p> 
<p><strong>检查数据库和表的</strong><strong>字符集</strong></p> 
<p>确保您的数据库和表都使用的是<code>utf8</code>或<code>utf8mb4</code>字符集。<code>utf8mb4</code>是一个更好的选择</p> 
<p>数据库</p> 
<p>SELECT DEFAULT_CHARACTER_SET_NAME</p> 
<p>FROM information_schema.SCHEMATA</p> 
<p>WHERE SCHEMA_NAME = 'crawlspider';</p> 
<p>结果</p> 
<p><img alt="" height="209" src="https://images2.imgbox.com/c4/72/is2nuWUi_o.png" width="536"></p> 
<p>查找特定表</p> 
<p>SHOW FULL COLUMNS FROM crawlspider.book;</p> 
<p><img alt="" height="277" src="https://images2.imgbox.com/14/2b/pYUHFNzG_o.png" width="1200">该表中name，src 字符集为utf-8（scrapy setting设置好了的）</p> 
<p>之后检查是否成功存入</p> 
<h5>综上所述:</h5> 
<p>在写完scrapy之后</p> 
<p>1.mysql是否安装</p> 
<p>2.mysql是否创建用户和用户权限，特别注意创建用户时密码要求，用户是否允许从哪些登录</p> 
<p>3.mysql配置是否能被其他访问</p> 
<p>4.mysql服务状态</p> 
<p>5.网络连接</p> 
<p>6.创建数据库和表的时候要注意格式和每个字段的字符集</p> 
<h3>扩展六日志信息及日志级别</h3> 
<p><img alt="" height="425" src="https://images2.imgbox.com/45/8b/1gsoZ3qO_o.png" width="883">在settings里面写</p> 
<p><img alt="" height="81" src="https://images2.imgbox.com/a2/e8/ytX2cVQU_o.png" width="510"></p> 
<h3>扩展七post请求</h3> 
<p><img alt="" height="798" src="https://images2.imgbox.com/60/27/VzAdKkxr_o.png" width="1160"></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0d186fc6cc7361279347a78c6658b312/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">c语言——运算符</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ad5419df6476a0855d2342a4c61070c4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">lua 实现 函数 判断两个时间戳是否在同一天</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>