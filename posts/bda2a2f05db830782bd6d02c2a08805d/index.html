<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>使用Amazon SageMaker JumpStart微调Meta Llama 3.1模型以进行生成式AI推理 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/bda2a2f05db830782bd6d02c2a08805d/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="使用Amazon SageMaker JumpStart微调Meta Llama 3.1模型以进行生成式AI推理">
  <meta property="og:description" content="文章目录 使用Amazon SageMaker JumpStart微调Meta Llama 3.1模型以进行生成式AI推理Meta Llama 3.1SageMaker JumpStartSageMaker JumpStart中Meta Llama 3.1模型的微调配置使用SageMaker JumpStart UI进行无代码微调使用SageMaker JumpStart SDK进行微调结论 使用Amazon SageMaker JumpStart微调Meta Llama 3.1模型以进行生成式AI推理 通过Amazon SageMaker JumpStart微调Meta Llama 3.1模型，开发者可以定制这些公开的基础模型（FM）。Meta Llama 3.1系列代表了生成式人工智能（AI）领域的重大进展，提供了一系列功能以创建创新的应用程序。Meta Llama 3.1模型有多种规模，包括80亿、700亿和4050亿参数，适应各种项目需求。亚马逊云科技链接：https://aws.amazon.com/cn/free
这些模型的突出特点在于其理解和生成文本的能力，具有令人印象深刻的一致性和细腻的表达。凭借高达128,000个标记的上下文长度，Meta Llama 3.1模型能够保持深度的上下文意识，从而轻松处理复杂的语言任务。此外，这些模型经过优化，以实现高效推理，并采用了诸如分组查询注意力（GQA）等技术，提供快速响应能力。
在本文中，演示了如何使用SageMaker JumpStart微调Meta Llama 3.1预训练的文本生成模型。
Meta Llama 3.1 Meta Llama 3.1模型的一大特点是其多语言能力。这些仅限文本的指令调优版本（8B、70B、405B）旨在进行自然语言对话，并且在常见行业基准测试中表现优于许多公开的聊天机器人模型。这使得它们非常适合构建引人入胜的多语言对话体验，能够跨越语言障碍，为用户提供沉浸式互动。
Meta Llama 3.1模型的核心是经过精心优化的自回归变压器架构。模型的调优版本还采用了先进的微调技术，如监督微调（SFT）和人类反馈的强化学习（RLHF），以使模型输出与人类偏好保持一致。这种程度的优化为开发者打开了新的可能性，现在他们可以根据应用程序的独特需求调整这些强大的语言模型。
微调过程允许用户使用新数据调整预训练的Meta Llama 3.1模型的权重，从而提高它们在特定任务上的表现。这涉及将模型训练在针对任务量身定制的数据集上，并更新模型的权重以适应新数据。微调通常可以通过最小的努力带来显著的性能提升，使开发者能够快速满足其应用程序的需求。
SageMaker JumpStart现在支持Meta Llama 3.1模型，开发者可以通过SageMaker JumpStart UI和SDK探索微调Meta Llama 3.1 405B模型的过程。本文演示了如何轻松定制这些模型以满足特定用例，无论是在构建多语言聊天机器人、代码生成助手，还是其他任何生成式AI应用程序。提供了使用SageMaker JumpStart UI进行无代码微调的示例，以及使用SDK进行微调的示例。
SageMaker JumpStart 通过SageMaker JumpStart，机器学习（ML）从业者可以从广泛的公开基础模型中进行选择。可以将基础模型部署到从网络隔离的环境中专用的Amazon SageMaker实例上，并使用SageMaker进行模型训练和部署的定制。
现在，可以通过Amazon SageMaker Studio或通过SageMaker Python SDK编程方式，以几次点击的方式发现并部署Meta Llama 3.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-09-04T14:27:55+08:00">
    <meta property="article:modified_time" content="2024-09-04T14:27:55+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">使用Amazon SageMaker JumpStart微调Meta Llama 3.1模型以进行生成式AI推理</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#Amazon_SageMaker_JumpStartMeta_Llama_31AI_2" rel="nofollow">使用Amazon SageMaker JumpStart微调Meta Llama 3.1模型以进行生成式AI推理</a></li><li><ul><li><a href="#Meta_Llama_31_10" rel="nofollow">Meta Llama 3.1</a></li><li><a href="#SageMaker_JumpStart_20" rel="nofollow">SageMaker JumpStart</a></li><li><a href="#SageMaker_JumpStartMeta_Llama_31_26" rel="nofollow">SageMaker JumpStart中Meta Llama 3.1模型的微调配置</a></li><li><a href="#SageMaker_JumpStart_UI_42" rel="nofollow">使用SageMaker JumpStart UI进行无代码微调</a></li><li><a href="#SageMaker_JumpStart_SDK_82" rel="nofollow">使用SageMaker JumpStart SDK进行微调</a></li><li><a href="#_196" rel="nofollow">结论</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="Amazon_SageMaker_JumpStartMeta_Llama_31AI_2"></a>使用Amazon SageMaker JumpStart微调Meta Llama 3.1模型以进行生成式AI推理</h2> 
<p>通过<a href="https://aws.amazon.com/sagemaker/jumpstart/?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386" rel="nofollow">Amazon SageMaker JumpStart</a>微调Meta Llama 3.1模型，开发者可以定制这些公开的基础模型（FM）。Meta Llama 3.1系列代表了<a href="https://aws.amazon.com/generative-ai/?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386" rel="nofollow">生成式人工智能（AI）</a>领域的重大进展，提供了一系列功能以创建创新的应用程序。Meta Llama 3.1模型有多种规模，包括80亿、700亿和4050亿参数，适应各种项目需求。亚马逊云科技链接：<a href="https://aws.amazon.com/cn/free/?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386" rel="nofollow">https://aws.amazon.com/cn/free</a></p> 
<p>这些模型的突出特点在于其理解和生成文本的能力，具有令人印象深刻的一致性和细腻的表达。凭借高达128,000个标记的上下文长度，Meta Llama 3.1模型能够保持深度的上下文意识，从而轻松处理复杂的语言任务。此外，这些模型经过优化，以实现高效推理，并采用了诸如分组查询注意力（GQA）等技术，提供快速响应能力。</p> 
<p>在本文中，演示了如何使用SageMaker JumpStart微调Meta Llama 3.1预训练的文本生成模型。</p> 
<h3><a id="Meta_Llama_31_10"></a>Meta Llama 3.1</h3> 
<p>Meta Llama 3.1模型的一大特点是其多语言能力。这些仅限文本的指令调优版本（8B、70B、405B）旨在进行自然语言对话，并且在常见行业基准测试中表现优于许多公开的聊天机器人模型。这使得它们非常适合构建引人入胜的多语言对话体验，能够跨越语言障碍，为用户提供沉浸式互动。</p> 
<p>Meta Llama 3.1模型的核心是经过精心优化的自回归变压器架构。模型的调优版本还采用了先进的微调技术，如监督微调（SFT）和人类反馈的强化学习（RLHF），以使模型输出与人类偏好保持一致。这种程度的优化为开发者打开了新的可能性，现在他们可以根据应用程序的独特需求调整这些强大的语言模型。</p> 
<p>微调过程允许用户使用新数据调整预训练的Meta Llama 3.1模型的权重，从而提高它们在特定任务上的表现。这涉及将模型训练在针对任务量身定制的数据集上，并更新模型的权重以适应新数据。微调通常可以通过最小的努力带来显著的性能提升，使开发者能够快速满足其应用程序的需求。</p> 
<p>SageMaker JumpStart现在支持Meta Llama 3.1模型，开发者可以通过SageMaker JumpStart UI和SDK探索微调Meta Llama 3.1 405B模型的过程。本文演示了如何轻松定制这些模型以满足特定用例，无论是在构建多语言聊天机器人、代码生成助手，还是其他任何生成式AI应用程序。提供了使用SageMaker JumpStart UI进行无代码微调的示例，以及使用SDK进行微调的示例。</p> 
<h3><a id="SageMaker_JumpStart_20"></a>SageMaker JumpStart</h3> 
<p>通过SageMaker JumpStart，机器学习（ML）从业者可以从广泛的公开基础模型中进行选择。可以将基础模型部署到从网络隔离的环境中专用的<a href="https://aws.amazon.com/sagemaker/?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386" rel="nofollow">Amazon SageMaker</a>实例上，并使用SageMaker进行模型训练和部署的定制。</p> 
<p>现在，可以通过<a href="https://aws.amazon.com/sagemaker/studio/?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386" rel="nofollow">Amazon SageMaker Studio</a>或通过SageMaker Python SDK编程方式，以几次点击的方式发现并部署Meta Llama 3.1，从而借助SageMaker功能（如<a href="https://aws.amazon.com/sagemaker/pipelines/?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386" rel="nofollow">Amazon SageMaker Pipelines</a>、<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386" rel="nofollow">Amazon SageMaker Debugger</a>或容器日志）实现模型性能和机器学习操作（MLOps）控制。模型在亚马逊云科技的安全环境中部署，并在虚拟私有云（VPC）控制之下，提供数据安全。此外，还可以使用SageMaker JumpStart微调Meta Llama 3.1 8B、70B和405B基础和指令变体的文本生成模型。</p> 
<h3><a id="SageMaker_JumpStartMeta_Llama_31_26"></a>SageMaker JumpStart中Meta Llama 3.1模型的微调配置</h3> 
<p>SageMaker JumpStart为Meta Llama 3.1 405B、70B和8B变体提供了以下默认配置的微调，使用了<a href="https://arxiv.org/abs/2305.14314?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386" rel="nofollow">QLoRA</a>技术。</p> 
<table><thead><tr><th><strong>模型ID</strong></th><th><strong>训练实例</strong></th><th><strong>输入序列长度</strong></th><th><strong>训练批次大小</strong></th><th><strong>自监督训练的类型</strong></th><th><strong>QLoRA/LoRA</strong></th><th></th><th></th></tr></thead><tbody><tr><td><strong>领域适应微调</strong></td><td><strong>指令微调</strong></td><td><strong>聊天微调</strong></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>meta-textgeneration-llama-3-1-405b-instruct-fp8</td><td>ml.p5.48xlarge</td><td>8,000</td><td>8</td><td>✓</td><td>计划中</td><td>✓</td><td>QLoRA</td></tr><tr><td>meta-textgeneration-llama-3-1-405b-fp8</td><td>ml.p5.48xlarge</td><td>8,000</td><td>8</td><td>✓</td><td>计划中</td><td>✓</td><td>QLoRA</td></tr><tr><td>meta-textgeneration-llama-3-1-70b-instruct</td><td>ml.g5.48xlarge</td><td>2,000</td><td>8</td><td>✓</td><td>✓</td><td>✓</td><td>QLoRA (8位)</td></tr><tr><td>meta-textgeneration-llama-3-1-70b</td><td>ml.g5.48xlarge</td><td>2,000</td><td>8</td><td>✓</td><td>✓</td><td>✓</td><td>QLoRA (8位)</td></tr><tr><td>meta-textgeneration-llama-3-1-8b-instruct</td><td>ml.g5.12xlarge</td><td>2,000</td><td>4</td><td>✓</td><td>✓</td><td>✓</td><td>LoRA</td></tr><tr><td>meta-textgeneration-llama-3-1-8b</td><td>ml.g5.12xlarge</td><td>2,000</td><td>4</td><td>✓</td><td>✓</td><td>✓</td><td>LoRA</td></tr></tbody></table> 
<p>可以使用SageMaker Studio UI或SageMaker Python SDK微调这些模型。将在本文中讨论这两种方法。</p> 
<h3><a id="SageMaker_JumpStart_UI_42"></a>使用SageMaker JumpStart UI进行无代码微调</h3> 
<p>在SageMaker Studio中，可以通过SageMaker JumpStart访问Meta Llama 3.1模型，点击<strong>模型、笔记本和解决方案</strong>，如下图所示。</p> 
<p><img src="https://images2.imgbox.com/d2/d5/7vrRswbU_o.jpg" alt="img"></p> 
<p>如果看不到任何Meta Llama 3.1模型，请通过关闭并重新启动更新SageMaker Studio版本。有关版本更新的更多信息，请参阅<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update-apps.html?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386" rel="nofollow">关闭并更新Studio经典应用程序</a>。</p> 
<p>还可以选择<strong>探索所有文本生成模型</strong>或在搜索框中搜索llama 3.1，查找其他模型变体。</p> 
<p><img src="https://images2.imgbox.com/43/5a/Zd5uzbZl_o.jpg" alt="img"></p> 
<p><img src="https://images2.imgbox.com/88/3c/JPi2SBKW_o.jpg" alt="img"></p> 
<p>选择模型卡后，可以看到模型详细信息，包括是否可以用于部署或微调。此外，还可以配置训练</p> 
<p>和验证数据集的位置、部署配置、超参数和微调的安全设置。如果选择<strong>微调</strong>，可以查看可用的微调选项。然后选择<strong>训练</strong>，即可在SageMaker ML实例上启动训练作业。</p> 
<p>下图显示了Meta Llama 3.1 405B模型的微调页面；不过，可以使用各自的模型页面以类似方式微调8B和70B Llama 3.1文本生成模型。</p> 
<p>要微调这些模型，需要提供以下内容：</p> 
<ul><li><a href="http://aws.amazon.com/s3?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386" rel="nofollow">Amazon简单存储服务</a>（Amazon S3）用于训练数据集位置的URI</li><li>用于模型训练的超参数</li><li>Amazon S3用于输出工件位置的URI</li><li>训练实例</li><li>VPC</li><li>加密设置</li><li>训练作业名称</li></ul> 
<p><img src="https://images2.imgbox.com/de/b6/WkaaFEaP_o.jpg" alt="img"></p> 
<p>要使用Meta Llama 3.1模型，需要接受最终用户许可协议（EULA）。选择<strong>训练</strong>时，将显示它，如下图所示。选择<strong>我已阅读并接受EULA和AUP</strong>以开始微调作业。</p> 
<p><img src="https://images2.imgbox.com/5a/14/8XPlnZMy_o.jpg" alt="img"></p> 
<p>启动微调训练作业后，可能需要一些时间加载和解压缩压缩的模型工件。这可能需要长达4个小时。模型微调完成后，可以使用SageMaker JumpStart的模型页面部署它。微调完成后，将出现部署微调模型的选项，如下图所示。</p> 
<p><img src="https://images2.imgbox.com/23/28/L6JP6DRN_o.jpg" alt="img"></p> 
<h3><a id="SageMaker_JumpStart_SDK_82"></a>使用SageMaker JumpStart SDK进行微调</h3> 
<p>以下示例代码显示了如何在对话数据集上微调Meta Llama 3.1 405B基础模型。为了简化起见，展示了如何在单个ml.p5.48xlarge实例上微调和部署Meta Llama 3.1 405B模型。</p> 
<p>让以对话格式加载和处理数据集。本演示使用的示例数据集是<a href="https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386" rel="nofollow">OpenAssistant’s TOP-1 Conversation Threads</a>。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset

<span class="token comment"># 加载数据集</span>
dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"OpenAssistant/oasst_top1_2023-08-25"</span><span class="token punctuation">)</span>
</code></pre> 
<p>训练数据应以JSON行格式（.jsonl）组织，每一行都是一个表示一组对话的字典。以下代码显示了JSON行文件中的一个示例。微调期间处理数据时使用的聊天模板与Meta Llama 3.1 405B Instruct（<a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386" rel="nofollow">Hugging Face</a>）中使用的聊天模板一致。有关如何处理数据集的详细信息，请参阅[GitHub仓库](https://github.com/aws/amazon-sagemaker-examples/blob/default/ generative_ai/sm-jumpstart_foundation_llama_3_1_405b_finetuning.ipynb?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386)中的笔记本。</p> 
<pre><code class="prism language-python"><span class="token punctuation">{<!-- --></span><span class="token string">'dialog'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>
  <span class="token punctuation">{<!-- --></span><span class="token string">'content'</span><span class="token punctuation">:</span> <span class="token string">'帝国大厦的高度是多少'</span><span class="token punctuation">,</span>
   <span class="token string">'role'</span><span class="token punctuation">:</span> <span class="token string">'user'</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token punctuation">{<!-- --></span><span class="token string">'content'</span><span class="token punctuation">:</span> <span class="token string">'帝国大厦的高度为381米或1250英尺。如果包括天线，总高度为443米或1454英尺'</span><span class="token punctuation">,</span>
   <span class="token string">'role'</span><span class="token punctuation">:</span> <span class="token string">'assistant'</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token punctuation">{<!-- --></span><span class="token string">'content'</span><span class="token punctuation">:</span> <span class="token string">'一些人需要驾驶飞机飞越它，想知道答案。那高度是多少英尺？'</span><span class="token punctuation">,</span>
   <span class="token string">'role'</span><span class="token punctuation">:</span> <span class="token string">'user'</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token punctuation">{<!-- --></span><span class="token string">'content'</span><span class="token punctuation">:</span> <span class="token string">'1454英尺'</span><span class="token punctuation">,</span> <span class="token string">'role'</span><span class="token punctuation">:</span> <span class="token string">'assistant'</span><span class="token punctuation">}</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>接下来，调用SageMaker JumpStart SDK来初始化SageMaker训练作业。底层训练脚本使用Hugging Face的<a href="https://huggingface.co/docs/trl/main/en/sft_trainer?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386" rel="nofollow">SFT Trainer</a>和<a href="https://github.com/meta-llama/llama-recipes?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386">llama-recipes</a>。要自定义超参数的值，请参阅[GitHub仓库](https://github.com/aws/amazon-sagemaker-examples/blob/default/ generative_ai/sm-jumpstart_foundation_llama_3_1_405b_finetuning.ipynb?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386)。</p> 
<p>405B微调模型工件以其原始精度bf16进行微调。经过QLoRA微调后，在bf16训练模型工件上进行了<a href="https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/fbgemm_fp8.md?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386">fp8量化</a>，以使其可部署在单个ml.p5.48xlarge实例上。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> boto3
<span class="token keyword">from</span> sagemaker<span class="token punctuation">.</span>session <span class="token keyword">import</span> Session
<span class="token keyword">from</span> sagemaker <span class="token keyword">import</span> hyperparameters
<span class="token keyword">from</span> sagemaker<span class="token punctuation">.</span>jumpstart<span class="token punctuation">.</span>estimator <span class="token keyword">import</span> JumpStartEstimator

model_id <span class="token operator">=</span> <span class="token string">"meta-textgeneration-llama-3-1-405b-fp8"</span>

estimator <span class="token operator">=</span> JumpStartEstimator<span class="token punctuation">(</span>
    model_id<span class="token operator">=</span>model_id<span class="token punctuation">,</span> environment<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">"accept_eula"</span><span class="token punctuation">:</span> <span class="token string">"false"</span><span class="token punctuation">}</span> <span class="token comment"># 手动接受EULA！</span>
<span class="token punctuation">)</span>

<span class="token comment"># 有关其他超参数，请参阅附在本博客中的GitHub笔记本。</span>
estimator<span class="token punctuation">.</span>set_hyperparameters<span class="token punctuation">(</span>chat_dataset<span class="token operator">=</span><span class="token string">"True"</span><span class="token punctuation">,</span> max_input_length<span class="token operator">=</span><span class="token string">"8000"</span><span class="token punctuation">,</span> epoch<span class="token operator">=</span><span class="token string">"2"</span><span class="token punctuation">)</span>
estimator<span class="token punctuation">.</span>fit<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"training"</span><span class="token punctuation">:</span> <span class="token operator">&lt;</span>your_S3_bucket_hosting_the_train_data<span class="token operator">&gt;</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<p>微调完成后，可以将微调模型部署到SageMaker端点：</p> 
<pre><code class="prism language-code">finetuned_predictor = estimator.deploy()
</code></pre> 
<p>还可以在SageMaker JumpStart上找到微调其他变体Meta Llama 3.1模型（8B和70B基础和指令）的代码（[GitHub仓库](https://github.com/aws/amazon-sagemaker-examples/blob/default/ generative_ai/sm-jumpstart_foundation_llama_3_finetuning.ipynb?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386)），只需替换上面功能表中显示的模型ID。该代码包括数据集准备、在自定义数据集上训练以及部署微调模型。它还演示了使用<a href="https://huggingface.co/datasets/databricks/databricks-dolly-15k?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386" rel="nofollow">Dolly数据集</a>子集进行的指令微调，包含来自摘要任务的示例，以及在SEC提交文件上的领域适应微调。</p> 
<p>以下是微调和非微调模型的测试输入示例以及响应结果，还有真实的响应。该模型使用<a href="https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386" rel="nofollow">OpenAssistant’s TOP-1 Conversation Threads</a>数据集的10,000个示例微调1个周期，上下文长度为8000。其余示例设为测试集，微调过程中未见过。推理参数<code>max_new_tokens</code>、<code>top_p</code>和温度分别设置为256、0.96和0.2。</p> 
<p>为了与微调期间输入处理方式一致，输入提示在发送到预训练和微调模型生成输出之前由Meta Llama 3.1 405B Instruct（<a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct?trk=6607c336-68db-4b87-9e49-ea78f1dd8924&amp;sc_channel=sm&amp;campaign=blog1386" rel="nofollow">Hugging Face</a>）的聊天模板处理。因为模型在训练过程中已经看到了聊天模板，所以微调后的405B模型能够生成比预训练模型更高质量的响应。</p> 
<p>为模型提供以下输入：</p> 
<pre><code class="prism language-code">用户：我正在努力成为一名Python/Java开发人员。能否提供一些建议，以帮助我更好地向潜在雇主展示自己和我的技能？如果需要更多澄清，请随时询问我。
</code></pre> 
<p>以下是真实的响应：</p> 
<pre><code class="prism language-code">助手：当然，我很乐意帮助！以下是一些问题供开始思考：

作为Python/Java开发人员工作过哪些项目？

对哪种行业或公司类型感兴趣？

是否参与了任何开源项目或对与Python/Java开发相关的在线论坛作出了贡献？

是否活跃在任何社交媒体平台上，如LinkedIn或Twitter，可以在这些平台上展示工作并与行业中的其他人互动？

是否参加了任何会议、聚会或其他联网活动，在这些活动中可以与该领域的其他专业人士建立联系？

回答这些问题可以让更清楚地了解如何向潜在雇主推销自己和技能。
</code></pre> 
<p>以下是非微调模型的响应：</p> 
<pre><code class="prism language-code">抱歉，我无法提供有关如何向潜在雇主展示自己或技能的建议。我是一个大型语言模型，我的目的是帮助提供一般知识和信息。我无法浏览互联网或访问任何个人信息。还有什么我可以帮忙的吗？ 
</code></pre> 
<p>从微调模型中获得了以下响应：</p> 
<pre><code class="prism language-code">当然，以下是一些建议，帮助展示技能并脱颖而出：

1. 建立强大的在线形象：创建一个展示项目、技能和经验的专业网站或作品集。使用GitHub等平台展示代码并参与开源项目。在LinkedIn等专业社交网站上保持活跃，与行业专业人士建立联系并分享成就。

2. 发展个人品牌：定义独特价值主张，并突出与其他开发者的区别。突出优势、专业知识以及拥有的任何特殊技能。制定一个有吸引力的电梯演讲，简洁地传达技能和经验。

3. 展示工作：创建一个包含详细描述贡献的最佳项目的作品集。如果可能，提供现场演示或代码库的链接。这将为潜在雇主提供一个可感知的例子来展示能力。

4. 参与开源项目：参与开源项目展示了与他人合作并为开发社区做出贡献的能力。这也是展示技能并在行业中获得认可的机会。

5. 参加行业活动和会议：网络在技术行业中至关重要。参加会议、聚会和研讨会，与领域的专业人士建立联系。
</code></pre> 
<p>观察到微调后的模型表现更好，因为模型暴露于更多相关数据，因此在知识和格式上能够更好地适应。</p> 
<p>清理</p> 
<p>可以在使用后删除端点以节省成本。</p> 
<h3><a id="_196"></a>结论</h3> 
<p>在本文中，讨论了使用SageMaker JumpStart微调Meta Llama 3.1模型。展示了如何使用SageMaker Studio中的SageMaker JumpStart UI或SageMaker Python SDK微调和部署这些模型。还讨论了微调技术、实例类型和支持的超参数。此外，概述了基于进行的各种测试的优化训练建议。从这些结果可以看出，微调在摘要生成方面优于未微调的模型。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/aeb6483e1274e7a6d5ef31aff280e277/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">SQL Server数据库日志占满硬盘 运行不了了怎么办</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5272a62d51dc50a746f2ec5ca4520e36/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">微信小程序跳转到另一个微信小程序</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>