<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>autodl 上 使用 LLaMA-Factory 微调 中文版 llama3 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/77def6b08708c08f68f8dff2809e53df/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="autodl 上 使用 LLaMA-Factory 微调 中文版 llama3">
  <meta property="og:description" content="autodl 上 使用 LLaMA-Factory 微调 中文版 llama3 环境准备创建虚拟环境下载微调工具 LLaMA-Factory下载 llama3-8B开始微调测试微调结果模型合并后导出vllm 加速推理 环境准备 autodl 服务器：
https://www.autodl.com/console/homepage/personal
基本上充 5 块钱就可以搞完。
强烈建议选 4090（24G），不然微调的显存不够。
我们用 LoRA 微调，至少得 20G（8B模型）。
微调工具：
https://github.com/hiyouga/LLaMA-Factory.git 模型：
https://www.modelscope.cn/LLM-Research/Meta-Llama-3-8B-Instruct.git 创建虚拟环境 conda activate LLaMA-Factory 上传中文微调 dpo_zh.json 数据：
https://www.123pan.com/s/cD4cjv-kvgVh.html提取码: NpsA 下载微调工具 LLaMA-Factory git clone https://github.com/hiyouga/LLaMA-Factory.git cd LLaMA-Factory pip install -e .[metrics] # 下载全部依赖 下载 llama3-8B # pip install modelscope import torch from modelscope import snapshot_download, AutoModel, AutoTokenizer import os model_dir = snapshot_download(&#39;LLM-Research/Meta-Llama-3-8B-Instruct&#39;, cache_dir=&#39;/root/autodl-tmp&#39;, revision=&#39;master&#39;) 模型路径：/root/autodl-tmp/LLM-Research/Meta-Llama-3-8B-Instruct">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-13T08:06:56+08:00">
    <meta property="article:modified_time" content="2024-05-13T08:06:56+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">autodl 上 使用 LLaMA-Factory 微调 中文版 llama3</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-dracula">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p></p> 
 <div class="toc"> 
  <h4>autodl 上 使用 LLaMA-Factory 微调 中文版 llama3</h4> 
  <ul><li><ul><li><a href="#_5" rel="nofollow">环境准备</a></li><li><a href="#_29" rel="nofollow">创建虚拟环境</a></li><li><a href="#_LLaMAFactory_39" rel="nofollow">下载微调工具 LLaMA-Factory</a></li><li><a href="#_llama38B_47" rel="nofollow">下载 llama3-8B</a></li><li><a href="#_82" rel="nofollow">开始微调</a></li><li><a href="#_137" rel="nofollow">测试微调结果</a></li><li><a href="#_154" rel="nofollow">模型合并后导出</a></li><li><a href="#vllm__169" rel="nofollow">vllm 加速推理</a></li></ul> 
  </li></ul> 
 </div> 
 <p></p> 
</blockquote> 
<p> </p> 
<hr> 
<h3><a id="_5"></a>环境准备</h3> 
<p>autodl 服务器：</p> 
<p><a href="https://www.autodl.com/console/homepage/personal" rel="nofollow">https://www.autodl.com/console/homepage/personal</a></p> 
<p><img src="https://images2.imgbox.com/04/db/tYdCNfyR_o.png" alt=""><br> 基本上充 5 块钱就可以搞完。</p> 
<p>强烈建议选 4090（24G），不然微调的显存不够。<br> <img src="https://images2.imgbox.com/78/82/EUZACjBY_o.png" alt=""><br> 我们用 LoRA 微调，至少得 20G（8B模型）。<br>  </p> 
<p>微调工具：</p> 
<ul><li><a href="https://github.com/hiyouga/LLaMA-Factory.git">https://github.com/hiyouga/LLaMA-Factory.git</a></li></ul> 
<p>模型：</p> 
<ul><li><a rel="nofollow">https://www.modelscope.cn/LLM-Research/Meta-Llama-3-8B-Instruct.git</a></li></ul> 
<p> </p> 
<hr> 
<h3><a id="_29"></a>创建虚拟环境</h3> 
<pre><code class="prism language-cpp">conda activate LLaMA<span class="token operator">-</span>Factory
</code></pre> 
<p>上传中文微调 <code>dpo_zh.json</code> 数据：</p> 
<ul><li><a href="https://www.123pan.com/s/cD4cjv-kvgVh.html" rel="nofollow">https://www.123pan.com/s/cD4cjv-kvgVh.html</a></li><li>提取码: NpsA</li></ul> 
<p><img src="https://images2.imgbox.com/44/d5/LLuMhXNP_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_LLaMAFactory_39"></a>下载微调工具 LLaMA-Factory</h3> 
<pre><code class="prism language-cpp">git clone https<span class="token operator">:</span><span class="token comment">//github.com/hiyouga/LLaMA-Factory.git</span>

cd LLaMA<span class="token operator">-</span>Factory 

pip install <span class="token operator">-</span>e <span class="token punctuation">.</span><span class="token punctuation">[</span>metrics<span class="token punctuation">]</span>  # 下载全部依赖
</code></pre> 
<h3><a id="_llama38B_47"></a>下载 llama3-8B</h3> 
<pre><code class="prism language-cpp"><span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">pip</span> <span class="token expression">install modelscope</span></span>
<span class="token keyword">import</span> <span class="token module">torch</span>
from modelscope <span class="token keyword">import</span> <span class="token module">snapshot_download</span><span class="token punctuation">,</span> AutoModel<span class="token punctuation">,</span> AutoTokenizer
<span class="token keyword">import</span> <span class="token module">os</span>

model_dir <span class="token operator">=</span> <span class="token function">snapshot_download</span><span class="token punctuation">(</span>'LLM<span class="token operator">-</span>Research<span class="token operator">/</span>Meta<span class="token operator">-</span>Llama<span class="token operator">-</span><span class="token number">3</span><span class="token operator">-</span><span class="token number">8</span>B<span class="token operator">-</span>Instruct<span class="token char">', cache_dir='</span><span class="token operator">/</span>root<span class="token operator">/</span>autodl<span class="token operator">-</span>tmp<span class="token char">', revision='</span>master<span class="token number">'</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/20/a5/uN2WBS9k_o.png" alt="在这里插入图片描述"><br> 模型路径：/root/autodl-tmp/LLM-Research/Meta-Llama-3-8B-Instruct</p> 
<p> </p> 
<p>在 <code>LLaMA-Factory/data</code> 文件夹下找到 <code>dataset_info.json </code>。</p> 
<p>方法一：设置镜像站<br> <img src="https://images2.imgbox.com/16/92/FyNmMxt5_o.png" alt=""><br> 这个数据，ta会去hf官方找，我们可以设置镜像站。</p> 
<pre><code class="prism language-cpp">pip install <span class="token operator">-</span>U huggingface_hub  # 安装依赖
<span class="token keyword">export</span> HF_ENDPOINT<span class="token operator">=</span>https<span class="token operator">:</span><span class="token comment">//hf-mirror.com/  # 镜像站</span>
</code></pre> 
<p>方法二：改成本地文件路径</p> 
<p><img src="https://images2.imgbox.com/e6/25/y5edY4Hu_o.png" alt=""><br> 俩个地方都要改：file_name、本地数据集路径。</p> 
<p>更新 <code>transformers</code> 库：</p> 
<pre><code class="prism language-cpp">pip install <span class="token operator">--</span>upgrade transformers
</code></pre> 
<h3><a id="_82"></a>开始微调</h3> 
<pre><code class="prism language-cpp">CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">0</span> llamafactory<span class="token operator">-</span>cli train \
    <span class="token operator">--</span>stage orpo \
    <span class="token operator">--</span>do_train True \
    <span class="token operator">--</span>model_name_or_path <span class="token operator">/</span>root<span class="token operator">/</span>autodl<span class="token operator">-</span>tmp<span class="token operator">/</span>LLM<span class="token operator">-</span>Research<span class="token operator">/</span>Meta<span class="token operator">-</span>Llama<span class="token operator">-</span><span class="token number">3</span><span class="token operator">-</span><span class="token number">8</span>B<span class="token operator">-</span>Instruct \
    <span class="token operator">--</span>finetuning_type lora \
    <span class="token operator">--</span><span class="token keyword">template</span> <span class="token keyword">default</span> \
    <span class="token operator">--</span>flash_attn <span class="token keyword">auto</span> \
    <span class="token operator">--</span>dataset_dir LLaMA<span class="token operator">-</span>Factory<span class="token operator">/</span>data\
    <span class="token operator">--</span>dataset dpo_mix_zh \
    <span class="token operator">--</span>cutoff_len <span class="token number">1024</span> \
    <span class="token operator">--</span>learning_rate <span class="token number">1e-05</span> \
    <span class="token operator">--</span>num_train_epochs <span class="token number">5.0</span> \
    <span class="token operator">--</span>max_samples <span class="token number">1</span> \
    <span class="token operator">--</span>per_device_train_batch_size <span class="token number">1</span> \
    <span class="token operator">--</span>gradient_accumulation_steps <span class="token number">8</span> \
    <span class="token operator">--</span>lr_scheduler_type cosine \
    <span class="token operator">--</span>max_grad_norm <span class="token number">1.0</span> \
    <span class="token operator">--</span>logging_steps <span class="token number">5</span> \
    <span class="token operator">--</span>save_steps <span class="token number">100</span> \
    <span class="token operator">--</span>warmup_steps <span class="token number">0</span> \
    <span class="token operator">--</span>optim adamw_torch \
    <span class="token operator">--</span>report_to none \
    <span class="token operator">--</span>output_dir saves<span class="token operator">/</span>LLaMA3<span class="token operator">-</span><span class="token number">8</span>B<span class="token operator">/</span>lora<span class="token operator">/</span>train_2024<span class="token operator">-</span><span class="token number">04</span><span class="token operator">-</span><span class="token number">25</span><span class="token operator">-</span><span class="token number">07</span><span class="token operator">-</span><span class="token number">48</span><span class="token operator">-</span><span class="token number">56</span> \
    <span class="token operator">--</span>fp16 True \
    <span class="token operator">--</span>lora_rank <span class="token number">8</span> \
    <span class="token operator">--</span>lora_alpha <span class="token number">16</span> \
    <span class="token operator">--</span>lora_dropout <span class="token number">0</span> \
    <span class="token operator">--</span>lora_target q_proj<span class="token punctuation">,</span>v_proj \
    <span class="token operator">--</span>orpo_beta <span class="token number">0.1</span> \
    <span class="token operator">--</span>plot_loss True 
</code></pre> 
<p><img src="https://images2.imgbox.com/99/be/MEcjGtmc_o.png" alt=""><br> 微调后，就找这个路径看一下。</p> 
<p>微调上面的参数是自定义的：</p> 
<ul><li>max_samples 1 只使用一个数据微调，一般越多越好，这步为演示，就1条了</li></ul> 
<pre><code class="prism language-cpp">stage	        当前训练的阶段，枚举值，有“sft”<span class="token punctuation">,</span><span class="token string">"pt"</span><span class="token punctuation">,</span><span class="token string">"rw"</span><span class="token punctuation">,</span><span class="token string">"ppo"</span>等，代表了训练的不同阶段，这里我们是有监督指令微调，所以是sft
do_train	    是否是训练模式
dataset	        使用的数据集列表，所有字段都需要按上文在data_info<span class="token punctuation">.</span>json里注册，多个数据集用<span class="token string">","</span>分隔
dataset_dir	    数据集所在目录，这里是 data，也就是项目自带的data目录
finetuning_type	微调训练的类型，枚举值，有<span class="token string">"lora"</span><span class="token punctuation">,</span><span class="token string">"full"</span><span class="token punctuation">,</span><span class="token string">"freeze"</span>等，这里使用lora
lora_target	    如果finetuning_type是lora，那训练的参数目标的定义，这个不同模型不同，请到https<span class="token operator">:</span><span class="token comment">//github.com/hiyouga/LLaMA-Factory/tree/main?tab=readme-ov-file#supported-models 获取 不同模型的 可支持module， 比如llama3 默认是 q_proj,v_proj</span>
output_dir	    训练结果保存的位置
cutoff_len	    训练数据集的长度截断
per_device_train_batch_size	    每个设备上的batch size，最小是<span class="token number">1</span>，如果GPU 显存够大，可以适当增加
fp16	        使用半精度混合精度训练
max_samples	    每个数据集采样多少数据
val_size	    随机从数据集中抽取多少比例的数据作为验证集
</code></pre> 
<h3><a id="_137"></a>测试微调结果</h3> 
<p>微调后，还可以马上测试微调结果。</p> 
<ul><li>调不好！重新调整参数和数据。</li><li>调得好！马上合并权重。</li></ul> 
<p>训练完后就可以在设置的output_dir下看到：</p> 
<p><img src="https://images2.imgbox.com/a4/6f/s9GqhZ4m_o.png" alt=""></p> 
<ul><li> <p>adapter开头的就是 LoRA 保存的结果了，后续用于模型推理融合</p> </li><li> <p>training_loss 和trainer_log等记录了训练的过程指标</p> </li><li> <p>其他是训练当时各种参数的备份</p> </li></ul> 
<h3><a id="_154"></a>模型合并后导出</h3> 
<p>如果想把训练的LoRA和原始的大模型进行融合，输出一个完整的模型文件的话：</p> 
<pre><code class="prism language-cpp">CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">0</span> llamafactory<span class="token operator">-</span>cli <span class="token keyword">export</span> \
    <span class="token operator">--</span>model_name_or_path <span class="token operator">/</span>root<span class="token operator">/</span>autodl<span class="token operator">-</span>tmp<span class="token operator">/</span>LLM<span class="token operator">-</span>Research<span class="token operator">/</span>Meta<span class="token operator">-</span>Llama<span class="token operator">-</span><span class="token number">3</span><span class="token operator">-</span><span class="token number">8</span>B<span class="token operator">-</span>Instruct \
    <span class="token operator">--</span>adapter_name_or_path <span class="token punctuation">.</span><span class="token operator">/</span>saves<span class="token operator">/</span>LLaMA3<span class="token operator">-</span><span class="token number">8</span>B<span class="token operator">/</span>lora<span class="token operator">/</span>sft  \
    <span class="token operator">--</span><span class="token keyword">template</span> llama3 \
    <span class="token operator">--</span>finetuning_type lora \
    <span class="token operator">--</span>export_dir megred<span class="token operator">-</span>model<span class="token operator">-</span>path \
    <span class="token operator">--</span>export_size <span class="token number">2</span> \
    <span class="token operator">--</span>export_device cpu \
    <span class="token operator">--</span>export_legacy_format False
</code></pre> 
<h3><a id="vllm__169"></a>vllm 加速推理</h3> 
<p>需要提前将LoRA 模型进行merge，使用merge后的完整版模型目录：</p> 
<pre><code class="prism language-cpp">CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">0</span> API_PORT<span class="token operator">=</span><span class="token number">8000</span> llamafactory<span class="token operator">-</span>cli api \
    <span class="token operator">--</span>model_name_or_path megred<span class="token operator">-</span>model<span class="token operator">-</span>path \
    <span class="token operator">--</span><span class="token keyword">template</span> llama3 \
    <span class="token operator">--</span>infer_backend vllm \
    <span class="token operator">--</span>vllm_enforce_eager
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8a47f436c893fa5492752ffb6eba9b8d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">最全IDEA 2024 配置 Maven 创建 Spring Boot 项目，那些年Java面试官常问的知识点</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/eed017b135b86fbf068978b5c3888388/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Linux安装mysql报错：失败的软件包是：mysql-community-libs-8.0.37-1.el7.x86_64 GPG</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>