<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Parallelize your massive SHAP computations with MLlib and PySpark - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/2be6bb66c8b0ef4e8038e4c7ec5c2296/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Parallelize your massive SHAP computations with MLlib and PySpark">
  <meta property="og:description" content="https://medium.com/towards-data-science/parallelize-your-massive-shap-computations-with-mllib-and-pyspark-b00accc8667c (能翻墙直接看原文） A stepwise guide for efficiently explaining your models using SHAP. Photo by Pietro Jeng on Unsplash
Introduction to MLlib Apache Spark’s Machine Learning Library (MLlib) is designed primarily for scalability and speed by leveraging the Spark runtime for common distributed use cases in supervised learning like classification and regression, unsupervised learning like clustering and collaborative filtering and in other cases like dimensionality reduction. In this article, I cover how we can use SHAP to explain a Gradient Boosted Trees (GBT) model that has fit our data at scale.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-16T13:48:29+08:00">
    <meta property="article:modified_time" content="2024-06-16T13:48:29+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Parallelize your massive SHAP computations with MLlib and PySpark</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 id="d825"><a href="https://medium.com/towards-data-science/parallelize-your-massive-shap-computations-with-mllib-and-pyspark-b00accc8667c" rel="nofollow" title="https://medium.com/towards-data-science/parallelize-your-massive-shap-computations-with-mllib-and-pyspark-b00accc8667c">https://medium.com/towards-data-science/parallelize-your-massive-shap-computations-with-mllib-and-pyspark-b00accc8667c</a></h2> 
<h3>(能翻墙直接看原文）</h3> 
<h3 id="7149">A stepwise guide for efficiently explaining your models using SHAP.</h3> 
<p></p> 
<p class="img-center"><img alt="" height="583" src="https://images2.imgbox.com/7a/71/WYimxx3M_o.jpg" width="875"></p> 
<p>Photo by <a href="https://unsplash.com/@pietrozj?utm_source=medium&amp;utm_medium=referral" rel="nofollow" title="Pietro Jeng">Pietro Jeng</a> on <a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="nofollow" title="Unsplash">Unsplash</a></p> 
<h3 id="d4ae"><strong>Introduction to MLlib</strong></h3> 
<p id="d728">Apache Spark’s Machine Learning Library (MLlib) is designed primarily for scalability and speed by leveraging the Spark runtime for common <strong>distributed </strong>use cases in supervised learning like classification and regression, unsupervised learning like clustering and collaborative filtering and in other cases like dimensionality reduction. In this article, I cover how we can use SHAP to explain a Gradient Boosted Trees (GBT) model that has fit our data at scale.</p> 
<h3 id="355c">What are Gradient Boosted Trees?</h3> 
<p id="6859">Before we understand what Gradient Boosted Trees are, we need to understand boosting. Boosting is an ensemble technique that <strong>sequentially </strong>combines a number of weak learners to achieve an overall strong learner. In case of Gradient Boosted Trees, each weak learner is a decision tree that sequentially minimizes the errors (MSE in case of regression and log loss in case of classification) generated by the previous decision tree in that sequence. To read about GBTs in more detail, please refer to <a href="https://towardsdatascience.com/gradient-boosted-decision-trees-explained-9259bd8205af" rel="nofollow" title="this ">this </a>blog post.</p> 
<h3 id="faa6">Understanding our imports</h3> 
<pre>from pyspark.sql import SparkSession
from pyspark import SparkContext, SparkConf
from pyspark.ml.classification import GBTClassificationModel
import shap
import pyspark.sql.functions as F
from pyspark.sql.types import *</pre> 
<p id="583b">The first two imports are for initializing a Spark session. It will be used for converting our pandas dataframe to a spark one. The third import is used to load our GBT model into memory which will be passed to our SHAP explainer to generate explanations. The SHAP explainer itself will be initialized using the SHAP package using the fourth import. The penultimate and last import is for performing SQL functions and using SQL types. These will be used in our User-Defined Function (UDF) which I shall describe later.</p> 
<h3 id="22cd">Converting our MLlib GBT feature vector to a Pandas dataframe</h3> 
<p id="ffd5">The SHAP Explainer takes a dataframe as input. However, training an MLlib GBT model requires data preprocessing. More specifically, the categorical variables in our data needs to be converted into numeric variables using either <strong>Category Indexing</strong> or <strong>One-Hot Encoding. </strong>To learn more about how to train a GBT model, refer to <a href="https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa" rel="nofollow" title="this ">this </a>article). The resulting “features” column<strong> </strong>is a<strong> </strong><a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.linalg.SparseVector.html#pyspark.ml.linalg.SparseVector" rel="nofollow" title="SparseVector">SparseVector</a> (to read more on it, check the “Preprocess Data” section in <a href="https://docs.databricks.com/applications/machine-learning/train-model/mllib/index.html#advanced-apache-spark-mllib-example" rel="nofollow" title="this">this</a> example). It looks like something below:</p> 
<p></p> 
<p class="img-center"><img alt="" height="249" src="https://images2.imgbox.com/dd/69/HS06sET7_o.png" width="750"></p> 
<p>SparseVector features column description — 1. default index value, 2. vector length, 3. list of indexes of the feature columns, 4. list of data values at the corresponding index at 3. [Image by author]</p> 
<p id="1cd4">The “features” column shown above is for a single training instance. We need to transform this SparseVector for all our training instances. One way to do it is to iteratively process each row and append to our pandas dataframe that we will feed to our SHAP explainer (ouch!). There is a much faster way, which leverages the fact that we have all of our data loaded in memory (if not, we can load it in batches and perform the preprocessing for each in-memory batch). In <a href="https://stackoverflow.com/users/2441506/shikhardua" rel="nofollow" title="Shikhar Dua">Shikhar Dua</a>’s words:</p> 
<blockquote> 
 <p id="17a4">1. Create a list of dictionaries in which each dictionary corresponds to an input data row.</p> 
 <p id="111a">2. Create a data frame from this list.</p> 
</blockquote> 
<p id="9995">So, based on the above method, we get something like this:</p> 
<pre>rows_list = []
for row in spark_df.rdd.collect(): 
    dict1 = {} 
    dict1.update({k:v for k,v in zip(spark_df.cols,row.features)})
    rows_list.append(dict1) 
pandas_df = pd.DataFrame(rows_list)</pre> 
<p id="036f">If <em>rdd.collect() </em>looks scary, it’s actually pretty simple to explain. <strong>Resilient Distributed Datasets (RDD) </strong>are fundamental Spark data structures that are<strong> </strong>an <strong>immutable </strong>distribution of objects. Each dataset in an RDD is further subdivided into logical partitions that can be computed in different worker nodes of our Spark cluster. So, all PySpark RDD <em>collect() </em>does is retrieve data from all the worker nodes to the driver node. As you might guess, this is a memory bottleneck, and if we are handling data larger than our driver node’s memory capacity, we need to increase the number of our RDD partitions and filter them by partition index. Read how to do that <a href="https://umbertogriffo.gitbook.io/apache-spark-best-practices-and-tuning/rdd/dont_collect_large_rdds" rel="nofollow" title="here">here</a>.</p> 
<p id="aafb">Don’t take my word on the execution performance. Check out the stats.</p> 
<p></p> 
<p class="img-center"><img alt="" height="236" src="https://images2.imgbox.com/8b/4c/SIxNZ5C3_o.png" width="694"></p> 
<p>Performance profiling for inserting rows to a pandas dataframe. [Source (Thanks to Mikhail_Sam and Peter Mortensen): <a href="https://stackoverflow.com/questions/10715965/create-a-pandas-dataframe-by-appending-one-row-at-a-time/17496530#17496530" rel="nofollow" title="here">here</a>]</p> 
<p id="2b0c">Here are the metrics from one of my Databricks notebook scheduled job runs:</p> 
<blockquote> 
 <p id="c08f">Input size: 11.9 GiB (~12.78GB), Total time Across All Tasks: 20 min, Number of records: 165.16K</p> 
</blockquote> 
<p></p> 
<p class="img-center"><img alt="" height="150" src="https://images2.imgbox.com/49/7a/SxgOLMh3_o.png" width="875"></p> 
<p>Summary Metrics for 125 Completed Tasks executed by the stage that run the above cell. [Image by author]</p> 
<h3 id="ea98">Working with the SHAP Library</h3> 
<p id="233a">We are now ready to pass our preprocessed dataset to the SHAP TreeExplainer. Remember that SHAP is a local feature attribution method that explains <strong>individual </strong>predictions as an algebraic sum of the shapley values of the features of our model.</p> 
<p id="9576">We use a TreeExplainer for the following reasons:</p> 
<ol><li id="c3e6"><strong>Suitable:</strong> TreeExplainer is a class that computes SHAP values for tree-based models (Random Forest, XGBoost, LightGBM, GBT, etc).</li><li id="c84c"><strong>Exact</strong>: Instead of simulating missing features by random sampling, it makes use of the tree structure by simply ignoring decision paths that rely on the missing features. The TreeExplainer output is therefore deterministic and does not vary based on the background dataset.</li><li id="9bdd"><strong>Efficient</strong>: Instead of iterating over each possible feature combination (or a subset thereof), all combinations are pushed through the tree simultaneously, using a more complex algorithm to keep track of each combination’s result — reducing complexity from O(<strong>TL2ᵐ</strong>) for all possible coalitions to the polynomial O(<strong>TLD²</strong>) (where <strong>m </strong>is the number of features, <strong>T </strong>is number of trees, <strong>L </strong>is maximum number of leaves and <strong>D </strong>is maximum tree depth).</li></ol> 
<p id="030b">The <em>check_additivity = False </em>flag runs a validation check to verify if the sum of SHAP values equals to the output of the model. However, this flag requires predictions to be run that are not supported by Spark, so it needs to be set to False as it is ignored anyway. Once we get the SHAP values, we convert it into a pandas dataframe from a Numpy array, so that it is easily interpretable.</p> 
<blockquote> 
 <p id="e3ec">One thing to note is that the dataset order is preserved when we convert a Spark dataframe to pandas, but the <strong>reverse is not true</strong>.</p> 
</blockquote> 
<p id="6cc2">The points above lead us to the code snippet below:</p> 
<pre>gbt = GBTClassificationModel.load('your-model-path') 
explainer = shap.TreeExplainer(gbt)
shap_values = explainer(pandas_df, check_additivity = False)
shap_pandas_df = pd.DataFrame(shap_values.values, cols = pandas_df.columns)</pre> 
<h3 id="995d">An Introduction to Pyspark UDFs and when to use them</h3> 
<p></p> 
<p class="img-center"><img alt="" height="445" src="https://images2.imgbox.com/1a/2f/Du7fJ42f_o.png" width="619"></p> 
<p>How PySpark UDFs distribute individual tasks to worker (executor) nodes [Source: <a href="https://medium.com/quantumblack/spark-udf-deep-insights-in-performance-f0a95a4d8c62#:~:text=In%20these%20circumstances%2C%20PySpark%20UDF,two%20types%20of%20PySpark%20UDFs." rel="nofollow" title="here">here</a>]</p> 
<p id="5779">User-Defined Functions are complex custom functions that operate on a <strong>particular row</strong> of our dataset. These functions are generally used when the native Spark functions are not deemed sufficient to solve the problem. Spark functions are inherently <strong>faster than UDFs</strong> because it is natively a JVM structure whose methods are implemented by local calls to Java APIs. However, PySpark UDFs are Python implementations that requires data movement between the Python interpreter and the JVM (refer to Arrow 4 in the picture above). This inevitably introduces some processing delay.</p> 
<p id="dd1e">If no processing delays can be tolerated, the best thing to do is create a Python wrapper to call the Scala UDF from PySpark itself. A great example is shown in <a href="https://medium.com/quantumblack/spark-udf-deep-insights-in-performance-f0a95a4d8c62#:~:text=In%20these%20circumstances%2C%20PySpark%20UDF,two%20types%20of%20PySpark%20UDFs." rel="nofollow" title="this">this</a> blog. However, using a PySpark UDF was sufficient for my use case, since it is easy to understand and code.</p> 
<p id="9685">The code below explains the Python function to be executed on each worker/executor node. We just pick up the highest SHAP values (absolute values as we want to find the most impactful negative features as well) and append it to the respective <em>pos_features </em>and <em>neg_features </em>list and in turn append both these lists to a <em>features </em>list that is returned to the caller.</p> 
<pre>def shap_udf(row):
    dict = {} 
    pos_features = [] 
    neg_features = [] 
    for feature in row.columns: 
        dict[feature] = row[feature]     dict_importance = {key: value for key, value in
    sorted(dict.items(), key=lambda item: __builtin__.abs(item[1]),   
    reverse = True)}     for k,v in dict_importance.items(): 
        if __builtin__.abs(v) &gt;= &lt;your-threshold-shap-value&gt;: 
             if v &gt; 0: 
                 pos_features.append((k,v)) 
             else: 
                 neg_features.append((k,v)) 
   features = [] 
   features.append(pos_features[:5]) 
   features.append(neg_features[:5])    return features</pre> 
<p id="f3f4">We then register our PySpark UDF with our Python function name (in my case, it is <em>shap_udf</em>) and specify the return type (mandatory in Python and Java) of the function in the parameters to <em>F.udf()</em>. There are two lists in the outer ArrayType(), one for positive features and the other for negative ones. Since each individual list comprises of at most 5 (feature-name, shap-value) StructType() pairs, it represents the inner ArrayType(). Below is the code:</p> 
<pre>udf_obj = F.udf(shap_udf, ArrayType(ArrayType(StructType([ StructField(‘Feature’, StringType()), 
StructField(‘Shap_Value’, FloatType()),
]))))</pre> 
<p id="ab63">Now, we just create a new Spark dataframe with a column called ‘Shap_Importance’ that invokes our UDF for each row of the <em>spark_shapdf </em>dataframe. To split the positive and negative features, we create two columns in a new Spark dataframe called <em>final_sparkdf</em>. Our final code-snippet looks like below:</p> 
<pre>new_sparkdf = spark_df.withColumn(‘Shap_Importance’, udf_obj(F.struct([spark_shapdf[x] for x in spark_shapdf.columns])))final_sparkdf = new_sparkdf.withColumn(‘Positive_Shap’, final_sparkdf.Shap_Importance[0]).withColumn(‘Negative_Shap’, new_sparkdf.Shap_Importance[1])</pre> 
<p id="8718">And finally, we have extracted all the important features of our GBT model per testing instance <strong>without </strong>the use of any explicit for loops! The consolidated code can be found in the below GitHub gist.</p> 
<pre><code class="language-python">from pyspark.sql import SparkSession
from pyspark import SparkContext, SparkConf
from pyspark.ml.classification import GBTClassificationModel
import shap
import pyspark.sql.functions as  F
from pyspark.sql.types import *

#convert the sparse feature vector that is passed to the MLlib GBT model into a pandas dataframe. 
#This 'pandas_df' will be passed to the Shap TreeExplainer.
rows_list = []
for row in spark_df.rdd.collect(): 
  dict1 = {}
  dict1.update({k:v for k,v in zip(spark_df.cols,row.features)})
  rows_list.append(dict1)
 
pandas_df = pd.DataFrame(rows_list)

#Load the GBT model from the path you have saved it
gbt = GBTClassificationModel.load("&lt;your path where the GBT model is loaded&gt;") 
#make sure the application where your notebook runs has access to the storage path!

explainer = shap.TreeExplainer(gbt)
#check_additivity requires predictions to be run that is not supported by spark [yet], so it needs to be set to False as it is ignored anyway.
shap_values = explainer(pandas_df, check_additivity = False)
shap_pandas_df = pd.DataFrame(shap_values.values, cols = pandas_df.columns)

spark = SparkSession.builder.config(conf=SparkConf().set("spark.master", "local[*]")).getOrCreate()
spark_shapdf = spark.createDataFrame(shap_pandas_df)


def shap_udf(row): #work on a single spark dataframe row, for all rows. This work is distributed among all the worker nodes of your Apache Spark cluster.
  dict = {}
  pos_features = []
  neg_features = []

  for feature in row.columns:
    dict[feature] = row[feature]

  dict_importance = {key: value for key, value in sorted(dict.items(), key=lambda item: __builtin__.abs(item[1]), reverse = True)}

  for k,v in dict_importance.items():
    if __builtin__.abs(v) &gt;= &lt;your-threshold-shap-value&gt;:
      if v &gt; 0:
        pos_features.append((k,v))
      else:
        neg_features.append((k,v))
  features = []
  #taking top 5 features from pos and neg features. We can increase this number.
  features.append(pos_features[:5])
  features.append(neg_features[:5])

  return features


udf_obj = F.udf(shap_udf, ArrayType(ArrayType(StructType([
  StructField('Feature', StringType()),
  StructField('Shap_Value', FloatType()),
]))))

new_sparkdf = spark_df.withColumn('Shap_Importance', udf_obj(F.struct([spark_shapdf[x] for x in spark_shapdf.columns])))
final_sparkdf = new_sparkdf.withColumn('Positive_Shap', final_sparkdf.Shap_Importance[0]).withColumn('Negative_Shap', new_sparkdf.Shap_Importance[1])</code></pre> 
<p>Get the most impactful Positive and Negative SHAP values from our fitted GBT Model</p> 
<p id="02e6">P.S. This is my first attempt at writing an article and if there are any factual or statistical inconsistencies, please reach out to me and I shall be more than happy to learn together with you! :)</p> 
<h3 id="d05d">References</h3> 
<p id="4d0a">[1] Soner Yıldırım, <a href="https://towardsdatascience.com/gradient-boosted-decision-trees-explained-9259bd8205af" rel="nofollow" title="Gradient Boosted Decision Trees-Explained">Gradient Boosted Decision Trees-Explained</a> (2020), Towards Data Science</p> 
<p id="c3d0">[2] Susan Li, <a href="https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa" rel="nofollow" title="Machine Learning with PySpark and MLlib — Solving a Binary Classification Problem">Machine Learning with PySpark and MLlib — Solving a Binary Classification Problem</a> (2018), Towards Data Science</p> 
<p id="92aa">[3] Stephen Offer, <a href="https://databricks.com/blog/2020/11/16/how-to-train-xgboost-with-spark.html" rel="nofollow" title="How to Train XGBoost With Spark">How to Train XGBoost With Spark</a> (2020), Data Science and ML</p> 
<p id="1497">[4] <a href="https://docs.databricks.com/applications/machine-learning/train-model/mllib/index.html#advanced-apache-spark-mllib-example" rel="nofollow" title="Use Apache Spark MLlib on Databricks">Use Apache Spark MLlib on Databricks</a> (2021), Databricks</p> 
<p id="c2f1">[5] Umberto Griffo, <a href="https://umbertogriffo.gitbook.io/apache-spark-best-practices-and-tuning/rdd/dont_collect_large_rdds" rel="nofollow" title="Don’t collect large RDDs">Don’t collect large RDDs</a> (2020), Apache Spark — Best Practices and Tuning</p> 
<p id="9fd2">[6] Nikhilesh Nukala, Yuhao Zhu, Guilherme Braccialli, Tom Goldenberg (2019), Spark UDF —<a href="https://medium.com/quantumblack/spark-udf-deep-insights-in-performance-f0a95a4d8c62#:~:text=In%20these%20circumstances%2C%20PySpark%20UDF,two%20types%20of%20PySpark%20UDFs." rel="nofollow" title=" Deep Insights in Performance"> Deep Insights in Performance</a>, QuantumBlack</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2ef42d1e46de7521cfa53fe496fbc8fb/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【JavaEE精炼宝库】多线程（6）线程池</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5e4ecba862c024fde7f018d4d594d48a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python在SQLite中的应用：从入门到进阶</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>