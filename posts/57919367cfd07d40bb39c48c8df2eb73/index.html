<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习之——K近邻（KNN）算法 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/57919367cfd07d40bb39c48c8df2eb73/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="机器学习之——K近邻（KNN）算法">
  <meta property="og:description" content="一、k-近邻算法概述 1、算法介绍 k-近邻算法（K-Nearest Neighbors，简称KNN）是一种用于分类和回归的统计方法。KNN 可以说是最简单的分类算法之一，同时，它也是最常用的分类算法之一。
2、算法原理 k-近邻算法基于某种距离度量来找到输入样本在训练集中的k个最近邻居，并且根据这k个邻居的属性来预测输入样本的属性。
比如我们的输入样本是图中的蓝色，那么k个近邻就是距离绿色小圆最近的k个邻居，然后在这k个邻居中，若黑色小圆的数量多于红色小圆，那么输入样本的属性就与蓝黑色小圆相同，反之则与红色小圆的属性相同，这就是k-近邻算法的算法思想。
3、KNN算法中常用的距离指标 在knn算法中怎样计算输入点与其他向量点之间的距离呢？这里就用到了两种距离公式。
欧几里得距离
欧几里得距离是我们在平面几何中最常用的距离计算方法，即两点之间的直线距离。
曼哈顿距离
曼哈顿距离是计算两点在一个网格上的路径距离，与上述的直线距离不同，它只允许沿着网格的水平和垂直方向移动。
4、算法优缺点 优点：
准确度较高：K近邻算法准确度较高为它可以适应不同的数据分布。适用性广泛：K近邻算法可用于分类和回归问题，同时也支持多分类和多回归问题。实现简单：K近邻算法的实现非常简单，特别适用于初学者学习模式识别的入门算法 缺点：
计算复杂度高：当数据集很大时，计算距离的时间和空间开销都会很大，影响算法执行效率。受样本分布影响大：K近邻算法对训练集中样本的密度很敏感，对于密度相差很大的数据集，分类精度会受到较大影响。数据不平衡问题：当训练集中某些类别的样本数目远远大于其他类别的样本数目时，K近邻算法的准确度会明显下降。 5、算法流程 1、准备数据集： 收集数据集，包括特征与对应的类别标签
对数据进行预处理，例如数据清洗、归一化等。
2、选择k值： 选择一个合适的k值，即确定最近邻居的个数。
3、选择距离度量方法 确定用于比较样本之间相似性的度量方法，常见的如欧几里得距离、曼哈顿距离等。
4、确定最近邻居 选择与待分类样本距离最近的k个训练样本
5、预测 对于分类任务：查看K个最近邻居中最常见的类别，作为预测结果。
对于回归任务：预测结果可以是K个最近邻居的平均值或加权平均值。
6、评估 使用适当的评价的评级骄傲指标评估模型的性能。
7、优化 基于性能评估结果，可能需要返回并调整某些参数，如K值、距离度量方法等，以获得更好的性能。
二、knn分类算法实例——性别预测 1、想法：设计一个KNN分类来用人的身高和体重来预测人的性别。 2、创建数据集：创立一个包含身高，体重与性别的数据集用来训练模型。 3、读取数据集 # 准备数据：从文本文件中解析数据 import numpy as np def file2matrix(filename): #打开文件 fr = open(filename) arrayOLines = fr.readlines() #得到文件行数 numberOfLines = len(arrayOLines) #返回的NumPy矩阵 returnMat = np.zeros((numberOfLines,2)) #返回的分类标签向量 classLabelVector = [] #行的索引值 index = 0 for line in arrayOLines: line = line.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-10-09T21:55:53+08:00">
    <meta property="article:modified_time" content="2023-10-09T21:55:53+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习之——K近邻（KNN）算法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 style="background-color:transparent;">一、k-近邻算法概述</h2> 
<h3 style="background-color:transparent;">        1、算法介绍</h3> 
<blockquote> 
 <p>                k-近邻算法（K-Nearest Neighbors，简称KNN）是一种用于分类和回归的统计方法。KNN 可以说是最简单的分类算法之一，同时，它也是最常用的分类算法之一。</p> 
</blockquote> 
<h3 style="background-color:transparent;">        2、算法原理</h3> 
<p>                k-近邻算法基于某种距离度量来找到输入样本在训练集中的k个最近邻居，并且根据这k个邻居的属性来预测输入样本的属性。</p> 
<p>                <img alt="" height="348" src="https://images2.imgbox.com/31/e9/Phvu4i9s_o.png" width="594"></p> 
<p>                比如我们的输入样本是图中的蓝色，那么k个近邻就是距离绿色小圆最近的k个邻居，然后在这k个邻居中，若黑色小圆的数量多于红色小圆，那么输入样本的属性就与蓝黑色小圆相同，反之则与红色小圆的属性相同，这就是k-近邻算法的算法思想。</p> 
<h3>        3、KNN算法中常用的距离指标</h3> 
<p>   在knn算法中怎样计算输入点与其他向量点之间的距离呢？这里就用到了两种距离公式。</p> 
<p>        欧几里得距离</p> 
<p>                欧几里得距离是我们在平面几何中最常用的距离计算方法，即两点之间的直线距离。<br>  <img alt="" height="132" src="https://images2.imgbox.com/29/9e/j4Nu0LJV_o.png" width="799"></p> 
<p>           曼哈顿距离</p> 
<p>                曼哈顿距离是计算两点在一个网格上的路径距离，与上述的直线距离不同，它只允许沿着网格的水平和垂直方向移动。</p> 
<p>                <img alt="" height="68" src="https://images2.imgbox.com/fa/77/SUglOUbT_o.png" width="325"></p> 
<h3>        4、算法优缺点  </h3> 
<p>优点：</p> 
<ul><li>准确度较高：K近邻算法准确度较高为它可以适应不同的数据分布。</li><li>适用性广泛：K近邻算法可用于分类和回归问题，同时也支持多分类和多回归问题。</li><li>实现简单：K近邻算法的实现非常简单，特别适用于初学者学习模式识别的入门算法</li><li></ul> 
<p>缺点：</p> 
<ul><li>计算复杂度高：当数据集很大时，计算距离的时间和空间开销都会很大，影响算法执行效率。</li><li>受样本分布影响大：K近邻算法对训练集中样本的密度很敏感，对于密度相差很大的数据集，分类精度会受到较大影响。</li><li>数据不平衡问题：当训练集中某些类别的样本数目远远大于其他类别的样本数目时，K近邻算法的准确度会明显下降。</li></ul> 
<h3>        5、算法流程</h3> 
<h4>                1、准备数据集：</h4> 
<p>                        收集数据集，包括特征与对应的类别标签</p> 
<p>                        对数据进行预处理，例如数据清洗、归一化等。</p> 
<h4>                2、选择k值：</h4> 
<p>                        选择一个合适的k值，即确定最近邻居的个数。</p> 
<h4>                3、选择距离度量方法</h4> 
<p>                        确定用于比较样本之间相似性的度量方法，常见的如欧几里得距离、曼哈顿距离等。</p> 
<h4>                4、确定最近邻居</h4> 
<p>                        选择与待分类样本距离最近的k个训练样本</p> 
<h4>                5、预测</h4> 
<p>                        对于分类任务：查看K个最近邻居中最常见的类别，作为预测结果。</p> 
<p>                        对于回归任务：预测结果可以是K个最近邻居的平均值或加权平均值。</p> 
<h4>                6、评估</h4> 
<p>                        使用适当的评价的评级骄傲指标评估模型的性能。</p> 
<h4>                7、优化</h4> 
<p>                        基于性能评估结果，可能需要返回并调整某些参数，如K值、距离度量方法等，以获得更好的性能。</p> 
<h2 style="background-color:transparent;">二、knn分类算法实例——性别预测</h2> 
<h5>                1、想法：设计一个KNN分类来用人的身高和体重来预测人的性别。</h5> 
<h5>                2、创建数据集：创立一个包含身高，体重与性别的数据集用来训练模型。</h5> 
<p>               </p> 
<p class="img-center"><img alt="" height="419" src="https://images2.imgbox.com/56/b1/0oEmKBDr_o.png" width="276"></p> 
<h5>3、读取数据集</h5> 
<pre><code class="language-java"># 准备数据：从文本文件中解析数据
import numpy as np
def file2matrix(filename):
    #打开文件
    fr = open(filename)
    arrayOLines = fr.readlines()
    #得到文件行数
    numberOfLines = len(arrayOLines)
    #返回的NumPy矩阵
    returnMat = np.zeros((numberOfLines,2))
    #返回的分类标签向量
    classLabelVector = []
    #行的索引值
    index = 0
    for line in arrayOLines:
        line = line.strip()
        listFromLine = line.split('\t')
        returnMat[index, :] = listFromLine[0:2]
        if listFromLine[-1] == 'male':
            classLabelVector.append(1)
        else:
            classLabelVector.append(0)
        index += 1
    return returnMat, classLabelVector</code></pre> 
<pre><code class="language-java">filename = r'C:\Users\Yusua\Desktop\1234.txt'
datingDataMat, datingLabels = file2matrix(filename)</code></pre> 
<ul><li style="text-align:justify;">读取文件内容：使用<code>readlines()</code>函数读取文件的所有行，每一行作为一个字符串存储在列表<code>arrayOLines</code>中。</li><li style="text-align:justify;">获取文件行数：使用<code>len()</code>函数获取<code>arrayOLines</code>列表的长度，即文件的行数，存储在变量<code>numberOfLines</code>中。</li><li style="text-align:justify;">创建返回的NumPy矩阵：使用<code>np.zeros()</code>函数创建一个形状为<code>(numberOfLines, 2)</code>的全零矩阵，存储在变量<code>returnMat</code>中。</li><li style="text-align:justify;">在数据集的最后一行，将male的标签转化为1，将female的标签转化为0</li></ul> 
<pre><code class="language-java">print(datingDataMat)
print(datingLabels)</code></pre> 
<p>将数据矩阵与标签矩阵打印出来</p> 
<p><img alt="" height="294" src="https://images2.imgbox.com/81/61/fhqGSS4S_o.png" width="413"></p> 
<h5>4、分析数据，创建身高-体重散点图。</h5> 
<pre><code class="language-java">import matplotlib.pyplot as plt
import matplotlib.lines as mlines
def showData(datingDataMat, datingLabels):
    fig, axs = plt.subplots(nrows=1, ncols=1, sharex=False, sharey=False, figsize=(13, 8))


    LabelsColors = []
    for i in datingLabels:
        if i == 1:
            LabelsColors.append('black')
        elif i == 0:
            LabelsColors.append('red')

    axs.scatter(x=datingDataMat[:, 0], y=datingDataMat[:, 1], color=LabelsColors, s=15, alpha=.5)
    axs0_title_text = axs.set_title('high_weight')
    axs0_xlabel_text = axs.set_xlabel('high')
    axs0_ylabel_text = axs.set_ylabel('weight')
    plt.setp(axs0_title_text, size=9, weight='bold', color='red')
    plt.setp(axs0_xlabel_text, size=7, weight='bold', color='black')
    plt.setp(axs0_ylabel_text, size=7, weight='bold', color='black')
    male = mlines.Line2D([], [], color='black', marker='.',
                              markersize=6, label='male')
    female = mlines.Line2D([], [], color='red', marker='.',
                               markersize=6, label='female')
    axs.legend(handles=[male, female])
    plt.show()</code></pre> 
<p><code>for i in datingLabels:</code>通过遍历<code>datingLabels标签</code>列表中的每个元素，判断标签是否为1或者0，然后将相应的颜色添加到<code>LabelsColors</code>列表中。</p> 
<p><code>axs.scatter</code>根据特征数据<code>datingDataMat</code>绘制散点图，其中<code>x=datingDataMat[:, 0]</code>表示取特征数据矩阵的第一列即身高作为x轴坐标，<code>y=datingDataMat[:, 1]</code>表示取特征数据矩阵的第二列即体重作为y轴坐标，来绘制散点图</p> 
<pre><code class="language-java">showData(datingDataMat,datingLabels)</code></pre> 
<p>展示散点图</p> 
<p><img alt="" height="363" src="https://images2.imgbox.com/50/76/PVv89lNg_o.png" width="561"></p> 
<h5>5、数据集的归一化处理</h5> 
<p></p> 
<pre><code class="language-java">def autoNorm(dataSet):
    #获得数据的最小值
    minVals = dataSet.min(0)
    maxVals = dataSet.max(0)
    #最大值和最小值的范围
    ranges = maxVals - minVals
    #shape(dataSet)返回dataSet的矩阵行列数
    normDataSet = np.zeros(np.shape(dataSet))
    #返回dataSet的行数
    m = dataSet.shape[0]
    #原始值减去最小值
    normDataSet = dataSet - np.tile(minVals, (m, 1))
    #除以最大和最小值的差,得到归一化数据
    normDataSet = normDataSet / np.tile(ranges, (m, 1))
    #返回归一化数据结果,数据范围,最小值
    return normDataSet, ranges, minVals</code></pre> 
<p>数据的归一化处理采用了newValue=（oldValue-min）/(max-min)的方法处理数据，使得数据变得更加合理。</p> 
<pre><code class="language-java">normDataSet, ranges, minVals = autoNorm(datingDataMat)
print(normDataSet)
print(ranges)
print(minVals)</code></pre> 
<p>显示结果</p> 
<p class="img-center"><img alt="" height="434" src="https://images2.imgbox.com/f0/30/246E05VD_o.png" width="276"></p> 
<h5>6、测试算法：作为完成程序验证分类器</h5> 
<pre><code class="language-java"># 分类器
import operator
# 输入：inX - 用于分类的数据（测试集）；dataSet - 训练集；labes - 分类标签；K - KNN算法参数，选择距离最小的K个点
# 输出：sortedClassCount[0][0] - 分类结果
def classify0(inX, dataSet, labels, k):
    #numpy函数shape[0]返回dataSet的行数
    dataSetSize = dataSet.shape[0]
    #在列向量方向上重复inX共1次(横向),行向量方向上重复inX共dataSetSize次(纵向)
    diffMat = np.tile(inX, (dataSetSize, 1)) - dataSet
    #二维特征相减后平方
    sqDiffMat = diffMat**2
    #sum()所有元素相加,sum(0)列相加,sum(1)行相加
    sqDistances = sqDiffMat.sum(axis=1)
    #开方,计算出距离
    distances = sqDistances**0.5
    #返回distances中元素从小到大排序后的索引值
    sortedDistIndices = distances.argsort()
    #定一个记录类别次数的字典
    classCount = {}
    for i in range(k):
        #取出前k个元素的类别
        voteIlabel = labels[sortedDistIndices[i]]
        #dict.get(key,default=None),字典的get()方法,返回指定键的值,如果值不在字典中返回默认值。
        #计算类别次数
        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1
    #python3中用items()替换python2中的iteritems()
    #key=operator.itemgetter(1)根据字典的值进行排序
    #key=operator.itemgetter(0)根据字典的键进行排序
    #reverse降序排序字典
    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)
    #返回次数最多的类别,即所要分类的类别
    return sortedClassCount[0][0]</code></pre> 
<blockquote> 
 <ul><li>首先，计算训练集<code>dataSet</code>的行数，即数据集中样本的数量，赋值给<code>dataSetSize</code>。</li><li>接着，将测试样本<code>inX</code>通过<code>np.tile(inX, (dataSetSize, 1)) - dataSet</code>得到一个与训练集形状相同的矩阵<code>diffMat</code>。这个矩阵的每一行表示测试样本与训练集中某个样本在每个特征上的差值。</li><li>然后，对<code>diffMat</code>中的每个元素进行平方操作，得到矩阵<code>sqDiffMat</code>，其中的每个元素表示测试样本与训练集中某个样本在每个特征上的差值的平方。</li><li>接下来，对<code>sqDiffMat</code>按行求和，得到一个包含每个样本与测试样本之间距离平方和的向量<code>sqDistances</code>。</li><li>对<code>sqDistances</code>中的每个元素进行开方操作，得到一个包含每个样本与测试样本之间距离的向量<code>distances</code>。</li><li>接着，将<code>distances</code>中的元素按从小到大进行排序，返回排序后的索引值，赋值给<code>sortedDistIndices</code>。</li><li>然后，定义一个记录类别次数的字典<code>classCount</code>。</li><li>接下来，遍历<code>sortedDistIndices</code>中的前<code>k</code>个元素，取出对应的训练样本的类别<code>voteIlabel</code>。</li><li>对于每个训练样本的类别<code>voteIlabel</code>，将其加入到<code>classCount</code>字典中，如果已经存在，则将对应的值加1；如果不存在，则设置初始值为1。</li><li>遍历完所有的训练样本后，得到一个包含各个类别及其出现次数的有序列表<code>sortedClassCount</code>，按照每个类别的出现次数从大到小排序。</li><li>最后，返回出现次数最多的类别，即为所要分类的类别。</li></ul> 
</blockquote> 
<h5>7、使用算法：构建完整可用系统</h5> 
<pre><code class="language-java"># 通过输入一个人的三维特征,进行分类输出
def classifyPerson():
    #输出结果
    resultList = ['male','female']
    #三维特征用户输入
    high = float(input("身高:"))
    weight = float(input("体重:"))
    
    #打开的文件名
    filename = r'C:\Users\Yusua\Desktop\1234.txt'
    #打开并处理数据
    datingDataMat, datingLabels = file2matrix(filename)
    #训练集归一化
    normMat, ranges, minVals = autoNorm(datingDataMat)
    #生成NumPy数组,测试集
    #inArr = np.array([precentTats, ffMiles, iceCream])
    inArr = np.array([high, weight])
    #测试集归一化
    norminArr = (inArr - minVals) / ranges
    #返回分类结果
    classifierResult = classify0(norminArr, normMat, datingLabels, 3)
    #打印结果
    print("你的性别是%s" % (resultList[classifierResult-1]))</code></pre> 
<pre><code class="language-java">classifyPerson()</code></pre> 
<p>构建了完整的系统后开始测试使用：输入身高：178 体重：60</p> 
<p><img alt="" height="93" src="https://images2.imgbox.com/51/30/VZ87Hri2_o.png" width="392"><img alt="" height="87" src="https://images2.imgbox.com/ce/77/QGIdEKQS_o.png" width="340"></p> 
<p>根据k近邻算法的出结果</p> 
<p><img alt="" height="54" src="https://images2.imgbox.com/2d/7a/ArKylaR9_o.png" width="165"></p> 
<p></p> 
<h2>三、总结</h2> 
<p>KNN是一种基于实例的学习算法，它根据训练样本的特征和对应的类别标签，通过计算测试样本与训练样本之间的距离来进行分类预测。本次实验使我们了解了K近邻算法的基本原理，概念模型以及算法流程，学会了如何使用KNN算法来解决一些简单的分类问题，</p> 
<p>       解决问题：在导入数据源后输出打印资源矩阵与标签矩阵时标签矩阵输出全为0，或者是显示字符串转换为float类型失败，在查找问题后发现是资源文件中数据的储存必须按照float型进行存储，否则资源数据读取有误，无法正常导入，</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/62419914b184bfc81544e3bc00c5b4c1/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">mysql中varchar长度为多少</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/df5d99effe6efa40a45cf60eddcfbb35/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">邻域搜索（Neighborhood Search ，NS）、大邻域搜索(Large NS , LNS)和自适应大邻域搜索（Adaptive LNS, ALNS）算法的联系与区别</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>