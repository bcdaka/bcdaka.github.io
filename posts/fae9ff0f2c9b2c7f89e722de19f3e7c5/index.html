<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【爬虫】Python实现爬取淘宝商品信息（超详细） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/fae9ff0f2c9b2c7f89e722de19f3e7c5/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【爬虫】Python实现爬取淘宝商品信息（超详细）">
  <meta property="og:description" content="目录 项目介绍
代码部分
引用第三方库
全局定义
主函数
爬虫主函数代码
翻页函数代码
​编辑
获取商品列表信息代码
完整代码
项目介绍 项目基于Python的第三方库Selenium模拟浏览器运行、PyQuery解析和操作HTML文档，获取淘宝平台中某类商品的详细信息（商品标题、价格、销量、商铺名称、地区、商品详情页链接、商铺链接等），并基于第三方库openpyxl建立、存储于Excel表格中。
效果预览：
代码部分 引用第三方库 # 声明第三方库/头文件 from selenium import webdriver from selenium.common.exceptions import TimeoutException from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from pyquery import PyQuery as pq import time import openpyxl as op #导入Excel读写库 【第三方库】主要运用到PyQuery、selenium、openpyxl等Python的第三方库；如若缺失，使用pip指令安装即可。
pip install pyquery pip install selenium pip install openpyxl 全局定义 # 全局变量 count = 1 # 写入Excel商品计数 # 启动ChromeDriver服务 options = webdriver.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-30T13:12:37+08:00">
    <meta property="article:modified_time" content="2024-08-30T13:12:37+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【爬虫】Python实现爬取淘宝商品信息（超详细）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 id="main-toc"><strong>目录</strong></h2> 
<blockquote> 
 <p id="%E9%A1%B9%E7%9B%AE%E4%BB%8B%E7%BB%8D-toc" style="margin-left:0px;"><a href="#%E9%A1%B9%E7%9B%AE%E4%BB%8B%E7%BB%8D" rel="nofollow">项目介绍</a></p> 
 <p id="%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86-toc" style="margin-left:0px;"><a href="#%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86" rel="nofollow">代码部分</a></p> 
 <p id="%E5%BC%95%E7%94%A8%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93-toc" style="margin-left:40px;"><a href="#%E5%BC%95%E7%94%A8%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93" rel="nofollow">引用第三方库</a></p> 
 <p id="%E5%85%A8%E5%B1%80%E5%AE%9A%E4%B9%89-toc" style="margin-left:40px;"><a href="#%E5%85%A8%E5%B1%80%E5%AE%9A%E4%B9%89" rel="nofollow">全局定义</a></p> 
 <p id="%E4%B8%BB%E5%87%BD%E6%95%B0-toc" style="margin-left:40px;"><a href="#%E4%B8%BB%E5%87%BD%E6%95%B0" rel="nofollow">主函数</a></p> 
 <p id="%E7%88%AC%E8%99%AB%E4%B8%BB%E5%87%BD%E6%95%B0%E4%BB%A3%E7%A0%81-toc" style="margin-left:40px;"><a href="#%E7%88%AC%E8%99%AB%E4%B8%BB%E5%87%BD%E6%95%B0%E4%BB%A3%E7%A0%81" rel="nofollow">爬虫主函数代码</a></p> 
 <p id="%E7%BF%BB%E9%A1%B5%E5%87%BD%E6%95%B0%E4%BB%A3%E7%A0%81-toc" style="margin-left:40px;"><a href="#%E7%BF%BB%E9%A1%B5%E5%87%BD%E6%95%B0%E4%BB%A3%E7%A0%81" rel="nofollow">翻页函数代码</a></p> 
 <p id="%E2%80%8B%E7%BC%96%E8%BE%91-toc" style="margin-left:40px;"><a href="#%E2%80%8B%E7%BC%96%E8%BE%91" rel="nofollow">​编辑</a></p> 
 <p id="%E8%8E%B7%E5%8F%96%E5%95%86%E5%93%81%E5%88%97%E8%A1%A8%E4%BF%A1%E6%81%AF%E4%BB%A3%E7%A0%81-toc" style="margin-left:40px;"><a href="#%E8%8E%B7%E5%8F%96%E5%95%86%E5%93%81%E5%88%97%E8%A1%A8%E4%BF%A1%E6%81%AF%E4%BB%A3%E7%A0%81" rel="nofollow">获取商品列表信息代码</a></p> 
 <p id="%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81-toc" style="margin-left:0px;"><a href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81" rel="nofollow">完整代码</a></p> 
</blockquote> 
<hr id="hr-toc"> 
<h2>项目介绍</h2> 
<blockquote> 
 <p>项目基于Python的第三方库Selenium模拟浏览器运行、PyQuery解析和操作HTML文档，获取淘宝平台中某类商品的详细信息（商品标题、价格、销量、商铺名称、地区、商品详情页链接、商铺链接等），并基于第三方库openpyxl建立、存储于Excel表格中。</p> 
</blockquote> 
<p>效果预览：</p> 
<p><img alt="" height="754" src="https://images2.imgbox.com/6f/b6/QbkCyBN9_o.png" width="1200"></p> 
<h2 id="%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86">代码部分</h2> 
<h3 id="%E5%BC%95%E7%94%A8%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93">引用第三方库</h3> 
<pre><code># 声明第三方库/头文件
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from pyquery import PyQuery as pq
import time
import openpyxl as op                      #导入Excel读写库</code></pre> 
<p>【第三方库】主要运用到PyQuery、selenium、openpyxl等Python的第三方库；如若缺失，使用pip指令安装即可。</p> 
<pre><code>pip install pyquery
pip install selenium
pip install openpyxl</code></pre> 
<h3 id="%E5%85%A8%E5%B1%80%E5%AE%9A%E4%B9%89">全局定义</h3> 
<pre><code># 全局变量
count = 1                           # 写入Excel商品计数

# 启动ChromeDriver服务
options = webdriver.ChromeOptions()
# 关闭自动测试状态显示 // 会导致浏览器报：请停用开发者模式
options.add_experimental_option("excludeSwitches", ['enable-automation'])
# 把chrome设为selenium驱动的浏览器代理；
driver = webdriver.Chrome(options=options)
# 窗口最大化
driver.maximize_window()
# wait是Selenium中的一个等待类，用于在特定条件满足之前等待一定的时间(这里是15秒)。
# 如果一直到等待时间都没满足则会捕获TimeoutException异常
wait = WebDriverWait(driver,10)
# 打开页面后会强制停止10秒，请在此时手动扫码登陆</code></pre> 
<h3 id="%E4%B8%BB%E5%87%BD%E6%95%B0">主函数</h3> 
<blockquote> 
 <p>1、输入初始参数：</p> 
 <ul><li>爬取商品的关键词Keyword</li><li>爬取网页的起始页PageStart</li><li>爬取网页的终止页PageEnd</li></ul> 
 <p>2、建立Excel表格，并设置第一行（表头）；</p> 
 <p>3、调用爬虫主函数Crawer_main，启动爬虫程序；</p> 
 <p>4、输入存储文件名称filename，输出.xlsx格式文件</p> 
</blockquote> 
<pre><code>if __name__ == '__main__':

    KEYWORD = input('输入搜索的商品关键词Keyword：')           # 要搜索的商品的关键词
    pageStart = int(input('输入爬取的起始页PageStart：'))     # 爬取起始页
    pageEnd = int(input('输入爬取的终止页PageEnd：'))         # 爬取终止页

    # 建立Excel表格
    try:
        ws = op.Workbook()                                  # 创建Workbook
        wb = ws.create_sheet(index=0)                       # 创建worsheet
        # Excel第一行：表头
        title_list = ['Page', 'Num', 'title', 'Price', 'Deal', 'Location', 'Shop', 'IsPostFree', 'Title_URL',
                      'Shop_URL', 'Img_URL']
        for i in range(0, len(title_list)):
            wb.cell(row=count, column=i + 1, value=title_list[i])
        count += 1  # 从第二行开始写爬取数据
    except Exception as exc:
        print("Excel建立失败！")

    # 开始爬取数据
    Crawer_main(KEYWORD,pageStart,pageEnd)

    # 保存Excel表格
    Filename = input("输入存储文件名称：")
    Filename = Filename + '(_From_Taobao).xlsx'
    ws.save(filename = Filename)
    print(Filename + "存储成功~")</code></pre> 
<p>（输入）效果预览：</p> 
<p><img alt="" height="300" src="https://images2.imgbox.com/89/6c/p1YA45eK_o.png" width="1122"></p> 
<h3 id="%E7%88%AC%E8%99%AB%E4%B8%BB%E5%87%BD%E6%95%B0%E4%BB%A3%E7%A0%81">爬虫主函数代码</h3> 
<blockquote> 
 <p>1、ChromeDriver服务请求淘宝（https://www.taobao.com）服务，模拟浏览器运行，找到“输入框”输入关键词KEYWORD，并点击“搜索”按键；</p> 
 <p>2、若弹出登录窗口，使用手机“淘宝”APP，扫码登录（如图所示）；</p> 
 <p>3、判断PageStart是否为1；若PageStart不为1，跳转至PageStart所在页；</p> 
 <p>4、调用get_goods获取起始页PageStart的商品列表信息；</p> 
 <p>5、调用page_turning进行翻页，并获取所在页商品列表信息；</p> 
</blockquote> 
<pre><code># 爬虫main函数
def Crawer_main(KEYWORD,pageStart,pageEnd):
    try:
        # 爬取从pageStart到pageAll页的数据
        search_goods(KEYWORD,pageStart,pageEnd)
    except Exception as exc:
        print('Crawer_main函数报错:', exc)

# 输入“关键词”，搜索，并进行首次爬取
def search_goods(KEYWORD,start_page,total_pages):
    try:
        print('正在搜索: ')
        driver.get('https://www.taobao.com')
        # time.sleep(10)           # 强制停止10秒，请在此时手动扫码登陆
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument",
                               {"source": """Object.defineProperty(navigator, 'webdriver', {get: () =&gt; undefined})"""})
        # 找到搜索“输入框”
        input = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#q")))
        # 找到“搜索”按钮
        submit = wait.until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, '#J_TSearchForm &gt; div.search-button &gt; button')))
        # 输入框写入“关键词KeyWord”
        input.send_keys(KEYWORD)
        # 点击“搜索”按键
        submit.click()
        # 搜索商品后会再强制停止5秒，如有滑块请手动操作
        time.sleep(5)

        # 如果不是从第一页开始爬取，就滑动到底部输入页面然后跳转
        if start_page != 1:
            # 滑动到页面底端
            # driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            # 滑动到底部后停留3s
            time.sleep(3)
            # 找到输入“页面”的表单，输入“起始页”
            pageInput = wait.until(EC.presence_of_element_located(
                (By.XPATH, '//*[@id="pageContent"]/div[1]/div[3]/div[4]/div/div/span[3]/input')))
            pageInput.send_keys(start_page)
            # 找到页面跳转的“确定”按钮，并且点击
            admit = wait.until(EC.element_to_be_clickable(
                (By.XPATH, '//*[@id="pageContent"]/div[1]/div[3]/div[4]/div/div/button[3]')))
            admit.click()

        # 获取商品信息
        get_goods(start_page)

        # 翻页操作
        for i in range(start_page + 1, total_pages+1):
            page_turning(i)

    except TimeoutException:
        print("search_goods: error")
        return search_goods(KEYWORD,start_page,total_pages)</code></pre> 
<p>淘宝登录界面示意图：</p> 
<p><img alt="" height="1041" src="https://images2.imgbox.com/6a/6b/7kU5jaNP_o.png" width="1200"></p> 
<h3 id="%E7%BF%BB%E9%A1%B5%E5%87%BD%E6%95%B0%E4%BB%A3%E7%A0%81">翻页函数代码</h3> 
<blockquote> 
 <p>翻页函数page_turning搜索并点击“下一页”按键，判断页码是否相等；若页码相等获取该页商品列表信息。</p> 
</blockquote> 
<pre><code># 翻页函数
def page_turning(page_number):
    print('正在翻页: ', page_number)
    try:
        # 强制等待2秒后翻页
        time.sleep(2)
        # 找到“下一页”的按钮
        submit = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id="pageContent"]/div[1]/div[3]/div[4]/div/div/button[2]')))
        submit.click()
        # 判断页数是否相等
        wait.until(EC.text_to_be_present_in_element((By.XPATH, '//*[@id="sortBarWrap"]/div[1]/div[2]/div[2]/div[8]/div/span/em'), str(page_number)))
        get_goods(page_number)
    except TimeoutException:
        print("page_number: error")
        page_turning(page_number)</code></pre> 
<p>“下一页”按键示意图：</p> 
<h3 id="%E2%80%8B%E7%BC%96%E8%BE%91"><img alt="" height="775" src="https://images2.imgbox.com/88/aa/AWXqHLZC_o.png" width="1200"></h3> 
<h3 id="%E8%8E%B7%E5%8F%96%E5%95%86%E5%93%81%E5%88%97%E8%A1%A8%E4%BF%A1%E6%81%AF%E4%BB%A3%E7%A0%81">获取商品列表信息代码</h3> 
<blockquote> 
 <p>1、强制等待10秒，用于刷新或滑动页面，使页面所有信息加载完成；</p> 
 <p>2、pyquery请求HTML页面信息，并进行解析；</p> 
 <p>3、商品详细信息（商品标题、价格、销量、商铺名称、地区、详情页链接、商铺链接等）</p> 
 <p>4、将获取的信息写入字典和Excel表格中；</p> 
</blockquote> 
<pre><code># 获取每一页的商品信息；
def get_goods(page):
    # 声明全局变量count
    global count

    # 获取商品前固定等待10秒,刷新/滑动界面，使所有信息都加载完成
    time.sleep(10)

    html = driver.page_source
    doc = pq(html)
    # 提取所有商品的共同父元素的类选择器
    items = doc('div.PageContent--contentWrap--mep7AEm &gt; div.LeftLay--leftWrap--xBQipVc &gt; div.LeftLay--leftContent--AMmPNfB &gt; div.Content--content--sgSCZ12 &gt; div &gt; a').items()

    for item in items:
        # 定位商品标题
        title = item.find('.Title--title--jCOPvpf span').text()
        # 定位价格
        price_int = item.find('.Price--priceInt--ZlsSi_M').text()
        price_float = item.find('.Price--priceFloat--h2RR0RK').text()
        if price_int and price_float:
            price = float(f"{price_int}{price_float}")
        else:
            price = 0.0
        # 定位交易量
        deal = item.find('.Price--realSales--FhTZc7U').text()
        # 定位所在地信息
        location = item.find('.Price--procity--_7Vt3mX').text()
        # 定位店名
        shop = item.find('.ShopInfo--TextAndPic--yH0AZfx a').text()
        # 定位包邮的位置
        postText = item.find('.SalesPoint--subIconWrapper--s6vanNY span').text()
        postText = "包邮" if "包邮" in postText else "/"
        # 定位商品url
        # t_url = item.find('.Card--doubleCardWrapper--L2XFE73')
        # t_url = t_url.attr('href')
        t_url = item.attr('href')
        # print(t_url)
        # 定位店名url
        shop_url = item.find('.ShopInfo--TextAndPic--yH0AZfx a')
        shop_url = shop_url.attr('href')
        # print(shop_url)
        # 定位商品图片url
        img = item.find('.MainPic--mainPicWrapper--iv9Yv90 img')
        img_url = img.attr('src')
        # print(img_url)

        # 构建商品信息字典
        product = {
            'Page': page,
            'Num': count-1,
            'title': title,
            'price': price,
            'deal': deal,
            'location': location,
            'shop': shop,
            'isPostFree': postText,
            'url': t_url,
            'shop_url': shop_url,
            'img_url': img_url
        }
        print(product)

        # 商品信息写入Excel表格中
        wb.cell(row=count, column=1, value=page)                # 页码
        wb.cell(row=count, column=2, value=count-1)             # 序号
        wb.cell(row=count, column=3, value=title)               # 标题
        wb.cell(row=count, column=4, value=price)               # 价格
        wb.cell(row=count, column=5, value=deal)                # 付款人数
        wb.cell(row=count, column=6, value=location)            # 地理位置
        wb.cell(row=count, column=7, value=shop)                # 店铺名称
        wb.cell(row=count, column=8, value=postText)            # 是否包邮
        wb.cell(row=count, column=9, value=t_url)               # 商品链接
        wb.cell(row=count, column=10, value=shop_url)           # 商铺链接
        wb.cell(row=count, column=11, value=img_url)            # 图片链接
        count += 1                                       </code></pre> 
<p>获取商品列表信息示意图：</p> 
<p><img alt="" height="495" src="https://images2.imgbox.com/27/38/rIp07GMJ_o.png" width="1200"></p> 
<h2 id="%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81">完整代码</h2> 
<pre><code># 代码说明：
'''
代码功能： 基于ChromeDriver爬取taobao（淘宝）平台商品列表数据
输入参数:  KEYWORLD --&gt; 搜索商品“关键词”；
          pageStart --&gt; 爬取起始页；
          pageEnd --&gt; 爬取终止页；
输出文件：爬取商品列表数据
        'Page'        ：页码
        'Num'         ：序号
        'title'       ：商品标题
        'Price'       ：商品价格
        'Deal'        ：商品销量
        'Location'    ：地理位置
        'Shop'        ：商品
        'IsPostFree'  ：是否包邮
        'Title_URL'   ：商品详细页链接
        'Shop_URL'    ：商铺链接
        'Img_URL'     ：图片链接
'''
# 声明第三方库/头文件
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from pyquery import PyQuery as pq
import time
import openpyxl as op               #导入Excel读写库
# 全局变量
count = 1                           # 写入Excel商品计数

# 启动ChromeDriver服务
options = webdriver.ChromeOptions()
# 关闭自动测试状态显示 // 会导致浏览器报：请停用开发者模式
options.add_experimental_option("excludeSwitches", ['enable-automation'])
# 把chrome设为selenium驱动的浏览器代理；
driver = webdriver.Chrome(options=options)
# 窗口最大化
driver.maximize_window()
# wait是Selenium中的一个等待类，用于在特定条件满足之前等待一定的时间(这里是15秒)。
# 如果一直到等待时间都没满足则会捕获TimeoutException异常
wait = WebDriverWait(driver,10)
# 打开页面后会强制停止10秒，请在此时手动扫码登陆

# 输入“关键词”，搜索，并进行首次爬取
def search_goods(KEYWORD,start_page,total_pages):
    try:
        print('正在搜索: ')
        driver.get('https://www.taobao.com')
        # time.sleep(10)           # 强制停止10秒，请在此时手动扫码登陆
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument",
                               {"source": """Object.defineProperty(navigator, 'webdriver', {get: () =&gt; undefined})"""})
        # 找到搜索“输入框”
        input = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#q")))
        # 找到“搜索”按钮
        submit = wait.until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, '#J_TSearchForm &gt; div.search-button &gt; button')))
        # 输入框写入“关键词KeyWord”
        input.send_keys(KEYWORD)
        # 点击“搜索”按键
        submit.click()
        # 搜索商品后会再强制停止5秒，如有滑块请手动操作
        # time.sleep(5)

        # 如果不是从第一页开始爬取，就滑动到底部输入页面然后跳转
        if start_page != 1:
            # 滑动到页面底端
            # driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            # 滑动到底部后停留3s
            time.sleep(3)
            # 找到输入“页面”的表单，输入“起始页”
            pageInput = wait.until(EC.presence_of_element_located(
                (By.XPATH, '//*[@id="pageContent"]/div[1]/div[3]/div[4]/div/div/span[3]/input')))
            pageInput.send_keys(start_page)
            # 找到页面跳转的“确定”按钮，并且点击
            admit = wait.until(EC.element_to_be_clickable(
                (By.XPATH, '//*[@id="pageContent"]/div[1]/div[3]/div[4]/div/div/button[3]')))
            admit.click()

        # 获取商品信息
        get_goods(start_page)

        # 翻页操作
        for i in range(start_page + 1, total_pages+1):
            page_turning(i)

    except TimeoutException:
        print("search_goods: error")
        return search_goods(KEYWORD,start_page,total_pages)

# 翻页函数
def page_turning(page_number):
    print('正在翻页: ', page_number)
    try:
        # 强制等待2秒后翻页
        time.sleep(2)
        # 找到“下一页”的按钮
        submit = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id="pageContent"]/div[1]/div[3]/div[4]/div/div/button[2]')))
        submit.click()
        # 判断页数是否相等
        wait.until(EC.text_to_be_present_in_element((By.XPATH, '//*[@id="sortBarWrap"]/div[1]/div[2]/div[2]/div[8]/div/span/em'), str(page_number)))
        get_goods(page_number)
    except TimeoutException:
        print("page_number: error")
        page_turning(page_number)

# 获取每一页的商品信息；
def get_goods(page):
    # 声明全局变量count
    global count

    # 获取商品前固定等待10秒,刷新/滑动界面，使所有信息都加载完成
    time.sleep(10)

    html = driver.page_source
    doc = pq(html)
    # 提取所有商品的共同父元素的类选择器
    items = doc('div.PageContent--contentWrap--mep7AEm &gt; div.LeftLay--leftWrap--xBQipVc &gt; div.LeftLay--leftContent--AMmPNfB &gt; div.Content--content--sgSCZ12 &gt; div &gt; a').items()

    for item in items:
        # 定位商品标题
        title = item.find('.Title--title--jCOPvpf span').text()
        # 定位价格
        price_int = item.find('.Price--priceInt--ZlsSi_M').text()
        price_float = item.find('.Price--priceFloat--h2RR0RK').text()
        if price_int and price_float:
            price = float(f"{price_int}{price_float}")
        else:
            price = 0.0
        # 定位交易量
        deal = item.find('.Price--realSales--FhTZc7U').text()
        # 定位所在地信息
        location = item.find('.Price--procity--_7Vt3mX').text()
        # 定位店名
        shop = item.find('.ShopInfo--TextAndPic--yH0AZfx a').text()
        # 定位包邮的位置
        postText = item.find('.SalesPoint--subIconWrapper--s6vanNY span').text()
        postText = "包邮" if "包邮" in postText else "/"
        # 定位商品url
        # t_url = item.find('.Card--doubleCardWrapper--L2XFE73')
        # t_url = t_url.attr('href')
        t_url = item.attr('href')
        # print(t_url)
        # 定位店名url
        shop_url = item.find('.ShopInfo--TextAndPic--yH0AZfx a')
        shop_url = shop_url.attr('href')
        # print(shop_url)
        # 定位商品图片url
        img = item.find('.MainPic--mainPicWrapper--iv9Yv90 img')
        img_url = img.attr('src')
        # print(img_url)

        # 构建商品信息字典
        product = {
            'Page': page,
            'Num': count-1,
            'title': title,
            'price': price,
            'deal': deal,
            'location': location,
            'shop': shop,
            'isPostFree': postText,
            'url': t_url,
            'shop_url': shop_url,
            'img_url': img_url
        }
        print(product)

        # 商品信息写入Excel表格中
        wb.cell(row=count, column=1, value=page)                # 页码
        wb.cell(row=count, column=2, value=count-1)             # 序号
        wb.cell(row=count, column=3, value=title)               # 标题
        wb.cell(row=count, column=4, value=price)               # 价格
        wb.cell(row=count, column=5, value=deal)                # 付款人数
        wb.cell(row=count, column=6, value=location)            # 地理位置
        wb.cell(row=count, column=7, value=shop)                # 店铺名称
        wb.cell(row=count, column=8, value=postText)            # 是否包邮
        wb.cell(row=count, column=9, value=t_url)               # 商品链接
        wb.cell(row=count, column=10, value=shop_url)           # 商铺链接
        wb.cell(row=count, column=11, value=img_url)            # 图片链接
        count += 1                                              # 下一行

# 爬虫main函数
def Crawer_main(KEYWORD,pageStart,pageEnd):
    try:
        # 爬取从pageStart到pageAll页的数据
        search_goods(KEYWORD,pageStart,pageEnd)
    except Exception as exc:
        print('Crawer_main函数报错:', exc)




if __name__ == '__main__':

    KEYWORD = input('输入搜索的商品关键词Keyword：')           # 要搜索的商品的关键词
    pageStart = int(input('输入爬取的起始页PageStart：'))     # 爬取起始页
    pageEnd = int(input('输入爬取的终止页PageEnd：'))         # 爬取终止页

    # 建立Excel表格
    try:
        ws = op.Workbook()                                  # 创建Workbook
        wb = ws.create_sheet(index=0)                       # 创建worsheet
        # Excel第一行：表头
        title_list = ['Page', 'Num', 'title', 'Price', 'Deal', 'Location', 'Shop', 'IsPostFree', 'Title_URL',
                      'Shop_URL', 'Img_URL']
        for i in range(0, len(title_list)):
            wb.cell(row=count, column=i + 1, value=title_list[i])
        count += 1  # 从第二行开始写爬取数据
    except Exception as exc:
        print("Excel建立失败！")

    # 开始爬取数据
    Crawer_main(KEYWORD,pageStart,pageEnd)

    # 保存Excel表格
    Filename = input("输入存储文件名称：")
    Filename = Filename + '(_From_Taobao).xlsx'
    ws.save(filename = Filename)
    print(Filename + "存储成功~")</code></pre> 
<blockquote> 
 <p>【不足】不足之处，恳请批评指正，我们共同进步！</p> 
 <p>【鸣谢】特别感谢“<a href="https://blog.csdn.net/weixin_48266589" title="芝士胡椒粉">芝士胡椒粉</a>”的<a class="link-info" href="https://blog.csdn.net/weixin_48266589/article/details/135303310" title="文章">文章</a>指导！</p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1a533577f2e66275a955e34586ffbd99/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【C&#43;&#43;】AVL树</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e5c8068b6f52a62f78175c5eb6201662/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">github怎么删除项目</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>