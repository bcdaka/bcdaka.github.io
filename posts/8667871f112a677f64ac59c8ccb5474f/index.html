<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Llama-Factory &#43; Ollama æ‰“é€ å±äºè‡ªå·±çš„ä¸­æ–‡ç‰ˆ Llama3 - ç¼–ç¨‹å¤§å’–</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/8667871f112a677f64ac59c8ccb5474f/">
  <meta property="og:site_name" content="ç¼–ç¨‹å¤§å’–">
  <meta property="og:title" content="Llama-Factory &#43; Ollama æ‰“é€ å±äºè‡ªå·±çš„ä¸­æ–‡ç‰ˆ Llama3">
  <meta property="og:description" content="Meta æ¨å‡º Llama3 ä¹Ÿæœ‰ä¸€å°æ®µæ—¶é—´äº†ã€‚Llama3 åŒ…å« 8B å’Œ 70B ä¸¤ç§å‚æ•°è§„æ¨¡ï¼Œæ¶µç›–é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒä¼˜çš„å˜ä½“ã€‚Llama 3 æ”¯æŒå¤šç§å•†ä¸šå’Œç ”ç©¶ç”¨é€”ï¼Œå¹¶å·²åœ¨å¤šä¸ªè¡Œä¸šæ ‡å‡†æµ‹è¯•ä¸­å±•ç¤ºäº†å…¶å“è¶Šçš„æ€§èƒ½ï¼ˆå…³äºLlama3çš„å…·ä½“ä»‹ç»å¯ä»¥å‚è€ƒæœ¬ç«™å¦å¤–ä¸€ç¯‡åšæ–‡ï¼šå°é²œ Meta Llama 3ï¼‰ã€‚å’Œä»¥å¾€çš„åŸå§‹ Llama æ¨¡å‹ä¸€æ ·ï¼ŒLlama 3 å¯¹ä¸­æ–‡çš„æ”¯æŒæ•ˆæœæ¬ ä½³ï¼Œç»å¸¸ä¼šå‡ºç°ä½ ç”¨ä¸­æ–‡æé—®ï¼Œå®ƒç”¨è‹±æ–‡æˆ–ä¸­æ–‡&#43;è‹±æ–‡å›å¤çš„ç°è±¡ã€‚
ä¾‹å¦‚å½“æˆ‘é—®å®ƒï¼šâ€ä½ æ˜¯è°ï¼Ÿâ€œï¼Œä¼šå¾—åˆ°å¦‚ä¸‹çš„å›ç­”ï¼š
&gt;&gt;&gt; ä½ æ˜¯è°ï¼Ÿ I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I&#39;m not a human, but a computer program designed to simulate conversation and answer questions to the best of my ability based on my training data. I can generate text on a wide range of topics, from science and history to entertainment and culture.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-06T17:20:41+08:00">
    <meta property="article:modified_time" content="2024-05-06T17:20:41+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="ç¼–ç¨‹å¤§å’–" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">ç¼–ç¨‹å¤§å’–</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Llama-Factory &#43; Ollama æ‰“é€ å±äºè‡ªå·±çš„ä¸­æ–‡ç‰ˆ Llama3</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><img alt="" height="720" src="https://images2.imgbox.com/e7/be/mMXdBmdm_o.png" width="1200"></p> 
<p>Meta æ¨å‡º Llama3 ä¹Ÿæœ‰ä¸€å°æ®µæ—¶é—´äº†ã€‚Llama3 åŒ…å« 8B å’Œ 70B ä¸¤ç§å‚æ•°è§„æ¨¡ï¼Œæ¶µç›–é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒä¼˜çš„å˜ä½“ã€‚Llama 3 æ”¯æŒå¤šç§å•†ä¸šå’Œç ”ç©¶ç”¨é€”ï¼Œå¹¶å·²åœ¨å¤šä¸ªè¡Œä¸šæ ‡å‡†æµ‹è¯•ä¸­å±•ç¤ºäº†å…¶å“è¶Šçš„æ€§èƒ½ï¼ˆå…³äºLlama3çš„å…·ä½“ä»‹ç»å¯ä»¥å‚è€ƒæœ¬ç«™å¦å¤–ä¸€ç¯‡åšæ–‡ï¼š<a href="https://blog.yanghong.dev/meta-llama3-release/" rel="nofollow" title="å°é²œ Meta Llama 3">å°é²œ Meta Llama 3</a>ï¼‰ã€‚å’Œä»¥å¾€çš„åŸå§‹ Llama æ¨¡å‹ä¸€æ ·ï¼ŒLlama 3 å¯¹ä¸­æ–‡çš„æ”¯æŒæ•ˆæœæ¬ ä½³ï¼Œç»å¸¸ä¼šå‡ºç°ä½ ç”¨ä¸­æ–‡æé—®ï¼Œå®ƒç”¨è‹±æ–‡æˆ–ä¸­æ–‡+è‹±æ–‡å›å¤çš„ç°è±¡ã€‚</p> 
<p>ä¾‹å¦‚å½“æˆ‘é—®å®ƒï¼šâ€ä½ æ˜¯è°ï¼Ÿâ€œï¼Œä¼šå¾—åˆ°å¦‚ä¸‹çš„å›ç­”ï¼š</p> 
<pre><code>&gt;&gt;&gt; ä½ æ˜¯è°ï¼Ÿ
I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm not a human, but a computer program designed to simulate conversation and answer questions to the best of my ability based on my training data. I can generate text on a wide range of topics, from science and history to entertainment and culture.</code></pre> 
<p></p> 
<p>ç„¶åè®©å®ƒâ€ä»‹ç»ä¸€ä¸‹ä¸Šæµ·çš„æ—…æ¸¸æ™¯ç‚¹â€œï¼Œåˆ™ä¼šå¾—åˆ°å¦‚ä¸‹çš„å›ç­”ï¼š</p> 
<pre><code class="hljs">&gt;&gt;&gt; ä»‹ç»ä¸€ä¸‹ä¸Šæµ·çš„æ—…æ¸¸æ™¯ç‚¹
Shanghai, a city with a rich history and culture, has numerous tourist attractions that showcase its modernity, traditional charm, and natural beauty. Here are some of the most popular tourist spots in Shanghai:

**Landmarks:**

1. **Oriental Pearl Tower**: A iconic TV tower with panoramic views of the city.
2. **Shanghai Tower**: The second-tallest building in the world, offering stunning views from its observation deck.
3. **Jin Mao Tower**: A 421-meter-tall skyscraper and home to the luxurious Mandarin Oriental Hotel.
4. **The Bund**: A famous waterfront promenade lined with colonial-era buildings and stunning views of the Huangpu River.

**Historical Sites:**

1. **Yu Garden**: A classic Chinese garden with beautiful landscaping, pavilions, and koi ponds.
2. **Shanghai Museum**: A museum showcasing Chinese art, history, and culture.
3. **Xintiandi**: A charming neighborhood with traditional Shikumen architecture and modern boutiques.
4. **French Concession**: A historic neighborhood with colonial-era buildings, cafes, and restaurants.

**Nature Reserves:**

1. **Changfeng Park**: A large public park with gardens, lakes, and walking trails.
2. **Fuxing Island**: A tranquil oasis in the Huangpu River with beautiful landscaping and walking paths.
3. **Shanghai Botanical Garden**: A peaceful garden featuring various plant species and a peaceful atmosphere.

**Other Attractions:**

1. **Disney Resort Shanghai**: An amusement park with Disney-themed rides, shows, and attractions.
2. **Shanghai Grand Opera House**: A stunning opera house with performances ranging from music to dance.
3. **Shanghai Zoo**: A popular zoo with various animal species and a panda enclosure.
4. **The Shanghai Tower's Observation Deck**: For panoramic views of the city.

These are just a few of the many tourist attractions in Shanghai. Depending on your interests, there are plenty more hidden gems to explore! ğŸ˜Š
</code></pre> 
<p>å›ç­”å¾—è¿˜æ˜¯å¾ˆç®€æ´è€Œå…¨é¢çš„ã€‚ç¼ºç‚¹å°±æ˜¯å…¨æ˜¯è‹±æ–‡ç­”æ¡ˆï¼Œå¯¹äºè‹±æ–‡ä¸å¤ªå¥½çš„äººæœ‰ç‚¹åƒåŠ›ã€‚å¯¹ Llama3 è¿›è¡Œå¾®è°ƒæ¥æ”¯æŒä¸­æ–‡æ˜¯ä¸€é¡¹å¿…é¡»çš„å·¥ä½œã€‚</p> 
<h3>Llama3 ä¸­æ–‡æ¨¡å‹çš„ç°çŠ¶</h3> 
<p>å®é™…ä¸Šï¼Œå½“ Llama3 æ¨å‡ºä»¥åï¼Œå¾ˆå¤šå¼€å‘è€…åœ¨ä¸­æ–‡æ”¯æŒæ–¹é¢åšäº†å¤§é‡çš„å·¥ä½œï¼Œä¸€å¤§æ‰¹å¾®è°ƒçš„ä¸­æ–‡æ¨¡å‹å¦‚é›¨åæ˜¥ç¬‹èˆ¬çº·çº·æ¶Œç°ã€‚ç›®å‰æ¯”è¾ƒçŸ¥åçš„ä¸­æ–‡æ¨¡å‹æœ‰ï¼š</p> 
<ul><li>Unichat-llama3-Chineseï¼ˆ<a href="https://www.modelscope.cn/models/UnicomAI/Unichat-llama3-Chinese/summary" rel="nofollow" title="åœ°å€">åœ°å€</a>ï¼‰ï¼šä¸­å›½è”é€šAIåˆ›æ–°ä¸­å¿ƒå‘å¸ƒä¸šç•Œç¬¬ä¸€ä¸ªllama3ä¸­æ–‡æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼Œå…¨å‚æ•°å¾®è°ƒ(élora)ï¼Œä»¥<strong>Meta Llama 3</strong>ä¸ºåŸºç¡€,å¢åŠ ä¸­æ–‡æ•°æ®è¿›è¡Œè®­ç»ƒ,å®ç°llama3æ¨¡å‹é«˜è´¨é‡ä¸­æ–‡é—®ç­”ï¼Œæ¨¡å‹ä¸Šä¸‹æ–‡ä¿æŒåŸç”Ÿé•¿åº¦8Kï¼Œæ”¯æŒé•¿åº¦64Kç‰ˆæœ¬å°†äºåç»­å‘å¸ƒã€‚</li><li>OpenBuddy â€“ Open Multilingual Chatbotï¼ˆ<a href="https://huggingface.co/OpenBuddy/openbuddy-llama3-8b-v21.1-8k" rel="nofollow" title="åœ°å€">åœ°å€</a>ï¼‰ï¼šOpenBuddyæ˜¯ä¸€ä¸ªé¢å‘å…¨çƒç”¨æˆ·çš„å¼ºå¤§çš„å¼€æ”¾å¼å¤šè¯­è¨€èŠå¤©æœºå™¨äººæ¨¡å‹ï¼Œå¼ºè°ƒå¯¹è¯å¼AIä»¥åŠå¯¹è‹±è¯­ã€ä¸­æ–‡å’Œå…¶ä»–è¯­è¨€çš„æ— ç¼å¤šè¯­è¨€æ”¯æŒã€‚</li><li>Llama3-Chineseï¼ˆ<a href="https://github.com/seanzhang-zhichen/llama3-chinese" title="åœ°å€">åœ°å€</a>ï¼‰ï¼šåœ¨500ké«˜è´¨é‡ä¸­æ–‡å¤šè½®SFTæ•°æ®ã€100kè‹±è¯­å¤šè½®SFTæ•°æ®å’Œ2kå•è½®è‡ªæˆ‘è®¤çŸ¥æ•°æ®ä¸Šè®­ç»ƒçš„å¤§å‹æ¨¡å‹ï¼Œé‡‡ç”¨åŸºäº<strong>Metaçš„</strong>DORAå’ŒLORA+çš„è®­ç»ƒæ–¹æ³•ã€‚ä»¥<strong>Llama-3-8B</strong>ä¸ºåŸºç¡€ã€‚</li><li>Llama3-8B-Chinese-Chatï¼ˆ<a href="https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat" rel="nofollow" title="åœ°å€">åœ°å€</a>ï¼‰ï¼šæ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸­æ–‡å’Œè‹±æ–‡ç”¨æˆ·çš„æŒ‡ä»¤è°ƒæ•´è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰è§’è‰²æ‰®æ¼”å’Œå·¥å…·ä½¿ç”¨ç­‰å„ç§èƒ½åŠ›ï¼Œå»ºç«‹åœ¨ Meta-Llama-3-8B-Instruct æ¨¡å‹çš„åŸºç¡€ä¸Šã€‚</li><li>Llama-3-8B-Instruct-Chinese-chatï¼ˆ<a href="https://github.com/Rookie1019/Llama-3-8B-Instruct-Chinese" title="åœ°å€">åœ°å€</a>ï¼‰ï¼šä½¿ç”¨Â <a href="https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M" rel="nofollow" title="firefly-train-1.1M">firefly-train-1.1M</a>ï¼Œ<a href="https://huggingface.co/datasets/YeungNLP/moss-003-sft-data" rel="nofollow" title="moss-003-sft-data">moss-003-sft-data</a>ï¼Œ<a href="https://huggingface.co/datasets/YeungNLP/school_math_0.25M" rel="nofollow" title="school_math_0.25M">school_math_0.25M</a>ï¼Œ<a href="https://huggingface.co/datasets/LooksJuicy/ruozhiba" rel="nofollow" title="ruozhiba">ruozhiba</a>Â ç­‰æ•°æ®é›†å¾®è°ƒçš„æ¨¡å‹ï¼ŒåŸºäºLlama-3-8B-Instructã€‚</li><li>Bunny-Llama-3-8B-Vï¼ˆ<a href="https://wisemodel.cn/models/BAAI/Bunny-Llama-3-8B-V" rel="nofollow" title="åœ°å€">åœ°å€</a>ï¼‰ï¼šBunny æ˜¯ä¸€ç³»åˆ—è½»é‡ä½†åŠŸèƒ½å¼ºå¤§çš„å¤šæ¨¡å¼æ¨¡å‹ã€‚å®ƒæä¾›å¤šç§å³æ’å³ç”¨è§†è§‰ç¼–ç å™¨ï¼Œå¦‚ EVA-CLIPã€SigLIP å’Œè¯­è¨€ä¸»å¹²ï¼ŒåŒ…æ‹¬ Llama-3-8Bã€Phi-1.5ã€StableLM-2ã€Qwen1.5ã€MiniCPM å’Œ Phi-2ã€‚ä¸ºäº†å¼¥è¡¥æ¨¡å‹å¤§å°çš„å‡å°ï¼Œé€šè¿‡ä»æ›´å¹¿æ³›çš„æ•°æ®æºä¸­è¿›è¡Œç²¾é€‰æ¥æ„å»ºä¿¡æ¯æ›´ä¸°å¯Œçš„è®­ç»ƒæ•°æ®ã€‚</li><li>llava-llama-3-8b-v1_1ï¼ˆ<a href="https://huggingface.co/xtuner/llava-llama-3-8b-v1_1" rel="nofollow" title="åœ°å€">åœ°å€</a>ï¼‰ï¼šllava-llama-3-8b-v1_1 æ˜¯ä¸€ä¸ª LLaVA æ¨¡å‹ï¼Œç”±<a href="https://github.com/InternLM/xtuner" title="XTuner">XTuner</a>ä½¿ç”¨<a href="https://huggingface.co/datasets/Lin-Chen/ShareGPT4V" rel="nofollow" title="ShareGPT4V-PT">ShareGPT4V-PT</a>å’Œ<a href="https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat#prepare-training-datasets" title="InternVL-SFTä»">InternVL-SFTä»</a><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" rel="nofollow" title="meta-llama/Meta-Llama-3-8B-Instruct">meta-llama/Meta-Llama-3-8B-Instruct</a>å’Œ<a href="https://huggingface.co/openai/clip-vit-large-patch14-336" rel="nofollow" title="CLIP-ViT-Large-patch14-336">CLIP-ViT-Large-patch14-336</a>è¿›è¡Œå¾®è°ƒã€‚</li><li>ã€‚ã€‚ã€‚</li></ul> 
<p>è¿˜æœ‰å¾ˆå¤šæ–°çš„æ¨¡å‹å°±ä¸ä¸€ä¸€åˆ—ä¸¾äº†ï¼Œæƒ³è¦å°è¯•è¿™äº›æ¨¡å‹å¯ä»¥ç›´æ¥éƒ¨ç½²è¯•ç”¨ã€‚æœ¬æ–‡åˆ™æ¢è®¨å¦‚ä½•ä½¿ç”¨ Llama-Factory å¯¹ Llama3 è¿›è¡Œä¸­æ–‡å¾®è°ƒçš„å…·ä½“è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡ Ollama æœ¬åœ°éƒ¨ç½²ä¸­æ–‡å¾®è°ƒçš„ Llama3 æ¨¡å‹ï¼Œæ‰“é€ å±äºè‡ªå·±çš„ä¸ªæ€§åŒ–çš„ Llama3 LLM ã€‚</p> 
<h3>ä½¿ç”¨ Llama-Factory å¯¹ Llama3 è¿›è¡Œä¸­æ–‡å¾®è°ƒ</h3> 
<p>LLaMA-Factory æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œå®ƒæä¾›äº†ä¸€å¥—å…¨é¢çš„å·¥å…·å’Œè„šæœ¬ï¼Œç”¨äºå¾®è°ƒã€æœåŠ¡å’ŒåŸºå‡†æµ‹è¯• LLaMA æ¨¡å‹ã€‚LLaMAï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹è‡ªé€‚åº”ï¼‰æ˜¯ Meta AI å¼€å‘çš„ä¸€ç»„åŸºç¡€è¯­è¨€æ¨¡å‹ï¼Œåœ¨å„ç§è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚</p> 
<p>LLaMA-Factory å­˜å‚¨åº“æä¾›ä»¥ä¸‹å†…å®¹ï¼Œè®©æ‚¨è½»æ¾å¼€å§‹ä½¿ç”¨ LLaMA æ¨¡å‹ï¼š</p> 
<ul><li>æ•°æ®é¢„å¤„ç†å’Œæ ‡è®°åŒ–çš„è„šæœ¬</li><li>ç”¨äºå¾®è°ƒ LLaMA æ¨¡å‹çš„è®­ç»ƒæµç¨‹</li><li>ä½¿ç”¨ç»è¿‡è®­ç»ƒçš„æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æ¨ç†è„šæœ¬</li><li>è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„åŸºå‡†æµ‹è¯•å·¥å…·</li><li>ç”¨äºäº¤äº’å¼æµ‹è¯•çš„ Gradio Web UI</li></ul> 
<p>å…³äº Llama-Factory çš„å…·ä½“ä»‹ç»å¯ä»¥å‚è€ƒæœ¬ç«™çš„å¦å¤–ä¸€ç¯‡åšæ–‡ï¼š<a href="https://blog.yanghong.dev/llama-factory/" rel="nofollow" title="LLaMA-Factory ç®€ä»‹">LLaMA-Factory ç®€ä»‹</a>ã€‚</p> 
<h4>å®‰è£… Llama-Factory</h4> 
<p>é¦–å…ˆä» github æ‹‰å– Llama-Factoryï¼š</p> 
<pre><code>git clone https://github.com/hiyouga/LLaMA-Factory.git</code></pre> 
<p>ä¸ºäº†æ–¹ä¾¿ä»Šåçš„è°ƒè¯•å’Œéƒ¨ç½²ï¼Œæˆ‘é€‰æ‹©äº†ä½¿ç”¨ docker çš„æ–¹å¼æ¥è¿è¡Œ Llama-Factoryã€‚å®ƒæä¾›äº†ä¸€ä¸ªå‚è€ƒçš„ Dockerfileï¼š</p> 
<pre><code>FROM nvcr.io/nvidia/pytorch:24.01-py3

WORKDIR /app

COPY requirements.txt /app/
RUN pip install -r requirements.txt

COPY . /app/
RUN pip install -e .[deepspeed,metrics,bitsandbytes,qwen]

VOLUME [ "/root/.cache/huggingface/", "/app/data", "/app/output" ]
EXPOSE 7860

CMD [ "llamafactory-cli webui" ]</code></pre> 
<p>å¯ä»¥æ ¹æ®è‡ªå·±çš„å®é™…æƒ…å†µè¿›è¡Œä¿®æ”¹ã€‚æˆ‘å®¶é‡Œçš„ç½‘ç»œè‡ªå·±æ­å»ºäº† proxy serverï¼ˆä¸ºäº†è®¿é—® github, huggingface ç­‰ç«™ç‚¹ï¼‰ï¼Œæ‰€ä»¥æ›´æ”¹ Dockerfile å¦‚ä¸‹ã€‚è¯¥ Dockerfile ä» docker buildx å‘½ä»¤è¡Œè·å– http_proxy å’Œ https_proxy å˜é‡ï¼Œå¹¶è®¾ç½® docker buildx ç¯å¢ƒé‡Œçš„ç›¸åº”ç¯å¢ƒå˜é‡ï¼Œè¿™æ ·ç¼–è¯‘ docker é•œåƒçš„è¿‡ç¨‹ä¸­å°±èƒ½ä½¿ç”¨ä»£ç†æœåŠ¡å™¨äº†ã€‚</p> 
<pre><code>FROM nvcr.io/nvidia/pytorch:24.01-py3


# ä½¿ç”¨æ„å»ºå‚æ•°è®¾ç½®ç¯å¢ƒå˜é‡
ARG http_proxy
ARG https_proxy

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV HTTP_PROXY=$http_proxy
ENV HTTPS_PROXY=$https_proxy
ENV http_proxy=$http_proxy
ENV https_proxy=$https_proxy

WORKDIR /app

COPY requirements.txt /app/
RUN pip install -r requirements.txt

COPY . /app/
RUN pip install -e .[deepspeed,metrics,bitsandbytes,qwen]

# unsetç¯å¢ƒå˜é‡ã€‚Containerè¿è¡Œè¿‡ç¨‹ä¸­éœ€è¦ä»£ç†æœåŠ¡å™¨çš„è¯é€šè¿‡-e ä¼ å…¥å‚æ•°
ENV HTTP_PROXY=""
ENV HTTPS_PROXY=""
ENV http_proxy=""
ENV https_proxy=""

VOLUME [ "/root/.cache/huggingface/", "/app/data", "/app/output" ]
EXPOSE 7860

CMD [ "python", "src/train_web.py" ]
</code></pre> 
<p>è¿è¡Œå¦‚ä¸‹è„šæœ¬ç”Ÿæˆè‡ªå·±çš„ Llama-Factory docker é•œåƒï¼š</p> 
<pre><code>docker buildx build --build-arg http_proxy=http://proxy_ip:port --build-arg https_proxy=http://proxy_ip:port -t llama-factory:v0.00 .</code></pre> 
<p>Docker é•œåƒç¼–è¯‘æˆåŠŸåï¼Œè¿è¡Œ docker image list å°±å¯ä»¥çœ‹åˆ°ç¼–è¯‘å‡ºæ¥çš„ docker é•œåƒäº†ï¼š</p> 
<pre><code>docker image list
</code></pre> 
<p><img alt="" height="92" src="https://images2.imgbox.com/69/cb/QBZlsd04_o.png" width="1111"></p> 
<p>æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦è¿è¡Œ Llama-Factory containerã€‚Llama-Factory æä¾›äº†ä¸€ä¸ªå‚è€ƒçš„ docker-compose.yml æ–‡ä»¶æ¥è¿è¡Œ dockerï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ç…§è‡ªå·±çš„å®é™…æƒ…å†µè¿›è¡Œä¿®æ”¹ã€‚æˆ‘è¿™è¾¹ä¿®æ”¹çš„ç‰ˆæœ¬å¦‚ä¸‹ï¼Œä¿®æ”¹çš„éƒ¨åˆ†å‚è€ƒæ³¨é‡Šã€‚</p> 
<pre><code>version: '3.8'

services:
  llama-factory:
    #build:
    #  dockerfile: Dockerfile
    #  context: .
    image: llama-factory:v0.00 # ä¿®æ”¹ä¸ºç¼–è¯‘å‡ºæ¥çš„ docker image åç§°/ç‰ˆæœ¬
    container_name: llama_factory # container åç§°
    volumes:
      - ./volumes/huggingface:/root/.cache/huggingface/
      - ./volumes/data:/app/data
      - ./volumes/output:/app/output
      - /mnt/dev/myprojects/llm-webui-docker/models:/app/models # æ˜ å°„è‡ªå·±çš„modelsç›®å½•
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - GRADIO_SERVER_PORT=7864 # webuiè·‘åœ¨7864ç«¯å£ä¸Šï¼Œ7860è¢«comfyuiå ç”¨äº†
    ports:
      - "7864:7864" # webuiè·‘åœ¨7864ç«¯å£ä¸Šï¼Œ7860è¢«comfyuiå ç”¨äº†
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: "all"
            capabilities: [gpu]
    restart: unless-stopped
</code></pre> 
<p>è¿è¡Œå¦‚ä¸‹å‘½ä»¤å¯åŠ¨ Llama-Factory å®¹å™¨ï¼š</p> 
<pre><code>docker-compose up --detach</code></pre> 
<p>ç°åœ¨å¯ä»¥çœ‹åˆ° llama_factory container å·²ç»åœ¨æ­£å¸¸è¿è¡Œäº†ï¼š</p> 
<p><img alt="" height="184" src="https://images2.imgbox.com/dd/92/xZM1U15W_o.png" width="1024"></p> 
<p>è®¿é—® http://server_ip:7864 åˆ™å¯ä»¥çœ‹åˆ°å¦‚ä¸‹çš„ Llama-Factory WebUI ç•Œé¢ï¼š</p> 
<p><img alt="" height="827" src="https://images2.imgbox.com/39/3f/XFQ4slCR_o.png" width="1024"></p> 
<h4>ä½¿ç”¨ Llama-Factory ä¸º Llama3 è®­ç»ƒä¸­æ–‡ LoRA</h4> 
<h4>1. æ¨¡å‹åç§°ä¸è·¯å¾„</h4> 
<p>è¿›å…¥ Llama-Factory WebUI åï¼Œå…ˆé€‰æ‹© Model name ä¸º LLaMA-8Bï¼Œè¿™æ—¶å€™ Model path ä¼šè‡ªåŠ¨å˜æˆ meta-llama/Meta-Llama-3-8B å¹¶è‡ªåŠ¨ä» huggingface æ‹‰å– Meta-Llama-3-8B æ¨¡å‹ã€‚æˆ‘è¿™é‡Œå› ä¸ºå·²ç»ä¸‹è½½äº† Meta-Llama-3-8B å¹¶æ˜ å°„åˆ°äº† Llama-Factory container çš„ /app/models/llama3-hf/Meta-Llama-3-8B ç›®å½•ï¼Œæ‰€ä»¥æˆ‘è¿™é‡Œçš„ Model path ä¹Ÿè®¾ç½®ä¸ºè¯¥è·¯å¾„ã€‚</p> 
<p><img alt="" height="211" src="https://images2.imgbox.com/ac/93/TsKuu14n_o.png" width="1024"></p> 
<h4>2. è®¾ç½® Advanced configurations</h4> 
<p>ç‚¹å³è¾¹çš„ç®­å¤´å±•å¼€ Advance configurationsã€‚é‡åŒ–ä½æ•°ï¼ˆQuantization bitï¼‰å¯ä»¥é€‰æ‹© 4 ï¼Œå‡å°æ¨¡å‹çš„ä½“ç§¯å¹¶æé«˜é€Ÿåº¦ã€‚Prompt template åˆ™é€‰æ‹© llama3ã€‚å¦‚æœæœ‰å®‰è£… flashattn2 æˆ–è€… unsloth çš„è¯å¯ä»¥åœ¨ boost é‡Œé€‰æ‹©ï¼Œæˆ‘è¿™é‡Œæ²¡æœ‰å®‰è£…æ‰€ä»¥é€‰æ‹© noneã€‚</p> 
<p><img alt="" height="126" src="https://images2.imgbox.com/e7/94/ySK18bhO_o.png" width="1024"></p> 
<h4>3. è®¾ç½®è®­ç»ƒå‚æ•°</h4> 
<p>åœ¨ Train æ ‡ç­¾é¡µé‡Œè®¾ç½®è®­ç»ƒç›¸å…³çš„å‚æ•°ã€‚ä¸»è¦çš„å‚æ•°æœ‰ï¼š</p> 
<ul><li>Stageï¼šè®¾ç½®ä¸º Supervised Fine-Tuning</li><li>Data dirï¼šæˆ‘è¿™é‡Œè®¾ç½®ä¸º data ï¼Œå› ä¸º Llama-Factory é¡¹ç›®è‡ªèº«ä¹Ÿå¸¦äº†ä¸€äº›ä¸­æ–‡æ•°æ®é›†ï¼Œæˆ‘æ‰“ç®—ç›´æ¥ä½¿ç”¨ã€‚å¦‚æœä½ è‡ªå·±ä¸‹è½½äº†åˆ«çš„ä¸­æ–‡æ•°æ®é›†ï¼Œè¯·è®¾ç½®ç›¸åº”çš„æ•°æ®é›†æ‰€åœ¨çš„ç›®å½•åœ°å€</li><li>Datasetï¼šæˆ‘é€‰æ‹©äº† Llama-Factory é¡¹ç›®é‡Œè‡ªå¸¦çš„ alpaca_zhï¼Œalpaca_gpt4_zh å’Œ oaast_sft_zh æ•°æ®é›†</li><li>Learning rateï¼šå®‰è£…è‡ªå·±çš„éœ€è¦è®¾ç½®ã€‚æˆ‘é‡‡ç”¨äº†ç¼ºçœçš„ 5e-5</li><li>Epochsï¼šæŒ‰ç…§è‡ªå·±çš„éœ€è¦è®¾ç½®ã€‚æˆ‘é‡‡ç”¨äº†ç¼ºçœçš„ 3.0</li><li>Cutoff lengthï¼šæŒ‰ç…§è‡ªå·±çš„éœ€è¦è®¾ç½®ã€‚æ•°å­—è¶Šå¤§ï¼Œå¯¹ GPU å’Œæ˜¾å­˜çš„è¦æ±‚è¶Šé«˜ï¼›æ•°å­—è¶Šå°ï¼Œåˆ™å¯èƒ½å¯¹é•¿å¥çš„è¯­ä¹‰ç†è§£ä¸å¤Ÿå……åˆ†ã€‚æˆ‘è¿™é‡Œé€‰æ‹©ç¼ºçœçš„ 1024</li><li>Batch sizeï¼šæŒ‰ç…§è‡ªå·±çš„éœ€è¦ç”šè‡³ã€‚æ•°å­—è¶Šå¤§ï¼Œå¯¹ GPU å’Œæ˜¾å­˜çš„è¦æ±‚è¶Šé«˜ã€‚æˆ‘è¿™é‡Œé€‰æ‹©ç¼ºçœçš„ 2</li><li>Output dir / Config pathï¼šæŒ‰ç…§è‡ªå·±çš„éœ€è¦è®¾ç½®</li></ul> 
<p><img alt="" height="701" src="https://images2.imgbox.com/f7/d9/O9LugHt7_o.png" width="1024"></p> 
<p>è®¾ç½®å®Œæˆåï¼Œå¯ä»¥ç‚¹å‡» Preview dataset æ¥æŸ¥çœ‹ä¸€ä¸‹æ•°æ®é›†å†…å®¹ã€‚</p> 
<p><img alt="" height="551" src="https://images2.imgbox.com/cd/1f/sHXeEkzI_o.png" width="1024"></p> 
<p>ç‚¹å‡» Preview command å¯ä»¥æŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹çš„å‘½ä»¤è¡Œå‚æ•°ã€‚å¦‚æœä¸å¸Œæœ›ä½¿ç”¨ WebUI è¿›è¡Œè®­ç»ƒï¼Œåˆ™å¯ä»¥ç›´æ¥æ‰§è¡Œå‘½ä»¤è¡Œï¼Œè¿™æ ·ä¹Ÿæœ‰åŠ©äºè¿›ä¸€æ­¥ç¼–ç¨‹å’Œè‡ªåŠ¨åŒ–ï¼š</p> 
<p><img alt="" height="623" src="https://images2.imgbox.com/c3/19/aRFGldTy_o.png" width="1200"></p> 
<p>ç‚¹å‡» Save arguments åˆ™å°†ç›®å‰çš„è®­ç»ƒè®¾ç½®ä¿å­˜åˆ°æŒ‡å®šçš„ json æ–‡ä»¶ã€‚ç‚¹å‡» Load arguments åˆ™å¯ä»¥åŠ è½½ä»¥å‰ä¿å­˜å¥½çš„è®­ç»ƒè®¾ç½®ã€‚</p> 
<h4>4. å¼€å§‹è®­ç»ƒ</h4> 
<p>å‚æ•°è®¾ç½®å¥½åï¼Œå°±å¯ä»¥ç‚¹å‡» Start å¼€å§‹è®­ç»ƒã€‚</p> 
<p>Llama-Factory è®­ç»ƒè„šæœ¬å¼€å§‹è§£ææ•°æ®ã€‚</p> 
<p><img alt="" height="347" src="https://images2.imgbox.com/3a/b0/v6L6xeGQ_o.png" width="1024"></p> 
<p>æ•´ä¸ªè®­ç»ƒçš„è¿‡ç¨‹é¢„è®¡ 8 ä¸ªå°æ—¶ä¸åˆ°ä¸€ç‚¹ã€‚</p> 
<p><img alt="" height="797" src="https://images2.imgbox.com/07/be/P4hqtwTb_o.png" width="1024"></p> 
<p>è®­ç»ƒå®Œæˆåï¼Œåœ¨ /app/output/llama3_cn_train_2024-04-27-16-32-46 ç›®å½•ä¸‹å¯ä»¥çœ‹åˆ°å¦‚ä¸‹çš„æ–‡ä»¶ç›®å½•ç»“æ„ï¼š</p> 
<p><img alt="" height="240" src="https://images2.imgbox.com/bc/87/nzSmG9Gm_o.png" width="1024"></p> 
<p>è¿™äº›å°±æ˜¯ Llama-Factory è®­ç»ƒå‡ºæ¥çš„ LoRAã€‚å¯ä»¥åœ¨è‡ªåŠ¨ç”Ÿæˆçš„ Readme.md æ–‡ä»¶æŸ¥çœ‹ LoRA çš„ä¿¡æ¯ï¼š</p> 
<pre><code>license: other
library_name: peft
tags:
- llama-factory
- lora
- generated_from_trainer
base_model: /app/models/llama3-hf/Meta-Llama-3-8B
model-index:
- name: llama3_cn_train_2024-04-27-16-32-46

ã€‚ã€‚ã€‚åé¢çš„å†…å®¹çœç•¥</code></pre> 
<h3>åœ¨ Ollama ä¸­æ‰“é€ è‡ªå·±çš„ä¸­æ–‡ç‰ˆ Llama3</h3> 
<p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¦åœ¨ Ollama ä¸­è¿è¡Œ llama3 å’Œæˆ‘ä»¬è®­ç»ƒå‡ºæ¥çš„ LoRAï¼Œæ‰“é€ å±äºè‡ªå·±çš„ä¸­æ–‡ç‰ˆ Llama3ã€‚</p> 
<p>Ollama æ˜¯ä¸€ä¸ªå¼€æºçš„å¤§æ¨¡å‹ç®¡ç†å·¥å…·ï¼Œå®ƒæä¾›äº†ä¸°å¯Œçš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ¨¡å‹çš„è®­ç»ƒã€éƒ¨ç½²ã€ç›‘æ§ç­‰ã€‚ é€šè¿‡Ollamaï¼Œä½ å¯ä»¥è½»æ¾åœ°ç®¡ç†æœ¬åœ°çš„å¤§æ¨¡å‹ï¼Œæé«˜æ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦å’Œéƒ¨ç½²æ•ˆç‡ã€‚ æ­¤å¤–ï¼ŒOllamaè¿˜æ”¯æŒå¤šç§æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼Œä½¿å¾—ä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¡†æ¶è¿›è¡Œæ¨¡å‹çš„è®­ç»ƒã€‚</p> 
<h4>è¿è¡Œ Ollama docker</h4> 
<p>ä» github æ‹‰å– ollama ä»£ç ï¼š</p> 
<pre><code>git clone https://github.com/ollama/ollama.git</code></pre> 
<p>Ollama githubé¡¹ç›®æä¾›äº†å‚è€ƒçš„ Dockerfileï¼Œå¯ä»¥ç¼–è¯‘è‡ªå·±çš„ ollama é•œåƒå¹¶è¿è¡Œã€‚ä¹Ÿå¯ä»¥ç›´æ¥è¿è¡Œ ollama å®˜æ–¹å‘å¸ƒçš„ docker é•œåƒï¼š</p> 
<pre><code>docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama</code></pre> 
<h4>è½¬æ¢ LoRA æ ¼å¼</h4> 
<p>æŒ‰ç…§ Ollama modelfile ADAPTER çš„è¯´æ˜ï¼ŒOllama æ”¯æŒ ggml æ ¼å¼çš„ LoRAï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦æŠŠåˆšæ‰ç”Ÿæˆçš„ LoRA è½¬æ¢æˆä¸º ggml æ ¼å¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨åˆ° Llama.cpp çš„æŸäº›è„šæœ¬ã€‚æœ‰å…³ Llama.cpp å¼€æºé¡¹ç›®çš„ä»‹ç»è¯·å‚è€ƒæœ¬ç«™å¦å¤–ä¸€ç¯‡åšæ–‡ï¼š<a href="https://blog.yanghong.dev/llama-cpp-practice/" rel="nofollow" title="Llama.cpp ä¸Šæ‰‹å®æˆ˜æŒ‡å—">Llama.cpp ä¸Šæ‰‹å®æˆ˜æŒ‡å—</a>Â ã€‚</p> 
<blockquote> 
 <p>ADAPTER</p> TheÂ  
 <code>ADAPTER</code>Â instruction is an optional instruction that specifies any LoRA adapter that should apply to the base model. The value of this instruction should be an absolute path or a path relative to the Modelfile and the file must be in a GGML file format. The adapter should be tuned from the base model otherwise the behaviour is undefined. 
</blockquote> 
<p>åœ¨ llama.cpp é¡¹ç›®ä¸­ï¼Œæœ‰å¦‚ä¸‹å‡ ä¸ªç”¨äºè½¬æ¢æ ¼å¼çš„ python è„šæœ¬ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ conver-lora-to-ggml.py è„šæœ¬æ¥è½¬æ¢æ ¼å¼ã€‚</p> 
<p><img alt="" height="111" src="https://images2.imgbox.com/72/62/hb2Vl0YD_o.png" width="1085"></p> 
<p>è¿è¡Œå¦‚ä¸‹çš„å‘½ä»¤ï¼ˆå…¶ä¸­ /app/output/llama3_cn_train_2024-04-27-16-32-46 æ˜¯ Llama-Factory ç”Ÿæˆ LoRA çš„è·¯å¾„ï¼‰ï¼š</p> 
<pre><code>./conver-lora-to-ggml.py /app/output/llama3_cn_train_2024-04-27-16-32-46 llama</code></pre> 
<p>è¿è¡Œå®Œè¿™ä¸ªå‘½ä»¤åï¼Œå°†åœ¨ /app/output/llama3_cn_train_2024-04-27-16-32-46 ä¸‹ç”Ÿæˆ ggml-adapter-model.bin æ–‡ä»¶ã€‚è¿™ä¸ªæ–‡ä»¶å°±æ˜¯ Ollama éœ€è¦çš„ ggml æ ¼å¼çš„ LoRA æ–‡ä»¶ã€‚</p> 
<h4>åœ¨ ollama ä¸­åˆ›å»ºè‡ªå·±çš„ llama3 ä¸­æ–‡æ¨¡å‹</h4> 
<p>æˆ‘ä»¬ä½¿ç”¨ ollama çš„ modelfile æ¥åˆ›å»ºè‡ªå·±çš„ llama3 ä¸­æ–‡æ¨¡å‹ã€‚æˆ‘è‡ªå·±ä½¿ç”¨çš„å‚è€ƒ llama3.modelfile å†…å®¹å¦‚ä¸‹ï¼š</p> 
<pre><code># set the base model
FROM llama3:8b

# set custom parameter values
PARAMETER temperature 1
PARAMETER num_keep 24
PARAMETER stop &lt;|start_header_id|&gt;
PARAMETER stop &lt;|end_header_id|&gt;
PARAMETER stop &lt;|eot_id|&gt;
PARAMETER stop &lt;|reserved_special_token

# set the model template
TEMPLATE """
{<!-- -->{ if .System }}&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;
{<!-- -->{ .System }}&lt;|eot_id|&gt;{<!-- -->{ end }}{<!-- -->{ if .Prompt }}&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;
{<!-- -->{ .Prompt }}&lt;|eot_id|&gt;{<!-- -->{ end }}&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;
{<!-- -->{ .Response }}&lt;|eot_id|&gt;
"""

# set the system message
SYSTEM You are llama3 from Meta, customized and hosted @ HY's Blog (https://blog.yanghong.dev).

# set Chinese lora support
ADAPTER /root/.ollama/models/lora/ggml-adapter-model.bin
</code></pre> 
<p>é¦–å…ˆï¼Œé€šè¿‡å‘½ä»¤è¡Œè¿›å…¥ ollama containerï¼š</p> 
<pre><code>docker exec -it ollama /bin/bash</code></pre> 
<p>ç„¶åä½¿ç”¨æˆ‘ä»¬åˆšæ‰ç”Ÿæˆçš„ llama3.modelfile æ¥åˆ›å»ºè‡ªå·±çš„ä¸­æ–‡ç‰ˆ Llama3ï¼Œå‘½åä¸º llama3:hyã€‚</p> 
<pre><code>ollama create llama3:hy -f llama3.modelfile</code></pre> 
<p>åˆ›å»ºæˆåŠŸåï¼Œè¿è¡Œå¦‚ä¸‹çš„å‘½ä»¤ï¼Œåˆ™å¯ä»¥çœ‹åˆ°åˆšæ‰åˆ›å»ºçš„ llama3:hy æ¨¡å‹ã€‚</p> 
<pre><code>ollama list</code></pre> 
<p><img alt="" height="231" src="https://images2.imgbox.com/22/21/6nYS7VYa_o.png" width="1017"></p> 
<p>æ‰§è¡Œå¦‚ä¸‹çš„å‘½ä»¤æ¥è¿è¡Œ llama3:hy æ¨¡å‹ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°±å¯ä»¥æ„‰å¿«åœ°è·Ÿå®ƒè¿›è¡Œä¸­æ–‡å¯¹è¯äº†ã€‚</p> 
<pre><code>ollama run llama3:hy</code></pre> 
<p>æˆ‘ä»¬åŒæ ·é—®å®ƒæœ¬æ–‡å¼€å¤´çš„é‚£ä¸¤ä¸ªé—®é¢˜ï¼šâ€ä½ æ˜¯è°ï¼Ÿâ€œï¼Œâ€ä»‹ç»ä¸€ä¸‹ä¸­å›½çš„é¦–éƒ½åŒ—äº¬â€œï¼Œçœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ã€‚</p> 
<p>ä½ æ˜¯è°ï¼Ÿ</p> 
<pre><code>&gt;&gt;&gt; ä½ æ˜¯è°ï¼Ÿ
æˆ‘æ˜¯ä¸€å€‹ AI å‹Chatbotï¼Œæ¥è‡ª Metaï¼Œç›®å‰è¢«Customized å’Œ Hosted åœ¨ HY çš„åšå®¢ï¼ˆhttps://blog.yanghong.devï¼‰ã€‚ç®€å•åœ°è¯´ï¼Œæˆ‘æ˜¯ä¸€å€‹èƒ½èˆ‡äººäº’å‹•ã€å›ç­”å•é¡Œå’Œç”Ÿæˆæ–‡æœ¬çš„ AI æœºå™¨äººã€‚</code></pre> 
<p>ä»‹ç»ä¸€ä¸‹ä¸­ä¸Šæµ·çš„æ—…æ¸¸æ™¯ç‚¹ï¼š</p> 
<pre><code>&gt;&gt;&gt; ä»‹ç»ä¸€ä¸‹ä¸Šæµ·çš„æ—…æ¸¸æ™¯ç‚¹
ä¸Šæµ·æ˜¯ä¸€åº§æµè¡Œçš„æ—…æ¸¸ç›®çš„åœ°ï¼Œæ‹¥æœ‰æ‚ ä¹…å†å²ã€å¤šæ ·æ–‡åŒ–å’Œç°ä»£å»ºç­‘ã€‚ä»¥ä¸‹æ˜¯ä¸Šæµ·ä¸€äº›æœ€å—æ¬¢è¿çš„æ—…æ¸¸æ™¯ç‚¹ï¼š

1. å¤–æ»¨è·¯ï¼ˆThe Bundï¼‰ï¼šé»„æµ¦æ±Ÿæ²¿å²¸çš„ä¸€å¸¦ï¼Œæä¾›åŸå¸‚å¤©é™…çº¿çš„å£®ä¸½æ™¯è±¡ã€‚æ­¤å¤–ï¼Œè¿™é‡Œä¹Ÿæœ‰ä¸€äº›å†å²å¤æ¥¼ã€é¤å…å’Œé…’å§ã€‚
2. ä¸Šæµ·å¡”ï¼ˆShanghai Towerï¼‰ï¼šè¿™æ˜¯ä¸–ç•Œæœ€é«˜å»ºç­‘ä¹‹ä¸€ï¼Œé«˜åº¦632ç±³ã€‚ä»124å±‚è§‚å…‰å°å¯ä»¥æ¬£èµåˆ°ç»•åŸå…¨æ™¯ã€‚
3. è±«å›­ï¼ˆYu Gardenï¼‰ï¼šæ˜ä»£çš„ä¸­å›½å¤å…¸èŠ±å›­ï¼Œknown for its beautiful architecture, lush greenery and peaceful atmosphere.
4. æ³•ç§ŸåŒºï¼ˆFrench Concessionï¼‰ï¼šä¸€å¸¦æ‹¥æœ‰æ³•å›½å½±å“çš„è¡—åŒºï¼Œfeaturing colonial-era buildings, trendy boutiqueså’Œhip restaurantsã€‚
5. æ–°å¤©åœ°ï¼ˆXintiandiï¼‰ï¼šå†å²åŒºåŸŸè¢«è½¬æ¢ä¸ºæ—¶å°šè´­ç‰©å’Œç¾é£Ÿè¡—åŒºï¼Œknown for its cobblestone streets, traditional Shikumen houseså’Œvibrant nightlife.
6. ä¸Šæµ·åšç‰©é¦†ï¼ˆShanghai Museumï¼‰ï¼šå±•ç¤ºåŸå¸‚å†å²ã€è‰ºæœ¯å’Œæ–‡åŒ–çš„ç»¼åˆåšç‰©é¦†ï¼Œæœ‰è¶…è¿‡10,000ä»¶å±•å“ã€‚
7. äººæ°‘å¹¿åœºï¼ˆPeople's Squareï¼‰ï¼šä¸€ä¸ªå¤§å‹å…¬å…±å¹¿åœºï¼Œsurrounded by iconic buildingsåŒ…æ‹¬ä¸Šæµ·å¤§å‰§é™¢ã€ä¸Šæµ·å¸‚æ”¿å…å’Œä¸­å›½é©å‘½åšç‰©é¦†ã€‚
8. ä¸œæ–¹æ˜ç å¡”ï¼ˆOriental Pearl Towerï¼‰ï¼šä¸€åº§ç”µè§†å¡”ï¼Œæä¾›åŸå¸‚å…¨æ™¯è§‚ä»263å±‚è§‚å…‰å°ã€‚
9. æœ±å®¶è§’æ°´ä¹¡ï¼ˆZhujiajiao Water Townï¼‰ï¼šä¸€ä¸ªå¤è€çš„æ°´é•‡ä½äºä¸Šæµ·å¤–å›´ï¼Œknown for its canals, bridgeså’Œtraditional architecture.
10. ä¸Šæµ·è¿ªå£«å°¼åº¦å‡ resortï¼ˆDisneyland Resort Shanghaiï¼‰ï¼šä¸€ä¸ªä¸–ç•Œä¸€æµçš„ä¸»é¢˜å…¬å›­ï¼Œfeaturing Disney, Pixar, Marvelå’ŒStar Wars attractions.

è¿™äº›åªæ˜¯ä¸Šæµ·æ—…æ¸¸æ™¯ç‚¹ä¹‹ä¸€ã€‚æ— è®ºä½ æ˜¯å¦æ„Ÿå…´è¶£å†å²ã€æ–‡åŒ–ã€ç¾é£Ÿæˆ–å¨±ä¹ï¼Œè¿™åº§åŸå¸‚éƒ½æœ‰è¶³å¤Ÿçš„é€‰æ‹©ï¼
</code></pre> 
<p></p> 
<p>å¯ä»¥çœ‹åˆ°ï¼Œå®ƒåŸºæœ¬ä¸Šèƒ½ä»¥ä¸­æ–‡å›ç­”é—®é¢˜ï¼Œæˆ‘ä»¬çš„ LoRA fine tuning è¿‡ç¨‹æˆåŠŸå®Œæˆäº†ã€‚</p> 
<h4>ä¸ºä¸­æ–‡ Llama3 æ·»åŠ  WebUI</h4> 
<p>Ollama æä¾›äº† REST API æ¥è·Ÿ LLM æ¨¡å‹è¿›è¡Œäº¤äº’ï¼Œæ¯”å¦‚æœ€å¸¸ç”¨çš„ Generateï¼ŒChat ç­‰æ–¹æ³•ã€‚</p> 
<p>Generate a response</p> 
<pre><code>curl http://localhost:11434/api/generate -d '{
  "model": "llama3",
  "prompt":"Why is the sky blue?"
}'
</code></pre> 
<p>Chat with a model</p> 
<pre><code>curl http://localhost:11434/api/chat -d '{
  "model": "llama3",
  "messages": [
    { "role": "user", "content": "why is the sky blue?" }
  ]
}'</code></pre> 
<p>å®Œæ•´çš„ REST API æ–‡æ¡£å¯å‚é˜…Â <a href="https://github.com/ollama/ollama/blob/main/docs/api.md" title="githubÂ ">githubÂ </a>ã€‚</p> 
<p>å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å‘ä¸€ä¸ªç®€å•çš„ gradio ç¨‹åºï¼Œé€šè¿‡ REST API è°ƒç”¨ llama3:hy æ¨¡å‹æ¥è¿›è¡Œä¸­æ–‡äº¤äº’ã€‚å‚è€ƒä»£ç ç‰‡æ®µå¦‚ä¸‹ï¼š</p> 
<pre><code>        response = requests.post('http://192.168.3.204:11434/api/generate',
            json={
                'model': model2endpoint[model_name],
                'prompt': prompt,
                #'context': context,
                'options': {
                    'top_k': top_k,
                    'temperature': top_p,
                    'top_p': temperature
                }
            },
            stream=True
        )

        yield "", history, user_message, ""
        output = ""

        # Check if the request was successful
        response.raise_for_status()

        # Initialize the output and history variables
        output = ""

        # Iterate over the streamed response lines
        for idx, line in enumerate(response.iter_lines()):
            if line:
                # Parse the line as JSON
                data = json.loads(line)
                token = data.get('response', '')  # Assuming 'response' contains the text
                # Check if the token is a special token
                if data.get('special', False):
                    continue

                # Append the token to the output
                output += token

                # Update history and chat based on the index
                if idx == 0:
                    history.append(output.strip())  # Append initial output
                else:
                    history[-1] = output.strip()  # Update the last history entry

                # Convert history to chat format
                chat = [
                    (history[i], history[i + 1]) if i + 1 &lt; len(history) else (history[i], "")
                    for i in range(0, len(history), 2)
                ]

            # Yield the current chat, history, and user message updates
            yield chat, history, user_message, ""
</code></pre> 
<p><img alt="" height="802" src="https://images2.imgbox.com/de/9c/GUGBw4p9_o.png" width="1024"></p> 
<p>è¿™æ ·å°±å¯ä»¥é€šè¿‡ä¸Šé¢çš„ WebUI ç•Œé¢è®©å®ƒè¿›è¡Œä¸­æ–‡å¯¹è¯ï¼Œé—®é—®é¢˜ï¼Œå¸®æˆ‘å†™ä»£ç äº†ã€‚åŒæ—¶ä¹Ÿæ•´åˆäº†å…¶å®ƒçš„ä¸€äº› coding LLM åœ¨ä¸€èµ·ï¼Œç¢°åˆ°ä¸ä¼šå†™çš„ä»£ç ï¼Œå°±è®©å®ƒä»¬åœ¨ä¸€èµ·æ¯”æ¯”æ­¦ã€‚</p> 
<p>ä»¥ä¸‹è§†é¢‘æ˜¯ fine tune çš„ä¸­æ–‡ llama3 å®é™…ä½¿ç”¨æ¼”ç¤ºã€‚</p> 
<div class="csdn-video-box"> 
 <iframe id="j9aeF6Qe-1714901643708" frameborder="0" src="https://live.csdn.net/v/embed/384464" allowfullscreen="true" data-mediaembed="csdn"></iframe> 
 <p>Llama3ä¸­æ–‡å¾®è°ƒæœ¬åœ°éƒ¨ç½²</p> 
</div> 
<p></p> 
<p>å¦‚æœæ‚¨å–œæ¬¢æœ¬æ–‡çš„å†…å®¹ï¼Œæ¬¢è¿æ‰«æä¸‹é¢çš„äºŒç»´ç è®¿é—®ä½œè€…çš„åšå®¢ï¼š<a href="https://blog.yanghong.dev/" rel="nofollow" title="HY's Blog">HY's Blog</a></p> 
<p><img alt="" height="260" src="https://images2.imgbox.com/1a/be/lks2Vm05_o.png" width="260"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f0ce4ff3aa92ff50b1c74679cd84e928/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">å®‰å“rootè¯¦è§£ï¼ˆsupersuï¼ŒmagiskåŸç†è¯´æ˜ï¼‰</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/faaca2dbd733677b3f16d328a74b4dba/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">kafkaæ—¥å¿—å­˜å‚¨</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 ç¼–ç¨‹å¤§å’–.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>