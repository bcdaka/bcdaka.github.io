<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Llama-Factory &#43; Ollama 打造属于自己的中文版 Llama3 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/8667871f112a677f64ac59c8ccb5474f/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Llama-Factory &#43; Ollama 打造属于自己的中文版 Llama3">
  <meta property="og:description" content="Meta 推出 Llama3 也有一小段时间了。Llama3 包含 8B 和 70B 两种参数规模，涵盖预训练和指令调优的变体。Llama 3 支持多种商业和研究用途，并已在多个行业标准测试中展示了其卓越的性能（关于Llama3的具体介绍可以参考本站另外一篇博文：尝鲜 Meta Llama 3）。和以往的原始 Llama 模型一样，Llama 3 对中文的支持效果欠佳，经常会出现你用中文提问，它用英文或中文&#43;英文回复的现象。
例如当我问它：”你是谁？“，会得到如下的回答：
&gt;&gt;&gt; 你是谁？ I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I&#39;m not a human, but a computer program designed to simulate conversation and answer questions to the best of my ability based on my training data. I can generate text on a wide range of topics, from science and history to entertainment and culture.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-06T17:20:41+08:00">
    <meta property="article:modified_time" content="2024-05-06T17:20:41+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Llama-Factory &#43; Ollama 打造属于自己的中文版 Llama3</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><img alt="" height="720" src="https://images2.imgbox.com/e7/be/mMXdBmdm_o.png" width="1200"></p> 
<p>Meta 推出 Llama3 也有一小段时间了。Llama3 包含 8B 和 70B 两种参数规模，涵盖预训练和指令调优的变体。Llama 3 支持多种商业和研究用途，并已在多个行业标准测试中展示了其卓越的性能（关于Llama3的具体介绍可以参考本站另外一篇博文：<a href="https://blog.yanghong.dev/meta-llama3-release/" rel="nofollow" title="尝鲜 Meta Llama 3">尝鲜 Meta Llama 3</a>）。和以往的原始 Llama 模型一样，Llama 3 对中文的支持效果欠佳，经常会出现你用中文提问，它用英文或中文+英文回复的现象。</p> 
<p>例如当我问它：”你是谁？“，会得到如下的回答：</p> 
<pre><code>&gt;&gt;&gt; 你是谁？
I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm not a human, but a computer program designed to simulate conversation and answer questions to the best of my ability based on my training data. I can generate text on a wide range of topics, from science and history to entertainment and culture.</code></pre> 
<p></p> 
<p>然后让它”介绍一下上海的旅游景点“，则会得到如下的回答：</p> 
<pre><code class="hljs">&gt;&gt;&gt; 介绍一下上海的旅游景点
Shanghai, a city with a rich history and culture, has numerous tourist attractions that showcase its modernity, traditional charm, and natural beauty. Here are some of the most popular tourist spots in Shanghai:

**Landmarks:**

1. **Oriental Pearl Tower**: A iconic TV tower with panoramic views of the city.
2. **Shanghai Tower**: The second-tallest building in the world, offering stunning views from its observation deck.
3. **Jin Mao Tower**: A 421-meter-tall skyscraper and home to the luxurious Mandarin Oriental Hotel.
4. **The Bund**: A famous waterfront promenade lined with colonial-era buildings and stunning views of the Huangpu River.

**Historical Sites:**

1. **Yu Garden**: A classic Chinese garden with beautiful landscaping, pavilions, and koi ponds.
2. **Shanghai Museum**: A museum showcasing Chinese art, history, and culture.
3. **Xintiandi**: A charming neighborhood with traditional Shikumen architecture and modern boutiques.
4. **French Concession**: A historic neighborhood with colonial-era buildings, cafes, and restaurants.

**Nature Reserves:**

1. **Changfeng Park**: A large public park with gardens, lakes, and walking trails.
2. **Fuxing Island**: A tranquil oasis in the Huangpu River with beautiful landscaping and walking paths.
3. **Shanghai Botanical Garden**: A peaceful garden featuring various plant species and a peaceful atmosphere.

**Other Attractions:**

1. **Disney Resort Shanghai**: An amusement park with Disney-themed rides, shows, and attractions.
2. **Shanghai Grand Opera House**: A stunning opera house with performances ranging from music to dance.
3. **Shanghai Zoo**: A popular zoo with various animal species and a panda enclosure.
4. **The Shanghai Tower's Observation Deck**: For panoramic views of the city.

These are just a few of the many tourist attractions in Shanghai. Depending on your interests, there are plenty more hidden gems to explore! 😊
</code></pre> 
<p>回答得还是很简洁而全面的。缺点就是全是英文答案，对于英文不太好的人有点吃力。对 Llama3 进行微调来支持中文是一项必须的工作。</p> 
<h3>Llama3 中文模型的现状</h3> 
<p>实际上，当 Llama3 推出以后，很多开发者在中文支持方面做了大量的工作，一大批微调的中文模型如雨后春笋般纷纷涌现。目前比较知名的中文模型有：</p> 
<ul><li>Unichat-llama3-Chinese（<a href="https://www.modelscope.cn/models/UnicomAI/Unichat-llama3-Chinese/summary" rel="nofollow" title="地址">地址</a>）：中国联通AI创新中心发布业界第一个llama3中文指令微调模型，全参数微调(非lora)，以<strong>Meta Llama 3</strong>为基础,增加中文数据进行训练,实现llama3模型高质量中文问答，模型上下文保持原生长度8K，支持长度64K版本将于后续发布。</li><li>OpenBuddy – Open Multilingual Chatbot（<a href="https://huggingface.co/OpenBuddy/openbuddy-llama3-8b-v21.1-8k" rel="nofollow" title="地址">地址</a>）：OpenBuddy是一个面向全球用户的强大的开放式多语言聊天机器人模型，强调对话式AI以及对英语、中文和其他语言的无缝多语言支持。</li><li>Llama3-Chinese（<a href="https://github.com/seanzhang-zhichen/llama3-chinese" title="地址">地址</a>）：在500k高质量中文多轮SFT数据、100k英语多轮SFT数据和2k单轮自我认知数据上训练的大型模型，采用基于<strong>Meta的</strong>DORA和LORA+的训练方法。以<strong>Llama-3-8B</strong>为基础。</li><li>Llama3-8B-Chinese-Chat（<a href="https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat" rel="nofollow" title="地址">地址</a>）：是一个针对中文和英文用户的指令调整语言模型，具有角色扮演和工具使用等各种能力，建立在 Meta-Llama-3-8B-Instruct 模型的基础上。</li><li>Llama-3-8B-Instruct-Chinese-chat（<a href="https://github.com/Rookie1019/Llama-3-8B-Instruct-Chinese" title="地址">地址</a>）：使用 <a href="https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M" rel="nofollow" title="firefly-train-1.1M">firefly-train-1.1M</a>，<a href="https://huggingface.co/datasets/YeungNLP/moss-003-sft-data" rel="nofollow" title="moss-003-sft-data">moss-003-sft-data</a>，<a href="https://huggingface.co/datasets/YeungNLP/school_math_0.25M" rel="nofollow" title="school_math_0.25M">school_math_0.25M</a>，<a href="https://huggingface.co/datasets/LooksJuicy/ruozhiba" rel="nofollow" title="ruozhiba">ruozhiba</a> 等数据集微调的模型，基于Llama-3-8B-Instruct。</li><li>Bunny-Llama-3-8B-V（<a href="https://wisemodel.cn/models/BAAI/Bunny-Llama-3-8B-V" rel="nofollow" title="地址">地址</a>）：Bunny 是一系列轻量但功能强大的多模式模型。它提供多种即插即用视觉编码器，如 EVA-CLIP、SigLIP 和语言主干，包括 Llama-3-8B、Phi-1.5、StableLM-2、Qwen1.5、MiniCPM 和 Phi-2。为了弥补模型大小的减小，通过从更广泛的数据源中进行精选来构建信息更丰富的训练数据。</li><li>llava-llama-3-8b-v1_1（<a href="https://huggingface.co/xtuner/llava-llama-3-8b-v1_1" rel="nofollow" title="地址">地址</a>）：llava-llama-3-8b-v1_1 是一个 LLaVA 模型，由<a href="https://github.com/InternLM/xtuner" title="XTuner">XTuner</a>使用<a href="https://huggingface.co/datasets/Lin-Chen/ShareGPT4V" rel="nofollow" title="ShareGPT4V-PT">ShareGPT4V-PT</a>和<a href="https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat#prepare-training-datasets" title="InternVL-SFT从">InternVL-SFT从</a><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" rel="nofollow" title="meta-llama/Meta-Llama-3-8B-Instruct">meta-llama/Meta-Llama-3-8B-Instruct</a>和<a href="https://huggingface.co/openai/clip-vit-large-patch14-336" rel="nofollow" title="CLIP-ViT-Large-patch14-336">CLIP-ViT-Large-patch14-336</a>进行微调。</li><li>。。。</li></ul> 
<p>还有很多新的模型就不一一列举了，想要尝试这些模型可以直接部署试用。本文则探讨如何使用 Llama-Factory 对 Llama3 进行中文微调的具体过程，并通过 Ollama 本地部署中文微调的 Llama3 模型，打造属于自己的个性化的 Llama3 LLM 。</p> 
<h3>使用 Llama-Factory 对 Llama3 进行中文微调</h3> 
<p>LLaMA-Factory 是一个开源项目，它提供了一套全面的工具和脚本，用于微调、服务和基准测试 LLaMA 模型。LLaMA（大型语言模型自适应）是 Meta AI 开发的一组基础语言模型，在各种自然语言任务上表现出色。</p> 
<p>LLaMA-Factory 存储库提供以下内容，让您轻松开始使用 LLaMA 模型：</p> 
<ul><li>数据预处理和标记化的脚本</li><li>用于微调 LLaMA 模型的训练流程</li><li>使用经过训练的模型生成文本的推理脚本</li><li>评估模型性能的基准测试工具</li><li>用于交互式测试的 Gradio Web UI</li></ul> 
<p>关于 Llama-Factory 的具体介绍可以参考本站的另外一篇博文：<a href="https://blog.yanghong.dev/llama-factory/" rel="nofollow" title="LLaMA-Factory 简介">LLaMA-Factory 简介</a>。</p> 
<h4>安装 Llama-Factory</h4> 
<p>首先从 github 拉取 Llama-Factory：</p> 
<pre><code>git clone https://github.com/hiyouga/LLaMA-Factory.git</code></pre> 
<p>为了方便今后的调试和部署，我选择了使用 docker 的方式来运行 Llama-Factory。它提供了一个参考的 Dockerfile：</p> 
<pre><code>FROM nvcr.io/nvidia/pytorch:24.01-py3

WORKDIR /app

COPY requirements.txt /app/
RUN pip install -r requirements.txt

COPY . /app/
RUN pip install -e .[deepspeed,metrics,bitsandbytes,qwen]

VOLUME [ "/root/.cache/huggingface/", "/app/data", "/app/output" ]
EXPOSE 7860

CMD [ "llamafactory-cli webui" ]</code></pre> 
<p>可以根据自己的实际情况进行修改。我家里的网络自己搭建了 proxy server（为了访问 github, huggingface 等站点），所以更改 Dockerfile 如下。该 Dockerfile 从 docker buildx 命令行获取 http_proxy 和 https_proxy 变量，并设置 docker buildx 环境里的相应环境变量，这样编译 docker 镜像的过程中就能使用代理服务器了。</p> 
<pre><code>FROM nvcr.io/nvidia/pytorch:24.01-py3


# 使用构建参数设置环境变量
ARG http_proxy
ARG https_proxy

# 设置环境变量
ENV HTTP_PROXY=$http_proxy
ENV HTTPS_PROXY=$https_proxy
ENV http_proxy=$http_proxy
ENV https_proxy=$https_proxy

WORKDIR /app

COPY requirements.txt /app/
RUN pip install -r requirements.txt

COPY . /app/
RUN pip install -e .[deepspeed,metrics,bitsandbytes,qwen]

# unset环境变量。Container运行过程中需要代理服务器的话通过-e 传入参数
ENV HTTP_PROXY=""
ENV HTTPS_PROXY=""
ENV http_proxy=""
ENV https_proxy=""

VOLUME [ "/root/.cache/huggingface/", "/app/data", "/app/output" ]
EXPOSE 7860

CMD [ "python", "src/train_web.py" ]
</code></pre> 
<p>运行如下脚本生成自己的 Llama-Factory docker 镜像：</p> 
<pre><code>docker buildx build --build-arg http_proxy=http://proxy_ip:port --build-arg https_proxy=http://proxy_ip:port -t llama-factory:v0.00 .</code></pre> 
<p>Docker 镜像编译成功后，运行 docker image list 就可以看到编译出来的 docker 镜像了：</p> 
<pre><code>docker image list
</code></pre> 
<p><img alt="" height="92" src="https://images2.imgbox.com/69/cb/QBZlsd04_o.png" width="1111"></p> 
<p>接下来我们需要运行 Llama-Factory container。Llama-Factory 提供了一个参考的 docker-compose.yml 文件来运行 docker，我们可以按照自己的实际情况进行修改。我这边修改的版本如下，修改的部分参考注释。</p> 
<pre><code>version: '3.8'

services:
  llama-factory:
    #build:
    #  dockerfile: Dockerfile
    #  context: .
    image: llama-factory:v0.00 # 修改为编译出来的 docker image 名称/版本
    container_name: llama_factory # container 名称
    volumes:
      - ./volumes/huggingface:/root/.cache/huggingface/
      - ./volumes/data:/app/data
      - ./volumes/output:/app/output
      - /mnt/dev/myprojects/llm-webui-docker/models:/app/models # 映射自己的models目录
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - GRADIO_SERVER_PORT=7864 # webui跑在7864端口上，7860被comfyui占用了
    ports:
      - "7864:7864" # webui跑在7864端口上，7860被comfyui占用了
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: "all"
            capabilities: [gpu]
    restart: unless-stopped
</code></pre> 
<p>运行如下命令启动 Llama-Factory 容器：</p> 
<pre><code>docker-compose up --detach</code></pre> 
<p>现在可以看到 llama_factory container 已经在正常运行了：</p> 
<p><img alt="" height="184" src="https://images2.imgbox.com/dd/92/xZM1U15W_o.png" width="1024"></p> 
<p>访问 http://server_ip:7864 则可以看到如下的 Llama-Factory WebUI 界面：</p> 
<p><img alt="" height="827" src="https://images2.imgbox.com/39/3f/XFQ4slCR_o.png" width="1024"></p> 
<h4>使用 Llama-Factory 为 Llama3 训练中文 LoRA</h4> 
<h4>1. 模型名称与路径</h4> 
<p>进入 Llama-Factory WebUI 后，先选择 Model name 为 LLaMA-8B，这时候 Model path 会自动变成 meta-llama/Meta-Llama-3-8B 并自动从 huggingface 拉取 Meta-Llama-3-8B 模型。我这里因为已经下载了 Meta-Llama-3-8B 并映射到了 Llama-Factory container 的 /app/models/llama3-hf/Meta-Llama-3-8B 目录，所以我这里的 Model path 也设置为该路径。</p> 
<p><img alt="" height="211" src="https://images2.imgbox.com/ac/93/TsKuu14n_o.png" width="1024"></p> 
<h4>2. 设置 Advanced configurations</h4> 
<p>点右边的箭头展开 Advance configurations。量化位数（Quantization bit）可以选择 4 ，减小模型的体积并提高速度。Prompt template 则选择 llama3。如果有安装 flashattn2 或者 unsloth 的话可以在 boost 里选择，我这里没有安装所以选择 none。</p> 
<p><img alt="" height="126" src="https://images2.imgbox.com/e7/94/ySK18bhO_o.png" width="1024"></p> 
<h4>3. 设置训练参数</h4> 
<p>在 Train 标签页里设置训练相关的参数。主要的参数有：</p> 
<ul><li>Stage：设置为 Supervised Fine-Tuning</li><li>Data dir：我这里设置为 data ，因为 Llama-Factory 项目自身也带了一些中文数据集，我打算直接使用。如果你自己下载了别的中文数据集，请设置相应的数据集所在的目录地址</li><li>Dataset：我选择了 Llama-Factory 项目里自带的 alpaca_zh，alpaca_gpt4_zh 和 oaast_sft_zh 数据集</li><li>Learning rate：安装自己的需要设置。我采用了缺省的 5e-5</li><li>Epochs：按照自己的需要设置。我采用了缺省的 3.0</li><li>Cutoff length：按照自己的需要设置。数字越大，对 GPU 和显存的要求越高；数字越小，则可能对长句的语义理解不够充分。我这里选择缺省的 1024</li><li>Batch size：按照自己的需要甚至。数字越大，对 GPU 和显存的要求越高。我这里选择缺省的 2</li><li>Output dir / Config path：按照自己的需要设置</li></ul> 
<p><img alt="" height="701" src="https://images2.imgbox.com/f7/d9/O9LugHt7_o.png" width="1024"></p> 
<p>设置完成后，可以点击 Preview dataset 来查看一下数据集内容。</p> 
<p><img alt="" height="551" src="https://images2.imgbox.com/cd/1f/sHXeEkzI_o.png" width="1024"></p> 
<p>点击 Preview command 可以查看训练过程的命令行参数。如果不希望使用 WebUI 进行训练，则可以直接执行命令行，这样也有助于进一步编程和自动化：</p> 
<p><img alt="" height="623" src="https://images2.imgbox.com/c3/19/aRFGldTy_o.png" width="1200"></p> 
<p>点击 Save arguments 则将目前的训练设置保存到指定的 json 文件。点击 Load arguments 则可以加载以前保存好的训练设置。</p> 
<h4>4. 开始训练</h4> 
<p>参数设置好后，就可以点击 Start 开始训练。</p> 
<p>Llama-Factory 训练脚本开始解析数据。</p> 
<p><img alt="" height="347" src="https://images2.imgbox.com/3a/b0/v6L6xeGQ_o.png" width="1024"></p> 
<p>整个训练的过程预计 8 个小时不到一点。</p> 
<p><img alt="" height="797" src="https://images2.imgbox.com/07/be/P4hqtwTb_o.png" width="1024"></p> 
<p>训练完成后，在 /app/output/llama3_cn_train_2024-04-27-16-32-46 目录下可以看到如下的文件目录结构：</p> 
<p><img alt="" height="240" src="https://images2.imgbox.com/bc/87/nzSmG9Gm_o.png" width="1024"></p> 
<p>这些就是 Llama-Factory 训练出来的 LoRA。可以在自动生成的 Readme.md 文件查看 LoRA 的信息：</p> 
<pre><code>license: other
library_name: peft
tags:
- llama-factory
- lora
- generated_from_trainer
base_model: /app/models/llama3-hf/Meta-Llama-3-8B
model-index:
- name: llama3_cn_train_2024-04-27-16-32-46

。。。后面的内容省略</code></pre> 
<h3>在 Ollama 中打造自己的中文版 Llama3</h3> 
<p>接下来，我们要在 Ollama 中运行 llama3 和我们训练出来的 LoRA，打造属于自己的中文版 Llama3。</p> 
<p>Ollama 是一个开源的大模型管理工具，它提供了丰富的功能，包括模型的训练、部署、监控等。 通过Ollama，你可以轻松地管理本地的大模型，提高模型的训练速度和部署效率。 此外，Ollama还支持多种机器学习框架，如TensorFlow、PyTorch等，使得你可以根据自己的需求选择合适的框架进行模型的训练。</p> 
<h4>运行 Ollama docker</h4> 
<p>从 github 拉取 ollama 代码：</p> 
<pre><code>git clone https://github.com/ollama/ollama.git</code></pre> 
<p>Ollama github项目提供了参考的 Dockerfile，可以编译自己的 ollama 镜像并运行。也可以直接运行 ollama 官方发布的 docker 镜像：</p> 
<pre><code>docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama</code></pre> 
<h4>转换 LoRA 格式</h4> 
<p>按照 Ollama modelfile ADAPTER 的说明，Ollama 支持 ggml 格式的 LoRA，所以我们需要把刚才生成的 LoRA 转换成为 ggml 格式。为此，我们需要使用到 Llama.cpp 的某些脚本。有关 Llama.cpp 开源项目的介绍请参考本站另外一篇博文：<a href="https://blog.yanghong.dev/llama-cpp-practice/" rel="nofollow" title="Llama.cpp 上手实战指南">Llama.cpp 上手实战指南</a> 。</p> 
<blockquote> 
 <p>ADAPTER</p> The  
 <code>ADAPTER</code> instruction is an optional instruction that specifies any LoRA adapter that should apply to the base model. The value of this instruction should be an absolute path or a path relative to the Modelfile and the file must be in a GGML file format. The adapter should be tuned from the base model otherwise the behaviour is undefined. 
</blockquote> 
<p>在 llama.cpp 项目中，有如下几个用于转换格式的 python 脚本。我们将使用 conver-lora-to-ggml.py 脚本来转换格式。</p> 
<p><img alt="" height="111" src="https://images2.imgbox.com/72/62/hb2Vl0YD_o.png" width="1085"></p> 
<p>运行如下的命令（其中 /app/output/llama3_cn_train_2024-04-27-16-32-46 是 Llama-Factory 生成 LoRA 的路径）：</p> 
<pre><code>./conver-lora-to-ggml.py /app/output/llama3_cn_train_2024-04-27-16-32-46 llama</code></pre> 
<p>运行完这个命令后，将在 /app/output/llama3_cn_train_2024-04-27-16-32-46 下生成 ggml-adapter-model.bin 文件。这个文件就是 Ollama 需要的 ggml 格式的 LoRA 文件。</p> 
<h4>在 ollama 中创建自己的 llama3 中文模型</h4> 
<p>我们使用 ollama 的 modelfile 来创建自己的 llama3 中文模型。我自己使用的参考 llama3.modelfile 内容如下：</p> 
<pre><code># set the base model
FROM llama3:8b

# set custom parameter values
PARAMETER temperature 1
PARAMETER num_keep 24
PARAMETER stop &lt;|start_header_id|&gt;
PARAMETER stop &lt;|end_header_id|&gt;
PARAMETER stop &lt;|eot_id|&gt;
PARAMETER stop &lt;|reserved_special_token

# set the model template
TEMPLATE """
{<!-- -->{ if .System }}&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;
{<!-- -->{ .System }}&lt;|eot_id|&gt;{<!-- -->{ end }}{<!-- -->{ if .Prompt }}&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;
{<!-- -->{ .Prompt }}&lt;|eot_id|&gt;{<!-- -->{ end }}&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;
{<!-- -->{ .Response }}&lt;|eot_id|&gt;
"""

# set the system message
SYSTEM You are llama3 from Meta, customized and hosted @ HY's Blog (https://blog.yanghong.dev).

# set Chinese lora support
ADAPTER /root/.ollama/models/lora/ggml-adapter-model.bin
</code></pre> 
<p>首先，通过命令行进入 ollama container：</p> 
<pre><code>docker exec -it ollama /bin/bash</code></pre> 
<p>然后使用我们刚才生成的 llama3.modelfile 来创建自己的中文版 Llama3，命名为 llama3:hy。</p> 
<pre><code>ollama create llama3:hy -f llama3.modelfile</code></pre> 
<p>创建成功后，运行如下的命令，则可以看到刚才创建的 llama3:hy 模型。</p> 
<pre><code>ollama list</code></pre> 
<p><img alt="" height="231" src="https://images2.imgbox.com/22/21/6nYS7VYa_o.png" width="1017"></p> 
<p>执行如下的命令来运行 llama3:hy 模型，接下来我们就可以愉快地跟它进行中文对话了。</p> 
<pre><code>ollama run llama3:hy</code></pre> 
<p>我们同样问它本文开头的那两个问题：”你是谁？“，”介绍一下中国的首都北京“，看看它的表现如何。</p> 
<p>你是谁？</p> 
<pre><code>&gt;&gt;&gt; 你是谁？
我是一個 AI 型Chatbot，来自 Meta，目前被Customized 和 Hosted 在 HY 的博客（https://blog.yanghong.dev）。简单地说，我是一個能與人互動、回答問題和生成文本的 AI 机器人。</code></pre> 
<p>介绍一下中上海的旅游景点：</p> 
<pre><code>&gt;&gt;&gt; 介绍一下上海的旅游景点
上海是一座流行的旅游目的地，拥有悠久历史、多样文化和现代建筑。以下是上海一些最受欢迎的旅游景点：

1. 外滨路（The Bund）：黄浦江沿岸的一带，提供城市天际线的壮丽景象。此外，这里也有一些历史古楼、餐厅和酒吧。
2. 上海塔（Shanghai Tower）：这是世界最高建筑之一，高度632米。从124层观光台可以欣赏到绕城全景。
3. 豫园（Yu Garden）：明代的中国古典花园，known for its beautiful architecture, lush greenery and peaceful atmosphere.
4. 法租区（French Concession）：一带拥有法国影响的街区，featuring colonial-era buildings, trendy boutiques和hip restaurants。
5. 新天地（Xintiandi）：历史区域被转换为时尚购物和美食街区，known for its cobblestone streets, traditional Shikumen houses和vibrant nightlife.
6. 上海博物馆（Shanghai Museum）：展示城市历史、艺术和文化的综合博物馆，有超过10,000件展品。
7. 人民广场（People's Square）：一个大型公共广场，surrounded by iconic buildings包括上海大剧院、上海市政厅和中国革命博物馆。
8. 东方明珠塔（Oriental Pearl Tower）：一座电视塔，提供城市全景观从263层观光台。
9. 朱家角水乡（Zhujiajiao Water Town）：一个古老的水镇位于上海外围，known for its canals, bridges和traditional architecture.
10. 上海迪士尼度假 resort（Disneyland Resort Shanghai）：一个世界一流的主题公园，featuring Disney, Pixar, Marvel和Star Wars attractions.

这些只是上海旅游景点之一。无论你是否感兴趣历史、文化、美食或娱乐，这座城市都有足够的选择！
</code></pre> 
<p></p> 
<p>可以看到，它基本上能以中文回答问题，我们的 LoRA fine tuning 过程成功完成了。</p> 
<h4>为中文 Llama3 添加 WebUI</h4> 
<p>Ollama 提供了 REST API 来跟 LLM 模型进行交互，比如最常用的 Generate，Chat 等方法。</p> 
<p>Generate a response</p> 
<pre><code>curl http://localhost:11434/api/generate -d '{
  "model": "llama3",
  "prompt":"Why is the sky blue?"
}'
</code></pre> 
<p>Chat with a model</p> 
<pre><code>curl http://localhost:11434/api/chat -d '{
  "model": "llama3",
  "messages": [
    { "role": "user", "content": "why is the sky blue?" }
  ]
}'</code></pre> 
<p>完整的 REST API 文档可参阅 <a href="https://github.com/ollama/ollama/blob/main/docs/api.md" title="github ">github </a>。</p> 
<p>因此，我们可以开发一个简单的 gradio 程序，通过 REST API 调用 llama3:hy 模型来进行中文交互。参考代码片段如下：</p> 
<pre><code>        response = requests.post('http://192.168.3.204:11434/api/generate',
            json={
                'model': model2endpoint[model_name],
                'prompt': prompt,
                #'context': context,
                'options': {
                    'top_k': top_k,
                    'temperature': top_p,
                    'top_p': temperature
                }
            },
            stream=True
        )

        yield "", history, user_message, ""
        output = ""

        # Check if the request was successful
        response.raise_for_status()

        # Initialize the output and history variables
        output = ""

        # Iterate over the streamed response lines
        for idx, line in enumerate(response.iter_lines()):
            if line:
                # Parse the line as JSON
                data = json.loads(line)
                token = data.get('response', '')  # Assuming 'response' contains the text
                # Check if the token is a special token
                if data.get('special', False):
                    continue

                # Append the token to the output
                output += token

                # Update history and chat based on the index
                if idx == 0:
                    history.append(output.strip())  # Append initial output
                else:
                    history[-1] = output.strip()  # Update the last history entry

                # Convert history to chat format
                chat = [
                    (history[i], history[i + 1]) if i + 1 &lt; len(history) else (history[i], "")
                    for i in range(0, len(history), 2)
                ]

            # Yield the current chat, history, and user message updates
            yield chat, history, user_message, ""
</code></pre> 
<p><img alt="" height="802" src="https://images2.imgbox.com/de/9c/GUGBw4p9_o.png" width="1024"></p> 
<p>这样就可以通过上面的 WebUI 界面让它进行中文对话，问问题，帮我写代码了。同时也整合了其它的一些 coding LLM 在一起，碰到不会写的代码，就让它们在一起比比武。</p> 
<p>以下视频是 fine tune 的中文 llama3 实际使用演示。</p> 
<div class="csdn-video-box"> 
 <iframe id="j9aeF6Qe-1714901643708" frameborder="0" src="https://live.csdn.net/v/embed/384464" allowfullscreen="true" data-mediaembed="csdn"></iframe> 
 <p>Llama3中文微调本地部署</p> 
</div> 
<p></p> 
<p>如果您喜欢本文的内容，欢迎扫描下面的二维码访问作者的博客：<a href="https://blog.yanghong.dev/" rel="nofollow" title="HY's Blog">HY's Blog</a></p> 
<p><img alt="" height="260" src="https://images2.imgbox.com/1a/be/lks2Vm05_o.png" width="260"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f0ce4ff3aa92ff50b1c74679cd84e928/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">安卓root详解（supersu，magisk原理说明）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/faaca2dbd733677b3f16d328a74b4dba/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">kafka日志存储</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>