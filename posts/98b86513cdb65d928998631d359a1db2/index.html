<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【07】LLaMA-Factory微调大模型——微调模型导出与微调参数分析 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/98b86513cdb65d928998631d359a1db2/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【07】LLaMA-Factory微调大模型——微调模型导出与微调参数分析">
  <meta property="og:description" content="上文介绍了如何对微调后的模型进行使用与简单评估。本文将介绍对微调后的模型进行导出的过程。
一、llama-3微调后的模型导出 首先进入虚拟环境，打开LLaMA-Factory的webui页面
conda activate GLM cd LLaMA-Factory llamafactory-cli webui 之后，选择微调后模型对应的检查点路径文件，设置最大分块的大小，建议2-5GB，选择导出设备的类型并对导出目录进行指定。
完成配置后开始导出模型
模型导出后，可在对应的路径下查看其参数详细情况
二、调用导出后的模型 在LLaMA-Factory的webui页面中选择chat标签，模型路径输入导出后模型的绝对路径，从而加载模型机械能对话
模型成功加载后，即可使用问答框进行应用，至此导出后的模型可应用于实际的生成环境之中
使用测试用例进行分析，可发现与模型微调评估的效果一致，模型导出与应用完成
三、模型微调参数分析 模型微调应用，参数的选择极为关键，具体参数分析可见以下这篇博客
LLaMA-Factory参数的解答（命令，单卡，预训练）_llama-factory 增量预训练-CSDN博客https://blog.csdn.net/m0_69655483/article/details/138229566?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-1-138229566-blog-139495955.235%5Ev43%5Epc_blog_bottom_relevance_base2&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-1-138229566-blog-139495955.235%5Ev43%5Epc_blog_bottom_relevance_base2&amp;utm_relevant_index=1现对关键的几个参数进行分析
（1）finetuning_type lora
使用LoRA（Low-Rank Adaptation）作为微调类型。
其他参数
​ 1.Full：这种方式就是从头到尾完全训练一个模型。想象一下，你有一块白纸，你要在上面画出一幅完整的画作，这就是Full Training。你从零开始，逐步训练模型，直到它能够完成你想要的任务。
​ 2.Freeze：这种方式有点像是在一幅半完成的画上继续作画。在模型中，有些部分（通常是模型的初级部分，如输入层或底层特征提取部分）是已经训练好的，这部分会被“冻结”，不再在训练过程中更新。你只更新模型的其他部分，这样可以节省训练时间和资源。
​ 3.LoRA：这是一种比较新的技术，全称是“Low-Rank Adaptation”。可以理解为一种轻量级的模型调整方式。它主要是在模型的某些核心部分插入小的、低秩的矩阵，通过调整这些小矩阵来实现对整个模型的微调。这种方法不需要对原始模型的大部分参数进行重训练，从而可以在不牺牲太多性能的情况下，快速适应新的任务或数据。
​ 4.QLoRA：这是在LoRA的基础上进一步发展的一种方法。它使用量化技术（也就是用更少的比特来表示每个数字），来进一步减少模型调整过程中需要的计算资源和存储空间。这样做可以使得模型更加高效，尤其是在资源有限的设备上运行时。
（2）gradient_accumulation_steps 梯度累积步数，用于在更新模型前累积更多的梯度，有助于使用较小的批次大小训练大模型。选择多少步骤进行梯度累积取决于你的具体需求和硬件限制。一般来说，步数越多，模拟的批量大小就越大，但同时每次更新权重的间隔也更长，可能会影响训练速度和效率。
（3）lr_scheduler_type
学习率调度器类型
linear（线性）: 描述：学习率从一个较高的初始值开始，然后随着时间线性地减少到一个较低的值。 使用场景：当你想要让模型在训练早期快速学习，然后逐渐减慢学习速度以稳定收敛时使用。 cosine（余弦）: 描述：学习率按照余弦曲线的形状进行周期性调整，这种周期性的起伏有助于模型在不同的训练阶段探索参数空间。 使用场景：在需要模型在训练过程中不断找到新解的复杂任务中使用，比如大规模的图像或文本处理。 cosine_with_restarts（带重启的余弦）: 描述：这是余弦调整的一种变体，每当学习率达到一个周期的最低点时，会突然重置到最高点，然后再次减少。 使用场景：适用于需要模型从局部最优解中跳出来，尝试寻找更好全局解的情况。 polynomial（多项式）: 描述：学习率按照一个多项式函数减少，通常是一个幂次递减的形式。 使用场景：当你需要更精细控制学习率减少速度时使用，适用于任务比较复杂，需要精细调优的模型。 constant（常数）: 描述：学习率保持不变。 使用场景：简单任务或者小数据集，模型容易训练到足够好的性能时使用。 constant_with_warmup（带预热的常数）: 描述：开始时使用较低的学习率“预热”模型，然后切换到一个固定的较高学习率。 使用场景：在训练大型模型或复杂任务时，帮助模型稳定地开始学习，避免一开始就进行大的权重调整。 inverse_sqrt（逆平方根）: 描述：学习率随训练步数的增加按逆平方根递减。 使用场景：常用于自然语言处理中，特别是在训练Transformer模型时，帮助模型在训练后期进行细微的调整。 reduce_lr_on_plateau（在平台期降低学习率）: 描述：当模型的验证性能不再提升时，自动减少学习率。 使用场景：适用于几乎所有类型的任务，特别是当模型很难进一步提高性能时，可以帮助模型继续优化和提升。 （4）warmup_steps
学习率预热步数。
预热步数（Warmup Steps）：
这是模型训练初期用于逐渐增加学习率的步骤数。在这个阶段，学习率从一个很小的值（或者接近于零）开始，逐渐增加到设定的初始学习率。这个过程可以帮助模型在训练初期避免因为学习率过高而导致的不稳定，比如参数更新过大，从而有助于模型更平滑地适应训练数据。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-21T16:26:12+08:00">
    <meta property="article:modified_time" content="2024-07-21T16:26:12+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【07】LLaMA-Factory微调大模型——微调模型导出与微调参数分析</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>上文介绍了如何对微调后的模型进行使用与简单评估。本文将介绍对微调后的模型进行导出的过程。</p> 
<h2>一、llama-3微调后的模型导出</h2> 
<p>首先进入虚拟环境，打开LLaMA-Factory的webui页面</p> 
<pre><code class="language-bash">conda activate GLM
cd LLaMA-Factory
llamafactory-cli webui</code></pre> 
<p><img alt="" height="184" src="https://images2.imgbox.com/7f/59/uT4GMqUP_o.png" width="1050"></p> 
<p>之后，选择微调后模型对应的检查点路径文件，设置最大分块的大小，建议2-5GB，选择导出设备的类型并对导出目录进行指定。</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/a7/ee/PVLPf6ZZ_o.png" width="1200"></p> 
<p>完成配置后开始导出模型</p> 
<p><img alt="" height="892" src="https://images2.imgbox.com/1b/55/f3FxHbMw_o.png" width="1200"></p> 
<p>模型导出后，可在对应的路径下查看其参数详细情况</p> 
<p><img alt="" height="314" src="https://images2.imgbox.com/7f/19/9O4cAEoj_o.png" width="1200"></p> 
<h2>二、调用导出后的模型</h2> 
<p>在LLaMA-Factory的webui页面中选择chat标签，模型路径输入导出后模型的绝对路径，从而加载模型机械能对话</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/28/57/fDS7md33_o.png" width="1200"></p> 
<p>模型成功加载后，即可使用问答框进行应用，至此导出后的模型可应用于实际的生成环境之中</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/70/e0/ZOwfZN4u_o.png" width="1200"></p> 
<p>使用测试用例进行分析，可发现与模型微调评估的效果一致，模型导出与应用完成</p> 
<h2>三、模型微调参数分析</h2> 
<p>模型微调应用，参数的选择极为关键，具体参数分析可见以下这篇博客</p> 
<p><a class="has-card" href="https://blog.csdn.net/m0_69655483/article/details/138229566?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromBaidu~Rate-1-138229566-blog-139495955.235%5Ev43%5Epc_blog_bottom_relevance_base2&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromBaidu~Rate-1-138229566-blog-139495955.235%5Ev43%5Epc_blog_bottom_relevance_base2&amp;utm_relevant_index=1" title="LLaMA-Factory参数的解答（命令，单卡，预训练）_llama-factory 增量预训练-CSDN博客"><span class="link-card-box"><span class="link-title">LLaMA-Factory参数的解答（命令，单卡，预训练）_llama-factory 增量预训练-CSDN博客</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/e6/be/m8fpw4RR_o.png" alt="icon-default.png?t=N7T8">https://blog.csdn.net/m0_69655483/article/details/138229566?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-1-138229566-blog-139495955.235%5Ev43%5Epc_blog_bottom_relevance_base2&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-1-138229566-blog-139495955.235%5Ev43%5Epc_blog_bottom_relevance_base2&amp;utm_relevant_index=1</span></span></a>现对关键的几个参数进行分析</p> 
<p>（1）<strong>finetuning_type lora</strong></p> 
<p>使用LoRA（Low-Rank Adaptation）作为微调类型。</p> 
<p>其他参数<br> ​ 1.Full：这种方式就是从头到尾完全训练一个模型。想象一下，你有一块白纸，你要在上面画出一幅完整的画作，这就是Full Training。你从零开始，逐步训练模型，直到它能够完成你想要的任务。<br> ​ 2.Freeze：这种方式有点像是在一幅半完成的画上继续作画。在模型中，有些部分（通常是模型的初级部分，如输入层或底层特征提取部分）是已经训练好的，这部分会被“冻结”，不再在训练过程中更新。你只更新模型的其他部分，这样可以节省训练时间和资源。<br> ​ 3.LoRA：这是一种比较新的技术，全称是“Low-Rank Adaptation”。可以理解为一种轻量级的模型调整方式。它主要是在模型的某些核心部分插入小的、低秩的矩阵，通过调整这些小矩阵来实现对整个模型的微调。这种方法不需要对原始模型的大部分参数进行重训练，从而可以在不牺牲太多性能的情况下，快速适应新的任务或数据。<br> ​ 4.QLoRA：这是在LoRA的基础上进一步发展的一种方法。它使用量化技术（也就是用更少的比特来表示每个数字），来进一步减少模型调整过程中需要的计算资源和存储空间。这样做可以使得模型更加高效，尤其是在资源有限的设备上运行时。</p> 
<p>（2）<strong>gradient_accumulation_steps </strong></p> 
<p>梯度累积步数，用于在更新模型前累积更多的梯度，有助于使用较小的批次大小训练大模型。选择多少步骤进行梯度累积取决于你的具体需求和硬件限制。一般来说，步数越多，模拟的批量大小就越大，但同时每次更新权重的间隔也更长，可能会影响训练速度和效率。</p> 
<p>（3）<strong>lr_scheduler_type</strong></p> 
<p>学习率调度器类型</p> 
<pre><code class="language-bash">linear（线性）:
描述：学习率从一个较高的初始值开始，然后随着时间线性地减少到一个较低的值。
使用场景：当你想要让模型在训练早期快速学习，然后逐渐减慢学习速度以稳定收敛时使用。

cosine（余弦）:
描述：学习率按照余弦曲线的形状进行周期性调整，这种周期性的起伏有助于模型在不同的训练阶段探索参数空间。
使用场景：在需要模型在训练过程中不断找到新解的复杂任务中使用，比如大规模的图像或文本处理。

cosine_with_restarts（带重启的余弦）:
描述：这是余弦调整的一种变体，每当学习率达到一个周期的最低点时，会突然重置到最高点，然后再次减少。
使用场景：适用于需要模型从局部最优解中跳出来，尝试寻找更好全局解的情况。

polynomial（多项式）:
描述：学习率按照一个多项式函数减少，通常是一个幂次递减的形式。
使用场景：当你需要更精细控制学习率减少速度时使用，适用于任务比较复杂，需要精细调优的模型。

constant（常数）:
描述：学习率保持不变。
使用场景：简单任务或者小数据集，模型容易训练到足够好的性能时使用。

constant_with_warmup（带预热的常数）:
描述：开始时使用较低的学习率“预热”模型，然后切换到一个固定的较高学习率。
使用场景：在训练大型模型或复杂任务时，帮助模型稳定地开始学习，避免一开始就进行大的权重调整。

inverse_sqrt（逆平方根）:
描述：学习率随训练步数的增加按逆平方根递减。
使用场景：常用于自然语言处理中，特别是在训练Transformer模型时，帮助模型在训练后期进行细微的调整。

reduce_lr_on_plateau（在平台期降低学习率）:
描述：当模型的验证性能不再提升时，自动减少学习率。
使用场景：适用于几乎所有类型的任务，特别是当模型很难进一步提高性能时，可以帮助模型继续优化和提升。
</code></pre> 
<p>（4）<strong>warmup_steps</strong></p> 
<p><img alt="" height="300" src="https://images2.imgbox.com/a7/17/Pg8UwM7t_o.png" width="1200"></p> 
<p>学习率预热步数。</p> 
<p>预热步数（Warmup Steps）：</p> 
<p>这是模型训练初期用于逐渐增加学习率的步骤数。在这个阶段，学习率从一个很小的值（或者接近于零）开始，逐渐增加到设定的初始学习率。这个过程可以帮助模型在训练初期避免因为学习率过高而导致的不稳定，比如参数更新过大，从而有助于模型更平滑地适应训练数据。</p> 
<p>例如，如果设置warmup_steps为20，那么在前20步训练中，学习率会从低到高逐步增加。</p> 
<p>预热步数的具体数值通常取决于几个因素：</p> 
<p>训练数据的大小：数据集越大，可能需要更多的预热步骤来帮助模型逐步适应。<br> 模型的复杂性：更复杂的模型可能需要更长时间的预热，以避免一开始就对复杂的参数空间进行过激的调整。<br> 总训练步数：如果训练步数本身就很少，可能不需要很多的预热步骤；反之，如果训练步数很多，增加预热步骤可以帮助模型更好地启动。</p> 
<p>（5）<strong>save_steps  eval_steps</strong></p> 
<p>保存和评估的步数</p> 
<p>（6）<strong>learning_rate</strong></p> 
<p><img alt="" height="430" src="https://images2.imgbox.com/f8/0d/AhrTRuAW_o.png" width="1200"></p> 
<p>学习率是机器学习和深度学习中控制模型学习速度的一个参数。你可以把它想象成你调节自行车踏板力度的旋钮：旋钮转得越多，踏板动得越快，自行车就跑得越快；但如果转得太快，可能会导致自行车失控。同理，学习率太高，模型学习过快，可能会导致学习过程不稳定；学习率太低，模型学习缓慢，训练时间长，效率低。</p> 
<pre><code class="language-bash">常见的学习率参数包括但不限于：
1e-1（0.1）：相对较大的学习率，用于初期快速探索。
1e-2（0.01）：中等大小的学习率，常用于许多标准模型的初始学习率。
1e-3（0.001）：较小的学习率，适用于接近优化目标时的细致调整。
1e-4（0.0001）：更小的学习率，用于当模型接近收敛时的微调。
5e-5（0.00005）：非常小的学习率，常见于预训练模型的微调阶段，例如在自然语言处理中微调BERT模型。
选择学习率的情况：
快速探索：在模型训练初期或者当你不确定最佳参数时，可以使用较大的学习率（例如0.1或0.01），快速找到一个合理的解。
细致调整：当你发现模型的性能开始稳定，但还需要进一步优化时，可以减小学习率（例如0.001或0.0001），帮助模型更精确地找到最优解。
微调预训练模型：当使用已经预训练好的模型（如在特定任务上微调BERT）时，通常使用非常小的学习率（例如5e-5或更小），这是因为预训练模型已经非常接近优化目标，我们只需要做一些轻微的调整。
</code></pre> 
<p>（7）精度相关</p> 
<pre><code class="language-bash">FP16 (Half Precision，半精度)：
这种方式使用16位的浮点数来保存和计算数据。想象一下，如果你有一个非常精细的秤，但现在只用这个秤的一半精度来称重，这就是FP16。它不如32位精度精确，但计算速度更快，占用的内存也更少。
BF16 (BFloat16)：
BF16也是16位的，但它在表示数的方式上和FP16不同，特别是它用更多的位来表示数的大小（指数部分），这让它在处理大范围数值时更加稳定。你可以把它想象成一个专为机器学习优化的“半精度”秤，尤其是在使用特殊的硬件加速器时。
FP32 (Single Precision，单精度)：
这是使用32位浮点数进行计算的方式，可以想象为一个标准的、全功能的精细秤。它在深度学习中非常常见，因为它提供了足够的精确度，适合大多数任务。
Pure BF16：
在表示数的方式上和FP16不同，特别是它用更多的位来表示数的大小（指数部分），这让它在处理大范围数值时更加稳定。你可以把它想象成一个专为机器学习优化的“半精度”秤，尤其是在使用特殊的硬件加速器时。
FP32 (Single Precision，单精度)：
这是使用32位浮点数进行计算的方式，可以想象为一个标准的、全功能的精细秤。它在深度学习中非常常见，因为它提供了足够的精确度，适合大多数任务。
Pure BF16：
这种模式下，所有计算都仅使用BF16格式。这意味着整个模型训练过程中，从输入到输出，都在使用为机器学习优化的半精度计算。
</code></pre> 
<p>（8）LoRA的秩</p> 
<p><img alt="" height="525" src="https://images2.imgbox.com/8b/c3/6K7qwmQf_o.png" width="1200"></p> 
<p>LoRA（Low-Rank Approximation）是一种用于大模型微调的方法，它通过降低模型参数矩阵的秩来减少模型的计算和存储成本。在微调大模型时，往往需要大量的计算资源和存储空间，而LoRA可以通过降低模型参数矩阵的秩来大幅度减少这些需求。</p> 
<p>具体来说，LoRA使用矩阵分解方法，将模型参数矩阵分解为两个较低秩的矩阵的乘积。这样做的好处是可以用较低秩的矩阵近似代替原始的参数矩阵，从而降低了模型的复杂度和存储需求。</p> 
<p>在微调过程中，LoRA首先将模型参数矩阵分解为两个较低秩的矩阵。然后，通过对分解后的矩阵进行微调，可以得到一个近似的模型参数矩阵。这个近似矩阵可以在保持较高性能的同时大幅度减少计算和存储资源的使用。</p> 
<p>LoRA的秩可以根据模型的需求进行设置。一般来说，秩越低，模型的复杂度越低，但性能可能会受到一定的影响。所以在微调大模型时，需要根据具体情况来选择合适的秩大小，以平衡模型的性能和资源的使用。</p> 
<p>建议根据硬件条件进行选择，一般可选16或32，模型微调效果较佳。</p> 
<p>（9）LoRA的缩放系数</p> 
<p>缩放系数是用来表示模型中每个层的相对重要性的参数。在LoRA中，每个层都有一个缩放系数，用于调整该层对总体损失函数的贡献。较高的缩放系数表示该层的权重更大，较低的缩放系数表示该层的权重较小。</p> 
<p>缩放系数的选取可以根据问题的特点和需求进行调整。通常情况下，较低层的缩放系数可以设置为较小的值，以保留更多的原始特征信息；而较高层的缩放系数可以设置为较大的值，以强调更高级别的抽象特征。</p> 
<h2>小结</h2> 
<p>本文介绍了对微调后的模型进行导出的过程与对微调过程中使用的参数进行分析的内容，下文【08】LLaMA-Factory微调大模型——GLM-4模型微调全流程将重数据准备到模型导出全流程进行记录分析。欢迎您持续关注，如果本文对您有所帮助，感谢您一键三连，多多支持。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4c0f275ee1bc76ee9da5067fed4a4c22/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【Linux】编辑器vscode与linux的联动</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ed8827ff672aa13c0bc9429a00e3a7ba/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【数据结构进阶】二叉搜索树</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>