<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Chinese-LLaMA-Alpaca-2模型量化部署&amp;测试 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/ebc16f6fa5295e125b8eba1032dd627d/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Chinese-LLaMA-Alpaca-2模型量化部署&测试">
  <meta property="og:description" content="简介 Chinese-LLaMA-Alpaca-2基于Meta发布的可商用大模型Llama-2开发, 是中文LLaMA&amp;Alpaca大模型的第二期项目.
量化 模型的下载还是应用脚本
bash hfd.sh hfl/chinese-alpaca-2-13b --tool aria2c -x 8 应用llama.cpp进行量化, 主要参考该教程.
其中比较折腾的是与BLAS一起编译.
OpenBLAS 这个真是一言难尽, 非常折腾也没起作用(issue1 &amp; issue2). 而且提升很小, 后续再尝试能不能成功.
cuBLAS 这个提升较为明显, 在有Nvidia GPU的情况下, 需要折腾应该就只有非root用户手动安装一下CUDA toolkit, 然后在CMakeLists.txt中指定一下路径即可.
手动安装CUDA toolkit和cuDnn后, 在CMakeLists.txt中加入:
# ${cuda path}示例: /home/orange/software/cuda-118 set(CUDA_TOOLKIT_ROOT_DIR ${cuda path}) 进行编译即可
mkdir build cd build cmake .. -DLLAMA_CUBLAS=ON cmake --build . --config Release 量化 编译完成llama.cpp后, 进行量化
python convert.py zh-models/chinese-alpaca-2-7b/ ./build/bin/quantize ./zh-models/chinese-alpaca-2-7b/ggml-model-f16.gguf ./zh-models/chinese-alpaca-2-7b/ggml-model-q8_0.gguf q8_0 部署测试 直接使用./build/bin/main -m ./zh-models/chinese-alpaca-2-7b/ggml-model-q8_0.gguf不能进行对话, 加入参数-i表示交互模式, 也可以使用教程中的脚本形式.
按照tutorial, 新建chat.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-03-24T13:21:38+08:00">
    <meta property="article:modified_time" content="2024-03-24T13:21:38+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Chinese-LLaMA-Alpaca-2模型量化部署&amp;测试</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>简介</h2> 
<p><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2">Chinese-LLaMA-Alpaca-2</a>基于Meta发布的可商用大模型<a href="https://github.com/facebookresearch/llama">Llama-2</a>开发, 是<a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca">中文LLaMA&amp;Alpaca</a>大模型的第二期项目.</p> 
<h2><a id="_2"></a>量化</h2> 
<p>模型的下载还是应用<a href="https://padeoe.com/file/hfd/hfd.sh" rel="nofollow">脚本</a></p> 
<pre><code class="prism language-bash"><span class="token function">bash</span> hfd.sh hfl/chinese-alpaca-2-13b <span class="token parameter variable">--tool</span> aria2c <span class="token parameter variable">-x</span> <span class="token number">8</span>
</code></pre> 
<p>应用<a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>进行量化, 主要参考该<a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/llamacpp_zh">教程</a>.<br> 其中比较折腾的是与<code>BLAS</code>一起编译.</p> 
<h3><a id="OpenBLAS_11"></a>OpenBLAS</h3> 
<p>这个真是一言难尽, 非常折腾也没起作用(<a href="https://github.com/ggerganov/llama.cpp/issues/627">issue1</a> &amp; <a href="https://github.com/ggerganov/llama.cpp/discussions/1070">issue2</a>). 而且提升很小, 后续再尝试能不能成功.</p> 
<h3><a id="cuBLAS_14"></a>cuBLAS</h3> 
<p>这个提升较为明显, 在有Nvidia GPU的情况下, 需要折腾应该就只有非root用户手动安装一下<code>CUDA toolkit</code>, 然后在<code>CMakeLists.txt</code>中指定一下路径即可.<br> 手动安装<code>CUDA toolkit</code>和<code>cuDnn</code>后, 在<code>CMakeLists.txt</code>中加入:</p> 
<pre><code class="prism language-bash"><span class="token comment"># ${cuda path}示例: /home/orange/software/cuda-118</span>
set<span class="token punctuation">(</span>CUDA_TOOLKIT_ROOT_DIR <span class="token variable">${cuda path}</span><span class="token punctuation">)</span>
</code></pre> 
<p>进行编译即可</p> 
<pre><code class="prism language-bash"><span class="token function">mkdir</span> build
<span class="token builtin class-name">cd</span> build
cmake <span class="token punctuation">..</span> <span class="token parameter variable">-DLLAMA_CUBLAS</span><span class="token operator">=</span>ON
cmake <span class="token parameter variable">--build</span> <span class="token builtin class-name">.</span> <span class="token parameter variable">--config</span> Release
</code></pre> 
<h3><a id="_30"></a>量化</h3> 
<p>编译完成<code>llama.cpp</code>后, 进行量化</p> 
<pre><code class="prism language-bash">python convert.py zh-models/chinese-alpaca-2-7b/
./build/bin/quantize ./zh-models/chinese-alpaca-2-7b/ggml-model-f16.gguf ./zh-models/chinese-alpaca-2-7b/ggml-model-q8_0.gguf q8_0
</code></pre> 
<h3><a id="_37"></a>部署测试</h3> 
<p>直接使用<code>./build/bin/main -m ./zh-models/chinese-alpaca-2-7b/ggml-model-q8_0.gguf</code>不能进行对话, 加入参数<code>-i</code>表示交互模式, 也可以使用教程中的脚本形式.<br> 按照tutorial, 新建<code>chat.sh</code>文件并填入以下内容</p> 
<pre><code class="prism language-bash"><span class="token shebang important">#!/bin/bash</span>

<span class="token comment"># temporary script to chat with Chinese Alpaca-2 model</span>
<span class="token comment"># usage: ./chat.sh alpaca2-ggml-model-path your-first-instruction</span>

<span class="token assign-left variable">SYSTEM</span><span class="token operator">=</span><span class="token string">'You are a helpful assistant. 你是一个乐于助人的助手。'</span>
<span class="token assign-left variable">FIRST_INSTRUCTION</span><span class="token operator">=</span><span class="token variable">$2</span>

./build/bin/main <span class="token parameter variable">-m</span> <span class="token variable">$1</span> <span class="token punctuation">\</span>
<span class="token parameter variable">--color</span> <span class="token parameter variable">-i</span> <span class="token parameter variable">-c</span> <span class="token number">4096</span> <span class="token parameter variable">-t</span> <span class="token number">8</span> <span class="token parameter variable">--temp</span> <span class="token number">0.5</span> <span class="token parameter variable">--top_k</span> <span class="token number">40</span> <span class="token parameter variable">--top_p</span> <span class="token number">0.9</span> <span class="token parameter variable">--repeat_penalty</span> <span class="token number">1.1</span> <span class="token punctuation">\</span>
--in-prefix-bos --in-prefix <span class="token string">' [INST] '</span> --in-suffix <span class="token string">' [/INST]'</span> <span class="token parameter variable">-p</span> <span class="token punctuation">\</span>
<span class="token string">"[INST] &lt;&lt;SYS&gt;&gt;
<span class="token variable">$SYSTEM</span>
&lt;&lt;/SYS&gt;&gt;

<span class="token variable">$FIRST_INSTRUCTION</span> [/INST]"</span>
</code></pre> 
<p>运行</p> 
<pre><code class="prism language-bash"><span class="token function">bash</span> chat.sh ./zh-models/chinese-alpaca-2-7b/ggml-model-q8_0.gguf <span class="token string">'请列举5条文明乘车的建议'</span>
</code></pre> 
<p>成功实现对话, 部署测试成功.</p> 
<h2><a id="_66"></a>测试</h2> 
<p>下载并解压<a href="https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip" rel="nofollow">测试数据</a></p> 
<h3><a id="chinesealpaca213b_68"></a>chinese-alpaca-2-1.3b</h3> 
<p>测试命令:</p> 
<pre><code class="prism language-bash">./build/bin/perplexity <span class="token parameter variable">-m</span> ./zh-models/chinese-alpaca-2-1.3b/ggml-model-q8_0.gguf <span class="token parameter variable">-f</span> ./wikitext-2-raw/wiki.test.raw <span class="token parameter variable">-ngl</span> <span class="token number">20</span>
</code></pre> 
<p>由于使用cmake编译, 可执行文件位于<code>build/bin</code>下, 注意执行文件和模型, 数据的路径替换即可.<br> 测试数据如下:</p> 
<pre><code class="prism language-bash">main: build <span class="token operator">=</span> <span class="token number">2509</span> <span class="token punctuation">(</span>50ccaf5e<span class="token punctuation">)</span>
main: built with cc <span class="token punctuation">(</span>Ubuntu <span class="token number">9.4</span>.0-1ubuntu1~20.04.2<span class="token punctuation">)</span> <span class="token number">9.4</span>.0 <span class="token keyword">for</span> x86_64-linux-gnu
main: seed  <span class="token operator">=</span> <span class="token number">1711210157</span>
llama_model_loader: loaded meta data with <span class="token number">23</span> key-value pairs and <span class="token number">39</span> tensors from ./zh-models/chinese-alpaca-2-1.3b/ggml-model-q8_0.gguf <span class="token punctuation">(</span>version GGUF V3 <span class="token punctuation">(</span>latest<span class="token punctuation">))</span>
llama_model_loader: Dumping metadata keys/values. Note: KV overrides <span class="token keyword">do</span> not apply <span class="token keyword">in</span> this output.
llama_model_loader: - kv   <span class="token number">0</span>:                       general.architecture str              <span class="token operator">=</span> llama
llama_model_loader: - kv   <span class="token number">1</span>:                               general.name str              <span class="token operator">=</span> LLaMA v2
llama_model_loader: - kv   <span class="token number">2</span>:                           llama.vocab_size u32              <span class="token operator">=</span> <span class="token number">55296</span>
llama_model_loader: - kv   <span class="token number">3</span>:                       llama.context_length u32              <span class="token operator">=</span> <span class="token number">4096</span>
llama_model_loader: - kv   <span class="token number">4</span>:                     llama.embedding_length u32              <span class="token operator">=</span> <span class="token number">4096</span>
llama_model_loader: - kv   <span class="token number">5</span>:                          llama.block_count u32              <span class="token operator">=</span> <span class="token number">4</span>
llama_model_loader: - kv   <span class="token number">6</span>:                  llama.feed_forward_length u32              <span class="token operator">=</span> <span class="token number">11008</span>
llama_model_loader: - kv   <span class="token number">7</span>:                 llama.rope.dimension_count u32              <span class="token operator">=</span> <span class="token number">128</span>
llama_model_loader: - kv   <span class="token number">8</span>:                 llama.attention.head_count u32              <span class="token operator">=</span> <span class="token number">32</span>
llama_model_loader: - kv   <span class="token number">9</span>:              llama.attention.head_count_kv u32              <span class="token operator">=</span> <span class="token number">32</span>
llama_model_loader: - kv  <span class="token number">10</span>:     llama.attention.layer_norm_rms_epsilon f32              <span class="token operator">=</span> <span class="token number">0.000010</span>
llama_model_loader: - kv  <span class="token number">11</span>:                       llama.rope.freq_base f32              <span class="token operator">=</span> <span class="token number">10000.000000</span>
llama_model_loader: - kv  <span class="token number">12</span>:                          general.file_type u32              <span class="token operator">=</span> <span class="token number">7</span>
llama_model_loader: - kv  <span class="token number">13</span>:                       tokenizer.ggml.model str              <span class="token operator">=</span> llama
llama_model_loader: - kv  <span class="token number">14</span>:                      tokenizer.ggml.tokens arr<span class="token punctuation">[</span>str,55296<span class="token punctuation">]</span>   <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"&lt;unk&gt;"</span>, <span class="token string">"&lt;s&gt;"</span>, <span class="token string">"&lt;/s&gt;"</span>, <span class="token string">"&lt;0x00&gt;"</span>, "<span class="token operator">&lt;</span><span class="token punctuation">..</span>.
llama_model_loader: - kv  <span class="token number">15</span>:                      tokenizer.ggml.scores arr<span class="token punctuation">[</span>f32,55296<span class="token punctuation">]</span>   <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.000000</span>, <span class="token number">0.000000</span>, <span class="token number">0.000000</span>, <span class="token number">0.0000</span><span class="token punctuation">..</span>.
llama_model_loader: - kv  <span class="token number">16</span>:                  tokenizer.ggml.token_type arr<span class="token punctuation">[</span>i32,55296<span class="token punctuation">]</span>   <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span>, <span class="token number">3</span>, <span class="token number">3</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token punctuation">..</span>.
llama_model_loader: - kv  <span class="token number">17</span>:                tokenizer.ggml.bos_token_id u32              <span class="token operator">=</span> <span class="token number">1</span>
llama_model_loader: - kv  <span class="token number">18</span>:                tokenizer.ggml.eos_token_id u32              <span class="token operator">=</span> <span class="token number">2</span>
llama_model_loader: - kv  <span class="token number">19</span>:            tokenizer.ggml.padding_token_id u32              <span class="token operator">=</span> <span class="token number">0</span>
llama_model_loader: - kv  <span class="token number">20</span>:               tokenizer.ggml.add_bos_token bool             <span class="token operator">=</span> <span class="token boolean">true</span>
llama_model_loader: - kv  <span class="token number">21</span>:               tokenizer.ggml.add_eos_token bool             <span class="token operator">=</span> <span class="token boolean">false</span>
llama_model_loader: - kv  <span class="token number">22</span>:               general.quantization_version u32              <span class="token operator">=</span> <span class="token number">2</span>
llama_model_loader: - <span class="token builtin class-name">type</span>  f32:    <span class="token number">9</span> tensors
llama_model_loader: - <span class="token builtin class-name">type</span> q8_0:   <span class="token number">30</span> tensors
llm_load_vocab: mismatch <span class="token keyword">in</span> special tokens definition <span class="token punctuation">(</span> <span class="token number">889</span>/55296 vs <span class="token number">259</span>/55296 <span class="token punctuation">)</span>.
llm_load_print_meta: <span class="token function">format</span>           <span class="token operator">=</span> GGUF V3 <span class="token punctuation">(</span>latest<span class="token punctuation">)</span>
llm_load_print_meta: arch             <span class="token operator">=</span> llama
llm_load_print_meta: vocab <span class="token builtin class-name">type</span>       <span class="token operator">=</span> SPM
llm_load_print_meta: n_vocab          <span class="token operator">=</span> <span class="token number">55296</span>
llm_load_print_meta: n_merges         <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: n_ctx_train      <span class="token operator">=</span> <span class="token number">4096</span>
llm_load_print_meta: n_embd           <span class="token operator">=</span> <span class="token number">4096</span>
llm_load_print_meta: n_head           <span class="token operator">=</span> <span class="token number">32</span>
llm_load_print_meta: n_head_kv        <span class="token operator">=</span> <span class="token number">32</span>
llm_load_print_meta: n_layer          <span class="token operator">=</span> <span class="token number">4</span>
llm_load_print_meta: n_rot            <span class="token operator">=</span> <span class="token number">128</span>
llm_load_print_meta: n_embd_head_k    <span class="token operator">=</span> <span class="token number">128</span>
llm_load_print_meta: n_embd_head_v    <span class="token operator">=</span> <span class="token number">128</span>
llm_load_print_meta: n_gqa            <span class="token operator">=</span> <span class="token number">1</span>
llm_load_print_meta: n_embd_k_gqa     <span class="token operator">=</span> <span class="token number">4096</span>
llm_load_print_meta: n_embd_v_gqa     <span class="token operator">=</span> <span class="token number">4096</span>
llm_load_print_meta: f_norm_eps       <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: f_norm_rms_eps   <span class="token operator">=</span> <span class="token number">1</span>.0e-05
llm_load_print_meta: f_clamp_kqv      <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: f_max_alibi_bias <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: f_logit_scale    <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: n_ff             <span class="token operator">=</span> <span class="token number">11008</span>
llm_load_print_meta: n_expert         <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: n_expert_used    <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: causal attn      <span class="token operator">=</span> <span class="token number">1</span>
llm_load_print_meta: pooling <span class="token builtin class-name">type</span>     <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: rope <span class="token builtin class-name">type</span>        <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: rope scaling     <span class="token operator">=</span> linear
llm_load_print_meta: freq_base_train  <span class="token operator">=</span> <span class="token number">10000.0</span>
llm_load_print_meta: freq_scale_train <span class="token operator">=</span> <span class="token number">1</span>
llm_load_print_meta: n_yarn_orig_ctx  <span class="token operator">=</span> <span class="token number">4096</span>
llm_load_print_meta: rope_finetuned   <span class="token operator">=</span> unknown
llm_load_print_meta: ssm_d_conv       <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_d_inner      <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_d_state      <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_dt_rank      <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: model <span class="token builtin class-name">type</span>       <span class="token operator">=</span> ?B
llm_load_print_meta: model ftype      <span class="token operator">=</span> Q8_0
llm_load_print_meta: model params     <span class="token operator">=</span> <span class="token number">1.26</span> B
llm_load_print_meta: model size       <span class="token operator">=</span> <span class="token number">1.25</span> GiB <span class="token punctuation">(</span><span class="token number">8.50</span> BPW<span class="token punctuation">)</span>
llm_load_print_meta: general.name     <span class="token operator">=</span> LLaMA v2
llm_load_print_meta: BOS token        <span class="token operator">=</span> <span class="token number">1</span> <span class="token string">'&lt;s&gt;'</span>
llm_load_print_meta: EOS token        <span class="token operator">=</span> <span class="token number">2</span> <span class="token string">'&lt;/s&gt;'</span>
llm_load_print_meta: UNK token        <span class="token operator">=</span> <span class="token number">0</span> <span class="token string">'&lt;unk&gt;'</span>
llm_load_print_meta: PAD token        <span class="token operator">=</span> <span class="token number">0</span> <span class="token string">'&lt;unk&gt;'</span>
llm_load_print_meta: LF token         <span class="token operator">=</span> <span class="token number">13</span> <span class="token string">'&lt;0x0A&gt;'</span>
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: <span class="token function">yes</span>
ggml_cuda_init: found <span class="token number">2</span> CUDA devices:
  Device <span class="token number">0</span>: NVIDIA A100-PCIE-40GB, compute capability <span class="token number">8.0</span>, VMM: <span class="token function">yes</span>
  Device <span class="token number">1</span>: NVIDIA A100-PCIE-40GB, compute capability <span class="token number">8.0</span>, VMM: <span class="token function">yes</span>
llm_load_tensors: ggml ctx size <span class="token operator">=</span>    <span class="token number">0.05</span> MiB
llm_load_tensors: offloading <span class="token number">4</span> repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded <span class="token number">5</span>/5 layers to GPU
llm_load_tensors:        CPU buffer size <span class="token operator">=</span>   <span class="token number">229.50</span> MiB
llm_load_tensors:      CUDA0 buffer size <span class="token operator">=</span>   <span class="token number">615.28</span> MiB
llm_load_tensors:      CUDA1 buffer size <span class="token operator">=</span>   <span class="token number">434.61</span> MiB
<span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>.
llama_new_context_with_model: n_ctx      <span class="token operator">=</span> <span class="token number">2048</span>
llama_new_context_with_model: n_batch    <span class="token operator">=</span> <span class="token number">2048</span>
llama_new_context_with_model: n_ubatch   <span class="token operator">=</span> <span class="token number">512</span>
llama_new_context_with_model: freq_base  <span class="token operator">=</span> <span class="token number">10000.0</span>
llama_new_context_with_model: freq_scale <span class="token operator">=</span> <span class="token number">1</span>
llama_kv_cache_init:      CUDA0 KV buffer size <span class="token operator">=</span>    <span class="token number">96.00</span> MiB
llama_kv_cache_init:      CUDA1 KV buffer size <span class="token operator">=</span>    <span class="token number">32.00</span> MiB
llama_new_context_with_model: KV self size  <span class="token operator">=</span>  <span class="token number">128.00</span> MiB, K <span class="token punctuation">(</span>f16<span class="token punctuation">)</span>:   <span class="token number">64.00</span> MiB, V <span class="token punctuation">(</span>f16<span class="token punctuation">)</span>:   <span class="token number">64.00</span> MiB
llama_new_context_with_model:  CUDA_Host  output buffer size <span class="token operator">=</span>   <span class="token number">432.00</span> MiB
llama_new_context_with_model: pipeline parallelism enabled <span class="token punctuation">(</span>n_copies<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>
llama_new_context_with_model:      CUDA0 compute buffer size <span class="token operator">=</span>   <span class="token number">208.01</span> MiB
llama_new_context_with_model:      CUDA1 compute buffer size <span class="token operator">=</span>   <span class="token number">200.01</span> MiB
llama_new_context_with_model:  CUDA_Host compute buffer size <span class="token operator">=</span>    <span class="token number">24.02</span> MiB
llama_new_context_with_model: graph nodes  <span class="token operator">=</span> <span class="token number">136</span>
llama_new_context_with_model: graph splits <span class="token operator">=</span> <span class="token number">3</span>

system_info: n_threads <span class="token operator">=</span> <span class="token number">76</span> / <span class="token number">152</span> <span class="token operator">|</span> AVX <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> AVX_VNNI <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX2 <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> AVX512 <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> AVX512_VBMI <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> AVX512_VNNI <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> FMA <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> NEON <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> ARM_FMA <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> F16C <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> FP16_VA <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> WASM_SIMD <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> BLAS <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> SSE3 <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> SSSE3 <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> VSX <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> MATMUL_INT8 <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span>
perplexity: tokenizing the input <span class="token punctuation">..</span>
perplexity: tokenization took <span class="token number">1187.98</span> ms
perplexity: calculating perplexity over <span class="token number">655</span> chunks, <span class="token assign-left variable">n_ctx</span><span class="token operator">=</span><span class="token number">512</span>, <span class="token assign-left variable">batch_size</span><span class="token operator">=</span><span class="token number">2048</span>, <span class="token assign-left variable">n_seq</span><span class="token operator">=</span><span class="token number">4</span>
perplexity: <span class="token number">0.06</span> seconds per pass - ETA <span class="token number">0.17</span> minutes
<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token number">35.2055</span>,<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token number">3151.7331</span>,<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token number">9745.8526</span>,<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token number">3056.9236</span>
<span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>
<span class="token punctuation">[</span><span class="token number">653</span><span class="token punctuation">]</span><span class="token number">1226.9638</span>,<span class="token punctuation">[</span><span class="token number">654</span><span class="token punctuation">]</span><span class="token number">1219.7704</span>,<span class="token punctuation">[</span><span class="token number">655</span><span class="token punctuation">]</span><span class="token number">1213.9217</span>,
Final estimate: PPL <span class="token operator">=</span> <span class="token number">1213.9217</span> +/- <span class="token number">16.09822</span>

llama_print_timings:        load <span class="token function">time</span> <span class="token operator">=</span>    <span class="token number">2998.83</span> ms
llama_print_timings:      sample <span class="token function">time</span> <span class="token operator">=</span>       <span class="token number">0.00</span> ms /     <span class="token number">1</span> runs   <span class="token punctuation">(</span>    <span class="token number">0.00</span> ms per token,      inf tokens per second<span class="token punctuation">)</span>
llama_print_timings: prompt <span class="token builtin class-name">eval</span> <span class="token function">time</span> <span class="token operator">=</span>    <span class="token number">8371.13</span> ms / <span class="token number">335360</span> tokens <span class="token punctuation">(</span>    <span class="token number">0.02</span> ms per token, <span class="token number">40061.49</span> tokens per second<span class="token punctuation">)</span>
llama_print_timings:        <span class="token builtin class-name">eval</span> <span class="token function">time</span> <span class="token operator">=</span>       <span class="token number">0.00</span> ms /     <span class="token number">1</span> runs   <span class="token punctuation">(</span>    <span class="token number">0.00</span> ms per token,      inf tokens per second<span class="token punctuation">)</span>
llama_print_timings:       total <span class="token function">time</span> <span class="token operator">=</span>   <span class="token number">17937.28</span> ms / <span class="token number">335361</span> tokens
</code></pre> 
<h3><a id="chinesealpaca213b_200"></a>chinese-alpaca-2-13b</h3> 
<p>测试命令:</p> 
<pre><code class="prism language-bash">./build/bin/perplexity <span class="token parameter variable">-m</span> ./zh-models/chinese-alpaca-2-13b/ggml-model-q8_0.gguf <span class="token parameter variable">-f</span> ./wikitext-2-raw/wiki.test.raw <span class="token parameter variable">-ngl</span> <span class="token number">10</span>
</code></pre> 
<p>测试数据如下:</p> 
<pre><code class="prism language-bash">main: build <span class="token operator">=</span> <span class="token number">2509</span> <span class="token punctuation">(</span>50ccaf5e<span class="token punctuation">)</span>
main: built with cc <span class="token punctuation">(</span>Ubuntu <span class="token number">9.4</span>.0-1ubuntu1~20.04.2<span class="token punctuation">)</span> <span class="token number">9.4</span>.0 <span class="token keyword">for</span> x86_64-linux-gnu
main: seed  <span class="token operator">=</span> <span class="token number">1711210012</span>
llama_model_loader: loaded meta data with <span class="token number">21</span> key-value pairs and <span class="token number">363</span> tensors from ./zh-models/chinese-alpaca-2-13b/ggml-model-q8_0.gguf <span class="token punctuation">(</span>version GGUF V3 <span class="token punctuation">(</span>latest<span class="token punctuation">))</span>
llama_model_loader: Dumping metadata keys/values. Note: KV overrides <span class="token keyword">do</span> not apply <span class="token keyword">in</span> this output.
llama_model_loader: - kv   <span class="token number">0</span>:                       general.architecture str              <span class="token operator">=</span> llama
llama_model_loader: - kv   <span class="token number">1</span>:                               general.name str              <span class="token operator">=</span> LLaMA v2
llama_model_loader: - kv   <span class="token number">2</span>:                           llama.vocab_size u32              <span class="token operator">=</span> <span class="token number">55296</span>
llama_model_loader: - kv   <span class="token number">3</span>:                       llama.context_length u32              <span class="token operator">=</span> <span class="token number">4096</span>
llama_model_loader: - kv   <span class="token number">4</span>:                     llama.embedding_length u32              <span class="token operator">=</span> <span class="token number">5120</span>
llama_model_loader: - kv   <span class="token number">5</span>:                          llama.block_count u32              <span class="token operator">=</span> <span class="token number">40</span>
llama_model_loader: - kv   <span class="token number">6</span>:                  llama.feed_forward_length u32              <span class="token operator">=</span> <span class="token number">13824</span>
llama_model_loader: - kv   <span class="token number">7</span>:                 llama.rope.dimension_count u32              <span class="token operator">=</span> <span class="token number">128</span>
llama_model_loader: - kv   <span class="token number">8</span>:                 llama.attention.head_count u32              <span class="token operator">=</span> <span class="token number">40</span>
llama_model_loader: - kv   <span class="token number">9</span>:              llama.attention.head_count_kv u32              <span class="token operator">=</span> <span class="token number">40</span>
llama_model_loader: - kv  <span class="token number">10</span>:     llama.attention.layer_norm_rms_epsilon f32              <span class="token operator">=</span> <span class="token number">0.000010</span>
llama_model_loader: - kv  <span class="token number">11</span>:                          general.file_type u32              <span class="token operator">=</span> <span class="token number">7</span>
llama_model_loader: - kv  <span class="token number">12</span>:                       tokenizer.ggml.model str              <span class="token operator">=</span> llama
llama_model_loader: - kv  <span class="token number">13</span>:                      tokenizer.ggml.tokens arr<span class="token punctuation">[</span>str,55296<span class="token punctuation">]</span>   <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"&lt;unk&gt;"</span>, <span class="token string">"&lt;s&gt;"</span>, <span class="token string">"&lt;/s&gt;"</span>, <span class="token string">"&lt;0x00&gt;"</span>, "<span class="token operator">&lt;</span><span class="token punctuation">..</span>.
llama_model_loader: - kv  <span class="token number">14</span>:                      tokenizer.ggml.scores arr<span class="token punctuation">[</span>f32,55296<span class="token punctuation">]</span>   <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.000000</span>, <span class="token number">0.000000</span>, <span class="token number">0.000000</span>, <span class="token number">0.0000</span><span class="token punctuation">..</span>.
llama_model_loader: - kv  <span class="token number">15</span>:                  tokenizer.ggml.token_type arr<span class="token punctuation">[</span>i32,55296<span class="token punctuation">]</span>   <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span>, <span class="token number">3</span>, <span class="token number">3</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token number">6</span>, <span class="token punctuation">..</span>.
llama_model_loader: - kv  <span class="token number">16</span>:                tokenizer.ggml.bos_token_id u32              <span class="token operator">=</span> <span class="token number">1</span>
llama_model_loader: - kv  <span class="token number">17</span>:                tokenizer.ggml.eos_token_id u32              <span class="token operator">=</span> <span class="token number">2</span>
llama_model_loader: - kv  <span class="token number">18</span>:               tokenizer.ggml.add_bos_token bool             <span class="token operator">=</span> <span class="token boolean">true</span>
llama_model_loader: - kv  <span class="token number">19</span>:               tokenizer.ggml.add_eos_token bool             <span class="token operator">=</span> <span class="token boolean">false</span>
llama_model_loader: - kv  <span class="token number">20</span>:               general.quantization_version u32              <span class="token operator">=</span> <span class="token number">2</span>
llama_model_loader: - <span class="token builtin class-name">type</span>  f32:   <span class="token number">81</span> tensors
llama_model_loader: - <span class="token builtin class-name">type</span> q8_0:  <span class="token number">282</span> tensors
llm_load_vocab: mismatch <span class="token keyword">in</span> special tokens definition <span class="token punctuation">(</span> <span class="token number">889</span>/55296 vs <span class="token number">259</span>/55296 <span class="token punctuation">)</span>.
llm_load_print_meta: <span class="token function">format</span>           <span class="token operator">=</span> GGUF V3 <span class="token punctuation">(</span>latest<span class="token punctuation">)</span>
llm_load_print_meta: arch             <span class="token operator">=</span> llama
llm_load_print_meta: vocab <span class="token builtin class-name">type</span>       <span class="token operator">=</span> SPM
llm_load_print_meta: n_vocab          <span class="token operator">=</span> <span class="token number">55296</span>
llm_load_print_meta: n_merges         <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: n_ctx_train      <span class="token operator">=</span> <span class="token number">4096</span>
llm_load_print_meta: n_embd           <span class="token operator">=</span> <span class="token number">5120</span>
llm_load_print_meta: n_head           <span class="token operator">=</span> <span class="token number">40</span>
llm_load_print_meta: n_head_kv        <span class="token operator">=</span> <span class="token number">40</span>
llm_load_print_meta: n_layer          <span class="token operator">=</span> <span class="token number">40</span>
llm_load_print_meta: n_rot            <span class="token operator">=</span> <span class="token number">128</span>
llm_load_print_meta: n_embd_head_k    <span class="token operator">=</span> <span class="token number">128</span>
llm_load_print_meta: n_embd_head_v    <span class="token operator">=</span> <span class="token number">128</span>
llm_load_print_meta: n_gqa            <span class="token operator">=</span> <span class="token number">1</span>
llm_load_print_meta: n_embd_k_gqa     <span class="token operator">=</span> <span class="token number">5120</span>
llm_load_print_meta: n_embd_v_gqa     <span class="token operator">=</span> <span class="token number">5120</span>
llm_load_print_meta: f_norm_eps       <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: f_norm_rms_eps   <span class="token operator">=</span> <span class="token number">1</span>.0e-05
llm_load_print_meta: f_clamp_kqv      <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: f_max_alibi_bias <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: f_logit_scale    <span class="token operator">=</span> <span class="token number">0</span>.0e+00
llm_load_print_meta: n_ff             <span class="token operator">=</span> <span class="token number">13824</span>
llm_load_print_meta: n_expert         <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: n_expert_used    <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: causal attn      <span class="token operator">=</span> <span class="token number">1</span>
llm_load_print_meta: pooling <span class="token builtin class-name">type</span>     <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: rope <span class="token builtin class-name">type</span>        <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: rope scaling     <span class="token operator">=</span> linear
llm_load_print_meta: freq_base_train  <span class="token operator">=</span> <span class="token number">10000.0</span>
llm_load_print_meta: freq_scale_train <span class="token operator">=</span> <span class="token number">1</span>
llm_load_print_meta: n_yarn_orig_ctx  <span class="token operator">=</span> <span class="token number">4096</span>
llm_load_print_meta: rope_finetuned   <span class="token operator">=</span> unknown
llm_load_print_meta: ssm_d_conv       <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_d_inner      <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_d_state      <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: ssm_dt_rank      <span class="token operator">=</span> <span class="token number">0</span>
llm_load_print_meta: model <span class="token builtin class-name">type</span>       <span class="token operator">=</span> 13B
llm_load_print_meta: model ftype      <span class="token operator">=</span> Q8_0
llm_load_print_meta: model params     <span class="token operator">=</span> <span class="token number">13.25</span> B
llm_load_print_meta: model size       <span class="token operator">=</span> <span class="token number">13.12</span> GiB <span class="token punctuation">(</span><span class="token number">8.50</span> BPW<span class="token punctuation">)</span>
llm_load_print_meta: general.name     <span class="token operator">=</span> LLaMA v2
llm_load_print_meta: BOS token        <span class="token operator">=</span> <span class="token number">1</span> <span class="token string">'&lt;s&gt;'</span>
llm_load_print_meta: EOS token        <span class="token operator">=</span> <span class="token number">2</span> <span class="token string">'&lt;/s&gt;'</span>
llm_load_print_meta: UNK token        <span class="token operator">=</span> <span class="token number">0</span> <span class="token string">'&lt;unk&gt;'</span>
llm_load_print_meta: LF token         <span class="token operator">=</span> <span class="token number">13</span> <span class="token string">'&lt;0x0A&gt;'</span>
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
ggml_cuda_init: CUDA_USE_TENSOR_CORES: <span class="token function">yes</span>
ggml_cuda_init: found <span class="token number">2</span> CUDA devices:
  Device <span class="token number">0</span>: NVIDIA A100-PCIE-40GB, compute capability <span class="token number">8.0</span>, VMM: <span class="token function">yes</span>
  Device <span class="token number">1</span>: NVIDIA A100-PCIE-40GB, compute capability <span class="token number">8.0</span>, VMM: <span class="token function">yes</span>
llm_load_tensors: ggml ctx size <span class="token operator">=</span>    <span class="token number">0.42</span> MiB
llm_load_tensors: offloading <span class="token number">10</span> repeating layers to GPU
llm_load_tensors: offloaded <span class="token number">10</span>/41 layers to GPU
llm_load_tensors:        CPU buffer size <span class="token operator">=</span> <span class="token number">13431.58</span> MiB
llm_load_tensors:      CUDA0 buffer size <span class="token operator">=</span>  <span class="token number">1607.23</span> MiB
llm_load_tensors:      CUDA1 buffer size <span class="token operator">=</span>  <span class="token number">1607.23</span> MiB
<span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>
llama_new_context_with_model: n_ctx      <span class="token operator">=</span> <span class="token number">2048</span>
llama_new_context_with_model: n_batch    <span class="token operator">=</span> <span class="token number">2048</span>
llama_new_context_with_model: n_ubatch   <span class="token operator">=</span> <span class="token number">512</span>
llama_new_context_with_model: freq_base  <span class="token operator">=</span> <span class="token number">10000.0</span>
llama_new_context_with_model: freq_scale <span class="token operator">=</span> <span class="token number">1</span>
llama_kv_cache_init:  CUDA_Host KV buffer size <span class="token operator">=</span>  <span class="token number">1200.00</span> MiB
llama_kv_cache_init:      CUDA0 KV buffer size <span class="token operator">=</span>   <span class="token number">200.00</span> MiB
llama_kv_cache_init:      CUDA1 KV buffer size <span class="token operator">=</span>   <span class="token number">200.00</span> MiB
llama_new_context_with_model: KV self size  <span class="token operator">=</span> <span class="token number">1600.00</span> MiB, K <span class="token punctuation">(</span>f16<span class="token punctuation">)</span>:  <span class="token number">800.00</span> MiB, V <span class="token punctuation">(</span>f16<span class="token punctuation">)</span>:  <span class="token number">800.00</span> MiB
llama_new_context_with_model:  CUDA_Host  output buffer size <span class="token operator">=</span>   <span class="token number">432.00</span> MiB
llama_new_context_with_model:      CUDA0 compute buffer size <span class="token operator">=</span>   <span class="token number">404.88</span> MiB
llama_new_context_with_model:      CUDA1 compute buffer size <span class="token operator">=</span>   <span class="token number">204.00</span> MiB
llama_new_context_with_model:  CUDA_Host compute buffer size <span class="token operator">=</span>    <span class="token number">24.00</span> MiB
llama_new_context_with_model: graph nodes  <span class="token operator">=</span> <span class="token number">1324</span>
llama_new_context_with_model: graph splits <span class="token operator">=</span> <span class="token number">335</span>

system_info: n_threads <span class="token operator">=</span> <span class="token number">76</span> / <span class="token number">152</span> <span class="token operator">|</span> AVX <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> AVX_VNNI <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> AVX2 <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> AVX512 <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> AVX512_VBMI <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> AVX512_VNNI <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> FMA <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> NEON <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> ARM_FMA <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> F16C <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> FP16_VA <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> WASM_SIMD <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> BLAS <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> SSE3 <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> SSSE3 <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">|</span> VSX <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span> MATMUL_INT8 <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">|</span>
perplexity: tokenizing the input <span class="token punctuation">..</span>
perplexity: tokenization took <span class="token number">728.604</span> ms
perplexity: calculating perplexity over <span class="token number">655</span> chunks, <span class="token assign-left variable">n_ctx</span><span class="token operator">=</span><span class="token number">512</span>, <span class="token assign-left variable">batch_size</span><span class="token operator">=</span><span class="token number">2048</span>, <span class="token assign-left variable">n_seq</span><span class="token operator">=</span><span class="token number">4</span>
perplexity: <span class="token number">7.36</span> seconds per pass - ETA <span class="token number">20.08</span> minutes
<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token number">4.8998</span>,<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token number">5.3381</span>,<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token number">6.0623</span>,
<span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>
<span class="token punctuation">[</span><span class="token number">654</span><span class="token punctuation">]</span><span class="token number">6.3736</span>,<span class="token punctuation">[</span><span class="token number">655</span><span class="token punctuation">]</span><span class="token number">6.3713</span>,
Final estimate: PPL <span class="token operator">=</span> <span class="token number">6.3713</span> +/- <span class="token number">0.03705</span>

llama_print_timings:        load <span class="token function">time</span> <span class="token operator">=</span>   <span class="token number">17705.89</span> ms
llama_print_timings:      sample <span class="token function">time</span> <span class="token operator">=</span>       <span class="token number">0.00</span> ms /     <span class="token number">1</span> runs   <span class="token punctuation">(</span>    <span class="token number">0.00</span> ms per token,      inf tokens per second<span class="token punctuation">)</span>
llama_print_timings: prompt <span class="token builtin class-name">eval</span> <span class="token function">time</span> <span class="token operator">=</span> <span class="token number">1130068.93</span> ms / <span class="token number">335360</span> tokens <span class="token punctuation">(</span>    <span class="token number">3.37</span> ms per token,   <span class="token number">296.76</span> tokens per second<span class="token punctuation">)</span>
llama_print_timings:        <span class="token builtin class-name">eval</span> <span class="token function">time</span> <span class="token operator">=</span>       <span class="token number">0.00</span> ms /     <span class="token number">1</span> runs   <span class="token punctuation">(</span>    <span class="token number">0.00</span> ms per token,      inf tokens per second<span class="token punctuation">)</span>
llama_print_timings:       total <span class="token function">time</span> <span class="token operator">=</span> <span class="token number">1137969.38</span> ms / <span class="token number">335361</span> tokens
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/dea1e92c0e8a49f2bf398ba13f3f9a6a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">python 与 neo4j 交互（py2neo 使用）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b9919dc4337036784635cb448e680452/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Spark Map 和 FlatMap 的比较</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>