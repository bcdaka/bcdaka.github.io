<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>CVPR2024｜AIGC(图像生成，视频生成，3D生成等)相关论文汇总（附论文链接/开源代码/解析）【持续更新】 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/f58726b21f9bd845b0997c4a9a4de8e6/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="CVPR2024｜AIGC(图像生成，视频生成，3D生成等)相关论文汇总（附论文链接/开源代码/解析）【持续更新】">
  <meta property="og:description" content="CVPR2024｜AIGC相关论文汇总（如果觉得有帮助，欢迎点赞和收藏） Awesome-CVPR2024-AIGC1.图像生成(Image Generation/Image Synthesis)Accelerating Diffusion Sampling with Optimized Time StepsAdversarial Text to Continuous Image GenerationAmodal Completion via Progressive Mixed Context DiffusionArbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural DecoderAtlantis: Enabling Underwater Depth Estimation with Stable DiffusionAttention Calibration for Disentangled Text-to-Image PersonalizationAttention-Driven Training-Free Efficiency Enhancement of Diffusion ModelsCapHuman: Capture Your Moments in Parallel UniversesCHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned NormalizationCheck, Locate, Rectify: A Training-Free Layout Calibration System for Text-to-Image GenerationCoarse-to-Fine Latent Diffusion for Pose-Guided Person Image SynthesisCoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image GenerationCondition-Aware Neural Network for Controlled Image GenerationCosmicMan: A Text-to-Image Foundation Model for HumansCountering Personalized Text-to-Image Generation with Influence WatermarksCross Initialization for Face Personalization of Text-to-Image ModelsCustomization Assistant for Text-to-image GenerationDeepCache: Accelerating Diffusion Models for FreeDemoFusion: Democratising High-Resolution Image Generation With No $Desigen: A Pipeline for Controllable Design Template GenerationDiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language ModelDiffusion-driven GAN Inversion for Multi-Modal Face Image GenerationDistriFusion: Distributed Parallel Inference for High-Resolution Diffusion ModelsDiversity-aware Channel Pruning for StyleGAN CompressionDiscriminative Probing and Tuning for Text-to-Image GenerationDon’t drop your samples!">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-15T21:34:57+08:00">
    <meta property="article:modified_time" content="2024-06-15T21:34:57+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">CVPR2024｜AIGC(图像生成，视频生成，3D生成等)相关论文汇总（附论文链接/开源代码/解析）【持续更新】</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>CVPR2024｜AIGC相关论文汇总（如果觉得有帮助，欢迎点赞和收藏）</h4> 
 <ul><li><a href="#AwesomeCVPR2024AIGC_1" rel="nofollow">Awesome-CVPR2024-AIGC</a></li><li><a href="#1Image_GenerationImage_Synthesis_40" rel="nofollow">1.图像生成(Image Generation/Image Synthesis)</a></li><li><ul><li><ul><li><a href="#Accelerating_Diffusion_Sampling_with_Optimized_Time_Steps_42" rel="nofollow">Accelerating Diffusion Sampling with Optimized Time Steps</a></li><li><a href="#Adversarial_Text_to_Continuous_Image_Generation_47" rel="nofollow">Adversarial Text to Continuous Image Generation</a></li><li><a href="#Amodal_Completion_via_Progressive_Mixed_Context_Diffusion_52" rel="nofollow">Amodal Completion via Progressive Mixed Context Diffusion</a></li><li><a href="#ArbitraryScale_Image_Generation_and_Upsampling_using_Latent_Diffusion_Model_and_Implicit_Neural_Decoder_57" rel="nofollow">Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural Decoder</a></li><li><a href="#Atlantis_Enabling_Underwater_Depth_Estimation_with_Stable_Diffusion_62" rel="nofollow">Atlantis: Enabling Underwater Depth Estimation with Stable Diffusion</a></li><li><a href="#Attention_Calibration_for_Disentangled_TexttoImage_Personalization_67" rel="nofollow">Attention Calibration for Disentangled Text-to-Image Personalization</a></li><li><a href="#AttentionDriven_TrainingFree_Efficiency_Enhancement_of_Diffusion_Models_72" rel="nofollow">Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models</a></li><li><a href="#CapHuman_Capture_Your_Moments_in_Parallel_Universes_77" rel="nofollow">CapHuman: Capture Your Moments in Parallel Universes</a></li><li><a href="#CHAIN_Enhancing_Generalization_in_DataEfficient_GANs_via_lipsCHitz_continuity_constrAIned_Normalization_82" rel="nofollow">CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization</a></li><li><a href="#Check_Locate_Rectify_A_TrainingFree_Layout_Calibration_System_for_TexttoImage_Generation_87" rel="nofollow">Check, Locate, Rectify: A Training-Free Layout Calibration System for Text-to-Image Generation</a></li><li><a href="#CoarsetoFine_Latent_Diffusion_for_PoseGuided_Person_Image_Synthesis_92" rel="nofollow">Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis</a></li><li><a href="#CoDi_Conditional_Diffusion_Distillation_for_HigherFidelity_and_Faster_Image_Generation_97" rel="nofollow">CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation</a></li><li><a href="#ConditionAware_Neural_Network_for_Controlled_Image_Generation_102" rel="nofollow">Condition-Aware Neural Network for Controlled Image Generation</a></li><li><a href="#CosmicMan_A_TexttoImage_Foundation_Model_for_Humans_107" rel="nofollow">CosmicMan: A Text-to-Image Foundation Model for Humans</a></li><li><a href="#Countering_Personalized_TexttoImage_Generation_with_Influence_Watermarks_112" rel="nofollow">Countering Personalized Text-to-Image Generation with Influence Watermarks</a></li><li><a href="#Cross_Initialization_for_Face_Personalization_of_TexttoImage_Models_117" rel="nofollow">Cross Initialization for Face Personalization of Text-to-Image Models</a></li><li><a href="#Customization_Assistant_for_Texttoimage_Generation_122" rel="nofollow">Customization Assistant for Text-to-image Generation</a></li><li><a href="#DeepCache_Accelerating_Diffusion_Models_for_Free_127" rel="nofollow">DeepCache: Accelerating Diffusion Models for Free</a></li><li><a href="#DemoFusion_Democratising_HighResolution_Image_Generation_With_No__132" rel="nofollow">DemoFusion: Democratising High-Resolution Image Generation With No $</a></li><li><a href="#Desigen_A_Pipeline_for_Controllable_Design_Template_Generation_137" rel="nofollow">Desigen: A Pipeline for Controllable Design Template Generation</a></li><li><a href="#DiffAgent_Fast_and_Accurate_TexttoImage_API_Selection_with_Large_Language_Model_142" rel="nofollow">DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model</a></li><li><a href="#Diffusiondriven_GAN_Inversion_for_MultiModal_Face_Image_Generation_147" rel="nofollow">Diffusion-driven GAN Inversion for Multi-Modal Face Image Generation</a></li><li><a href="#DistriFusion_Distributed_Parallel_Inference_for_HighResolution_Diffusion_Models_152" rel="nofollow">DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models</a></li><li><a href="#Diversityaware_Channel_Pruning_for_StyleGAN_Compression_157" rel="nofollow">Diversity-aware Channel Pruning for StyleGAN Compression</a></li><li><a href="#Discriminative_Probing_and_Tuning_for_TexttoImage_Generation_162" rel="nofollow">Discriminative Probing and Tuning for Text-to-Image Generation</a></li><li><a href="#Dont_drop_your_samples_Coherenceaware_training_benefits_Conditional_diffusion_167" rel="nofollow">Don’t drop your samples! Coherence-aware training benefits Conditional diffusion</a></li><li><a href="#Drag_Your_Noise_Interactive_Pointbased_Editing_via_Diffusion_Semantic_Propagation_172" rel="nofollow">Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic Propagation</a></li><li><a href="#DreamMatcher_Appearance_Matching_SelfAttention_for_SemanticallyConsistent_TexttoImage_Personalization_177" rel="nofollow">DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization</a></li><li><a href="#Dynamic_Prompt_Optimizing_for_TexttoImage_Generation_182" rel="nofollow">Dynamic Prompt Optimizing for Text-to-Image Generation</a></li><li><a href="#ECLIPSE_A_ResourceEfficient_TexttoImage_Prior_for_Image_Generations_187" rel="nofollow">ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations</a></li><li><a href="#Efficient_Dataset_Distillation_via_Minimax_Diffusion_192" rel="nofollow">Efficient Dataset Distillation via Minimax Diffusion</a></li><li><a href="#ElasticDiffusion_Trainingfree_Arbitrary_Size_Image_Generation_197" rel="nofollow">ElasticDiffusion: Training-free Arbitrary Size Image Generation</a></li><li><a href="#EmoGen_Emotional_Image_Content_Generation_with_TexttoImage_Diffusion_Models_202" rel="nofollow">EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models</a></li><li><a href="#Enabling_MultiConcept_Fusion_in_TexttoImage_Models_207" rel="nofollow">Enabling Multi-Concept Fusion in Text-to-Image Models</a></li><li><a href="#Exact_Fusion_via_Feature_Distribution_Matching_for_Fewshot_Image_Generation_212" rel="nofollow">Exact Fusion via Feature Distribution Matching for Few-shot Image Generation</a></li><li><a href="#FaceChainSuDe_Building_Derived_Class_to_Inherit_Category_Attributes_for_Oneshot_SubjectDriven_Generation_217" rel="nofollow">FaceChain-SuDe: Building Derived Class to Inherit Category Attributes for One-shot Subject-Driven Generation</a></li><li><a href="#Fast_ODEbased_Sampling_for_Diffusion_Models_in_Around_5_Steps_222" rel="nofollow">Fast ODE-based Sampling for Diffusion Models in Around 5 Steps</a></li><li><a href="#FreeControl_TrainingFree_Spatial_Control_of_Any_TexttoImage_Diffusion_Model_with_Any_Condition_227" rel="nofollow">FreeControl: Training-Free Spatial Control of Any Text-to-Image Diffusion Model with Any Condition</a></li><li><a href="#FreeCustom_TuningFree_Customized_Image_Generation_for_MultiConcept_Composition_232" rel="nofollow">FreeCustom: Tuning-Free Customized Image Generation for Multi-Concept Composition</a></li><li><a href="#Generalizable_Tumor_Synthesis_237" rel="nofollow">Generalizable Tumor Synthesis</a></li><li><a href="#Generating_Daylightdriven_Architectural_Design_via_Diffusion_Models_242" rel="nofollow">Generating Daylight-driven Architectural Design via Diffusion Models</a></li><li><a href="#Generative_Unlearning_for_Any_Identity_247" rel="nofollow">Generative Unlearning for Any Identity</a></li><li><a href="#HanDiffuser_TexttoImage_Generation_With_Realistic_Hand_Appearances_252" rel="nofollow">HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances</a></li><li><a href="#Highfidelity_Personcentric_SubjecttoImage_Synthesis_258" rel="nofollow">High-fidelity Person-centric Subject-to-Image Synthesis</a></li><li><a href="#InitNO_Boosting_TexttoImage_Diffusion_Models_via_Initial_Noise_Optimization_263" rel="nofollow">InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise Optimization</a></li><li><a href="#InstantBooth_Personalized_TexttoImage_Generation_without_TestTime_Finetuning_268" rel="nofollow">InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning</a></li><li><a href="#InstanceDiffusion_Instancelevel_Control_for_Image_Generation_273" rel="nofollow">InstanceDiffusion: Instance-level Control for Image Generation</a></li><li><a href="#InstructImagen_Image_Generation_with_Multimodal_Instruction_278" rel="nofollow">Instruct-Imagen: Image Generation with Multi-modal Instruction</a></li><li><a href="#Intelligent_Grimm__Openended_Visual_Storytelling_via_Latent_Diffusion_Models_283" rel="nofollow">Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models</a></li><li><a href="#InteractDiffusion_InteractionControl_for_TexttoImage_Diffusion_Model_288" rel="nofollow">InteractDiffusion: Interaction-Control for Text-to-Image Diffusion Model</a></li><li><a href="#Intriguing_Properties_of_Diffusion_Models_An_Empirical_Study_of_the_Natural_Attack_Capability_in_TexttoImage_Generative_Models_293" rel="nofollow">Intriguing Properties of Diffusion Models: An Empirical Study of the Natural Attack Capability in Text-to-Image Generative Models</a></li><li><a href="#InversionFree_Image_Editing_with_Natural_Language_298" rel="nofollow">Inversion-Free Image Editing with Natural Language</a></li><li><a href="#JeDi_JointImage_Diffusion_Models_for_FinetuningFree_Personalized_TexttoImage_Generation_303" rel="nofollow">JeDi: Joint-Image Diffusion Models for Finetuning-Free Personalized Text-to-Image Generation</a></li><li><a href="#LAKERED_Camouflaged_Images_Generation_by_Latent_Background_Knowledge_RetrievalAugmented_Diffusion_308" rel="nofollow">LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge Retrieval-Augmented Diffusion</a></li><li><a href="#Learned_representationguided_diffusion_models_for_largeimage_generation_313" rel="nofollow">Learned representation-guided diffusion models for large-image generation</a></li><li><a href="#Learning_Continuous_3D_Words_for_TexttoImage_Generation_318" rel="nofollow">Learning Continuous 3D Words for Text-to-Image Generation</a></li><li><a href="#Learning_Disentangled_Identifiers_for_ActionCustomized_TexttoImage_Generation_323" rel="nofollow">Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation</a></li><li><a href="#Learning_Multidimensional_Human_Preference_for_TexttoImage_Generation_328" rel="nofollow">Learning Multi-dimensional Human Preference for Text-to-Image Generation</a></li><li><a href="#LeftRefill_Filling_Right_Canvas_based_on_Left_Reference_through_Generalized_TexttoImage_Diffusion_Model_333" rel="nofollow">LeftRefill: Filling Right Canvas based on Left Reference through Generalized Text-to-Image Diffusion Model</a></li><li><a href="#MACE_Mass_Concept_Erasure_in_Diffusion_Models_338" rel="nofollow">MACE: Mass Concept Erasure in Diffusion Models</a></li><li><a href="#MarkovGen_Structured_Prediction_for_Efficient_TexttoImage_Generation_343" rel="nofollow">MarkovGen: Structured Prediction for Efficient Text-to-Image Generation</a></li><li><a href="#MedM2G_Unifying_Medical_MultiModal_Generation_via_CrossGuided_Diffusion_with_Visual_Invariant_348" rel="nofollow">MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided Diffusion with Visual Invariant</a></li><li><a href="#MIGC_MultiInstance_Generation_Controller_for_TexttoImage_Synthesis_353" rel="nofollow">MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis</a></li><li><a href="#MindBridge_A_CrossSubject_Brain_Decoding_Framework_358" rel="nofollow">MindBridge: A Cross-Subject Brain Decoding Framework</a></li><li><a href="#MULAN_A_Multi_Layer_Annotated_Dataset_for_Controllable_TexttoImage_Generation_363" rel="nofollow">MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation</a></li><li><a href="#On_the_Scalability_of_Diffusionbased_TexttoImage_Generation_368" rel="nofollow">On the Scalability of Diffusion-based Text-to-Image Generation</a></li><li><a href="#OpenBias_Openset_Bias_Detection_in_TexttoImage_Generative_Models_373" rel="nofollow">OpenBias: Open-set Bias Detection in Text-to-Image Generative Models</a></li><li><a href="#Personalized_Residuals_for_ConceptDriven_TexttoImage_Generation_378" rel="nofollow">Personalized Residuals for Concept-Driven Text-to-Image Generation</a></li><li><a href="#Perturbing_Attention_Gives_You_More_Bang_for_the_Buck_Subtle_Imaging_Perturbations_That_Efficiently_Fool_Customized_Diffusion_Models_383" rel="nofollow">Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging Perturbations That Efficiently Fool Customized Diffusion Models</a></li><li><a href="#PhotoMaker_Customizing_Realistic_Human_Photos_via_Stacked_ID_Embedding_388" rel="nofollow">PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding</a></li><li><a href="#PLACE_Adaptive_LayoutSemantic_Fusion_for_Semantic_Image_Synthesis_393" rel="nofollow">PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis</a></li><li><a href="#PlugandPlay_Diffusion_Distillation_398" rel="nofollow">Plug-and-Play Diffusion Distillation</a></li><li><a href="#PromptFree_Diffusion_Taking_Text_out_of_TexttoImage_Diffusion_Models_403" rel="nofollow">Prompt-Free Diffusion: Taking "Text" out of Text-to-Image Diffusion Models</a></li><li><a href="#Ranni_Taming_TexttoImage_Diffusion_for_Accurate_Instruction_Following_408" rel="nofollow">Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following</a></li><li><a href="#Readout_Guidance_Learning_Control_from_Diffusion_Features_413" rel="nofollow">Readout Guidance: Learning Control from Diffusion Features</a></li><li><a href="#Relation_Rectification_in_Diffusion_Model_418" rel="nofollow">Relation Rectification in Diffusion Model</a></li><li><a href="#Residual_Denoising_Diffusion_Models_423" rel="nofollow">Residual Denoising Diffusion Models</a></li><li><a href="#Rethinking_FID_Towards_a_Better_Evaluation_Metric_for_Image_Generation_428" rel="nofollow">Rethinking FID: Towards a Better Evaluation Metric for Image Generation</a></li><li><a href="#Rethinking_the_Spatial_Inconsistency_in_ClassifierFree_Diffusion_Guidance_433" rel="nofollow">Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance</a></li><li><a href="#RetrievalAugmented_Layout_Transformer_for_ContentAware_Layout_Generation_438" rel="nofollow">Retrieval-Augmented Layout Transformer for Content-Aware Layout Generation</a></li><li><a href="#Rich_Human_Feedback_for_TexttoImage_Generation_443" rel="nofollow">Rich Human Feedback for Text-to-Image Generation</a></li><li><a href="#SCoFT_SelfContrastive_FineTuning_for_Equitable_Image_Generation_448" rel="nofollow">SCoFT: Self-Contrastive Fine-Tuning for Equitable Image Generation</a></li><li><a href="#Selfcorrecting_LLMcontrolled_Diffusion_Models_453" rel="nofollow">Self-correcting LLM-controlled Diffusion Models</a></li><li><a href="#SelfDiscovering_Interpretable_Diffusion_Latent_Directions_for_Responsible_TexttoImage_Generation_458" rel="nofollow">Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation</a></li><li><a href="#Shadow_Generation_for_Composite_Image_Using_Diffusion_Model_463" rel="nofollow">Shadow Generation for Composite Image Using Diffusion Model</a></li><li><a href="#Smooth_Diffusion_Crafting_Smooth_Latent_Spaces_in_Diffusion_Models_468" rel="nofollow">Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models</a></li><li><a href="#SSREncoder_Encoding_Selective_Subject_Representation_for_SubjectDriven_Generation_473" rel="nofollow">SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation</a></li><li><a href="#StableVITON_Learning_Semantic_Correspondence_with_Latent_Diffusion_Model_for_Virtual_TryOn_478" rel="nofollow">StableVITON: Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On</a></li><li><a href="#StructureGuided_Adversarial_Training_of_Diffusion_Models_483" rel="nofollow">Structure-Guided Adversarial Training of Diffusion Models</a></li><li><a href="#Style_Aligned_Image_Generation_via_Shared_Attention_488" rel="nofollow">Style Aligned Image Generation via Shared Attention</a></li><li><a href="#SVGDreamer_Text_Guided_SVG_Generation_with_Diffusion_Model_493" rel="nofollow">SVGDreamer: Text Guided SVG Generation with Diffusion Model</a></li><li><a href="#SwiftBrush_OneStep_TexttoImage_Diffusion_Model_with_Variational_Score_Distillation_498" rel="nofollow">SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation</a></li><li><a href="#Tailored_Visions_Enhancing_TexttoImage_Generation_with_Personalized_Prompt_Rewriting_503" rel="nofollow">Tailored Visions: Enhancing Text-to-Image Generation with Personalized Prompt Rewriting</a></li><li><a href="#Tackling_the_Singularities_at_the_Endpoints_of_Time_Intervals_in_Diffusion_Models_508" rel="nofollow">Tackling the Singularities at the Endpoints of Time Intervals in Diffusion Models</a></li><li><a href="#Taming_Stable_Diffusion_for_Text_to_360_Panorama_Image_Generation_513" rel="nofollow">Taming Stable Diffusion for Text to 360∘ Panorama Image Generation</a></li><li><a href="#TextCraftor_Your_Text_Encoder_Can_be_Image_Quality_Controller_518" rel="nofollow">TextCraftor: Your Text Encoder Can be Image Quality Controller</a></li><li><a href="#TextGuided_Variational_Image_Generation_for_Industrial_Anomaly_Detection_and_Segmentation_523" rel="nofollow">Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation</a></li><li><a href="#TFMQDM_Temporal_Feature_Maintenance_Quantization_for_Diffusion_Models_528" rel="nofollow">TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models</a></li><li><a href="#TokenCompose_Grounding_Diffusion_with_Tokenlevel_Supervision_533" rel="nofollow">TokenCompose: Grounding Diffusion with Token-level Supervision</a></li><li><a href="#Towards_Accurate_Posttraining_Quantization_for_Diffusion_Models_538" rel="nofollow">Towards Accurate Post-training Quantization for Diffusion Models</a></li><li><a href="#Towards_Effective_Usage_of_HumanCentric_Priors_in_Diffusion_Models_for_Textbased_Human_Image_Generation_543" rel="nofollow">Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation</a></li><li><a href="#Towards_MemorizationFree_Diffusion_Models_548" rel="nofollow">Towards Memorization-Free Diffusion Models</a></li><li><a href="#Training_Diffusion_Models_Towards_Diverse_Image_Generation_with_Reinforcement_Learning_553" rel="nofollow">Training Diffusion Models Towards Diverse Image Generation with Reinforcement Learning</a></li><li><a href="#UFOGen_You_Forward_Once_Large_Scale_TexttoImage_Generation_via_Diffusion_GANs_558" rel="nofollow">UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs</a></li><li><a href="#UniGS_Unified_Representation_for_Image_Generation_and_Segmentation_563" rel="nofollow">UniGS: Unified Representation for Image Generation and Segmentation</a></li><li><a href="#Using_Human_Feedback_to_Finetune_Diffusion_Models_without_Any_Reward_Model_568" rel="nofollow">Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model</a></li><li><a href="#UVAP_Userspecified_Visual_Appearance_Personalization_via_Decoupled_Self_Augmentation_573" rel="nofollow">U-VAP: User-specified Visual Appearance Personalization via Decoupled Self Augmentation</a></li><li><a href="#ViewDiff_3DConsistent_Image_Generation_with_TextToImage_Models_578" rel="nofollow">ViewDiff: 3D-Consistent Image Generation with Text-To-Image Models</a></li><li><a href="#When_StyleGAN_Meets_Stable_Diffusion_a__Adapter_for_Personalized_Image_Generation_583" rel="nofollow">When StyleGAN Meets Stable Diffusion: a 𝒲+ Adapter for Personalized Image Generation</a></li><li><a href="#XAdapter_Adding_Universal_Compatibility_of_Plugins_for_Upgraded_Diffusion_Model_588" rel="nofollow">X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion Model</a></li></ul> 
  </li></ul> 
  </li><li><a href="#2Image_Editing_596" rel="nofollow">2.图像编辑(Image Editing)</a></li><li><ul><li><ul><li><a href="#An_Edit_Friendly_DDPM_Noise_Space_Inversion_and_Manipulations_598" rel="nofollow">An Edit Friendly DDPM Noise Space: Inversion and Manipulations</a></li><li><a href="#Choose_What_You_Need_Disentangled_Representation_Learning_for_Scene_Text_Recognition_Removal_and_Editing_603" rel="nofollow">Choose What You Need: Disentangled Representation Learning for Scene Text Recognition, Removal and Editing</a></li><li><a href="#ContentStyle_Decoupling_for_Unsupervised_Makeup_Transfer_without_Generating_Pseudo_Ground_Truth_608" rel="nofollow">Content-Style Decoupling for Unsupervised Makeup Transfer without Generating Pseudo Ground Truth</a></li><li><a href="#Contrastive_Denoising_Score_for_Textguided_Latent_Diffusion_Image_Editing_613" rel="nofollow">Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing</a></li><li><a href="#DEADiff_An_Efficient_Stylization_Diffusion_Model_with_Disentangled_Representations_618" rel="nofollow">DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations</a></li><li><a href="#Deformable_Oneshot_Face_Stylization_via_DINO_Semantic_Guidance_623" rel="nofollow">Deformable One-shot Face Stylization via DINO Semantic Guidance</a></li><li><a href="#DemoCaricature_Democratising_Caricature_Generation_with_a_Rough_Sketch_628" rel="nofollow">DemoCaricature: Democratising Caricature Generation with a Rough Sketch</a></li><li><a href="#DiffAM_Diffusionbased_Adversarial_Makeup_Transfer_for_Facial_Privacy_Protection_633" rel="nofollow">DiffAM: Diffusion-based Adversarial Makeup Transfer for Facial Privacy Protection</a></li><li><a href="#DiffMorpher_Unleashing_the_Capability_of_Diffusion_Models_for_Image_Morphing_638" rel="nofollow">DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing</a></li><li><a href="#Diffusion_Handles_Enabling_3D_Edits_for_Diffusion_Models_by_Lifting_Activations_to_3D_643" rel="nofollow">Diffusion Handles: Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D</a></li><li><a href="#DiffusionLight_Light_Probes_for_Free_by_Painting_a_Chrome_Ball_648" rel="nofollow">DiffusionLight: Light Probes for Free by Painting a Chrome Ball</a></li><li><a href="#Diffusion_Models_Without_Attention_653" rel="nofollow">Diffusion Models Without Attention</a></li><li><a href="#Doubly_Abductive_Counterfactual_Inference_for_Textbased_Image_Editing_658" rel="nofollow">Doubly Abductive Counterfactual Inference for Text-based Image Editing</a></li><li><a href="#Edit_One_for_All_Interactive_Batch_Image_Editing_663" rel="nofollow">Edit One for All: Interactive Batch Image Editing</a></li><li><a href="#Face2Diffusion_for_Fast_and_Editable_Face_Personalization_668" rel="nofollow">Face2Diffusion for Fast and Editable Face Personalization</a></li><li><a href="#Focus_on_Your_Instruction_Finegrained_and_Multiinstruction_Image_Editing_by_Attention_Modulation_673" rel="nofollow">Focus on Your Instruction: Fine-grained and Multi-instruction Image Editing by Attention Modulation</a></li><li><a href="#FreeDrag_Feature_Dragging_for_Reliable_Pointbased_Image_Editing_678" rel="nofollow">FreeDrag: Feature Dragging for Reliable Point-based Image Editing</a></li><li><a href="#HoloRelighting_Controllable_Volumetric_Portrait_Relighting_from_a_Single_Image_683" rel="nofollow">Holo-Relighting: Controllable Volumetric Portrait Relighting from a Single Image</a></li><li><a href="#Image_Sculpting_Precise_Object_Editing_with_3D_Geometry_Control_688" rel="nofollow">Image Sculpting: Precise Object Editing with 3D Geometry Control</a></li><li><a href="#InversionFree_Image_Editing_with_Natural_Language_693" rel="nofollow">Inversion-Free Image Editing with Natural Language</a></li><li><a href="#PAIRDiffusion_ObjectLevel_Image_Editing_with_StructureandAppearance_Paired_Diffusion_Models_698" rel="nofollow">PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models</a></li><li><a href="#Person_in_Place_Generating_Associative_SkeletonGuidance_Maps_for_HumanObject_Interaction_Image_Editing_703" rel="nofollow">Person in Place: Generating Associative Skeleton-Guidance Maps for Human-Object Interaction Image Editing</a></li><li><a href="#PuffNet_Efficient_Style_Transfer_with_Pure_Content_and_Style_Feature_Fusion_Network_708" rel="nofollow">Puff-Net: Efficient Style Transfer with Pure Content and Style Feature Fusion Network</a></li><li><a href="#PIA_Your_Personalized_Image_Animator_via_PlugandPlay_Modules_in_TexttoImage_Models_713" rel="nofollow">PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models</a></li><li><a href="#RealCustom_Narrowing_Real_Text_Word_for_RealTime_OpenDomain_TexttoImage_Customization_718" rel="nofollow">RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization</a></li><li><a href="#SmartEdit_Exploring_Complex_Instructionbased_Image_Editing_with_Multimodal_Large_Language_Models_723" rel="nofollow">SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models</a></li><li><a href="#Style_Injection_in_Diffusion_A_Trainingfree_Approach_for_Adapting_Largescale_Diffusion_Models_for_Style_Transfer_728" rel="nofollow">Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer</a></li><li><a href="#SwitchLight_Codesign_of_Physicsdriven_Architecture_and_Pretraining_Framework_for_Human_Portrait_Relighting_733" rel="nofollow">SwitchLight: Co-design of Physics-driven Architecture and Pre-training Framework for Human Portrait Relighting</a></li><li><a href="#TextDriven_Image_Editing_via_Learnable_Regions_738" rel="nofollow">Text-Driven Image Editing via Learnable Regions</a></li><li><a href="#TexturePreserving_Diffusion_Models_for_HighFidelity_Virtual_TryOn_743" rel="nofollow">Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On</a></li><li><a href="#TiNOEdit_Timestep_and_Noise_Optimization_for_Robust_DiffusionBased_Image_Editing_748" rel="nofollow">TiNO-Edit: Timestep and Noise Optimization for Robust Diffusion-Based Image Editing</a></li><li><a href="#UniHuman_A_Unified_Model_For_Editing_Human_Images_in_the_Wild_753" rel="nofollow">UniHuman: A Unified Model For Editing Human Images in the Wild</a></li><li><a href="#ZONE_ZeroShot_InstructionGuided_Local_Editing_758" rel="nofollow">ZONE: Zero-Shot Instruction-Guided Local Editing</a></li></ul> 
  </li></ul> 
  </li><li><a href="#3Video_GenerationVideo_Synthesis_766" rel="nofollow">3.视频生成(Video Generation/Video Synthesis)</a></li><li><ul><li><ul><li><a href="#360DVD_Controllable_Panorama_Video_Generation_with_360Degree_Video_Diffusion_Model_768" rel="nofollow">360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model</a></li><li><a href="#A_Recipe_for_Scaling_up_TexttoVideo_Generation_with_Textfree_Videos_773" rel="nofollow">A Recipe for Scaling up Text-to-Video Generation with Text-free Videos</a></li><li><a href="#BIVDiff_A_TrainingFree_Framework_for_GeneralPurpose_Video_Synthesis_via_Bridging_Image_and_Video_Diffusion_Models_778" rel="nofollow">BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models</a></li><li><a href="#ConvoFusion_MultiModal_Conversational_Diffusion_for_CoSpeech_Gesture_Synthesis_783" rel="nofollow">ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis</a></li><li><a href="#CoSpeech_Gesture_Video_Generation_via_MotionDecoupled_Diffusion_Model_788" rel="nofollow">Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model</a></li><li><a href="#DiffPerformer_Iterative_Learning_of_Consistent_Latent_Guidance_for_Diffusionbased_Human_Video_Generation_793" rel="nofollow">DiffPerformer: Iterative Learning of Consistent Latent Guidance for Diffusion-based Human Video Generation</a></li><li><a href="#DisCo_Disentangled_Control_for_Realistic_Human_Dance_Generation_798" rel="nofollow">DisCo: Disentangled Control for Realistic Human Dance Generation</a></li><li><a href="#FaceChainImagineID_Freely_Crafting_HighFidelity_Diverse_Talking_Faces_from_Disentangled_Audio_803" rel="nofollow">FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio</a></li><li><a href="#Faces_that_Speak_Jointly_Synthesising_Talking_Face_and_Speech_from_Text_808" rel="nofollow">Faces that Speak: Jointly Synthesising Talking Face and Speech from Text</a></li><li><a href="#FlowVid_Taming_Imperfect_Optical_Flows_for_Consistent_VideotoVideo_Synthesis_813" rel="nofollow">FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis</a></li><li><a href="#Generative_Rendering_Controllable_4DGuided_Video_Generation_with_2D_Diffusion_Models_818" rel="nofollow">Generative Rendering: Controllable 4D-Guided Video Generation with 2D Diffusion Models</a></li><li><a href="#GenTron_Diffusion_Transformers_for_Image_and_Video_Generation_823" rel="nofollow">GenTron: Diffusion Transformers for Image and Video Generation</a></li><li><a href="#Grid_Diffusion_Models_for_TexttoVideo_Generation_828" rel="nofollow">Grid Diffusion Models for Text-to-Video Generation</a></li><li><a href="#Hierarchical_Patchwise_Diffusion_Models_for_HighResolution_Video_Generation_833" rel="nofollow">Hierarchical Patch-wise Diffusion Models for High-Resolution Video Generation</a></li><li><a href="#Hierarchical_Spatiotemporal_Decoupling_for_TexttoVideo_Generation_838" rel="nofollow">Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation</a></li><li><a href="#LAMP_Learn_A_Motion_Pattern_for_FewShot_Video_Generation_843" rel="nofollow">LAMP: Learn A Motion Pattern for Few-Shot Video Generation</a></li><li><a href="#Learning_Dynamic_Tetrahedra_for_HighQuality_Talking_Head_Synthesis_848" rel="nofollow">Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis</a></li><li><a href="#Lodge_A_Coarse_to_Fine_Diffusion_Network_for_Long_Dance_Generation_guided_by_the_Characteristic_Dance_Primitives_853" rel="nofollow">Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation guided by the Characteristic Dance Primitives</a></li><li><a href="#MagicAnimate_Temporally_Consistent_Human_Image_Animation_using_Diffusion_Model_858" rel="nofollow">MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model</a></li><li><a href="#MakeYourAnchor_A_Diffusionbased_2D_Avatar_Generation_Framework_863" rel="nofollow">Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework</a></li><li><a href="#Make_Your_Dream_A_Vlog_868" rel="nofollow">Make Your Dream A Vlog</a></li><li><a href="#Make_Pixels_Dance_HighDynamic_Video_Generation_873" rel="nofollow">Make Pixels Dance: High-Dynamic Video Generation</a></li><li><a href="#MicroCinema_A_DivideandConquer_Approach_for_TexttoVideo_Generation_878" rel="nofollow">MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation</a></li><li><a href="#Panacea_Panoramic_and_Controllable_Video_Generation_for_Autonomous_Driving_883" rel="nofollow">Panacea: Panoramic and Controllable Video Generation for Autonomous Driving</a></li><li><a href="#PEEKABOO_Interactive_Video_Generation_via_MaskedDiffusion_888" rel="nofollow">PEEKABOO: Interactive Video Generation via Masked-Diffusion</a></li><li><a href="#Seeing_and_Hearing_Opendomain_VisualAudio_Generation_with_Diffusion_Latent_Aligners_893" rel="nofollow">Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners</a></li><li><a href="#SimDA_Simple_Diffusion_Adapter_for_Efficient_Video_Generation_898" rel="nofollow">SimDA: Simple Diffusion Adapter for Efficient Video Generation</a></li><li><a href="#StyleCineGAN_Landscape_Cinemagraph_Generation_using_a_Pretrained_StyleGAN_903" rel="nofollow">StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN</a></li><li><a href="#SyncTalk_The_Devil_is_in_the_Synchronization_for_Talking_Head_Synthesis_908" rel="nofollow">SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis</a></li><li><a href="#TI2VZero_ZeroShot_Image_Conditioning_for_TexttoVideo_Diffusion_Models_913" rel="nofollow">TI2V-Zero: Zero-Shot Image Conditioning for Text-to-Video Diffusion Models</a></li><li><a href="#TuneAVideo_OneShot_Tuning_of_Image_Diffusion_Models_for_TexttoVideo_Generation_918" rel="nofollow">Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation</a></li><li><a href="#VideoBooth_Diffusionbased_Video_Generation_with_Image_Prompts_923" rel="nofollow">VideoBooth: Diffusion-based Video Generation with Image Prompts</a></li><li><a href="#VideoCrafter2_Overcoming_Data_Limitations_for_HighQuality_Video_Diffusion_Models_928" rel="nofollow">VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models</a></li><li><a href="#VideoP2P_Video_Editing_with_Crossattention_Control_933" rel="nofollow">Video-P2P: Video Editing with Cross-attention Control</a></li></ul> 
  </li></ul> 
  </li><li><a href="#4Video_Editing_941" rel="nofollow">4.视频编辑(Video Editing)</a></li><li><ul><li><ul><li><a href="#A_Video_is_Worth_256_Bases_SpatialTemporal_ExpectationMaximization_Inversion_for_ZeroShot_Video_Editing_943" rel="nofollow">A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing</a></li><li><a href="#CAMEL_Causal_Motion_Enhancement_tailored_for_lifting_textdriven_video_editing_948" rel="nofollow">CAMEL: Causal Motion Enhancement tailored for lifting text-driven video editing</a></li><li><a href="#CCEdit_Creative_and_Controllable_Video_Editing_via_Diffusion_Models_953" rel="nofollow">CCEdit: Creative and Controllable Video Editing via Diffusion Models</a></li><li><a href="#CoDeF_Content_Deformation_Fields_for_Temporally_Consistent_Video_Processing_958" rel="nofollow">CoDeF: Content Deformation Fields for Temporally Consistent Video Processing</a></li><li><a href="#FRESCO_SpatialTemporal_Correspondence_for_ZeroShot_Video_Translation_963" rel="nofollow">FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation</a></li><li><a href="#RAVE_Randomized_Noise_Shuffling_for_Fast_and_Consistent_Video_Editing_with_Diffusion_Models_968" rel="nofollow">RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models</a></li><li><a href="#VidToMe_Video_Token_Merging_for_ZeroShot_Video_Editing_973" rel="nofollow">VidToMe: Video Token Merging for Zero-Shot Video Editing</a></li><li><a href="#VMC_Video_Motion_Customization_using_Temporal_Attention_Adaption_for_TexttoVideo_Diffusion_Models_978" rel="nofollow">VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models</a></li></ul> 
  </li></ul> 
  </li><li><a href="#53D3D_Generation3D_Synthesis_985" rel="nofollow">5.3D生成(3D Generation/3D Synthesis)</a></li><li><ul><li><ul><li><a href="#4D_Gaussian_Splatting_for_RealTime_Dynamic_Scene_Rendering_987" rel="nofollow">4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</a></li><li><a href="#Animatable_Gaussians_Learning_Posedependent_Gaussian_Maps_for_Highfidelity_Human_Avatar_Modeling_992" rel="nofollow">Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling</a></li><li><a href="#A_Unified_Approach_for_Text_and_Imageguided_4D_Scene_Generation_997" rel="nofollow">A Unified Approach for Text- and Image-guided 4D Scene Generation</a></li><li><a href="#BEHAVIOR_Vision_Suite_Customizable_Dataset_Generation_via_Simulation_1002" rel="nofollow">BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation</a></li><li><a href="#BerfScene_Bevconditioned_Equivariant_Radiance_Fields_for_Infinite_3D_Scene_Generation_1007" rel="nofollow">BerfScene: Bev-conditioned Equivariant Radiance Fields for Infinite 3D Scene Generation</a></li><li><a href="#CAD_Photorealistic_3D_Generation_via_Adversarial_Distillation_1012" rel="nofollow">CAD: Photorealistic 3D Generation via Adversarial Distillation</a></li><li><a href="#CAGE_Controllable_Articulation_GEneration_1017" rel="nofollow">CAGE: Controllable Articulation GEneration</a></li><li><a href="#CityDreamer_Compositional_Generative_Model_of_Unbounded_3D_Cities_1022" rel="nofollow">CityDreamer: Compositional Generative Model of Unbounded 3D Cities</a></li><li><a href="#Consistent3D_Towards_Consistent_HighFidelity_Textto3D_Generation_with_Deterministic_Sampling_Prior_1027" rel="nofollow">Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior</a></li><li><a href="#ConTexHuman_FreeView_Rendering_of_Human_from_a_Single_Image_with_TextureConsistent_Synthesis_1032" rel="nofollow">ConTex-Human: Free-View Rendering of Human from a Single Image with Texture-Consistent Synthesis</a></li><li><a href="#ControlRoom3D_Room_Generation_using_Semantic_Proxy_Rooms_1037" rel="nofollow">ControlRoom3D: Room Generation using Semantic Proxy Rooms</a></li><li><a href="#DanceCamera3D_3D_Camera_Movement_Synthesis_with_Music_and_Dance_1042" rel="nofollow">DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance</a></li><li><a href="#DiffPortrait3D_Controllable_Diffusion_for_ZeroShot_Portrait_View_Synthesis_1047" rel="nofollow">DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis</a></li><li><a href="#DiffSHEG_A_DiffusionBased_Approach_for_RealTime_Speechdriven_Holistic_3D_Expression_and_Gesture_Generation_1052" rel="nofollow">DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation</a></li><li><a href="#DiffuScene_Denoising_Diffusion_Models_for_Generative_Indoor_Scene_Synthesis_1057" rel="nofollow">DiffuScene: Denoising Diffusion Models for Generative Indoor Scene Synthesis</a></li><li><a href="#Diffusion_3D_Features_Diff3F_Decorating_Untextured_Shapes_with_Distilled_Semantic_Features_1062" rel="nofollow">Diffusion 3D Features (Diff3F): Decorating Untextured Shapes with Distilled Semantic Features</a></li><li><a href="#Diffusion_Timestep_Curriculum_for_One_Image_to_3D_Generation_1067" rel="nofollow">Diffusion Time-step Curriculum for One Image to 3D Generation</a></li><li><a href="#DreamAvatar_TextandShape_Guided_3D_Human_Avatar_Generation_via_Diffusion_Models_1072" rel="nofollow">DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models</a></li><li><a href="#DreamComposer_Controllable_3D_Object_Generation_via_MultiView_Conditions_1077" rel="nofollow">DreamComposer: Controllable 3D Object Generation via Multi-View Conditions</a></li><li><a href="#DreamControl_ControlBased_Textto3D_Generation_with_3D_SelfPrior_1082" rel="nofollow">DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior</a></li><li><a href="#Emotional_Speechdriven_3D_Body_Animation_via_Disentangled_Latent_Diffusion_1087" rel="nofollow">Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion</a></li><li><a href="#EscherNet_A_Generative_Model_for_Scalable_View_Synthesis_1092" rel="nofollow">EscherNet: A Generative Model for Scalable View Synthesis</a></li><li><a href="#GaussianDreamer_Fast_Generation_from_Text_to_3D_Gaussians_by_Bridging_2D_and_3D_Diffusion_Models_1097" rel="nofollow">GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models</a></li><li><a href="#GPT4Vision_is_a_HumanAligned_Evaluator_for_Textto3D_Generation_1102" rel="nofollow">GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation</a></li><li><a href="#Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_1107" rel="nofollow">Gaussian Shell Maps for Efficient 3D Human Generation</a></li><li><a href="#HarmonyView_Harmonizing_Consistency_and_Diversity_in_OneImageto3D_1112" rel="nofollow">HarmonyView: Harmonizing Consistency and Diversity in One-Image-to-3D</a></li><li><a href="#HIG_Hierarchical_Interlacement_Graph_Approach_to_Scene_Graph_Generation_in_Video_Understanding_1117" rel="nofollow">HIG: Hierarchical Interlacement Graph Approach to Scene Graph Generation in Video Understanding</a></li><li><a href="#Holodeck_Language_Guided_Generation_of_3D_Embodied_AI_Environments_1122" rel="nofollow">Holodeck: Language Guided Generation of 3D Embodied AI Environments</a></li><li><a href="#HumanNorm_Learning_Normal_Diffusion_Model_for_Highquality_and_Realistic_3D_Human_Generation_1127" rel="nofollow">HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation</a></li><li><a href="#Interactive3D_Create_What_You_Want_by_Interactive_3D_Generation_1132" rel="nofollow">Interactive3D: Create What You Want by Interactive 3D Generation</a></li><li><a href="#InterHandGen_TwoHand_Interaction_Generation_via_Cascaded_Reverse_Diffusio_1137" rel="nofollow">InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse Diffusio</a></li><li><a href="#Intrinsic_Image_Diffusion_for_Singleview_Material_Estimation_1142" rel="nofollow">Intrinsic Image Diffusion for Single-view Material Estimation</a></li><li><a href="#MakeItVivid_Dressing_Your_Animatable_Biped_Cartoon_Characters_from_Text_1147" rel="nofollow">Make-It-Vivid: Dressing Your Animatable Biped Cartoon Characters from Text</a></li><li><a href="#MoMask_Generative_Masked_Modeling_of_3D_Human_Motions_1152" rel="nofollow">MoMask: Generative Masked Modeling of 3D Human Motions</a></li><li><a href="#Editable_Scene_Simulation_for_Autonomous_Driving_via_LLMAgent_Collaboration_1157" rel="nofollow">Editable Scene Simulation for Autonomous Driving via LLM-Agent Collaboration</a></li><li><a href="#EpiDiff_Enhancing_MultiView_Synthesis_via_Localized_EpipolarConstrained_Diffusion_1162" rel="nofollow">EpiDiff: Enhancing Multi-View Synthesis via Localized Epipolar-Constrained Diffusion</a></li><li><a href="#OED_Towards_Onestage_EndtoEnd_Dynamic_Scene_Graph_Generation_1167" rel="nofollow">OED: Towards One-stage End-to-End Dynamic Scene Graph Generation</a></li><li><a href="#One2345_Fast_Single_Image_to_3D_Objects_with_Consistent_MultiView_Generation_and_3D_Diffusion_1172" rel="nofollow">One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion</a></li><li><a href="#Paintit_TexttoTexture_Synthesis_via_Deep_Convolutional_Texture_Map_Optimization_and_PhysicallyBased_Rendering_1177" rel="nofollow">Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization and Physically-Based Rendering</a></li><li><a href="#PEGASUS_Personalized_Generative_3D_Avatars_with_Composable_Attributes_1182" rel="nofollow">PEGASUS: Personalized Generative 3D Avatars with Composable Attributes</a></li><li><a href="#PhysGaussian_PhysicsIntegrated_3D_Gaussians_for_Generative_Dynamics_1187" rel="nofollow">PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics</a></li><li><a href="#RichDreamer_A_Generalizable_NormalDepth_Diffusion_Model_for_Detail_Richness_in_Textto3D_1192" rel="nofollow">RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D.</a></li><li><a href="#SceneTex_HighQuality_Texture_Synthesis_for_Indoor_Scenes_via_Diffusion_Priors_1197" rel="nofollow">SceneTex: High-Quality Texture Synthesis for Indoor Scenes via Diffusion Priors</a></li><li><a href="#SceneWiz3D_Towards_Textguided_3D_Scene_Composition_1202" rel="nofollow">SceneWiz3D: Towards Text-guided 3D Scene Composition</a></li><li><a href="#SemCity_Semantic_Scene_Generation_with_Triplane_Diffusion_1207" rel="nofollow">SemCity: Semantic Scene Generation with Triplane Diffusion</a></li><li><a href="#Sherpa3D_Boosting_HighFidelity_Textto3D_Generation_via_Coarse_3D_Prior_1212" rel="nofollow">Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior</a></li><li><a href="#SIGNeRF_Scene_Integrated_Generation_for_Neural_Radiance_Fields_1217" rel="nofollow">SIGNeRF: Scene Integrated Generation for Neural Radiance Fields</a></li><li><a href="#Single_Mesh_Diffusion_Models_with_Field_Latents_for_Texture_Generation_1222" rel="nofollow">Single Mesh Diffusion Models with Field Latents for Texture Generation</a></li><li><a href="#SiTH_Singleview_Textured_Human_Reconstruction_with_ImageConditioned_Diffusion_1227" rel="nofollow">SiTH: Single-view Textured Human Reconstruction with Image-Conditioned Diffusion</a></li><li><a href="#SPAD_Spatially_Aware_Multiview_Diffusers_1232" rel="nofollow">SPAD: Spatially Aware Multiview Diffusers</a></li><li><a href="#Textto3D_Generation_with_Bidirectional_Diffusion_using_both_2D_and_3D_priors_1237" rel="nofollow">Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors</a></li><li><a href="#Textto3D_using_Gaussian_Splatting_1242" rel="nofollow">Text-to-3D using Gaussian Splatting</a></li><li><a href="#The_More_You_See_in_2D_the_More_You_Perceive_in_3D_1247" rel="nofollow">The More You See in 2D, the More You Perceive in 3D</a></li><li><a href="#Tiger_TimeVarying_Denoising_Model_for_3D_Point_Cloud_Generation_with_Diffusion_Process_1252" rel="nofollow">Tiger: Time-Varying Denoising Model for 3D Point Cloud Generation with Diffusion Process</a></li><li><a href="#Towards_Realistic_Scene_Generation_with_LiDAR_Diffusion_Models_1257" rel="nofollow">Towards Realistic Scene Generation with LiDAR Diffusion Models</a></li><li><a href="#UDiFF_Generating_Conditional_Unsigned_Distance_Fields_with_Optimal_Wavelet_Diffusion_1262" rel="nofollow">UDiFF: Generating Conditional Unsigned Distance Fields with Optimal Wavelet Diffusion</a></li><li><a href="#ViVid1to3_Novel_View_Synthesis_with_Video_Diffusion_Models_1267" rel="nofollow">ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models</a></li></ul> 
  </li></ul> 
  </li><li><a href="#63D3D_Editing_1275" rel="nofollow">6.3D编辑(3D Editing)</a></li><li><ul><li><ul><li><a href="#GaussianEditor_Swift_and_Controllable_3D_Editing_with_Gaussian_Splatting_1277" rel="nofollow">GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting</a></li><li><a href="#GenN2N_Generative_NeRF2NeRF_Translation_1282" rel="nofollow">GenN2N: Generative NeRF2NeRF Translation</a></li><li><a href="#Makeup_Prior_Models_for_3D_Facial_Makeup_Estimation_and_Applications_1287" rel="nofollow">Makeup Prior Models for 3D Facial Makeup Estimation and Applications</a></li></ul> 
  </li></ul> 
  </li><li><a href="#7MultiModal_Large_Language_Models_1295" rel="nofollow">7.多模态大语言模型(Multi-Modal Large Language Models)</a></li><li><ul><li><ul><li><a href="#AlphaCLIP_A_CLIP_Model_Focusing_on_Wherever_You_Want_1297" rel="nofollow">Alpha-CLIP: A CLIP Model Focusing on Wherever You Want</a></li><li><a href="#Anchorbased_Robust_Finetuning_of_VisionLanguage_Models_1302" rel="nofollow">Anchor-based Robust Finetuning of Vision-Language Models</a></li><li><a href="#Boosting_Continual_Learning_of_VisionLanguage_Models_via_MixtureofExperts_Adapters_1307" rel="nofollow">Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters</a></li><li><a href="#Can_Language_Beat_Numerical_Regression_LanguageBased_Multimodal_Trajectory_Prediction_1312" rel="nofollow">Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction</a></li><li><a href="#Cant_make_an_Omelette_without_Breaking_some_Eggs_Plausible_Action_Anticipation_using_Large_VideoLanguage_Models_1317" rel="nofollow">Can't make an Omelette without Breaking some Eggs: Plausible Action Anticipation using Large Video-Language Models</a></li><li><a href="#ChatUniVi_Unified_Visual_Representation_Empowers_Large_Language_Models_with_Image_and_Video_Understanding_1322" rel="nofollow">Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding</a></li><li><a href="#Compositional_ChainofThought_Prompting_for_Large_Multimodal_Models_1327" rel="nofollow">Compositional Chain-of-Thought Prompting for Large Multimodal Models</a></li><li><a href="#Describing_Differences_in_Image_Sets_with_Natural_Language_1332" rel="nofollow">Describing Differences in Image Sets with Natural Language</a></li><li><a href="#Dual_Memory_Networks_A_Versatile_Adaptation_Approach_for_VisionLanguage_Models_1337" rel="nofollow">Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models</a></li><li><a href="#Efficient_Stitchable_Task_Adaptation_1342" rel="nofollow">Efficient Stitchable Task Adaptation</a></li><li><a href="#Efficient_TestTime_Adaptation_of_VisionLanguage_Models_1347" rel="nofollow">Efficient Test-Time Adaptation of Vision-Language Models</a></li><li><a href="#Exploring_the_Transferability_of_Visual_Prompting_for_Multimodal_Large_Language_Models_1352" rel="nofollow">Exploring the Transferability of Visual Prompting for Multimodal Large Language Models</a></li><li><a href="#FairCLIP_Harnessing_Fairness_in_VisionLanguage_Learning_1357" rel="nofollow">FairCLIP: Harnessing Fairness in Vision-Language Learning</a></li><li><a href="#FairDeDup_Detecting_and_Mitigating_VisionLanguage_Fairness_Disparities_in_Semantic_Dataset_Deduplication_1362" rel="nofollow">FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication</a></li><li><a href="#FFF_Fixing_Flawed_Foundations_in_contrastive_pretraining_results_in_very_strong_VisionLanguage_models_1367" rel="nofollow">FFF: Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models</a></li><li><a href="#Generative_Multimodal_Models_are_InContext_Learners_1372" rel="nofollow">Generative Multimodal Models are In-Context Learners</a></li><li><a href="#GLaMM_Pixel_Grounding_Large_Multimodal_Model_1377" rel="nofollow">GLaMM: Pixel Grounding Large Multimodal Model</a></li><li><a href="#GPT4Point_A_Unified_Framework_for_PointLanguage_Understanding_and_Generation_1382" rel="nofollow">GPT4Point: A Unified Framework for Point-Language Understanding and Generation</a></li><li><a href="#InternVL_Scaling_up_Vision_Foundation_Models_and_Aligning_for_Generic_VisualLinguistic_Tasks_1387" rel="nofollow">InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks</a></li><li><a href="#Learning_by_Correction_Efficient_Tuning_Task_for_ZeroShot_Generative_VisionLanguage_Reasoning_1392" rel="nofollow">Learning by Correction: Efficient Tuning Task for Zero-Shot Generative Vision-Language Reasoning</a></li><li><a href="#Lets_Think_Outside_the_Box_Exploring_LeapofThought_in_Large_Language_Models_with_Creative_Humor_Generation_1397" rel="nofollow">Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation</a></li><li><a href="#LION__Empowering_Multimodal_Large_Language_Model_with_DualLevel_Visual_Knowledge_1402" rel="nofollow">LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge</a></li><li><a href="#LL3DA_Visual_Interactive_Instruction_Tuning_for_Omni3D_Understanding_Reasoning_and_Planning_1407" rel="nofollow">LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning</a></li><li><a href="#Mitigating_Object_Hallucinations_in_Large_VisionLanguage_Models_through_Visual_Contrastive_Decoding_1412" rel="nofollow">Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding</a></li><li><a href="#MobileCLIP_Fast_ImageText_Models_through_MultiModal_Reinforced_Training_1417" rel="nofollow">MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training</a></li><li><a href="#MoPECLIP_Structured_Pruning_for_Efficient_VisionLanguage_Models_with_Modulewise_Pruning_Error_Metric_1422" rel="nofollow">MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric</a></li><li><a href="#Narrative_Action_Evaluation_with_PromptGuided_Multimodal_Interaction_1427" rel="nofollow">Narrative Action Evaluation with Prompt-Guided Multimodal Interaction</a></li><li><a href="#OneLLM_One_Framework_to_Align_All_Modalities_with_Language_1432" rel="nofollow">OneLLM: One Framework to Align All Modalities with Language</a></li><li><a href="#One_Prompt_Word_is_Enough_to_Boost_Adversarial_Robustness_for_Pretrained_VisionLanguage_Models_1437" rel="nofollow">One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models</a></li><li><a href="#OPERA_Alleviating_Hallucination_in_MultiModal_Large_Language_Models_via_OverTrust_Penalty_and_RetrospectionAllocation_1442" rel="nofollow">OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation</a></li><li><a href="#Panda70M_Captioning_70M_Videos_with_Multiple_CrossModality_Teachers_1447" rel="nofollow">Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</a></li><li><a href="#PixelLM_Pixel_Reasoning_with_Large_Multimodal_Model_1452" rel="nofollow">PixelLM: Pixel Reasoning with Large Multimodal Model</a></li><li><a href="#PracticalDG_Perturbation_Distillation_on_VisionLanguage_Models_for_Hybrid_Domain_Generalization_1457" rel="nofollow">PracticalDG: Perturbation Distillation on Vision-Language Models for Hybrid Domain Generalization</a></li><li><a href="#Prompt_Highlighter_Interactive_Control_for_MultiModal_LLMs_1462" rel="nofollow">Prompt Highlighter: Interactive Control for Multi-Modal LLMs</a></li><li><a href="#PromptKD_Unsupervised_Prompt_Distillation_for_VisionLanguage_Models_1467" rel="nofollow">PromptKD: Unsupervised Prompt Distillation for Vision-Language Models</a></li><li><a href="#QInstruct_Improving_Lowlevel_Visual_Abilities_for_Multimodality_Foundation_Models_1472" rel="nofollow">Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models</a></li><li><a href="#SCTune_Unleashing_SelfConsistent_Referential_Comprehension_in_Large_Vision_Language_Models_1477" rel="nofollow">SC-Tune: Unleashing Self-Consistent Referential Comprehension in Large Vision Language Models</a></li><li><a href="#SEEDBench_Benchmarking_Multimodal_Large_Language_Models_1482" rel="nofollow">SEED-Bench: Benchmarking Multimodal Large Language Models</a></li><li><a href="#SyncMask_Synchronized_Attentional_Masking_for_Fashioncentric_VisionLanguage_Pretraining_1487" rel="nofollow">SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining</a></li><li><a href="#The_Manga_Whisperer_Automatically_Generating_Transcriptions_for_Comics_1492" rel="nofollow">The Manga Whisperer: Automatically Generating Transcriptions for Comics</a></li><li><a href="#UniBind_LLMAugmented_Unified_and_Balanced_Representation_Space_to_Bind_Them_All_1497" rel="nofollow">UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All</a></li><li><a href="#VBench_Comprehensive_Benchmark_Suite_for_Video_Generative_Models_1502" rel="nofollow">VBench: Comprehensive Benchmark Suite for Video Generative Models</a></li><li><a href="#VideoChat_ChatCentric_Video_Understanding_1507" rel="nofollow">VideoChat: Chat-Centric Video Understanding</a></li><li><a href="#ViPLLaVA_Making_Large_Multimodal_Models_Understand_Arbitrary_Visual_Prompts_1512" rel="nofollow">ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts</a></li><li><a href="#ViTamin_Designing_Scalable_Vision_Models_in_the_Visionlanguage_Era_1517" rel="nofollow">ViTamin: Designing Scalable Vision Models in the Vision-language Era</a></li><li><a href="#ViTLens_Towards_Omnimodal_Representations_1522" rel="nofollow">ViT-Lens: Towards Omni-modal Representations</a></li></ul> 
  </li></ul> 
  </li><li><a href="#8Others_1531" rel="nofollow">8.其他任务(Others)</a></li><li><ul><li><ul><li><a href="#AEROBLADE_TrainingFree_Detection_of_Latent_Diffusion_Images_Using_Autoencoder_Reconstruction_Error_1533" rel="nofollow">AEROBLADE: Training-Free Detection of Latent Diffusion Images Using Autoencoder Reconstruction Error</a></li><li><a href="#DiffBGM_A_Diffusion_Model_for_Video_Background_Music_Generation_1538" rel="nofollow">Diff-BGM: A Diffusion Model for Video Background Music Generation</a></li><li><a href="#EvalCrafter_Benchmarking_and_Evaluating_Large_Video_Generation_Models_1543" rel="nofollow">EvalCrafter: Benchmarking and Evaluating Large Video Generation Models</a></li><li><a href="#On_the_Content_Bias_in_Frchet_Video_Distance_1548" rel="nofollow">On the Content Bias in Fréchet Video Distance</a></li><li><a href="#TexTile_A_Differentiable_Metric_for_Texture_Tileability_1553" rel="nofollow">TexTile: A Differentiable Metric for Texture Tileability</a></li></ul> 
  </li></ul> 
  </li><li><a href="#_1560" rel="nofollow">参考</a></li><li><a href="#_1563" rel="nofollow">相关整理</a></li></ul> 
</div> 
<p></p> 
<h2><a id="AwesomeCVPR2024AIGC_1"></a>Awesome-CVPR2024-AIGC</h2> 
<p>A Collection of Papers and Codes for CVPR2024 AIGC</p> 
<p><strong>整理汇总下今年CVPR AIGC相关的论文和代码，具体如下。</strong></p> 
<p><strong>欢迎star，fork和PR~</strong><br> <strong>优先在Github更新</strong>：<a href="https://github.com/Kobaayyy/Awesome-CVPR2024-AIGC/blob/main/README.md">Awesome-CVPR2024-AIGC</a>，欢迎star~<br> <strong>知乎</strong>：<a href="https://zhuanlan.zhihu.com/p/684325134" rel="nofollow">https://zhuanlan.zhihu.com/p/684325134</a></p> 
<p><strong>参考或转载请注明出处</strong></p> 
<p>CVPR2024官网：<a href="https://cvpr.thecvf.com/Conferences/2024" rel="nofollow">https://cvpr.thecvf.com/Conferences/2024</a></p> 
<p>CVPR接收论文列表：https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers</p> 
<p>CVPR完整论文库：https://openaccess.thecvf.com/CVPR2024</p> 
<p>开会时间：2024年6月17日-6月21日</p> 
<p>论文接收公布时间：2024年2月27日</p> 
<p><strong>【Contents】</strong></p> 
<ul><li><a href="#1.%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90" rel="nofollow">1.图像生成(Image Generation/Image Synthesis)</a></li><li><a href="#2.%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91" rel="nofollow">2.图像编辑（Image Editing)</a></li><li><a href="#3.%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90" rel="nofollow">3.视频生成(Video Generation/Image Synthesis)</a></li><li><a href="#4.%E8%A7%86%E9%A2%91%E7%BC%96%E8%BE%91" rel="nofollow">4.视频编辑(Video Editing)</a></li><li><a href="#5.3D%E7%94%9F%E6%88%90" rel="nofollow">5.3D生成(3D Generation/3D Synthesis)</a></li><li><a href="#6.3D%E7%BC%96%E8%BE%91" rel="nofollow">6.3D编辑(3D Editing)</a></li><li><a href="#7.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B" rel="nofollow">7.多模态大语言模型(Multi-Modal Large Language Model)</a></li><li><a href="#8.%E5%85%B6%E4%BB%96" rel="nofollow">8.其他多任务(Others)</a></li></ul> 
<p></p> 
<h2><a id="1Image_GenerationImage_Synthesis_40"></a>1.图像生成(Image Generation/Image Synthesis)</h2> 
<h4><a id="Accelerating_Diffusion_Sampling_with_Optimized_Time_Steps_42"></a>Accelerating Diffusion Sampling with Optimized Time Steps</h4> 
<ul><li>Paper: https://arxiv.org/abs/2402.17376</li><li>Code: https://github.com/scxue/DM-NonUniform</li></ul> 
<h4><a id="Adversarial_Text_to_Continuous_Image_Generation_47"></a>Adversarial Text to Continuous Image Generation</h4> 
<ul><li>Paper: https://openreview.net/forum?id=9X3UZJSGIg9</li><li>Code:</li></ul> 
<h4><a id="Amodal_Completion_via_Progressive_Mixed_Context_Diffusion_52"></a>Amodal Completion via Progressive Mixed Context Diffusion</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.15540</li><li>Code: https://github.com/k8xu/amodal</li></ul> 
<h4><a id="ArbitraryScale_Image_Generation_and_Upsampling_using_Latent_Diffusion_Model_and_Implicit_Neural_Decoder_57"></a>Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural Decoder</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.10255</li><li>Code: https://github.com/zhenshij/arbitrary-scale-diffusion</li></ul> 
<h4><a id="Atlantis_Enabling_Underwater_Depth_Estimation_with_Stable_Diffusion_62"></a>Atlantis: Enabling Underwater Depth Estimation with Stable Diffusion</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.12471</li><li>Code: https://github.com/zkawfanx/Atlantis</li></ul> 
<h4><a id="Attention_Calibration_for_Disentangled_TexttoImage_Personalization_67"></a>Attention Calibration for Disentangled Text-to-Image Personalization</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.18551</li><li>Code: https://github.com/Monalissaa/DisenDiff</li></ul> 
<h4><a id="AttentionDriven_TrainingFree_Efficiency_Enhancement_of_Diffusion_Models_72"></a>Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2405.05252</li><li>Code:</li></ul> 
<h4><a id="CapHuman_Capture_Your_Moments_in_Parallel_Universes_77"></a>CapHuman: Capture Your Moments in Parallel Universes</h4> 
<ul><li>Paper: https://arxiv.org/abs/2402.18078</li><li>Code: https://github.com/VamosC/CapHuman</li></ul> 
<h4><a id="CHAIN_Enhancing_Generalization_in_DataEfficient_GANs_via_lipsCHitz_continuity_constrAIned_Normalization_82"></a>CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.00521</li><li>Code:</li></ul> 
<h4><a id="Check_Locate_Rectify_A_TrainingFree_Layout_Calibration_System_for_TexttoImage_Generation_87"></a>Check, Locate, Rectify: A Training-Free Layout Calibration System for Text-to-Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.15773</li><li>Code:</li></ul> 
<h4><a id="CoarsetoFine_Latent_Diffusion_for_PoseGuided_Person_Image_Synthesis_92"></a>Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis</h4> 
<ul><li>Paper: https://arxiv.org/abs/2402.00627</li><li>Code: https://github.com/YanzuoLu/CFLD</li></ul> 
<h4><a id="CoDi_Conditional_Diffusion_Distillation_for_HigherFidelity_and_Faster_Image_Generation_97"></a>CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2310.01407</li><li>Code: https://github.com/fast-codi/CoDi</li></ul> 
<h4><a id="ConditionAware_Neural_Network_for_Controlled_Image_Generation_102"></a>Condition-Aware Neural Network for Controlled Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.01143v1</li><li>Code:</li></ul> 
<h4><a id="CosmicMan_A_TexttoImage_Foundation_Model_for_Humans_107"></a>CosmicMan: A Text-to-Image Foundation Model for Humans</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.01294</li><li>Code: https://github.com/cosmicman-cvpr2024/CosmicMan</li></ul> 
<h4><a id="Countering_Personalized_TexttoImage_Generation_with_Influence_Watermarks_112"></a>Countering Personalized Text-to-Image Generation with Influence Watermarks</h4> 
<ul><li>Paper: https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Countering_Personalized_Text-to-Image_Generation_with_Influence_Watermarks_CVPR_2024_paper.html</li><li>Code:</li></ul> 
<h4><a id="Cross_Initialization_for_Face_Personalization_of_TexttoImage_Models_117"></a>Cross Initialization for Face Personalization of Text-to-Image Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.15905</li><li>Code: https://github.com/lyuPang/CrossInitialization</li></ul> 
<h4><a id="Customization_Assistant_for_Texttoimage_Generation_122"></a>Customization Assistant for Text-to-image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.03045</li><li>Code:</li></ul> 
<h4><a id="DeepCache_Accelerating_Diffusion_Models_for_Free_127"></a>DeepCache: Accelerating Diffusion Models for Free</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.00858</li><li>Code: https://github.com/horseee/DeepCache</li></ul> 
<h4><a id="DemoFusion_Democratising_HighResolution_Image_Generation_With_No__132"></a>DemoFusion: Democratising High-Resolution Image Generation With No $</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.16973</li><li>Code: https://github.com/PRIS-CV/DemoFusion</li></ul> 
<h4><a id="Desigen_A_Pipeline_for_Controllable_Design_Template_Generation_137"></a>Desigen: A Pipeline for Controllable Design Template Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.09093</li><li>Code: https://github.com/whaohan/desigen</li></ul> 
<h4><a id="DiffAgent_Fast_and_Accurate_TexttoImage_API_Selection_with_Large_Language_Model_142"></a>DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.01342</li><li>Code: https://github.com/OpenGVLab/DiffAgent</li></ul> 
<h4><a id="Diffusiondriven_GAN_Inversion_for_MultiModal_Face_Image_Generation_147"></a>Diffusion-driven GAN Inversion for Multi-Modal Face Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2405.04356v1</li><li>Code:</li></ul> 
<h4><a id="DistriFusion_Distributed_Parallel_Inference_for_HighResolution_Diffusion_Models_152"></a>DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2402.19481</li><li>Code: https://github.com/mit-han-lab/distrifuser</li></ul> 
<h4><a id="Diversityaware_Channel_Pruning_for_StyleGAN_Compression_157"></a>Diversity-aware Channel Pruning for StyleGAN Compression</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.13548</li><li>Code: https://github.com/jiwoogit/DCP-GAN</li></ul> 
<h4><a id="Discriminative_Probing_and_Tuning_for_TexttoImage_Generation_162"></a>Discriminative Probing and Tuning for Text-to-Image Generation</h4> 
<ul><li>Paper: https://www.arxiv.org/abs/2403.04321</li><li>Code: https://github.com/LgQu/DPT-T2I</li></ul> 
<h4><a id="Dont_drop_your_samples_Coherenceaware_training_benefits_Conditional_diffusion_167"></a>Don’t drop your samples! Coherence-aware training benefits Conditional diffusion</h4> 
<ul><li>Paper: https://arxiv.org/abs/2405.20324</li><li>Code: https://github.com/nicolas-dufour/CAD</li></ul> 
<h4><a id="Drag_Your_Noise_Interactive_Pointbased_Editing_via_Diffusion_Semantic_Propagation_172"></a>Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic Propagation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.01050</li><li>Code: https://github.com/haofengl/DragNoise</li></ul> 
<h4><a id="DreamMatcher_Appearance_Matching_SelfAttention_for_SemanticallyConsistent_TexttoImage_Personalization_177"></a>DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization</h4> 
<ul><li>Paper: https://arxiv.org/abs/2402.09812</li><li>Code: https://github.com/KU-CVLAB/DreamMatcher</li></ul> 
<h4><a id="Dynamic_Prompt_Optimizing_for_TexttoImage_Generation_182"></a>Dynamic Prompt Optimizing for Text-to-Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.04095</li><li>Code: https://github.com/Mowenyii/PAE</li></ul> 
<h4><a id="ECLIPSE_A_ResourceEfficient_TexttoImage_Prior_for_Image_Generations_187"></a>ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.04655</li><li>Code: https://github.com/eclipse-t2i/eclipse-inference</li></ul> 
<h4><a id="Efficient_Dataset_Distillation_via_Minimax_Diffusion_192"></a>Efficient Dataset Distillation via Minimax Diffusion</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.15529</li><li>Code: https://github.com/vimar-gu/MinimaxDiffusion</li></ul> 
<h4><a id="ElasticDiffusion_Trainingfree_Arbitrary_Size_Image_Generation_197"></a>ElasticDiffusion: Training-free Arbitrary Size Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.18822</li><li>Code: https://github.com/MoayedHajiAli/ElasticDiffusion-official</li></ul> 
<h4><a id="EmoGen_Emotional_Image_Content_Generation_with_TexttoImage_Diffusion_Models_202"></a>EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2401.04608</li><li>Code: https://github.com/JingyuanYY/EmoGen</li></ul> 
<h4><a id="Enabling_MultiConcept_Fusion_in_TexttoImage_Models_207"></a>Enabling Multi-Concept Fusion in Text-to-Image Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.03913v1</li><li>Code:</li></ul> 
<h4><a id="Exact_Fusion_via_Feature_Distribution_Matching_for_Fewshot_Image_Generation_212"></a>Exact Fusion via Feature Distribution Matching for Few-shot Image Generation</h4> 
<ul><li>Paper: https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Exact_Fusion_via_Feature_Distribution_Matching_for_Few-shot_Image_Generation_CVPR_2024_paper.html</li><li>Code:</li></ul> 
<h4><a id="FaceChainSuDe_Building_Derived_Class_to_Inherit_Category_Attributes_for_Oneshot_SubjectDriven_Generation_217"></a>FaceChain-SuDe: Building Derived Class to Inherit Category Attributes for One-shot Subject-Driven Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.06775</li><li>Code:</li></ul> 
<h4><a id="Fast_ODEbased_Sampling_for_Diffusion_Models_in_Around_5_Steps_222"></a>Fast ODE-based Sampling for Diffusion Models in Around 5 Steps</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.00094</li><li>Code: https://github.com/zju-pi/diff-sampler</li></ul> 
<h4><a id="FreeControl_TrainingFree_Spatial_Control_of_Any_TexttoImage_Diffusion_Model_with_Any_Condition_227"></a>FreeControl: Training-Free Spatial Control of Any Text-to-Image Diffusion Model with Any Condition</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.07536</li><li>Code: https://github.com/genforce/freecontrol</li></ul> 
<h4><a id="FreeCustom_TuningFree_Customized_Image_Generation_for_MultiConcept_Composition_232"></a>FreeCustom: Tuning-Free Customized Image Generation for Multi-Concept Composition</h4> 
<ul><li>Paper: https://arxiv.org/abs/2405.13870</li><li>Code: https://github.com/aim-uofa/FreeCustom</li></ul> 
<h4><a id="Generalizable_Tumor_Synthesis_237"></a>Generalizable Tumor Synthesis</h4> 
<ul><li>Paper: https://www.cs.jhu.edu/~alanlab/Pubs24/chen2024towards.pdf</li><li>Code: https://github.com/MrGiovanni/DiffTumor</li></ul> 
<h4><a id="Generating_Daylightdriven_Architectural_Design_via_Diffusion_Models_242"></a>Generating Daylight-driven Architectural Design via Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.13353</li><li>Code: https://github.com/unlimitedli/DDADesign</li></ul> 
<h4><a id="Generative_Unlearning_for_Any_Identity_247"></a>Generative Unlearning for Any Identity</h4> 
<ul><li>Paper: https://arxiv.org/abs/2405.09879</li><li>Code: https://github.com/JJuOn/GUIDE</li></ul> 
<h4><a id="HanDiffuser_TexttoImage_Generation_With_Realistic_Hand_Appearances_252"></a>HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.01693</li><li>Code: https://github.com/JJuOn/GUIDE</li></ul> 
<h4><a id="Highfidelity_Personcentric_SubjecttoImage_Synthesis_258"></a>High-fidelity Person-centric Subject-to-Image Synthesis</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.10329</li><li>Code: https://github.com/CodeGoat24/Face-diffuser?tab=readme-ov-file</li></ul> 
<h4><a id="InitNO_Boosting_TexttoImage_Diffusion_Models_via_Initial_Noise_Optimization_263"></a>InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise Optimization</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.04650</li><li>Code: https://github.com/xiefan-guo/initno</li></ul> 
<h4><a id="InstantBooth_Personalized_TexttoImage_Generation_without_TestTime_Finetuning_268"></a>InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning</h4> 
<ul><li>Paper: https://arxiv.org/abs/2304.03411</li><li>Code:</li></ul> 
<h4><a id="InstanceDiffusion_Instancelevel_Control_for_Image_Generation_273"></a>InstanceDiffusion: Instance-level Control for Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2402.03290</li><li>Code: https://github.com/frank-xwang/InstanceDiffusion</li></ul> 
<h4><a id="InstructImagen_Image_Generation_with_Multimodal_Instruction_278"></a>Instruct-Imagen: Image Generation with Multi-modal Instruction</h4> 
<ul><li>Paper: https://arxiv.org/abs/2401.01952</li><li>Code:</li></ul> 
<h4><a id="Intelligent_Grimm__Openended_Visual_Storytelling_via_Latent_Diffusion_Models_283"></a>Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2306.00973</li><li>Code: https://github.com/haoningwu3639/StoryGen</li></ul> 
<h4><a id="InteractDiffusion_InteractionControl_for_TexttoImage_Diffusion_Model_288"></a>InteractDiffusion: Interaction-Control for Text-to-Image Diffusion Model</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.05849</li><li>Code: https://github.com/jiuntian/interactdiffusion</li></ul> 
<h4><a id="Intriguing_Properties_of_Diffusion_Models_An_Empirical_Study_of_the_Natural_Attack_Capability_in_TexttoImage_Generative_Models_293"></a>Intriguing Properties of Diffusion Models: An Empirical Study of the Natural Attack Capability in Text-to-Image Generative Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2308.15692</li><li>Code:</li></ul> 
<h4><a id="InversionFree_Image_Editing_with_Natural_Language_298"></a>Inversion-Free Image Editing with Natural Language</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.04965</li><li>Code: https://github.com/sled-group/InfEdit</li></ul> 
<h4><a id="JeDi_JointImage_Diffusion_Models_for_FinetuningFree_Personalized_TexttoImage_Generation_303"></a>JeDi: Joint-Image Diffusion Models for Finetuning-Free Personalized Text-to-Image Generation</h4> 
<ul><li>Paper: https://openaccess.thecvf.com/content/CVPR2024/html/Zeng_JeDi_Joint-Image_Diffusion_Models_for_Finetuning-Free_Personalized_Text-to-Image_Generation_CVPR_2024_paper.html</li><li>Code:</li></ul> 
<h4><a id="LAKERED_Camouflaged_Images_Generation_by_Latent_Background_Knowledge_RetrievalAugmented_Diffusion_308"></a>LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge Retrieval-Augmented Diffusion</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.00292</li><li>Code: https://github.com/PanchengZhao/LAKE-RED</li></ul> 
<h4><a id="Learned_representationguided_diffusion_models_for_largeimage_generation_313"></a>Learned representation-guided diffusion models for large-image generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.07330</li><li>Code: https://github.com/cvlab-stonybrook/Large-Image-Diffusion</li></ul> 
<h4><a id="Learning_Continuous_3D_Words_for_TexttoImage_Generation_318"></a>Learning Continuous 3D Words for Text-to-Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2402.08654</li><li>Code: https://github.com/ttchengab/continuous_3d_words_code/</li></ul> 
<h4><a id="Learning_Disentangled_Identifiers_for_ActionCustomized_TexttoImage_Generation_323"></a>Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.15841</li><li>Code:</li></ul> 
<h4><a id="Learning_Multidimensional_Human_Preference_for_TexttoImage_Generation_328"></a>Learning Multi-dimensional Human Preference for Text-to-Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.15841</li><li>Code:</li></ul> 
<h4><a id="LeftRefill_Filling_Right_Canvas_based_on_Left_Reference_through_Generalized_TexttoImage_Diffusion_Model_333"></a>LeftRefill: Filling Right Canvas based on Left Reference through Generalized Text-to-Image Diffusion Model</h4> 
<ul><li>Paper: https://arxiv.org/abs/2305.11577</li><li>Code: https://github.com/ewrfcas/LeftRefill</li></ul> 
<h4><a id="MACE_Mass_Concept_Erasure_in_Diffusion_Models_338"></a>MACE: Mass Concept Erasure in Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2402.05408</li><li>Code: https://github.com/Shilin-LU/MACE</li></ul> 
<h4><a id="MarkovGen_Structured_Prediction_for_Efficient_TexttoImage_Generation_343"></a>MarkovGen: Structured Prediction for Efficient Text-to-Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2308.10997</li><li>Code:</li></ul> 
<h4><a id="MedM2G_Unifying_Medical_MultiModal_Generation_via_CrossGuided_Diffusion_with_Visual_Invariant_348"></a>MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided Diffusion with Visual Invariant</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.04290</li><li>Code:</li></ul> 
<h4><a id="MIGC_MultiInstance_Generation_Controller_for_TexttoImage_Synthesis_353"></a>MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis</h4> 
<ul><li>Paper: https://arxiv.org/abs/2402.05408</li><li>Code: https://github.com/limuloo/MIGC</li></ul> 
<h4><a id="MindBridge_A_CrossSubject_Brain_Decoding_Framework_358"></a>MindBridge: A Cross-Subject Brain Decoding Framework</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.07850</li><li>Code: https://github.com/littlepure2333/MindBridge</li></ul> 
<h4><a id="MULAN_A_Multi_Layer_Annotated_Dataset_for_Controllable_TexttoImage_Generation_363"></a>MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.02790</li><li>Code: https://huggingface.co/datasets/mulan-dataset/v1.0</li></ul> 
<h4><a id="On_the_Scalability_of_Diffusionbased_TexttoImage_Generation_368"></a>On the Scalability of Diffusion-based Text-to-Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.02883</li><li>Code:</li></ul> 
<h4><a id="OpenBias_Openset_Bias_Detection_in_TexttoImage_Generative_Models_373"></a>OpenBias: Open-set Bias Detection in Text-to-Image Generative Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.07990</li><li>Code: https://github.com/Picsart-AI-Research/OpenBias</li></ul> 
<h4><a id="Personalized_Residuals_for_ConceptDriven_TexttoImage_Generation_378"></a>Personalized Residuals for Concept-Driven Text-to-Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2405.12978</li><li>Code:</li></ul> 
<h4><a id="Perturbing_Attention_Gives_You_More_Bang_for_the_Buck_Subtle_Imaging_Perturbations_That_Efficiently_Fool_Customized_Diffusion_Models_383"></a>Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging Perturbations That Efficiently Fool Customized Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.15081</li><li>Code:</li></ul> 
<h4><a id="PhotoMaker_Customizing_Realistic_Human_Photos_via_Stacked_ID_Embedding_388"></a>PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.04461</li><li>Code: https://github.com/TencentARC/PhotoMaker</li></ul> 
<h4><a id="PLACE_Adaptive_LayoutSemantic_Fusion_for_Semantic_Image_Synthesis_393"></a>PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.01852</li><li>Code: https://github.com/cszy98/PLACE</li></ul> 
<h4><a id="PlugandPlay_Diffusion_Distillation_398"></a>Plug-and-Play Diffusion Distillation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2406.01954</li><li>Code:</li></ul> 
<h4><a id="PromptFree_Diffusion_Taking_Text_out_of_TexttoImage_Diffusion_Models_403"></a>Prompt-Free Diffusion: Taking “Text” out of Text-to-Image Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2305.16223</li><li>Code: https://github.com/SHI-Labs/Prompt-Free-Diffusion</li></ul> 
<h4><a id="Ranni_Taming_TexttoImage_Diffusion_for_Accurate_Instruction_Following_408"></a>Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.17002</li><li>Code: https://github.com/ali-vilab/Ranni</li></ul> 
<h4><a id="Readout_Guidance_Learning_Control_from_Diffusion_Features_413"></a>Readout Guidance: Learning Control from Diffusion Features</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.02150</li><li>Code: https://github.com/google-research/readout_guidance</li></ul> 
<h4><a id="Relation_Rectification_in_Diffusion_Model_418"></a>Relation Rectification in Diffusion Model</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.20249</li><li>Code: https://github.com/WUyinwei-hah/RRNet</li></ul> 
<h4><a id="Residual_Denoising_Diffusion_Models_423"></a>Residual Denoising Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2308.13712</li><li>Code: https://github.com/nachifur/RDDM</li></ul> 
<h4><a id="Rethinking_FID_Towards_a_Better_Evaluation_Metric_for_Image_Generation_428"></a>Rethinking FID: Towards a Better Evaluation Metric for Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2401.09603</li><li>Code: https://github.com/google-research/google-research/tree/master/cmmd</li></ul> 
<h4><a id="Rethinking_the_Spatial_Inconsistency_in_ClassifierFree_Diffusion_Guidance_433"></a>Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.05384</li><li>Code: https://github.com/SmilesDZgk/S-CFG</li></ul> 
<h4><a id="RetrievalAugmented_Layout_Transformer_for_ContentAware_Layout_Generation_438"></a>Retrieval-Augmented Layout Transformer for Content-Aware Layout Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.13602</li><li>Code: https://github.com/CyberAgentAILab/RALF</li></ul> 
<h4><a id="Rich_Human_Feedback_for_TexttoImage_Generation_443"></a>Rich Human Feedback for Text-to-Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.10240</li><li>Code:</li></ul> 
<h4><a id="SCoFT_SelfContrastive_FineTuning_for_Equitable_Image_Generation_448"></a>SCoFT: Self-Contrastive Fine-Tuning for Equitable Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2401.08053</li><li>Code:</li></ul> 
<h4><a id="Selfcorrecting_LLMcontrolled_Diffusion_Models_453"></a>Self-correcting LLM-controlled Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.16090</li><li>Code: https://github.com/tsunghan-wu/SLD</li></ul> 
<h4><a id="SelfDiscovering_Interpretable_Diffusion_Latent_Directions_for_Responsible_TexttoImage_Generation_458"></a>Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.17216</li><li>Code: https://github.com/hangligit/InterpretDiffusion</li></ul> 
<h4><a id="Shadow_Generation_for_Composite_Image_Using_Diffusion_Model_463"></a>Shadow Generation for Composite Image Using Diffusion Model</h4> 
<ul><li>Paper: https://arxiv.org/abs/2308.09972</li><li>Code: https://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBAv2</li></ul> 
<h4><a id="Smooth_Diffusion_Crafting_Smooth_Latent_Spaces_in_Diffusion_Models_468"></a>Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.04410</li><li>Code: https://github.com/SHI-Labs/Smooth-Diffusion</li></ul> 
<h4><a id="SSREncoder_Encoding_Selective_Subject_Representation_for_SubjectDriven_Generation_473"></a>SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.16272</li><li>Code: https://github.com/Xiaojiu-z/SSR_Encoder</li></ul> 
<h4><a id="StableVITON_Learning_Semantic_Correspondence_with_Latent_Diffusion_Model_for_Virtual_TryOn_478"></a>StableVITON: Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.01725</li><li>Code: https://github.com/rlawjdghek/StableVITON</li></ul> 
<h4><a id="StructureGuided_Adversarial_Training_of_Diffusion_Models_483"></a>Structure-Guided Adversarial Training of Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2402.17563</li><li>Code:</li></ul> 
<h4><a id="Style_Aligned_Image_Generation_via_Shared_Attention_488"></a>Style Aligned Image Generation via Shared Attention</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.02133</li><li>Code: https://github.com/google/style-aligned/</li></ul> 
<h4><a id="SVGDreamer_Text_Guided_SVG_Generation_with_Diffusion_Model_493"></a>SVGDreamer: Text Guided SVG Generation with Diffusion Model</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.16476</li><li>Code: https://github.com/ximinng/SVGDreamer</li></ul> 
<h4><a id="SwiftBrush_OneStep_TexttoImage_Diffusion_Model_with_Variational_Score_Distillation_498"></a>SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.05239</li><li>Code: https://github.com/VinAIResearch/SwiftBrush</li></ul> 
<h4><a id="Tailored_Visions_Enhancing_TexttoImage_Generation_with_Personalized_Prompt_Rewriting_503"></a>Tailored Visions: Enhancing Text-to-Image Generation with Personalized Prompt Rewriting</h4> 
<ul><li>Paper: https://arxiv.org/abs/2310.08129</li><li>Code: https://github.com/zzjchen/Tailored-Visions</li></ul> 
<h4><a id="Tackling_the_Singularities_at_the_Endpoints_of_Time_Intervals_in_Diffusion_Models_508"></a>Tackling the Singularities at the Endpoints of Time Intervals in Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.08381</li><li>Code: https://github.com/PangzeCheung/SingDiffusion</li></ul> 
<h4><a id="Taming_Stable_Diffusion_for_Text_to_360_Panorama_Image_Generation_513"></a>Taming Stable Diffusion for Text to 360∘ Panorama Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.07949</li><li>Code: https://github.com/chengzhag/PanFusion</li></ul> 
<h4><a id="TextCraftor_Your_Text_Encoder_Can_be_Image_Quality_Controller_518"></a>TextCraftor: Your Text Encoder Can be Image Quality Controller</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.18978</li><li>Code:</li></ul> 
<h4><a id="TextGuided_Variational_Image_Generation_for_Industrial_Anomaly_Detection_and_Segmentation_523"></a>Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.06247</li><li>Code: https://github.com/MingyuLee82/TGI_AD_v1</li></ul> 
<h4><a id="TFMQDM_Temporal_Feature_Maintenance_Quantization_for_Diffusion_Models_528"></a>TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.16503</li><li>Code: https://github.com/ModelTC/TFMQ-DM</li></ul> 
<h4><a id="TokenCompose_Grounding_Diffusion_with_Tokenlevel_Supervision_533"></a>TokenCompose: Grounding Diffusion with Token-level Supervision</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.03626</li><li>Code: https://github.com/mlpc-ucsd/TokenCompose</li></ul> 
<h4><a id="Towards_Accurate_Posttraining_Quantization_for_Diffusion_Models_538"></a>Towards Accurate Post-training Quantization for Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2305.18723</li><li>Code: https://github.com/ChangyuanWang17/APQ-DM</li></ul> 
<h4><a id="Towards_Effective_Usage_of_HumanCentric_Priors_in_Diffusion_Models_for_Textbased_Human_Image_Generation_543"></a>Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.05239</li><li>Code:</li></ul> 
<h4><a id="Towards_MemorizationFree_Diffusion_Models_548"></a>Towards Memorization-Free Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.00922</li><li>Code: https://github.com/chenchen-usyd/AMG</li></ul> 
<h4><a id="Training_Diffusion_Models_Towards_Diverse_Image_Generation_with_Reinforcement_Learning_553"></a>Training Diffusion Models Towards Diverse Image Generation with Reinforcement Learning</h4> 
<ul><li>Paper: https://openaccess.thecvf.com/content/CVPR2024/html/Miao_Training_Diffusion_Models_Towards_Diverse_Image_Generation_with_Reinforcement_Learning_CVPR_2024_paper.html</li><li>Code:</li></ul> 
<h4><a id="UFOGen_You_Forward_Once_Large_Scale_TexttoImage_Generation_via_Diffusion_GANs_558"></a>UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.09257</li><li>Code:</li></ul> 
<h4><a id="UniGS_Unified_Representation_for_Image_Generation_and_Segmentation_563"></a>UniGS: Unified Representation for Image Generation and Segmentation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.01985</li><li>Code: https://github.com/qqlu/Entity</li></ul> 
<h4><a id="Using_Human_Feedback_to_Finetune_Diffusion_Models_without_Any_Reward_Model_568"></a>Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.13231</li><li>Code: https://github.com/yk7333/d3po</li></ul> 
<h4><a id="UVAP_Userspecified_Visual_Appearance_Personalization_via_Decoupled_Self_Augmentation_573"></a>U-VAP: User-specified Visual Appearance Personalization via Decoupled Self Augmentation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.20231</li><li>Code: https://github.com/ICTMCG/U-VAP</li></ul> 
<h4><a id="ViewDiff_3DConsistent_Image_Generation_with_TextToImage_Models_578"></a>ViewDiff: 3D-Consistent Image Generation with Text-To-Image Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.01807</li><li>Code: https://github.com/facebookresearch/ViewDiff</li></ul> 
<h4><a id="When_StyleGAN_Meets_Stable_Diffusion_a__Adapter_for_Personalized_Image_Generation_583"></a>When StyleGAN Meets Stable Diffusion: a 𝒲+ Adapter for Personalized Image Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.17461</li><li>Code: https://github.com/csxmli2016/w-plus-adapter</li></ul> 
<h4><a id="XAdapter_Adding_Universal_Compatibility_of_Plugins_for_Upgraded_Diffusion_Model_588"></a>X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion Model</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.02238</li><li>Code: https://github.com/showlab/X-Adapter</li></ul> 
<p></p> 
<h2><a id="2Image_Editing_596"></a>2.图像编辑(Image Editing)</h2> 
<h4><a id="An_Edit_Friendly_DDPM_Noise_Space_Inversion_and_Manipulations_598"></a>An Edit Friendly DDPM Noise Space: Inversion and Manipulations</h4> 
<ul><li>Paper: https://arxiv.org/abs/2304.06140</li><li>Code: https://github.com/inbarhub/DDPM_inversion</li></ul> 
<h4><a id="Choose_What_You_Need_Disentangled_Representation_Learning_for_Scene_Text_Recognition_Removal_and_Editing_603"></a>Choose What You Need: Disentangled Representation Learning for Scene Text Recognition, Removal and Editing</h4> 
<ul><li>Paper: https://arxiv.org/abs/2405.04377</li><li>Code:</li></ul> 
<h4><a id="ContentStyle_Decoupling_for_Unsupervised_Makeup_Transfer_without_Generating_Pseudo_Ground_Truth_608"></a>Content-Style Decoupling for Unsupervised Makeup Transfer without Generating Pseudo Ground Truth</h4> 
<ul><li>Paper: https://openaccess.thecvf.com/content/CVPR2024/html/Sun_Content-Style_Decoupling_for_Unsupervised_Makeup_Transfer_without_Generating_Pseudo_Ground_CVPR_2024_paper.html</li><li>Code: https://github.com/Snowfallingplum/CSD-MT</li></ul> 
<h4><a id="Contrastive_Denoising_Score_for_Textguided_Latent_Diffusion_Image_Editing_613"></a>Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.18608</li><li>Code: https://github.com/HyelinNAM/ContrastiveDenoisingScore</li></ul> 
<h4><a id="DEADiff_An_Efficient_Stylization_Diffusion_Model_with_Disentangled_Representations_618"></a>DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.06951</li><li>Code: https://github.com/Tianhao-Qi/DEADiff_code</li></ul> 
<h4><a id="Deformable_Oneshot_Face_Stylization_via_DINO_Semantic_Guidance_623"></a>Deformable One-shot Face Stylization via DINO Semantic Guidance</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.00459</li><li>Code: https://github.com/zichongc/DoesFS</li></ul> 
<h4><a id="DemoCaricature_Democratising_Caricature_Generation_with_a_Rough_Sketch_628"></a>DemoCaricature: Democratising Caricature Generation with a Rough Sketch</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.04364</li><li>Code: https://github.com/ChenDarYen/DemoCaricature</li></ul> 
<h4><a id="DiffAM_Diffusionbased_Adversarial_Makeup_Transfer_for_Facial_Privacy_Protection_633"></a>DiffAM: Diffusion-based Adversarial Makeup Transfer for Facial Privacy Protection</h4> 
<ul><li>Paper: https://openaccess.thecvf.com/content/CVPR2024/html/Sun_DiffAM_Diffusion-based_Adversarial_Makeup_Transfer_for_Facial_Privacy_Protection_CVPR_2024_paper.html</li><li>Code: https://github.com/HansSunY/DiffAM</li></ul> 
<h4><a id="DiffMorpher_Unleashing_the_Capability_of_Diffusion_Models_for_Image_Morphing_638"></a>DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.07409</li><li>Code: https://github.com/Kevin-thu/DiffMorpher</li></ul> 
<h4><a id="Diffusion_Handles_Enabling_3D_Edits_for_Diffusion_Models_by_Lifting_Activations_to_3D_643"></a>Diffusion Handles: Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.02190</li><li>Code: https://github.com/adobe-research/DiffusionHandles</li></ul> 
<h4><a id="DiffusionLight_Light_Probes_for_Free_by_Painting_a_Chrome_Ball_648"></a>DiffusionLight: Light Probes for Free by Painting a Chrome Ball</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.09168</li><li>Code: https://github.com/DiffusionLight/DiffusionLight</li></ul> 
<h4><a id="Diffusion_Models_Without_Attention_653"></a>Diffusion Models Without Attention</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.18257</li><li>Code: https://github.com/Kevin-thu/DiffMorpher</li></ul> 
<h4><a id="Doubly_Abductive_Counterfactual_Inference_for_Textbased_Image_Editing_658"></a>Doubly Abductive Counterfactual Inference for Text-based Image Editing</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.02981</li><li>Code: https://github.com/xuesong39/DAC</li></ul> 
<h4><a id="Edit_One_for_All_Interactive_Batch_Image_Editing_663"></a>Edit One for All: Interactive Batch Image Editing</h4> 
<ul><li>Paper: https://arxiv.org/abs/2401.10219</li><li>Code: https://github.com/thaoshibe/edit-one-for-all</li></ul> 
<h4><a id="Face2Diffusion_for_Fast_and_Editable_Face_Personalization_668"></a>Face2Diffusion for Fast and Editable Face Personalization</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.05094</li><li>Code: https://github.com/mapooon/Face2Diffusion</li></ul> 
<h4><a id="Focus_on_Your_Instruction_Finegrained_and_Multiinstruction_Image_Editing_by_Attention_Modulation_673"></a>Focus on Your Instruction: Fine-grained and Multi-instruction Image Editing by Attention Modulation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.10113</li><li>Code: https://github.com/guoqincode/Focus-on-Your-Instruction</li></ul> 
<h4><a id="FreeDrag_Feature_Dragging_for_Reliable_Pointbased_Image_Editing_678"></a>FreeDrag: Feature Dragging for Reliable Point-based Image Editing</h4> 
<ul><li>Paper: https://arxiv.org/abs/2307.04684</li><li>Code: https://github.com/LPengYang/FreeDrag</li></ul> 
<h4><a id="HoloRelighting_Controllable_Volumetric_Portrait_Relighting_from_a_Single_Image_683"></a>Holo-Relighting: Controllable Volumetric Portrait Relighting from a Single Image</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.09632</li><li>Code: https://github.com/guoqincode/Focus-on-Your-Instruction</li></ul> 
<h4><a id="Image_Sculpting_Precise_Object_Editing_with_3D_Geometry_Control_688"></a>Image Sculpting: Precise Object Editing with 3D Geometry Control</h4> 
<ul><li>Paper: https://arxiv.org/abs/2401.01702</li><li>Code: https://github.com/vision-x-nyu/image-sculpting</li></ul> 
<h4><a id="InversionFree_Image_Editing_with_Natural_Language_693"></a>Inversion-Free Image Editing with Natural Language</h4> 
<ul><li>Paper: hhttps://arxiv.org/abs/2312.04965</li><li>Code: https://github.com/sled-group/InfEdit</li></ul> 
<h4><a id="PAIRDiffusion_ObjectLevel_Image_Editing_with_StructureandAppearance_Paired_Diffusion_Models_698"></a>PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2303.17546</li><li>Code: https://github.com/Picsart-AI-Research/PAIR-Diffusion</li></ul> 
<h4><a id="Person_in_Place_Generating_Associative_SkeletonGuidance_Maps_for_HumanObject_Interaction_Image_Editing_703"></a>Person in Place: Generating Associative Skeleton-Guidance Maps for Human-Object Interaction Image Editing</h4> 
<ul><li>Paper: https://arxiv.org/abs/2303.17546</li><li>Code: https://github.com/YangChangHee/CVPR2024_Person-In-Place_RELEASE?tab=readme-ov-file</li></ul> 
<h4><a id="PuffNet_Efficient_Style_Transfer_with_Pure_Content_and_Style_Feature_Fusion_Network_708"></a>Puff-Net: Efficient Style Transfer with Pure Content and Style Feature Fusion Network</h4> 
<ul><li>Paper: https://arxiv.org/abs/2405.19775</li><li>Code:</li></ul> 
<h4><a id="PIA_Your_Personalized_Image_Animator_via_PlugandPlay_Modules_in_TexttoImage_Models_713"></a>PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.13964</li><li>Code: https://github.com/open-mmlab/PIA</li></ul> 
<h4><a id="RealCustom_Narrowing_Real_Text_Word_for_RealTime_OpenDomain_TexttoImage_Customization_718"></a>RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.00483</li><li>Code:</li></ul> 
<h4><a id="SmartEdit_Exploring_Complex_Instructionbased_Image_Editing_with_Multimodal_Large_Language_Models_723"></a>SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.06739</li><li>Code: https://github.com/TencentARC/SmartEdit</li></ul> 
<h4><a id="Style_Injection_in_Diffusion_A_Trainingfree_Approach_for_Adapting_Largescale_Diffusion_Models_for_Style_Transfer_728"></a>Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.09008</li><li>Code: https://github.com/jiwoogit/StyleID</li></ul> 
<h4><a id="SwitchLight_Codesign_of_Physicsdriven_Architecture_and_Pretraining_Framework_for_Human_Portrait_Relighting_733"></a>SwitchLight: Co-design of Physics-driven Architecture and Pre-training Framework for Human Portrait Relighting</h4> 
<ul><li>Paper: https://arxiv.org/abs/2402.18848</li><li>Code:</li></ul> 
<h4><a id="TextDriven_Image_Editing_via_Learnable_Regions_738"></a>Text-Driven Image Editing via Learnable Regions</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.16432</li><li>Code: https://github.com/yuanze-lin/Learnable_Regions</li></ul> 
<h4><a id="TexturePreserving_Diffusion_Models_for_HighFidelity_Virtual_TryOn_743"></a>Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.01089</li><li>Code: https://github.com/Gal4way/TPD</li></ul> 
<h4><a id="TiNOEdit_Timestep_and_Noise_Optimization_for_Robust_DiffusionBased_Image_Editing_748"></a>TiNO-Edit: Timestep and Noise Optimization for Robust Diffusion-Based Image Editing</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.11120</li><li>Code: https://github.com/SherryXTChen/TiNO-Edit</li></ul> 
<h4><a id="UniHuman_A_Unified_Model_For_Editing_Human_Images_in_the_Wild_753"></a>UniHuman: A Unified Model For Editing Human Images in the Wild</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.14985</li><li>Code: https://github.com/NannanLi999/UniHuman</li></ul> 
<h4><a id="ZONE_ZeroShot_InstructionGuided_Local_Editing_758"></a>ZONE: Zero-Shot Instruction-Guided Local Editing</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.16794</li><li>Code: https://github.com/lsl001006/ZONE</li></ul> 
<p></p> 
<h2><a id="3Video_GenerationVideo_Synthesis_766"></a>3.视频生成(Video Generation/Video Synthesis)</h2> 
<h4><a id="360DVD_Controllable_Panorama_Video_Generation_with_360Degree_Video_Diffusion_Model_768"></a>360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model</h4> 
<ul><li>Paper: https://arxiv.org/abs/2401.06578</li><li>Code: https://github.com/Akaneqwq/360DVD</li></ul> 
<h4><a id="A_Recipe_for_Scaling_up_TexttoVideo_Generation_with_Textfree_Videos_773"></a>A Recipe for Scaling up Text-to-Video Generation with Text-free Videos</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.15770</li><li>Code: https://github.com/ali-vilab/VGen</li></ul> 
<h4><a id="BIVDiff_A_TrainingFree_Framework_for_GeneralPurpose_Video_Synthesis_via_Bridging_Image_and_Video_Diffusion_Models_778"></a>BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.02813</li><li>Code: https://github.com/MCG-NJU/BIVDiff</li></ul> 
<h4><a id="ConvoFusion_MultiModal_Conversational_Diffusion_for_CoSpeech_Gesture_Synthesis_783"></a>ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.17936</li><li>Code: https://github.com/m-hamza-mughal/convofusion</li></ul> 
<h4><a id="CoSpeech_Gesture_Video_Generation_via_MotionDecoupled_Diffusion_Model_788"></a>Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.01862</li><li>Code: https://github.com/thuhcsi/S2G-MDDiffusion</li></ul> 
<h4><a id="DiffPerformer_Iterative_Learning_of_Consistent_Latent_Guidance_for_Diffusionbased_Human_Video_Generation_793"></a>DiffPerformer: Iterative Learning of Consistent Latent Guidance for Diffusion-based Human Video Generation</h4> 
<ul><li>Paper:</li><li>Code:</li></ul> 
<h4><a id="DisCo_Disentangled_Control_for_Realistic_Human_Dance_Generation_798"></a>DisCo: Disentangled Control for Realistic Human Dance Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2307.00040</li><li>Code: https://github.com/Wangt-CN/DisCo</li></ul> 
<h4><a id="FaceChainImagineID_Freely_Crafting_HighFidelity_Diverse_Talking_Faces_from_Disentangled_Audio_803"></a>FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.01901</li><li>Code:</li></ul> 
<h4><a id="Faces_that_Speak_Jointly_Synthesising_Talking_Face_and_Speech_from_Text_808"></a>Faces that Speak: Jointly Synthesising Talking Face and Speech from Text</h4> 
<ul><li>Paper: https://arxiv.org/abs/2405.10272</li><li>Code: https://github.com/Wangt-CN/DisCo</li></ul> 
<h4><a id="FlowVid_Taming_Imperfect_Optical_Flows_for_Consistent_VideotoVideo_Synthesis_813"></a>FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis</h4> 
<ul><li>Paper: https://openaccess.thecvf.com/content/CVPR2024/html/Liang_FlowVid_Taming_Imperfect_Optical_Flows_for_Consistent_Video-to-Video_Synthesis_CVPR_2024_paper.html</li><li>Code:</li></ul> 
<h4><a id="Generative_Rendering_Controllable_4DGuided_Video_Generation_with_2D_Diffusion_Models_818"></a>Generative Rendering: Controllable 4D-Guided Video Generation with 2D Diffusion Models</h4> 
<ul><li>Paper: https://openaccess.thecvf.com/content/CVPR2024/html/Cai_Generative_Rendering_Controllable_4D-Guided_Video_Generation_with_2D_Diffusion_Models_CVPR_2024_paper.html</li><li>Code:</li></ul> 
<h4><a id="GenTron_Diffusion_Transformers_for_Image_and_Video_Generation_823"></a>GenTron: Diffusion Transformers for Image and Video Generation</h4> 
<ul><li>Paper: https://openaccess.thecvf.com/content/CVPR2024/html/Chen_GenTron_Diffusion_Transformers_for_Image_and_Video_Generation_CVPR_2024_paper.html</li><li>Code:</li></ul> 
<h4><a id="Grid_Diffusion_Models_for_TexttoVideo_Generation_828"></a>Grid Diffusion Models for Text-to-Video Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.00234</li><li>Code: https://github.com/taegyeong-lee/Grid-Diffusion-Models-for-Text-to-Video-Generation</li></ul> 
<h4><a id="Hierarchical_Patchwise_Diffusion_Models_for_HighResolution_Video_Generation_833"></a>Hierarchical Patch-wise Diffusion Models for High-Resolution Video Generation</h4> 
<ul><li>Paper: https://openaccess.thecvf.com/content/CVPR2024/html/Skorokhodov_Hierarchical_Patch_Diffusion_Models_for_High-Resolution_Video_Generation_CVPR_2024_paper.html</li><li>Code:</li></ul> 
<h4><a id="Hierarchical_Spatiotemporal_Decoupling_for_TexttoVideo_Generation_838"></a>Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation</h4> 
<ul><li>Paper: https://openaccess.thecvf.com/content/CVPR2024/html/Qing_Hierarchical_Spatio-temporal_Decoupling_for_Text-to-Video_Generation_CVPR_2024_paper.html</li><li>Code:</li></ul> 
<h4><a id="LAMP_Learn_A_Motion_Pattern_for_FewShot_Video_Generation_843"></a>LAMP: Learn A Motion Pattern for Few-Shot Video Generation</h4> 
<ul><li>Paper: https://openaccess.thecvf.com/content/CVPR2024/html/Wu_LAMP_Learn_A_Motion_Pattern_for_Few-Shot_Video_Generation_CVPR_2024_paper.html</li><li>Code: https://github.com/RQ-Wu/LAMP</li></ul> 
<h4><a id="Learning_Dynamic_Tetrahedra_for_HighQuality_Talking_Head_Synthesis_848"></a>Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis</h4> 
<ul><li>Paper: https://arxiv.org/abs/2402.17364</li><li>Code: https://github.com/zhangzc21/DynTet</li></ul> 
<h4><a id="Lodge_A_Coarse_to_Fine_Diffusion_Network_for_Long_Dance_Generation_guided_by_the_Characteristic_Dance_Primitives_853"></a>Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation guided by the Characteristic Dance Primitives</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.10518</li><li>Code: https://github.com/li-ronghui/LODGE</li></ul> 
<h4><a id="MagicAnimate_Temporally_Consistent_Human_Image_Animation_using_Diffusion_Model_858"></a>MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.16498</li><li>Code: https://github.com/magic-research/magic-animate</li></ul> 
<h4><a id="MakeYourAnchor_A_Diffusionbased_2D_Avatar_Generation_Framework_863"></a>Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.16510</li><li>Code: https://github.com/ICTMCG/Make-Your-Anchor</li></ul> 
<h4><a id="Make_Your_Dream_A_Vlog_868"></a>Make Your Dream A Vlog</h4> 
<ul><li>Paper: https://arxiv.org/abs/2401.09414</li><li>Code: https://github.com/Vchitect/Vlogger</li></ul> 
<h4><a id="Make_Pixels_Dance_HighDynamic_Video_Generation_873"></a>Make Pixels Dance: High-Dynamic Video Generation</h4> 
<ul><li>Paper: https://openaccess.thecvf.com/content/CVPR2024/html/Zeng_Make_Pixels_Dance_High-Dynamic_Video_Generation_CVPR_2024_paper.html</li><li>Code:</li></ul> 
<h4><a id="MicroCinema_A_DivideandConquer_Approach_for_TexttoVideo_Generation_878"></a>MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation</h4> 
<ul><li>Paper: https://openaccess.thecvf.com/content/CVPR2024/html/Wang_MicroCinema_A_Divide-and-Conquer_Approach_for_Text-to-Video_Generation_CVPR_2024_paper.html</li><li>Code:</li></ul> 
<h4><a id="Panacea_Panoramic_and_Controllable_Video_Generation_for_Autonomous_Driving_883"></a>Panacea: Panoramic and Controllable Video Generation for Autonomous Driving</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.16813</li><li>Code: https://github.com/wenyuqing/panacea</li></ul> 
<h4><a id="PEEKABOO_Interactive_Video_Generation_via_MaskedDiffusion_888"></a>PEEKABOO: Interactive Video Generation via Masked-Diffusion</h4> 
<ul><li>Paper: https://openaccess.thecvf.com/content/CVPR2024/html/Jain_PEEKABOO_Interactive_Video_Generation_via_Masked-Diffusion_CVPR_2024_paper.html</li><li>Code:</li></ul> 
<h4><a id="Seeing_and_Hearing_Opendomain_VisualAudio_Generation_with_Diffusion_Latent_Aligners_893"></a>Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners</h4> 
<ul><li>Paper: https://arxiv.org/abs/2308.13712</li><li>Code: https://github.com/yzxing87/Seeing-and-Hearing</li></ul> 
<h4><a id="SimDA_Simple_Diffusion_Adapter_for_Efficient_Video_Generation_898"></a>SimDA: Simple Diffusion Adapter for Efficient Video Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2308.09710</li><li>Code: https://github.com/ChenHsing/SimDA</li></ul> 
<h4><a id="StyleCineGAN_Landscape_Cinemagraph_Generation_using_a_Pretrained_StyleGAN_903"></a>StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.14186</li><li>Code: https://github.com/jeolpyeoni/StyleCineGAN</li></ul> 
<h4><a id="SyncTalk_The_Devil_is_in_the_Synchronization_for_Talking_Head_Synthesis_908"></a>SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.17590</li><li>Code: https://github.com/ZiqiaoPeng/SyncTalk</li></ul> 
<h4><a id="TI2VZero_ZeroShot_Image_Conditioning_for_TexttoVideo_Diffusion_Models_913"></a>TI2V-Zero: Zero-Shot Image Conditioning for Text-to-Video Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.17590</li><li>Code:</li></ul> 
<h4><a id="TuneAVideo_OneShot_Tuning_of_Image_Diffusion_Models_for_TexttoVideo_Generation_918"></a>Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.16306</li><li>Code: https://github.com/showlab/Tune-A-Video</li></ul> 
<h4><a id="VideoBooth_Diffusionbased_Video_Generation_with_Image_Prompts_923"></a>VideoBooth: Diffusion-based Video Generation with Image Prompts</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.00777</li><li>Code: https://github.com/Vchitect/VideoBooth</li></ul> 
<h4><a id="VideoCrafter2_Overcoming_Data_Limitations_for_HighQuality_Video_Diffusion_Models_928"></a>VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2401.09047</li><li>Code: https://github.com/AILab-CVC/VideoCrafter</li></ul> 
<h4><a id="VideoP2P_Video_Editing_with_Crossattention_Control_933"></a>Video-P2P: Video Editing with Cross-attention Control</h4> 
<ul><li>Paper: https://arxiv.org/abs/2303.04761</li><li>Code: https://github.com/dvlab-research/Video-P2P</li></ul> 
<p></p> 
<h2><a id="4Video_Editing_941"></a>4.视频编辑(Video Editing)</h2> 
<h4><a id="A_Video_is_Worth_256_Bases_SpatialTemporal_ExpectationMaximization_Inversion_for_ZeroShot_Video_Editing_943"></a>A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.05856</li><li>Code: https://github.com/STEM-Inv/stem-inv</li></ul> 
<h4><a id="CAMEL_Causal_Motion_Enhancement_tailored_for_lifting_textdriven_video_editing_948"></a>CAMEL: Causal Motion Enhancement tailored for lifting text-driven video editing</h4> 
<ul><li>Paper: https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_CAMEL_CAusal_Motion_Enhancement_Tailored_for_Lifting_Text-driven_Video_Editing_CVPR_2024_paper.html</li><li>Code: https://github.com/zhangguiwei610/CAMEL</li></ul> 
<h4><a id="CCEdit_Creative_and_Controllable_Video_Editing_via_Diffusion_Models_953"></a>CCEdit: Creative and Controllable Video Editing via Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2309.16496</li><li>Code: https://github.com/RuoyuFeng/CCEdit</li></ul> 
<h4><a id="CoDeF_Content_Deformation_Fields_for_Temporally_Consistent_Video_Processing_958"></a>CoDeF: Content Deformation Fields for Temporally Consistent Video Processing</h4> 
<ul><li>Paper: https://arxiv.org/abs/2308.07926</li><li>Code: https://github.com/qiuyu96/CoDeF</li></ul> 
<h4><a id="FRESCO_SpatialTemporal_Correspondence_for_ZeroShot_Video_Translation_963"></a>FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.12962</li><li>Code: https://github.com/williamyang1991/FRESCO/tree/main</li></ul> 
<h4><a id="RAVE_Randomized_Noise_Shuffling_for_Fast_and_Consistent_Video_Editing_with_Diffusion_Models_968"></a>RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.04524</li><li>Code: https://github.com/rehg-lab/RAVE</li></ul> 
<h4><a id="VidToMe_Video_Token_Merging_for_ZeroShot_Video_Editing_973"></a>VidToMe: Video Token Merging for Zero-Shot Video Editing</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.10656</li><li>Code: https://github.com/lixirui142/VidToMe</li></ul> 
<h4><a id="VMC_Video_Motion_Customization_using_Temporal_Attention_Adaption_for_TexttoVideo_Diffusion_Models_978"></a>VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.00845</li><li>Code: https://github.com/HyeonHo99/Video-Motion-Customization</li></ul> 
<p></p> 
<h2><a id="53D3D_Generation3D_Synthesis_985"></a>5.3D生成(3D Generation/3D Synthesis)</h2> 
<h4><a id="4D_Gaussian_Splatting_for_RealTime_Dynamic_Scene_Rendering_987"></a>4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</h4> 
<ul><li>Paper: https://arxiv.org/abs/2310.08528</li><li>Code: https://github.com/hustvl/4DGaussians</li></ul> 
<h4><a id="Animatable_Gaussians_Learning_Posedependent_Gaussian_Maps_for_Highfidelity_Human_Avatar_Modeling_992"></a>Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.16096</li><li>Code: https://github.com/lizhe00/AnimatableGaussians</li></ul> 
<h4><a id="A_Unified_Approach_for_Text_and_Imageguided_4D_Scene_Generation_997"></a>A Unified Approach for Text- and Image-guided 4D Scene Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.16854</li><li>Code: https://github.com/NVlabs/dream-in-4d</li></ul> 
<h4><a id="BEHAVIOR_Vision_Suite_Customizable_Dataset_Generation_via_Simulation_1002"></a>BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2405.09546</li><li>Code: https://github.com/behavior-vision-suite/behavior-vision-suite.github.io</li></ul> 
<h4><a id="BerfScene_Bevconditioned_Equivariant_Radiance_Fields_for_Infinite_3D_Scene_Generation_1007"></a>BerfScene: Bev-conditioned Equivariant Radiance Fields for Infinite 3D Scene Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.02136</li><li>Code: https://github.com/zqh0253/BerfScene</li></ul> 
<h4><a id="CAD_Photorealistic_3D_Generation_via_Adversarial_Distillation_1012"></a>CAD: Photorealistic 3D Generation via Adversarial Distillation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.06663</li><li>Code: https://github.com/raywzy/CAD</li></ul> 
<h4><a id="CAGE_Controllable_Articulation_GEneration_1017"></a>CAGE: Controllable Articulation GEneration</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.09570</li><li>Code: https://github.com/3dlg-hcvc/cage</li></ul> 
<h4><a id="CityDreamer_Compositional_Generative_Model_of_Unbounded_3D_Cities_1022"></a>CityDreamer: Compositional Generative Model of Unbounded 3D Cities</h4> 
<ul><li>Paper: https://arxiv.org/abs/2309.00610</li><li>Code: https://github.com/hzxie/CityDreamer</li></ul> 
<h4><a id="Consistent3D_Towards_Consistent_HighFidelity_Textto3D_Generation_with_Deterministic_Sampling_Prior_1027"></a>Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior</h4> 
<ul><li>Paper: https://arxiv.org/abs/2401.09050</li><li>Code: https://github.com/sail-sg/Consistent3D</li></ul> 
<h4><a id="ConTexHuman_FreeView_Rendering_of_Human_from_a_Single_Image_with_TextureConsistent_Synthesis_1032"></a>ConTex-Human: Free-View Rendering of Human from a Single Image with Texture-Consistent Synthesis</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.17123</li><li>Code: https://github.com/gaoxiangjun/ConTex-Human</li></ul> 
<h4><a id="ControlRoom3D_Room_Generation_using_Semantic_Proxy_Rooms_1037"></a>ControlRoom3D: Room Generation using Semantic Proxy Rooms</h4> 
<ul><li>Paper： https://arxiv.org/abs/2312.05208</li><li>Code:</li></ul> 
<h4><a id="DanceCamera3D_3D_Camera_Movement_Synthesis_with_Music_and_Dance_1042"></a>DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.13667</li><li>Code: https://github.com/Carmenw1203/DanceCamera3D-Official</li></ul> 
<h4><a id="DiffPortrait3D_Controllable_Diffusion_for_ZeroShot_Portrait_View_Synthesis_1047"></a>DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.13016</li><li>Code: https://github.com/FreedomGu/DiffPortrait3D</li></ul> 
<h4><a id="DiffSHEG_A_DiffusionBased_Approach_for_RealTime_Speechdriven_Holistic_3D_Expression_and_Gesture_Generation_1052"></a>DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2401.04747</li><li>Code: https://github.com/JeremyCJM/DiffSHEG</li></ul> 
<h4><a id="DiffuScene_Denoising_Diffusion_Models_for_Generative_Indoor_Scene_Synthesis_1057"></a>DiffuScene: Denoising Diffusion Models for Generative Indoor Scene Synthesis</h4> 
<ul><li>Paper: https://arxiv.org/abs/2303.14207</li><li>Code: https://github.com/tangjiapeng/DiffuScene</li></ul> 
<h4><a id="Diffusion_3D_Features_Diff3F_Decorating_Untextured_Shapes_with_Distilled_Semantic_Features_1062"></a>Diffusion 3D Features (Diff3F): Decorating Untextured Shapes with Distilled Semantic Features</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.17024</li><li>Code: https://github.com/niladridutt/Diffusion-3D-Features</li></ul> 
<h4><a id="Diffusion_Timestep_Curriculum_for_One_Image_to_3D_Generation_1067"></a>Diffusion Time-step Curriculum for One Image to 3D Generation</h4> 
<ul><li>Paper: https://paperswithcode.com/paper/diffusion-time-step-curriculum-for-one-image</li><li>Code: https://github.com/yxymessi/DTC123</li></ul> 
<h4><a id="DreamAvatar_TextandShape_Guided_3D_Human_Avatar_Generation_via_Diffusion_Models_1072"></a>DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2304.00916</li><li>Code: https://github.com/yukangcao/DreamAvatar</li></ul> 
<h4><a id="DreamComposer_Controllable_3D_Object_Generation_via_MultiView_Conditions_1077"></a>DreamComposer: Controllable 3D Object Generation via Multi-View Conditions</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.03611</li><li>Code: https://github.com/yhyang-myron/DreamComposer</li></ul> 
<h4><a id="DreamControl_ControlBased_Textto3D_Generation_with_3D_SelfPrior_1082"></a>DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.06439</li><li>Code: https://github.com/tyhuang0428/DreamControl</li></ul> 
<h4><a id="Emotional_Speechdriven_3D_Body_Animation_via_Disentangled_Latent_Diffusion_1087"></a>Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.04466</li><li>Code: https://github.com/kiranchhatre/amuse</li></ul> 
<h4><a id="EscherNet_A_Generative_Model_for_Scalable_View_Synthesis_1092"></a>EscherNet: A Generative Model for Scalable View Synthesis</h4> 
<ul><li>Paper: https://arxiv.org/abs/2402.03908</li><li>Code: https://github.com/hzxie/city-dreamer</li></ul> 
<h4><a id="GaussianDreamer_Fast_Generation_from_Text_to_3D_Gaussians_by_Bridging_2D_and_3D_Diffusion_Models_1097"></a>GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2310.08529</li><li>Code: https://github.com/hustvl/GaussianDreamer</li></ul> 
<h4><a id="GPT4Vision_is_a_HumanAligned_Evaluator_for_Textto3D_Generation_1102"></a>GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2401.04092</li><li>Code: https://github.com/3DTopia/GPTEval3D</li></ul> 
<h4><a id="Gaussian_Shell_Maps_for_Efficient_3D_Human_Generation_1107"></a>Gaussian Shell Maps for Efficient 3D Human Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.17857</li><li>Code: https://github.com/computational-imaging/GSM</li></ul> 
<h4><a id="HarmonyView_Harmonizing_Consistency_and_Diversity_in_OneImageto3D_1112"></a>HarmonyView: Harmonizing Consistency and Diversity in One-Image-to-3D</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.15980</li><li>Code: https://github.com/byeongjun-park/HarmonyView</li></ul> 
<h4><a id="HIG_Hierarchical_Interlacement_Graph_Approach_to_Scene_Graph_Generation_in_Video_Understanding_1117"></a>HIG: Hierarchical Interlacement Graph Approach to Scene Graph Generation in Video Understanding</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.03050</li><li>Code:</li></ul> 
<h4><a id="Holodeck_Language_Guided_Generation_of_3D_Embodied_AI_Environments_1122"></a>Holodeck: Language Guided Generation of 3D Embodied AI Environments</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.09067</li><li>Code: https://github.com/allenai/Holodeck</li></ul> 
<h4><a id="HumanNorm_Learning_Normal_Diffusion_Model_for_Highquality_and_Realistic_3D_Human_Generation_1127"></a>HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2310.01406</li><li>Code:</li></ul> 
<h4><a id="Interactive3D_Create_What_You_Want_by_Interactive_3D_Generation_1132"></a>Interactive3D: Create What You Want by Interactive 3D Generation</h4> 
<ul><li>Paper: https://hub.baai.ac.cn/paper/494efc8d-f4ed-4ca4-8469-b882f9489f5e</li><li>Code:</li></ul> 
<h4><a id="InterHandGen_TwoHand_Interaction_Generation_via_Cascaded_Reverse_Diffusio_1137"></a>InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse Diffusio</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.17422</li><li>Code: https://github.com/jyunlee/InterHandGen</li></ul> 
<h4><a id="Intrinsic_Image_Diffusion_for_Singleview_Material_Estimation_1142"></a>Intrinsic Image Diffusion for Single-view Material Estimation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.12274</li><li>Code: https://github.com/Peter-Kocsis/IntrinsicImageDiffusion</li></ul> 
<h4><a id="MakeItVivid_Dressing_Your_Animatable_Biped_Cartoon_Characters_from_Text_1147"></a>Make-It-Vivid: Dressing Your Animatable Biped Cartoon Characters from Text</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.16897</li><li>Code: https://github.com/junshutang/Make-It-Vivid</li></ul> 
<h4><a id="MoMask_Generative_Masked_Modeling_of_3D_Human_Motions_1152"></a>MoMask: Generative Masked Modeling of 3D Human Motions</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.00063</li><li>Code: https://github.com/EricGuo5513/momask-codes</li></ul> 
<h4><a id="Editable_Scene_Simulation_for_Autonomous_Driving_via_LLMAgent_Collaboration_1157"></a>Editable Scene Simulation for Autonomous Driving via LLM-Agent Collaboration</h4> 
<ul><li>Paper: https://arxiv.org/abs/2402.05746</li><li>Code: https://github.com/yifanlu0227/ChatSim?tab=readme-ov-file</li></ul> 
<h4><a id="EpiDiff_Enhancing_MultiView_Synthesis_via_Localized_EpipolarConstrained_Diffusion_1162"></a>EpiDiff: Enhancing Multi-View Synthesis via Localized Epipolar-Constrained Diffusion</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.06725</li><li>Code: https://github.com/huanngzh/EpiDiff</li></ul> 
<h4><a id="OED_Towards_Onestage_EndtoEnd_Dynamic_Scene_Graph_Generation_1167"></a>OED: Towards One-stage End-to-End Dynamic Scene Graph Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2405.16925</li><li>Code:</li></ul> 
<h4><a id="One2345_Fast_Single_Image_to_3D_Objects_with_Consistent_MultiView_Generation_and_3D_Diffusion_1172"></a>One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.07885</li><li>Code: https://github.com/SUDO-AI-3D/One2345plus</li></ul> 
<h4><a id="Paintit_TexttoTexture_Synthesis_via_Deep_Convolutional_Texture_Map_Optimization_and_PhysicallyBased_Rendering_1177"></a>Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization and Physically-Based Rendering</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.11360</li><li>Code: https://github.com/postech-ami/Paint-it</li></ul> 
<h4><a id="PEGASUS_Personalized_Generative_3D_Avatars_with_Composable_Attributes_1182"></a>PEGASUS: Personalized Generative 3D Avatars with Composable Attributes</h4> 
<ul><li>Paper: https://arxiv.org/abs/2402.10636</li><li>Code: https://github.com/snuvclab/pegasus</li></ul> 
<h4><a id="PhysGaussian_PhysicsIntegrated_3D_Gaussians_for_Generative_Dynamics_1187"></a>PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.12198</li><li>Code: https://github.com/XPandora/PhysGaussian</li></ul> 
<h4><a id="RichDreamer_A_Generalizable_NormalDepth_Diffusion_Model_for_Detail_Richness_in_Textto3D_1192"></a>RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D.</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.16918</li><li>Code: https://github.com/modelscope/richdreamer</li></ul> 
<h4><a id="SceneTex_HighQuality_Texture_Synthesis_for_Indoor_Scenes_via_Diffusion_Priors_1197"></a>SceneTex: High-Quality Texture Synthesis for Indoor Scenes via Diffusion Priors</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.17261</li><li>Code: https://github.com/daveredrum/SceneTex</li></ul> 
<h4><a id="SceneWiz3D_Towards_Textguided_3D_Scene_Composition_1202"></a>SceneWiz3D: Towards Text-guided 3D Scene Composition</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.08885</li><li>Code: https://github.com/zqh0253/SceneWiz3D</li></ul> 
<h4><a id="SemCity_Semantic_Scene_Generation_with_Triplane_Diffusion_1207"></a>SemCity: Semantic Scene Generation with Triplane Diffusion</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.07773</li><li>Code: https://github.com/zoomin-lee/SemCity?tab=readme-ov-file</li></ul> 
<h4><a id="Sherpa3D_Boosting_HighFidelity_Textto3D_Generation_via_Coarse_3D_Prior_1212"></a>Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.06655</li><li>Code: https://github.com/liuff19/Sherpa3D</li></ul> 
<h4><a id="SIGNeRF_Scene_Integrated_Generation_for_Neural_Radiance_Fields_1217"></a>SIGNeRF: Scene Integrated Generation for Neural Radiance Fields</h4> 
<ul><li>Paper: https://arxiv.org/abs/2401.01647</li><li>Code: https://github.com/cgtuebingen/SIGNeRF</li></ul> 
<h4><a id="Single_Mesh_Diffusion_Models_with_Field_Latents_for_Texture_Generation_1222"></a>Single Mesh Diffusion Models with Field Latents for Texture Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.09250</li><li>Code: https://github.com/google-research/google-research/tree/master/mesh_diffusion</li></ul> 
<h4><a id="SiTH_Singleview_Textured_Human_Reconstruction_with_ImageConditioned_Diffusion_1227"></a>SiTH: Single-view Textured Human Reconstruction with Image-Conditioned Diffusion</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.15855</li><li>Code: https://github.com/SiTH-Diffusion/SiTH</li></ul> 
<h4><a id="SPAD_Spatially_Aware_Multiview_Diffusers_1232"></a>SPAD: Spatially Aware Multiview Diffusers</h4> 
<ul><li>Paper: https://arxiv.org/abs/2402.05235</li><li>Code: https://github.com/yashkant/spad</li></ul> 
<h4><a id="Textto3D_Generation_with_Bidirectional_Diffusion_using_both_2D_and_3D_priors_1237"></a>Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.04963</li><li>Code: https://github.com/BiDiff/bidiff</li></ul> 
<h4><a id="Textto3D_using_Gaussian_Splatting_1242"></a>Text-to-3D using Gaussian Splatting</h4> 
<ul><li>Paper: https://arxiv.org/abs/2309.16585</li><li>Code: https://github.com/gsgen3d/gsgen</li></ul> 
<h4><a id="The_More_You_See_in_2D_the_More_You_Perceive_in_3D_1247"></a>The More You See in 2D, the More You Perceive in 3D</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.03652</li><li>Code: https://github.com/sap3d/sap3d</li></ul> 
<h4><a id="Tiger_TimeVarying_Denoising_Model_for_3D_Point_Cloud_Generation_with_Diffusion_Process_1252"></a>Tiger: Time-Varying Denoising Model for 3D Point Cloud Generation with Diffusion Process</h4> 
<ul><li>Paper: https://cvlab.cse.msu.edu/pdfs/Ren_Kim_Liu_Liu_TIGER_supp.pdf</li><li>Code: https://github.com/Zhiyuan-R/Tiger-Diffusion</li></ul> 
<h4><a id="Towards_Realistic_Scene_Generation_with_LiDAR_Diffusion_Models_1257"></a>Towards Realistic Scene Generation with LiDAR Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.00815</li><li>Code: https://github.com/hancyran/LiDAR-Diffusion</li></ul> 
<h4><a id="UDiFF_Generating_Conditional_Unsigned_Distance_Fields_with_Optimal_Wavelet_Diffusion_1262"></a>UDiFF: Generating Conditional Unsigned Distance Fields with Optimal Wavelet Diffusion</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.06851</li><li>Code: https://github.com/weiqi-zhang/UDiFF</li></ul> 
<h4><a id="ViVid1to3_Novel_View_Synthesis_with_Video_Diffusion_Models_1267"></a>ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.01305</li><li>Code: https://github.com/ubc-vision/vivid123</li></ul> 
<p></p> 
<h2><a id="63D3D_Editing_1275"></a>6.3D编辑(3D Editing)</h2> 
<h4><a id="GaussianEditor_Swift_and_Controllable_3D_Editing_with_Gaussian_Splatting_1277"></a>GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.14521</li><li>Code: https://github.com/buaacyw/GaussianEditor</li></ul> 
<h4><a id="GenN2N_Generative_NeRF2NeRF_Translation_1282"></a>GenN2N: Generative NeRF2NeRF Translation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.02788</li><li>Code: https://github.com/Lxiangyue/GenN2N</li></ul> 
<h4><a id="Makeup_Prior_Models_for_3D_Facial_Makeup_Estimation_and_Applications_1287"></a>Makeup Prior Models for 3D Facial Makeup Estimation and Applications</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.17761</li><li>Code: https://github.com/YangXingchao/makeup-priors</li></ul> 
<p></p> 
<h2><a id="7MultiModal_Large_Language_Models_1295"></a>7.多模态大语言模型(Multi-Modal Large Language Models)</h2> 
<h4><a id="AlphaCLIP_A_CLIP_Model_Focusing_on_Wherever_You_Want_1297"></a>Alpha-CLIP: A CLIP Model Focusing on Wherever You Want</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.03818</li><li>Code: https://github.com/SunzeY/AlphaCLIP</li></ul> 
<h4><a id="Anchorbased_Robust_Finetuning_of_VisionLanguage_Models_1302"></a>Anchor-based Robust Finetuning of Vision-Language Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.06244</li><li>Code: https://github.com/LixDemon/ARF</li></ul> 
<h4><a id="Boosting_Continual_Learning_of_VisionLanguage_Models_via_MixtureofExperts_Adapters_1307"></a>Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.11549</li><li>Code: https://github.com/JiazuoYu/MoE-Adapters4CL</li></ul> 
<h4><a id="Can_Language_Beat_Numerical_Regression_LanguageBased_Multimodal_Trajectory_Prediction_1312"></a>Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.18447</li><li>Code: https://github.com/InhwanBae/LMTrajectory</li></ul> 
<h4><a id="Cant_make_an_Omelette_without_Breaking_some_Eggs_Plausible_Action_Anticipation_using_Large_VideoLanguage_Models_1317"></a>Can’t make an Omelette without Breaking some Eggs: Plausible Action Anticipation using Large Video-Language Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2405.20305</li><li>Code:</li></ul> 
<h4><a id="ChatUniVi_Unified_Visual_Representation_Empowers_Large_Language_Models_with_Image_and_Video_Understanding_1322"></a>Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.08046</li><li>Code: https://github.com/PKU-YuanGroup/Chat-UniVi</li></ul> 
<h4><a id="Compositional_ChainofThought_Prompting_for_Large_Multimodal_Models_1327"></a>Compositional Chain-of-Thought Prompting for Large Multimodal Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.17076</li><li>Code: https://github.com/chancharikmitra/CCoT</li></ul> 
<h4><a id="Describing_Differences_in_Image_Sets_with_Natural_Language_1332"></a>Describing Differences in Image Sets with Natural Language</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.02974</li><li>Code: https://github.com/Understanding-Visual-Datasets/VisDiff</li></ul> 
<h4><a id="Dual_Memory_Networks_A_Versatile_Adaptation_Approach_for_VisionLanguage_Models_1337"></a>Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.17589</li><li>Code: https://github.com/YBZh/DMN</li></ul> 
<h4><a id="Efficient_Stitchable_Task_Adaptation_1342"></a>Efficient Stitchable Task Adaptation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.17352</li><li>Code: https://github.com/ziplab/Stitched_LLaMA</li></ul> 
<h4><a id="Efficient_TestTime_Adaptation_of_VisionLanguage_Models_1347"></a>Efficient Test-Time Adaptation of Vision-Language Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.18293</li><li>Code: https://github.com/kdiAAA/TDA</li></ul> 
<h4><a id="Exploring_the_Transferability_of_Visual_Prompting_for_Multimodal_Large_Language_Models_1352"></a>Exploring the Transferability of Visual Prompting for Multimodal Large Language Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.11207</li><li>Code: https://github.com/zycheiheihei/transferable-visual-prompting</li></ul> 
<h4><a id="FairCLIP_Harnessing_Fairness_in_VisionLanguage_Learning_1357"></a>FairCLIP: Harnessing Fairness in Vision-Language Learning</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.19949</li><li>Code: https://github.com/Harvard-Ophthalmology-AI-Lab/FairCLIP</li></ul> 
<h4><a id="FairDeDup_Detecting_and_Mitigating_VisionLanguage_Fairness_Disparities_in_Semantic_Dataset_Deduplication_1362"></a>FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.16123</li><li>Code:</li></ul> 
<h4><a id="FFF_Fixing_Flawed_Foundations_in_contrastive_pretraining_results_in_very_strong_VisionLanguage_models_1367"></a>FFF: Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.16123</li><li>Code:</li></ul> 
<h4><a id="Generative_Multimodal_Models_are_InContext_Learners_1372"></a>Generative Multimodal Models are In-Context Learners</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.13286</li><li>Code: https://github.com/baaivision/Emu/tree/main/Emu2</li></ul> 
<h4><a id="GLaMM_Pixel_Grounding_Large_Multimodal_Model_1377"></a>GLaMM: Pixel Grounding Large Multimodal Model</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.03356</li><li>Code: https://github.com/mbzuai-oryx/groundingLMM</li></ul> 
<h4><a id="GPT4Point_A_Unified_Framework_for_PointLanguage_Understanding_and_Generation_1382"></a>GPT4Point: A Unified Framework for Point-Language Understanding and Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.02980</li><li>Code: https://github.com/Pointcept/GPT4Point</li></ul> 
<h4><a id="InternVL_Scaling_up_Vision_Foundation_Models_and_Aligning_for_Generic_VisualLinguistic_Tasks_1387"></a>InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.14238</li><li>Code: https://github.com/OpenGVLab/InternVL</li></ul> 
<h4><a id="Learning_by_Correction_Efficient_Tuning_Task_for_ZeroShot_Generative_VisionLanguage_Reasoning_1392"></a>Learning by Correction: Efficient Tuning Task for Zero-Shot Generative Vision-Language Reasoning</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.00909</li><li>Code:</li></ul> 
<h4><a id="Lets_Think_Outside_the_Box_Exploring_LeapofThought_in_Large_Language_Models_with_Creative_Humor_Generation_1397"></a>Let’s Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.02439</li><li>Code: https://github.com/sail-sg/CLoT</li></ul> 
<h4><a id="LION__Empowering_Multimodal_Large_Language_Model_with_DualLevel_Visual_Knowledge_1402"></a>LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.11860</li><li>Code: https://github.com/rshaojimmy/JiuTian</li></ul> 
<h4><a id="LL3DA_Visual_Interactive_Instruction_Tuning_for_Omni3D_Understanding_Reasoning_and_Planning_1407"></a>LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.18651</li><li>Code: https://github.com/Open3DA/LL3DA</li></ul> 
<h4><a id="Mitigating_Object_Hallucinations_in_Large_VisionLanguage_Models_through_Visual_Contrastive_Decoding_1412"></a>Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.16922</li><li>Code: https://github.com/DAMO-NLP-SG/VCD</li></ul> 
<h4><a id="MobileCLIP_Fast_ImageText_Models_through_MultiModal_Reinforced_Training_1417"></a>MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.17049</li><li>Code: https://github.com/apple/ml-mobileclip</li></ul> 
<h4><a id="MoPECLIP_Structured_Pruning_for_Efficient_VisionLanguage_Models_with_Modulewise_Pruning_Error_Metric_1422"></a>MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.07839</li><li>Code:</li></ul> 
<h4><a id="Narrative_Action_Evaluation_with_PromptGuided_Multimodal_Interaction_1427"></a>Narrative Action Evaluation with Prompt-Guided Multimodal Interaction</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.14471</li><li>Code: https://github.com/shiyi-zh0408/NAE_CVPR2024</li></ul> 
<h4><a id="OneLLM_One_Framework_to_Align_All_Modalities_with_Language_1432"></a>OneLLM: One Framework to Align All Modalities with Language</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.03700</li><li>Code: https://github.com/csuhan/OneLLM</li></ul> 
<h4><a id="One_Prompt_Word_is_Enough_to_Boost_Adversarial_Robustness_for_Pretrained_VisionLanguage_Models_1437"></a>One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.01849</li><li>Code: https://github.com/TreeLLi/APT</li></ul> 
<h4><a id="OPERA_Alleviating_Hallucination_in_MultiModal_Large_Language_Models_via_OverTrust_Penalty_and_RetrospectionAllocation_1442"></a>OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation</h4> 
<ul><li>Paper: https://arxiv.org/abs/2402.19479</li><li>Code: https://github.com/shikiw/OPERA</li></ul> 
<h4><a id="Panda70M_Captioning_70M_Videos_with_Multiple_CrossModality_Teachers_1447"></a>Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.17911</li><li>Code: https://github.com/snap-research/Panda-70M</li></ul> 
<h4><a id="PixelLM_Pixel_Reasoning_with_Large_Multimodal_Model_1452"></a>PixelLM: Pixel Reasoning with Large Multimodal Model</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.02228</li><li>Code: https://github.com/MaverickRen/PixelLM</li></ul> 
<h4><a id="PracticalDG_Perturbation_Distillation_on_VisionLanguage_Models_for_Hybrid_Domain_Generalization_1457"></a>PracticalDG: Perturbation Distillation on Vision-Language Models for Hybrid Domain Generalization</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.09011</li><li>Code:</li></ul> 
<h4><a id="Prompt_Highlighter_Interactive_Control_for_MultiModal_LLMs_1462"></a>Prompt Highlighter: Interactive Control for Multi-Modal LLMs</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.04302</li><li>Code: https://github.com/dvlab-research/Prompt-Highlighter</li></ul> 
<h4><a id="PromptKD_Unsupervised_Prompt_Distillation_for_VisionLanguage_Models_1467"></a>PromptKD: Unsupervised Prompt Distillation for Vision-Language Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.02781</li><li>Code: https://github.com/zhengli97/PromptKD</li></ul> 
<h4><a id="QInstruct_Improving_Lowlevel_Visual_Abilities_for_Multimodality_Foundation_Models_1472"></a>Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.06783</li><li>Code: https://github.com/Q-Future/Q-Instruct</li></ul> 
<h4><a id="SCTune_Unleashing_SelfConsistent_Referential_Comprehension_in_Large_Vision_Language_Models_1477"></a>SC-Tune: Unleashing Self-Consistent Referential Comprehension in Large Vision Language Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.13263</li><li>Code: https://github.com/ivattyue/SC-Tune</li></ul> 
<h4><a id="SEEDBench_Benchmarking_Multimodal_Large_Language_Models_1482"></a>SEED-Bench: Benchmarking Multimodal Large Language Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.17092</li><li>Code: https://github.com/AILab-CVC/SEED-Bench</li></ul> 
<h4><a id="SyncMask_Synchronized_Attentional_Masking_for_Fashioncentric_VisionLanguage_Pretraining_1487"></a>SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.01156</li><li>Code:</li></ul> 
<h4><a id="The_Manga_Whisperer_Automatically_Generating_Transcriptions_for_Comics_1492"></a>The Manga Whisperer: Automatically Generating Transcriptions for Comics</h4> 
<ul><li>Paper: https://arxiv.org/abs/2401.10224</li><li>Code: https://github.com/ragavsachdeva/magi</li></ul> 
<h4><a id="UniBind_LLMAugmented_Unified_and_Balanced_Representation_Space_to_Bind_Them_All_1497"></a>UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.12532</li><li>Code:</li></ul> 
<h4><a id="VBench_Comprehensive_Benchmark_Suite_for_Video_Generative_Models_1502"></a>VBench: Comprehensive Benchmark Suite for Video Generative Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2311.17982</li><li>Code: https://github.com/Vchitect/VBench</li></ul> 
<h4><a id="VideoChat_ChatCentric_Video_Understanding_1507"></a>VideoChat: Chat-Centric Video Understanding</h4> 
<ul><li>Paper: https://arxiv.org/abs/2305.06355</li><li>Code: https://github.com/OpenGVLab/Ask-Anything</li></ul> 
<h4><a id="ViPLLaVA_Making_Large_Multimodal_Models_Understand_Arbitrary_Visual_Prompts_1512"></a>ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts</h4> 
<ul><li>Paper: https://arxiv.org/abs/2312.00784</li><li>Code: https://github.com/mu-cai/ViP-LLaVA</li></ul> 
<h4><a id="ViTamin_Designing_Scalable_Vision_Models_in_the_Visionlanguage_Era_1517"></a>ViTamin: Designing Scalable Vision Models in the Vision-language Era</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.02132</li><li>Code: https://github.com/Beckschen/ViTamin</li></ul> 
<h4><a id="ViTLens_Towards_Omnimodal_Representations_1522"></a>ViT-Lens: Towards Omni-modal Representations</h4> 
<ul><li>Paper: https://github.com/TencentARC/ViT-Lens</li><li>Code: https://arxiv.org/abs/2308.10185</li></ul> 
<p></p> 
<h2><a id="8Others_1531"></a>8.其他任务(Others)</h2> 
<h4><a id="AEROBLADE_TrainingFree_Detection_of_Latent_Diffusion_Images_Using_Autoencoder_Reconstruction_Error_1533"></a>AEROBLADE: Training-Free Detection of Latent Diffusion Images Using Autoencoder Reconstruction Error</h4> 
<ul><li>Paper: https://arxiv.org/abs/2401.17879</li><li>Code: https://github.com/jonasricker/aeroblade</li></ul> 
<h4><a id="DiffBGM_A_Diffusion_Model_for_Video_Background_Music_Generation_1538"></a>Diff-BGM: A Diffusion Model for Video Background Music Generation</h4> 
<ul><li>Paper: https://openaccess.thecvf.com/content/CVPR2024/html/Li_Diff-BGM_A_Diffusion_Model_for_Video_Background_Music_Generation_CVPR_2024_paper.html</li><li>Code: https://github.com/sizhelee/Diff-BGM</li></ul> 
<h4><a id="EvalCrafter_Benchmarking_and_Evaluating_Large_Video_Generation_Models_1543"></a>EvalCrafter: Benchmarking and Evaluating Large Video Generation Models</h4> 
<ul><li>Paper: https://arxiv.org/abs/2310.11440</li><li>Code: https://github.com/evalcrafter/EvalCrafter</li></ul> 
<h4><a id="On_the_Content_Bias_in_Frchet_Video_Distance_1548"></a>On the Content Bias in Fréchet Video Distance</h4> 
<ul><li>Paper: https://arxiv.org/abs/2404.12391</li><li>Code: https://github.com/songweige/content-debiased-fvd</li></ul> 
<h4><a id="TexTile_A_Differentiable_Metric_for_Texture_Tileability_1553"></a>TexTile: A Differentiable Metric for Texture Tileability</h4> 
<ul><li>Paper: https://arxiv.org/abs/2403.12961v1</li><li>Code: https://github.com/crp94/textile</li></ul> 
<p><font color="red" size="5">持续更新~</font></p> 
<h2><a id="_1560"></a>参考</h2> 
<p><a href="https://github.com/amusi/CVPR2024-Papers-with-Code">CVPR 2024 论文和开源项目合集(Papers with Code)</a></p> 
<h2><a id="_1563"></a>相关整理</h2> 
<ul><li><a href="https://github.com/Kobaayyy/Awesome-AIGC-Research-Groups">Awesome-AIGC-Research-Groups</a></li><li><a href="https://github.com/Kobaayyy/Awesome-Low-Level-Vision-Research-Groups">Awesome-Low-Level-Vision-Research-Groups</a></li><li><a href="https://github.com/Kobaayyy/Awesome-CVPR2024-CVPR2021-CVPR2020-Low-Level-Vision">Awesome-CVPR2024-CVPR2021-CVPR2020-Low-Level-Vision</a></li><li><a href="https://github.com/Kobaayyy/Awesome-ECCV2020-Low-Level-Vision">Awesome-ECCV2020-Low-Level-Vision</a></li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e960d07a5bfce020d7b0dce2f5db2ff5/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【C#】图形图像编程</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/064d6e746c06a11b6a93b7c1b2e4f8af/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Oracle的这些BUG你要遇到,说明你是一个DBA老鸟...</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>