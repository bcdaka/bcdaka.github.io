<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>大数据几种任务调度工具 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/42ff9b3db5eafee08319eab10c579da1/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="大数据几种任务调度工具">
  <meta property="og:description" content="文章目录 一、DolphinScheduler概述和部署1、DolphinScheduler简介1.1 概述1.2 核心架构 2、DolphinScheduler部署模式2.1 概述2.2 单机模式2.3 伪集群模式2.4 集群模式 3、DolphinScheduler集群模式部署3.1 集群规划与准备3.2 下载与配置部署脚本3.3 初始化数据库3.4 一键部署DolphinScheduler3.5 DolphinScheduler启停命令 二、DolphinScheduler操作1、工作流传参1.1 内置参数1.2 参数传递 2、引用依赖资源3、数据源配置4、告警实例配置4.1 邮箱告警实例配置4.2 其他告警 5、其他注意事项 三、Airflow1、Airflow基本概念1.1 概述1.2 名词解释 2、Airflow安装2.1 python环境安装2.2 安装Airflow 3、修改数据库与调度器3.1 修改数据库为mysql3.2 修改执行器 4、部署使用4.1 环境部署启动 4.2 Dag任务操作4.3 配置邮件服务器 四、Azkaban1、Azkaban入门1.1 上传jar包和配置sql1.2 配置Executor Server1.3 配置Web Server 2、Work Flow案例实操2.1 HelloWorld案例2.2 作业依赖案例2.3 自动失败重试案例2.4 手动失败重试案例 3、JavaProcess作业类型案例3.1 概述3.2 案例 4、条件工作流案例4.1 概述4.2 运行时参数案例4.3 预定义宏案例 5、邮箱告警6、Azkaban多Executor模式注意事项 一、DolphinScheduler概述和部署 官网：https://dolphinscheduler.apache.org/
1、DolphinScheduler简介 1.1 概述 Apache DolphinScheduler是一个分布式、易扩展的可视化DAG工作流任务调度平台。致力于解决数据处理流程中错综复杂的依赖关系，使调度系统在数据处理流程中开箱即用
1.2 核心架构 DolphinScheduler的主要角色如下：
MasterServer采用分布式无中心设计理念，MasterServer主要负责 DAG 任务切分、任务提交、任务监控，并同时监听其它MasterServer和WorkerServer的健康状态WorkerServer也采用分布式无中心设计理念，WorkerServer主要负责任务的执行和提供日志服务ZooKeeper服务，系统中的MasterServer和WorkerServer节点都通过ZooKeeper来进行集群管理和容错Alert服务，提供告警相关服务API接口层，主要负责处理前端UI层的请求UI，系统的前端页面，提供系统的各种可视化操作界面 2、DolphinScheduler部署模式 https://dolphinscheduler.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-15T18:33:55+08:00">
    <meta property="article:modified_time" content="2024-04-15T18:33:55+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大数据几种任务调度工具</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#DolphinScheduler_1" rel="nofollow">一、DolphinScheduler概述和部署</a></li><li><ul><li><a href="#1DolphinScheduler_5" rel="nofollow">1、DolphinScheduler简介</a></li><li><ul><li><a href="#11__7" rel="nofollow">1.1 概述</a></li><li><a href="#12__11" rel="nofollow">1.2 核心架构</a></li></ul> 
   </li><li><a href="#2DolphinScheduler_24" rel="nofollow">2、DolphinScheduler部署模式</a></li><li><ul><li><a href="#21__28" rel="nofollow">2.1 概述</a></li><li><a href="#22__32" rel="nofollow">2.2 单机模式</a></li><li><a href="#23__42" rel="nofollow">2.3 伪集群模式</a></li><li><a href="#24__46" rel="nofollow">2.4 集群模式</a></li></ul> 
   </li><li><a href="#3DolphinScheduler_50" rel="nofollow">3、DolphinScheduler集群模式部署</a></li><li><ul><li><a href="#31__52" rel="nofollow">3.1 集群规划与准备</a></li><li><a href="#32__64" rel="nofollow">3.2 下载与配置部署脚本</a></li><li><a href="#33__248" rel="nofollow">3.3 初始化数据库</a></li><li><a href="#34_DolphinScheduler_275" rel="nofollow">3.4 一键部署DolphinScheduler</a></li><li><a href="#35_DolphinScheduler_295" rel="nofollow">3.5 DolphinScheduler启停命令</a></li></ul> 
  </li></ul> 
  </li><li><a href="#DolphinScheduler_321" rel="nofollow">二、DolphinScheduler操作</a></li><li><ul><li><a href="#1_325" rel="nofollow">1、工作流传参</a></li><li><ul><li><a href="#11__331" rel="nofollow">1.1 内置参数</a></li><li><a href="#12__374" rel="nofollow">1.2 参数传递</a></li></ul> 
   </li><li><a href="#2_380" rel="nofollow">2、引用依赖资源</a></li><li><a href="#3_388" rel="nofollow">3、数据源配置</a></li><li><a href="#4_414" rel="nofollow">4、告警实例配置</a></li><li><ul><li><a href="#41__416" rel="nofollow">4.1 邮箱告警实例配置</a></li><li><a href="#42__434" rel="nofollow">4.2 其他告警</a></li></ul> 
   </li><li><a href="#5_440" rel="nofollow">5、其他注意事项</a></li></ul> 
  </li><li><a href="#Airflow_444" rel="nofollow">三、Airflow</a></li><li><ul><li><a href="#1Airflow_446" rel="nofollow">1、Airflow基本概念</a></li><li><ul><li><a href="#11__450" rel="nofollow">1.1 概述</a></li><li><a href="#12__454" rel="nofollow">1.2 名词解释</a></li></ul> 
   </li><li><a href="#2Airflow_461" rel="nofollow">2、Airflow安装</a></li><li><ul><li><a href="#21_python_463" rel="nofollow">2.1 python环境安装</a></li><li><a href="#22_Airflow_491" rel="nofollow">2.2 安装Airflow</a></li></ul> 
   </li><li><a href="#3_544" rel="nofollow">3、修改数据库与调度器</a></li><li><ul><li><a href="#31_mysql_546" rel="nofollow">3.1 修改数据库为mysql</a></li><li><a href="#32__591" rel="nofollow">3.2 修改执行器</a></li></ul> 
   </li><li><a href="#4_607" rel="nofollow">4、部署使用</a></li><li><ul><li><a href="#41__611" rel="nofollow">4.1 环境部署启动</a></li></ul> 
   </li><li><a href="#42_Dag_696" rel="nofollow">4.2 Dag任务操作</a></li><li><ul><li><a href="#43__708" rel="nofollow">4.3 配置邮件服务器</a></li></ul> 
  </li></ul> 
  </li><li><a href="#Azkaban_794" rel="nofollow">四、Azkaban</a></li><li><ul><li><a href="#1Azkaban_798" rel="nofollow">1、Azkaban入门</a></li><li><ul><li><a href="#11_jarsql_800" rel="nofollow">1.1 上传jar包和配置sql</a></li><li><a href="#12_Executor_Server_842" rel="nofollow">1.2 配置Executor Server</a></li><li><a href="#13_Web_Server_881" rel="nofollow">1.3 配置Web Server</a></li></ul> 
   </li><li><a href="#2Work_Flow_924" rel="nofollow">2、Work Flow案例实操</a></li><li><ul><li><a href="#21_HelloWorld_926" rel="nofollow">2.1 HelloWorld案例</a></li><li><a href="#22__949" rel="nofollow">2.2 作业依赖案例</a></li><li><a href="#23__977" rel="nofollow">2.3 自动失败重试案例</a></li><li><a href="#24__1004" rel="nofollow">2.4 手动失败重试案例</a></li></ul> 
   </li><li><a href="#3JavaProcess_1059" rel="nofollow">3、JavaProcess作业类型案例</a></li><li><ul><li><a href="#31__1061" rel="nofollow">3.1 概述</a></li><li><a href="#32__1071" rel="nofollow">3.2 案例</a></li></ul> 
   </li><li><a href="#4_1099" rel="nofollow">4、条件工作流案例</a></li><li><ul><li><a href="#41__1101" rel="nofollow">4.1 概述</a></li><li><a href="#42__1105" rel="nofollow">4.2 运行时参数案例</a></li><li><a href="#43__1163" rel="nofollow">4.3 预定义宏案例</a></li></ul> 
   </li><li><a href="#5_1214" rel="nofollow">5、邮箱告警</a></li><li><a href="#6AzkabanExecutor_1245" rel="nofollow">6、Azkaban多Executor模式注意事项</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="DolphinScheduler_1"></a>一、DolphinScheduler概述和部署</h2> 
<blockquote> 
 <p>官网：<a href="https://dolphinscheduler.apache.org/" rel="nofollow" title="https://dolphinscheduler.apache.org/">https://dolphinscheduler.apache.org/</a></p> 
</blockquote> 
<h3><a id="1DolphinScheduler_5"></a>1、DolphinScheduler简介</h3> 
<h4><a id="11__7"></a>1.1 概述</h4> 
<p>Apache DolphinScheduler是一个分布式、易扩展的可视化DAG工作流任务调度平台。致力于解决数据处理流程中错综复杂的依赖关系，使调度系统在数据处理流程中开箱即用</p> 
<h4><a id="12__11"></a>1.2 核心架构</h4> 
<p>DolphinScheduler的主要角色如下：</p> 
<ul><li><strong>MasterServer</strong>采用分布式无中心设计理念，MasterServer主要负责 DAG 任务切分、任务提交、任务监控，并同时监听其它MasterServer和WorkerServer的健康状态</li><li><strong>WorkerServer</strong>也采用分布式无中心设计理念，WorkerServer主要负责任务的执行和提供日志服务</li><li><strong>ZooKeeper</strong>服务，系统中的MasterServer和WorkerServer节点都通过ZooKeeper来进行集群管理和容错</li><li><strong>Alert</strong>服务，提供告警相关服务</li><li><strong>API</strong>接口层，主要负责处理前端UI层的请求</li><li><strong>UI</strong>，系统的前端页面，提供系统的各种可视化操作界面</li></ul> 
<p><img src="https://images2.imgbox.com/18/d5/Fkx6m70z_o.png" alt=""></p> 
<h3><a id="2DolphinScheduler_24"></a>2、DolphinScheduler部署模式</h3> 
<blockquote> 
 <p><a href="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97_menu" rel="nofollow" title="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/部署指南_menu">https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/部署指南_menu</a></p> 
</blockquote> 
<h4><a id="21__28"></a>2.1 概述</h4> 
<p>DolphinScheduler支持多种部署模式，包括单机模式（Standalone）、伪集群模式（Pseudo-Cluster）、集群模式（Cluster）等</p> 
<h4><a id="22__32"></a>2.2 单机模式</h4> 
<p>单机模式（standalone）模式下，所有服务均集中于一个StandaloneServer进程中，并且其中内置了注册中心Zookeeper和数据库H2。只需配置JDK环境，就可一键启动DolphinScheduler，快速体验其功能</p> 
<p>由于DolphinScheduler的单机模式使用的是内置的ZK和数据库，故在集群模式下所做的相关配置在单机模式下并不可见，所以需要重新配置，必要的配置为创建租户和创建用户</p> 
<pre><code class="prism language-bash">bin/dolphinscheduler-daemon.sh start standalone-server
</code></pre> 
<h4><a id="23__42"></a>2.3 伪集群模式</h4> 
<p>伪集群模式（Pseudo-Cluster）是在单台机器部署 DolphinScheduler 各项服务，该模式下master、worker、api server、logger server等服务都只在同一台机器上。Zookeeper和数据库需单独安装并进行相应配置</p> 
<h4><a id="24__46"></a>2.4 集群模式</h4> 
<p>集群模式（Cluster）与伪集群模式的区别就是在多台机器部署 DolphinScheduler各项服务，并且可以配置多个Master及多个Worker</p> 
<h3><a id="3DolphinScheduler_50"></a>3、DolphinScheduler集群模式部署</h3> 
<h4><a id="31__52"></a>3.1 集群规划与准备</h4> 
<ul><li>三台节点均需部署JDK（1.8+），并配置相关环境变量</li><li>需部署数据库，支持MySQL（5.7+）或者PostgreSQL（8.2.15+）。如 MySQL 则需要 JDBC Driver 8.0.16</li><li>需部署Zookeeper（3.4.6+）</li><li>如果启用 HDFS 文件系统，则需要 Hadoop（2.6+）环境</li><li>三台节点均需安装进程管理工具包psmisc</li></ul> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> yum <span class="token function">install</span> <span class="token parameter variable">-y</span> psmisc
</code></pre> 
<h4><a id="32__64"></a>3.2 下载与配置部署脚本</h4> 
<pre><code class="prism language-bash"><span class="token function">wget</span> https://archive.apache.org/dist/dolphinscheduler/2.0.3/apache-dolphinscheduler-2.0.3-bin.tar.gz
<span class="token function">tar</span> <span class="token parameter variable">-zxvf</span> apache-dolphinscheduler-2.0.3-bin.tar,gz

</code></pre> 
<p>修改解压目录下的conf/config目录下的install_config.conf文件，不需要修改的可以直接略过</p> 
<pre><code class="prism language-bash"><span class="token comment"># ---------------------------------------------------------</span>
<span class="token comment"># INSTALL MACHINE</span>
<span class="token comment"># ---------------------------------------------------------</span>
<span class="token comment"># A comma separated list of machine hostname or IP would be installed DolphinScheduler,</span>
<span class="token comment"># including master, worker, api, alert. If you want to deploy in pseudo-distributed</span>
<span class="token comment"># mode, just write a pseudo-distributed hostname</span>
<span class="token comment"># Example for hostnames: ips="ds1,ds2,ds3,ds4,ds5", Example for IPs: ips="192.168.8.1,192.168.8.2,192.168.8.3,192.168.8.4,192.168.8.5"</span>
<span class="token assign-left variable">ips</span><span class="token operator">=</span><span class="token string">"hadoop102,hadoop103,hadoop104"</span> 
<span class="token comment"># 将要部署任一 DolphinScheduler 服务的服务器主机名或 ip 列表</span>

<span class="token comment"># Port of SSH protocol, default value is 22. For now we only support same port in all `ips` machine</span>
<span class="token comment"># modify it if you use different ssh port</span>
<span class="token assign-left variable">sshPort</span><span class="token operator">=</span><span class="token string">"22"</span>

<span class="token comment"># A comma separated list of machine hostname or IP would be installed Master server, it</span>
<span class="token comment"># must be a subset of configuration `ips`.</span>
<span class="token comment"># Example for hostnames: masters="ds1,ds2", Example for IPs: masters="192.168.8.1,192.168.8.2"</span>
<span class="token assign-left variable">masters</span><span class="token operator">=</span><span class="token string">"hadoop102"</span> 
<span class="token comment"># master 所在主机名列表，必须是 ips 的子集</span>

<span class="token comment"># A comma separated list of machine &lt;hostname&gt;:&lt;workerGroup&gt; or &lt;IP&gt;:&lt;workerGroup&gt;.All hostname or IP must be a</span>
<span class="token comment"># subset of configuration `ips`, And workerGroup have default value as `default`, but we recommend you declare behind the hosts</span>
<span class="token comment"># Example for hostnames: workers="ds1:default,ds2:default,ds3:default", Example for IPs: workers="192.168.8.1:default,192.168.8.2:default,192.168.8.3:default"</span>
<span class="token assign-left variable">workers</span><span class="token operator">=</span><span class="token string">"hadoop102:default,hadoop103:default,hadoop104:default"</span> 
<span class="token comment"># worker主机名及队列，此处的 ip 必须在 ips 列表中</span>

<span class="token comment"># A comma separated list of machine hostname or IP would be installed Alert server, it</span>
<span class="token comment"># must be a subset of configuration `ips`.</span>
<span class="token comment"># Example for hostname: alertServer="ds3", Example for IP: alertServer="192.168.8.3"</span>
<span class="token assign-left variable">alertServer</span><span class="token operator">=</span><span class="token string">"hadoop102"</span>
<span class="token comment"># 告警服务所在服务器主机名</span>
<span class="token comment"># A comma separated list of machine hostname or IP would be installed API server, it</span>
<span class="token comment"># must be a subset of configuration `ips`.</span>
<span class="token comment"># Example for hostname: apiServers="ds1", Example for IP: apiServers="192.168.8.1"</span>
<span class="token assign-left variable">apiServers</span><span class="token operator">=</span><span class="token string">"hadoop102"</span>
<span class="token comment"># api服务所在服务器主机名</span>

<span class="token comment"># A comma separated list of machine hostname or IP would be installed Python gateway server, it</span>
<span class="token comment"># must be a subset of configuration `ips`.</span>
<span class="token comment"># Example for hostname: pythonGatewayServers="ds1", Example for IP: pythonGatewayServers="192.168.8.1"</span>
<span class="token comment"># pythonGatewayServers="ds1" </span>
<span class="token comment"># 不需要的配置项，可以保留默认值，也可以用 # 注释</span>

<span class="token comment"># The directory to install DolphinScheduler for all machine we config above. It will automatically be created by `install.sh` script if not exists.</span>
<span class="token comment"># Do not set this configuration same as the current path (pwd)</span>
<span class="token assign-left variable">installPath</span><span class="token operator">=</span><span class="token string">"/opt/module/dolphinscheduler"</span>
<span class="token comment"># DS 安装路径，如果不存在会创建</span>

<span class="token comment"># The user to deploy DolphinScheduler for all machine we config above. For now user must create by yourself before running `install.sh`</span>
<span class="token comment"># script. The user needs to have sudo privileges and permissions to operate hdfs. If hdfs is enabled than the root directory needs</span>
<span class="token comment"># to be created by this user</span>
<span class="token assign-left variable">deployUser</span><span class="token operator">=</span><span class="token string">"atguigu"</span>
<span class="token comment"># 部署用户，任务执行服务是以 sudo -u {linux-user} 切换不同 Linux 用户的方式来实现多租户运行作业，因此该用户必须有免密的 sudo 权限。</span>

<span class="token comment"># The directory to store local data for all machine we config above. Make sure user `deployUser` have permissions to read and write this directory.</span>
<span class="token assign-left variable">dataBasedirPath</span><span class="token operator">=</span><span class="token string">"/tmp/dolphinscheduler"</span>
<span class="token comment"># 前文配置的所有节点的本地数据存储路径，需要确保部署用户拥有该目录的读写权限</span>

<span class="token comment"># ---------------------------------------------------------</span>
<span class="token comment"># DolphinScheduler ENV</span>
<span class="token comment"># ---------------------------------------------------------</span>
<span class="token comment"># JAVA_HOME, we recommend use same JAVA_HOME in all machine you going to install DolphinScheduler</span>
<span class="token comment"># and this configuration only support one parameter so far.</span>
<span class="token assign-left variable">javaHome</span><span class="token operator">=</span><span class="token string">"/opt/module/jdk1.8.0_212"</span>
<span class="token comment"># JAVA_HOME 路径</span>

<span class="token comment"># DolphinScheduler API service port, also this is your DolphinScheduler UI component's URL port, default value is 12345</span>
<span class="token assign-left variable">apiServerPort</span><span class="token operator">=</span><span class="token string">"12345"</span>

<span class="token comment"># ---------------------------------------------------------</span>
<span class="token comment"># Database</span>
<span class="token comment"># NOTICE: If database value has special characters, such as `.*[]^${}\+?|()@#&amp;`, Please add prefix `\` for escaping.</span>
<span class="token comment"># ---------------------------------------------------------</span>
<span class="token comment"># The type for the metadata database</span>
<span class="token comment"># Supported values: ``postgresql``, ``mysql`, `h2``.</span>
<span class="token comment"># 注意：数据库相关配置的 value 必须加引号，否则配置无法生效</span>

<span class="token assign-left variable">DATABASE_TYPE</span><span class="token operator">=</span><span class="token string">"mysql"</span>
<span class="token comment"># 数据库类型</span>

<span class="token comment"># Spring datasource url, following &lt;HOST&gt;:&lt;PORT&gt;/&lt;database&gt;?&lt;parameter&gt; format, If you using mysql, you could use jdbc</span>
<span class="token comment"># string jdbc:mysql://127.0.0.1:3306/dolphinscheduler?useUnicode=true&amp;characterEncoding=UTF-8 as example</span>
<span class="token comment"># SPRING_DATASOURCE_URL=${SPRING_DATASOURCE_URL:-"jdbc:h2:mem:dolphinscheduler;MODE=MySQL;DB_CLOSE_DELAY=-1;DATABASE_TO_LOWER=true"}</span>
<span class="token assign-left variable">SPRING_DATASOURCE_URL</span><span class="token operator">=</span><span class="token string">"jdbc:mysql://hadoop102:3306/dolphinscheduler?useUnicode=true&amp;characterEncoding=UTF-8"</span>
<span class="token comment"># 数据库 URL</span>

<span class="token comment"># Spring datasource username</span>
<span class="token comment"># SPRING_DATASOURCE_USERNAME=${SPRING_DATASOURCE_USERNAME:-"sa"}</span>
<span class="token assign-left variable">SPRING_DATASOURCE_USERNAME</span><span class="token operator">=</span><span class="token string">"dolphinscheduler"</span>
<span class="token comment"># 数据库用户名</span>

<span class="token comment"># Spring datasource password</span>
<span class="token comment"># SPRING_DATASOURCE_PASSWORD=${SPRING_DATASOURCE_PASSWORD:-""}</span>
<span class="token assign-left variable">SPRING_DATASOURCE_PASSWORD</span><span class="token operator">=</span><span class="token string">"dolphinscheduler"</span>
<span class="token comment"># 数据库密码</span>

<span class="token comment"># ---------------------------------------------------------</span>
<span class="token comment"># Registry Server</span>
<span class="token comment"># ---------------------------------------------------------</span>
<span class="token comment"># Registry Server plugin name, should be a substring of `registryPluginDir`, DolphinScheduler use this for verifying configuration consistency</span>
<span class="token assign-left variable">registryPluginName</span><span class="token operator">=</span><span class="token string">"zookeeper"</span>
<span class="token comment"># 注册中心插件名称，DS 通过注册中心来确保集群配置的一致性</span>

<span class="token comment"># Registry Server address.</span>
<span class="token assign-left variable">registryServers</span><span class="token operator">=</span><span class="token string">"hadoop102:2181,hadoop103:2181,hadoop104:2181"</span>
<span class="token comment"># 注册中心地址，即 Zookeeper 集群的地址</span>

<span class="token comment"># Registry Namespace</span>
<span class="token assign-left variable">registryNamespace</span><span class="token operator">=</span><span class="token string">"dolphinscheduler"</span>
<span class="token comment"># DS 在 Zookeeper 的结点名称</span>

<span class="token comment"># ---------------------------------------------------------</span>
<span class="token comment"># Worker Task Server</span>
<span class="token comment"># ---------------------------------------------------------</span>
<span class="token comment"># Worker Task Server plugin dir. DolphinScheduler will find and load the worker task plugin jar package from this dir.</span>
<span class="token assign-left variable">taskPluginDir</span><span class="token operator">=</span><span class="token string">"lib/plugin/task"</span>

<span class="token comment"># resource storage type: HDFS, S3, NONE</span>
<span class="token assign-left variable">resourceStorageType</span><span class="token operator">=</span><span class="token string">"HDFS"</span>  
<span class="token comment"># 资源存储类型</span>

<span class="token comment"># resource store on HDFS/S3 path, resource file will store to this hdfs path, self configuration, please make sure the directory exists on hdfs and has read write permissions. "/dolphinscheduler" is recommended</span>
<span class="token assign-left variable">resourceUploadPath</span><span class="token operator">=</span><span class="token string">"/dolphinscheduler"</span>
<span class="token comment"># 资源上传路径</span>

<span class="token comment"># if resourceStorageType is HDFS，defaultFS write namenode address，HA, you need to put core-site.xml and hdfs-site.xml in the conf directory.</span>
<span class="token comment"># if S3，write S3 address，HA，for example ：s3a://dolphinscheduler，</span>
<span class="token comment"># Note，S3 be sure to create the root directory /dolphinscheduler</span>
<span class="token assign-left variable">defaultFS</span><span class="token operator">=</span><span class="token string">"hdfs://hadoop102:8020"</span>
<span class="token comment"># 默认文件系统</span>

<span class="token comment"># if resourceStorageType is S3, the following three configuration is required, otherwise please ignore</span>
<span class="token assign-left variable">s3Endpoint</span><span class="token operator">=</span><span class="token string">"http://192.168.xx.xx:9010"</span>
<span class="token assign-left variable">s3AccessKey</span><span class="token operator">=</span><span class="token string">"xxxxxxxxxx"</span>
<span class="token assign-left variable">s3SecretKey</span><span class="token operator">=</span><span class="token string">"xxxxxxxxxx"</span>

<span class="token comment"># resourcemanager port, the default value is 8088 if not specified</span>
<span class="token assign-left variable">resourceManagerHttpAddressPort</span><span class="token operator">=</span><span class="token string">"8088"</span>
<span class="token comment"># yarn RM http 访问端口</span>

<span class="token comment"># if resourcemanager HA is enabled, please set the HA IPs; if resourcemanager is single node, keep this value empty</span>
<span class="token assign-left variable">yarnHaIps</span><span class="token operator">=</span>
<span class="token comment"># Yarn RM 高可用 ip，若未启用 RM 高可用，则将该值置空</span>

<span class="token comment"># if resourcemanager HA is enabled or not use resourcemanager, please keep the default value; If resourcemanager is single node, you only need to replace 'yarnIp1' to actual resourcemanager hostname</span>
<span class="token assign-left variable">singleYarnIp</span><span class="token operator">=</span><span class="token string">"hadoop103"</span>
<span class="token comment"># Yarn RM 主机名，若启用了 HA 或未启用 RM，保留默认值</span>

<span class="token comment"># who has permission to create directory under HDFS/S3 root path</span>
<span class="token comment"># Note: if kerberos is enabled, please config hdfsRootUser=</span>
<span class="token assign-left variable">hdfsRootUser</span><span class="token operator">=</span><span class="token string">"atguigu"</span>
<span class="token comment"># 拥有 HDFS 根目录操作权限的用户</span>

<span class="token comment"># 下面是如果hdfs开启了验证在操作的</span>
<span class="token comment"># kerberos config</span>
<span class="token comment"># whether kerberos starts, if kerberos starts, following four items need to config, otherwise please ignore</span>
<span class="token assign-left variable">kerberosStartUp</span><span class="token operator">=</span><span class="token string">"false"</span>
<span class="token comment"># kdc krb5 config file path</span>
<span class="token assign-left variable">krb5ConfPath</span><span class="token operator">=</span><span class="token string">"<span class="token variable">$installPath</span>/conf/krb5.conf"</span>
<span class="token comment"># keytab username,watch out the @ sign should followd by \\</span>
<span class="token assign-left variable">keytabUserName</span><span class="token operator">=</span><span class="token string">"hdfs-mycluster<span class="token entity" title="\\">\\</span>@ESZ.COM"</span>
<span class="token comment"># username keytab path</span>
<span class="token assign-left variable">keytabPath</span><span class="token operator">=</span><span class="token string">"<span class="token variable">$installPath</span>/conf/hdfs.headless.keytab"</span>
<span class="token comment"># kerberos expire time, the unit is hour</span>
<span class="token assign-left variable">kerberosExpireTime</span><span class="token operator">=</span><span class="token string">"2"</span>

<span class="token comment"># use sudo or not</span>
<span class="token assign-left variable">sudoEnable</span><span class="token operator">=</span><span class="token string">"true"</span>

<span class="token comment"># worker tenant auto create</span>
<span class="token assign-left variable">workerTenantAutoCreate</span><span class="token operator">=</span><span class="token string">"false"</span>
</code></pre> 
<h4><a id="33__248"></a>3.3 初始化数据库</h4> 
<p>DolphinScheduler 元数据存储在关系型数据库中，故需创建相应的数据库和用户</p> 
<pre><code class="prism language-bash"><span class="token comment"># 创建数据库</span>
CREATE DATABASE dolphinscheduler DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci<span class="token punctuation">;</span>
<span class="token comment"># 创建用户</span>
CREATE <span class="token environment constant">USER</span> <span class="token string">'dolphinscheduler'</span>@<span class="token string">'%'</span> IDENTIFIED BY <span class="token string">'dolphinscheduler'</span><span class="token punctuation">;</span>

<span class="token comment"># 提高密码复杂度或者执行以下命令降低MySQL密码强度级别</span>
<span class="token builtin class-name">set</span> global <span class="token assign-left variable">validate_password_length</span><span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">;</span>
<span class="token builtin class-name">set</span> global <span class="token assign-left variable">validate_password_policy</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>
<span class="token comment"># 赋予用户相应权限</span>
GRANT ALL PRIVILEGES ON dolphinscheduler.* TO <span class="token string">'dolphinscheduler'</span>@<span class="token string">'%'</span><span class="token punctuation">;</span>
flush privileges<span class="token punctuation">;</span>


<span class="token comment"># 拷贝MySQL驱动到DolphinScheduler的解压目录下的lib中</span>
<span class="token function">cp</span> /opt/software/mysql-connector-java-8.0.16.jar lib/

<span class="token comment"># 执行数据库初始化脚本</span>
<span class="token comment"># 数据库初始化脚本位于DolphinScheduler解压目录下的script目录中，即/opt/software/ds/apache-dolphinscheduler-2.0.3-bin/script/</span>
script/create-dolphinscheduler.sh

</code></pre> 
<h4><a id="34_DolphinScheduler_275"></a>3.4 一键部署DolphinScheduler</h4> 
<pre><code class="prism language-bash"><span class="token comment"># 启动zk</span>
zk.sh start
<span class="token comment"># 一键部署并启动DolphinScheduler</span>
./install.sh
<span class="token comment"># 查看DolphinScheduler进程</span>
<span class="token comment"># ApiApplicationServer</span>
<span class="token comment"># WorkerServer</span>
<span class="token comment"># MasterServer</span>
<span class="token comment"># AlertServer</span>
<span class="token comment"># LoggerServer</span>

<span class="token comment"># ----------</span>
<span class="token comment"># 访问DolphinScheduler UI</span>
<span class="token comment"># DolphinScheduler UI地址为http://hadoop102:12345/dolphinscheduler</span>
<span class="token comment"># 初始用户的用户名为：admin，密码为dolphinscheduler123</span>
</code></pre> 
<h4><a id="35_DolphinScheduler_295"></a>3.5 DolphinScheduler启停命令</h4> 
<p>安装完后得去<code>/opt/module/dolphinscheduler</code>修改或启停</p> 
<pre><code class="prism language-bash"><span class="token comment"># 一键启停所有服务</span>
./bin/start-all.sh
./bin/stop-all.sh
<span class="token comment"># 注意同Hadoop的启停脚本进行区分</span>
<span class="token comment"># 启停 Master</span>
./bin/dolphinscheduler-daemon.sh start master-server
./bin/dolphinscheduler-daemon.sh stop master-server
<span class="token comment"># 启停 Worker</span>
./bin/dolphinscheduler-daemon.sh start worker-server
./bin/dolphinscheduler-daemon.sh stop worker-server
<span class="token comment"># 启停 Api</span>
./bin/dolphinscheduler-daemon.sh start api-server
./bin/dolphinscheduler-daemon.sh stop api-server
<span class="token comment"># 启停 Logger</span>
./bin/dolphinscheduler-daemon.sh start logger-server
./bin/dolphinscheduler-daemon.sh stop logger-server
<span class="token comment"># 启停 Alert</span>
./bin/dolphinscheduler-daemon.sh start alert-server
./bin/dolphinscheduler-daemon.sh stop alert-server
</code></pre> 
<h2><a id="DolphinScheduler_321"></a>二、DolphinScheduler操作</h2> 
<blockquote> 
 <p>入门文档可以参考：<a href="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/guide/quick-start" rel="nofollow" title="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/guide/quick-start">https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/guide/quick-start</a></p> 
</blockquote> 
<h3><a id="1_325"></a>1、工作流传参</h3> 
<blockquote> 
 <p><a href="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D_menu/%E5%8F%82%E6%95%B0_menu" rel="nofollow" title="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/功能介绍_menu/参数_menu">https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/功能介绍_menu/参数_menu</a></p> 
</blockquote> 
<p>DolphinScheduler支持对任务节点进行灵活的传参，任务节点可通过<code>${参数名}</code>引用参数值</p> 
<h4><a id="11__331"></a>1.1 内置参数</h4> 
<p><strong>基础内置参数</strong></p> 
<table><thead><tr><th><strong>变量名</strong></th><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td><strong>system.biz.date</strong></td><td>${system.biz.date}</td><td>定时时间前一天，格式为 yyyyMMdd</td></tr><tr><td><strong>system.biz.curdate</strong></td><td>${system.biz.curdate}</td><td>定时时间，格式为 yyyyMMdd</td></tr><tr><td><strong>system.datetime</strong></td><td>${system.datetime}</td><td>定时时间，格式为 yyyyMMddHHmmss</td></tr></tbody></table> 
<p><strong>衍生内置参数</strong></p> 
<p>可通过衍生内置参数，设置任意格式、任意时间的日期。</p> 
<ul><li>自定义日期格式：可以对 $[yyyyMMddHHmmss] 任意分解组合，如 $[yyyyMMdd], $[HHmmss], $[yyyy-MM-dd]。</li><li>使用 add_months() 函数：该函数用于加减月份， 第一个入口参数为[yyyyMMdd]，表示返回时间的格式 第二个入口参数为月份偏移量，表示加减多少个月</li></ul> 
<table><thead><tr><th><strong>参数</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td><strong>$[add_months(yyyyMMdd,12*N)]</strong></td><td>后 N 年</td></tr><tr><td><strong>$[add_months(yyyyMMdd,-12*N)]</strong></td><td>前 N 年</td></tr><tr><td><strong>$[add_months(yyyyMMdd,N)]</strong></td><td>后 N 月</td></tr><tr><td><strong>$[add_months(yyyyMMdd,-N)]</strong></td><td>前 N 月</td></tr><tr><td><strong>$[yyyyMMdd+7*N]</strong></td><td>后 N 周</td></tr><tr><td><strong>$[yyyyMMdd-7*N]</strong></td><td>前 N 周</td></tr><tr><td><strong>$[yyyyMMdd+N]</strong></td><td>后 N 天</td></tr><tr><td><strong>$[yyyyMMdd-N]</strong></td><td>前 N 天</td></tr><tr><td><strong>$[HHmmss+N/24]</strong></td><td>后 N 小时</td></tr><tr><td><strong>$[HHmmss-N/24]</strong></td><td>前 N 小时</td></tr><tr><td><strong>$[HHmmss+N/24/60]</strong></td><td>后 N 分钟</td></tr><tr><td><strong>$[HHmmss-N/24/60]</strong></td><td>前 N 分钟</td></tr></tbody></table> 
<p><img src="https://images2.imgbox.com/ed/f2/uCv2v8ME_o.png" alt=""></p> 
<p>相关说明</p> 
<ul><li>dt：参数名</li><li>IN：IN 表示局部参数仅能在当前节点使用，OUT 表示局部参数可以向下游传递(目前支持这个特性的任务类型有：Shell、SQL、Procedure；同时若节点之间没有依赖关系，则局部参数无法传递)</li><li>DATE：数据类型，日期</li><li>$[yyyy-MM-dd]：自定义格式的衍生内置参数</li></ul> 
<p>全局参数在工作流定义，本地参数在节点定义，<strong>本地参数 &gt; 全局参数 &gt; 上游任务传递的参数</strong></p> 
<h4><a id="12__374"></a>1.2 参数传递</h4> 
<ul><li>本地参数 &gt; 全局参数 &gt; 上游任务传递的参数；</li><li>多个上游节点均传递同名参数时，下游节点会优先使用值为非空的参数；</li><li>如果存在多个值为非空的参数，则按照上游任务的完成时间排序，选择完成时间最早的上游任务对应的参数。</li></ul> 
<h3><a id="2_380"></a>2、引用依赖资源</h3> 
<p>有些任务需要引用一些额外的资源，例如MR、Spark等任务须引用jar包，Shell任务需要引用其他脚本等。DolphinScheduler提供了资源中心来对这些资源进行统一管理。</p> 
<p>如果需要用到资源上传功能，针对单机可以选择本地文件目录作为上传文件夹(此操作不需要部署 Hadoop)。当然也可以选择上传到 Hadoop or MinIO 集群上，此时则需要有Hadoop (2.6+) 或者 MinIO 等相关环境。本文在部署 DS 集群时指定了文件系统为 HDFS</p> 
<blockquote> 
 <p><a href="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/guide/resource" rel="nofollow" title="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/guide/resource">https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/guide/resource</a></p> 
</blockquote> 
<h3><a id="3_388"></a>3、数据源配置</h3> 
<blockquote> 
 <p><a href="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D_menu/%E6%95%B0%E6%8D%AE%E6%BA%90%E4%B8%AD%E5%BF%83_menu" rel="nofollow" title="https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/功能介绍_menu/数据源中心_menu">https://dolphinscheduler.apache.org/zh-cn/docs/2.0.3/功能介绍_menu/数据源中心_menu</a></p> 
</blockquote> 
<p>数据源中心支持MySQL、POSTGRESQL、HIVE/IMPALA、SPARK、CLICKHOUSE、ORACLE、SQLSERVER等数据源。此处仅对 HIVE 数据源进行介绍</p> 
<ul><li>数据源：选择HIVE</li><li>数据源名称：输入数据源的名称</li><li>描述：输入数据源的描述，可置空</li><li>IP/主机名：输入连接HIVE的IP</li><li>端口：输入连接HIVE的端口，默认 10000</li><li>用户名：设置连接HIVE的用户名，如果没有配置 HIVE 权限管理，则用户名可以任意，但 HIVE 表数据存储在 HDFS，为了保证对所有表的数据均有操作权限，此处选择 HDFS 超级用户 atguigu（注：HDFS 超级用户名与执行 HDFS 启动命令的 Linux 节点用户名相同）</li><li>密码：设置连接HIVE的密码，如果没有配置 HIVE 权限管理，则密码置空即可</li><li>数据库名：输入连接HIVE的数据库名称</li><li>Jdbc连接参数：用于HIVE连接的参数设置，以JSON形式填写，没有参数可置空</li></ul> 
<p>然后在工作流中可以选择SQL</p> 
<p><img src="https://images2.imgbox.com/77/f0/PruTvDmP_o.png" alt=""></p> 
<ul><li>节点名称：自定义节点名称</li><li>环境名称：HIVE 执行所需环境</li><li>数据源：类型选择 HIVE，数据源选择上文配置的 HIVE 数据源</li><li>SQL 类型：根据SQL 语句选择，此处选用默认的“查询”即可</li><li>SQL 语句：要执行的 SQL 语句，末尾不能有分号，否则报错：语法错误</li></ul> 
<h3><a id="4_414"></a>4、告警实例配置</h3> 
<h4><a id="41__416"></a>4.1 邮箱告警实例配置</h4> 
<blockquote> 
 <p><a href="https://help.mail.163.com/faqDetail.do?code=d7a5dc8471cd0c0e8b4b8f4f8e49998b374173cfe9171305fa1ce630d7f67ac21b87735d7227c217" rel="nofollow" title="POP3，IMAP，SMTP描述">POP3，IMAP，SMTP描述</a></p> 
</blockquote> 
<p>需要登陆管理员账户</p> 
<ul><li>告警实例名称：在告警组配置时可以选择的告警插件实例名称，用户自定义</li><li>选择插件：选择 Email 则为邮箱告警实例</li><li>收件人：接收方邮箱地址，收件人不需要开启 SMTP 服务</li><li>抄送人：抄送是指用户给收件人发出邮件的同时把该邮件发送给另外的人，收件人之外的收件方都是抄送人，“收件人”可以获知该邮件的所有抄送人；抄送人可以为空。</li><li>mail.smtp.host：邮箱的 SMTP 服务器域名，对于 QQ 邮箱，为 <a href="http://smtp.qq.com" rel="nofollow" title="smtp.qq.com">smtp.qq.com</a>。各邮箱的 SMTP 服务器见此链接：<a href="https://blog.csdn.net/wustzjf/article/details/52481309" title="https://blog.csdn.net/wustzjf/article/details/52481309">https://blog.csdn.net/wustzjf/article/details/52481309</a></li><li>mail.smtp.port：邮箱的 SMTP 服务端口号，主流邮箱均为 25 端口，使用默认值即可</li><li>mail.sender：发件方邮箱地址，需要开启 SMTP 服务</li><li>mail.user：与 mail.sender 保持一致即可</li><li>mail.password：获取的邮箱授权码。未列出的选项保留默认值或默认选项即可</li></ul> 
<p><img src="https://images2.imgbox.com/f9/e3/lomuJKLU_o.png" alt=""></p> 
<h4><a id="42__434"></a>4.2 其他告警</h4> 
<blockquote> 
 <p>其他告警可以参考：<a href="https://dolphinscheduler.apache.org/zh-cn/docs/3.0.0" rel="nofollow" title="https://dolphinscheduler.apache.org/zh-cn/docs/3.0.0">https://dolphinscheduler.apache.org/zh-cn/docs/3.0.0</a></p> 
</blockquote> 
<p>同时还可以电话告警，这里有个运维平台是一站式集成的，睿象云官网：<a href="https://www.aiops.com/" rel="nofollow" title="https://www.aiops.com/">https://www.aiops.com/</a></p> 
<h3><a id="5_440"></a>5、其他注意事项</h3> 
<p>DolphinScheduler的环境变量是不和主机共享的，默认需要进入<code>/opt/module/dolphinscheduler/conf/env/dolphinscheduler_env.sh</code>进行修改，也可以直接在admin用户下在可视化界面进行创建，创建节点的时候选择即可</p> 
<h2><a id="Airflow_444"></a>三、Airflow</h2> 
<h3><a id="1Airflow_446"></a>1、Airflow基本概念</h3> 
<blockquote> 
 <p>官方网站：<a href="https://airflow.apache.org" rel="nofollow" title="https://airflow.apache.org">https://airflow.apache.org</a></p> 
</blockquote> 
<h4><a id="11__450"></a>1.1 概述</h4> 
<p>Airflow是一个以编程方式编写，安排和监视工作流的平台。使用Airflow将工作流编写任务的有向无环图（DAG）。Airflow计划程序在遵循指定的依赖项，同时在一组工作线程上执行任务。丰富的命令实用程序使在DAG上执行复杂的调度变的轻而易举。丰富的用户界面使查看生产中正在运行的管道，监视进度以及需要时对问题进行故障排除变的容易</p> 
<h4><a id="12__454"></a>1.2 名词解释</h4> 
<ul><li><strong>Dynamic</strong>：Airflow配置需要实用Python，允许动态生产管道。这允许编写可动态。这允许编写可动态实例化管道的代码</li><li><strong>Extensible</strong>：轻松定义自己的运算符，执行程序并扩展库，使其适合于您的环境</li><li><strong>Elegant</strong>：Airlfow是精简的，使用功能强大的Jinja模板引擎，将脚本参数化内置于Airflow的核心中</li><li><strong>Scalable</strong>：Airflow具有模板块架构，并使用消息队列来安排任意数量的工作任务</li></ul> 
<h3><a id="2Airflow_461"></a>2、Airflow安装</h3> 
<h4><a id="21_python_463"></a>2.1 python环境安装</h4> 
<pre><code class="prism language-bash"><span class="token comment"># Superset是由Python语言编写的Web应用，要求Python3.8的环境</span>
<span class="token comment"># 这里使用MiniConda作为包管理器</span>
<span class="token comment"># 下载地址：https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</span>
<span class="token function">bash</span> Miniconda3-latest-Linux-x86_64.sh
<span class="token comment"># 加载环境变量配置文件，使之生效</span>
<span class="token builtin class-name">source</span> ~/.bashrc
<span class="token comment"># Miniconda安装完成后，每次打开终端都会激活其默认的base环境，我们可通过以下命令，禁止激活默认base环境</span>
conda config <span class="token parameter variable">--set</span> auto_activate_base <span class="token boolean">false</span>

<span class="token comment"># 配置conda国内镜像</span>
conda config <span class="token parameter variable">--add</span> channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free
conda config <span class="token parameter variable">--add</span> channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
conda config <span class="token parameter variable">--set</span> show_channel_urls <span class="token function">yes</span>
<span class="token comment"># 创建Python3.8环境</span>
conda create <span class="token parameter variable">--name</span> airflow <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.8</span>
<span class="token comment"># 创建环境：conda create -n env_name</span>
<span class="token comment"># 查看所有环境：conda info --envs</span>
<span class="token comment"># 删除一个环境：conda remove -n env_name --all</span>
<span class="token comment"># 激活airflow环境</span>
conda activate airflow
<span class="token comment"># 执行python -V命令查看python版本</span>
python <span class="token parameter variable">-V</span>

</code></pre> 
<h4><a id="22_Airflow_491"></a>2.2 安装Airflow</h4> 
<pre><code class="prism language-bash">conda activate airflow
pip <span class="token function">install</span> numpy <span class="token parameter variable">-i</span> https://pypi.tuna.tsinghua.edu.cn/simple
<span class="token function">sudo</span> <span class="token function">mkdir</span> ~/.pip
<span class="token function">sudo</span> <span class="token function">vim</span>  ~/.pip/pip.conf
<span class="token comment">#添加以下内容</span>
<span class="token punctuation">[</span>global<span class="token punctuation">]</span>
index-url <span class="token operator">=</span> https://pypi.tuna.tsinghua.edu.cn/simple
<span class="token punctuation">[</span>install<span class="token punctuation">]</span>
trusted-host <span class="token operator">=</span> https://pypi.tuna.tsinghua.edu.cn
<span class="token comment"># 安装airflow</span>
pip <span class="token function">install</span> <span class="token string">"apache-airflow==2.4.3"</span>
<span class="token comment"># 初始化airflow</span>
airflow db init
<span class="token comment"># 查看版本</span>
airflow version
<span class="token comment"># airflow安装好存放路径</span>
<span class="token builtin class-name">pwd</span>
<span class="token comment"># 启动airflow web服务,启动后浏览器访问http://hadoop102:8081</span>
airflow webserver <span class="token parameter variable">-p</span> <span class="token number">8081</span> <span class="token parameter variable">-D</span>
<span class="token comment"># 启动airflow调度</span>
airflow scheduler <span class="token parameter variable">-D</span>
<span class="token comment"># 创建账号</span>
airflow <span class="token function">users</span> create <span class="token punctuation">\</span>
<span class="token parameter variable">--username</span> admin <span class="token punctuation">\</span>
<span class="token parameter variable">--firstname</span> atguigu <span class="token punctuation">\</span>
<span class="token parameter variable">--lastname</span> atguigu <span class="token punctuation">\</span>
<span class="token parameter variable">--role</span> Admin <span class="token punctuation">\</span>
<span class="token parameter variable">--email</span> shawn@atguigu.com


<span class="token comment"># 启动停止脚本</span>
<span class="token function">vim</span> af.sh
<span class="token comment">#!/bin/bash</span>

<span class="token keyword">case</span> <span class="token variable">$1</span> <span class="token keyword">in</span>
<span class="token string">"start"</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
    <span class="token builtin class-name">echo</span> <span class="token string">" --------启动 airflow-------"</span>
    <span class="token function">ssh</span> hadoop102 <span class="token string">"conda activate airflow;airflow webserver -p 8081 -D;airflow scheduler -D; conda deactivate"</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token punctuation">;</span>
<span class="token string">"stop"</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
    <span class="token builtin class-name">echo</span> <span class="token string">" --------关闭 airflow-------"</span>
    <span class="token function">ps</span> -ef<span class="token operator">|</span><span class="token function">egrep</span> <span class="token string">'scheduler|airflow-webserver'</span><span class="token operator">|</span><span class="token function">grep</span> <span class="token parameter variable">-v</span> <span class="token function">grep</span><span class="token operator">|</span><span class="token function">awk</span> <span class="token string">'{print $2}'</span><span class="token operator">|</span><span class="token function">xargs</span> <span class="token function">kill</span> <span class="token parameter variable">-15</span> 
<span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token punctuation">;</span>
<span class="token keyword">esac</span>

<span class="token comment"># 添加权限即可使用</span>
<span class="token function">chmod</span> +x af.sh

</code></pre> 
<h3><a id="3_544"></a>3、修改数据库与调度器</h3> 
<h4><a id="31_mysql_546"></a>3.1 修改数据库为mysql</h4> 
<pre><code class="prism language-bash"><span class="token comment"># https://airflow.apache.org/docs/apache-airflow/2.4.3/howto/set-up-database.html#setting-up-a-mysql-database</span>
<span class="token comment"># 在MySQL中建库</span>
CREATE DATABASE airflow_db CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci<span class="token punctuation">;</span>
<span class="token comment"># 如果报错Linux error:1425F102:SSL routines:ssl_choose_client_version:unsupported protocol，可以关闭MySQL的SSL证书</span>
SHOW VARIABLES LIKE <span class="token string">'%ssl%'</span><span class="token punctuation">;</span>
<span class="token comment"># 修改配置文件my.cnf，加入以下内容</span>
<span class="token comment"># disable_ssl</span>
skip_ssl

<span class="token comment"># 添加python连接的依赖：</span>
pip <span class="token function">install</span> mysql-connector-python
<span class="token comment"># 修改airflow的配置文件</span>
<span class="token function">vim</span> ~/airflow/airflow.cfg
<span class="token punctuation">[</span>database<span class="token punctuation">]</span>
<span class="token comment"># The SqlAlchemy connection string to the metadata database.</span>
<span class="token comment"># SqlAlchemy supports many different database engines.</span>
<span class="token comment"># More information here:</span>
<span class="token comment"># http://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html#database-uri</span>
<span class="token comment">#sql_alchemy_conn = sqlite:home/atguigu/airflow/airflow.db</span>
sql_alchemy_conn <span class="token operator">=</span> mysql+mysqlconnector://root:123456@hadoop102:3306/airflow_db

<span class="token comment"># 关闭airflow，初始化后重启</span>
af.sh stop
airflow db init
<span class="token comment"># 初始化报错1067 - Invalid default value for ‘update_at’</span>
<span class="token comment"># 原因：字段 'update_at' 为 timestamp类型，取值范围是：1970-01-01 00:00:00 到 2037-12-31 23:59:59（UTC +8 北京时间从1970-01-01 08:00:00 开始），而这里默认给了空值，所以导致失败</span>
<span class="token builtin class-name">set</span> GLOBAL <span class="token assign-left variable">sql_mode</span><span class="token operator">=</span><span class="token string">'STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'</span><span class="token punctuation">;</span>
<span class="token comment"># 重启MySQL会造成参数失效，推荐将参数写入到配置文件my.cnf中</span>
sql_mode <span class="token operator">=</span> STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION 
<span class="token comment"># 重启</span>
af.sh start

<span class="token comment"># 重新创建账号登录</span>
airflow <span class="token function">users</span> create <span class="token punctuation">\</span>
<span class="token parameter variable">--username</span> admin <span class="token punctuation">\</span>
<span class="token parameter variable">--firstname</span> atguigu <span class="token punctuation">\</span>
<span class="token parameter variable">--lastname</span> atguigu <span class="token punctuation">\</span>
<span class="token parameter variable">--role</span> Admin <span class="token punctuation">\</span>
<span class="token parameter variable">--email</span> shawn@atguigu.com

</code></pre> 
<h4><a id="32__591"></a>3.2 修改执行器</h4> 
<p>官网不推荐在开发中使用顺序执行器，会造成任务调度阻塞</p> 
<pre><code class="prism language-bash"><span class="token comment"># 修改airflow的配置文件</span>
<span class="token punctuation">[</span>core<span class="token punctuation">]</span>
<span class="token comment"># The executor class that airflow should use. Choices include</span>
<span class="token comment"># ``SequentialExecutor``, ``LocalExecutor``, ``CeleryExecutor``, ``DaskExecutor``,</span>
<span class="token comment"># ``KubernetesExecutor``, ``CeleryKubernetesExecutor`` or the</span>
<span class="token comment"># full import path to the class when using a custom executor.</span>
executor <span class="token operator">=</span> LocalExecutor

<span class="token comment"># dags_folder是保存文件位置</span>
</code></pre> 
<h3><a id="4_607"></a>4、部署使用</h3> 
<blockquote> 
 <p>文档：<a href="https://airflow.apache.org/docs/apache-airflow/2.4.3/howto/index.html" rel="nofollow" title="https://airflow.apache.org/docs/apache-airflow/2.4.3/howto/index.html">https://airflow.apache.org/docs/apache-airflow/2.4.3/howto/index.html</a></p> 
</blockquote> 
<h4><a id="41__611"></a>4.1 环境部署启动</h4> 
<pre><code class="prism language-bash"><span class="token comment"># 需要启动hadoop和spark的历史服务器</span>
<span class="token comment"># 编写.py脚本，创建work-py目录用于存放python调度脚本</span>
<span class="token function">mkdir</span> ~/airflow/dags 
<span class="token builtin class-name">cd</span> dags/
<span class="token function">vim</span> test.py

</code></pre> 
<p>编写脚本</p> 
<pre><code class="prism language-python"><span class="token comment">#!/usr/bin/python</span>
<span class="token keyword">from</span> airflow <span class="token keyword">import</span> DAG
<span class="token keyword">from</span> airflow<span class="token punctuation">.</span>operators<span class="token punctuation">.</span>bash_operator <span class="token keyword">import</span> BashOperator
<span class="token keyword">from</span> datetime <span class="token keyword">import</span> datetime<span class="token punctuation">,</span> timedelta

default_args <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment"># 用户</span>
    <span class="token string">'owner'</span><span class="token punctuation">:</span> <span class="token string">'test_owner'</span><span class="token punctuation">,</span>
    <span class="token comment"># 是否开启任务依赖</span>
    <span class="token string">'depends_on_past'</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span> 
    <span class="token comment"># 邮箱</span>
    <span class="token string">'email'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'403627000@qq.com'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token comment"># 启动时间</span>
    <span class="token string">'start_date'</span><span class="token punctuation">:</span>datetime<span class="token punctuation">(</span><span class="token number">2022</span><span class="token punctuation">,</span><span class="token number">11</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 出错是否发邮件报警</span>
    <span class="token string">'email_on_failure'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    <span class="token comment"># 重试是否发邮件报警</span>
    <span class="token string">'email_on_retry'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    <span class="token comment"># 重试次数</span>
    <span class="token string">'retries'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
    <span class="token comment"># 重试时间间隔</span>
    <span class="token string">'retry_delay'</span><span class="token punctuation">:</span> timedelta<span class="token punctuation">(</span>minutes<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
<span class="token comment"># 声明任务图</span>
dag <span class="token operator">=</span> DAG<span class="token punctuation">(</span><span class="token string">'test'</span><span class="token punctuation">,</span> default_args<span class="token operator">=</span>default_args<span class="token punctuation">,</span> schedule_interval<span class="token operator">=</span>timedelta<span class="token punctuation">(</span>days<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 
<span class="token comment"># 创建单个任务</span>
t1 <span class="token operator">=</span> BashOperator<span class="token punctuation">(</span>
    <span class="token comment"># 任务id</span>
    task_id<span class="token operator">=</span><span class="token string">'dwd'</span><span class="token punctuation">,</span>
    <span class="token comment"># 任务命令</span>
    bash_command<span class="token operator">=</span><span class="token string">'ssh hadoop102 "/opt/module/spark-yarn/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn /opt/module/spark-yarn/examples/jars/spark-examples_2.12-3.1.3.jar 10 "'</span><span class="token punctuation">,</span>
    <span class="token comment"># 重试次数</span>
    retries<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
    <span class="token comment"># 把任务添加进图中</span>
    dag<span class="token operator">=</span>dag<span class="token punctuation">)</span>

t2 <span class="token operator">=</span> BashOperator<span class="token punctuation">(</span>
    task_id<span class="token operator">=</span><span class="token string">'dws'</span><span class="token punctuation">,</span>
    bash_command<span class="token operator">=</span><span class="token string">'ssh hadoop102 "/opt/module/spark-yarn/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn /opt/module/spark-yarn/examples/jars/spark-examples_2.12-3.1.3.jar 10 "'</span><span class="token punctuation">,</span>
    retries<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
    dag<span class="token operator">=</span>dag<span class="token punctuation">)</span>

t3 <span class="token operator">=</span> BashOperator<span class="token punctuation">(</span>
    task_id<span class="token operator">=</span><span class="token string">'ads'</span><span class="token punctuation">,</span>
    bash_command<span class="token operator">=</span><span class="token string">'ssh hadoop102 "/opt/module/spark-yarn/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn /opt/module/spark-yarn/examples/jars/spark-examples_2.12-3.1.3.jar 10 "'</span><span class="token punctuation">,</span>
    retries<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
    dag<span class="token operator">=</span>dag<span class="token punctuation">)</span>

<span class="token comment"># 设置任务依赖</span>
t2<span class="token punctuation">.</span>set_upstream<span class="token punctuation">(</span>t1<span class="token punctuation">)</span>
t3<span class="token punctuation">.</span>set_upstream<span class="token punctuation">(</span>t2<span class="token punctuation">)</span>
</code></pre> 
<p>注意一些注意事项</p> 
<ul><li> <p>必须导包</p> <p>from airflow import DAG</p> <p>from airflow.operators.bash_operator import BashOperator</p> </li><li> <p>default_args 设置默认参数</p> </li><li> <p>depends_on_past 是否开启任务依赖</p> </li><li> <p>schedule_interval 调度频率</p> </li><li> <p>retries 重试次数 </p> </li><li> <p>start_date 开始时间</p> </li><li> <p>BashOperator 具体执行任务，如果为true前置任务必须成功完成才会走下一个依赖任务，如果为false则忽略是否成功完成</p> </li><li> <p>task_id 任务唯一标识（必填）</p> </li><li> <p>bash_command 具体任务执行命令</p> </li><li> <p>set_upstream 设置依赖</p> </li></ul> 
<h3><a id="42_Dag_696"></a>4.2 Dag任务操作</h3> 
<pre><code class="prism language-bash"><span class="token comment"># 过段时间会加载</span>
airflow dags list
<span class="token comment"># 查看所有任务</span>
airflow list_dags 
<span class="token comment"># 查看单个任务</span>
airflow tasks list <span class="token builtin class-name">test</span> <span class="token parameter variable">--tree</span>
<span class="token comment"># 如果删除的话需要UI和底层都删除才行</span>
</code></pre> 
<h4><a id="43__708"></a>4.3 配置邮件服务器</h4> 
<p>修改airflow配置文件，用stmps服务对应587端口， </p> 
<pre><code class="prism language-bash"><span class="token function">vim</span> ~/airflow/airflow.cfg 
smtp_host <span class="token operator">=</span> smtp.qq.com
smtp_starttls <span class="token operator">=</span> True
smtp_ssl <span class="token operator">=</span> False
smtp_user <span class="token operator">=</span> xx@qq.com
<span class="token comment"># smtp_user =</span>
smtp_password <span class="token operator">=</span> qluxdbuhgrhgbigi
<span class="token comment"># smtp_password =</span>
smtp_port <span class="token operator">=</span> <span class="token number">587</span>
smtp_mail_from <span class="token operator">=</span> xx@qq.com

<span class="token comment"># 然后重启</span>
af.sh stop
af.sh star
<span class="token comment"># 编辑test.py脚本，并且替换</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token comment">#!/usr/bin/python</span>
<span class="token keyword">from</span> airflow <span class="token keyword">import</span> DAG
<span class="token keyword">from</span> airflow<span class="token punctuation">.</span>operators<span class="token punctuation">.</span>bash_operator <span class="token keyword">import</span> BashOperator
<span class="token keyword">from</span> airflow<span class="token punctuation">.</span>operators<span class="token punctuation">.</span>email_operator <span class="token keyword">import</span> EmailOperator
<span class="token keyword">from</span> datetime <span class="token keyword">import</span> datetime<span class="token punctuation">,</span> timedelta

default_args <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment"># 用户</span>
    <span class="token string">'owner'</span><span class="token punctuation">:</span> <span class="token string">'test_owner'</span><span class="token punctuation">,</span>
    <span class="token comment"># 是否开启任务依赖</span>
    <span class="token string">'depends_on_past'</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span> 
    <span class="token comment"># 邮箱</span>
    <span class="token string">'email'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'xx@qq.com'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token comment"># 启动时间</span>
    <span class="token string">'start_date'</span><span class="token punctuation">:</span>datetime<span class="token punctuation">(</span><span class="token number">2022</span><span class="token punctuation">,</span><span class="token number">11</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 出错是否发邮件报警</span>
    <span class="token string">'email_on_failure'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    <span class="token comment"># 重试是否发邮件报警</span>
    <span class="token string">'email_on_retry'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    <span class="token comment"># 重试次数</span>
    <span class="token string">'retries'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
    <span class="token comment"># 重试时间间隔</span>
    <span class="token string">'retry_delay'</span><span class="token punctuation">:</span> timedelta<span class="token punctuation">(</span>minutes<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
<span class="token comment"># 声明任务图</span>
dag <span class="token operator">=</span> DAG<span class="token punctuation">(</span><span class="token string">'test'</span><span class="token punctuation">,</span> default_args<span class="token operator">=</span>default_args<span class="token punctuation">,</span> schedule_interval<span class="token operator">=</span>timedelta<span class="token punctuation">(</span>days<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 
<span class="token comment"># 创建单个任务</span>
t1 <span class="token operator">=</span> BashOperator<span class="token punctuation">(</span>
    <span class="token comment"># 任务id</span>
    task_id<span class="token operator">=</span><span class="token string">'dwd'</span><span class="token punctuation">,</span>
    <span class="token comment"># 任务命令</span>
    bash_command<span class="token operator">=</span><span class="token string">'ssh hadoop102 "/opt/module/spark-yarn/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn /opt/module/spark-yarn/examples/jars/spark-examples_2.12-3.1.3.jar 10 "'</span><span class="token punctuation">,</span>
    <span class="token comment"># 重试次数</span>
    retries<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
    <span class="token comment"># 把任务添加进图中</span>
    dag<span class="token operator">=</span>dag<span class="token punctuation">)</span>

t2 <span class="token operator">=</span> BashOperator<span class="token punctuation">(</span>
    task_id<span class="token operator">=</span><span class="token string">'dws'</span><span class="token punctuation">,</span>
    bash_command<span class="token operator">=</span><span class="token string">'ssh hadoop102 "/opt/module/spark-yarn/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn /opt/module/spark-yarn/examples/jars/spark-examples_2.12-3.1.3.jar 10 "'</span><span class="token punctuation">,</span>
    retries<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
    dag<span class="token operator">=</span>dag<span class="token punctuation">)</span>

t3 <span class="token operator">=</span> BashOperator<span class="token punctuation">(</span>
    task_id<span class="token operator">=</span><span class="token string">'ads'</span><span class="token punctuation">,</span>
    bash_command<span class="token operator">=</span><span class="token string">'ssh hadoop102 "/opt/module/spark-yarn/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn /opt/module/spark-yarn/examples/jars/spark-examples_2.12-3.1.3.jar 10 "'</span><span class="token punctuation">,</span>
    retries<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
    dag<span class="token operator">=</span>dag<span class="token punctuation">)</span>

email<span class="token operator">=</span>EmailOperator<span class="token punctuation">(</span>
   task_id<span class="token operator">=</span><span class="token string">"email"</span><span class="token punctuation">,</span>
   to<span class="token operator">=</span><span class="token string">"yaohm163@163.com "</span><span class="token punctuation">,</span>
    subject<span class="token operator">=</span><span class="token string">"test-subject"</span><span class="token punctuation">,</span>
    html_content<span class="token operator">=</span><span class="token string">"&lt;h1&gt;test-content&lt;/h1&gt;"</span><span class="token punctuation">,</span>
    cc<span class="token operator">=</span><span class="token string">"xx@qq.com "</span><span class="token punctuation">,</span>
   dag<span class="token operator">=</span>dag<span class="token punctuation">)</span>

t2<span class="token punctuation">.</span>set_upstream<span class="token punctuation">(</span>t1<span class="token punctuation">)</span>
t3<span class="token punctuation">.</span>set_upstream<span class="token punctuation">(</span>t2<span class="token punctuation">)</span>
email<span class="token punctuation">.</span>set_upstream<span class="token punctuation">(</span>t3<span class="token punctuation">)</span>
</code></pre> 
<h2><a id="Azkaban_794"></a>四、Azkaban</h2> 
<blockquote> 
 <p>azkaban官网：<a href="https://azkaban.github.io/downloads.html" rel="nofollow" title="https://azkaban.github.io/downloads.html">https://azkaban.github.io/downloads.html</a></p> 
</blockquote> 
<h3><a id="1Azkaban_798"></a>1、Azkaban入门</h3> 
<h4><a id="11_jarsql_800"></a>1.1 上传jar包和配置sql</h4> 
<p>首先获取azkaban的三个包，可以自行编译，<a href="https://github.com/azkaban/azkaban" title="github地址">github地址</a></p> 
<pre><code class="prism language-bash"><span class="token comment"># https://pan.baidu.com/s/10zD2Y_h0oB_rC-BAjLal1g%C2%A0  密码：zsxa</span>
<span class="token comment"># 将azkaban-db-3.84.4.tar.gz，azkaban-exec-server-3.84.4.tar.gz，azkaban-web-server-3.84.4.tar.gz上传到hadoop102的/opt/software路径</span>
<span class="token comment"># 新建/opt/module/azkaban目录，并将所有tar包解压到这个目录下</span>
<span class="token function">mkdir</span> /opt/module/azkaban
<span class="token comment"># 解压</span>
<span class="token function">tar</span> <span class="token parameter variable">-zxvf</span> azkaban-db-3.84.4.tar.gz <span class="token parameter variable">-C</span> /opt/module/azkaban/
<span class="token function">tar</span> <span class="token parameter variable">-zxvf</span> azkaban-exec-server-3.84.4.tar.gz <span class="token parameter variable">-C</span> /opt/module/azkaban/
<span class="token function">tar</span> <span class="token parameter variable">-zxvf</span> azkaban-web-server-3.84.4.tar.gz <span class="token parameter variable">-C</span> /opt/module/azkaban/
<span class="token comment"># 进入到/opt/module/azkaban目录，依次修改名称</span>
<span class="token function">mv</span> azkaban-exec-server-3.84.4/ azkaban-exec
<span class="token function">mv</span> azkaban-web-server-3.84.4/ azkaban-web

<span class="token comment"># ==============然后配置mysql=====================</span>
mysql <span class="token parameter variable">-uroot</span> <span class="token parameter variable">-p123456</span>
<span class="token comment"># 登陆MySQL，创建Azkaban数据库</span>
create database azkaban<span class="token punctuation">;</span>
<span class="token comment"># 创建azkaban用户并赋予权限</span>
<span class="token builtin class-name">set</span> global <span class="token assign-left variable">validate_password_length</span><span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">;</span>
<span class="token builtin class-name">set</span> global <span class="token assign-left variable">validate_password_policy</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>
CREATE <span class="token environment constant">USER</span> <span class="token string">'azkaban'</span>@<span class="token string">'%'</span> IDENTIFIED BY <span class="token string">'000000'</span><span class="token punctuation">;</span>
<span class="token comment"># 赋予Azkaban用户增删改查权限 </span>
GRANT SELECT,INSERT,UPDATE,DELETE ON azkaban.* to <span class="token string">'azkaban'</span>@<span class="token string">'%'</span> WITH GRANT OPTION<span class="token punctuation">;</span>
<span class="token comment"># 创建Azkaban表，完成后退出MySQL</span>
use azkaban<span class="token punctuation">;</span>
<span class="token builtin class-name">source</span> /opt/module/azkaban/azkaban-db-3.84.4/create-all-sql-3.84.4.sql
quit<span class="token punctuation">;</span>

<span class="token comment"># 更改MySQL包大小；防止Azkaban连接MySQL阻塞</span>
<span class="token function">sudo</span> <span class="token function">vim</span> /etc/my.cnf
<span class="token comment"># 在[mysqld]下面加一行max_allowed_packet=1024M</span>
<span class="token punctuation">[</span>mysqld<span class="token punctuation">]</span>
<span class="token assign-left variable">max_allowed_packet</span><span class="token operator">=</span>1024M
<span class="token comment"># 重启MySQL</span>
<span class="token function">sudo</span> systemctl restart mysqld

</code></pre> 
<h4><a id="12_Executor_Server_842"></a>1.2 配置Executor Server</h4> 
<p>Azkaban Executor Server处理工作流和作业的实际执行</p> 
<pre><code class="prism language-bash"><span class="token comment"># 编辑azkaban.properties</span>
<span class="token function">vim</span> /opt/module/azkaban/azkaban-exec/conf/azkaban.properties
<span class="token comment"># 修改如下属性</span>
<span class="token comment">#...</span>
<span class="token assign-left variable">default.timezone.id</span><span class="token operator">=</span>Asia/Shanghai
<span class="token comment">#...</span>
<span class="token assign-left variable">azkaban.webserver.url</span><span class="token operator">=</span>http://hadoop102:8081

<span class="token assign-left variable">executor.port</span><span class="token operator">=</span><span class="token number">12321</span>
<span class="token comment">#...</span>
<span class="token assign-left variable">database.type</span><span class="token operator">=</span>mysql
<span class="token assign-left variable">mysql.port</span><span class="token operator">=</span><span class="token number">3306</span>
<span class="token assign-left variable">mysql.host</span><span class="token operator">=</span>hadoop102
<span class="token assign-left variable">mysql.database</span><span class="token operator">=</span>azkaban
<span class="token assign-left variable">mysql.user</span><span class="token operator">=</span>azkaban
<span class="token assign-left variable">mysql.password</span><span class="token operator">=</span>000000
<span class="token assign-left variable">mysql.numconnections</span><span class="token operator">=</span><span class="token number">100</span>

<span class="token comment"># 同步azkaban-exec到所有节点</span>
xsync /opt/module/azkaban/azkaban-exec
<span class="token comment"># 必须进入到/opt/module/azkaban/azkaban-exec路径，分别在三台机器上，启动executor server</span>
bin/start-exec.sh
bin/start-exec.sh
bin/start-exec.sh
<span class="token comment"># 注意：如果在/opt/module/azkaban/azkaban-exec目录下出现executor.port文件，说明启动成功</span>
<span class="token comment"># 下面激活executor，需要分别在三台机器依次执行</span>
<span class="token function">curl</span> <span class="token parameter variable">-G</span> <span class="token string">"hadoop102:12321/executor?action=activate"</span> <span class="token operator">&amp;&amp;</span> <span class="token builtin class-name">echo</span>
<span class="token function">curl</span> <span class="token parameter variable">-G</span> <span class="token string">"hadoop103:12321/executor?action=activate"</span> <span class="token operator">&amp;&amp;</span> <span class="token builtin class-name">echo</span>
<span class="token function">curl</span> <span class="token parameter variable">-G</span> <span class="token string">"hadoop104:12321/executor?action=activate"</span> <span class="token operator">&amp;&amp;</span> <span class="token builtin class-name">echo</span>
<span class="token comment"># 如果三台机器都出现如下提示，则表示激活成功</span>
<span class="token punctuation">{<!-- --></span><span class="token string">"status"</span><span class="token builtin class-name">:</span><span class="token string">"success"</span><span class="token punctuation">}</span>

</code></pre> 
<h4><a id="13_Web_Server_881"></a>1.3 配置Web Server</h4> 
<p>Azkaban Web Server处理项目管理，身份验证，计划和执行触发</p> 
<pre><code class="prism language-bash"><span class="token comment"># 编辑azkaban.properties</span>
<span class="token function">vim</span> /opt/module/azkaban/azkaban-web/conf/azkaban.properties
<span class="token comment"># 修改如下属性</span>
<span class="token punctuation">..</span>.
<span class="token assign-left variable">default.timezone.id</span><span class="token operator">=</span>Asia/Shanghai
<span class="token punctuation">..</span>.
<span class="token assign-left variable">database.type</span><span class="token operator">=</span>mysql
<span class="token assign-left variable">mysql.port</span><span class="token operator">=</span><span class="token number">3306</span>
<span class="token assign-left variable">mysql.host</span><span class="token operator">=</span>hadoop102
<span class="token assign-left variable">mysql.database</span><span class="token operator">=</span>azkaban
<span class="token assign-left variable">mysql.user</span><span class="token operator">=</span>azkaban
<span class="token assign-left variable">mysql.password</span><span class="token operator">=</span>000000
<span class="token assign-left variable">mysql.numconnections</span><span class="token operator">=</span><span class="token number">100</span>
<span class="token punctuation">..</span>.
<span class="token assign-left variable">azkaban.executorselector.filters</span><span class="token operator">=</span>StaticRemainingFlowSize,CpuStatus

<span class="token comment"># 说明：</span>
<span class="token comment"># StaticRemainingFlowSize：正在排队的任务数；</span>
<span class="token comment"># CpuStatus：CPU占用情况</span>
<span class="token comment"># MinimumFreeMemory：内存占用情况。测试环境，必须将MinimumFreeMemory删除掉，否则它会认为集群资源不够，不执行。</span>


<span class="token comment"># 修改azkaban-users.xml文件，添加atguigu用户</span>
<span class="token function">vim</span> /opt/module/azkaban/azkaban-web/conf/azkaban-users.xml
<span class="token operator">&lt;</span>azkaban-users<span class="token operator">&gt;</span>
  <span class="token operator">&lt;</span>user <span class="token assign-left variable">groups</span><span class="token operator">=</span><span class="token string">"azkaban"</span> <span class="token assign-left variable">password</span><span class="token operator">=</span><span class="token string">"azkaban"</span> <span class="token assign-left variable">roles</span><span class="token operator">=</span><span class="token string">"admin"</span> <span class="token assign-left variable">username</span><span class="token operator">=</span><span class="token string">"azkaban"</span>/<span class="token operator">&gt;</span>
  <span class="token operator">&lt;</span>user <span class="token assign-left variable">password</span><span class="token operator">=</span><span class="token string">"metrics"</span> <span class="token assign-left variable">roles</span><span class="token operator">=</span><span class="token string">"metrics"</span> <span class="token assign-left variable">username</span><span class="token operator">=</span><span class="token string">"metrics"</span>/<span class="token operator">&gt;</span>
  <span class="token operator">&lt;</span>user <span class="token assign-left variable">password</span><span class="token operator">=</span><span class="token string">"atguigu"</span> <span class="token assign-left variable">roles</span><span class="token operator">=</span><span class="token string">"admin"</span> <span class="token assign-left variable">username</span><span class="token operator">=</span><span class="token string">"atguigu"</span>/<span class="token operator">&gt;</span>
  <span class="token operator">&lt;</span>role <span class="token assign-left variable">name</span><span class="token operator">=</span><span class="token string">"admin"</span> <span class="token assign-left variable">permissions</span><span class="token operator">=</span><span class="token string">"ADMIN"</span>/<span class="token operator">&gt;</span>
  <span class="token operator">&lt;</span>role <span class="token assign-left variable">name</span><span class="token operator">=</span><span class="token string">"metrics"</span> <span class="token assign-left variable">permissions</span><span class="token operator">=</span><span class="token string">"METRICS"</span>/<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>/azkaban-users<span class="token operator">&gt;</span>

<span class="token comment"># 必须进入到hadoop102的/opt/module/azkaban/azkaban-web路径，启动web server</span>
bin/start-web.sh
<span class="token comment"># 访问http://hadoop102:8081,并用atguigu用户登陆</span>

</code></pre> 
<h3><a id="2Work_Flow_924"></a>2、Work Flow案例实操</h3> 
<h4><a id="21_HelloWorld_926"></a>2.1 HelloWorld案例</h4> 
<pre><code class="prism language-bash"><span class="token comment"># 在windows环境，新建azkaban.project文件，编辑内容如下</span>
<span class="token comment"># 注意：该文件作用，是采用新的Flow-API方式解析flow文件</span>
azkaban-flow-version: <span class="token number">2.0</span>
<span class="token comment"># 新建basic.flow文件，内容如下</span>
nodes:
  - name: jobA
    type: <span class="token builtin class-name">command</span>
    config:
      command: <span class="token builtin class-name">echo</span> <span class="token string">"Hello World"</span>

<span class="token comment"># Name：job名称</span>
<span class="token comment"># Type：job类型。command表示你要执行作业的方式为命令</span>
<span class="token comment"># Config：job配置</span>

<span class="token comment"># 将azkaban.project、basic.flow文件压缩到一个zip文件，文件名称必须是英文</span>
<span class="token comment"># 在WebServer新建项目：http://hadoop102:8081/index</span>
<span class="token comment"># 然后上传压缩文件，执行，查看日志</span>

</code></pre> 
<h4><a id="22__949"></a>2.2 作业依赖案例</h4> 
<p>需求：JobA和JobB执行完了，才能执行JobC</p> 
<pre><code class="prism language-bash"><span class="token comment"># 修改basic.flow为如下内容</span>
nodes:
  - name: jobC
    type: <span class="token builtin class-name">command</span>
    <span class="token comment"># jobC 依赖 JobA和JobB</span>
    dependsOn:
      - jobA
      - jobB
    config:
      command: <span class="token builtin class-name">echo</span> <span class="token string">"I’m JobC"</span>

  - name: jobA
    type: <span class="token builtin class-name">command</span>
    config:
      command: <span class="token builtin class-name">echo</span> <span class="token string">"I’m JobA"</span>

  - name: jobB
    type: <span class="token builtin class-name">command</span>
    config:
      command: <span class="token builtin class-name">echo</span> <span class="token string">"I’m JobB"</span>

</code></pre> 
<h4><a id="23__977"></a>2.3 自动失败重试案例</h4> 
<p>需求：如果执行任务失败，需要重试3次，重试的时间间隔10000ms</p> 
<pre><code class="prism language-bash">nodes:
  - name: JobA
    type: <span class="token builtin class-name">command</span>
    config:
      command: <span class="token function">sh</span> /not_exists.sh
      retries: <span class="token number">3</span>
      retry.backoff: <span class="token number">10000</span>
</code></pre> 
<p>也可以在Flow全局配置中添加任务失败重试配置，此时重试配置会应用到所有Job</p> 
<pre><code class="prism language-bash">config:
  retries: <span class="token number">3</span>
  retry.backoff: <span class="token number">10000</span>
nodes:
  - name: JobA
    type: <span class="token builtin class-name">command</span>
    config:
      command: <span class="token function">sh</span> /not_exists.sh
</code></pre> 
<h4><a id="24__1004"></a>2.4 手动失败重试案例</h4> 
<p>需求：JobA⇒JobB（依赖于A）⇒JobC⇒JobD⇒JobE⇒JobF。生产环境，任何Job都有可能挂掉，可以根据需求执行想要执行的Job。</p> 
<pre><code class="prism language-bash">nodes:
  - name: JobA
    type: <span class="token builtin class-name">command</span>
    config:
      command: <span class="token builtin class-name">echo</span> <span class="token string">"This is JobA."</span>

  - name: JobB
    type: <span class="token builtin class-name">command</span>
    dependsOn:
      - JobA
    config:
      command: <span class="token builtin class-name">echo</span> <span class="token string">"This is JobB."</span>

  - name: JobC
    type: <span class="token builtin class-name">command</span>
    dependsOn:
      - JobB
    config:
      command: <span class="token builtin class-name">echo</span> <span class="token string">"This is JobC."</span>

  - name: JobD
    type: <span class="token builtin class-name">command</span>
    dependsOn:
      - JobC
    config:
      command: <span class="token builtin class-name">echo</span> <span class="token string">"This is JobD."</span>

  - name: JobE
    type: <span class="token builtin class-name">command</span>
    dependsOn:
      - JobD
    config:
      command: <span class="token builtin class-name">echo</span> <span class="token string">"This is JobE."</span>

  - name: JobF
    type: <span class="token builtin class-name">command</span>
    dependsOn:
      - JobE
    config:
      command: <span class="token builtin class-name">echo</span> <span class="token string">"This is JobF."</span>
</code></pre> 
<p>在可视化界面，Enable和Disable下面都分别有如下参数：</p> 
<ul><li>Parents：该作业的上一个任务</li><li>Ancestors：该作业前的所有任务</li><li>Children：该作业后的一个任务</li><li>Descendents：该作业后的所有任务</li><li>Enable All：所有的任务</li></ul> 
<h3><a id="3JavaProcess_1059"></a>3、JavaProcess作业类型案例</h3> 
<h4><a id="31__1061"></a>3.1 概述</h4> 
<p>JavaProcess类型可以运行一个自定义主类方法，type类型为javaprocess，可用的配置为：</p> 
<ul><li>Xms：最小堆</li><li>Xmx：最大堆</li><li>classpath：类路径</li><li>java.class：要运行的Java对象，其中必须包含Main方法</li><li>main.args：main方法的参数</li></ul> 
<h4><a id="32__1071"></a>3.2 案例</h4> 
<p>新建一个azkaban的maven工程，然后创建包名：com.atguigu，创建AzTest类</p> 
<pre><code class="prism language-java"><span class="token keyword">package</span> <span class="token namespace">com<span class="token punctuation">.</span>atguigu</span><span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">AzTest</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"This is for testing!"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>打包成jar包azkaban-1.0-SNAPSHOT.jar，新建testJava.flow，内容如下</p> 
<pre><code class="prism language-bash">nodes:
  - name: test_java
    type: javaprocess
    config:
      Xms: 96M
      Xmx: 200M
      java.class: com.atguigu.AzTest
</code></pre> 
<p>**将Jar包、flow文件和project文件打包成javatest.zip **，然后上传执行</p> 
<h3><a id="4_1099"></a>4、条件工作流案例</h3> 
<h4><a id="41__1101"></a>4.1 概述</h4> 
<p>条件工作流功能允许用户自定义执行条件来决定是否运行某些Job。条件可以由当前Job的父Job输出的运行时参数构成，也可以使用预定义宏。在这些条件下，用户可以在确定Job执行逻辑时获得更大的灵活性，例如，只要父Job之一成功，就可以运行当前Job</p> 
<h4><a id="42__1105"></a>4.2 运行时参数案例</h4> 
<p><strong>基本原理</strong>：父Job将参数写入<code>JOB_OUTPUT_PROP_FILE</code>环境变量所指向的文件；子Job使用 <code>${jobName:param}</code>来获取父Job输出的参数并定义执行条件</p> 
<p><strong>支持的条件运算符</strong>：</p> 
<p>（1）== 等于</p> 
<p>（2）!= 不等于</p> 
<p>（3）&gt; 大于</p> 
<p>（4）&gt;= 大于等于</p> 
<p>（5）&lt; 小于</p> 
<p>（6）&lt;= 小于等于</p> 
<p>（7）&amp;&amp; 与</p> 
<p>（8）|| 或</p> 
<p>（9）! 非</p> 
<p><strong>需求分析：</strong></p> 
<pre><code class="prism language-bash"><span class="token comment"># JobA执行一个shell脚本。</span>
<span class="token comment"># JobB执行一个shell脚本，但JobB不需要每天都执行，而只需要每个周一执行</span>

<span class="token comment"># 新建JobA.sh</span>
<span class="token comment">#!/bin/bash</span>
<span class="token builtin class-name">echo</span> <span class="token string">"do JobA"</span>
<span class="token assign-left variable">wk</span><span class="token operator">=</span><span class="token variable"><span class="token variable">`</span><span class="token function">date</span> +%w<span class="token variable">`</span></span>
<span class="token builtin class-name">echo</span> <span class="token string">"{<!-- --><span class="token entity" title='\"'>\"</span>wk<span class="token entity" title='\"'>\"</span>:<span class="token variable">$wk</span>}"</span> <span class="token operator">&gt;</span> <span class="token variable">$JOB_OUTPUT_PROP_FILE</span>

<span class="token comment"># 新建JobB.sh</span>
<span class="token comment">#!/bin/bash</span>
<span class="token builtin class-name">echo</span> <span class="token string">"do JobB"</span>

<span class="token comment"># 新建condition.flow</span>
nodes:
 - name: JobA
   type: <span class="token builtin class-name">command</span>
   config:
     command: <span class="token function">sh</span> JobA.sh

 - name: JobB
   type: <span class="token builtin class-name">command</span>
   dependsOn:
     - JobA
   config:
     command: <span class="token function">sh</span> JobB.sh
   condition: <span class="token variable">${JobA<span class="token operator">:</span>wk}</span> <span class="token operator">==</span> <span class="token number">1</span>
   
<span class="token comment"># 最后将JobA.sh、JobB.sh、condition.flow和azkaban.project打包成condition.zip</span>
</code></pre> 
<h4><a id="43__1163"></a>4.3 预定义宏案例</h4> 
<p>Azkaban中预置了几个特殊的判断条件，称为预定义宏。预定义宏会根据所有父Job的完成情况进行判断，再决定是否执行。可用的预定义宏如下：</p> 
<p>（1）all_success: 表示父Job全部成功才执行(默认)</p> 
<p>（2）all_done：表示父Job全部完成才执行</p> 
<p>（3）all_failed：表示父Job全部失败才执行</p> 
<p>（4）one_success：表示父Job至少一个成功才执行</p> 
<p>（5）one_failed：表示父Job至少一个失败才执行</p> 
<pre><code class="prism language-bash"><span class="token comment"># 需求</span>
<span class="token comment"># JobA执行一个shell脚本</span>
<span class="token comment"># JobB执行一个shell脚本</span>
<span class="token comment"># JobC执行一个shell脚本，要求JobA、JobB中有一个成功即可执行</span>

<span class="token comment"># 新建JobA.sh</span>
<span class="token comment">#!/bin/bash</span>
<span class="token builtin class-name">echo</span> <span class="token string">"do JobA"</span>

<span class="token comment"># 新建JobC.sh</span>
<span class="token comment">#!/bin/bash</span>
<span class="token builtin class-name">echo</span> <span class="token string">"do JobC"</span>

<span class="token comment"># 新建macro.flow</span>
nodes:
 - name: JobA
   type: <span class="token builtin class-name">command</span>
   config:
     command: <span class="token function">sh</span> JobA.sh

 - name: JobB
   type: <span class="token builtin class-name">command</span>
   config:
     command: <span class="token function">sh</span> JobB.sh

 - name: JobC
   type: <span class="token builtin class-name">command</span>
   dependsOn:
     - JobA
     - JobB
   config:
     command: <span class="token function">sh</span> JobC.sh
   condition: one_success

</code></pre> 
<h3><a id="5_1214"></a>5、邮箱告警</h3> 
<p>首先申请好邮箱，然后配置</p> 
<pre><code class="prism language-bash"><span class="token comment"># 在azkaban-web节点hadoop102上，编辑/opt/module/azkaban/azkaban-web/conf/azkaban.properties，修改如下内容</span>
<span class="token function">vim</span> /opt/module/azkaban/azkaban-web/conf/azkaban.properties
<span class="token comment"># 添加如下内容：</span>
<span class="token comment">#这里设置邮件发送服务器，需要 申请邮箱，切开通stmp服务，以下只是例子</span>
<span class="token assign-left variable">mail.sender</span><span class="token operator">=</span>atguigu@126.com
<span class="token assign-left variable">mail.host</span><span class="token operator">=</span>smtp.126.com
<span class="token assign-left variable">mail.user</span><span class="token operator">=</span>atguigu@126.com
<span class="token assign-left variable">mail.password</span><span class="token operator">=</span>用邮箱的授权码

<span class="token comment"># 保存并重启web-server</span>
bin/shutdown-web.sh
bin/start-web.sh

<span class="token comment"># 编辑basic.flow</span>
nodes:
  - name: jobA
    type: <span class="token builtin class-name">command</span>
    config:
      command: <span class="token builtin class-name">echo</span> <span class="token string">"This is an email test."</span>


<span class="token comment"># 将azkaban.project和basic.flow压缩成email.zip</span>
<span class="token comment"># 然后上传，在可视化页面里选择邮箱告警</span>
<span class="token comment"># 针对电话告警，可以使用睿象云，https://www.aiops.com/</span>
</code></pre> 
<h3><a id="6AzkabanExecutor_1245"></a>6、Azkaban多Executor模式注意事项</h3> 
<p>Azkaban多Executor模式是指，在集群中多个节点部署Executor。在这种模式下， <strong>Azkaban web Server会根据策略，选取其中一个Executor去执行任务</strong>。为确保所选的Executor能够准确的执行任务，我们须在以下两种方案任选其一，推荐使用方案二。</p> 
<p>方案一：指定特定的Executor（hadoop102）去执行任务</p> 
<pre><code class="prism language-bash"><span class="token comment"># 在MySQL中azkaban数据库executors表中，查询hadoop102上的Executor的id</span>

mysql<span class="token operator">&gt;</span> use azkaban<span class="token punctuation">;</span>
mysql<span class="token operator">&gt;</span> <span class="token keyword">select</span> * from executors<span class="token punctuation">;</span>

<span class="token comment"># 在执行工作流程时选择Flow Parameters加入useExecutor属性</span>
</code></pre> 
<p>方案二：在Executor所在所有节点部署任务所需脚本和应用</p> 
<p>官网文档：<a href="https://azkaban.readthedocs.io/en/latest/configuration.html" rel="nofollow" title="https://azkaban.readthedocs.io/en/latest/configuration.html">https://azkaban.readthedocs.io/en/latest/configuration.html</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/430953b0b40ed40cc73021ed068185f0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Sql-lab全解_sqllab，算法太TM重要了</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2a0e48673d6c262fab9e5fe23c3f9c2d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python 基于列表实现的通讯录管理系统(有完整源码)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>