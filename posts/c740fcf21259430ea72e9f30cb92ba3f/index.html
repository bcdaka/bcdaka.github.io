<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>超详细的YOLO系列算法全家桶--YOLOv1-YOLOv8 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/c740fcf21259430ea72e9f30cb92ba3f/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="超详细的YOLO系列算法全家桶--YOLOv1-YOLOv8">
  <meta property="og:description" content="文章目录 前言一、YOLO算法的核心思想1. YOLO系列算法的步骤 二、YOLO系列算法1.YOLOv11.1 YOLOv1核心思想1.2 网络结构（backbone）1.3 优势与不足 2.YOLOv22.1 YOLOv2网络结构（backbone）2.2 YOLOv2主要改进部分2.3 优势与不足 3.YOLOv33.1 YOLOv3网络结构3.2 YOLOv3主要改进部分3.3 优势与不足 4.YOLOv44.1 YOLOv4网络结构4.1 YOLOv4主要改进部分4.3 优势与不足 5.YOLOv55.1 YOLOv5网络结构5.2 YOLOv5主要改进部分5.3 优势与不足 6.YOLOv66.1 YOLOv6网络结构6.2 YOLOv6主要改进部分6.3 优势与不足 7.YOLOv77.1 YOLOv7网络结构7.2 YOLOv7主要改进部分7.3 优势与不足 8.YOLOv88.1 YOLOv8网络结构8.2 YOLOv8主要改进部分8.3 优势与不足 前言 YOLOv1-YOLOv8的对比图先给大家呈上，本文介绍了从YOLOv1一直到YOLOv8的网络结构，以及各个版本之间的迭代，非常适合研究生们汇报PPT的制作和cv面试。
一、YOLO算法的核心思想 YOLO系列的核心思想就是把目标检测转变为一个回归问题，利用整张图片作为网络的输入，通过神经网络，得到边界框的位置及其所属的类别。
1. YOLO系列算法的步骤 划分图像：YOLO将输入图像划分为一个固定大小的网格。预测边界框和类别：对于每个网格，YOLO预测出固定数量（通常为5或3个）的边界框。每个边界框由5个主要属性描述：边界框的位置（中心坐标和宽高）和边界框包含的目标的置信度（confidence）。此外，每个边界框还预测目标的类别。单次前向传递：YOLO通过一个卷积神经网络（CNN）进行单次前向传递，同时预测所有边界框的位置和类别。相比于其他目标检测算法，如基于滑动窗口或区域提议的方法，YOLO具有更快的速度，因为它只需要一次前向传递即可完成预测。损失函数：YOLO使用多任务损失函数来训练网络。该损失函数包括位置损失、置信度损失和类别损失。位置损失衡量预测边界框和真实边界框之间的位置差异。置信度损失衡量边界框是否正确地预测了目标，并惩罚背景框的置信度。类别损失衡量目标类别的预测准确性。非最大抑制（Non-Maximum Suppression）：在预测的边界框中，可能存在多个相互重叠的框，代表同一个目标。为了消除冗余的边界框，YOLO使用非最大抑制算法，根据置信度和重叠程度筛选出最佳的边界框。 二、YOLO系列算法 1.YOLOv1 1.1 YOLOv1核心思想 如图，YOLOv1算法将输入图片进行7×7的划分，划分之后就有很多小格子。我们再看图片中待检测物体的中心是落在哪个格子里面，落在哪个格子哪个格子就负责预测这个物体。每个格子形成 2 个预测框，同时预测 20 个类别（class）概率，每个预测框包含 5 个特征的预测结果(x,y,w,h,c)，其中(x,y)为预测框相对于格子的位置，(w,h)为预测框相对于图片的宽和高比例，c 为置信度，表示预测框内存在被检物体的概率及预测框的位置准确度。将预测到的类别概率与预测框的置信度相乘，即可获得每个预测框中各类别的概率，以概率最高的结果作为预测结果输出，通过非极大值抑制（Non-MaximumSuppression，NMS）得到预测结果。
1. 边界框的中心坐标（x, y）：
中心坐标（x, y）表示边界框在图像中的位置。这些坐标是相对于所在网格单元的位置而言的。中心坐标的值通常被限制在0和1之间，表示相对于网格单元的比例。 2. 边界框的宽度和高度（w, h）：
边界框的宽度和高度表示目标在图像中的尺寸大小。
这些值也是相对于所在网格单元的尺寸而言的。
宽度和高度的值通常被限制在0和1之间，表示相对于网格单元的比例。
3. 置信度（confidence）：
置信度表示边界框内是否存在目标以及预测的准确性。置信度是一个0到1之间的值，其中1表示高置信度，0表示低置信度。置信度可以用于筛选边界框，以便在后处理中选择具有高置信度的边界框。 1.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-01-25T21:02:48+08:00">
    <meta property="article:modified_time" content="2024-01-25T21:02:48+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">超详细的YOLO系列算法全家桶--YOLOv1-YOLOv8</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_7" rel="nofollow">前言</a></li><li><a href="#YOLO_19" rel="nofollow">一、YOLO算法的核心思想</a></li><li><ul><li><a href="#1__YOLO_22" rel="nofollow">1. YOLO系列算法的步骤</a></li></ul> 
  </li><li><a href="#YOLO_30" rel="nofollow">二、YOLO系列算法</a></li><li><ul><li><a href="#1YOLOv1_31" rel="nofollow">1.YOLOv1</a></li><li><ul><li><a href="#11_YOLOv1_32" rel="nofollow">1.1 YOLOv1核心思想</a></li><li><a href="#12_backbone_57" rel="nofollow">1.2 网络结构（backbone）</a></li><li><a href="#13__61" rel="nofollow">1.3 优势与不足</a></li></ul> 
   </li><li><a href="#2YOLOv2_63" rel="nofollow">2.YOLOv2</a></li><li><ul><li><a href="#21_YOLOv2backbone_67" rel="nofollow">2.1 YOLOv2网络结构（backbone）</a></li><li><a href="#22_YOLOv2_70" rel="nofollow">2.2 YOLOv2主要改进部分</a></li><li><a href="#23__76" rel="nofollow">2.3 优势与不足</a></li></ul> 
   </li><li><a href="#3YOLOv3_80" rel="nofollow">3.YOLOv3</a></li><li><ul><li><a href="#31_YOLOv3_81" rel="nofollow">3.1 YOLOv3网络结构</a></li><li><a href="#32_YOLOv3_85" rel="nofollow">3.2 YOLOv3主要改进部分</a></li><li><a href="#33__117" rel="nofollow">3.3 优势与不足</a></li></ul> 
   </li><li><a href="#4YOLOv4_119" rel="nofollow">4.YOLOv4</a></li><li><ul><li><a href="#41_YOLOv4_120" rel="nofollow">4.1 YOLOv4网络结构</a></li><li><a href="#41_YOLOv4_122" rel="nofollow">4.1 YOLOv4主要改进部分</a></li><li><a href="#43__139" rel="nofollow">4.3 优势与不足</a></li></ul> 
   </li><li><a href="#5YOLOv5_141" rel="nofollow">5.YOLOv5</a></li><li><ul><li><a href="#51_YOLOv5_142" rel="nofollow">5.1 YOLOv5网络结构</a></li><li><a href="#52_YOLOv5_144" rel="nofollow">5.2 YOLOv5主要改进部分</a></li><li><a href="#53__153" rel="nofollow">5.3 优势与不足</a></li></ul> 
   </li><li><a href="#6YOLOv6_155" rel="nofollow">6.YOLOv6</a></li><li><ul><li><a href="#61_YOLOv6_156" rel="nofollow">6.1 YOLOv6网络结构</a></li><li><a href="#62_YOLOv6_159" rel="nofollow">6.2 YOLOv6主要改进部分</a></li><li><a href="#63__164" rel="nofollow">6.3 优势与不足</a></li></ul> 
   </li><li><a href="#7YOLOv7_166" rel="nofollow">7.YOLOv7</a></li><li><ul><li><a href="#71_YOLOv7_167" rel="nofollow">7.1 YOLOv7网络结构</a></li><li><a href="#72_YOLOv7_169" rel="nofollow">7.2 YOLOv7主要改进部分</a></li><li><a href="#73__180" rel="nofollow">7.3 优势与不足</a></li></ul> 
   </li><li><a href="#8YOLOv8_183" rel="nofollow">8.YOLOv8</a></li><li><ul><li><a href="#81_YOLOv8_184" rel="nofollow">8.1 YOLOv8网络结构</a></li><li><a href="#82_YOLOv8_186" rel="nofollow">8.2 YOLOv8主要改进部分</a></li><li><a href="#83__192" rel="nofollow">8.3 优势与不足</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_7"></a>前言</h2> 
<p>  YOLOv1-YOLOv8的对比图先给大家呈上，本文介绍了从YOLOv1一直到YOLOv8的网络结构，以及各个版本之间的迭代，非常适合研究生们汇报PPT的制作和cv面试。<br> <img src="https://images2.imgbox.com/c3/47/eBPQ5Ue8_o.png" alt="在这里插入图片描述"></p> 
<hr> 
<h2><a id="YOLO_19"></a>一、YOLO算法的核心思想</h2> 
<p><mark>  YOLO系列的核心思想就是把目标检测转变为一个回归问题，利用整张图片作为网络的输入，通过神经网络，得到边界框的位置及其所属的类别。</mark></p> 
<h3><a id="1__YOLO_22"></a>1. YOLO系列算法的步骤</h3> 
<blockquote> 
 <ol><li>划分图像：YOLO将输入图像划分为一个固定大小的网格。</li><li>预测边界框和类别：对于每个网格，YOLO预测出固定数量（通常为5或3个）的边界框。每个边界框由5个主要属性描述：边界框的位置（中心坐标和宽高）和边界框包含的目标的置信度（confidence）。此外，每个边界框还预测目标的类别。</li><li>单次前向传递：YOLO通过一个卷积神经网络（CNN）进行单次前向传递，同时预测所有边界框的位置和类别。相比于其他目标检测算法，如基于滑动窗口或区域提议的方法，YOLO具有更快的速度，因为它只需要一次前向传递即可完成预测。</li><li>损失函数：YOLO使用多任务损失函数来训练网络。该损失函数包括位置损失、置信度损失和类别损失。位置损失衡量预测边界框和真实边界框之间的位置差异。置信度损失衡量边界框是否正确地预测了目标，并惩罚背景框的置信度。类别损失衡量目标类别的预测准确性。</li><li>非最大抑制（Non-Maximum Suppression）：在预测的边界框中，可能存在多个相互重叠的框，代表同一个目标。为了消除冗余的边界框，YOLO使用非最大抑制算法，根据置信度和重叠程度筛选出最佳的边界框。</li></ol> 
</blockquote> 
<h2><a id="YOLO_30"></a>二、YOLO系列算法</h2> 
<h3><a id="1YOLOv1_31"></a>1.YOLOv1</h3> 
<h4><a id="11_YOLOv1_32"></a>1.1 YOLOv1核心思想</h4> 
<p><img src="https://images2.imgbox.com/6f/73/TBjV7C3L_o.png" alt="在这里插入图片描述"><br>   如图，YOLOv1算法将输入图片进行7×7的划分，划分之后就有很多小格子。我们再看图片中待检测物体的<strong>中心</strong>是落在哪个格子里面，落在哪个格子哪个格子就负责预测这个物体。每个格子形成 2 个预测框，同时预测 20 个类别（class）概率，每个预测框包含 5 个特征的预测结果(x,y,w,h,c)，其中(x,y)为预测框相对于格子的位置，(w,h)为预测框相对于图片的宽和高比例，c 为置信度，表示预测框内存在被检物体的概率及预测框的位置准确度。将预测到的类别概率与预测框的置信度相乘，即可获得每个预测框中各类别的概率，以概率最高的结果作为预测结果输出，通过非极大值抑制（Non-MaximumSuppression，NMS）得到预测结果。</p> 
<blockquote> 
 <p><strong>1. 边界框的中心坐标（x, y）：</strong></p> 
 <ul><li>中心坐标（x, y）表示边界框在图像中的位置。</li><li>这些坐标是相对于所在网格单元的位置而言的。</li><li>中心坐标的值通常被限制在0和1之间，表示相对于网格单元的比例。</li></ul> 
 <p><strong>2. 边界框的宽度和高度（w, h）：</strong></p> 
 <ul><li> <p>边界框的宽度和高度表示目标在图像中的尺寸大小。</p> </li><li> <p>这些值也是相对于所在网格单元的尺寸而言的。</p> </li><li> <p>宽度和高度的值通常被限制在0和1之间，表示相对于网格单元的比例。</p> </li></ul> 
 <p><strong>3. 置信度（confidence）：</strong></p> 
 <ul><li>置信度表示边界框内是否存在目标以及预测的准确性。</li><li>置信度是一个0到1之间的值，其中1表示高置信度，0表示低置信度。</li><li>置信度可以用于筛选边界框，以便在后处理中选择具有高置信度的边界框。</li></ul> 
</blockquote> 
<h4><a id="12_backbone_57"></a>1.2 网络结构（backbone）</h4> 
<p>  YOLOv1 主干网络类似于 GoogleNet,采用卷积层提取特征 ，最大池化层进行空间降采样 ，并由全连接层输出预测值，除最后一层使用线性激活函数外，其余层使用Leaky ReLu 激活函数,采用Dropout防止过拟合。<br>   从图中可以看到，yolo网络的输出的网格是7x7大小的，另外，输出的channel数目为30。一个格子内，前20个元素是类别概率值，然后2个元素是边界框confidence，最后8个元素是边界框的 (x, y,w,h) 。<img src="https://images2.imgbox.com/f9/e2/CO0oghcY_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="13__61"></a>1.3 优势与不足</h4> 
<p>  YOLOv1在速度和简洁性方面具有显著优势，但也存在一些限制，如对小尺寸目标的检测能力较弱以及定位精度相对较低。这些限制在后续版本的YOLO中得到了改进和优化。</p> 
<h3><a id="2YOLOv2_63"></a>2.YOLOv2</h3> 
<h4><a id="21_YOLOv2backbone_67"></a>2.1 YOLOv2网络结构（backbone）</h4> 
<p>  YOLOv2在 YOLOv1 基础上，结合视觉几何组（Visual Geometry Group，VGG）网络构建出新的DarkNet-19 网络，新网络采用 1×1 的卷积进行通道降维、3×3 的卷积进行特征提取、5 层最大池化层进行 5 次下采样，没有FC层，每一个卷积层后面都使BN和Relu防止过拟合。DarkNet-19 网络结构如图所示<br> <img src="https://images2.imgbox.com/4f/71/qjrGVvOp_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="22_YOLOv2_70"></a>2.2 YOLOv2主要改进部分</h4> 
<ul><li>YOLOv2 引入 Faster R-CNN 中<mark>先验框</mark>（AnchorBox）思想。先验框是一组预定义的边界框，具有不同的尺寸和长宽比，通过先验框同时预测类别和位置，一个栅格内的 5个先验框可以分别预测 5 种类别，预测边界框的数量也由 7×7×2 提高到 13×13×5，提高了不同尺寸、不同长宽比物体预测的准确率。</li><li>YOLOv2采用了<mark>批归一化</mark>（Batch Normalization，BN）技术来改善模型的训练和收敛性，批归一化是一种在深度神经网络中常用的技术，通过对每个小批量数据进行归一化操作，使得输入数据的均值接近于0，标准差接近于1。在YOLOv2中，批归一化被应用于卷积层和全连接层之后，通过对每个批量的数据进行归一化和调整来规范化网络中的激活值。这有助于缓解梯度消失和梯度爆炸问题，使得网络更容易训练和优化。</li><li>YOLOv2引入了<mark>高分辨率图像分类器</mark>，它可以在更高的分辨率下进行目标分类，从而提高了模型对目标细节的感知能力。</li><li>YOLOv2采用了<mark>多尺度训练策略</mark>，即在训练过程中随机地对输入图像进行尺度变换，使得模型能够学习到不同尺度下的目标特征，从而提高了模型在不同尺度目标上的检测能力。</li></ul> 
<h4><a id="23__76"></a>2.3 优势与不足</h4> 
<p>  相对于YOLOv1而言，不足之处在于，没有进行多尺度特征的结合预测，传递模块（Pass-Through Module）的使用在提升细粒度特征的同时也对特征的空间，分布产生了一定影响，以及对小目标的检测能力没有明显提升。</p> 
<h3><a id="3YOLOv3_80"></a>3.YOLOv3</h3> 
<h4><a id="31_YOLOv3_81"></a>3.1 YOLOv3网络结构</h4> 
<p>  YOLOv3 网络结构主要由输入端、主干、颈部和头部构成。在输入端部分，网络对数据进行加载，可以根据需求对加载后的数据进行 Mosaic 数据扩增 、自适应锚框计算 、自适应缩放等预处理操作。主干的主要功能是对图像的特征信息进行提取。在颈部结构中，对主干部分上的多个层级的特征图进行融合加工，提高网络的表达能力，并将融合后的特征图传递到头部进行预测 。 在头部结构中 ，生成预测框和预测类别 。<br> <img src="https://images2.imgbox.com/d6/fb/wpqTSmR8_o.png" alt="在这里插入图片描述"><br>                      <strong>YOLOv3网络结构</strong></p> 
<h4><a id="32_YOLOv3_85"></a>3.2 YOLOv3主要改进部分</h4> 
<p><mark><strong>1.输入端</strong></mark></p> 
<ul><li><strong>Mosaic 数据扩增</strong>：将四张不同的训练图像随机拼接在一起，形成一张马赛克图像。这种方式可以帮助模型学习并适应不同的场景、目标形状和尺度变化。</li><li><strong>自适应锚框计算</strong>：引入了自适应锚框计算的机制，旨在更好地适应各种目标的尺寸和长宽比例变化。</li></ul> 
<blockquote> 
 <p><strong>初始锚框定义</strong>：首先，根据训练数据集的标注框，选择初始的锚框。可以使用一些聚类算法（如k-means）对标注框进行聚类，以确定一组代表性的锚框。<br> <strong>锚框调整</strong>：对于每个训练样本，根据该样本中的目标框与初始锚框的匹配程度，调整初始锚框的大小和长宽比例。这可以通过计算目标框与锚框的IoU（交并比）来确定匹配程度，并根据匹配程度调整锚框的尺寸。<br> <strong>锚框聚类</strong>：根据经过调整的锚框，再次进行聚类，得到一组更适应当前数据集的锚框。这些聚类过程通常是迭代进行的，直到达到一定的收敛条件。<br> <strong>锚框选择</strong>：根据聚类得到的锚框集合，可以选择一定数量的锚框用于目标检测。通常，可以根据聚类结果中的锚框长宽比例的分布情况，选择一些具有代表性的锚框。</p> 
</blockquote> 
<ul><li><strong>自适应缩放</strong>：根据目标尺寸来自动调整输入图像的大小。这样可以更好地适应不同尺度的目标，提高目标检测的准确性。</li></ul> 
<p><strong><mark>2.主干网络</mark></strong><br>   YOLOv3的主干网络 DarkNet-53 包含卷积层（Convolutional Layer）、残差层（Residual Layer）、特征融合层（Feature Fusion Layer），网络层数的加深提高了检测精度，大量残差网络模块的引入减少了由网络层数加深引起的梯度下降问题，金字塔池化模块的引入可以实现多尺寸的输入和统一尺寸的输出。</p> 
<p><strong><mark>3.颈部网络</mark></strong><br>   YOLOv3的颈部网络是FPN(多尺度检测，特征融合)，FPN（Feature Pyramid Network）是一种用于目标检测和语义分割任务的特征金字塔网络结构。它的设计目的是解决单尺度特征提取网络在处理不同尺度目标时的困难。</p> 
<p><strong>FPN的主要思想如下：</strong></p> 
<blockquote> 
 <ul><li>特征提取：首先，通过卷积神经网络（如ResNet）等进行特征提取。这些特征具有不同的尺度和语义信息。</li><li>顶层池化：为了获取更高分辨率的特征，FPN使用自顶向下的顶层池化操作，将较低分辨率的特征图上采样到较高分辨率。这可以通过上采样或插值等方法实现。</li><li>横向连接：为了融合不同层次的特征信息，FPN引入横向连接，将上一层的特征与下一层的上采样特征进行逐元素相加（Element-wise Sum）。这样可以将低级别的细节信息与高级别的语义信息相结合，产生具有多尺度特征的金字塔结构。</li><li>特征融合：为了进一步提升特征表达能力，FPN在每个金字塔层级上引入一个额外的卷积层，进行特征融合和调整。</li></ul> 
</blockquote> 
<p><strong><mark>4.输出端</mark></strong><br>   YOLOv3在输出端的改进是多标签预测（softmax函数变为logistics分类器）。在YOLOv1中，通常使用softmax函数作为分类器的激活函数，将每个类别的输出转化为概率分布。然而，对于YOLOv3这样的多标签检测任务，一个目标可能属于多个类别，使用softmax函数会导致多个类别的概率之和超过1，不符合多标签问题的要求。因此，在YOLOv3中，采用了logistic分类器作为分类器的激活函数。logistic分类器将每个类别的输出视为独立的二元分类问题，对每个类别使用sigmoid函数进行激活。sigmoid函数将输出限制在0到1之间，表示每个类别的存在概率。</p> 
<h4><a id="33__117"></a>3.3 优势与不足</h4> 
<p>  虽然模型的召回率相对较低，但是极大提升了整体检测精度，可以在不提高输入图片分辨率的情况下实现对小目标的精确检测，使 YOLOv3 成为一阶段算法中的里程碑式算法，在工业领域得到广泛应用。</p> 
<h3><a id="4YOLOv4_119"></a>4.YOLOv4</h3> 
<h4><a id="41_YOLOv4_120"></a>4.1 YOLOv4网络结构</h4> 
<p><img src="https://images2.imgbox.com/e2/1c/I7CeKhNP_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="41_YOLOv4_122"></a>4.1 YOLOv4主要改进部分</h4> 
<p><mark><strong>1.输入端</strong></mark>：无明显变化。<br> <strong><mark>2.主干网络</mark></strong>：</p> 
<ul><li> <p><strong>CSPDarknet53骨干网络</strong>：YOLOv4采用了称为CSPDarknet53的新的骨干网络结构，它基于Darknet53，并通过使用CSP（Cross<br> Stage Partial）模块来提高特征表示的能力。</p> </li><li> <p><strong>SAM</strong>（Spatial Attention Module）：通过引入SAM模块，YOLOv4能够自适应地调整特征图的通道注意力权重，以增强对目标的感知能力。</p> </li><li> <p><strong>Mish激活函数</strong>：YOLOv4采用了CSPDarknet53作为其主干网络，该网络中的每个残差块（residual block）都应用了Mish激活函数。这使得网络能够从输入到输出的特征变换过程中引入非线性变换，并帮助网络更好地捕捉输入数据的复杂特。</p> </li></ul> 
<p><strong><mark>3.颈部网络</mark></strong>：</p> 
<ul><li><strong>PANet特征融合</strong>：YOLOv4引入了PANet（Path Aggregation Network）模块，用于在不同尺度的特征图之间进行信息传递和融合，以获取更好的多尺度特征表示。</li><li><strong>SPP</strong>：具体是在CSPDarknet53网络的后面，通过在不同大小的池化层上进行特征提取，从而捕捉到不同尺度上的上下文信息。</li></ul> 
<p><strong><mark>4.输出端</mark></strong>：<br>   在YOLOv4中，确实引入了一种新的距离度量指标，称为<strong>CIOU</strong>。CIOU是一种改进的目标检测损失函数，用于衡量预测框和真实框之间的距离。CIOU是DIoU的进一步扩展，除了考虑框的位置和形状之间的距离外，还引入了一个附加的参数用于衡量框的长宽比例的一致性。CIOU的计算公式如下：<em>CIOU = IoU - (d^2 / c^2 + αv) + v</em>，其中，IoU表示传统的交并比（Intersection over Union），d表示预测框和真实框中心点之间的欧氏距离，c表示预测框和真实框的对角线距离。在CIOU中，α是一个参数，用于平衡框长宽比例的一致性和框位置之间的距离。v是一个辅助项，用于惩罚预测框和真实框之间的长宽比例差异。CIOU损失是通过最小化CIOU来优化目标检测模型。它可以作为定位损失函数的一部分，用于衡量预测框的定位准确性。通过CIOU损失的引入，YOLOv4可以更好地优化边界框的位置、形状和长宽比例，从而提高目标检测的准确性和鲁棒性。</p> 
<h4><a id="43__139"></a>4.3 优势与不足</h4> 
<p>  YOLOv4算法在YOLO算法的框架上，广泛整合目标检测领域的优化策略进行全方位优化，获得了高效、快速、精确的检测特性。</p> 
<h3><a id="5YOLOv5_141"></a>5.YOLOv5</h3> 
<h4><a id="51_YOLOv5_142"></a>5.1 YOLOv5网络结构</h4> 
<p><img src="https://images2.imgbox.com/b8/29/Egvkj6fi_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="52_YOLOv5_144"></a>5.2 YOLOv5主要改进部分</h4> 
<p><mark><strong>1.输入端</strong></mark>：无明显变化。<br> <strong><mark>2.主干网络</mark></strong>：YOLOv5相比于YOLOv4，引入了一些新的网络结构和改进。</p> 
<ol><li><strong>Focus结构</strong>：Focus结构是YOLOv5中的一个重要组件，用于提取高分辨率特征。它采用的是一种轻量级的卷积操作，帮助模型在保持较高感受野的同时减少计算负担。Focus结构通过将输入特征图进行通道划分和空间划分，将原始特征图转换为更小尺寸的特征图，并保留了原始特征图中的重要信息。这样做有助于提高模型的感知能力和对小尺寸目标的检测准确性。</li><li><strong>CSPDarknet53结构</strong>：CSP（Cross StagePartial）Darknet53是YOLOv5中的主干网络结构。相对于YOLOv4中的Darknet53，CSPDarknet53引入了跨阶段部分连接的想法，通过将特征图在通道维度上分为两个部分，将其中一部分直接连入下一阶段，以增加信息流动的路径，提高特征的传递效率。CSPDarknet53结构在减少参数和计算量的同时，保持了较高的特征表示能力，有助于提高目标检测的准确性和速度。</li></ol> 
<p><strong><mark>3.颈部网络</mark></strong>：无明显变化。<br> <strong><mark>4.输出端</mark></strong>：无明显变化。</p> 
<h4><a id="53__153"></a>5.3 优势与不足</h4> 
<p>  YOLOv5 提供 YOLOv5-n/s/m/l/x 共 5 种版本，有更快的检测速度和精度，适合实时应用场景；并且在最新的版本中引入实时分割模块，集成分类、检测、实例分割任务。</p> 
<h3><a id="6YOLOv6_155"></a>6.YOLOv6</h3> 
<h4><a id="61_YOLOv6_156"></a>6.1 YOLOv6网络结构</h4> 
<p><img src="https://images2.imgbox.com/bf/c4/GKeke2Hb_o.jpg" alt="在这里插入图片描述"></p> 
<h4><a id="62_YOLOv6_159"></a>6.2 YOLOv6主要改进部分</h4> 
<p><mark><strong>1.输入端</strong></mark>：无锚框，取消了YOLOv1到YOLOv5一直沿用的锚框。<br> <strong><mark>2.主干网络</mark></strong>： YOLOv6的Backbone的实现的基本模块为RVB1_X结构，其全程为RepVGGBlock_X，表示由多个RepVGGBlock组成。RepVGGBlock是RepVGG网络的重复模块，由多个RepVGGConv模块组成。每个RepVGGBlock由两个RepVGGConv模块组成，第一个RepVGGConv是一个3x3卷积操作，而第二个RepVGGConv是一个1x1卷积操作。这两个卷积模块之间使用了批归一BatchNorm）和ReLU激活函数。RepVGGConv模块是RepVGG网络中的基本卷积模块，由一个卷积层、批归一化和ReLU激活函数组成。这样的设计使得RepVGGBlock具有较强的表达能力，并且可以适应不同的特征提取需求。RepVGGBlock在RepVGG网络中被重复堆叠多次，形成了深层的网络结构。通过堆叠多个RepVGGBlock，可以提高网络的表示能力和复杂度，从而实现更准确的特征提取和目标识别。<br> <strong><mark>3.颈部网络</mark></strong>： PANet结构类似更换为RepVGGBlock结构，<br> <strong><mark>4.输出端</mark></strong>：YOLOv6对检测头进行了解耦，分开了边框与类别的分类过程。</p> 
<h4><a id="63__164"></a>6.3 优势与不足</h4> 
<p>  YOLOv6 的检测精度和速度均优于以前最先进的模型，同时设计了 8 种缩放模型为不同场景中的工业应用定制不同规模的网络模型，可以检测不同尺度的图像从而提高检测效果，其部署简单、计算量较低，适用于实时检测。并且支持在不同平台上面的部署，简化工程部署的适配工作。但检测准确率其同时期的其他先进算法相比较低。</p> 
<h3><a id="7YOLOv7_166"></a>7.YOLOv7</h3> 
<h4><a id="71_YOLOv7_167"></a>7.1 YOLOv7网络结构</h4> 
<p><img src="https://images2.imgbox.com/ff/7f/jdBa43N3_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="72_YOLOv7_169"></a>7.2 YOLOv7主要改进部分</h4> 
<p><mark><strong>1.输入端</strong></mark>：与YOLOv5类似。<br> <strong><mark>2.主干网络</mark></strong>：Backbone为骨干网络由CBS、ELAN、MP-1组成。</p> 
<ol><li><strong>CBS结构</strong>：特征提取和通道转换。</li><li><strong>ELAN</strong>：通过不同深度的分支将特征图拼接起来，进而促进更深层网络的有效学习和收敛。</li><li><strong>MP-1</strong>：将经过不同下采样方式所得到的特征图进行融合，在不增加计算量的同时保留更多的特征信息。</li></ol> 
<p><strong><mark>3.颈部网络</mark></strong>：该网络主要包括SPPCSPC 、ELANW、UPSample 三个子模块和 Cat 结构，其中，SPPCSPC 模块用于提高特征提取的效率和准确率；ELANW 模块相比于 ELAN 模块增加了两个拼接操作；UPSample 模块用于实现不同层次特征的高效融合；Cat 结构旨在进一步优化卷积层的效果。</p> 
<p><strong><mark>4.输出端</mark></strong>：与YOLOv6类似。检测头负责网络最终的预测输出，针对 Neck 处理后的特征信息进行解耦，采用重参数化模块对Neck 输出的三种不同尺寸的特征进行通道数调整，再经过 1×1 的卷积操作，得出目标物体的位置、置信度和类别的预测。</p> 
<h4><a id="73__180"></a>7.3 优势与不足</h4> 
<p>  YOLOv7 算法提出了基于级联的模型缩放策略从而生成不同尺寸的模型，减少参数量和计算量，可以进行实时目标检测，在大数据集进行训练检测有较高精度且整体检测性能有所提升。但是其网络架构也相对复杂进行训练测试需要大量计算资源，且对小目标和密集场景的检测效果较差。</p> 
<h3><a id="8YOLOv8_183"></a>8.YOLOv8</h3> 
<h4><a id="81_YOLOv8_184"></a>8.1 YOLOv8网络结构</h4> 
<p><img src="https://images2.imgbox.com/d0/96/A05LJ3yJ_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="82_YOLOv8_186"></a>8.2 YOLOv8主要改进部分</h4> 
<p><mark><strong>1.输入端</strong></mark>：与YOLOv7类似。<br> <strong><mark>2.主干网络</mark></strong>：Backbone 部分采用的结构为 Darknet53，其中包括基本卷积单元 Conv、实现局部特征和全局特征的featherMap级别的融合的空间金字塔池化模块 SPPF、增加网络的深度和感受野，提高特征提取的能力的 C2f 模块。<br> <strong><mark>3.颈部网络</mark></strong>：与YOLOv5类似。</p> 
<p><strong><mark>4.输出端</mark></strong>：在损失函数计算方面，采用了 Task AlignedAssigner 正样本分配策略 ， 由分类损失VFL(Varifocal Loss) 和回归损失CIOU(Complete-IOU)+DFL(Deep Feature Loss)两部分的三个损失函数加权组合而成。</p> 
<h4><a id="83__192"></a>8.3 优势与不足</h4> 
<p>  YOLOv8 相比之前版本,其检测结果更高且更具有灵活性，在工程实践中能够广泛使用；然而，由于模型较复杂，在进行检测时需要较大的计算资源和时间。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ba23fc69a1d1f1c313aa857051085e50/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">常见错误：com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure解决</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/89f55ec03f45fe0ac979b24cb9b276b5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python对象操作数据库—SQLAlchemy的基本介绍与简单使用</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>