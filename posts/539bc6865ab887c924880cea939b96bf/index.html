<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AI大模型探索之路-训练篇15：大语言模型预训练之全量参数微调 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/539bc6865ab887c924880cea939b96bf/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="AI大模型探索之路-训练篇15：大语言模型预训练之全量参数微调">
  <meta property="og:description" content="系列篇章💥 AI大模型探索之路-训练篇1：大语言模型微调基础认知
AI大模型探索之路-训练篇2：大语言模型预训练基础认知
AI大模型探索之路-训练篇3：大语言模型全景解读
AI大模型探索之路-训练篇4：大语言模型训练数据集概览
AI大模型探索之路-训练篇5：大语言模型预训练数据准备-词元化
AI大模型探索之路-训练篇6：大语言模型预训练数据准备-预处理
AI大模型探索之路-训练篇7：大语言模型Transformer库之HuggingFace介绍
AI大模型探索之路-训练篇8：大语言模型Transformer库-预训练流程编码体验
AI大模型探索之路-训练篇9：大语言模型Transformer库-Pipeline组件实践
AI大模型探索之路-训练篇10：大语言模型Transformer库-Tokenizer组件实践
AI大模型探索之路-训练篇11：大语言模型Transformer库-Model组件实践
AI大模型探索之路-训练篇12：语言模型Transformer库-Datasets组件实践
AI大模型探索之路-训练篇13：大语言模型Transformer库-Evaluate组件实践
AI大模型探索之路-训练篇14：大语言模型Transformer库-Trainer组件实践
目录 系列篇章💥前言一、预训练任务类型二、模型和数据集选择三、指令微调数据处理四、全量参数微调实践学术资源加速步骤1：导入相关包步骤2：加载数据集步骤3：数据预处理步骤4：创建模型步骤5：配置训练参数步骤6：创建训练器步骤7：模型训练步骤8：模型推理 总结 前言 在自然语言处理（NLP）领域，预训练模型的应用已经越来越广泛。预训练模型通过大规模的无监督学习，能够捕捉到丰富的语言知识和上下文信息。然而，由于预训练模型通常需要大量的计算资源和时间进行训练，因此在实际使用时，我们往往需要对预训练模型进行微调，以便更好地适应特定的任务需求。本文将介绍全量参数微调的方法，以及如何在实践中进行操作。
一、预训练任务类型 1）掩码语言模型，自编码模型
将一些位置的token替换成特殊[MASK]字符，预测被替换的字符；(代表：BERT)
2）因果模型，自回归模型
将完整序列输入，基于上文的token预测下文的token；(代表：GPT)
3）序列到序列模型
采用编码器解码器的方式，预测放在解码器部分 （代表：GLM）
二、模型和数据集选择 目标：训练一个对话模型
模型：https://huggingface.co/Langboat/bloom-800m-zh
数据集：https://huggingface.co/datasets/c-s-ale/alpaca-gpt4-data-zh
三、指令微调数据处理 自回归编码指令微调数据处理过程
1）input输入构建：首先，我们将数据集中的指令（instruction），用户输入（input），以及预期输出（output）拼接成单一的字符串。这创建了一个格式为[instruction] [input] [output]的序列。
2）label标签创建：接着，为了构建训练标签，我们将用户输入部分保持不变，而对于输出部分，我们将其转化为目标标签。在自回归编码语言模型中，除了输出部分外，其他部分（包括指令和输入）的标签被替换为一个特殊的分隔符（例如：[SEP]）加上-100，表示这部分不需要模型去预测。（前面instruction 和 input，对应部分不需要推理，采用-100填充；后面补上output）
四、全量参数微调实践 在自然语言处理（NLP）领域，全量参数微调（Fine-tuning）是释放预训练语言模型潜力的关键步骤。该过程涉及对大规模模型进行细微调整，以适应特定的下游任务。全量参数微调的标准流程包括：导包、加载数据集、数据预处理、创建模型、配置训练参数、创建训练器、模型训练、模型推理。
学术资源加速 方便从huggingface下载模型，这云平台autodl提供的，仅适用于autodl。
import subprocess import os result = subprocess.run(&#39;bash -c &#34;source /etc/network_turbo &amp;&amp; env | grep proxy&#34;&#39;, shell=True, capture_output=True, text=True) output = result.stdout for line in output.splitlines(): if &#39;=&#39; in line: var, value = line.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-10T08:36:45+08:00">
    <meta property="article:modified_time" content="2024-05-10T08:36:45+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AI大模型探索之路-训练篇15：大语言模型预训练之全量参数微调</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_2"></a>系列篇章💥</h2> 
<p><a href="https://xundaomalu.blog.csdn.net/article/details/138107946" rel="nofollow">AI大模型探索之路-训练篇1：大语言模型微调基础认知</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138143923" rel="nofollow">AI大模型探索之路-训练篇2：大语言模型预训练基础认知</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138161057" rel="nofollow">AI大模型探索之路-训练篇3：大语言模型全景解读</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138205204" rel="nofollow">AI大模型探索之路-训练篇4：大语言模型训练数据集概览</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138225299" rel="nofollow">AI大模型探索之路-训练篇5：大语言模型预训练数据准备-词元化</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138267915" rel="nofollow">AI大模型探索之路-训练篇6：大语言模型预训练数据准备-预处理</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138294519" rel="nofollow">AI大模型探索之路-训练篇7：大语言模型Transformer库之HuggingFace介绍</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138348834" rel="nofollow">AI大模型探索之路-训练篇8：大语言模型Transformer库-预训练流程编码体验</a><br> <a href="https://blog.csdn.net/xiaobing259/article/details/138373677">AI大模型探索之路-训练篇9：大语言模型Transformer库-Pipeline组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138391592" rel="nofollow">AI大模型探索之路-训练篇10：大语言模型Transformer库-Tokenizer组件实践</a><br> <a href="https://blog.csdn.net/xiaobing259/article/details/138424867">AI大模型探索之路-训练篇11：大语言模型Transformer库-Model组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138426216" rel="nofollow">AI大模型探索之路-训练篇12：语言模型Transformer库-Datasets组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138448172" rel="nofollow">AI大模型探索之路-训练篇13：大语言模型Transformer库-Evaluate组件实践</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138448511" rel="nofollow">AI大模型探索之路-训练篇14：大语言模型Transformer库-Trainer组件实践</a></p> 
<hr> 
<p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#_2" rel="nofollow">系列篇章💥</a></li><li><a href="#_22" rel="nofollow">前言</a></li><li><a href="#_24" rel="nofollow">一、预训练任务类型</a></li><li><a href="#_39" rel="nofollow">二、模型和数据集选择</a></li><li><a href="#_45" rel="nofollow">三、指令微调数据处理</a></li><li><a href="#_52" rel="nofollow">四、全量参数微调实践</a></li><li><ul><li><a href="#_54" rel="nofollow">学术资源加速</a></li><li><a href="#1_69" rel="nofollow">步骤1：导入相关包</a></li><li><a href="#2_77" rel="nofollow">步骤2：加载数据集</a></li><li><a href="#3_110" rel="nofollow">步骤3：数据预处理</a></li><li><a href="#4_201" rel="nofollow">步骤4：创建模型</a></li><li><a href="#5_209" rel="nofollow">步骤5：配置训练参数</a></li><li><a href="#6_230" rel="nofollow">步骤6：创建训练器</a></li><li><a href="#7_249" rel="nofollow">步骤7：模型训练</a></li><li><a href="#8_268" rel="nofollow">步骤8：模型推理</a></li></ul> 
  </li><li><a href="#_285" rel="nofollow">总结</a></li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_22"></a>前言</h2> 
<p>在自然语言处理（NLP）领域，预训练模型的应用已经越来越广泛。预训练模型通过大规模的无监督学习，能够捕捉到丰富的语言知识和上下文信息。然而，由于预训练模型通常需要大量的计算资源和时间进行训练，因此在实际使用时，我们往往需要对预训练模型进行微调，以便更好地适应特定的任务需求。本文将介绍全量参数微调的方法，以及如何在实践中进行操作。</p> 
<h2><a id="_24"></a>一、预训练任务类型</h2> 
<p>1）掩码语言模型，自编码模型<br> 将一些位置的token替换成特殊[MASK]字符，预测被替换的字符；(代表：BERT)<br> <img src="https://images2.imgbox.com/74/a8/KRN7gvQo_o.png" alt="在这里插入图片描述"></p> 
<p>2）因果模型，自回归模型<br> 将完整序列输入，基于上文的token预测下文的token；(代表：GPT)<br> <img src="https://images2.imgbox.com/f8/84/o8xGHVVC_o.png" alt="在这里插入图片描述"></p> 
<p>3）序列到序列模型<br> 采用编码器解码器的方式，预测放在解码器部分 （代表：GLM）<br> <img src="https://images2.imgbox.com/b3/00/N0jbXO4m_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_39"></a>二、模型和数据集选择</h2> 
<p>目标：训练一个对话模型<br> 模型：https://huggingface.co/Langboat/bloom-800m-zh<br> 数据集：https://huggingface.co/datasets/c-s-ale/alpaca-gpt4-data-zh<br> <img src="https://images2.imgbox.com/7c/b2/gHBWkrMe_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_45"></a>三、指令微调数据处理</h2> 
<p>自回归编码指令微调数据处理过程<br> 1）<strong>input输入构建</strong>：首先，我们将数据集中的指令（instruction），用户输入（input），以及预期输出（output）拼接成单一的字符串。这创建了一个格式为[instruction] [input] [output]的序列。<br> 2）<strong>label标签创建</strong>：接着，为了构建训练标签，我们将用户输入部分保持不变，而对于输出部分，我们将其转化为目标标签。在自回归编码语言模型中，除了输出部分外，其他部分（包括指令和输入）的标签被替换为一个特殊的分隔符（例如：[SEP]）加上-100，表示这部分不需要模型去预测。（前面instruction 和 input，对应部分不需要推理，采用-100填充；后面补上output）<br> <img src="https://images2.imgbox.com/c0/b1/LUNh4Tq0_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_52"></a>四、全量参数微调实践</h2> 
<p>在自然语言处理（NLP）领域，全量参数微调（Fine-tuning）是释放预训练语言模型潜力的关键步骤。该过程涉及对大规模模型进行细微调整，以适应特定的下游任务。全量参数微调的标准流程包括：导包、加载数据集、数据预处理、创建模型、配置训练参数、创建训练器、模型训练、模型推理。</p> 
<h3><a id="_54"></a>学术资源加速</h3> 
<p>方便从huggingface下载模型，这云平台<a href="https://www.autodl.com/" rel="nofollow">autodl</a>提供的，仅适用于autodl。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> subprocess
<span class="token keyword">import</span> os

result <span class="token operator">=</span> subprocess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token string">'bash -c "source /etc/network_turbo &amp;&amp; env | grep proxy"'</span><span class="token punctuation">,</span> shell<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> capture_output<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> result<span class="token punctuation">.</span>stdout
<span class="token keyword">for</span> line <span class="token keyword">in</span> output<span class="token punctuation">.</span>splitlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token string">'='</span> <span class="token keyword">in</span> line<span class="token punctuation">:</span>
        var<span class="token punctuation">,</span> value <span class="token operator">=</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'='</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span>var<span class="token punctuation">]</span> <span class="token operator">=</span> value
</code></pre> 
<h3><a id="1_69"></a>步骤1：导入相关包</h3> 
<p>开始之前，我们需要导入适用于模型训练和推理的必要库，如transformers。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> datasets <span class="token keyword">import</span> Dataset
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForCausalLM<span class="token punctuation">,</span> DataCollatorForSeq2Seq<span class="token punctuation">,</span> TrainingArguments<span class="token punctuation">,</span> Trainer
</code></pre> 
<h3><a id="2_77"></a>步骤2：加载数据集</h3> 
<p>使用适当的数据加载器，例如datasets库，来加载预处理过的指令遵循性任务数据集。</p> 
<pre><code class="prism language-python">ds <span class="token operator">=</span> Dataset<span class="token punctuation">.</span>load_from_disk<span class="token punctuation">(</span><span class="token string">"data/alpaca_data_zh/"</span><span class="token punctuation">)</span>
ds
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">Dataset<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>
    features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'output'</span><span class="token punctuation">,</span> <span class="token string">'input'</span><span class="token punctuation">,</span> <span class="token string">'instruction'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    num_rows<span class="token punctuation">:</span> <span class="token number">26858</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<p>数据查看：</p> 
<pre><code class="prism language-python">ds<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python"><span class="token punctuation">{<!-- --></span><span class="token string">'output'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'以下是保持健康的三个提示：\n\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\n\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\n\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。'</span><span class="token punctuation">,</span>
  <span class="token string">'4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。'</span><span class="token punctuation">,</span>
  <span class="token string">'朱利叶斯·凯撒，又称尤利乌斯·恺撒（Julius Caesar）是古罗马的政治家、军事家和作家。他于公元前44年3月15日被刺杀。 \n\n根据历史记载，当时罗马元老院里一些参议员联合起来策划了对恺撒的刺杀行动，因为他们担心恺撒的统治将给罗马共和制带来威胁。在公元前44年3月15日（又称“3月的艾达之日”），恺撒去参加元老院会议时，被一群参议员包围并被攻击致死。据记载，他身中23刀，其中一刀最终致命。'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token string">'input'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">''</span><span class="token punctuation">,</span> <span class="token string">'输入：4/16'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token string">'instruction'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'保持健康的三个提示。'</span><span class="token punctuation">,</span> <span class="token string">'解释为什么以下分数等同于1/4'</span><span class="token punctuation">,</span> <span class="token string">'朱利叶斯·凯撒是如何死亡的？'</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
</code></pre> 
<h3><a id="3_110"></a>步骤3：数据预处理</h3> 
<p>利用预训练模型的分词器（Tokenizer）对原始文本进行编码，并生成相应的输入ID、注意力掩码和标签。<br> 1）获取分词器</p> 
<pre><code class="prism language-python">tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"Langboat/bloom-800m-zh"</span><span class="token punctuation">)</span>
tokenizer
</code></pre> 
<p><img src="https://images2.imgbox.com/6d/3c/FE5Belev_o.png" alt="在这里插入图片描述"></p> 
<p>输出：</p> 
<pre><code class="prism language-python">BloomTokenizerFast<span class="token punctuation">(</span>name_or_path<span class="token operator">=</span><span class="token string">'Langboat/bloom-800m-zh'</span><span class="token punctuation">,</span> vocab_size<span class="token operator">=</span><span class="token number">46145</span><span class="token punctuation">,</span> model_max_length<span class="token operator">=</span><span class="token number">1000000000000000019884624838656</span><span class="token punctuation">,</span> is_fast<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> padding_side<span class="token operator">=</span><span class="token string">'left'</span><span class="token punctuation">,</span> truncation_side<span class="token operator">=</span><span class="token string">'right'</span><span class="token punctuation">,</span> special_tokens<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">'bos_token'</span><span class="token punctuation">:</span> <span class="token string">'&lt;s&gt;'</span><span class="token punctuation">,</span> <span class="token string">'eos_token'</span><span class="token punctuation">:</span> <span class="token string">'&lt;/s&gt;'</span><span class="token punctuation">,</span> <span class="token string">'unk_token'</span><span class="token punctuation">:</span> <span class="token string">'&lt;unk&gt;'</span><span class="token punctuation">,</span> <span class="token string">'pad_token'</span><span class="token punctuation">:</span> <span class="token string">'&lt;pad&gt;'</span><span class="token punctuation">}</span><span class="token punctuation">,</span> clean_up_tokenization_spaces<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  added_tokens_decoder<span class="token operator">=</span><span class="token punctuation">{<!-- --></span>
	<span class="token number">0</span><span class="token punctuation">:</span> AddedToken<span class="token punctuation">(</span><span class="token string">"&lt;unk&gt;"</span><span class="token punctuation">,</span> rstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> lstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> single_word<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> normalized<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> special<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
	<span class="token number">1</span><span class="token punctuation">:</span> AddedToken<span class="token punctuation">(</span><span class="token string">"&lt;s&gt;"</span><span class="token punctuation">,</span> rstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> lstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> single_word<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> normalized<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> special<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
	<span class="token number">2</span><span class="token punctuation">:</span> AddedToken<span class="token punctuation">(</span><span class="token string">"&lt;/s&gt;"</span><span class="token punctuation">,</span> rstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> lstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> single_word<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> normalized<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> special<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
	<span class="token number">3</span><span class="token punctuation">:</span> AddedToken<span class="token punctuation">(</span><span class="token string">"&lt;pad&gt;"</span><span class="token punctuation">,</span> rstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> lstrip<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> single_word<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> normalized<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> special<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>

</code></pre> 
<p>2）定义数据处理函数</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">process_func</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span>
    MAX_LENGTH <span class="token operator">=</span> <span class="token number">256</span>  <span class="token comment"># 定义最大长度为256</span>
    input_ids<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment"># 初始化输入ID、注意力掩码和标签列表</span>
    <span class="token comment"># 对指令和输入进行编码，返回输入ID和注意力掩码</span>
    instruction <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"Human: "</span> <span class="token operator">+</span> example<span class="token punctuation">[</span><span class="token string">"instruction"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> example<span class="token punctuation">[</span><span class="token string">"input"</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"\n\nAssistant: "</span><span class="token punctuation">)</span>
    <span class="token comment"># 对输出进行编码，返回输出ID和注意力掩码</span>
    response <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>example<span class="token punctuation">[</span><span class="token string">"output"</span><span class="token punctuation">]</span> <span class="token operator">+</span> tokenizer<span class="token punctuation">.</span>eos_token<span class="token punctuation">)</span>
    <span class="token comment"># 将指令和回应的输入ID合并</span>
    input_ids <span class="token operator">=</span> instruction<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span>
    <span class="token comment"># 将指令和回应的注意力掩码合并</span>
    attention_mask <span class="token operator">=</span> instruction<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"attention_mask"</span><span class="token punctuation">]</span>
    <span class="token comment"># 标签列表的前半部分是指令的长度个-100（表示这些位置的标签是被忽略的），后半部分是回应的输入ID</span>
    labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>instruction<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> response<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span>
    <span class="token comment"># 如果输入ID的长度大于最大长度，将其截断为最大长度</span>
    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span> <span class="token operator">&gt;</span> MAX_LENGTH<span class="token punctuation">:</span>
        input_ids <span class="token operator">=</span> input_ids<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
        attention_mask <span class="token operator">=</span> attention_mask<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
        labels <span class="token operator">=</span> labels<span class="token punctuation">[</span><span class="token punctuation">:</span>MAX_LENGTH<span class="token punctuation">]</span>
    <span class="token comment"># 返回一个包含输入ID列表、注意力掩码列表和标签列表的字典</span>
    <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"input_ids"</span><span class="token punctuation">:</span> input_ids<span class="token punctuation">,</span>
        <span class="token string">"attention_mask"</span><span class="token punctuation">:</span> attention_mask<span class="token punctuation">,</span>
        <span class="token string">"labels"</span><span class="token punctuation">:</span> labels
    <span class="token punctuation">}</span>
</code></pre> 
<p>3）对数据进行预处理</p> 
<pre><code class="prism language-python">tokenized_ds <span class="token operator">=</span> ds<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>process_func<span class="token punctuation">,</span> remove_columns<span class="token operator">=</span>ds<span class="token punctuation">.</span>column_names<span class="token punctuation">)</span>
tokenized_ds
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">Dataset<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>
    features<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">,</span> <span class="token string">'attention_mask'</span><span class="token punctuation">,</span> <span class="token string">'labels'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    num_rows<span class="token punctuation">:</span> <span class="token number">26858</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre> 
<p>4）数据格式检查<br> 检查input部分</p> 
<pre><code class="prism language-python"><span class="token comment">#检查一下数据格式，知识部分是否符合我们的需求</span>
tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>tokenized_ds<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python"><span class="token string">'Human: 解释为什么以下分数等同于1/4\n输入：4/16\n\nAssistant: 4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。&lt;/s&gt;'</span>
</code></pre> 
<p>检查label部分</p> 
<pre><code class="prism language-python"><span class="token comment">## 检查一下数据格式，目标值是否符合我们的需求</span>
tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">filter</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">,</span> tokenized_ds<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python"><span class="token string">'4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。&lt;/s&gt;'</span>
</code></pre> 
<h3><a id="4_201"></a>步骤4：创建模型</h3> 
<p>实例化一个预训练模型，它将作为我们微调的基础。</p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"Langboat/bloom-800m-zh"</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/a6/44/UpjjcA5D_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="5_209"></a>步骤5：配置训练参数</h3> 
<p>定义训练参数，包括输出目录、学习率、批次大小、梯度累积步数、优化器选择等。</p> 
<pre><code class="prism language-python">args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span><span class="token string">"/root/autodl-tmp/tuningdata/boomtuning"</span><span class="token punctuation">,</span><span class="token comment"># 指定模型训练结果的输出目录。</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>  <span class="token comment"># 指定每个设备（如GPU）上的批次大小</span>
    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>  <span class="token comment"># 指定梯度累积步数。在本例子中，每8个步骤进行一次梯度更新。</span>
    logging_steps<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token comment">#指定日志记录的频率。在本例子中，每10个步骤记录一次日志</span>
    num_train_epochs<span class="token operator">=</span><span class="token number">1</span> <span class="token comment">#指定训练的总轮数。在本例子中，训练将进行1轮, 实际使用是会是多轮</span>
<span class="token punctuation">)</span>

</code></pre> 
<p>查看服务器当前GPU使用情况，大概48G</p> 
<pre><code class="prism language-bash">nvidia-smi
</code></pre> 
<p><img src="https://images2.imgbox.com/82/48/PwocJHW6_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="6_230"></a>步骤6：创建训练器</h3> 
<p>初始化Trainer类，它封装了训练循环，并提供了一种简单的方式来运行训练和评估。</p> 
<pre><code class="prism language-python">trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
    model<span class="token operator">=</span>model<span class="token punctuation">,</span><span class="token comment">#指定训练模型</span>
    args<span class="token operator">=</span>args<span class="token punctuation">,</span> <span class="token comment">#指定训练参数</span>
    train_dataset<span class="token operator">=</span>tokenized_ds<span class="token punctuation">,</span> <span class="token comment">#指定数据集</span>
    data_collator<span class="token operator">=</span>DataCollatorForSeq2Seq<span class="token punctuation">(</span>tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment">#指定数据收集器。其中tokenizer是分词器，padding=True表示对输入进行填充以保持批次大小一致。</span>
<span class="token punctuation">)</span>
</code></pre> 
<p>data_collator在训练机器学习模型时，有以下作用：<br> <strong>1）数据转换</strong>：data_collator负责将输入的特征数据转换成统一形状和格式的张量(tensor)，这是为了便于模型进行统一的处理。<br> <strong>2）批量处理</strong>：它能够将多个数据样本整合成一个小批次(batch)的数据，这有助于提高模型训练的效率。<br> <strong>3）填充(padding)</strong>：在文本处理中，不同大小的输入需要被填充或截断到相同的长度以形成统一的形状，这对于很多自然语言处理模型来说是必要的，而DataCollatorWithPadding就是执行这一操作的常用collator。</p> 
<blockquote> 
 <p>综上所述，data_collator是连接数据处理与模型训练之间的重要桥梁，确保了数据的有效整理和组织，以便模型可以高效地从中学习。</p> 
</blockquote> 
<h3><a id="7_249"></a>步骤7：模型训练</h3> 
<p><strong>模型训练资源估算：</strong><br> <strong>1）模型大小</strong>：4G（模型参数数量是8亿，按10亿估算，如果参数使用32位浮点数，每个参数需要4字节的空间。因此，模型大小差不多是4G）<br> <strong>2）梯度大小</strong>：4G（梯度的大小通常与模型大小相同，因为它们是针对每个参数计算的。所以，如果模型大小是4GB，梯度大小也应该是4GB）<br> <strong>3）优化器状态</strong>：4G * 2 = 8G （优化器状态的大小取决于使用的优化器类型和其内部参数的数量。Adam优化器通常会为每个模型参数维护两个额外的向量。因此，如果模型大小是4GB，优化器状态可能需要额外的8GB，每个参数两个额外的值。）<br> <strong>4）其他开销</strong>：还需要考虑数据加载、中间变量、系统进程等其他内存开销。<br> 大约需要16G，实际资源则要比16G多一点</p> 
<p>调用train()方法启动训练过程。</p> 
<pre><code class="prism language-python">trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/74/62/osR9M57V_o.png" alt="在这里插入图片描述"></p> 
<p>训练后查看GPU<img src="https://images2.imgbox.com/7a/f2/1AcogIgO_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="8_268"></a>步骤8：模型推理</h3> 
<p>使用pipeline进行推理，展示模型的能力。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

pipe <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"text-generation"</span><span class="token punctuation">,</span> model<span class="token operator">=</span>model<span class="token punctuation">,</span> tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
ipt <span class="token operator">=</span> <span class="token string">"Human: {}\n{}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token string">"如何写好一个简历？"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"\n\nAssistant: "</span>
pipe<span class="token punctuation">(</span>ipt<span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> do_sample<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token punctuation">)</span>
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python"><span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span><span class="token string">'generated_text'</span><span class="token punctuation">:</span> <span class="token string">'Human: 如何写好一个简历？\n\nAssistant: 在撰写一份简历时，需要考虑很多细节。以下是一些关键因素：\n\n1. 结构清晰：将信息组织在一个清晰的的结构里。例如，使用一个目录或列表来将信息组织。避免使用令人反感的混乱结构，例如使用树、流程图或层次结构。\n\n2. 内容丰富：列出最重要的信息。包括关键字和关键问题，以吸引招聘师的注意。此外，可以使用大量的数据和图表来增强整张简历的信息性。\n\n3. 恰当的背景信息：用适当的背景信息进行补充和修饰，例如加入家庭信息、教育经历或工作经验等。可以提供更加个性化和关键性的背景信息来帮助招聘师更好地了解你的工作技能和能力。'</span><span class="token punctuation">}</span><span class="token punctuation">]</span>
</code></pre> 
<h2><a id="_285"></a>总结</h2> 
<p>在当今的自然语言处理领域，全量参数微调（Fine-tuning）已成为释放大型预训练语言模型潜力的关键技术手段。然而，随着模型规模的扩大，这一过程对计算资源的要求也急剧上升。</p> 
<p>在本次实践中，我们选择了一个拥有约8亿参数的模型进行全量参数微调。这一规模级别的模型，通常需要大约18GB的显存资源。尽管这在现代硬件上是可行的，但当模型规模扩大到80亿参数时，所需的显存将飙升至180GB，这种级别的资源消耗通常只有资金雄厚的企业才能承担。</p> 
<p>进一步地，如果模型的规模达到惊人的800亿参数，那么所需的显存将达到庞大的1800GB。如此巨大的资源需求，即便是对于许多公司而言，也是一项极具挑战的任务，往往超出了他们的能力范围。因此，尽管全量参数微调在技术上可行，但在实际的应用和研究中，由于资源的限制，这样的实践相对较少。</p> 
<p>鉴于此，大多数实际应用场景倾向于采用部分微调（Partial Fine-tuning）或迁移学习，这样既可以利用预训练模型中的知识，又可以针对性地调整模型以适应特定任务，而无需动用如全量微调那般庞大的计算资源。这种方法更加经济高效，同时也能产生令人满意的结果。</p> 
<p><img src="https://images2.imgbox.com/ea/73/wWkzP2VT_o.png" alt="在这里插入图片描述"></p> 
<p>🎯🔖更多专栏系列文章：<a href="https://blog.csdn.net/xiaobing259/category_12628007.html?spm=1001.2014.3001.5482"><strong>AIGC-AI大模型探索之路</strong></a></p> 
<blockquote> 
 <p>如果文章内容对您有所触动，别忘了<font color="red"><strong>点赞、⭐关注，收藏</strong></font>！加入我，让我们携手同行AI的探索之旅，一起开启智能时代的大门！</p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b2072d3258cdd6ee05943739392cc9a3/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">什么是MVC？什么是SpringMVC？什么是三层架构？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d9367c3b7ee2e6dc3155230cd8c6595e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">（附源码）Spring Boot毕业论文管理系统的设计与实现 毕业设计-78897</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>