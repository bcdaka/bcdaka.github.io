<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>英伟达、Mistral AI 开源企业级大模型，120亿参数、可商用 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/86f30fc005791ae4a18a7f327bd60072/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="英伟达、Mistral AI 开源企业级大模型，120亿参数、可商用">
  <meta property="og:description" content="全球AI领导者英伟达（Nvidia）和著名开源大模型平台Mistral.ai联合开源了，企业级大模型Mistral NeMo 12B。（以下简称“MN 12B”）
据悉，MN 12B一共有基础和指令微调两种模型，支持128K上下文长度，能生成文本、代码、摘要等，其性能比最新开源的Gemma 2更好。
基础模型开源地址：https://huggingface.co/mistralai/Mistral-Nemo-Base-2407
指令微调模型：https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407
MN 12B在多轮对话、数学、常识推理、世界知识以及编码方面表现出色，比谷歌开源的Gemma 2 9B、Meta开源的Llama 3 8B 性能更好。支持128K的上下文长度，能够更连贯、更准确地处理大量复杂信息。
MN 12B以Apache 2.0许可证发布，允许企业、个人开发者进行商业化基础训练和微调。此外，模型采用FP8数据格式进行模型推理，极大减少了内存大小并加快了部署速度，同时没有任何准确性的降低。这意味着模型可以更好地学习任务，更有效地处理多样化的场景，使其非常适合企业级业务用例。
MN 12B作为NVIDIA NIM推理微服务的一部分，使用了NVIDIA TensorRT-LLM引擎的优化推理性能。这种容器化格式允许在任何地方轻松部署，为各种应用提供了增强的灵活性，模型可以在几分钟内部署在任何地方，无需耗费几天的时间。
在多语言方面，MN 12B支持英语、中文、法语、德语、西班牙语、意大利语、葡萄牙语、日语、韩语、阿拉伯语等主流语言，在MMLU等多语言基准测试中超过了同类开源模型。
此外，MN 12B使用了一种基于Tiktoken的更高效分词器Tekken。该分词器经过100多种语言的训练，比之前Mistral模型中使用的 SentencePiece 分词器更有效地压缩自然语言文本和源代码。
尤其是在压缩源代码、中文、意大利语、法语、德语、西班牙语和俄语时，效率提升了大约30%。在压缩韩语和阿拉伯语时效率相比之前，分别提升了2倍和3倍。
Mistral AI的创始人兼首席科学家Guillaume Lample表示，Mistral NeMo结合了Mistral AI在训练数据方面的专长与NVIDIA优化的硬件和软件生态系统为各种应用场景提供了高性能。
本次与NVIDIA团队的合作，借助其顶级的硬件和软件，共同开发出了具有前所未有的准确度、灵活性、高效性的企业级大模型。
本文素材来源英伟达官网，如有侵权请联系删除
END">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-22T14:13:43+08:00">
    <meta property="article:modified_time" content="2024-07-22T14:13:43+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">英伟达、Mistral AI 开源企业级大模型，120亿参数、可商用</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>全球AI领导者英伟达（Nvidia）和著名开源大模型平台Mistral.ai联合开源了，企业级大模型Mistral NeMo 12B。（以下简称“MN 12B”）</p> 
<p>据悉，MN 12B一共有基础和指令微调两种模型，支持128K上下文长度，能生成文本、代码、摘要等，其性能比最新开源的Gemma 2更好。</p> 
<p>基础模型开源地址：https://huggingface.co/mistralai/Mistral-Nemo-Base-2407</p> 
<p>指令微调模型：https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="107" src="https://images2.imgbox.com/a3/35/3R11HYrc_o.png" width="554"></p> 
<p></p> 
<p class="img-center"><img alt="图片" height="280" src="https://images2.imgbox.com/69/42/hP7vkPJT_o.png" width="554"></p> 
<p>MN 12B在多轮对话、数学、常识推理、世界知识以及编码方面表现出色，比谷歌开源的Gemma 2 9B、Meta开源的Llama 3 8B 性能更好。支持128K的上下文长度，能够更连贯、更准确地处理大量复杂信息。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="160" src="https://images2.imgbox.com/82/6d/nJNCcYWS_o.png" width="554"></p> 
<p>MN 12B以Apache 2.0许可证发布，允许企业、个人开发者进行商业化基础训练和微调。此外，模型采用FP8数据格式进行模型推理，极大减少了内存大小并加快了部署速度，同时没有任何准确性的降低。这意味着模型可以更好地学习任务，更有效地处理多样化的场景，使其非常适合企业级业务用例。</p> 
<p>MN 12B作为NVIDIA NIM推理微服务的一部分，使用了NVIDIA TensorRT-LLM引擎的优化推理性能。这种容器化格式允许在任何地方轻松部署，为各种应用提供了增强的灵活性，模型可以在几分钟内部署在任何地方，无需耗费几天的时间。</p> 
<p><strong>在多语言方面，MN 12B支持英语、中文、法语、德语、西班牙语、意大利语、葡萄牙语、日语、韩语、阿拉伯语等主流语言</strong>，在MMLU等多语言基准测试中超过了同类开源模型。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="153" src="https://images2.imgbox.com/33/2c/4udQQqEl_o.png" width="554"></p> 
<p>此外，MN 12B使用了一种基于Tiktoken的更高效分词器Tekken。该分词器经过100多种语言的训练，比之前Mistral模型中使用的 SentencePiece 分词器更有效地压缩自然语言文本和源代码。</p> 
<p>尤其是在压缩源代码、中文、意大利语、法语、德语、西班牙语和俄语时，效率提升了大约30%。在压缩韩语和阿拉伯语时效率相比之前，分别提升了2倍和3倍。</p> 
<p><strong>Mistral AI的创始人兼首席科学家Guillaume Lample表示</strong>，Mistral NeMo结合了Mistral AI在训练数据方面的专长与NVIDIA优化的硬件和软件生态系统为各种应用场景提供了高性能。</p> 
<p>本次与NVIDIA团队的合作，借助其顶级的硬件和软件，共同开发出了具有前所未有的准确度、灵活性、高效性的企业级大模型。</p> 
<p style="text-align:center;">本文素材来源英伟达官网，如有侵权请联系删除</p> 
<p style="text-align:center;">END</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/23c2e569d32fefdb63bf41b966ea0afe/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Zookeeper入门篇，了解ZK存储特点</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/fce87a18f725c5248b9f89b00d413734/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Text Control 控件教程：使用 .NET C# 中的二维码和条形码增强文档</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>