<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【大数据实训】基于当当网图书信息的数据分析与可视化(八) - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/dd7c8f4aa2dfe66ee8c9847ed2d1b513/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【大数据实训】基于当当网图书信息的数据分析与可视化(八)">
  <meta property="og:description" content="基于当当网图书信息的数据分析与可视化 一、实验环境
（1）Linux： Ubuntu 16.04
（2）Python: 3.5
（3）Hadoop：3.1.3（4）Spark: 2.4.0（5）Web框架：flask 1.0.3
（6）可视化工具：Echarts
（7）开发工具：Visual Studio Code
二、小组成员及分工
（1）成员：林海滢，王惠玲，陈嘉怡，郭诗念
（2）分工：xxx负责xxxx部分，xxx负责xxxx部分，xxx负责xxxx部分。
三、数据采集
3.1数据集说明
爬取网站：http://search.dangdang.com/?key=java，是当当网的java图书的信息网站。数据文件：java_books.xlsx。其中包含了1661条与java图书信息有关的数据。
数据格式为：
图 3. 1 采集数据格式
数据中包含的内容如下：
（1）book_name： 图书的标题
（2）introduction：图书的简介
（3）author： 图书的作者
（4）price： 图书的价格（元/本）
（5）press： 图书出版社
（6）comment： 图书的评论
3.2.爬取数据集以及将其保存到本地D盘文件中的流程
（1）选取所需要爬取的页面进行遍历爬取
（2）通过正则表达式抓取所需要的数据
（3）将爬取出的数据转化为dataframe格式并保存为xlsx文件存放在D盘
四、数据清洗与预处理
4.1预处理中提取的数据
图 4.1 数据处理前的数据格式及存在问题的特征列
4.2 清洗预处理后的数据格式
图 4.2 数据处理后的数据格式及特征列
4.3 清洗与预处理的流程
（1）首先检查数据的结构以及是否有数据缺失。
（2）发现book_name特征列的数据格式不对，于是处理转换为了整数类型。
（3）发现price特征列的数据格式不对，于是处理转换为了浮点类型。
（4）内容简介列数据清洗 删除异常值。
（5）保存清洗与预处理后的数据集。
五、spark数据分析
5.1 数据分析目标
（1）图书的售价分布情况（观察图书价格大体集中在哪个分段得出图书价格趋势）
（2）部分图书出版社的出书数量统计
（3）图书的作者出书（观察哪个作者出的书最多）
（4）图书的评论分布情况（观察图书评论大体集中在哪个分段得出图书评论趋势）
（5）图书的部分作者数量统计
（6）分析价格的最大值、最小值、均值、方差和中位数
六、数据可视化">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-23T18:45:20+08:00">
    <meta property="article:modified_time" content="2024-04-23T18:45:20+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【大数据实训】基于当当网图书信息的数据分析与可视化(八)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_4"></a>基于当当网图书信息的数据分析与可视化</h3> 
<p>一、实验环境</p> 
<p>（1）Linux： Ubuntu 16.04<br> （2）Python: 3.5<br> （3）Hadoop：3.1.3（4）Spark: 2.4.0（5）Web框架：flask 1.0.3<br> （6）可视化工具：Echarts<br> （7）开发工具：Visual Studio Code</p> 
<p>二、小组成员及分工</p> 
<p>（1）成员：林海滢，王惠玲，陈嘉怡，郭诗念</p> 
<p>（2）分工：xxx负责xxxx部分，xxx负责xxxx部分，xxx负责xxxx部分。</p> 
<p>三、数据采集</p> 
<p>3.1数据集说明</p> 
<p>爬取网站：http://search.dangdang.com/?key=java，是当当网的java图书的信息网站。数据文件：java_books.xlsx。其中包含了1661条与java图书信息有关的数据。</p> 
<p>数据格式为：</p> 
<p><img src="https://images2.imgbox.com/6b/e8/pLF1dfFr_o.jpg" alt="img"></p> 
<p>图 3. 1 采集数据格式</p> 
<p>数据中包含的内容如下：<br> （1）book_name： 图书的标题</p> 
<p>（2）introduction：图书的简介</p> 
<p>（3）author： 图书的作者<br> （4）price： 图书的价格（元/本）</p> 
<p>（5）press： 图书出版社<br> （6）comment： 图书的评论</p> 
<p>3.2.爬取数据集以及将其保存到本地D盘文件中的流程</p> 
<p>（1）选取所需要爬取的页面进行遍历爬取</p> 
<p>（2）通过正则表达式抓取所需要的数据</p> 
<p>（3）将爬取出的数据转化为dataframe格式并保存为xlsx文件存放在D盘</p> 
<p>四、数据清洗与预处理</p> 
<p>4.1预处理中提取的数据</p> 
<p><img src="https://images2.imgbox.com/8b/f6/AAtxjmna_o.jpg" alt="img"><img src="https://images2.imgbox.com/ec/ee/sbD8TgYZ_o.jpg" alt="img"></p> 
<p>图 4.1 数据处理前的数据格式及存在问题的特征列</p> 
<p>4.2 清洗预处理后的数据格式</p> 
<p><img src="https://images2.imgbox.com/59/29/bRl6LDry_o.jpg" alt="img"><img src="https://images2.imgbox.com/b2/05/eiIClZtK_o.jpg" alt="img"></p> 
<p>图 4.2 数据处理后的数据格式及特征列</p> 
<p>4.3 清洗与预处理的流程</p> 
<p>（1）首先检查数据的结构以及是否有数据缺失。</p> 
<p>（2）发现book_name特征列的数据格式不对，于是处理转换为了整数类型。</p> 
<p>（3）发现price特征列的数据格式不对，于是处理转换为了浮点类型。</p> 
<p>（4）内容简介列数据清洗 删除异常值。</p> 
<p>（5）保存清洗与预处理后的数据集。</p> 
<p>五、spark数据分析</p> 
<p>5.1 数据分析目标</p> 
<p>（1）图书的售价分布情况（观察图书价格大体集中在哪个分段得出图书价格趋势）</p> 
<p><img src="https://images2.imgbox.com/7c/cc/bY62IC1g_o.jpg" alt="img"></p> 
<p>（2）部分图书出版社的出书数量统计</p> 
<p><img src="https://images2.imgbox.com/d5/a5/030Z05B3_o.jpg" alt="img"></p> 
<p>（3）图书的作者出书（观察哪个作者出的书最多）</p> 
<p>（4）图书的评论分布情况（观察图书评论大体集中在哪个分段得出图书评论趋势）</p> 
<p><img src="https://images2.imgbox.com/57/00/3Or2GLaO_o.jpg" alt="img"></p> 
<p>（5）图书的部分作者数量统计</p> 
<p><img src="https://images2.imgbox.com/05/9b/DRzHYR14_o.jpg" alt="img"></p> 
<p>（6）分析价格的最大值、最小值、均值、方差和中位数</p> 
<p><img src="https://images2.imgbox.com/d6/1a/zmtQ67RH_o.jpg" alt="img"></p> 
<p>六、数据可视化</p> 
<p>本实验的可视化基于mutplotlib实现。</p> 
<p>6.1.可视化环境</p> 
<p>利用和anaconda里面的jubiter和vscode进行可视化操作，最后的代码结构如下。</p> 
<p>6.2 图表展示与结论分析</p> 
<p>（1）图书的售价分布情况（观察图书价格大体集中在哪个分段得出图书价格趋势）</p> 
<p><img src="https://images2.imgbox.com/eb/04/0Ky2dNJ7_o.jpg" alt="img"></p> 
<p>图6.2.1图书的售价分布情况</p> 
<p>分析结论：通过这个柱状图可以看出图书售卖价格集中在20<sub>60这里。说明了大多数人购书倾向于中端价格。比如20</sub>40这里，售价比较便宜图书的销量就会多。而60<sub>80这里的价格上升了购买的人就相对少了，销量也随之减少。我们也可以从中得出20</sub>60的销量有1841，而20一下和60以上的销量有1159。所以大胆推测出我国中层收入人数是低高层收入人数的1.6倍左右。</p> 
<p>（2）部分图书出版社的出书数量统计</p> 
<p><img src="https://images2.imgbox.com/c0/36/VieBRWK0_o.jpg" alt="img"></p> 
<p>图6.2.2部分图书出版社的出书数量统计</p> 
<p>（3）图书的作者出书（观察哪个作者出的书最多）</p> 
<p>（4）图书的评论分布情况（观察图书评论大体集中在哪个分段得出图书评论趋势）</p> 
<p><img src="https://images2.imgbox.com/f0/5e/fAadlRkE_o.jpg" alt="img"></p> 
<p>图6.2.4图书的评论分布情况</p> 
<p>分析结论：通过这个图我们可以看出92%的图书评论都是在0~100之间。也就是说92%的人不爱对图书做出评论，其余少部分人会对图书做出评论。所以我们可以大胆推测现在大多数人都不爱对看过的书发布之间的看法。</p> 
<p>（5）图书的部分作者数量统计进行数据可视化图表分析</p> 
<p><img src="https://images2.imgbox.com/42/bf/fhmrun1s_o.jpg" alt="img"></p> 
<p>图6.2.5部分作者数量统计</p> 
<p>分析结论：通过这个图我们可以看出我们找出来的部分作者54%左右的作者写的书都在65本以下（这就与第五部分的数据分析相对应），也侧面说明了这个数据里面50%左右的作者写的java的书不多即50%以下的作者可能不是专门做java这个领域的，可能还包括别的领域，大多数关于java的图书都是专攻这个专业的组织和作者写的。</p> 
<p>（5）分析价格的最大值、最小值、均值、方差和中位数</p> 
<p><img src="https://images2.imgbox.com/f0/46/yRVhoA8k_o.jpg" alt="img"></p> 
<p>图6.2.6分析价格的最大值、最小值、均值、方差和中位数</p> 
<p>分析结论：通过这个图我们可以看出图书价格的中位数和均值在55块钱左右，方差在50左右，证明了图书的价格波动不是很大，从最大值、最小值中可以看出最便宜的图书是10元左右，最贵的图书是120元左右，所以表明了买一本java图书总体的均价为55元左右。</p> 
<h3><a id="_178"></a>代码部分：</h3> 
<p>附录A（代码）：</p> 
<pre><code class="prism language-bash"><span class="token comment">#清洗代码：</span>
<span class="token comment"># coding=utf-8</span>
<span class="token comment"># 数据清洗</span>
<span class="token function">import</span> os
<span class="token function">import</span> numpy as np
<span class="token function">import</span> pandas as pd
<span class="token function">import</span> csv
<span class="token function">df</span> <span class="token operator">=</span> pd.read_csv<span class="token punctuation">(</span><span class="token string">"java_books.csv"</span><span class="token punctuation">)</span>

df<span class="token punctuation">[</span><span class="token string">'book_name'</span><span class="token punctuation">]</span><span class="token operator">=</span>df<span class="token punctuation">[</span><span class="token string">'book_name'</span><span class="token punctuation">]</span>.str.strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#去掉book_name列所有值前后的空格</span>
df<span class="token punctuation">[</span><span class="token string">'author'</span><span class="token punctuation">]</span><span class="token operator">=</span>df<span class="token punctuation">[</span><span class="token string">'author'</span><span class="token punctuation">]</span>.str.strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#去掉price列所有值前后的空格</span>
df<span class="token punctuation">[</span><span class="token string">'introduction'</span><span class="token punctuation">]</span><span class="token operator">=</span>df<span class="token punctuation">[</span><span class="token string">'introduction'</span><span class="token punctuation">]</span>.astype<span class="token punctuation">(</span>str<span class="token punctuation">)</span><span class="token comment">#把book_name列所有值转化为字符串类型</span>
df.dropna<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span>,how<span class="token operator">=</span><span class="token string">'any'</span>,thresh<span class="token operator">=</span>None,subset<span class="token operator">=</span>None,inplace<span class="token operator">=</span>True<span class="token punctuation">)</span>
df<span class="token punctuation">[</span><span class="token string">'price'</span><span class="token punctuation">]</span>.astype<span class="token punctuation">(</span>float<span class="token punctuation">)</span>
<span class="token comment"># 内容简介列数据清洗 删除异常值</span>
df.drop<span class="token punctuation">(</span>df<span class="token punctuation">[</span>df<span class="token punctuation">[</span><span class="token string">'introduction'</span><span class="token punctuation">]</span>.str.contains<span class="token punctuation">(</span><span class="token string">'内容简介'</span><span class="token punctuation">)</span><span class="token punctuation">]</span>.index,inplace<span class="token operator">=</span>True<span class="token punctuation">)</span>
<span class="token comment"># 输出清理完的文件</span>
df.to_csv<span class="token punctuation">(</span>r<span class="token string">'D:/data/clean_ganji_rent1.csv'</span>,encoding<span class="token operator">=</span><span class="token string">'utf-8'</span>,index<span class="token operator">=</span>False<span class="token punctuation">)</span>

处理代码+spark分析代码:
（1）图书的售价分布情况（观察图书价格大体集中在哪个分段得出图书价格趋势）
<span class="token function">import</span> os
os.environ<span class="token punctuation">[</span><span class="token string">"JAVA_HOME"</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">"/usr/lib/jvm/jdk1.8.0_162"</span>
os.environ<span class="token punctuation">[</span><span class="token string">"PYSPARK_PYTHON"</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">"/usr/bin/python3.5"</span>

from pyspark.sql <span class="token function">import</span> SparkSession
<span class="token function">import</span> pandas as pd

xlsx <span class="token operator">=</span> <span class="token string">'/home/guosn/data/java_books.xlsx'</span>
csv <span class="token operator">=</span> <span class="token string">'/home/guosn/data/java_books.csv'</span>


def save_xlsx_to_text<span class="token punctuation">(</span><span class="token punctuation">)</span>:
    <span class="token string">""</span>"
    使用pandas将xlsx文件转换为csv文件
    供spark读取
    <span class="token string">""</span>"
    <span class="token function">df</span> <span class="token operator">=</span> pd.read_excel<span class="token punctuation">(</span>xlsx<span class="token punctuation">)</span>
    df.to_csv<span class="token punctuation">(</span>csv,index<span class="token operator">=</span>False<span class="token punctuation">)</span>



def price_distribution<span class="token punctuation">(</span>spark<span class="token punctuation">)</span>:
    <span class="token string">""</span>"计算价格区间分布<span class="token string">""</span>"
    <span class="token function">df</span> <span class="token operator">=</span> spark.read.csv<span class="token punctuation">(</span>csv,header<span class="token operator">=</span>True<span class="token punctuation">)</span>
    df.printSchema<span class="token punctuation">(</span><span class="token punctuation">)</span>
    df.createOrReplaceTempView<span class="token punctuation">(</span><span class="token string">"books"</span><span class="token punctuation">)</span>

    <span class="token comment"># 为每条数据打上所属价格区间标识</span>
    price_sql <span class="token operator">=</span> <span class="token string">""</span>"
    <span class="token keyword">select</span> 
        <span class="token keyword">case</span>  
        when <span class="token punctuation">(</span>price <span class="token operator">&lt;</span> <span class="token number">20.0</span><span class="token punctuation">)</span> <span class="token keyword">then</span> <span class="token string">'0-20'</span>
        when <span class="token punctuation">(</span>price <span class="token operator">&gt;=</span><span class="token number">20.0</span><span class="token punctuation">)</span> and <span class="token punctuation">(</span>price <span class="token operator">&lt;</span> <span class="token number">40.0</span><span class="token punctuation">)</span> <span class="token keyword">then</span> <span class="token string">'20-40'</span>
        when <span class="token punctuation">(</span>price <span class="token operator">&gt;=</span><span class="token number">40.0</span><span class="token punctuation">)</span> and price <span class="token operator">&lt;</span><span class="token number">60.0</span> <span class="token keyword">then</span> <span class="token string">'40-60'</span>
        when price <span class="token operator">&gt;=</span><span class="token number">60.0</span> and price <span class="token operator">&lt;</span><span class="token number">80.0</span> <span class="token keyword">then</span> <span class="token string">'60-80'</span>
        when price <span class="token operator">&gt;=</span><span class="token number">80.0</span> and price <span class="token operator">&lt;</span><span class="token number">100.0</span> <span class="token keyword">then</span> <span class="token string">'80-100'</span>
        <span class="token keyword">else</span> <span class="token string">'greater than 100'</span>
        end as price_range,
        books.*
    from books   
    <span class="token string">""</span>"

    price_result <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span>price_sql<span class="token punctuation">)</span>
    price_result.createOrReplaceTempView<span class="token punctuation">(</span><span class="token string">"price_results"</span><span class="token punctuation">)</span>

    price_result.show<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 按照价格区间进行分组，计算各组内数据总数</span>
    group_price_sql <span class="token operator">=</span> <span class="token string">'select price_range, count(*) from price_results group by price_range'</span>
    group_price_results <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span>group_price_sql<span class="token punctuation">)</span>

    group_price_results.show<span class="token punctuation">(</span><span class="token punctuation">)</span>


def comment_distribution<span class="token punctuation">(</span>spark<span class="token punctuation">)</span>:
    <span class="token string">""</span>"计算价格区间分布<span class="token string">""</span>"
    <span class="token function">df</span> <span class="token operator">=</span> spark.read.csv<span class="token punctuation">(</span>csv,header<span class="token operator">=</span>True<span class="token punctuation">)</span>
    df.printSchema<span class="token punctuation">(</span><span class="token punctuation">)</span>
    df.createOrReplaceTempView<span class="token punctuation">(</span><span class="token string">"books"</span><span class="token punctuation">)</span>

    <span class="token comment"># 为每条数据打上所属价格区间标识</span>
    comment_sql <span class="token operator">=</span> <span class="token string">""</span>"
    <span class="token keyword">select</span> 
        <span class="token keyword">case</span>  
        when <span class="token punctuation">(</span>comment <span class="token operator">&lt;</span> <span class="token number">100</span><span class="token punctuation">)</span> <span class="token keyword">then</span> <span class="token string">'0-100'</span>
        when <span class="token punctuation">(</span>comment <span class="token operator">&gt;=</span><span class="token number">100</span><span class="token punctuation">)</span> and <span class="token punctuation">(</span>comment <span class="token operator">&lt;</span> <span class="token number">200.0</span><span class="token punctuation">)</span> <span class="token keyword">then</span> <span class="token string">'100-200'</span>
        when <span class="token punctuation">(</span>comment <span class="token operator">&gt;=</span><span class="token number">200</span><span class="token punctuation">)</span> and <span class="token punctuation">(</span>comment <span class="token operator">&lt;</span><span class="token number">400</span><span class="token punctuation">)</span> <span class="token keyword">then</span> <span class="token string">'200-400'</span>
        when <span class="token punctuation">(</span>comment <span class="token operator">&gt;=</span><span class="token number">400</span><span class="token punctuation">)</span> and <span class="token punctuation">(</span>comment <span class="token operator">&lt;</span><span class="token number">600</span><span class="token punctuation">)</span> <span class="token keyword">then</span> <span class="token string">'400-600'</span>
        when comment <span class="token operator">&gt;=</span><span class="token number">600</span> and comment <span class="token operator">&lt;</span><span class="token number">800</span> <span class="token keyword">then</span> <span class="token string">'600-800'</span>
        when comment <span class="token operator">&gt;=</span><span class="token number">800</span> and comment <span class="token operator">&lt;</span><span class="token number">1000</span> <span class="token keyword">then</span> <span class="token string">'800-1000'</span>
        <span class="token keyword">else</span> <span class="token string">'greater than 10000'</span>
        end as comment_range,
        books.*
    from books   
    <span class="token string">""</span>"

    comment_result <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span>comment_sql<span class="token punctuation">)</span>
    comment_result.createOrReplaceTempView<span class="token punctuation">(</span><span class="token string">"comment_results"</span><span class="token punctuation">)</span>
    comment_result.show<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 按照价格区间进行分组，计算各组内数据总数</span>
    group_comment_sql <span class="token operator">=</span> <span class="token string">'select comment_range, count(*) from comment_results group by comment_range'</span>
    group_comment_results <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span>group_comment_sql<span class="token punctuation">)</span>
    group_comment_results.show<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token builtin class-name">:</span>
    sc <span class="token operator">=</span> SparkSession.builder.appName<span class="token punctuation">(</span><span class="token string">"Python Spark SQL basic example"</span><span class="token punctuation">)</span>.master<span class="token punctuation">(</span><span class="token string">'local'</span><span class="token punctuation">)</span> .getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 具体执行方法</span>
    save_xlsx_to_text<span class="token punctuation">(</span><span class="token punctuation">)</span>
    price_distribution<span class="token punctuation">(</span>sc<span class="token punctuation">)</span>
    comment_distribution<span class="token punctuation">(</span>sc<span class="token punctuation">)</span>

    sc.stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
（2）部分图书出版社的出书数量统计
<span class="token function">import</span> os
os.environ<span class="token punctuation">[</span><span class="token string">"JAVA_HOME"</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">"/usr/lib/jvm/jdk1.8.0_162"</span>
os.environ<span class="token punctuation">[</span><span class="token string">"PYSPARK_PYTHON"</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">'/usr/bin/python3.5'</span>

from pyspark <span class="token function">import</span> SparkConf,SparkContext
from pyspark.sql.session <span class="token function">import</span> SparkSession
from pyspark <span class="token function">import</span> SparkContext
from pyspark.sql.types <span class="token function">import</span> Row
<span class="token function">import</span> pandas as pd
<span class="token function">import</span> numpy as np

<span class="token comment">#</span>
xlsx <span class="token operator">=</span> <span class="token string">'/home/hadoop/data/java_books.xlsx'</span>
csv <span class="token operator">=</span> <span class="token string">'/home/hadoop/data/java_books.csv'</span>


def save_xlsx_to_text<span class="token punctuation">(</span><span class="token punctuation">)</span>:
    <span class="token string">""</span>"
    使用pandas将xlsx文件转换为csv文件
    供spark读取
    <span class="token string">""</span>"
    <span class="token function">df</span> <span class="token operator">=</span> pd.read_excel<span class="token punctuation">(</span>xlsx<span class="token punctuation">)</span>
    df.to_csv<span class="token punctuation">(</span>csv,index<span class="token operator">=</span>False<span class="token punctuation">)</span>


<span class="token keyword">if</span> <span class="token assign-left variable">__name__</span><span class="token operator">==</span><span class="token string">'__main__'</span><span class="token builtin class-name">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span>.setAppName<span class="token punctuation">(</span><span class="token string">"IpSearch"</span><span class="token punctuation">)</span>.setMaster<span class="token punctuation">(</span><span class="token string">"local[2]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext.getOrCreate<span class="token punctuation">(</span>conf<span class="token punctuation">)</span>
    spark <span class="token operator">=</span> SparkSession.builder.getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>
    sc.setLogLevel<span class="token punctuation">(</span><span class="token string">"WARN"</span><span class="token punctuation">)</span>

    <span class="token comment"># data = spark.read.csv(csv, header=True)</span>
    <span class="token assign-left variable">data</span><span class="token operator">=</span>sc.textFile<span class="token punctuation">(</span><span class="token string">"file:///home/hadoop/data/java_books .csv"</span><span class="token punctuation">)</span>
    <span class="token assign-left variable">data</span><span class="token operator">=</span>data.map<span class="token punctuation">(</span>lambda line: line.split<span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">))</span>
    <span class="token assign-left variable">datardd</span><span class="token operator">=</span>data.map<span class="token punctuation">(</span>lambda local:Row<span class="token punctuation">(</span>press<span class="token operator">=</span>local<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">))</span>
    print<span class="token punctuation">(</span>datardd.take<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">))</span>
    zhaopin <span class="token operator">=</span> spark.createDataFrame<span class="token punctuation">(</span>datardd<span class="token punctuation">)</span>
    zhaopin.createOrReplaceTempView<span class="token punctuation">(</span><span class="token string">"zhaopin"</span><span class="token punctuation">)</span>

    <span class="token comment"># datardd = sc.textFile("file:///home/hadoop/data/employee.txt")</span>
    <span class="token comment"># employee = spark.createDataFrame(datardd)</span>
    <span class="token comment"># employee.createOrReplaceTempView("employee")</span>
    <span class="token comment"># loc = spark.sql("select count(*) as count from employee group by location order by count desc")</span>

    siming <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span><span class="token string">"select count(*) as simingcount from (select press from zhaopin where press Like '%机械工业%' )"</span><span class="token punctuation">)</span>
    print<span class="token punctuation">(</span>siming.rdd.map<span class="token punctuation">(</span>lambda p: <span class="token string">'机械工业出版社:'</span> + str<span class="token punctuation">(</span>p.simingcount<span class="token punctuation">))</span>.collect<span class="token punctuation">(</span><span class="token punctuation">))</span>
    jimei <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span><span class="token string">"select count(*) as jimei from (select press from zhaopin where press like '%清华大学%' )"</span><span class="token punctuation">)</span>
    print<span class="token punctuation">(</span>jimei.rdd.map<span class="token punctuation">(</span>lambda p: <span class="token string">'清华大学出版社:'</span> + str<span class="token punctuation">(</span>p.jimei<span class="token punctuation">))</span>.collect<span class="token punctuation">(</span><span class="token punctuation">))</span>
    huli <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span><span class="token string">"select count(*) as huli from (select press from zhaopin where press like '%电子工业%')"</span><span class="token punctuation">)</span>
    print<span class="token punctuation">(</span>huli.rdd.map<span class="token punctuation">(</span>lambda p: <span class="token string">'电子工业出版社:'</span> + str<span class="token punctuation">(</span>p.huli<span class="token punctuation">))</span>.collect<span class="token punctuation">(</span><span class="token punctuation">))</span>
    haicang <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span><span class="token string">"select count(*) as haicang from (select press from zhaopin where press like '%中国铁道%' )"</span><span class="token punctuation">)</span>
    print<span class="token punctuation">(</span>haicang.rdd.map<span class="token punctuation">(</span>lambda p: <span class="token string">'中国铁道出版社:'</span> + str<span class="token punctuation">(</span>p.haicang<span class="token punctuation">))</span>.collect<span class="token punctuation">(</span><span class="token punctuation">))</span>
    tongan <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span><span class="token string">"select count(*) as tongan from (select press from zhaopin where press like '%人民邮电%')"</span><span class="token punctuation">)</span>
    print<span class="token punctuation">(</span>tongan.rdd.map<span class="token punctuation">(</span>lambda p: <span class="token string">'人民邮电出版社:'</span> + str<span class="token punctuation">(</span>p.tongan<span class="token punctuation">))</span>.collect<span class="token punctuation">(</span><span class="token punctuation">))</span>
    xiangan <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span><span class="token string">"select count(*) as xiangan from (select press from zhaopin where press like '%东南大学%')"</span><span class="token punctuation">)</span>
    print<span class="token punctuation">(</span>xiangan.rdd.map<span class="token punctuation">(</span>lambda p: <span class="token string">'东南大学出版社:'</span> + str<span class="token punctuation">(</span>p.xiangan<span class="token punctuation">))</span>.collect<span class="token punctuation">(</span><span class="token punctuation">))</span>
（3）图书的作者出书（观察哪个作者出的书最多）

（4）图书的评论分布情况（观察图书评论大体集中在哪个分段得出图书评论趋势）
def comment_distribution<span class="token punctuation">(</span>spark<span class="token punctuation">)</span>:
    <span class="token string">""</span>"计算价格区间分布<span class="token string">""</span>"
    <span class="token function">df</span> <span class="token operator">=</span> spark.read.csv<span class="token punctuation">(</span>csv,header<span class="token operator">=</span>True<span class="token punctuation">)</span>
    df.printSchema<span class="token punctuation">(</span><span class="token punctuation">)</span>
    df.createOrReplaceTempView<span class="token punctuation">(</span><span class="token string">"books"</span><span class="token punctuation">)</span>

    <span class="token comment"># 为每条数据打上所属价格区间标识</span>
    comment_sql <span class="token operator">=</span> <span class="token string">""</span>"
    <span class="token keyword">select</span> 
        <span class="token keyword">case</span>  
        when <span class="token punctuation">(</span>comment <span class="token operator">&lt;</span> <span class="token number">100</span><span class="token punctuation">)</span> <span class="token keyword">then</span> <span class="token string">'0-100'</span>
        when <span class="token punctuation">(</span>comment <span class="token operator">&gt;=</span><span class="token number">100</span><span class="token punctuation">)</span> and <span class="token punctuation">(</span>comment <span class="token operator">&lt;</span> <span class="token number">200.0</span><span class="token punctuation">)</span> <span class="token keyword">then</span> <span class="token string">'100-200'</span>
        when <span class="token punctuation">(</span>comment <span class="token operator">&gt;=</span><span class="token number">200</span><span class="token punctuation">)</span> and <span class="token punctuation">(</span>comment <span class="token operator">&lt;</span><span class="token number">400</span><span class="token punctuation">)</span> <span class="token keyword">then</span> <span class="token string">'200-400'</span>
        when <span class="token punctuation">(</span>comment <span class="token operator">&gt;=</span><span class="token number">400</span><span class="token punctuation">)</span> and <span class="token punctuation">(</span>comment <span class="token operator">&lt;</span><span class="token number">600</span><span class="token punctuation">)</span> <span class="token keyword">then</span> <span class="token string">'400-600'</span>
        when comment <span class="token operator">&gt;=</span><span class="token number">600</span> and comment <span class="token operator">&lt;</span><span class="token number">800</span> <span class="token keyword">then</span> <span class="token string">'600-800'</span>
        when comment <span class="token operator">&gt;=</span><span class="token number">800</span> and comment <span class="token operator">&lt;</span><span class="token number">1000</span> <span class="token keyword">then</span> <span class="token string">'800-1000'</span>
        <span class="token keyword">else</span> <span class="token string">'greater than 10000'</span>
        end as comment_range,
        books.*
    from books   
    <span class="token string">""</span>"

    comment_result <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span>comment_sql<span class="token punctuation">)</span>
    comment_result.createOrReplaceTempView<span class="token punctuation">(</span><span class="token string">"comment_results"</span><span class="token punctuation">)</span>
    comment_result.show<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 按照价格区间进行分组，计算各组内数据总数</span>
    group_comment_sql <span class="token operator">=</span> <span class="token string">'select comment_range, count(*) from comment_results group by comment_range'</span>
    group_comment_results <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span>group_comment_sql<span class="token punctuation">)</span>
    group_comment_results.show<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token builtin class-name">:</span>
    sc <span class="token operator">=</span> SparkSession.builder.appName<span class="token punctuation">(</span><span class="token string">"Python Spark SQL basic example"</span><span class="token punctuation">)</span>.master<span class="token punctuation">(</span><span class="token string">'local'</span><span class="token punctuation">)</span> .getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 具体执行方法</span>
    save_xlsx_to_text<span class="token punctuation">(</span><span class="token punctuation">)</span>
    price_distribution<span class="token punctuation">(</span>sc<span class="token punctuation">)</span>
    comment_distribution<span class="token punctuation">(</span>sc<span class="token punctuation">)</span>

    sc.stop<span class="token punctuation">(</span><span class="token punctuation">)</span>


（5）图书的部分作者数量统计
<span class="token function">import</span> os
os.environ<span class="token punctuation">[</span><span class="token string">'JAVA_HOME'</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">'/usr/lib/jvm/jdk1.8.0_162'</span>
os.environ<span class="token punctuation">[</span><span class="token string">"PYSPARK_PYTHON"</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">'/usr/bin/python3.5'</span>

from pyspark <span class="token function">import</span> SparkConf,SparkContext
from pyspark.sql.session <span class="token function">import</span> SparkSession
from pyspark <span class="token function">import</span> SparkContext
from pyspark.sql.types <span class="token function">import</span> Row
<span class="token function">import</span> pandas as pd
<span class="token function">import</span> numpy as np

xlsx <span class="token operator">=</span> <span class="token string">'/home/hadoop/data/java_books.xlsx'</span>
csv <span class="token operator">=</span> <span class="token string">'/home/hadoop/data/java_books.csv'</span>
def save_xlsx_to_text<span class="token punctuation">(</span><span class="token punctuation">)</span>:
    <span class="token function">df</span> <span class="token operator">=</span> pd.read_excel<span class="token punctuation">(</span>xlsx<span class="token punctuation">)</span>
    df.to_csv<span class="token punctuation">(</span>csv,index<span class="token operator">=</span>False<span class="token punctuation">)</span>


<span class="token keyword">if</span> <span class="token assign-left variable">__name__</span><span class="token operator">==</span><span class="token string">'__main__'</span><span class="token builtin class-name">:</span>
    conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span>.setAppName<span class="token punctuation">(</span><span class="token string">"IpSearch"</span><span class="token punctuation">)</span>.setMaster<span class="token punctuation">(</span><span class="token string">"local[2]"</span><span class="token punctuation">)</span>
    sc <span class="token operator">=</span> SparkContext.getOrCreate<span class="token punctuation">(</span>conf<span class="token punctuation">)</span>
    spark <span class="token operator">=</span> SparkSession.builder.getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>
    sc.setLogLevel<span class="token punctuation">(</span><span class="token string">"WARN"</span><span class="token punctuation">)</span>

    <span class="token comment"># data = spark.read.csv(csv, header=True)</span>
    <span class="token assign-left variable">data</span><span class="token operator">=</span>sc.textFile<span class="token punctuation">(</span><span class="token string">"file:///home/hadoop/data/java_books .csv"</span><span class="token punctuation">)</span>
    <span class="token assign-left variable">data</span><span class="token operator">=</span>data.map<span class="token punctuation">(</span>lambda line: line.split<span class="token punctuation">(</span><span class="token string">","</span><span class="token punctuation">))</span>
    <span class="token assign-left variable">datardd</span><span class="token operator">=</span>data.map<span class="token punctuation">(</span>lambda local:Row<span class="token punctuation">(</span>press<span class="token operator">=</span>local<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">))</span>
    print<span class="token punctuation">(</span>datardd.take<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">))</span>
    zhaopin <span class="token operator">=</span> spark.createDataFrame<span class="token punctuation">(</span>datardd<span class="token punctuation">)</span>
    zhaopin.createOrReplaceTempView<span class="token punctuation">(</span><span class="token string">"zhaopin"</span><span class="token punctuation">)</span>
    siming <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span><span class="token string">"select count(*) as simingcount from (select press from zhaopin where press Like '%明日科技%' )"</span><span class="token punctuation">)</span>
    print<span class="token punctuation">(</span>siming.rdd.map<span class="token punctuation">(</span>lambda p: <span class="token string">'明日科技:'</span> + str<span class="token punctuation">(</span>p.simingcount<span class="token punctuation">))</span>.collect<span class="token punctuation">(</span><span class="token punctuation">))</span>
    jimei <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span><span class="token string">"select count(*) as jimei from (select press from zhaopin where press like '%霍斯特曼%' )"</span><span class="token punctuation">)</span>
    print<span class="token punctuation">(</span>jimei.rdd.map<span class="token punctuation">(</span>lambda p: <span class="token string">'霍斯特曼:'</span> + str<span class="token punctuation">(</span>p.jimei<span class="token punctuation">))</span>.collect<span class="token punctuation">(</span><span class="token punctuation">))</span>
    huli <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span><span class="token string">"select count(*) as huli from (select press from zhaopin where press like '%传智播客高教产品研发部%')"</span><span class="token punctuation">)</span>
    print<span class="token punctuation">(</span>huli.rdd.map<span class="token punctuation">(</span>lambda p: <span class="token string">'传智播客高教产品研发部:'</span> + str<span class="token punctuation">(</span>p.huli<span class="token punctuation">))</span>.collect<span class="token punctuation">(</span><span class="token punctuation">))</span>
    haicang <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span><span class="token string">"select count(*) as haicang from (select press from zhaopin where press like '%丁振凡%' )"</span><span class="token punctuation">)</span>
    print<span class="token punctuation">(</span>haicang.rdd.map<span class="token punctuation">(</span>lambda p: <span class="token string">'丁振凡:'</span> + str<span class="token punctuation">(</span>p.haicang<span class="token punctuation">))</span>.collect<span class="token punctuation">(</span><span class="token punctuation">))</span>
    tongan <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span><span class="token string">"select count(*) as tongan from (select press from zhaopin where press like '%高洪岩%')"</span><span class="token punctuation">)</span>
    print<span class="token punctuation">(</span>tongan.rdd.map<span class="token punctuation">(</span>lambda p: <span class="token string">'高洪岩:'</span> + str<span class="token punctuation">(</span>p.tongan<span class="token punctuation">))</span>.collect<span class="token punctuation">(</span><span class="token punctuation">))</span>
    xiangan <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span><span class="token string">"select count(*) as xiangan from (select press from zhaopin where press like '%李兴华%')"</span><span class="token punctuation">)</span>
    print<span class="token punctuation">(</span>xiangan.rdd.map<span class="token punctuation">(</span>lambda p: <span class="token string">'李兴华:'</span> + str<span class="token punctuation">(</span>p.xiangan<span class="token punctuation">))</span>.collect<span class="token punctuation">(</span><span class="token punctuation">))</span>
    quanzhou <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span><span class="token string">"select count(*) as quanzhou from (select press from zhaopin where press like '%软件开发技术联盟%')"</span><span class="token punctuation">)</span>
    print<span class="token punctuation">(</span>quanzhou.rdd.map<span class="token punctuation">(</span>lambda p: <span class="token string">'软件开发技术联盟:'</span> + str<span class="token punctuation">(</span>p.quanzhou<span class="token punctuation">))</span>.collect<span class="token punctuation">(</span><span class="token punctuation">))</span>
    nanan <span class="token operator">=</span> spark.sql<span class="token punctuation">(</span><span class="token string">"select count(*) as nanan from (select press from zhaopin where press like '%其他作者%')"</span><span class="token punctuation">)</span>
    print<span class="token punctuation">(</span>nanan.rdd.map<span class="token punctuation">(</span>lambda p: <span class="token string">'其他作者:'</span> + str<span class="token punctuation">(</span>p.nanan<span class="token punctuation">))</span>.collect<span class="token punctuation">(</span><span class="token punctuation">))</span>

（6）分析价格的最大值、最小值、均值、方差和中位数

<span class="token function">import</span> os
os.environ<span class="token punctuation">[</span><span class="token string">'JAVA_HOME'</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">'/usr/lib/jvm/jdk1.8.0_162'</span>
os.environ<span class="token punctuation">[</span><span class="token string">"PYSPARK_PYTHON"</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">'/usr/bin/python3.5'</span>

<span class="token function">import</span> pandas as pd
from pyspark.sql <span class="token function">import</span> SparkSession

csv  <span class="token operator">=</span> <span class="token string">'/home/hadoop/data/java_books .csv'</span>

<span class="token comment"># SparkSession 配置</span>
spark <span class="token operator">=</span> SparkSession.builder .appName<span class="token punctuation">(</span><span class="token string">"My test"</span><span class="token punctuation">)</span> .getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># spark.conf.set("spark.executor.memory", "1g")</span>
spark.conf.set<span class="token punctuation">(</span><span class="token string">"spark.sql.execution.arrow.enabled"</span>, <span class="token string">"true"</span><span class="token punctuation">)</span>
sc <span class="token operator">=</span> spark.sparkContext
sc.setLogLevel<span class="token punctuation">(</span><span class="token string">"WARN"</span><span class="token punctuation">)</span>

air <span class="token operator">=</span> spark.read.csv<span class="token punctuation">(</span>csv,header<span class="token operator">=</span>True<span class="token punctuation">)</span>
air.printSchema<span class="token punctuation">(</span><span class="token punctuation">)</span>
air.describe<span class="token punctuation">(</span><span class="token string">'price'</span><span class="token punctuation">)</span>.show<span class="token punctuation">(</span><span class="token punctuation">)</span>
可视化代码:
（1）图书的售价分布情况（观察图书价格大体集中在哪个分段得出图书价格趋势）
from pyecharts.charts <span class="token function">import</span> Bar
from pyecharts <span class="token function">import</span> options as opts
<span class="token comment">#//设置行名</span>
columns <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"0-20"</span>, <span class="token string">"20-40"</span>, <span class="token string">"40-60"</span>, <span class="token string">"60-80"</span>, <span class="token string">"80-100"</span>, <span class="token string">"more than 100"</span><span class="token punctuation">]</span>
<span class="token comment">#//设置数据</span>
data1 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">282</span>, <span class="token number">1057</span>, <span class="token number">784</span>, <span class="token number">407</span>, <span class="token number">204</span>, <span class="token number">266</span><span class="token punctuation">]</span>

<span class="token comment">#//添加柱状图的配置项</span>
<span class="token assign-left variable">bar</span><span class="token operator">=</span><span class="token punctuation">(</span>
         Bar<span class="token punctuation">(</span><span class="token punctuation">)</span>
         .add_xaxis<span class="token punctuation">(</span>xaxis_data<span class="token operator">=</span>columns<span class="token punctuation">)</span>
         .add_yaxis<span class="token punctuation">(</span><span class="token string">"数量"</span>, <span class="token assign-left variable">y_axis</span><span class="token operator">=</span>data1<span class="token punctuation">)</span>
         .set_global_opts<span class="token punctuation">(</span>title_opts<span class="token operator">=</span>opts.TitleOpts<span class="token punctuation">(</span>title<span class="token operator">=</span><span class="token string">"图书的售价分布情况"</span><span class="token punctuation">))</span>
     <span class="token punctuation">)</span>
bar.render<span class="token punctuation">(</span><span class="token punctuation">)</span>
（2）部分图书出版社的出书数量统计
from pyecharts.charts <span class="token function">import</span> Bar
from pyecharts <span class="token function">import</span> options as opts

<span class="token comment">#//设置行名</span>
columns <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"清华大学出版社"</span>, <span class="token string">"机械工业出版社"</span>, <span class="token string">"电子工业出版社"</span>, <span class="token string">"人民邮电出版社"</span>, <span class="token string">"中国铁道出版社"</span>, <span class="token string">"东南大学出版社"</span><span class="token punctuation">]</span>
<span class="token comment">#//设置数据</span>
data1 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">565</span>, <span class="token number">440</span>, <span class="token number">387</span>, <span class="token number">282</span>, <span class="token number">64</span>, <span class="token number">40</span><span class="token punctuation">]</span>

<span class="token comment">#//添加柱状图的配置项</span>
<span class="token assign-left variable">bar</span><span class="token operator">=</span><span class="token punctuation">(</span>
     Bar<span class="token punctuation">(</span><span class="token punctuation">)</span>
         .add_xaxis<span class="token punctuation">(</span>xaxis_data<span class="token operator">=</span>columns<span class="token punctuation">)</span>
         .add_yaxis<span class="token punctuation">(</span><span class="token string">"数量"</span>, <span class="token assign-left variable">y_axis</span><span class="token operator">=</span>data1<span class="token punctuation">)</span>
         .set_global_opts<span class="token punctuation">(</span>title_opts<span class="token operator">=</span>opts.TitleOpts<span class="token punctuation">(</span>title<span class="token operator">=</span><span class="token string">"图书的出版社出书情况"</span><span class="token punctuation">))</span>
     <span class="token punctuation">)</span>
bar.render<span class="token punctuation">(</span><span class="token punctuation">)</span>
（3）图书的作者出书（观察哪个作者出的书最多）

（4）图书的评论分布情况（观察图书评论大体集中在哪个分段得出图书评论趋势）
from pyecharts.charts <span class="token function">import</span> Pie
from pyecharts <span class="token function">import</span> options as opts

columns <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"0-100"</span>, <span class="token string">"100-200"</span>, <span class="token string">"200-400"</span>, <span class="token string">"400-600"</span>, <span class="token string">"600-800"</span>, <span class="token string">"800-1000"</span>, <span class="token string">"more than 1000"</span><span class="token punctuation">]</span>
<span class="token comment">#//设置数据</span>
data1 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2756</span>, <span class="token number">61</span>, <span class="token number">46,22</span> , <span class="token number">15</span>, <span class="token number">14</span>, <span class="token number">86</span><span class="token punctuation">]</span>



pie <span class="token operator">=</span> <span class="token punctuation">(</span>
        Pie<span class="token punctuation">(</span><span class="token punctuation">)</span>
        	<span class="token comment"># 以[(lable,value),(lable,value),(lable,value)......]形式传入数据。</span>
            .add<span class="token punctuation">(</span><span class="token string">"饼图"</span>, list<span class="token punctuation">(</span>z <span class="token keyword">for</span> <span class="token for-or-select variable">z</span> <span class="token keyword">in</span> zip<span class="token punctuation">(</span>columns, data1<span class="token punctuation">))</span><span class="token punctuation">)</span>
            .set_series_opts<span class="token punctuation">(</span>label_opts<span class="token operator">=</span>opts.LabelOpts<span class="token punctuation">(</span>formatter<span class="token operator">=</span><span class="token string">"{b}: {c}"</span><span class="token punctuation">))</span>
            .set_global_opts<span class="token punctuation">(</span>title_opts<span class="token operator">=</span>opts.TitleOpts<span class="token punctuation">(</span>title<span class="token operator">=</span><span class="token string">"评论分布情况"</span><span class="token punctuation">))</span>
    <span class="token punctuation">)</span>
pie.render<span class="token punctuation">(</span><span class="token string">'饼图.html'</span><span class="token punctuation">)</span>

（5）图书的部分作者数量统计
<span class="token function">import</span> numpy as np
<span class="token function">import</span> matplotlib.pyplot as plt
<span class="token function">import</span> pandas as pd

plt.rcParams<span class="token punctuation">[</span><span class="token string">'font.sans-serif'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'SimHei'</span>  <span class="token comment"># 设置中文显示，否则可能无法显示中文或者是各种字符错乱</span>
plt.rcParams<span class="token punctuation">[</span><span class="token string">'axes.unicode_minus'</span><span class="token punctuation">]</span> <span class="token operator">=</span> False
size <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">72,65</span>,19,20,23,26,24,4<span class="token punctuation">]</span>
labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'明日科技'</span>, <span class="token string">'霍斯特曼'</span>, <span class="token string">'传智播客高教产品研发部'</span>,<span class="token string">'丁振凡'</span>,<span class="token string">'高洪岩'</span>,<span class="token string">'李兴华'</span>,<span class="token string">'软件开发技术联盟'</span>,<span class="token string">'其他作者'</span><span class="token punctuation">]</span>
explode <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">0.1</span>,0,0,0,0,0,0,0<span class="token punctuation">)</span>
plt.pie<span class="token punctuation">(</span>size,explode<span class="token operator">=</span>explode,labels<span class="token operator">=</span>labels,autopct<span class="token operator">=</span><span class="token string">'%1.1f%%'</span>,shadow<span class="token operator">=</span>False,startangle<span class="token operator">=</span><span class="token number">150</span><span class="token punctuation">)</span>
plt.title<span class="token punctuation">(</span><span class="token string">"饼图示例--部分作者数量统计"</span><span class="token punctuation">)</span>
plt.show<span class="token punctuation">(</span><span class="token punctuation">)</span>

（6）分析价格的最大值、最小值、均值、方差和中位数
<span class="token function">import</span> numpy as np
<span class="token function">import</span> pandas as pd
<span class="token function">import</span> matplotlib.pyplot as plt

plt.rcParams<span class="token punctuation">[</span><span class="token string">'font.sans-serif'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'SimHei'</span>  <span class="token comment"># 设置中文显示，否则可能无法显示中文或者是各种字符错乱</span>
plt.rcParams<span class="token punctuation">[</span><span class="token string">'axes.unicode_minus'</span><span class="token punctuation">]</span> <span class="token operator">=</span> False
data <span class="token operator">=</span> pd.read_csv<span class="token punctuation">(</span><span class="token string">"new/books.csv"</span><span class="token punctuation">)</span>  <span class="token comment"># 加载数据 必须加上</span>
values <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">'price'</span><span class="token punctuation">]</span>
plt.figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">6</span>, <span class="token number">6</span><span class="token punctuation">))</span>
plt.ylim<span class="token punctuation">(</span><span class="token number">0,180</span><span class="token punctuation">)</span>
label <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'图书价格箱线图'</span><span class="token punctuation">]</span>
<span class="token comment"># gdp = list(values[:, 3])</span>
plt.boxplot<span class="token punctuation">(</span>values, <span class="token assign-left variable">notch</span><span class="token operator">=</span>True, <span class="token assign-left variable">labels</span><span class="token operator">=</span>label, <span class="token assign-left variable">meanline</span><span class="token operator">=</span>True<span class="token punctuation">)</span>  <span class="token comment"># 第一个参数是数据，第二个参数是是否带有缺口，第三个参数是标签，第四个参数是是否带有均值线</span>
plt.title<span class="token punctuation">(</span><span class="token string">'图书价格箱线图'</span><span class="token punctuation">)</span>
plt.savefig<span class="token punctuation">(</span><span class="token string">'new/图书价格箱线图箱线图.png'</span><span class="token punctuation">)</span>
plt.show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/808172c285415cfc07eec015e02edfe5/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Llama 3 最强开源模型？深入剖析Meta Llama 3技术细节</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0a24a488287f1e2ed8327d592a33ba1b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【AI工具】LM Studio 部署本地llama3以及python调用openai的API与llama3本地服务器进行问答...</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>