<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>利用llama.cpp量化部署Llama-3-Chinese-8B-Instruct大模型 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/030bd57f57a0b250df592157590e81a0/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="利用llama.cpp量化部署Llama-3-Chinese-8B-Instruct大模型">
  <meta property="og:description" content="相关大模型项目：https://github.com/ymcui/Chinese-LLaMA-Alpaca-3
量化部署是将机器学习模型压缩以减少内存占用和计算成本的过程。本文将详细介绍如何使用llama.cpp工具进行模型量化并在本地部署。
环境准备 首先，确保你的系统满足以下要求：
编译工具：
MacOS/Linux：自带makeWindows：需自行安装cmake Python：
建议使用Python 3.10及以上版本 此外，推荐使用经过指令精调的Llama-3-Chinese-Instruct模型，并选择6-bit或8-bit的量化版本，以获得最佳效果。
步骤一：克隆和编译llama.cpp 重要提示：llama.cpp在2024年4月30日对Llama-3 pre-tokenizer进行了重大改动，请务必拉取最新代码进行编译。
如果你已下载旧版仓库，建议执行以下命令更新代码并清理旧文件：
$ git pull $ make clean 克隆最新版llama.cpp仓库代码：
$ git clone https://github.com/ggerganov/llama.cpp 编译llama.cpp项目，生成./main（用于推理）和./quantize（用于量化）二进制文件：
$ make 启用GPU推理：
Windows/Linux用户：可与BLAS（或cuBLAS）一起编译，以提高处理速度。以下是与cuBLAS一起编译的命令：
$ make LLAMA_CUDA=1 macOS用户：无需额外操作，llama.cpp已对ARM NEON进行优化，并默认启用BLAS。M系列芯片用户可使用Metal启用GPU推理，只需将编译命令改为：
$ LLAMA_METAL=1 make 步骤二：生成量化版本模型 你可以直接下载已量化好的GGUF模型：下载地址。
此外，llama.cpp支持将.safetensors文件和Hugging Face格式的.bin文件转换为FP16的GGUF格式。具体命令如下：
$ python convert-hf-to-gguf.py llama-3-chinese-8b-instruct $ ./quantize llama-3-chinese-instruct-8b/ggml-model-f16.gguf llama-3-chinese-8b-instruct/ggml-model-q4_0.gguf q4_0 步骤三：加载并启动模型 由于Llama-3-Chinese-Instruct模型使用了原版Llama-3-Instruct的指令模板，请将scripts/llama_cpp/chat.sh拷贝至llama.cpp的根目录。chat.sh文件内容如下，可根据需要进行修改：
FIRST_INSTRUCTION=$2 SYSTEM_PROMPT=&#34;You are a helpful assistant. 你是一个乐于助人的助手。&#34; ./main -m $1 --color -i \ -c 0 -t 6 --temp 0.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-15T17:00:19+08:00">
    <meta property="article:modified_time" content="2024-05-15T17:00:19+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">利用llama.cpp量化部署Llama-3-Chinese-8B-Instruct大模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>相关大模型项目：<a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-3">https://github.com/ymcui/Chinese-LLaMA-Alpaca-3</a></p> 
<p>量化部署是将机器学习模型压缩以减少内存占用和计算成本的过程。本文将详细介绍如何使用<a href="https://github.com/ggerganov/llama.cpp">llama.cpp工具</a>进行模型量化并在本地部署。</p> 
<p><img src="https://images2.imgbox.com/94/52/kG0AxXNw_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_7"></a>环境准备</h3> 
<p>首先，确保你的系统满足以下要求：</p> 
<ol><li> <p><strong>编译工具</strong>：</p> 
  <ul><li>MacOS/Linux：自带<code>make</code></li><li>Windows：需自行安装<code>cmake</code></li></ul> </li><li> <p><strong>Python</strong>：</p> 
  <ul><li>建议使用Python 3.10及以上版本</li></ul> </li></ol> 
<p>此外，推荐使用经过指令精调的Llama-3-Chinese-Instruct模型，并选择6-bit或8-bit的量化版本，以获得最佳效果。</p> 
<h3><a id="llamacpp_20"></a>步骤一：克隆和编译llama.cpp</h3> 
<p><strong>重要提示</strong>：llama.cpp在2024年4月30日对Llama-3 pre-tokenizer进行了重大改动，请务必拉取最新代码进行编译。</p> 
<ol><li> <p>如果你已下载旧版仓库，建议执行以下命令更新代码并清理旧文件：</p> <pre><code class="prism language-bash">$ <span class="token function">git</span> pull
$ <span class="token function">make</span> clean
</code></pre> </li><li> <p>克隆最新版llama.cpp仓库代码：</p> <pre><code class="prism language-bash">$ <span class="token function">git</span> clone https://github.com/ggerganov/llama.cpp
</code></pre> </li><li> <p>编译llama.cpp项目，生成<code>./main</code>（用于推理）和<code>./quantize</code>（用于量化）二进制文件：</p> <pre><code class="prism language-bash">$ <span class="token function">make</span>
</code></pre> </li><li> <p><strong>启用GPU推理</strong>：</p> 
  <ul><li> <p><strong>Windows/Linux用户</strong>：可与<a href="https://github.com/ggerganov/llama.cpp#blas-build">BLAS（或cuBLAS）一起编译</a>，以提高处理速度。以下是与cuBLAS一起编译的命令：</p> <pre><code class="prism language-bash">$ <span class="token function">make</span> <span class="token assign-left variable">LLAMA_CUDA</span><span class="token operator">=</span><span class="token number">1</span>
</code></pre> </li><li> <p><strong>macOS用户</strong>：无需额外操作，llama.cpp已对ARM NEON进行优化，并默认启用BLAS。M系列芯片用户可使用Metal启用GPU推理，只需将编译命令改为：</p> <pre><code class="prism language-bash">$ <span class="token assign-left variable">LLAMA_METAL</span><span class="token operator">=</span><span class="token number">1</span> <span class="token function">make</span>
</code></pre> </li></ul> </li></ol> 
<h3><a id="_57"></a>步骤二：生成量化版本模型</h3> 
<p>你可以直接下载已量化好的GGUF模型：<a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-3/blob/main/README.md#%E4%B8%8B%E8%BD%BD%E5%9C%B0%E5%9D%80">下载地址</a>。</p> 
<p>此外，llama.cpp支持将<code>.safetensors</code>文件和Hugging Face格式的<code>.bin</code>文件转换为FP16的GGUF格式。具体命令如下：</p> 
<pre><code class="prism language-bash">$ python convert-hf-to-gguf.py llama-3-chinese-8b-instruct
$ ./quantize llama-3-chinese-instruct-8b/ggml-model-f16.gguf llama-3-chinese-8b-instruct/ggml-model-q4_0.gguf q4_0
</code></pre> 
<h3><a id="_68"></a>步骤三：加载并启动模型</h3> 
<p>由于Llama-3-Chinese-Instruct模型使用了原版<a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" rel="nofollow">Llama-3-Instruct</a>的指令模板，请将<code>scripts/llama_cpp/chat.sh</code>拷贝至llama.cpp的根目录。<code>chat.sh</code>文件内容如下，可根据需要进行修改：</p> 
<pre><code class="prism language-bash"><span class="token assign-left variable">FIRST_INSTRUCTION</span><span class="token operator">=</span><span class="token variable">$2</span>
<span class="token assign-left variable">SYSTEM_PROMPT</span><span class="token operator">=</span><span class="token string">"You are a helpful assistant. 你是一个乐于助人的助手。"</span>

./main <span class="token parameter variable">-m</span> <span class="token variable">$1</span> <span class="token parameter variable">--color</span> <span class="token parameter variable">-i</span> <span class="token punctuation">\</span>
<span class="token parameter variable">-c</span> <span class="token number">0</span> <span class="token parameter variable">-t</span> <span class="token number">6</span> <span class="token parameter variable">--temp</span> <span class="token number">0.2</span> <span class="token parameter variable">--repeat_penalty</span> <span class="token number">1.1</span> <span class="token parameter variable">-ngl</span> <span class="token number">999</span> <span class="token punctuation">\</span>
<span class="token parameter variable">-r</span> <span class="token string">''</span> <span class="token punctuation">\</span>
--in-prefix <span class="token string">'user\n\n'</span> <span class="token punctuation">\</span>
--in-suffix <span class="token string">'assistant\n\n'</span> <span class="token punctuation">\</span>
<span class="token parameter variable">-p</span> <span class="token string">"system<span class="token entity" title="\n">\n</span><span class="token entity" title="\n">\n</span><span class="token variable">$SYSTEM_PROMPTuser</span><span class="token entity" title="\n">\n</span><span class="token entity" title="\n">\n</span><span class="token variable">$FIRST_INSTRUCTIONassistant</span><span class="token entity" title="\n">\n</span><span class="token entity" title="\n">\n</span>"</span>
</code></pre> 
<p>使用以下命令启动聊天：</p> 
<pre><code class="prism language-bash">$ <span class="token function">chmod</span> +x chat.sh
$ ./chat.sh ggml-model-q4_0.gguf 你好
</code></pre> 
<p>在提示符 <code>&gt;</code> 后输入你的prompt，按 <code>cmd/ctrl+c</code> 中断输出，多行信息以<code>\</code>作为行尾。查看帮助和参数说明请执行<code>./main -h</code>命令。</p> 
<p>更多详细说明请参考：<a href="https://github.com/ggerganov/llama.cpp/tree/master/examples/main">官方文档</a>。</p> 
<p>通过以上步骤，你可以成功在本地量化并部署Llama-3模型，享受高效的AI模型推理体验。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6d424889be6b0edbad2704f60e04579c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">一文带你了解三大开源关系型数据库：SQLite、MySQL和PostgreSQL</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/21f432f3099ce9d012076089fc7cf731/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">linux系统安装kafka（新版本3.7.0）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>