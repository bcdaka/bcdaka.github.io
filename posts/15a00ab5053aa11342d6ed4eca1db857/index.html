<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Llama中的曼巴：通过推测解码加速推理 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/15a00ab5053aa11342d6ed4eca1db857/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Llama中的曼巴：通过推测解码加速推理">
  <meta property="og:description" content="大型语言模型（LLMs）已经彻底改变了自然语言处理领域，但在处理非常长的序列时面临重大挑战。主要问题来自于Transformer架构的计算复杂度随序列长度呈二次方增长以及其巨大的键值（KV）缓存需求。这些限制严重影响了模型的效率，特别是在推理过程中，使生成长序列的速度极其缓慢。这一瓶颈限制了需要对多个长文件进行推理、处理大型代码库或在基于代理的系统中建模复杂环境的应用程序的发展。因此，研究人员正在寻找更高效的架构，既能保持或超越Transformer的性能，同时显著降低计算需求。
研究人员已经探索了各种方法来解决LLM中的效率挑战。无注意力模型，如S4、GSS和BiGS，展示了更高的计算和内存效率。结合了特定输入上下文选择的Mamba模型在不同规模上比Transformer表现出色。其他次二次方和混合架构也已被提出。知识蒸馏技术已被用来将Transformer的知识转移到线性RNN风格的模型中，如在Laughing Hyena和渐进知识方法中所示。推测解码成为加速推理的一种有前途的方法，利用较小的草稿模型生成候选标记，由较大的目标模型验证。这些方法包括拒绝采样方案、树状候选组织，以及训练和无训练的草稿模型。
康奈尔大学、日内瓦大学、Together AI和普林斯顿大学的研究人员提出了一种独特的方法，通过将预训练的Transformer蒸馏到线性RNN中来缓解LLM模型的效率挑战。此方法旨在保持生成质量的同时显著提高推理速度。该方法涉及将Transformer的权重映射到修改后的Mamba架构，该架构可以直接从预训练模型的注意力模块初始化。提出了一种多阶段蒸馏管道，结合渐进蒸馏、监督微调和定向偏好优化，以提高复杂度和下游性能。研究人员还开发了一种硬件感知的推测采样算法和快速内核，以在Mamba和混合架构上进行推测解码，实现了一个7B参数模型超过300标记/秒的吞吐量。此方法有效地将推测解码应用于混合架构，解决了复杂LLM应用中高效推理的需求。
该方法通过使用线性RNN将Transformer模型转换为Mamba模型，解决了注意力机制的限制。通过Mamba的连续时间状态空间模型扩展线性隐藏状态容量，该方法动态构建了离散时间线性RNN。这种创新架构从注意力参数初始化，并采用硬件感知分解以实现高效实施。然后，方法应用知识蒸馏将大型Transformer模型压缩到较小的基于Mamba的网络中，重点在于微调和对齐步骤。此过程结合了序列级知识蒸馏和单词级KL散度的监督微调，同时适应直接偏好优化以实现偏好对齐。
蒸馏过程使学生模型能够从教师模型的输出分布和生成中学习，优化性能并与预期偏好对齐。在此过程中，原始模型中的MLP层保持冻结状态，而Mamba层则进行训练以捕获蒸馏知识。这种方法使得可以用线性RNN块替换注意力块，同时保持模型性能。通过扩大隐藏状态大小和使用硬件感知的分解方法，该方法实现了高效的实现，允许更大的隐藏状态大小而不会显著增加计算成本。最终的基于Mamba的模型结合了Transformer架构的优势与线性RNN的效率，可能会在LLM领域取得进展。
蒸馏后的混合Mamba模型在各种基准测试中表现出竞争力。在AlpacaEval和MT-Bench这样的聊天基准测试中，50%混合模型的得分与其教师模型相当或略有优势，甚至超过了一些更大的Transformer模型。在零样本和少样本评估中，混合模型超越了从头训练的开源线性RNN模型，随着更多注意力层被替换，性能有所下降。混合模型在OpenLLM排行榜和ZeroEval基准测试中也表现出了良好结果。使用这些混合模型进行的推测解码实验在单个GPU上实现了高达1.88倍的加速。总体来看，结果表明蒸馏后的混合Mamba模型在效率和性能之间提供了良好的平衡。
本研究提出了一种通过线性RNN将Transformer模型转化为更高效的基于Mamba的模型的方法。结果表明，蒸馏后的混合Mamba模型在各种基准测试中，尤其是在聊天任务和通用语言理解任务中，表现出与教师模型相当或更好的性能。该方法在减少计算成本的同时，尤其是当保留25-50%的注意力层时，表现出特别的成功。此外，研究人员介绍了一种创新的线性RNN推测解码算法，进一步提升了推理速度。这些发现表明，在提高LLM效率的同时保持其能力方面具有显著潜力。
总结： 大型语言模型（LLMs）虽然在自然语言处理领域取得了革命性突破，但在面对长序列处理时依然存在显著的计算复杂度和内存需求问题，影响了高效推理的能力。为了解决这一瓶颈问题，研究人员提出了多种高效架构，如无注意力模型（S4、GSS和BiGS），以及结合特定上下文选择的Mamba模型，展示了较高的计算和内存效率。特别是通过将预训练的Transformer蒸馏到线性RNN中，康奈尔大学和其他机构的研究人员显著提高了推理速度，同时保持了生成质量。
通过使用线性RNN和多阶段蒸馏管道，这种新方法实现了复杂度优化和下游性能提升。研究人员还开发了硬件感知的推测采样算法和快速内核，使7B参数模型的标记生成速度超过300标记/秒。这一创新显著提高了LLM的推断效率，特别是在聊天任务和通用语言理解任务中表现优秀，并超越了一些更大的Transformer模型。
在AI技术的加持下，光年AI不仅助您轻松提升私域流量转化，还能全面优化私域流量管理，精准触达每一位潜在客户。光年AI智能化的私域流量管理体系，使您在竞争激烈的市场中快速突破，实现业务的飞速增长。选择光年AI，全面提升私域营销效果，让品牌影响力大幅提升！">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-09-07T09:53:17+08:00">
    <meta property="article:modified_time" content="2024-09-07T09:53:17+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Llama中的曼巴：通过推测解码加速推理</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>大型语言模型（LLMs）已经彻底改变了自然语言处理领域，但在处理非常长的序列时面临重大挑战。主要问题来自于Transformer架构的计算复杂度随序列长度呈二次方增长以及其巨大的键值（KV）缓存需求。这些限制严重影响了模型的效率，特别是在推理过程中，使生成长序列的速度极其缓慢。这一瓶颈限制了需要对多个长文件进行推理、处理大型代码库或在基于代理的系统中建模复杂环境的应用程序的发展。因此，研究人员正在寻找更高效的架构，既能保持或超越Transformer的性能，同时显著降低计算需求。</p> 
<p>研究人员已经探索了各种方法来解决LLM中的效率挑战。无注意力模型，如S4、GSS和BiGS，展示了更高的计算和内存效率。结合了特定输入上下文选择的Mamba模型在不同规模上比Transformer表现出色。其他次二次方和混合架构也已被提出。知识蒸馏技术已被用来将Transformer的知识转移到线性RNN风格的模型中，如在Laughing Hyena和渐进知识方法中所示。推测解码成为加速推理的一种有前途的方法，利用较小的草稿模型生成候选标记，由较大的目标模型验证。这些方法包括拒绝采样方案、树状候选组织，以及训练和无训练的草稿模型。</p> 
<p>康奈尔大学、日内瓦大学、Together AI和普林斯顿大学的研究人员提出了一种独特的方法，通过将预训练的Transformer蒸馏到线性RNN中来缓解LLM模型的效率挑战。此方法旨在保持生成质量的同时显著提高推理速度。该方法涉及将Transformer的权重映射到修改后的Mamba架构，该架构可以直接从预训练模型的注意力模块初始化。提出了一种多阶段蒸馏管道，结合渐进蒸馏、监督微调和定向偏好优化，以提高复杂度和下游性能。研究人员还开发了一种硬件感知的推测采样算法和快速内核，以在Mamba和混合架构上进行推测解码，实现了一个7B参数模型超过300标记/秒的吞吐量。此方法有效地将推测解码应用于混合架构，解决了复杂LLM应用中高效推理的需求。</p> 
<p>该方法通过使用线性RNN将Transformer模型转换为Mamba模型，解决了注意力机制的限制。通过Mamba的连续时间状态空间模型扩展线性隐藏状态容量，该方法动态构建了离散时间线性RNN。这种创新架构从注意力参数初始化，并采用硬件感知分解以实现高效实施。然后，方法应用知识蒸馏将大型Transformer模型压缩到较小的基于Mamba的网络中，重点在于微调和对齐步骤。此过程结合了序列级知识蒸馏和单词级KL散度的监督微调，同时适应直接偏好优化以实现偏好对齐。</p> 
<p>蒸馏过程使学生模型能够从教师模型的输出分布和生成中学习，优化性能并与预期偏好对齐。在此过程中，原始模型中的MLP层保持冻结状态，而Mamba层则进行训练以捕获蒸馏知识。这种方法使得可以用线性RNN块替换注意力块，同时保持模型性能。通过扩大隐藏状态大小和使用硬件感知的分解方法，该方法实现了高效的实现，允许更大的隐藏状态大小而不会显著增加计算成本。最终的基于Mamba的模型结合了Transformer架构的优势与线性RNN的效率，可能会在LLM领域取得进展。</p> 
<p>蒸馏后的混合Mamba模型在各种基准测试中表现出竞争力。在AlpacaEval和MT-Bench这样的聊天基准测试中，50%混合模型的得分与其教师模型相当或略有优势，甚至超过了一些更大的Transformer模型。在零样本和少样本评估中，混合模型超越了从头训练的开源线性RNN模型，随着更多注意力层被替换，性能有所下降。混合模型在OpenLLM排行榜和ZeroEval基准测试中也表现出了良好结果。使用这些混合模型进行的推测解码实验在单个GPU上实现了高达1.88倍的加速。总体来看，结果表明蒸馏后的混合Mamba模型在效率和性能之间提供了良好的平衡。</p> 
<p>本研究提出了一种通过线性RNN将Transformer模型转化为更高效的基于Mamba的模型的方法。结果表明，蒸馏后的混合Mamba模型在各种基准测试中，尤其是在聊天任务和通用语言理解任务中，表现出与教师模型相当或更好的性能。该方法在减少计算成本的同时，尤其是当保留25-50%的注意力层时，表现出特别的成功。此外，研究人员介绍了一种创新的线性RNN推测解码算法，进一步提升了推理速度。这些发现表明，在提高LLM效率的同时保持其能力方面具有显著潜力。</p> 
<h4>总结：</h4> 
<p>大型语言模型（LLMs）虽然在自然语言处理领域取得了革命性突破，但在面对长序列处理时依然存在显著的计算复杂度和内存需求问题，影响了高效推理的能力。为了解决这一瓶颈问题，研究人员提出了多种高效架构，如无注意力模型（S4、GSS和BiGS），以及结合特定上下文选择的Mamba模型，展示了较高的计算和内存效率。特别是通过将预训练的Transformer蒸馏到线性RNN中，康奈尔大学和其他机构的研究人员显著提高了推理速度，同时保持了生成质量。</p> 
<p>通过使用线性RNN和多阶段蒸馏管道，这种新方法实现了复杂度优化和下游性能提升。研究人员还开发了硬件感知的推测采样算法和快速内核，使7B参数模型的标记生成速度超过300标记/秒。这一创新显著提高了LLM的推断效率，特别是在聊天任务和通用语言理解任务中表现优秀，并超越了一些更大的Transformer模型。</p> 
<p>在AI技术的加持下，光年AI不仅助您轻松提升私域流量转化，还能全面优化私域流量管理，精准触达每一位潜在客户。光年AI智能化的私域流量管理体系，使您在竞争激烈的市场中快速突破，实现业务的飞速增长。选择<a class="link-info" href="https://gnlab.com/?hmsr=csdn&amp;hmpl=2401_86744188&amp;hmcu=141821521" rel="nofollow" title="光年AI">光年AI</a>，全面提升私域营销效果，让品牌影响力大幅提升！</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/09a39ee24f5dc3fb74120d973146a21c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">HTML 列表</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8fa98c200f6236fcfd96201652b05573/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Spark2.x 入门： KMeans 聚类算法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>