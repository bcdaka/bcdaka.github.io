<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AFAC2024-基于保险条款的问答 比赛日记 llamafactory qwen npu 910B1 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/7dd158867cc7575b2224e4eb13afa210/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="AFAC2024-基于保险条款的问答 比赛日记 llamafactory qwen npu 910B1">
  <meta property="og:description" content="AFAC2024: 基于保险条款的问答挑战——我的实战日记 概述 在最近的AFAC2024竞赛中，我参与了基于保险条款的问答赛道。这是一次深度学习与自然语言处理的实战演练，旨在提升模型在复杂保险文本理解与问答生成方面的能力。本文将分享我的参赛过程，包括数据处理、模型选择、微调策略、实验观察及最终成果。
比赛报名链接 https://tianchi.aliyun.com/competition/entrance/532194/introduction
数据与挑战 竞赛提供的数据集包含约6000条基于保险条款的问答对。这些数据覆盖了多种保险类型，如人寿保险、财产保险和健康保险，涉及保险条款的解释、索赔流程、覆盖范围等问题。数据集的多样性和专业性构成了此次竞赛的主要挑战。
llama factory 数据预处理 import json train = json.load(open(&#34;round1_training_data/train.json&#34;,&#39;r&#39;)) dev = json.load(open(&#34;round1_training_data/dev.json&#34;,&#39;r&#39;)) a = [] for train_one in train: a.append({&#34;input&#34;:&#34;&#34;&#34;目前有产品名称、相关条款。如果问题与产品名称、相关条款有关系，那么就依照产品名称、相关条款回答问题，如果没有关系直接回答问题。 根据&#34;&#34;&#34;&#43;train_one[&#39;产品名&#39;]&#43;&#34;&#34;&#34;,相关条款&#34;&#34;&#34;&#43;train_one[&#39;条款&#39;]&#43;&#34;，问题：&#34;&#43;train_one[&#39;问题&#39;],&#34;output&#34;:train_one[&#39;答案&#39;]}) for train_one in dev: a.append({&#34;input&#34;:&#34;&#34;&#34;目前有产品名称、相关条款。如果问题与产品名称、相关条款有关系，那么就依照产品名称、相关条款回答问题，如果没有关系直接回答问题。 根据&#34;&#34;&#34;&#43;train_one[&#39;产品名&#39;]&#43;&#34;&#34;&#34;,相关条款&#34;&#34;&#34;&#43;train_one[&#39;条款&#39;]&#43;&#34;，问题：&#34;&#43;train_one[&#39;问题&#39;],&#34;output&#34;:train_one[&#39;答案&#39;]}) json.dump(a,open(&#39;data/a.json&#39;,&#39;w&#39;),ensure_ascii=False) 任务总共数据6000条。
最大长度超过10000。
修改llama factory data下的data_info.json 加入我们的数据集
&#34;a&#34;: { &#34;file_name&#34;: &#34;a.json&#34;, &#34;columns&#34;: { &#34;prompt&#34;: &#34;input&#34;, &#34;response&#34;: &#34;output&#34; } }, 启动云脑任务的时候可以预先选择上我们要进行作业的模型。
配置好这个以后我们就可以启动任务了。在项目启动后我们需要把模型文件拉到本地。
from c2net.context import prepare #初始化导入数据集和预训练模型到容器内 c2net_context = prepare() #获取数据集路径 chat_json_path = c2net_context.dataset_path&#43;&#34;/&#34;&#43;&#34;chat.json&#34; #获取预训练模型路径 qwen1_5_14b_chat_path = c2net_context.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-19T18:20:52+08:00">
    <meta property="article:modified_time" content="2024-07-19T18:20:52+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AFAC2024-基于保险条款的问答 比赛日记 llamafactory qwen npu 910B1</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h4><a id="AFAC2024__0"></a>AFAC2024: 基于保险条款的问答挑战——我的实战日记</h4> 
<h5><a id="_2"></a>概述</h5> 
<p>在最近的AFAC2024竞赛中，我参与了基于保险条款的问答赛道。这是一次深度学习与自然语言处理的实战演练，旨在提升模型在复杂保险文本理解与问答生成方面的能力。本文将分享我的参赛过程，包括数据处理、模型选择、微调策略、实验观察及最终成果。<br> 比赛报名链接 https://tianchi.aliyun.com/competition/entrance/532194/introduction</p> 
<h5><a id="_6"></a>数据与挑战</h5> 
<p>竞赛提供的数据集包含约6000条基于保险条款的问答对。这些数据覆盖了多种保险类型，如人寿保险、财产保险和健康保险，涉及保险条款的解释、索赔流程、覆盖范围等问题。数据集的多样性和专业性构成了此次竞赛的主要挑战。</p> 
<h5><a id="llama_factory__10"></a>llama factory 数据预处理</h5> 
<pre><code class="prism language-python"><span class="token keyword">import</span> json



train <span class="token operator">=</span> json<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"round1_training_data/train.json"</span><span class="token punctuation">,</span><span class="token string">'r'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
dev <span class="token operator">=</span> json<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"round1_training_data/dev.json"</span><span class="token punctuation">,</span><span class="token string">'r'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> train_one <span class="token keyword">in</span> train<span class="token punctuation">:</span>
    a<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"input"</span><span class="token punctuation">:</span><span class="token triple-quoted-string string">"""目前有产品名称、相关条款。如果问题与产品名称、相关条款有关系，那么就依照产品名称、相关条款回答问题，如果没有关系直接回答问题。
              根据"""</span><span class="token operator">+</span>train_one<span class="token punctuation">[</span><span class="token string">'产品名'</span><span class="token punctuation">]</span><span class="token operator">+</span><span class="token triple-quoted-string string">""",相关条款"""</span><span class="token operator">+</span>train_one<span class="token punctuation">[</span><span class="token string">'条款'</span><span class="token punctuation">]</span><span class="token operator">+</span><span class="token string">"，问题："</span><span class="token operator">+</span>train_one<span class="token punctuation">[</span><span class="token string">'问题'</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">"output"</span><span class="token punctuation">:</span>train_one<span class="token punctuation">[</span><span class="token string">'答案'</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> train_one <span class="token keyword">in</span> dev<span class="token punctuation">:</span>
    a<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"input"</span><span class="token punctuation">:</span><span class="token triple-quoted-string string">"""目前有产品名称、相关条款。如果问题与产品名称、相关条款有关系，那么就依照产品名称、相关条款回答问题，如果没有关系直接回答问题。
              根据"""</span><span class="token operator">+</span>train_one<span class="token punctuation">[</span><span class="token string">'产品名'</span><span class="token punctuation">]</span><span class="token operator">+</span><span class="token triple-quoted-string string">""",相关条款"""</span><span class="token operator">+</span>train_one<span class="token punctuation">[</span><span class="token string">'条款'</span><span class="token punctuation">]</span><span class="token operator">+</span><span class="token string">"，问题："</span><span class="token operator">+</span>train_one<span class="token punctuation">[</span><span class="token string">'问题'</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">"output"</span><span class="token punctuation">:</span>train_one<span class="token punctuation">[</span><span class="token string">'答案'</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
json<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'data/a.json'</span><span class="token punctuation">,</span><span class="token string">'w'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>ensure_ascii<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<p>任务总共数据6000条。<br> 最大长度超过10000。<br> 修改llama factory data下的data_info.json 加入我们的数据集</p> 
<pre><code class="prism language-json"> <span class="token string-property property">"a"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"file_name"</span><span class="token operator">:</span> <span class="token string">"a.json"</span><span class="token punctuation">,</span>
    <span class="token string-property property">"columns"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string-property property">"prompt"</span><span class="token operator">:</span> <span class="token string">"input"</span><span class="token punctuation">,</span>
      <span class="token string-property property">"response"</span><span class="token operator">:</span> <span class="token string">"output"</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
</code></pre> 
<p>启动云脑任务的时候可以预先选择上我们要进行作业的模型。<br> <img src="https://images2.imgbox.com/18/52/21DGguAc_o.png" alt="在这里插入图片描述"><br> 配置好这个以后我们就可以启动任务了。在项目启动后我们需要把模型文件拉到本地。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> c2net<span class="token punctuation">.</span>context <span class="token keyword">import</span> prepare

<span class="token comment">#初始化导入数据集和预训练模型到容器内</span>
c2net_context <span class="token operator">=</span> prepare<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment">#获取数据集路径</span>
chat_json_path <span class="token operator">=</span> c2net_context<span class="token punctuation">.</span>dataset_path<span class="token operator">+</span><span class="token string">"/"</span><span class="token operator">+</span><span class="token string">"chat.json"</span>

<span class="token comment">#获取预训练模型路径</span>
qwen1_5_14b_chat_path <span class="token operator">=</span> c2net_context<span class="token punctuation">.</span>pretrain_model_path<span class="token operator">+</span><span class="token string">"/"</span><span class="token operator">+</span><span class="token string">"Qwen1.5-14B-Chat"</span>

<span class="token comment">#输出结果必须保存在该目录</span>
you_should_save_here <span class="token operator">=</span> c2net_context<span class="token punctuation">.</span>output_path


</code></pre> 
<p>如果不选择会浪费更多的时间在下载数据集上。平台不支持访问transformers只能访问国内的modelscope。</p> 
<h5><a id="_64"></a>模型选择与微调</h5> 
<p>为了应对挑战，我选择了Qwen的多个版本作为基础模型。具体来说，我尝试了两种策略：</p> 
<ol><li><strong>LoRA微调</strong>：首先，我使用了qwen2-7b-instruct和qwen1.5-14B-chat模型，通过LoRA（低秩适配）进行微调。LoRA允许在不修改原模型权重的情况下，仅优化少量新增参数，从而有效减少了计算资源需求。<br> lora 微调 qwen2-7b-instruct 在比赛中拿到了489分<br> lora 微调 qwen1.5-14B-chat<br> 超参数 qwen1.5-14B-chat</li></ol> 
<pre><code class="prism language-yaml"><span class="token comment">### model</span>
<span class="token key atrule">model_name_or_path</span><span class="token punctuation">:</span> pretrainmodel/Qwen1.5<span class="token punctuation">-</span>14B<span class="token punctuation">-</span>Chat

<span class="token comment">### method</span>
<span class="token key atrule">stage</span><span class="token punctuation">:</span> sft
<span class="token key atrule">do_train</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
<span class="token key atrule">finetuning_type</span><span class="token punctuation">:</span> lora
<span class="token key atrule">lora_target</span><span class="token punctuation">:</span> all
<span class="token key atrule">deepspeed</span><span class="token punctuation">:</span> examples/deepspeed/ds_z2_config.json

<span class="token comment">### dataset</span>
<span class="token key atrule">dataset</span><span class="token punctuation">:</span> a
<span class="token key atrule">template</span><span class="token punctuation">:</span> qwen
<span class="token key atrule">cutoff_len</span><span class="token punctuation">:</span> <span class="token number">2048</span>
<span class="token key atrule">overwrite_cache</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
<span class="token key atrule">preprocessing_num_workers</span><span class="token punctuation">:</span> <span class="token number">16</span>

<span class="token comment">### output</span>
<span class="token key atrule">output_dir</span><span class="token punctuation">:</span> saves/qwen1.5<span class="token punctuation">-</span>14b/full/sft
<span class="token key atrule">logging_steps</span><span class="token punctuation">:</span> <span class="token number">10</span>
<span class="token key atrule">save_steps</span><span class="token punctuation">:</span> <span class="token number">500</span>
<span class="token key atrule">plot_loss</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
<span class="token key atrule">overwrite_output_dir</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>

<span class="token comment">### train</span>
<span class="token key atrule">per_device_train_batch_size</span><span class="token punctuation">:</span> <span class="token number">1</span>
<span class="token key atrule">gradient_accumulation_steps</span><span class="token punctuation">:</span> <span class="token number">2</span>
<span class="token key atrule">learning_rate</span><span class="token punctuation">:</span> <span class="token number">5.0e-5</span>
<span class="token key atrule">num_train_epochs</span><span class="token punctuation">:</span> <span class="token number">3.0</span>
<span class="token key atrule">lr_scheduler_type</span><span class="token punctuation">:</span> cosine
<span class="token key atrule">warmup_ratio</span><span class="token punctuation">:</span> <span class="token number">0.1</span>
<span class="token key atrule">bf16</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
<span class="token key atrule">ddp_timeout</span><span class="token punctuation">:</span> <span class="token number">180000000</span>

<span class="token comment">### eval</span>
<span class="token key atrule">val_size</span><span class="token punctuation">:</span> <span class="token number">0.1</span>
<span class="token key atrule">per_device_eval_batch_size</span><span class="token punctuation">:</span> <span class="token number">1</span>
<span class="token key atrule">eval_strategy</span><span class="token punctuation">:</span> steps
<span class="token key atrule">eval_steps</span><span class="token punctuation">:</span> <span class="token number">500</span>
</code></pre> 
<p>这里我们选择了使用deepspeed zero2的方式进行微调工作。在910B1 npu上可以使用bf16精度进行模型训练。验证集设置的大小是百分之十的数据作为验证数据集规模。<br> 第一个500步验证精度</p> 
<pre><code class="prism language-python"><span class="token punctuation">{<!-- --></span>
<span class="token string">'eval_loss'</span><span class="token punctuation">:</span> <span class="token number">0.2957316040992737</span><span class="token punctuation">,</span> 
<span class="token string">'eval_accuracy'</span><span class="token punctuation">:</span> <span class="token number">0.910640437309614</span><span class="token punctuation">,</span> 
<span class="token string">'eval_runtime'</span><span class="token punctuation">:</span> <span class="token number">156.6478</span><span class="token punctuation">,</span> 
<span class="token string">'eval_samples_per_second'</span><span class="token punctuation">:</span> <span class="token number">3.83</span><span class="token punctuation">,</span> 
<span class="token string">'eval_steps_per_second'</span><span class="token punctuation">:</span> <span class="token number">3.83</span><span class="token punctuation">,</span> 
<span class="token string">'epoch'</span><span class="token punctuation">:</span> <span class="token number">0.19</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>第二个500步验证精度</p> 
<pre><code class="prism language-python"><span class="token punctuation">{<!-- --></span>
 <span class="token string">'eval_loss'</span><span class="token punctuation">:</span> <span class="token number">0.27959996461868286</span><span class="token punctuation">,</span> 
 <span class="token string">'eval_accuracy'</span><span class="token punctuation">:</span> <span class="token number">0.9174682889249636</span><span class="token punctuation">,</span> 
 <span class="token string">'eval_runtime'</span><span class="token punctuation">:</span> <span class="token number">158.7283</span><span class="token punctuation">,</span> 
 <span class="token string">'eval_samples_per_second'</span><span class="token punctuation">:</span> <span class="token number">3.78</span><span class="token punctuation">,</span> 
 <span class="token string">'eval_steps_per_second'</span><span class="token punctuation">:</span> <span class="token number">3.78</span><span class="token punctuation">,</span> 
 <span class="token string">'epoch'</span><span class="token punctuation">:</span> <span class="token number">0.37</span>
 <span class="token punctuation">}</span>
</code></pre> 
<p>第三个500步验证</p> 
<pre><code class="prism language-python"><span class="token punctuation">{<!-- --></span>
<span class="token string">'eval_loss'</span><span class="token punctuation">:</span> <span class="token number">0.25857865810394287</span><span class="token punctuation">,</span> 
<span class="token string">'eval_accuracy'</span><span class="token punctuation">:</span> <span class="token number">0.9208686929382228</span><span class="token punctuation">,</span> 
<span class="token string">'eval_runtime'</span><span class="token punctuation">:</span> <span class="token number">158.2571</span><span class="token punctuation">,</span> 
<span class="token string">'eval_samples_per_second'</span><span class="token punctuation">:</span> <span class="token number">3.791</span><span class="token punctuation">,</span> 
<span class="token string">'eval_steps_per_second'</span><span class="token punctuation">:</span> <span class="token number">3.791</span><span class="token punctuation">,</span> 
<span class="token string">'epoch'</span><span class="token punctuation">:</span> <span class="token number">0.56</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>llama factory只有在最后训练结束的时候才会把图生成出来，但是我们在openi平台上只有四个小时。所以我们可以在模型输出目录下找到train log文件自己绘图。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token comment"># 将日志数据转换为Pandas DataFrame</span>
<span class="token keyword">import</span> json
log_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
trainer_log <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"sft/trainer_log.jsonl"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> trainer_log_one <span class="token keyword">in</span> trainer_log<span class="token punctuation">:</span>
    trainer_log_data <span class="token operator">=</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>trainer_log_one<span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token string">"loss"</span> <span class="token keyword">in</span> trainer_log_data<span class="token punctuation">:</span>
        log_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>trainer_log_data<span class="token punctuation">)</span>
df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>log_data<span class="token punctuation">)</span>

<span class="token comment"># 设置图表样式</span>
plt<span class="token punctuation">.</span>style<span class="token punctuation">.</span>use<span class="token punctuation">(</span><span class="token string">'ggplot'</span><span class="token punctuation">)</span>

<span class="token comment"># 绘制损失随训练步骤变化的图表</span>
plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>df<span class="token punctuation">[</span><span class="token string">'current_steps'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> df<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training Loss Over Steps'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'Steps'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span><span class="token string">'Training Loss Over Steps'</span><span class="token punctuation">)</span>

<span class="token comment"># 绘制学习率随训练步骤变化的图表</span>
plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>df<span class="token punctuation">[</span><span class="token string">'current_steps'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> df<span class="token punctuation">[</span><span class="token string">'learning_rate'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Learning Rate Over Steps'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'Steps'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'Learning Rate'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span><span class="token string">'Learning Rate Over Steps'</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/33/6b/dnTTH9kD_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e0/7d/O0XJB7dA_o.png" alt="在这里插入图片描述"><br> 验证集相关图表</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token comment"># 将日志数据转换为Pandas DataFrame</span>
<span class="token keyword">import</span> json
log_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
trainer_log <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"sft/trainer_log.jsonl"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> trainer_log_one <span class="token keyword">in</span> trainer_log<span class="token punctuation">:</span>
    trainer_log_data <span class="token operator">=</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>trainer_log_one<span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token string">"eval_loss"</span> <span class="token keyword">in</span> trainer_log_data<span class="token punctuation">:</span>
        log_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>trainer_log_data<span class="token punctuation">)</span>
df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>log_data<span class="token punctuation">)</span>

<span class="token comment"># 设置图表样式</span>
plt<span class="token punctuation">.</span>style<span class="token punctuation">.</span>use<span class="token punctuation">(</span><span class="token string">'ggplot'</span><span class="token punctuation">)</span>

<span class="token comment"># 绘制损失随训练步骤变化的图表</span>
plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>df<span class="token punctuation">[</span><span class="token string">'current_steps'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> df<span class="token punctuation">[</span><span class="token string">'eval_loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'eval Loss Over Steps'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'Steps'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span><span class="token string">'eval Loss Over Steps'</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/e2/86/pApvGFnW_o.png" alt="验证集损失"></p> 
<p>针对所出现的拟合过快的问题，我们提出以下的优化策略。</p> 
<ol><li><strong>降低学习率（<code>learning_rate</code>）</strong>：<br> 学习率是影响模型训练速度和稳定性的重要因素。降低学习率可以让模型在训练过程中更加谨慎地更新权重，从而减慢拟合速度。<pre><code class="prism language-yaml"><span class="token key atrule">learning_rate</span><span class="token punctuation">:</span> <span class="token number">2.0e-5</span>  <span class="token comment"># 从5.0e-5降低到2.0e-5</span>
</code></pre> </li><li><strong>增加warmup步数或比例（<code>warmup_ratio</code>）</strong>：<br> 增加warmup的步数或比例可以让模型在前期以更慢的速度学习，这有助于模型在后期训练中更稳定。<pre><code class="prism language-yaml"><span class="token key atrule">warmup_ratio</span><span class="token punctuation">:</span> <span class="token number">0.2</span>  <span class="token comment"># 从0.1增加到0.2</span>
</code></pre> </li><li><strong>减少梯度累积步数（<code>gradient_accumulation_steps</code>）</strong>：<br> 减少梯度累积步数会增加每次更新权重的间隔，从而使模型的学习速度减慢。<pre><code class="prism language-yaml"><span class="token key atrule">gradient_accumulation_steps</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token comment"># 从2减少到1</span>
</code></pre> </li><li><strong>增加训练批次大小（<code>per_device_train_batch_size</code>）</strong>：<br> 增加批次大小可以提高训练的稳定性，但同时需要相应地调整学习率。<pre><code class="prism language-yaml"><span class="token key atrule">per_device_train_batch_size</span><span class="token punctuation">:</span> <span class="token number">2</span>  <span class="token comment"># 从1增加到2，同时可能需要再次调整学习率</span>
</code></pre> </li><li><strong>调整优化器的动量或重量衰减（如果使用的话）</strong>：<br> 对于使用动量或重量衰减的优化器，调整这些参数可以影响模型的收敛速度。<pre><code class="prism language-yaml"><span class="token comment"># 假设使用的是AdamW优化器</span>
<span class="token key atrule">optimizer</span><span class="token punctuation">:</span>
  <span class="token key atrule">type</span><span class="token punctuation">:</span> AdamW
  <span class="token key atrule">betas</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0.8</span><span class="token punctuation">,</span> <span class="token number">0.999</span><span class="token punctuation">]</span>  <span class="token comment"># 降低动量参数</span>
  <span class="token key atrule">weight_decay</span><span class="token punctuation">:</span> <span class="token number">0.01</span>  <span class="token comment"># 增加重力衰减</span>
</code></pre> </li><li><strong>使用更保守的lr调度器（<code>lr_scheduler_type</code>）</strong>：<br> 选择一个更保守的调度器，如<code>linear</code>或<code>cosine</code>的缓慢下降版本。<pre><code class="prism language-yaml"><span class="token key atrule">lr_scheduler_type</span><span class="token punctuation">:</span> linear  <span class="token comment"># 从cosine改为linear</span>
</code></pre> </li><li><strong>增加正则化</strong>：<br> 增加L1或L2正则化可以防止模型过拟合，从而减慢拟合速度。<pre><code class="prism language-yaml"><span class="token comment"># 增加L2正则化</span>
<span class="token key atrule">optimizer</span><span class="token punctuation">:</span>
  <span class="token key atrule">weight_decay</span><span class="token punctuation">:</span> <span class="token number">0.01</span>  <span class="token comment"># 增加这个值可以增加正则化</span>
</code></pre> </li></ol> 
<p>请记住，调整超参数是一个试验和错误的过程，可能需要多次尝试才能找到最佳的配置。每次调整后，都应该监控模型的性能，以确保它仍然在正确的方向上前进。</p> 
<ol start="2"><li><strong>全参数量微调</strong>：其次，我利用qwen2-7B-instruct模型进行了全参数量微调，以探索模型在充分学习数据集方面的潜力。<br> 最开始我也想全参数量微调qwen1.5 14B chat模型。目前观察的情况是会爆显存。所以暂时搁浅。</li></ol> 
<p>合并模型部分<br> 通过对训练过程的观察，发现在2.5k步的时候验证损失是最低的。所以采用2.5k步的模型作为此次验证最优模型。这里我在第一个4小时结束训练后启动第二次四小时训练的开始选择模型合并操作。模型合并操作执行了将近半个小时。</p> 
<pre><code class="prism language-yaml"><span class="token comment">### Note: DO NOT use quantized model or quantization_bit when merging lora adapters</span>

<span class="token comment">### model</span>
<span class="token key atrule">model_name_or_path</span><span class="token punctuation">:</span> pretrainmodel/Qwen1.5<span class="token punctuation">-</span>14B<span class="token punctuation">-</span>Chat
<span class="token key atrule">adapter_name_or_path</span><span class="token punctuation">:</span> saves/qwen1.5<span class="token punctuation">-</span>14b/full/sft/checkpoint<span class="token punctuation">-</span><span class="token number">2500</span>
<span class="token key atrule">template</span><span class="token punctuation">:</span> qwen
<span class="token key atrule">finetuning_type</span><span class="token punctuation">:</span> lora

<span class="token comment">### export</span>
<span class="token comment">##export_dir: /home/songzhijun/work/Langchain-Chatchat/longbao1</span>
<span class="token key atrule">export_dir</span><span class="token punctuation">:</span> models/Qwen1.5<span class="token punctuation">-</span>14B<span class="token punctuation">-</span>match
<span class="token key atrule">export_size</span><span class="token punctuation">:</span> <span class="token number">2</span>
<span class="token key atrule">export_device</span><span class="token punctuation">:</span> cpu
<span class="token key atrule">export_legacy_format</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>
</code></pre> 
<p>生成答案部分<br> 因为在openi平台中启动api接口后无法本地调用。所以这里我选择了使用huggingface transformers原生的办法进行生成提交数据的操作。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer

model_name <span class="token operator">=</span> <span class="token string">"models/Qwen1.5-14B-match"</span>
device <span class="token operator">=</span> <span class="token string">"npu"</span> <span class="token comment"># the device to load the model onto</span>

model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_name<span class="token punctuation">,</span>
    torch_dtype<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span>
    device_map<span class="token operator">=</span><span class="token string">"auto"</span>
<span class="token punctuation">)</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span>

data_test <span class="token operator">=</span> json<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"round1_training_data/test.json"</span><span class="token punctuation">,</span><span class="token string">'r'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

outfile <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"tianyan_result.jsonl"</span><span class="token punctuation">,</span><span class="token string">'w'</span><span class="token punctuation">,</span>encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> data_test_one <span class="token keyword">in</span> data_test<span class="token punctuation">:</span>
        messages <span class="token operator">=</span> <span class="token punctuation">[</span>
                            <span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"根据条款"</span><span class="token operator">+</span>data_test_one<span class="token punctuation">[</span><span class="token string">'条款'</span><span class="token punctuation">]</span><span class="token operator">+</span><span class="token string">"回答问题"</span><span class="token operator">+</span>data_test_one<span class="token punctuation">[</span><span class="token string">'问题'</span><span class="token punctuation">]</span><span class="token punctuation">}</span>

        <span class="token punctuation">]</span>
        text <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>apply_chat_template<span class="token punctuation">(</span>
            messages<span class="token punctuation">,</span>
            tokenize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
            add_generation_prompt<span class="token operator">=</span><span class="token boolean">True</span>
        <span class="token punctuation">)</span>
        model_inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token punctuation">[</span>text<span class="token punctuation">]</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

        generated_ids <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>
            <span class="token operator">**</span>model_inputs<span class="token punctuation">,</span>
            max_new_tokens<span class="token operator">=</span><span class="token number">512</span>
        <span class="token punctuation">)</span>
        generated_ids <span class="token operator">=</span> <span class="token punctuation">[</span>
            output_ids<span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token keyword">for</span> input_ids<span class="token punctuation">,</span> output_ids <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>model_inputs<span class="token punctuation">.</span>input_ids<span class="token punctuation">,</span> generated_ids<span class="token punctuation">)</span>
        <span class="token punctuation">]</span>

        response <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>batch_decode<span class="token punctuation">(</span>generated_ids<span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

        <span class="token comment"># data = response.json()</span>
        outfile<span class="token punctuation">.</span>write<span class="token punctuation">(</span>json<span class="token punctuation">.</span>dumps<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"ID"</span><span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">(</span>data_test_one<span class="token punctuation">[</span><span class="token string">'ID'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"question"</span><span class="token punctuation">:</span> data_test_one<span class="token punctuation">[</span><span class="token string">'问题'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"answer"</span><span class="token punctuation">:</span> response<span class="token punctuation">}</span><span class="token punctuation">,</span> ensure_ascii<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"\n"</span><span class="token punctuation">)</span>
 
</code></pre> 
<h5><a id="_333"></a>计算资源</h5> 
<p>实验是在华为910B1 GPU上进行的，配备了64GB显存。这一配置足以支持大型语言模型的高效训练和微调。<br> 我做采用的资源来自openi平台。每天启动云脑任务会给10积分。相当于两个半小时的910B1 64GB版本计算资源。<br> 您的好友正在邀请您加入OpenI启智AI协作平台，畅享充沛的普惠算力资源(GPU/NPU/GCU/GPGPU/DCU/MLU)。<br> 注册地址：https://openi.pcl.ac.cn/user/sign_up?sharedUser=nlp_future_01<br> 推荐人：nlp_future_01</p> 
<h5><a id="_340"></a>实验观察</h5> 
<p>在初步实验中，我发现模型在较短时间内便达到了较高的训练集准确率，显示出了快速拟合的趋势。这可能是由于数据集的大小相对于模型容量而言较小，导致过拟合现象。<br> 7b lora 微调后<br> score:489.9103<br> 14B lora 微调后<br> score:592.4397</p> 
<h5><a id="_347"></a>参数调节与时间限制</h5> 
<p>为解决过拟合问题，我开始调整学习率、批次大小和正则化参数。此外，我还增加了Dropout比例，以增强模型的泛化能力。然而，openi平台的时间限制（每轮实验仅限4小时）为模型训练和验证带来了额外的挑战。我不得不精心设计实验计划，确保在有限时间内完成尽可能多的有效迭代。</p> 
<h5><a id="_351"></a>结论</h5> 
<p>尽管面临时间限制和技术难题，这次竞赛经历极大地丰富了我的知识库，特别是在处理特定领域文本和优化模型训练流程方面。未来，我计划继续探索更高级的微调技术和模型架构，以提高模型在保险条款问答任务上的表现。</p> 
<hr> 
<p>通过本次竞赛，我深刻体会到理论与实践结合的重要性，以及在有限资源下优化模型性能的挑战。希望我的经验能为同样热衷于自然语言处理领域的研究者和工程师们提供有价值的参考。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/975f2dd00949b4f6cd00cf825b4fafc7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">jupyter学习笔记</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a30f956c2d6bfc699b35dae15c6f6363/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C语言相关知识点（不定期更新内容）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>