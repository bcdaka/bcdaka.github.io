<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Llama模型结构解析（源码阅读） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/4b79fbcc9f3d4a52015108f0c91fd676/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Llama模型结构解析（源码阅读）">
  <meta property="og:description" content="目录 1. LlamaModel整体结构流程图2. LlamaRMSNorm3. LlamaMLP4. LlamaRotaryEmbedding 参考资料：
https://zhuanlan.zhihu.com/p/636784644
https://spaces.ac.cn/archives/8265 ——《Transformer升级之路：2、博采众长的旋转式位置编码》 前言：本次阅读代码位置，在transformers库底下的modeling_llama.py，具体位置在：transformers/models/llama/modeling_llama.py，如下图所示：
1. LlamaModel整体结构流程图 2. LlamaRMSNorm 代码如下 class LlamaRMSNorm(nn.Module): def __init__(self, hidden_size, eps=1e-6): &#34;&#34;&#34; LlamaRMSNorm is equivalent to T5LayerNorm &#34;&#34;&#34; super().__init__() self.weight = nn.Parameter(torch.ones(hidden_size)) self.variance_epsilon = eps def forward(self, hidden_states): input_dtype = hidden_states.dtype variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True) hidden_states = hidden_states * torch.rsqrt(variance &#43; self.variance_epsilon) return (self.weight * hidden_states).to(input_dtype) RMSNorm的公式如下所示：
x i 1 n ∑ i = 1 n x i 2 &#43; e p s ∗ w e i g h t i \frac{x_i}{\sqrt{\frac{1}{n}\sum\limits_{i=1}^{n}{x_i}^2 &#43; eps}} * weight_i n1​i=1∑n​xi​2&#43;eps ​xi​​∗weighti​">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-08-29T10:10:12+08:00">
    <meta property="article:modified_time" content="2023-08-29T10:10:12+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Llama模型结构解析（源码阅读）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-dracula">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#1_LlamaModel_10" rel="nofollow">1. LlamaModel整体结构流程图</a></li><li><a href="#2_LlamaRMSNorm_13" rel="nofollow">2. LlamaRMSNorm</a></li><li><a href="#3_LlamaMLP_41" rel="nofollow">3. LlamaMLP</a></li><li><a href="#4_LlamaRotaryEmbedding_68" rel="nofollow">4. LlamaRotaryEmbedding</a></li></ul> 
</div> 
<p></p> 
<ul><li>参考资料：<br> https://zhuanlan.zhihu.com/p/636784644<br> https://spaces.ac.cn/archives/8265 ——《Transformer升级之路：2、博采众长的旋转式位置编码》</li></ul> 
<p><strong>前言</strong>：本次阅读代码位置，在transformers库底下的modeling_llama.py，具体位置在：transformers/models/llama/modeling_llama.py，如下图所示：<img src="https://images2.imgbox.com/49/84/A7xIvYbh_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="1_LlamaModel_10"></a>1. LlamaModel整体结构流程图</h2> 
<p><img src="https://images2.imgbox.com/d1/b0/SAJOGWTn_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="2_LlamaRMSNorm_13"></a>2. LlamaRMSNorm</h2> 
<ul><li>代码如下</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">LlamaRMSNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-6</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        LlamaRMSNorm is equivalent to T5LayerNorm
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>variance_epsilon <span class="token operator">=</span> eps

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_states<span class="token punctuation">)</span><span class="token punctuation">:</span>
        input_dtype <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>dtype
        variance <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        hidden_states <span class="token operator">=</span> hidden_states <span class="token operator">*</span> torch<span class="token punctuation">.</span>rsqrt<span class="token punctuation">(</span>variance <span class="token operator">+</span> self<span class="token punctuation">.</span>variance_epsilon<span class="token punctuation">)</span>

        <span class="token keyword">return</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight <span class="token operator">*</span> hidden_states<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>input_dtype<span class="token punctuation">)</span>
</code></pre> 
<ul><li> <p>RMSNorm的公式如下所示：<br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
          
           
           
             x 
            
           
             i 
            
           
           
            
             
             
               1 
              
             
               n 
              
             
             
             
               ∑ 
              
              
              
                i 
               
              
                = 
               
              
                1 
               
              
             
               n 
              
             
             
              
              
                x 
               
              
                i 
               
              
             
               2 
              
             
            
              + 
             
            
              e 
             
            
              p 
             
            
              s 
             
            
           
          
         
           ∗ 
          
         
           w 
          
         
           e 
          
         
           i 
          
         
           g 
          
         
           h 
          
          
          
            t 
           
          
            i 
           
          
         
        
          \frac{x_i}{\sqrt{\frac{1}{n}\sum\limits_{i=1}^{n}{x_i}^2 + eps}} * weight_i 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 2.3411em; vertical-align: -1.6296em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.7115em;"><span class="" style="top: -2.19em;"><span class="pstrut" style="height: 3.0805em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.5436em;"><span class="svg-align" style="top: -4.5714em;"><span class="pstrut" style="height: 4.5714em;"></span><span class="mord mtight" style="padding-left: 1.4286em;"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8443em;"><span class="" style="top: -2.656em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="" style="top: -3.2255em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line mtight" style="border-bottom-width: 0.049em;"></span></span><span class="" style="top: -3.384em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.344em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span><span class="mspace mtight" style="margin-right: 0.1952em;"></span><span class="mop op-limits mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.4005em;"><span class="" style="top: -1.8629em; margin-left: 0em;"><span class="pstrut" style="height: 2.75em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -2.75em;"><span class="pstrut" style="height: 2.75em;"></span><span class=""><span class="mop op-symbol small-op mtight">∑</span></span></span><span class="" style="top: -3.7em; margin-left: 0em;"><span class="pstrut" style="height: 2.75em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.0301em;"><span class=""></span></span></span></span></span><span class="mspace mtight" style="margin-right: 0.1952em;"></span><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3281em;"><span class="" style="top: -2.357em; margin-left: 0em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7463em;"><span class="" style="top: -2.786em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">s</span></span></span><span class="" style="top: -3.5156em;"><span class="pstrut" style="height: 4.5714em;"></span><span class="hide-tail mtight" style="min-width: 1.02em; height: 2.6857em;"> 
                     <svg width="400em" height="2.6857em" viewbox="0 0 400000 1944" preserveaspectratio="xMinYMin slice"> 
                      <path d="M983 90
l0 -0
c4,-6.7,10,-10,18,-10 H400000v40
H1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7
s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744
c-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30
c26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722
c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5
c53.7,-170.3,84.5,-266.8,92.5,-289.5z
M1001 80h400000v40h-400000z"></path> 
                     </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.0559em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -3.3105em;"><span class="pstrut" style="height: 3.0805em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.4906em;"><span class="pstrut" style="height: 3.0805em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3281em;"><span class="" style="top: -2.357em; margin-left: 0em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.6296em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="mord mathnormal">e</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right: 0.0359em;">g</span><span class="mord mathnormal">h</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span></p> 
  <ul><li>其中，公式与代码的对应关系如下：<br> <img src="https://images2.imgbox.com/4f/97/xw8RuHSl_o.png" alt="在这里插入图片描述"></li></ul> </li></ul> 
<h2><a id="3_LlamaMLP_41"></a>3. LlamaMLP</h2> 
<ul><li>代码如下：</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">LlamaMLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        hidden_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
        intermediate_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span>
        hidden_act<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>gate_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> intermediate_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>down_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>intermediate_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>up_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> intermediate_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>act_fn <span class="token operator">=</span> ACT2FN<span class="token punctuation">[</span>hidden_act<span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>down_proj<span class="token punctuation">(</span>self<span class="token punctuation">.</span>act_fn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>gate_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>up_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li> <p>流程图：<br> <img src="https://images2.imgbox.com/bc/12/JgKaLSkS_o.png" alt="在这里插入图片描述"></p> </li><li> <p>其中输入为x，输出为y</p> </li><li> <p>代码中intermediate_size一般比hidden_size大，我们通过在jupyter notebook中打印Llama-13B的模型，可以看到如下所示：<br> <img src="https://images2.imgbox.com/65/46/dvpG3KHh_o.png" alt="在这里插入图片描述"></p> </li><li> <p>总结：MLP模块就是几个nn.Linear的组合</p> </li></ul> 
<h2><a id="4_LlamaRotaryEmbedding_68"></a>4. LlamaRotaryEmbedding</h2> 
<ul><li>代码如下</li></ul> 
<pre><code class="prism language-python">
<span class="token keyword">class</span> <span class="token class-name">LlamaRotaryEmbedding</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> max_position_embeddings<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> base<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        inv_freq <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span>base <span class="token operator">**</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> dim<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token operator">/</span> dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"inv_freq"</span><span class="token punctuation">,</span> inv_freq<span class="token punctuation">)</span>

        <span class="token comment"># Build here to make `torch.jit.trace` work.</span>
        self<span class="token punctuation">.</span>max_seq_len_cached <span class="token operator">=</span> max_position_embeddings
        t <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>self<span class="token punctuation">.</span>max_seq_len_cached<span class="token punctuation">,</span> device<span class="token operator">=</span>self<span class="token punctuation">.</span>inv_freq<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>self<span class="token punctuation">.</span>inv_freq<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        freqs <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"i,j-&gt;ij"</span><span class="token punctuation">,</span> t<span class="token punctuation">,</span> self<span class="token punctuation">.</span>inv_freq<span class="token punctuation">)</span>
        <span class="token comment"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span>
        emb <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>freqs<span class="token punctuation">,</span> freqs<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"cos_cached"</span><span class="token punctuation">,</span> emb<span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"sin_cached"</span><span class="token punctuation">,</span> emb<span class="token punctuation">.</span>sin<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> seq_len<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># x: [bs, num_attention_heads, seq_len, head_size]</span>
        <span class="token comment"># This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.</span>
        <span class="token keyword">if</span> seq_len <span class="token operator">&gt;</span> self<span class="token punctuation">.</span>max_seq_len_cached<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>max_seq_len_cached <span class="token operator">=</span> seq_len
            t <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>self<span class="token punctuation">.</span>max_seq_len_cached<span class="token punctuation">,</span> device<span class="token operator">=</span>x<span class="token punctuation">.</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>self<span class="token punctuation">.</span>inv_freq<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
            freqs <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"i,j-&gt;ij"</span><span class="token punctuation">,</span> t<span class="token punctuation">,</span> self<span class="token punctuation">.</span>inv_freq<span class="token punctuation">)</span>
            <span class="token comment"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span>
            emb <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>freqs<span class="token punctuation">,</span> freqs<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>x<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"cos_cached"</span><span class="token punctuation">,</span> emb<span class="token punctuation">.</span>cos<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"sin_cached"</span><span class="token punctuation">,</span> emb<span class="token punctuation">.</span>sin<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>cos_cached<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>seq_len<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>sin_cached<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>seq_len<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
</code></pre> 
<ul><li>具体的使用，还调用了另外两个函数，如下所示：</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">rotate_half</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Rotates half the hidden dims of the input."""</span>
    x1 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">]</span>
    x2 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> <span class="token number">2</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span>x2<span class="token punctuation">,</span> x1<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">apply_rotary_pos_emb</span><span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> cos<span class="token punctuation">,</span> sin<span class="token punctuation">,</span> position_ids<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># The first two dimensions of cos and sin are always 1, so we can `squeeze` them.</span>
    cos <span class="token operator">=</span> cos<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># [seq_len, dim]</span>
    sin <span class="token operator">=</span> sin<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># [seq_len, dim]</span>
    cos <span class="token operator">=</span> cos<span class="token punctuation">[</span>position_ids<span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [bs, 1, seq_len, dim]</span>
    sin <span class="token operator">=</span> sin<span class="token punctuation">[</span>position_ids<span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [bs, 1, seq_len, dim]</span>
    q_embed <span class="token operator">=</span> <span class="token punctuation">(</span>q <span class="token operator">*</span> cos<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>rotate_half<span class="token punctuation">(</span>q<span class="token punctuation">)</span> <span class="token operator">*</span> sin<span class="token punctuation">)</span>
    k_embed <span class="token operator">=</span> <span class="token punctuation">(</span>k <span class="token operator">*</span> cos<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>rotate_half<span class="token punctuation">(</span>k<span class="token punctuation">)</span> <span class="token operator">*</span> sin<span class="token punctuation">)</span>
    <span class="token keyword">return</span> q_embed<span class="token punctuation">,</span> k_embed
    
</code></pre> 
<ul><li> <p>注意这里的实现跟原始推导有点区别，这里实现的方式如下图所示：<br> <img src="https://images2.imgbox.com/66/0b/X81cSCeC_o.png" alt="在这里插入图片描述"></p> </li><li> <p>原始推导如下图所示：<br> <img src="https://images2.imgbox.com/f8/14/RlVQbOKN_o.png" alt="在这里插入图片描述"><br> 具体可以查看作者的博客：👉<a href="https://spaces.ac.cn/archives/8265" rel="nofollow">戳我</a>👈</p> </li><li> <p>总结：RoPE就是在attention计算时，K跟Q做内积之前，先给各自注入位置信息。</p> </li></ul> 
<p><strong>结束。</strong></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/310021add1c7ca187d72fa14a403a197/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">99个基于JAVA的管理系统（源码&#43;论文）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bd3e9d492826bd97998d263e743cc82f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">JDK下载安装</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>