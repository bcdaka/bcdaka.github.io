<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【PyTorch单点知识】自动求导机制的原理与实践 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/59f761eccd35cd7df1b991775d4cb8d4/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【PyTorch单点知识】自动求导机制的原理与实践">
  <meta property="og:description" content="文章目录 0. 前言1. 自动求导的基本原理2. PyTorch中的自动求导2.1 创建计算图2.2 反向传播2.3 反向传播详解2.4 梯度清零2.5 定制自动求导 3. 代码实例：线性回归的自动求导4. 结论 0. 前言 按照国际惯例，首先声明：本文只是我自己学习的理解，虽然参考了他人的宝贵见解及成果，但是内容可能存在不准确的地方。如果发现文中错误，希望批评指正，共同进步。
在深度学习中，自动求导（Automatic Differentiation, AD）是一项至关重要的技术，它使我们能够高效地计算神经网络的梯度，进而通过反向传播算法更新权重。
PyTorch作为一款动态计算图的深度学习框架，以其灵活性和易用性著称，其自动求导机制是其实现高效、灵活训练的核心。本文将深入探讨PyTorch中的自动求导机制，从原理到实践，通过代码示例来展示其工作流程。
如果对计算图不太了解，可以参考我的往期文章：基于TorchViz详解计算图（附代码）
1. 自动求导的基本原理 自动求导是一种数学方法，用于计算函数的导数。与数值微分相比，自动求导能够提供精确的导数计算结果，同时避免了符号微分中可能出现的手动求导错误。在深度学习中，我们通常关注的是反向模式backward的自动求导，即从输出向输入方向传播梯度的过程。
反向模式自动求导基于链式法则，它允许我们将复杂的复合函数的导数分解成多个简单函数的导数的乘积。在神经网络中，每一层都可以看作是一个简单的函数，通过链式法则，我们可以从前向传播的输出开始，逆向计算每个参数的梯度。
2. PyTorch中的自动求导 PyTorch通过其autograd模块实现了自动求导机制。autograd记录了所有的计算步骤，创建了一个计算图（Computational Graph），并在需要时执行反向传播，计算梯度。
2.1 创建计算图 在PyTorch中，当一个张量（Tensor）的requires_grad=True时，任何对该张量的操作都会被记录在计算图中。例如：
import torch x = torch.ones(2, 2, requires_grad=True) y = x &#43; 2 z = y * y * 3 out = z.mean() print(y.grad_fn) # 查看y的计算节点 print(z.grad_fn) # 查看z的计算节点 输出为：
&lt;AddBackward0 object at 0x000001CADEC6AB60&gt; &lt;MulBackward0 object at 0x000001CADEC6AB60&gt; 在上述代码中，z的计算节点显示了z是如何由y计算得来的，而y的计算节点则显示了y是如何由x计算得来的。这样就形成了一个计算图。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-09T11:19:29+08:00">
    <meta property="article:modified_time" content="2024-07-09T11:19:29+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【PyTorch单点知识】自动求导机制的原理与实践</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><ul><li><a href="#0__2" rel="nofollow">0. 前言</a></li><li><a href="#1__11" rel="nofollow">1. 自动求导的基本原理</a></li><li><a href="#2_PyTorch_17" rel="nofollow">2. PyTorch中的自动求导</a></li><li><ul><li><a href="#21__21" rel="nofollow">2.1 创建计算图</a></li><li><a href="#22__45" rel="nofollow">2.2 反向传播</a></li><li><a href="#23__67" rel="nofollow">2.3 反向传播详解</a></li><li><a href="#24__151" rel="nofollow">2.4 梯度清零</a></li><li><a href="#25__155" rel="nofollow">2.5 定制自动求导</a></li></ul> 
    </li><li><a href="#3__159" rel="nofollow">3. 代码实例：线性回归的自动求导</a></li><li><a href="#4__231" rel="nofollow">4. 结论</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h4><a id="0__2"></a>0. 前言</h4> 
<blockquote> 
 <p>按照国际惯例，首先声明：本文只是我自己学习的理解，虽然参考了他人的宝贵见解及成果，但是内容可能存在不准确的地方。如果发现文中错误，希望批评指正，共同进步。</p> 
</blockquote> 
<p>在深度学习中，自动求导（Automatic Differentiation, AD）是一项至关重要的技术，它使我们能够高效地计算神经网络的梯度，进而通过反向传播算法更新权重。</p> 
<p>PyTorch作为一款动态计算图的深度学习框架，以其灵活性和易用性著称，其自动求导机制是其实现高效、灵活训练的核心。本文将深入探讨PyTorch中的自动求导机制，从原理到实践，通过代码示例来展示其工作流程。</p> 
<blockquote> 
 <p>如果对计算图不太了解，可以参考我的往期文章：<a href="https://cypher.blog.csdn.net/article/details/132136218?spm=1001.2014.3001.5502" rel="nofollow">基于TorchViz详解计算图（附代码）</a></p> 
</blockquote> 
<h4><a id="1__11"></a>1. 自动求导的基本原理</h4> 
<p>自动求导是一种数学方法，用于计算函数的导数。与数值微分相比，自动求导能够提供精确的导数计算结果，同时避免了符号微分中可能出现的手动求导错误。在深度学习中，我们通常关注的是反向模式<code>backward</code>的自动求导，即从输出向输入方向传播梯度的过程。</p> 
<p>反向模式自动求导基于<strong>链式法则</strong>，它允许我们将复杂的复合函数的导数分解成多个简单函数的导数的乘积。在神经网络中，每一层都可以看作是一个简单的函数，通过链式法则，我们可以从前向传播的输出开始，逆向计算每个参数的梯度。</p> 
<h4><a id="2_PyTorch_17"></a>2. PyTorch中的自动求导</h4> 
<p>PyTorch通过其<code>autograd</code>模块实现了自动求导机制。<code>autograd</code>记录了所有的计算步骤，创建了一个计算图（Computational Graph），并在需要时执行反向传播，计算梯度。</p> 
<h5><a id="21__21"></a>2.1 创建计算图</h5> 
<p>在PyTorch中，当一个张量（Tensor）的<code>requires_grad=True</code>时，任何对该张量的操作都会被记录在计算图中。例如：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch

x <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">2</span>
z <span class="token operator">=</span> y <span class="token operator">*</span> y <span class="token operator">*</span> <span class="token number">3</span>
out <span class="token operator">=</span> z<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span>  <span class="token comment"># 查看y的计算节点</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span>  <span class="token comment"># 查看z的计算节点</span>
</code></pre> 
<p>输出为：</p> 
<pre><code class="prism language-python"><span class="token operator">&lt;</span>AddBackward0 <span class="token builtin">object</span> at <span class="token number">0x000001CADEC6AB60</span><span class="token operator">&gt;</span>
<span class="token operator">&lt;</span>MulBackward0 <span class="token builtin">object</span> at <span class="token number">0x000001CADEC6AB60</span><span class="token operator">&gt;</span>
</code></pre> 
<p>在上述代码中，<code>z</code>的计算节点显示了<code>z</code>是如何由<code>y</code>计算得来的，而<code>y</code>的计算节点则显示了<code>y</code>是如何由<code>x</code>计算得来的。这样就形成了一个计算图。</p> 
<h5><a id="22__45"></a>2.2 反向传播</h5> 
<p>一旦我们完成了前向传播并得到了最终的输出，就可以调用<code>out.backward()</code>来进行反向传播，计算梯度。例如：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch

x <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">2</span>
z <span class="token operator">=</span> y <span class="token operator">*</span> y <span class="token operator">*</span> <span class="token number">3</span>
out <span class="token operator">=</span> z<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>

out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
</code></pre> 
<p>这里，<code>x.grad</code>就是<code>out</code>相对于<code>x</code>的梯度。输出为：</p> 
<pre><code class="prism language-python">tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">4.5000</span><span class="token punctuation">,</span> <span class="token number">4.5000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">4.5000</span><span class="token punctuation">,</span> <span class="token number">4.5000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="23__67"></a>2.3 反向传播详解</h5> 
<p>下面我们来详细分析下1.2节的具体计算过程：</p> 
<ol><li>首先，创建了一个2x2的张量<code>x</code>，其值全为1，并且设置了<code>requires_grad=True</code>，这意味着PyTorch将会追踪这个张量上的所有操作，以便能够计算梯度。</li></ol> 
<pre><code class="prism language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="2"><li>然后，将<code>x</code>与2相加得到<code>y</code>：</li></ol> 
<pre><code class="prism language-python">y <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">2</span>
</code></pre> 
<p>此时<code>y</code>的值为：</p> 
<pre><code>tensor([[3., 3.],
        [3., 3.]])
</code></pre> 
<ol start="3"><li>接下来，将<code>y</code>的每个元素平方再乘以3得到<code>z</code>：</li></ol> 
<pre><code class="prism language-python">z <span class="token operator">=</span> y <span class="token operator">*</span> y <span class="token operator">*</span> <span class="token number">3</span>
</code></pre> 
<p>此时<code>z</code>的值为：</p> 
<pre><code>tensor([[27., 27.],
        [27., 27.]])
</code></pre> 
<ol start="4"><li>计算<code>z</code>的平均值作为输出<code>out</code>：</li></ol> 
<pre><code class="prism language-python">out <span class="token operator">=</span> z<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>此时<code>out</code>的值为：</p> 
<pre><code>tensor(27.)
</code></pre> 
<ol start="5"><li>使用<code>backward()</code>函数对<code>out</code>进行反向传播，计算梯度：</li></ol> 
<pre><code class="prism language-python">out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="6"><li>最后，打印<code>x</code>的梯度：</li></ol> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
</code></pre> 
<p>由于<code>out</code>是通过一系列操作从<code>x</code>得到的，我们可以根据链式法则计算出<code>x</code>的梯度。具体来说，<code>out</code>相对于<code>x</code>的梯度可以通过以下步骤计算得出：</p> 
<ul><li><code>out</code>相对于<code>z</code>的梯度是<code>1/z.size(0)</code>（因为<code>z.mean()</code>是对<code>z</code>的所有元素取平均），这里<code>z.size(0)</code>等于4，所以<code>out</code>相对于<code>z</code>的梯度是<code>1/4</code>。</li><li><code>z</code>相对于<code>y</code>的梯度是<code>y * 3 * 2</code>（因为<code>z = y^2 * 3</code>，所以<code>dz/dy = 2*y*3</code>）。</li><li><code>y</code>相对于<code>x</code>的梯度是<code>1</code>（因为<code>y = x + 2</code>，所以<code>dy/dx = 1</code>）。</li></ul> 
<p>综合以上，<code>out</code>相对于<code>x</code>的梯度是：</p> 
<pre><code>1/4 * (y * 3 * 2) * 1
</code></pre> 
<p>由于<code>y</code>的值为<code>[[3, 3], [3, 3]]</code>，那么上述梯度计算结果为：</p> 
<pre><code>1/4 * (3 * 3 * 2) * 1 = 9/2 = 4.5
</code></pre> 
<p>因此，最终<code>x.grad</code>的值为：</p> 
<pre><code>tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
</code></pre> 
<h5><a id="24__151"></a>2.4 梯度清零</h5> 
<p>在多次迭代中，梯度会累积在张量中，因此在每次迭代开始之前，我们需要调用<code>optimizer.zero_grad()</code>来清零梯度，防止梯度累积。（PyTorch为了训练方便，会默认梯度累积）</p> 
<h5><a id="25__155"></a>2.5 定制自动求导</h5> 
<p>PyTorch还允许我们定义自己的自动求导函数，通过继承<code>torch.autograd.Function</code>类并重写<code>forward</code>和<code>backward</code>方法。这为实现更复杂的计算提供了可能。</p> 
<h4><a id="3__159"></a>3. 代码实例：线性回归的自动求导</h4> 
<p>接下来，我们将通过一个简单的线性回归问题，演示PyTorch自动求导机制的实际应用。</p> 
<p>假设我们有一组数据点，我们想找到一条直线(y = wx + b)，使得这条直线尽可能接近这些数据点。我们的目标是最小化损失函数（例如均方误差）。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment"># 准备数据</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
Y <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token number">3</span> <span class="token operator">*</span> X <span class="token operator">+</span> <span class="token number">0.1</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

X <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>Y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 初始化权重和偏置</span>
w <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 定义模型和损失函数</span>
<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> w <span class="token operator">*</span> x <span class="token operator">+</span> b

loss_fn <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 训练循环</span>
learning_rate <span class="token operator">=</span> <span class="token number">0.01</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 前向传播</span>
    y_pred <span class="token operator">=</span> forward<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    
    <span class="token comment"># 计算损失</span>
    loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span> Y<span class="token punctuation">)</span>
    
    <span class="token comment"># 反向传播</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 更新权重</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        w <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> w<span class="token punctuation">.</span>grad
        b <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> b<span class="token punctuation">.</span>grad
        
        <span class="token comment"># 清零梯度</span>
        w<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
        b<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch [</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/1000], Loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Final weights:'</span><span class="token punctuation">,</span> w<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'bias:'</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">Epoch <span class="token punctuation">[</span><span class="token number">100</span><span class="token operator">/</span><span class="token number">1000</span><span class="token punctuation">]</span><span class="token punctuation">,</span> Loss<span class="token punctuation">:</span> <span class="token number">0.1273</span>
Epoch <span class="token punctuation">[</span><span class="token number">200</span><span class="token operator">/</span><span class="token number">1000</span><span class="token punctuation">]</span><span class="token punctuation">,</span> Loss<span class="token punctuation">:</span> <span class="token number">0.0782</span>
Epoch <span class="token punctuation">[</span><span class="token number">300</span><span class="token operator">/</span><span class="token number">1000</span><span class="token punctuation">]</span><span class="token punctuation">,</span> Loss<span class="token punctuation">:</span> <span class="token number">0.0620</span>
Epoch <span class="token punctuation">[</span><span class="token number">400</span><span class="token operator">/</span><span class="token number">1000</span><span class="token punctuation">]</span><span class="token punctuation">,</span> Loss<span class="token punctuation">:</span> <span class="token number">0.0497</span>
Epoch <span class="token punctuation">[</span><span class="token number">500</span><span class="token operator">/</span><span class="token number">1000</span><span class="token punctuation">]</span><span class="token punctuation">,</span> Loss<span class="token punctuation">:</span> <span class="token number">0.0404</span>
Epoch <span class="token punctuation">[</span><span class="token number">600</span><span class="token operator">/</span><span class="token number">1000</span><span class="token punctuation">]</span><span class="token punctuation">,</span> Loss<span class="token punctuation">:</span> <span class="token number">0.0332</span>
Epoch <span class="token punctuation">[</span><span class="token number">700</span><span class="token operator">/</span><span class="token number">1000</span><span class="token punctuation">]</span><span class="token punctuation">,</span> Loss<span class="token punctuation">:</span> <span class="token number">0.0277</span>
Epoch <span class="token punctuation">[</span><span class="token number">800</span><span class="token operator">/</span><span class="token number">1000</span><span class="token punctuation">]</span><span class="token punctuation">,</span> Loss<span class="token punctuation">:</span> <span class="token number">0.0235</span>
Epoch <span class="token punctuation">[</span><span class="token number">900</span><span class="token operator">/</span><span class="token number">1000</span><span class="token punctuation">]</span><span class="token punctuation">,</span> Loss<span class="token punctuation">:</span> <span class="token number">0.0203</span>
Epoch <span class="token punctuation">[</span><span class="token number">1000</span><span class="token operator">/</span><span class="token number">1000</span><span class="token punctuation">]</span><span class="token punctuation">,</span> Loss<span class="token punctuation">:</span> <span class="token number">0.0179</span>
Final weights<span class="token punctuation">:</span> <span class="token number">2.68684983253479</span> bias<span class="token punctuation">:</span> <span class="token number">2.17771577835083</span>
</code></pre> 
<p>在这个例子中，我们首先准备了一些随机生成的数据，然后初始化了权重<code>w</code>和偏置<code>b</code>。在训练循环中，我们通过前向传播计算预测值，使用均方误差损失函数计算损失，然后通过调用<code>loss.backward()</code>进行反向传播，最后更新权重和偏置。通过多次迭代，我们最终找到了使损失最小化的权重和偏置。</p> 
<h4><a id="4__231"></a>4. 结论</h4> 
<p>PyTorch的自动求导机制是其强大功能的关键所在。通过<code>autograd</code>模块，PyTorch能够自动跟踪计算图并高效地计算梯度，这大大简化了深度学习模型的开发过程。本文通过理论解释和代码示例，深入探讨了PyTorch中的自动求导机制，希望读者能够从中获得对这一重要概念的深刻理解，并在实际项目中灵活运用。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/666979c82e8d6472d12c3492119d10c4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Kafka发送对象消息</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1a47df23b32cb81b9a6f2fccb248ffdc/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">「Pytorch」CopyPaste 数据增强</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>