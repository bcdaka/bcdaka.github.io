<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【AI落地应用实战】DAMODEL深度学习平台部署&#43;本地调用ChatGLM-6B解决方案 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/6c8daedfaf901ad8d8ea5844fbe2a837/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【AI落地应用实战】DAMODEL深度学习平台部署&#43;本地调用ChatGLM-6B解决方案">
  <meta property="og:description" content="ChatGLM-6B是由清华大学和智谱AI开源的一款对话语言模型，基于 General Language Model (GLM)架构，具有 62亿参数。该模型凭借其强大的语言理解和生成能力、轻量级的参数量以及开源的特性，已经成为在学术界和工业界引起了广泛关注。
本篇将介绍使用DAMODEL深度学习平台部署ChatGLM-6B模型，然后通过Web API的形式使用本地代码调用服务端的模型进行对话。
一、DAMODEL-ChatGLM-6B服务端部署 DAMODEL（丹摩智算）是专为AI打造的智算云，致力于提供丰富的算力资源与基础设施助力AI应用的开发、训练、部署。目前给新用户提供了近100小时的免费4090算力可供使用。
1.1、实例创建 首先进入DAMODEL控制台，点击资源-GPU云实例，点击创建实例：
进入创建页面后，首先在实例配置中首先选择付费类型为按量付费，其次选择单卡启动，然后选择需求的GPU型号，本次实验可以选择选择：按量付费–GPU数量1–NVIDIA-GeForc-RTX-4090，该配置为60GB内存，24GB的显存。
继续往下翻，配置数据硬盘的大小，每个实例默认附带了50GB的数据硬盘，本次创建可以就选择默认大小50GB，可以看到，平台提供了一些基础镜像供快速启动，镜像中安装了对应的基础环境和框架，这里选择PyTorch1.13.1的框架启动，也可以选择PyTorch2.1.2版本启动。
点击创建密钥对，输入自定义的名称，创建好密钥对后，选择刚刚创建好的密钥对，并点击立即创建，等待一段时间后即可启动成功！
1.2、模型准备 启动环境后，打开终端，用git 克隆https://github.com/THUDM/ChatGLM-6B.git项目，若遇到github连接超时，可以选择离线下载并上传到项目中。
cd /home/aistudio/work/ git clone https://github.com/THUDM/ChatGLM-6B.git 成功克隆项目后，会显示如下文件夹：
其次cd进入该文件夹，使用pip安装项目依赖：pip install -r requirements.txt，可以看到DAMODEL平台环境依赖的下载安装速度可以达到18MB/s以上，非常迅速，等待片刻显示如下Successfully installed则说明依赖安装完成！
依赖安装成功后，我们需要引入模型文件，比较方便的是，DAMODEL（丹摩智算）提供了数据上传功能，用户有20GB免费存储空间，该空间被挂载到实例的/root/shared-storage目录，跨实例共享。
这里首先点击文件存储，点击上传文件。
然后下载Hugging Face上的ChatGLM-6B预训练模型，也可以进入魔塔社区选择https://www.modelscope.cn/models/ZhipuAI/chatglm3-6b/files里的所有文件进行下载
然后将下载下来的模型文件及配置进行上传并解压，上传时尽量保持该界面首页显示，等待上传成功后再进行其他操作。
1.3、模型启动 上传好预训练模型及解压后，我们就可以去启动python脚本运行了，ChatGLM-6B提供了cli_demo.py和web_demo.py两个文件来启动模型，第一个是使用命令行进行交互，第二个是使用本机服务器进行网页交互。
由于要使用本地模型启动，所以我们需要把从Hugging Face Hub加载改为本地路径加载，打开cli_demo.py文件，将这两行代码改为从本地文件目录加载预训练模型。
然后在终端输入python cli_demo就可以成功启动模型了，在cli_demo.py中，main函数启动一个无限循环，等待用户输入。用户可以输入文本与模型进行对话，或输入&#34;clear&#34;清空对话历史并清除屏幕，或输入&#34;stop&#34;退出程序。对于正常的对话输入。
启动模型后的效果如下图所示：
也可以在终端输入python web_demo.py，通过Web界面与模型进行交互。
不过由于Jupyter的限制，无法直接打开访问服务器的127.0.0.1:7860网页端交互界面，这里可以利用MobaXterm建立ssh隧道，实现远程端口到本机端口的转发。首先打开tunneling，新建SSH通道，填入ssh的相关配置，并将7860通道内容转发到本机，点击start开始转发：
转发成功后，就可以成功在网页与模型进行交互了，效果如下：
二、通过Web API实现本地使用 2.1、启动服务 通过以上步骤，我们成功启动了ChatGLM-6B模型，并可以在服务器的JupyterLab中进行对话，下面我们将其部署为API服务，然后在本地进行使用。
同样ChatGLM-6B模型为我们提供了api.py文件，它实现了一个基于FastAPI框架API服务，其接收一个HTTP POST请求，该请求体包含文本生成所需的参数，如prompt（提示文本）、history（对话历史）、max_length（生成文本的最大长度）、top_p（采样时的累积概率阈值）和temperature（采样时的温度参数，影响生成文本的随机性）。在接收到请求后，该服务使用预训练的模型和分词器（tokenizer）来生成一个响应文本，并更新对话历史。随后生成的响应文本、更新后的对话历史、状态码（固定为200）和当前时间戳被打包成一个JSON对象并返回给客户端。
我们可以直接在终端cd进入ChatGLM-6B文件夹，运行api.py文件：
cd ChatGLM-6B python api.py 显示如下提示则说明启动成功！
2.2、开放端口 其次需要为本地访问开放对应的端口，首先需要点击访问控制，进入端口开放页面。
然后点击添加端口，输入端口号，并点击确定开放：
点击确认开放端口后，平台会给出访问链接，将其复制以便后续使用
做完以上这些步骤后，服务器端就已经部署好了，接下来测试本地调用效果!
2.3、使用PostMan测试功能 打开PostMan，新建一个Post，将刚刚复制的网址粘贴到url栏，然后在Body中填入相应的内容。
Body示例内容如下：
{ &#34;prompt&#34;: &#34;你好，你是谁？&#34;, &#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-31T15:14:20+08:00">
    <meta property="article:modified_time" content="2024-07-31T15:14:20+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【AI落地应用实战】DAMODEL深度学习平台部署&#43;本地调用ChatGLM-6B解决方案</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>ChatGLM-6B是由清华大学和智谱AI开源的一款对话语言模型，基于 General Language Model (GLM)架构，具有 62亿参数。该模型凭借其强大的语言理解和生成能力、轻量级的参数量以及开源的特性，已经成为在学术界和工业界引起了广泛关注。</p> 
<p>本篇将介绍使用DAMODEL深度学习平台部署ChatGLM-6B模型，然后通过Web API的形式使用本地代码调用服务端的模型进行对话。</p> 
<h2><a id="DAMODELChatGLM6B_4"></a>一、DAMODEL-ChatGLM-6B服务端部署</h2> 
<p>DAMODEL（丹摩智算）是专为AI打造的智算云，致力于提供丰富的算力资源与基础设施助力AI应用的开发、训练、部署。目前给新用户提供了近100小时的免费4090算力可供使用。</p> 
<h3><a id="11_6"></a>1.1、实例创建</h3> 
<p>首先进入<a href="https://damodel.com/register?source=1D5686A0" rel="nofollow">DAMODEL控制台</a>，点击资源-GPU云实例，点击创建实例：<br> <img src="https://images2.imgbox.com/27/5f/hLgzn90M_o.png" alt="在这里插入图片描述"><br> 进入创建页面后，首先在实例配置中首先选择付费类型为按量付费，其次选择单卡启动，然后选择需求的GPU型号，本次实验可以选择选择：按量付费–GPU数量1–NVIDIA-GeForc-RTX-4090，该配置为60GB内存，24GB的显存。<br> <img src="https://images2.imgbox.com/00/6e/3UbT1EeP_o.png" alt="在这里插入图片描述"></p> 
<p>继续往下翻，配置数据硬盘的大小，每个实例默认附带了50GB的数据硬盘，本次创建可以就选择默认大小50GB，可以看到，平台提供了一些基础镜像供快速启动，镜像中安装了对应的基础环境和框架，这里选择PyTorch1.13.1的框架启动，也可以选择PyTorch2.1.2版本启动。<br> <img src="https://images2.imgbox.com/16/7e/WUScPpFj_o.png" alt="在这里插入图片描述"></p> 
<p>点击创建密钥对，输入自定义的名称，创建好密钥对后，选择刚刚创建好的密钥对，并点击立即创建，等待一段时间后即可启动成功！<br> <img src="https://images2.imgbox.com/d8/de/1TQeN8Wf_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="12_17"></a>1.2、模型准备</h3> 
<p>启动环境后，打开终端，用git 克隆https://github.com/THUDM/ChatGLM-6B.git项目，若遇到github连接超时，可以选择离线下载并上传到项目中。</p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">cd</span> /home/aistudio/work/
<span class="token function">git</span> clone https://github.com/THUDM/ChatGLM-6B.git
</code></pre> 
<p>成功克隆项目后，会显示如下文件夹：<br> <img src="https://images2.imgbox.com/29/98/mxUHCzkb_o.png" alt="在这里插入图片描述"><br> 其次cd进入该文件夹，使用pip安装项目依赖：<code>pip install -r requirements.txt</code>，可以看到DAMODEL平台环境依赖的下载安装速度可以达到18MB/s以上，非常迅速，等待片刻显示如下Successfully installed则说明依赖安装完成！<br> <img src="https://images2.imgbox.com/1c/d1/JvTLhrHf_o.png" alt="在这里插入图片描述"><br> 依赖安装成功后，我们需要引入模型文件，比较方便的是，DAMODEL（丹摩智算）提供了数据上传功能，用户有20GB免费存储空间，该空间被挂载到实例的/root/shared-storage目录，跨实例共享。</p> 
<p>这里首先点击文件存储，点击上传文件。<br> <img src="https://images2.imgbox.com/b3/25/Xk5HgPPr_o.png" alt="在这里插入图片描述"><br> 然后下载Hugging Face上的ChatGLM-6B预训练模型，也可以进入魔塔社区选择https://www.modelscope.cn/models/ZhipuAI/chatglm3-6b/files里的所有文件进行下载<br> <img src="https://images2.imgbox.com/d8/e0/jbFbCT5H_o.png" alt="在这里插入图片描述"><br> 然后将下载下来的模型文件及配置进行上传并解压，上传时尽量保持该界面首页显示，等待上传成功后再进行其他操作。<br> <img src="https://images2.imgbox.com/32/c9/UjrguZe4_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="13_36"></a>1.3、模型启动</h3> 
<p>上传好预训练模型及解压后，我们就可以去启动python脚本运行了，ChatGLM-6B提供了cli_demo.py和web_demo.py两个文件来启动模型，第一个是使用命令行进行交互，第二个是使用本机服务器进行网页交互。<br> 由于要使用本地模型启动，所以我们需要把从Hugging Face Hub加载改为本地路径加载，打开cli_demo.py文件，将这两行代码改为从本地文件目录加载预训练模型。<br> <img src="https://images2.imgbox.com/dc/36/rZXxXr7D_o.png" alt="在这里插入图片描述"><br> 然后在终端输入python cli_demo就可以成功启动模型了，在cli_demo.py中，main函数启动一个无限循环，等待用户输入。用户可以输入文本与模型进行对话，或输入"clear"清空对话历史并清除屏幕，或输入"stop"退出程序。对于正常的对话输入。</p> 
<p>启动模型后的效果如下图所示：<br> <img src="https://images2.imgbox.com/ab/d2/xQrkwobZ_o.png" alt="在这里插入图片描述"><br> 也可以在终端输入python web_demo.py，通过Web界面与模型进行交互。<br> <img src="https://images2.imgbox.com/9a/26/PZbgruzV_o.png" alt="在这里插入图片描述"><br> 不过由于Jupyter的限制，无法直接打开访问服务器的127.0.0.1:7860网页端交互界面，这里可以利用MobaXterm建立ssh隧道，实现远程端口到本机端口的转发。首先打开tunneling，新建SSH通道，填入ssh的相关配置，并将7860通道内容转发到本机，点击start开始转发：<br> <img src="https://images2.imgbox.com/5d/18/IidbHLpG_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/93/1f/tu6jmXgC_o.png" alt="在这里插入图片描述"></p> 
<p>转发成功后，就可以成功在网页与模型进行交互了，效果如下：</p> 
<p><img src="https://images2.imgbox.com/98/fa/tpcDj9Z3_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="Web_API_54"></a>二、通过Web API实现本地使用</h2> 
<h3><a id="21_55"></a>2.1、启动服务</h3> 
<p>通过以上步骤，我们成功启动了ChatGLM-6B模型，并可以在服务器的JupyterLab中进行对话，下面我们将其部署为API服务，然后在本地进行使用。</p> 
<p>同样ChatGLM-6B模型为我们提供了api.py文件，它实现了一个基于FastAPI框架API服务，其接收一个HTTP POST请求，该请求体包含文本生成所需的参数，如prompt（提示文本）、history（对话历史）、max_length（生成文本的最大长度）、top_p（采样时的累积概率阈值）和temperature（采样时的温度参数，影响生成文本的随机性）。在接收到请求后，该服务使用预训练的模型和分词器（tokenizer）来生成一个响应文本，并更新对话历史。随后生成的响应文本、更新后的对话历史、状态码（固定为200）和当前时间戳被打包成一个JSON对象并返回给客户端。</p> 
<p>我们可以直接在终端cd进入ChatGLM-6B文件夹，运行api.py文件：</p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">cd</span> ChatGLM-6B
python api.py
</code></pre> 
<p>显示如下提示则说明启动成功！<br> <img src="https://images2.imgbox.com/f6/4c/n9VIz5tz_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="22_68"></a>2.2、开放端口</h3> 
<p>其次需要为本地访问开放对应的端口，首先需要点击访问控制，进入端口开放页面。<br> <img src="https://images2.imgbox.com/0f/a3/HEqVcuWd_o.png" alt="在这里插入图片描述"><br> 然后点击添加端口，输入端口号，并点击确定开放：<br> <img src="https://images2.imgbox.com/99/b2/Jc1Eue7n_o.png" alt="在这里插入图片描述"><br> 点击确认开放端口后，平台会给出访问链接，将其复制以便后续使用<br> <img src="https://images2.imgbox.com/26/c9/J7NVf1Xc_o.png" alt="在这里插入图片描述"><br> 做完以上这些步骤后，服务器端就已经部署好了，接下来测试本地调用效果!</p> 
<h3><a id="23PostMan_76"></a>2.3、使用PostMan测试功能</h3> 
<p>打开PostMan，新建一个Post，将刚刚复制的网址粘贴到url栏，然后在Body中填入相应的内容。<br> <img src="https://images2.imgbox.com/28/c7/9jBfTZQd_o.png" alt="在这里插入图片描述"><br> Body示例内容如下：</p> 
<pre><code class="prism language-json"><span class="token punctuation">{<!-- --></span>  
  <span class="token string-property property">"prompt"</span><span class="token operator">:</span> <span class="token string">"你好，你是谁？"</span><span class="token punctuation">,</span>  
  <span class="token string-property property">"max_length"</span><span class="token operator">:</span> <span class="token number">512</span><span class="token punctuation">,</span>  
  <span class="token string-property property">"top_p"</span><span class="token operator">:</span> <span class="token number">0.9</span><span class="token punctuation">,</span>  
  <span class="token string-property property">"temperature"</span><span class="token operator">:</span> <span class="token number">0.7</span>  
<span class="token punctuation">}</span>
</code></pre> 
<p>点击send后，显示以下response则说明成功！<br> <img src="https://images2.imgbox.com/f0/0d/RgKDHmIp_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="24_91"></a>2.4、本地代码使用功能</h3> 
<p>测试完成后，下面开始转到本地开发，以下是一个基础的单轮对话功能示例代码：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> requests  
<span class="token keyword">import</span> json  

<span class="token comment"># API的URL  </span>
<span class="token comment"># 即刚刚复制的访问链接</span>
api_url <span class="token operator">=</span> <span class="token string">"http://cqbiq6nhri0c73eq3cv0-8000.agent.damodel.com"</span>  
   
data <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>  
    <span class="token string">"prompt"</span><span class="token punctuation">:</span> <span class="token string">"你好，你是谁?"</span><span class="token punctuation">,</span>  
    <span class="token string">"max_length"</span><span class="token punctuation">:</span> <span class="token number">500</span><span class="token punctuation">,</span>  
    <span class="token string">"top_p"</span><span class="token punctuation">:</span> <span class="token number">0.9</span><span class="token punctuation">,</span>  
    <span class="token string">"temperature"</span><span class="token punctuation">:</span> <span class="token number">1.0</span>  
<span class="token punctuation">}</span>  

<span class="token comment"># 发送POST请求   </span>
response <span class="token operator">=</span> requests<span class="token punctuation">.</span>post<span class="token punctuation">(</span>api_url<span class="token punctuation">,</span> json<span class="token operator">=</span>data<span class="token punctuation">)</span>  

<span class="token comment"># 检查响应状态码   </span>
<span class="token keyword">if</span> response<span class="token punctuation">.</span>status_code <span class="token operator">==</span> <span class="token number">200</span><span class="token punctuation">:</span>  
    result <span class="token operator">=</span> response<span class="token punctuation">.</span>json<span class="token punctuation">(</span><span class="token punctuation">)</span>  
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Response:"</span><span class="token punctuation">,</span> result<span class="token punctuation">[</span><span class="token string">'response'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Updated History:"</span><span class="token punctuation">,</span> result<span class="token punctuation">[</span><span class="token string">'history'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Time:"</span><span class="token punctuation">,</span> result<span class="token punctuation">[</span><span class="token string">'time'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  
<span class="token keyword">else</span><span class="token punctuation">:</span>  
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Failed to get response from the API. Status code:"</span><span class="token punctuation">,</span> response<span class="token punctuation">.</span>status_code<span class="token punctuation">)</span>  
    <span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span>
</code></pre> 
<p>在此基础上，我们可以实现一个基于ChatGLM-6B模型的简单对话系统，在本地通过命令行与DAMODEL部署好的模型进行交互。对于正常的对话输入，程序将用户的输入作为prompt，连同当前的对话历史记录conversation_history、最大生成长度max_length、top_p和temperature等参数一起发送到指定的API URL。然后，它检查API的响应状态码。如果状态码为200，表示请求成功，程序将打印出API返回的响应内容和更新后的对话历史记录，并更新conversation_history列表。</p> 
<p>以下是一个多轮对话的示例代码：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> requests  
<span class="token keyword">import</span> json  
  
api_url <span class="token operator">=</span> <span class="token string">"http://cqbiq6nhri0c73eq3cv0-8000.agent.damodel.com"</span>  

conversation_history <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"欢迎使用 ChatGLM-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序"</span><span class="token punctuation">)</span>

<span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
    query <span class="token operator">=</span> <span class="token builtin">input</span><span class="token punctuation">(</span><span class="token string">"\n用户："</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> query<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">"stop"</span><span class="token punctuation">:</span>
        <span class="token keyword">break</span>
    <span class="token keyword">if</span> query<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">"clear"</span><span class="token punctuation">:</span>
        history <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        os<span class="token punctuation">.</span>system<span class="token punctuation">(</span>clear_command<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"欢迎使用 ChatGLM-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序"</span><span class="token punctuation">)</span>
        <span class="token keyword">continue</span>
    prompt <span class="token operator">=</span> query
    data <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>  
        <span class="token string">"prompt"</span><span class="token punctuation">:</span> prompt<span class="token punctuation">,</span>  
        <span class="token string">"history"</span><span class="token punctuation">:</span> conversation_history<span class="token punctuation">,</span>  
        <span class="token string">"max_length"</span><span class="token punctuation">:</span> <span class="token number">5000</span><span class="token punctuation">,</span>  
        <span class="token string">"top_p"</span><span class="token punctuation">:</span> <span class="token number">0.9</span><span class="token punctuation">,</span>  
        <span class="token string">"temperature"</span><span class="token punctuation">:</span> <span class="token number">0.9</span>  
    <span class="token punctuation">}</span>
    response <span class="token operator">=</span> requests<span class="token punctuation">.</span>post<span class="token punctuation">(</span>api_url<span class="token punctuation">,</span> json<span class="token operator">=</span>data<span class="token punctuation">)</span>
    <span class="token keyword">if</span> response<span class="token punctuation">.</span>status_code <span class="token operator">==</span> <span class="token number">200</span><span class="token punctuation">:</span>  
        result <span class="token operator">=</span> response<span class="token punctuation">.</span>json<span class="token punctuation">(</span><span class="token punctuation">)</span>  
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Response:"</span><span class="token punctuation">,</span> result<span class="token punctuation">[</span><span class="token string">'response'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Updated History:"</span><span class="token punctuation">,</span> result<span class="token punctuation">[</span><span class="token string">'history'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  
          
        conversation_history <span class="token operator">=</span> result<span class="token punctuation">[</span><span class="token string">'history'</span><span class="token punctuation">]</span>
          
    <span class="token keyword">else</span><span class="token punctuation">:</span>  
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Failed to get response from the API. Status code:"</span><span class="token punctuation">,</span> response<span class="token punctuation">.</span>status_code<span class="token punctuation">)</span>  
        <span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span>  
</code></pre> 
<p>运行的效果如下：</p> 
<p><img src="https://images2.imgbox.com/97/1d/qT9uD6xk_o.png" alt="在这里插入图片描述"><br> 除了常规的4090显卡和P40显卡，DAMODEL（丹摩智算）上线了H800系列显卡，基于最新Ampere架构，7纳米制程，80GB显存，拥有10240个CUDA核心和320个Tensor核心强势登陆。</p> 
<pre><code class="prism language-python"><span class="token comment"># 体验链接：</span>
https<span class="token punctuation">:</span><span class="token operator">//</span>damodel<span class="token punctuation">.</span>com<span class="token operator">/</span>register?source<span class="token operator">=</span>1D5686A0
<span class="token comment"># 代金券：</span>
damodelkele
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/74c554a692e29625ca6992e985b0b281/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">仅需三步，在 Mac 电脑部署本地大模型，打造私人 ChatGPT</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/98c19910c1eccc4336f7bac717fd40cb/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">不要怕，手把手带你做好小程序关键词优化</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>