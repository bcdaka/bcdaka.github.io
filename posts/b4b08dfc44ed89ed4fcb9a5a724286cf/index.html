<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于Boosting的四种机器算法的简单运用 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/b4b08dfc44ed89ed4fcb9a5a724286cf/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="基于Boosting的四种机器算法的简单运用">
  <meta property="og:description" content="Boosting是一种集成学习算法，其核心思想是通过组合多个弱学习器来构建一个强学习器。本文包含 AdaBoost、Gradient Boosting Decision Tree（GBDT）、XGBoost、LightGBM 四种机器学习算法，主要给出示例代码以及模型参数，旨在快速的上手相应算法。
目录
一、 AdaBoost
1、AdaBoostClassifier（分类）
2、AdaBoostRegressor（回归）
二、GBDT
1、GradientBoostingClassifier（分类）
2、GradientBoostingRegressor（回归）
三、XGBoost
1、XGBClassifier（分类）
2、XGBRegressor（回归）
四、LightGBM 1、LGBMClassifier（分类）
2、LGBMRegressor （回归）
总结 一、 AdaBoost AdaBoost（Adaptive Boosting，自适应增强）算法是一种集成学习技术，通过结合多个弱分类器来构建一个强分类器。
AdaBoost 的核心思想是通过修正数据的权重来训练一系列的弱学习器,由这些弱学习器的预测结果进行加权求和, 得到我们最终的预测结果。
1、AdaBoostClassifier（分类） 官方链接：AdaBoostClassifier — scikit-learn 1.5.1 documentation
class sklearn.ensemble.AdaBoostClassifier
示例代码：
from sklearn.ensemble import AdaBoostClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report # 加载数据集 iris = load_iris() X, y = iris.data, iris.target # 划分训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-13T14:53:52+08:00">
    <meta property="article:modified_time" content="2024-08-13T14:53:52+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于Boosting的四种机器算法的简单运用</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>        <strong>Boosting</strong>是一种集成学习算法，其核心思想是通过组合多个弱学习器来构建一个强学习器。本文包含 <strong>AdaBoost</strong>、<strong>Gradient Boosting Decision Tree（GBDT）</strong>、<strong>XGBoost</strong>、<strong>LightGBM </strong>四种机器学习算法，主要给出示例代码以及模型参数，旨在快速的上手相应算法。</p> 
<hr> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%C2%A0AdaBoost-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%C2%A0AdaBoost" rel="nofollow">一、 AdaBoost</a></p> 
<p id="1%E3%80%81AdaBoostClassifier%EF%BC%88%E5%88%86%E7%B1%BB%EF%BC%89-toc" style="margin-left:40px;"><a href="#1%E3%80%81AdaBoostClassifier%EF%BC%88%E5%88%86%E7%B1%BB%EF%BC%89" rel="nofollow">1、AdaBoostClassifier（分类）</a></p> 
<p id="2%E3%80%81AdaBoostRegressor%EF%BC%88%E5%9B%9E%E5%BD%92%EF%BC%89-toc" style="margin-left:40px;"><a href="#2%E3%80%81AdaBoostRegressor%EF%BC%88%E5%9B%9E%E5%BD%92%EF%BC%89" rel="nofollow">2、AdaBoostRegressor（回归）</a></p> 
<p id="%E4%BA%8C%E3%80%81GBDT-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81GBDT" rel="nofollow">二、GBDT</a></p> 
<p id="1%E3%80%81GradientBoostingClassifier%EF%BC%88%E5%88%86%E7%B1%BB%EF%BC%89-toc" style="margin-left:40px;"><a href="#1%E3%80%81GradientBoostingClassifier%EF%BC%88%E5%88%86%E7%B1%BB%EF%BC%89" rel="nofollow">1、GradientBoostingClassifier（分类）</a></p> 
<p id="2%E3%80%81GradientBoostingRegressor%EF%BC%88%E5%9B%9E%E5%BD%92%EF%BC%89-toc" style="margin-left:40px;"><a href="#2%E3%80%81GradientBoostingRegressor%EF%BC%88%E5%9B%9E%E5%BD%92%EF%BC%89" rel="nofollow">2、GradientBoostingRegressor（回归）</a></p> 
<p id="%E4%B8%89%E3%80%81XGBoost-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81XGBoost" rel="nofollow">三、XGBoost</a></p> 
<p id="1%E3%80%81XGBClassifier%EF%BC%88%E5%88%86%E7%B1%BB%EF%BC%89-toc" style="margin-left:40px;"><a href="#1%E3%80%81XGBClassifier%EF%BC%88%E5%88%86%E7%B1%BB%EF%BC%89" rel="nofollow">1、XGBClassifier（分类）</a></p> 
<p id="2%E3%80%81XGBRegressor%EF%BC%88%E5%9B%9E%E5%BD%92%EF%BC%89-toc" style="margin-left:40px;"><a href="#2%E3%80%81XGBRegressor%EF%BC%88%E5%9B%9E%E5%BD%92%EF%BC%89" rel="nofollow">2、XGBRegressor（回归）</a></p> 
<p id="%E5%9B%9B%E3%80%81LightGBM%C2%A0-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81LightGBM%C2%A0" rel="nofollow">四、LightGBM </a></p> 
<p id="1%E3%80%81LGBMClassifier%EF%BC%88%E5%88%86%E7%B1%BB%EF%BC%89-toc" style="margin-left:40px;"><a href="#1%E3%80%81LGBMClassifier%EF%BC%88%E5%88%86%E7%B1%BB%EF%BC%89" rel="nofollow">1、LGBMClassifier（分类）</a></p> 
<p id="2%E3%80%81LGBMRegressor%20%EF%BC%88%E5%9B%9E%E5%BD%92%EF%BC%89-toc" style="margin-left:40px;"><a href="#2%E3%80%81LGBMRegressor%20%EF%BC%88%E5%9B%9E%E5%BD%92%EF%BC%89" rel="nofollow">2、LGBMRegressor （回归）</a></p> 
<p id="%E6%80%BB%E7%BB%93%C2%A0-toc" style="margin-left:0px;"><a href="#%E6%80%BB%E7%BB%93%C2%A0" rel="nofollow">总结 </a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E4%B8%80%E3%80%81%C2%A0AdaBoost" style="background-color:transparent;">一、 AdaBoost</h2> 
<p>        <strong>AdaBoost</strong>（<strong>Adaptive Boosting，自适应增强</strong>）算法是一种集成学习技术，通过结合多个弱分类器来构建一个强分类器。</p> 
<p>        AdaBoost 的核心思想是通过修正数据的权重来训练一系列的弱学习器,由这些弱学习器的预测结果进行加权求和, 得到我们最终的预测结果。</p> 
<h3 id="1%E3%80%81AdaBoostClassifier%EF%BC%88%E5%88%86%E7%B1%BB%EF%BC%89" style="background-color:transparent;">1、AdaBoostClassifier（分类）</h3> 
<p>官方链接：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html" rel="nofollow" title="AdaBoostClassifier — scikit-learn 1.5.1 documentation">AdaBoostClassifier — scikit-learn 1.5.1 documentation</a></p> 
<blockquote> 
 <p><em>class </em>sklearn.ensemble.<span style="color:#fe2c24;"><em>AdaBoostClassifier</em></span></p> 
</blockquote> 
<p><em>示例代码：</em></p> 
<pre><code class="language-python">from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# 加载数据集
iris = load_iris()
X, y = iris.data, iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 初始化AdaBoost分类器，使用决策树作为基分类器
clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50,
                         learning_rate=1.0,algorithm='SAMME',random_state=66)

# 训练模型
clf.fit(X_train, y_train)

# 进行预测
y_pred = clf.predict(X_test)

# 打印准确率
print(f"Test Accuracy: {clf.score(X_test, y_test)}\n")

# 打印分类报告
print("classification report:")
print(classification_report(y_test, y_pred))</code></pre> 
<p><em> output：</em></p> 
<p class="img-center"><img alt="" height="205" src="https://images2.imgbox.com/21/74/oXr1RewI_o.png" width="399"></p> 
<p></p> 
<h3 id="2%E3%80%81AdaBoostRegressor%EF%BC%88%E5%9B%9E%E5%BD%92%EF%BC%89" style="background-color:transparent;">2、AdaBoostRegressor（回归）</h3> 
<p>官方链接：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html" rel="nofollow" title="AdaBoostRegressor — scikit-learn 1.5.1 documentation">AdaBoostRegressor — scikit-learn 1.5.1 documentation</a></p> 
<blockquote> 
 <p><em>class </em>sklearn.ensemble.<span style="color:#ff9900;"><em>AdaBoostRegressor</em></span></p> 
</blockquote> 
<p><em>示例代码：</em></p> 
<pre><code class="language-python">import numpy as np
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor

# 准备数据
rng = np.random.RandomState(66)
num = 100
X = np.linspace(0, 6, num)[:, np.newaxis]
y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])

# 初始化模型
regr = AdaBoostRegressor(
    DecisionTreeRegressor(max_depth=4), n_estimators=300, random_state=rng
)
# 训练模型
regr.fit(X, y)
# 进行预测
y_pred = regr.predict(X)

# 打印准确度
print(f"Accuracy: {regr.score(X, y):.4f}")

# output:
# Accuracy: 0.9723</code></pre> 
<p><em>可视化：</em></p> 
<pre><code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns

colors = sns.color_palette("colorblind")

plt.figure()
plt.scatter(X, y, color=colors[0], label="samples")
plt.plot(X, y, color=colors[1], label="Regressor", linewidth=3)
plt.xlabel("data")
plt.ylabel("target")
plt.title("AdaBoostRegressor")
plt.legend()
plt.show()</code></pre> 
<p class="img-center"><img alt="" height="288" src="https://images2.imgbox.com/18/cb/Cs3r8sPx_o.png" width="399"></p> 
<p></p> 
<h2 id="%E4%BA%8C%E3%80%81GBDT" style="background-color:transparent;">二、GBDT</h2> 
<p>        <strong>GBDT</strong>（<strong>Gradient Boosting Decision Tree</strong>，<strong>梯度提升决策树</strong>）是一种集成学习算法，它通过构造一组弱的学习树，逐步添加决策树来最小化一个可导的损失函数，每棵树都在前一棵树的残差上进行训练，并把多颗决策树的结果累加起来作为最终的预测输出，它既能用于分类问题也可以用于回归问题。</p> 
<h3 id="1%E3%80%81GradientBoostingClassifier%EF%BC%88%E5%88%86%E7%B1%BB%EF%BC%89" style="background-color:transparent;">1、GradientBoostingClassifier（分类）</h3> 
<p>官方链接：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html" rel="nofollow" title="GradientBoostingClassifier — scikit-learn 1.5.1 documentation">GradientBoostingClassifier — scikit-learn 1.5.1 documentation</a></p> 
<blockquote> 
 <p><em>class </em>sklearn.ensemble.<span style="color:#fe2c24;"><em>GradientBoostingClassifier</em></span></p> 
</blockquote> 
<p><em> 示例代码：</em></p> 
<pre><code class="language-python">from sklearn.ensemble import GradientBoostingClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# 加载数据集
iris = load_iris()
X, y = iris.data, iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 模型参数
params = {
    "n_estimators": 100,       # 弱学习器的数量
    "max_depth": 3,            # 树的最大深度
    "learning_rate": 0.1,     # 学习率
    "random_state": 66         # 随机状态，确保结果可复现
}

# 初始化 GradientBoostingClassifier
gb_clf = GradientBoostingClassifier(**params)

# 训练模型
gb_clf.fit(X_train, y_train)

# 进行预测
y_pred = gb_clf.predict(X_test)

# 评估模型的准确度
# 打印分类报告
print("classification report:")
print(classification_report(y_test, y_pred))

# 打印特征重要性
feature_importances = gb_clf.feature_importances_
print("Feature importances:")
for feature, importance in zip(iris.feature_names, feature_importances):
    print(f"\t{feature}: {importance:.4f}")</code></pre> 
<p><em>output： </em></p> 
<p class="img-center"><img alt="" height="257" src="https://images2.imgbox.com/84/fc/ScivVj0w_o.png" width="399"></p> 
<p></p> 
<h3 id="2%E3%80%81GradientBoostingRegressor%EF%BC%88%E5%9B%9E%E5%BD%92%EF%BC%89" style="background-color:transparent;">2、GradientBoostingRegressor（回归）</h3> 
<p>官方链接：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html" rel="nofollow" title="GradientBoostingRegressor — scikit-learn 1.5.1 documentation">GradientBoostingRegressor — scikit-learn 1.5.1 documentation</a></p> 
<blockquote> 
 <p><em>class </em>sklearn.ensemble.<span style="color:#ff9900;"><em>GradientBoostingRegressor</em></span></p> 
</blockquote> 
<p> <em>示例代码：</em></p> 
<pre><code class="language-python">from sklearn.ensemble import GradientBoostingRegressor
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 生成合成回归数据
X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 模型参数
params = {
    'n_estimators': 100,        # 弱学习器的数量
    'learning_rate': 0.1,       # 学习率
    'max_depth': 3,             # 树的最大深度
    'random_state': 42,         # 随机状态，确保结果可复现
    'loss': 'squared_error'     # 使用均方误差作为损失函数
    }

# 初始化 GradientBoostingRegressor
gbr = GradientBoostingRegressor(**params)

# 训练模型
gbr.fit(X_train, y_train)

# 对测试集进行预测
y_pred = gbr.predict(X_test)

# 计算测试集的均方误差（MSE）
mse = mean_squared_error(y_test, y_pred)
print(f"Test Mean Squared Error: {mse:.2f}")

# 计算测试集的确定系数（R^2）
r2_score = gbr.score(X_test, y_test)
print(f"Test R^2 Score: {r2_score:.2f}")

# output
# Test Mean Squared Error: 1237.63
# Test R^2 Score: 0.93</code></pre> 
<p><em>可视化：</em></p> 
<pre><code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns

colors = sns.color_palette("colorblind")
check_len = len(y_test)
data = [x+(1000-check_len) for x in range(check_len)]
plt.figure()
plt.plot(data, y_test[-check_len:], color=colors[1], label="Actual", linewidth=2)
plt.plot(data, y_pred[-check_len:], color=colors[2], label="Predictions", linewidth=2)
plt.xlabel("data")
plt.ylabel("target")
plt.title("GradientBoostingRegressor")
plt.legend()
plt.show()</code></pre> 
<p class="img-center"><img alt="" height="287" src="https://images2.imgbox.com/87/c4/sFb6dLOi_o.png" width="399"></p> 
<p></p> 
<h2 id="%E4%B8%89%E3%80%81XGBoost" style="background-color:transparent;">三、XGBoost</h2> 
<p>        <strong>XGBoost（eXtreme Gradient Boosting）</strong>是一种高效的机器学习算法，属于梯度提升（Gradient Boosting）算法的扩展。它由陈天奇等人在2016年开发，旨在提供一种快速、灵活且可扩展的树提升框架。</p> 
<p> 官方链接：<a href="https://xgboost.readthedocs.io/en/stable/python/python_intro.html#py-data" rel="nofollow" title="Python Package Introduction — xgboost 2.1.1 documentation">Python Package Introduction — xgboost 2.1.1 documentation</a></p> 
<p> <strong>说明：本文使用标准版（CPU版本）XGBoost，关于GPU加速版内容请参考官方文档。</strong></p> 
<h3 id="1%E3%80%81XGBClassifier%EF%BC%88%E5%88%86%E7%B1%BB%EF%BC%89">1、XGBClassifier（分类）</h3> 
<blockquote> 
 <p><em>class </em>xgboost.<em><span style="color:#fe2c24;">XGBClassifier</span></em>(<em>*</em>, <em>objective='binary:logistic'</em>, <em>**kwargs</em>)</p> 
</blockquote> 
<p> <em>示例代码：</em></p> 
<pre><code class="language-python">from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

import xgboost as xgb

# 加载数据集
iris = load_iris()
X, y = iris.data, iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 设定参数
param = {
    "tree_method": 'hist',        # 树方法
    "objective": 'multi:softmax', # 目标函数
    "eval_metric": 'merror',      # 评估指标
    "num_class": 3,               # 类别数
    "n_estimators": 10,           # 弱学习器的数量
    "max_depth": 3,               # 树的最大深度
    "learning_rate": 0.1,         # 学习率
    "subsample": 0.8,             # 训练实例的子样本率
    "random_state": 42            # 随机状态，确保结果可复现
}

# 早停回调函数
early_stop = xgb.callback.EarlyStopping(
    rounds=2,                    # 早停回合数
    metric_name='merror',        # 用于早停的指标名
    save_best=True               # 返回最佳模型
)

# 创建XGBClassifier实例
xgb_clf = xgb.XGBClassifier(**param, callbacks=[early_stop])

# 训练模型
xgb_clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)])

# 预测测试集
y_pred = xgb_clf.predict(X_test)</code></pre> 
<p>output：（每轮早停的验证信息）</p> 
<p class="img-center"><img alt="" height="49" src="https://images2.imgbox.com/2e/c3/PJPClVbu_o.png" width="399"></p> 
<p><em> 评估性能指标--示例代码：</em></p> 
<pre><code class="language-python"># 打印分类报告
print("classification report:")
print(classification_report(y_test, y_pred))

# 打印特征重要性
feature_importances = xgb_clf.feature_importances_
print("Feature importances:")
for feature, importance in zip(iris.feature_names, feature_importances):
    print(f"\t{feature}: {importance:.4f}")</code></pre> 
<p><em>output： </em></p> 
<p class="img-center"><img alt="" height="239" src="https://images2.imgbox.com/0c/58/C1ZmNDad_o.png" width="399"></p> 
<p></p> 
<h3 id="2%E3%80%81XGBRegressor%EF%BC%88%E5%9B%9E%E5%BD%92%EF%BC%89" style="background-color:transparent;">2、XGBRegressor（回归）</h3> 
<blockquote> 
 <p> <em>class </em>xgboost.<em><span style="color:#ff9900;">XGBRegressor</span></em>(<em>*</em>, <em>objective='reg:squarederror'</em>, <em>**kwargs</em>)</p> 
</blockquote> 
<p> <em>示例代码：</em></p> 
<pre><code class="language-python">from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import xgboost as xgb

# 生成合成回归数据
X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 设定参数
param = {
    "tree_method": 'hist',             # 树方法
    "objective": 'reg:squarederror',   # 目标函数
    "eval_metric": 'rmse',             # 评估指标
    "n_estimators": 100,               # 弱学习器的数量
    "max_depth": 3,                    # 树的最大深度
    "learning_rate": 0.1,              # 学习率
    "base_score": 0.5,                 # 全局偏差
    "random_state": 42                 # 随机状态，确保结果可复现
}

# 创建XGBRegressor实例
xgb_reg = xgb.XGBRegressor(**param)

# 训练模型
xgb_reg.fit(X_train, y_train)

# 预测测试集
y_pred = xgb_reg.predict(X_test)

# 计算测试集的均方误差（MSE）
mse = mean_squared_error(y_test, y_pred)
print(f"Test Mean Squared Error: {mse:.2f}")

# 计算测试集的确定系数（R^2）
r2_score = xgb_reg.score(X_test, y_test)
print(f"Test R^2 Score: {r2_score:.2f}")

# output:
# Test Mean Squared Error: 1357.97
# Test R^2 Score: 0.93</code></pre> 
<p> <em>可视化：</em></p> 
<pre><code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns

colors = sns.color_palette("colorblind")
check_len = len(y_test)
data = [x+(1000-check_len) for x in range(check_len)]
plt.figure()
plt.plot(data, y_test[-check_len:], color=colors[1], label="Actual", linewidth=2)
plt.plot(data, y_pred[-check_len:], color=colors[2], label="Predictions", linewidth=2)
plt.xlabel("data")
plt.ylabel("target")
plt.title("XGBRegressor")
plt.legend()
plt.show()</code></pre> 
<p class="img-center"><img alt="" height="290" src="https://images2.imgbox.com/b2/ca/gefmpt1i_o.png" width="399"></p> 
<p></p> 
<h2 id="%E5%9B%9B%E3%80%81LightGBM%C2%A0" style="background-color:transparent;">四、LightGBM </h2> 
<p>         LightGBM 是一个开源的梯度提升框架，是由微软开发的boosting集成模型，和XGBoost一样是对GBDT的优化和高效实现，使用基于树的学习算法。它设计用于处理大规模数据，具有高性能、低内存消耗和高准确率的特点。</p> 
<p>官方链接：<a href="https://lightgbm.readthedocs.io/en/latest/Python-API.html" rel="nofollow" title="Python API — LightGBM 4.5.0.99 documentation">Python API — LightGBM 4.5.0.99 documentation</a></p> 
<h3 id="1%E3%80%81LGBMClassifier%EF%BC%88%E5%88%86%E7%B1%BB%EF%BC%89">1、LGBMClassifier（分类）</h3> 
<blockquote> 
 <p><em>class </em>lightgbm.<em><span style="color:#fe2c24;">LGBMClassifier </span></em></p> 
</blockquote> 
<p><em> 示例代码：</em></p> 
<pre><code class="language-python">from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

import lightgbm as lgb

# 加载数据集
iris = load_iris()
X, y = iris.data, iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 设置参数
param = {
    "boosting_type": 'gbdt',   # boosting 类型
    "num_leaves": 31,          # 最大树叶数
    "max_depth": -1,           # -1即不限制树深度
    "learning_rate": 0.1,
    "n_estimators": 10,
    "objective": 'multiclass', # 多分类
    "random_state": 42,
    
}
# 创建LGBMClassifier实例
lgbm_clf = lgb.LGBMClassifier(**param)

# 训练模型
lgbm_clf.fit(X_train, y_train)

# 预测测试集
y_pred = lgbm_clf.predict(X_test)</code></pre> 
<p><em>训练模型时，输出 ：</em></p> 
<p class="img-center"><img alt="" height="135" src="https://images2.imgbox.com/af/72/qwMajEx2_o.png" width="399"></p> 
<p>注：<em>"[LightGBM] [Warning] No further splits with positive gain, best gain: -inf"  警告出现可能是模型已经训练足够，或者所有可能的分裂均已测试，不一定是模型性能不佳。</em></p> 
<p><em>评估性能指标--示例代码：</em> </p> 
<pre><code class="language-python"># 打印分类报告
print("classification report:")
print(classification_report(y_test, y_pred))

# 打印特征重要性
feature_importances = lgbm_clf.feature_importances_
print("Feature importances:")
for feature, importance in zip(iris.feature_names, feature_importances):
    print(f"\t{feature}: {importance:.4f}")</code></pre> 
<p><em> output：</em></p> 
<p class="img-center"><img alt="" height="245" src="https://images2.imgbox.com/67/97/RvUN2B6V_o.png" width="399"></p> 
<p></p> 
<h3 id="2%E3%80%81LGBMRegressor%20%EF%BC%88%E5%9B%9E%E5%BD%92%EF%BC%89" style="background-color:transparent;">2、<span style="color:#0d0016;">LGBMRegressor （回归）</span></h3> 
<blockquote> 
 <p><em>class </em>lightgbm.<span style="color:#ff9900;"><em>LGBMRegressor </em></span></p> 
</blockquote> 
<p> <em>示例代码：</em></p> 
<p></p> 
<pre><code class="language-python">from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

import lightgbm as lgb

# 生成合成回归数据
X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 设置参数
param = {
    "boosting_type": 'gbdt',   # boosting 类型
    "num_leaves": 31,          
    "max_depth": -1,           
    "learning_rate": 0.05,
    "n_estimators": 100,
    "objective": 'regression', # 回归
    "random_state": 42,
}

# 创建LGBMRegressor实例
lgb_reg = lgb.LGBMRegressor(**param)

# 训练模型
lgb_reg.fit(X_train, y_train)

# 预测测试集
y_pred = lgb_reg.predict(X_test)

# 计算测试集的均方误差（MSE）
mse = mean_squared_error(y_test, y_pred)
print(f"Test Mean Squared Error: {mse:.2f}")

# 计算测试集的确定系数（R^2）
r2_score = lgb_reg.score(X_test, y_test)
print(f"Test R^2 Score: {r2_score:.2f}")

# output:
# Test Mean Squared Error: 1219.03
# Test R^2 Score: 0.93</code></pre> 
<p><em>可视化： </em></p> 
<pre><code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns

colors = sns.color_palette("colorblind")
check_len = len(y_test)
data = [x+(1000-check_len) for x in range(check_len)]
plt.figure()
plt.plot(data, y_test[-check_len:], color=colors[1], label="Actual", linewidth=2)
plt.plot(data, y_pred[-check_len:], color=colors[2], label="Predictions", linewidth=2)
plt.xlabel("data")
plt.ylabel("target")
plt.title("LGBMRegressor")
plt.legend()
plt.show()</code></pre> 
<p class="img-center"><img alt="" height="285" src="https://images2.imgbox.com/19/8f/pLpW1CHI_o.png" width="399"></p> 
<p> </p> 
<h2 id="%E6%80%BB%E7%BB%93%C2%A0" style="background-color:transparent;">总结 </h2> 
<p>        本文列举了AdaBoost、GBDT、XGBoost、LightGBM四种机器学习算法，通过scikit-learn API 描述内容进行简单实操。实际上，本文只是使用简单的数据进行验证，并不能体现算法的实际效率，比如XGBoost、LightGBM两种在处理超大数据集上性能。与此同时，对于上述算法的使用仅仅是停留在最基础层次，还有许多可以优化的地方，比如<em><strong>设置早停参数、超参数调优、交叉验证 </strong></em>等方法。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bf4c77c52434c1a8be6b298ed8b19bd8/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">SSM基于web的火车订票系统设计s5088 座位预定</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/947d9cfb279bb601524244badb531b6f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">EasyExcel-高性能的 Java Excel 处理库</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>