<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Mac本地部署大模型体验AIGC能力 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/95055484887e8cb20792530d3ee0d5f9/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Mac本地部署大模型体验AIGC能力">
  <meta property="og:description" content="介绍 随着ChatGPT的横空出世，国内互联网大厂、创业公司纷纷加了AIGC赛道，不断推出各种大模型，而这些大模型由于规模庞大、结构复杂，往往包含了数十亿至数千亿的参数。这些模型在训练阶段，一般需要使用高效能的GPU集群训练数十天时间，在推理阶段，一般也需要高效能的GPU集群才能支撑一定量级的并发请求且实时返回。目前也有不少公司推出了规模相对较小但效果仍有一定优势的大模型，可以在消费级的单卡GPU上进行推理、甚至训练。本文尝试在普通的Macbook Pro上部署大模型开源方案，实现自然语言问答和对话等功能，虽然性能和效果一般，但可以在不借助深度学习专用GPU服务器的前提下，体验一下目前AIGC的能力。
配置 所使用的Macbook Pro配置如下：
机型，Macbook Pro（14英寸，2021年）；芯片，Apple M1 Pro；内存，16G；系统，macOS Monterey，12.6.2。 前置条件 首先默认本地已安装macOS的软件包管理工具Homebrew。
Git 安装Git：
brew install git
由于使用git命令下载的模型文件较大，因此还需要安装Git Large File Storage：
brew install git-lfs
Conda Conda是一个依赖和环境管理工具，支持的语言包括Python、R、Ruby、Lua、Scala、Java、JavaScript、C/C&#43;&#43;、Fortran等，且目前在Python语言生态中得到广泛的应用，通过其可以创建、管理多个相互独立、隔离的Python环境，并在环境中安装、管理Python依赖，而MiniConda是Conda的免费、最小可用版本。下载并安装MiniConda：
wget repo.anaconda.com/miniconda/M… bash ./Miniconda3-latest-MacOSX-arm64.sh -b -p $HOME/miniconda source ~/miniconda/bin/activate
ChatGLM-6B 介绍 论文《Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond》通过图1所示的树状图详细列举了自2018年以来自然语言大模型（LLM）这一领域的发展路线和相应的各大模型，其中一部分是在Transformer出现之前、不基于Transformer的大模型，例如AI2的ELMo，另一大部分是在Transfomer出现之后、基于Transformer的大模型，其又分为三个发展路线：
仅基于Transformer解码器的大模型（图中的蓝色部分），例如，OpenAI的GPT系列、Meta的LLaMa、Google的PaLM等；仅基于Transformer编码器的大模型（图中的粉丝部分），例如，Google的BERT、Meta的RoBERTa等；同时基于Transformer编码器和解码器的大模型（图中的绿色部分），例如，Meta的BART、Google的T5、清华大学的GLM/ChatGLM等。 这里选择ChatGLM-6B进行本地部署，其官网上的介绍如下：ChatGLM-6B是一个开源的、支持中英双语问答的对话语言模型，基于General Language Model（GLM）架构，具有62亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4量化级别下最低只需6GB显存）。ChatGLM-6B使用了和ChatGLM相同的技术，针对中文问答和对话进行了优化。经过约1T标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62亿参数的ChatGLM-6B已经能生成相当符合人类偏好的回答。而ChatGLM-6B-INT4是ChatGLM-6B量化后的模型权重。具体的，ChatGLM-6B-INT4对ChatGLM-6B中的28个GLM Block进行了INT4量化，没有对Embedding和LM Head进行量化。量化后的模型理论上6G显存（使用CPU即内存）即可推理，具有在嵌入式设备（如树莓派）上运行的可能。
部署 创建并激活环境：
conda create --name chatglm python=3.9 conda activate chatglm
下载ChatGLM-6B源码：
cd ~/workspace/ git clone github.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-02-03T21:37:57+08:00">
    <meta property="article:modified_time" content="2024-02-03T21:37:57+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Mac本地部署大模型体验AIGC能力</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>介绍</h2> 
<p>随着ChatGPT的横空出世，国内互联网大厂、创业公司纷纷加了AIGC赛道，不断推出各种大模型，而这些大模型由于规模庞大、结构复杂，往往包含了数十亿至数千亿的参数。这些模型在训练阶段，一般需要使用高效能的GPU集群训练数十天时间，在推理阶段，一般也需要高效能的GPU集群才能支撑一定量级的并发请求且实时返回。目前也有不少公司推出了规模相对较小但效果仍有一定优势的大模型，可以在消费级的单卡GPU上进行推理、甚至训练。本文尝试在普通的Macbook Pro上部署大模型开源方案，实现自然语言问答和对话等功能，虽然性能和效果一般，但可以在不借助深度学习专用GPU服务器的前提下，体验一下目前AIGC的能力。</p> 
<h2><a id="_4"></a>配置</h2> 
<p>所使用的Macbook Pro配置如下：</p> 
<ul><li>机型，Macbook Pro（14英寸，2021年）；</li><li>芯片，Apple M1 Pro；</li><li>内存，16G；</li><li>系统，macOS Monterey，12.6.2。</li></ul> 
<h2><a id="_13"></a>前置条件</h2> 
<p>首先默认本地已安装macOS的软件包管理工具<a href="https://link.juejin.cn?target=https%3A%2F%2Fbrew.sh%2F" rel="nofollow">Homebrew</a>。</p> 
<h3><a id="Git_17"></a>Git</h3> 
<p>安装<a href="https://link.juejin.cn?target=https%3A%2F%2Fgit-scm.com%2F" rel="nofollow">Git</a>：</p> 
<blockquote> 
 <p>brew install git</p> 
</blockquote> 
<p>由于使用git命令下载的模型文件较大，因此还需要安装<a href="https://link.juejin.cn?target=https%3A%2F%2Fdocs.github.com%2Fzh%2Frepositories%2Fworking-with-files%2Fmanaging-large-files%2Finstalling-git-large-file-storage" rel="nofollow">Git Large File Storage</a>：</p> 
<blockquote> 
 <p>brew install git-lfs</p> 
</blockquote> 
<h3><a id="Conda_27"></a>Conda</h3> 
<p><a href="https://link.juejin.cn?target=https%3A%2F%2Fdocs.conda.io%2Fen%2Flatest%2F" rel="nofollow">Conda</a>是一个依赖和环境管理工具，支持的语言包括Python、R、Ruby、Lua、Scala、Java、JavaScript、C/C++、Fortran等，且目前在Python语言生态中得到广泛的应用，通过其可以创建、管理多个相互独立、隔离的Python环境，并在环境中安装、管理Python依赖，而<a href="https://link.juejin.cn?target=https%3A%2F%2Fdocs.conda.io%2Fen%2Flatest%2Fminiconda.html" rel="nofollow">MiniConda</a>是Conda的免费、最小可用版本。下载并安装MiniConda：</p> 
<blockquote> 
 <p>wget <a href="https://link.juejin.cn?target=https%3A%2F%2Frepo.anaconda.com%2Fminiconda%2FMiniconda3-latest-MacOSX-arm64.sh" rel="nofollow">repo.anaconda.com/miniconda/M…</a> bash ./Miniconda3-latest-MacOSX-arm64.sh -b -p $HOME/miniconda source ~/miniconda/bin/activate</p> 
</blockquote> 
<h2><a id="ChatGLM6B_33"></a>ChatGLM-6B</h2> 
<h3><a id="_35"></a>介绍</h3> 
<p><img src="https://images2.imgbox.com/46/26/drIS8D2b_o.png" alt="图1 自然语言大模型（LLM）的发展路线和相应大模型"></p> 
<p>论文<a href="https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fabs%2F2304.13712" rel="nofollow">《Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond》</a>通过图1所示的树状图详细列举了自2018年以来自然语言大模型（LLM）这一领域的发展路线和相应的各大模型，其中一部分是在Transformer出现之前、不基于Transformer的大模型，例如AI2的ELMo，另一大部分是在Transfomer出现之后、基于Transformer的大模型，其又分为三个发展路线：</p> 
<ul><li>仅基于Transformer解码器的大模型（图中的蓝色部分），例如，OpenAI的GPT系列、Meta的LLaMa、Google的PaLM等；</li><li>仅基于Transformer编码器的大模型（图中的粉丝部分），例如，Google的BERT、Meta的RoBERTa等；</li><li>同时基于Transformer编码器和解码器的大模型（图中的绿色部分），例如，Meta的BART、Google的T5、清华大学的GLM/ChatGLM等。</li></ul> 
<p>这里选择ChatGLM-6B进行本地部署，其<a href="https://link.juejin.cn?target=https%3A%2F%2Fchatglm.cn%2Fblog" rel="nofollow">官网</a>上的介绍如下：ChatGLM-6B是一个开源的、支持中英双语问答的对话语言模型，基于<a href="https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FTHUDM%2FGLM" rel="nofollow">General Language Model（GLM</a>）架构，具有62亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4量化级别下最低只需6GB显存）。ChatGLM-6B使用了和<a href="https://link.juejin.cn?target=https%3A%2F%2Fchatglm.cn%2F" rel="nofollow">ChatGLM</a>相同的技术，针对中文问答和对话进行了优化。经过约1T标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62亿参数的ChatGLM-6B已经能生成相当符合人类偏好的回答。而ChatGLM-6B-INT4是ChatGLM-6B量化后的模型权重。具体的，ChatGLM-6B-INT4对ChatGLM-6B中的28个GLM Block进行了INT4量化，没有对Embedding和LM Head进行量化。量化后的模型理论上6G显存（使用CPU即内存）即可推理，具有在嵌入式设备（如树莓派）上运行的可能。</p> 
<h3><a id="_47"></a>部署</h3> 
<p>创建并激活环境：</p> 
<blockquote> 
 <p>conda create --name chatglm python=3.9 conda activate chatglm</p> 
</blockquote> 
<p>下载ChatGLM-6B源码：</p> 
<blockquote> 
 <p>cd ~/workspace/ git clone <a href="https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FTHUDM%2FChatGLM-6B.git" rel="nofollow">github.com/THUDM/ChatG…</a></p> 
</blockquote> 
<p>安装依赖：</p> 
<blockquote> 
 <p>cd ~/workspace/ChatGLM-6B pip install -r requirements.txt</p> 
</blockquote> 
<p>下载ChatGLM-6B INT4量化的模型权重ChatGLM-6B-INT4：</p> 
<blockquote> 
 <p>cd ~/workspace/models/ git lfs install git clone <a href="https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FTHUDM%2Fchatglm-6b-int4" rel="nofollow">huggingface.co/THUDM/chatg…</a></p> 
</blockquote> 
<p>Macbook直接加载量化后的模型可能出现提示——“clang: error: unsupported option ‘-fopenmp’”，还需<a href="https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FTHUDM%2FChatGLM-6B%2Fblob%2Fmain%2FFAQ.md%23q1" rel="nofollow">单独安装OpenMP依赖</a>，此时会安装下面几个文件：/usr/local/lib/libomp.dylib, /usr/local/include/ompt.h, /usr/local/include/omp.h, /usr/local/include/omp-tools.h：</p> 
<blockquote> 
 <p>curl -O <a href="https://link.juejin.cn?target=https%3A%2F%2Fmac.r-project.org%2Fopenmp%2Fopenmp-14.0.6-darwin20-Release.tar.gz" rel="nofollow">mac.r-project.org/openmp/open…</a> sudo tar fvxz openmp-14.0.6-darwin20-Release.tar.gz -C /</p> 
</blockquote> 
<p>执行以下Python代码，从本地地址加载模型并进行推理，对“你好”和“如何读一本书”进行回答：</p> 
<pre><code class="prism language-python">python复制代码<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModel
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"/Users/xxx/workspace/models/chatglm-6b-int4"</span><span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"/Users/xxx/workspace/models/chatglm-6b-int4"</span><span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> <span class="token string">"你好"</span><span class="token punctuation">,</span> history<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> <span class="token string">"如何读一本书"</span><span class="token punctuation">,</span> history<span class="token operator">=</span>history<span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
</code></pre> 
<p>执行结果如图2所示，推理耗时约6分钟，比较慢。</p> 
<p><img src="https://images2.imgbox.com/b6/82/eSP3N8vW_o.png" alt="图2 ChatGLM-6B执行结果"></p> 
<p>修改ChatGLM-6B源码目录下的web_demo.py文件的7、8两行，使用本地已下载的INT4量化的模型权重ChatGLM-6B-INT4，并且不使用半精度（Mac不支持）和CUDA（无GPU）：</p> 
<pre><code class="prism language-python">python复制代码tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"/Users/xxx/workspace/models/chatglm-6b-int4"</span><span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment">#tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)</span>
model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"/Users/xxx/workspace/models/chatglm-6b-int4"</span><span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()</span>
</code></pre> 
<p>启动web_demo.py：</p> 
<blockquote> 
 <p>python web_demo.py</p> 
</blockquote> 
<p>可在网页中提问，由模型进行推理，如图3所示。 <img src="https://images2.imgbox.com/5f/0b/eI41kjQg_o.png" alt="图3 ChatGLM-6B网页示例"></p> 
<h2><a id="ChatGLM26B_99"></a>ChatGLM2-6B</h2> 
<h3><a id="_101"></a>介绍</h3> 
<p>ChatGLM2-6B是开源中英双语对话模型<a href="https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FTHUDM%2FChatGLM-6B" rel="nofollow">ChatGLM-6B</a>的第二代版本，于2023年6月25日发布，对于其介绍直接引用官网上的内容。ChatGLM2-6B在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，还引入了如下新特性：</p> 
<ul><li>更强大的性能：基于ChatGLM初代模型的开发经验，全面升级了ChatGLM2-6B的基座模型。ChatGLM2-6B使用了<a href="https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FTHUDM%2FGLM" rel="nofollow">GLM</a>的混合目标函数，经过了1.4T中英标识符的预训练与人类偏好对齐训练，<a href="https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FTHUDM%2FChatGLM2-6B%23%E8%AF%84%E6%B5%8B%E7%BB%93%E6%9E%9C" rel="nofollow">评测结果</a>显示，相比于初代模型，ChatGLM2-6B在MMLU（+23%）、CEval（+33%）、GSM8K（+571%） 、BBH（+60%）等数据集上的性能取得了大幅度的提升，在同尺寸开源模型中具有较强的竞争力。</li><li>更长的上下文：基于<a href="https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FHazyResearch%2Fflash-attention" rel="nofollow">FlashAttention</a>技术，其将基座模型的上下文长度（Context Length）由ChatGLM-6B 的2K扩展到了32K，并在对话阶段使用8K的上下文长度训练，允许更多轮次的对话。但当前版本的 ChatGLM2-6B对单轮超长文档的理解能力有限，其会在后续迭代升级中着重进行优化。</li><li>更高效的推理：基于<a href="https://link.juejin.cn?target=http%3A%2F%2Farxiv.org%2Fabs%2F1911.02150" rel="nofollow">Multi-Query Attention</a>技术，ChatGLM2-6B有更高效的推理速度和更低的显存占用，在官方的模型实现下，推理速度相比初代提升了42%，INT4量化下，6G显存支持的对话长度由1K提升到了 8K。</li><li>更开放的协议：ChatGLM2-6B权重对学术研究完全开放，在获得官方的书面许可后，亦允许商业使用。</li></ul> 
<h3><a id="_110"></a>部署</h3> 
<p>以下步骤和ChatGLM-6基本相同。首先创建并激活环境：</p> 
<blockquote> 
 <p>conda create --name chatglm2 python=3.9 conda activate chatglm2</p> 
</blockquote> 
<p>下载ChatGLM2-6B源码：</p> 
<blockquote> 
 <p>cd ~/workspace/ git clone <a href="https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FTHUDM%2FChatGLM2-6B.git" rel="nofollow">github.com/THUDM/ChatG…</a></p> 
</blockquote> 
<p>安装依赖：</p> 
<blockquote> 
 <p>cd ~/workspace/ChatGLM2-6B pip install -r requirements.txt</p> 
</blockquote> 
<p>下载ChatGLM2-6B INT4量化的模型权重ChatGLM2-6B-INT4：</p> 
<blockquote> 
 <p>cd ~/workspace/models/ git lfs install #若ChatGLM-6B部分已执行，则无需再执行 git clone <a href="https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FTHUDM%2Fchatglm2-6b-int4" rel="nofollow">huggingface.co/THUDM/chatg…</a></p> 
</blockquote> 
<p>执行以下Python代码，从本地地址加载模型并进行推理，对“你好”和“如何读一本书”进行回答，代码与ChatGLM部分基本相同，仅更改模型地址：</p> 
<pre><code class="prism language-python">python复制代码<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModel
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"/Users/xxx/workspace/models/chatglm2-6b-int4"</span><span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"/Users/xxx/workspace/models/chatglm2-6b-int4"</span><span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> <span class="token string">"你好"</span><span class="token punctuation">,</span> history<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
response<span class="token punctuation">,</span> history <span class="token operator">=</span> model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>tokenizer<span class="token punctuation">,</span> <span class="token string">"如何读一本书"</span><span class="token punctuation">,</span> history<span class="token operator">=</span>history<span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
</code></pre> 
<p>执行结果如图4所示。</p> 
<p><img src="https://images2.imgbox.com/55/c0/nBCJqHW1_o.png" alt="图4 ChatGLM2-6B执行结果"></p> 
<p>对于ChatGLM2-6B源码目录下的web_demo.py的修改和启动和ChatGLM-6B部分类似，修改其中的6、7两行：</p> 
<pre><code class="prism language-python">python复制代码tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"/Users/xxx/workspace/models/chatglm2-6b-int4"</span><span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment">#tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)</span>
model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"/Users/xxx/workspace/models/chatglm2-6b-int4"</span><span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True).cuda()</span>
</code></pre> 
<p>启动web_demo.py：</p> 
<blockquote> 
 <p>python web_demo.py</p> 
</blockquote> 
<p>启动后如图5所示。</p> 
<p><img src="https://images2.imgbox.com/fd/3d/okjGXZSQ_o.png" alt="图5 ChatGLM2-6B网页示例"></p> 
<h2><a id="LangChain_160"></a>LangChain</h2> 
<h3><a id="_162"></a>介绍</h3> 
<p><a href="https://link.juejin.cn?target=https%3A%2F%2Fdocs.langchain.com%2Fdocs%2F" rel="nofollow">LangChain</a>是一个面向大语言模型的应用开发框架，如果将大语言模型比作人的大脑，那么可以将LangChain可以比作人的五官和四肢，它可以将外部数据源、工具和大语言模型连接在一起，既可以补充大语言模型的输入，也可以承接大语言模型的输出。LangChain包含以下核心组件：</p> 
<ul><li>Model，表示大语言模型，</li><li>Prompt，表示提示；</li><li>Tool，表示工具；</li><li>Chain，表示将Model、Tool等组件串联在一起，甚至可以递归地将其他Chain串联在一起；</li><li>Agent，相对于Chain已固定执行链路，Agent能够实现动态的执行链路。</li></ul> 
<h3><a id="_172"></a>部署</h3> 
<h4><a id="_174"></a>安装依赖</h4> 
<p>在chatglm2环境下继续安装LangChain依赖：</p> 
<blockquote> 
 <p>pip install langchain</p> 
</blockquote> 
<p>注意，以上命令只是安装LangChain依赖的最小集，因为LangChain集成了多种模型、存储等工具，而这些工具的依赖并不会被安装，所以后续进一步使用这些工具时可能会报缺少特定依赖的错误，可以使用pip进行安装，也可以这里直接使用“pip install’langchain[all]'”安装LangChain的所有依赖，但比较耗时。</p> 
<h4><a id="Model_182"></a>Model</h4> 
<p>继承LangChain的LLM，接入ChatGLM2，实现对话和问答，代码文件chatglm_llm.py如下所示：</p> 
<pre><code class="prism language-python">python复制代码<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>llms<span class="token punctuation">.</span>base <span class="token keyword">import</span> LLM
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>llms<span class="token punctuation">.</span>utils <span class="token keyword">import</span> enforce_stop_tokens
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModel
<span class="token keyword">from</span> typing <span class="token keyword">import</span> List<span class="token punctuation">,</span> Optional

<span class="token keyword">class</span> <span class="token class-name">ChatGLM2</span><span class="token punctuation">(</span>LLM<span class="token punctuation">)</span><span class="token punctuation">:</span>
    max_token<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">4096</span>
    temperature<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">0.8</span>
    top_p <span class="token operator">=</span> <span class="token number">0.9</span>
    tokenizer<span class="token punctuation">:</span> <span class="token builtin">object</span> <span class="token operator">=</span> <span class="token boolean">None</span>
    model<span class="token punctuation">:</span> <span class="token builtin">object</span> <span class="token operator">=</span> <span class="token boolean">None</span>
    history <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
    <span class="token decorator annotation punctuation">@property</span>
    <span class="token keyword">def</span> <span class="token function">_llm_type</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">str</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token string">"ChatGLM2"</span>

    <span class="token comment"># 定义load_model方法，进行模型的加载        </span>
    <span class="token keyword">def</span> <span class="token function">load_model</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model_path <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_path<span class="token punctuation">,</span>trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_path<span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 实现_call方法，进行模型的推理</span>
    <span class="token keyword">def</span> <span class="token function">_call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>prompt<span class="token punctuation">:</span><span class="token builtin">str</span><span class="token punctuation">,</span> stop<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">str</span><span class="token punctuation">:</span>
        response<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>
                    self<span class="token punctuation">.</span>tokenizer<span class="token punctuation">,</span>
                    prompt<span class="token punctuation">,</span>
                    history<span class="token operator">=</span>self<span class="token punctuation">.</span>history<span class="token punctuation">,</span>
                    max_length<span class="token operator">=</span>self<span class="token punctuation">.</span>max_token<span class="token punctuation">,</span>
                    temperature<span class="token operator">=</span>self<span class="token punctuation">.</span>temperature<span class="token punctuation">,</span>
                    top_p<span class="token operator">=</span>self<span class="token punctuation">.</span>top_p<span class="token punctuation">)</span>
        <span class="token keyword">if</span> stop <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            response <span class="token operator">=</span> enforce_stop_tokens<span class="token punctuation">(</span>response<span class="token punctuation">,</span> stop<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>history <span class="token operator">=</span> self<span class="token punctuation">.</span>history <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> response<span class="token punctuation">]</span><span class="token punctuation">]</span>
        <span class="token keyword">return</span> response

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    llm<span class="token operator">=</span>ChatGLM2<span class="token punctuation">(</span><span class="token punctuation">)</span>
    llm<span class="token punctuation">.</span>load_model<span class="token punctuation">(</span><span class="token string">"/Users/xxx/workspace/models/chatglm2-6b-int4"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>llm<span class="token punctuation">.</span>_call<span class="token punctuation">(</span><span class="token string">"如何读一本书"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>chatglm2_llm.py的执行结果如图6所示。</p> 
<p><img src="https://images2.imgbox.com/d3/45/m6drF3Q8_o.png" alt="图6 chatglm2_llm.py执行结果"></p> 
<h4><a id="Chain_236"></a>Chain</h4> 
<h5><a id="LLMChain_238"></a>LLMChain</h5> 
<p>LLMChain是最基础的Chain，其引入一个提示模板将问题转化为提示输入模型，并输出模型的回答。</p> 
<p><img src="https://images2.imgbox.com/b0/80/IgUpMaun_o.png" alt="图7 LLMChain实现原理"></p> 
<p>其实现原理如图7所示，包含三步：</p> 
<ul><li>输入问题；</li><li>拼接提示，根据提示模板将问题转化为提示；</li><li>模型推理，输出答案。</li></ul> 
<p>代码文件chain_demo.py如下所示：</p> 
<pre><code class="prism language-python">python复制代码<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>prompts <span class="token keyword">import</span> PromptTemplate
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> LLMChain
<span class="token keyword">from</span> chatglm2_llm <span class="token keyword">import</span> ChatGLM2

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    <span class="token comment"># 定义模型</span>
    llm <span class="token operator">=</span> ChatGLM2<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 加载模型</span>
    llm<span class="token punctuation">.</span>load_model<span class="token punctuation">(</span><span class="token string">"/Users/xxx/workspace/models/chatglm2-6b-int4"</span><span class="token punctuation">)</span>
    <span class="token comment"># 定义提示模板</span>
    prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"question"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> template<span class="token operator">=</span><span class="token triple-quoted-string string">"""
    简洁和专业的来回答用户的问题。
    
    如果无法从中得到答案，请说 “根据已知信息无法回答该问题” 或 “没有提供足够的相关信息”，不允许在答案中添加编造成分，答案请使用中文。 
    
    问题是：{question}"""</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
    <span class="token comment"># 定义chain</span>
    chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>prompt<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token comment"># 执行chain</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token string">"如何读一本书"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>其中模型采用自定义模型，接入本地部署的ChatGLM2。chain_demo.py运行结果如图8所示。</p> 
<p><img src="https://images2.imgbox.com/48/6d/IeqeDYCj_o.png" alt="图8 LLMChain执行结果"></p> 
<h5><a id="RetrievalQA_279"></a>RetrievalQA</h5> 
<p>除了基础的链接提示和模型的LLMChain外，LangChain还提供了其他多种Chain，例如实现本地知识库功能的RetrievalQA和自动生成SQL并执行的SQLDatabaseChain。</p> 
<p><img src="https://images2.imgbox.com/df/aa/lfxMYWms_o.png" alt="图9 RetrievalQA实现原理"></p> 
<p>RetrievalQA实现原理如图9所示，先构建本地知识库，包含三步：</p> 
<ul><li>加载文档，LangChain提供多种BaseLoader实现进行文档加载；</li><li>切分文本段，LangChain同时提供多种TextSplitter实现进行文本段切分；</li><li>向量化文本段，使用向量化模型将文本段转化为向量，LangChain也支持多种方式的向量化模型，比如，OpenAIEmbeddings通过调用OpenAI的相关服务进行向量化，HuggingFaceEmbeddings可以远程或本地加载<a href="https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2F" rel="nofollow">HuggingFace</a>上的模型进行向量化；</li><li>对文本段向量构建向量索引，LangChain也支持多种向量索引引擎，包括<a href="https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Ffaiss" rel="nofollow">Faiss</a>、<a href="https://link.juejin.cn?target=https%3A%2F%2Fdocs.trychroma.com%2F" rel="nofollow">Chroma</a>、<a href="https://link.juejin.cn?target=https%3A%2F%2Fmilvus.io%2F" rel="nofollow">Milvus</a>等。</li></ul> 
<p>再基于本地知识库进行模型推理，包含五步：</p> 
<ul><li>输入问题；</li><li>向量化问题，和文本段向量化一致，将问题转化为向量；</li><li>搜索相关文本段，从向量索引中搜索和问题相关的文本段；</li><li>拼接提示，根据提示模板将问题和相关文本段转化为提示；</li><li>模型推理，输出答案。</li></ul> 
<p>代码文件retrieval_qa_demo.py如下所示：</p> 
<pre><code class="prism language-python">python复制代码<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> RetrievalQA
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>document_loaders <span class="token keyword">import</span> UnstructuredMarkdownLoader
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>huggingface <span class="token keyword">import</span> HuggingFaceEmbeddings
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>text_splitter <span class="token keyword">import</span> MarkdownTextSplitter
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>vectorstores <span class="token keyword">import</span> Chroma
<span class="token keyword">from</span> chatglm2_llm <span class="token keyword">import</span> ChatGLM2

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    <span class="token comment"># 加载文档</span>
    loader <span class="token operator">=</span> UnstructuredMarkdownLoader<span class="token punctuation">(</span><span class="token string">"/Users/xxx/workspace/docs/creative.md"</span><span class="token punctuation">)</span>
    documents <span class="token operator">=</span> loader<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 切分文本</span>
    text_splitter <span class="token operator">=</span> MarkdownTextSplitter<span class="token punctuation">(</span>chunk_size<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> chunk_overlap<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    texts <span class="token operator">=</span> text_splitter<span class="token punctuation">.</span>split_documents<span class="token punctuation">(</span>documents<span class="token punctuation">)</span>
    <span class="token comment"># 初始化向量化模型</span>
    embeddings <span class="token operator">=</span> HuggingFaceEmbeddings<span class="token punctuation">(</span>model_name<span class="token operator">=</span><span class="token string">"/Users/xxx/workspace/models/text2vec-large-chinese"</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
    <span class="token comment"># 构建向量索引</span>
    db <span class="token operator">=</span> Chroma<span class="token punctuation">.</span>from_documents<span class="token punctuation">(</span>texts<span class="token punctuation">,</span> embeddings<span class="token punctuation">)</span>
    <span class="token comment"># 定义模型</span>
    llm <span class="token operator">=</span> ChatGLM2<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 加载模型</span>
    llm<span class="token punctuation">.</span>load_model<span class="token punctuation">(</span><span class="token string">"/Users/xxx/workspace/models/chatglm2-6b-int4"</span><span class="token punctuation">)</span>
    <span class="token comment"># 执行链路</span>
    qa <span class="token operator">=</span> RetrievalQA<span class="token punctuation">.</span>from_chain_type<span class="token punctuation">(</span>llm<span class="token punctuation">,</span> chain_type<span class="token operator">=</span><span class="token string">"stuff"</span><span class="token punctuation">,</span> retriever<span class="token operator">=</span>db<span class="token punctuation">.</span>as_retriever<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>qa<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token string">"怎么创建程序化创意"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>其中，对于知识库文档，笔者使用<a href="https://link.juejin.cn?target=https%3A%2F%2Fwww.yuque.com%2Fhanxu-l5ahv%2Fguchgf%2Fpydrra" rel="nofollow">《超级汇川程序化创意产品手册》</a>这一文档，将其以Markdown格式下载至本地，使用UnstructuredMarkdownLoader进行加载，并使用MarkdownTextSplitter进行切分得到文本段。对于向量化模型，笔者使用HuggingFace上的<a href="https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FGanymedeNil%2Ftext2vec-large-chinese" rel="nofollow">GanymedeNil/text2vec-large-chinese</a>，并下载至本地：</p> 
<blockquote> 
 <p>cd ~/workspace/models/ git lfs install #若ChatGLM-6B部分已执行，则无需再执行 git clone <a href="https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FGanymedeNil%2Ftext2vec-large-chinese" rel="nofollow">huggingface.co/GanymedeNil…</a></p> 
</blockquote> 
<p>对于向量索引引擎，笔者使用Chroma；对于大语言模型，笔者使用之前已定义的ChatGLM2。对于问题和从向量索引返回的相关文本段，RetrievalQA按下述提示模板拼接提示：</p> 
<blockquote> 
 <p>Use the following pieces of context to answer the question at the end. If you don’t know the answer, just say that you don’t know, don’t try to make up an answer.</p> 
 <p>{context}</p> 
 <p>Question: {question} Helpful Answer:</p> 
</blockquote> 
<p>retrieval_qa_demo.py运行结果如图10所示。</p> 
<p><img src="https://images2.imgbox.com/54/dd/5RiXmKdm_o.png" alt="图10 RetrievalQA Chain运行结果"></p> 
<h5><a id="SQLDatabaseChain_346"></a>SQLDatabaseChain</h5> 
<p><img src="https://images2.imgbox.com/a2/0a/7K2dwtqQ_o.png" alt="图11 SQLDatabaseChain实现原理"></p> 
<p>SQLDatabaseChain能够通过模型自动生成SQL并执行，其实现原理如图11所示，包含五步：</p> 
<ul><li>输入问题；</li><li>获取数据库Schema，Schema包含数据库所有表的建表语句和数据示例，LangChain支持多种关系型数据库，包括MariaDB、Oracle SQL、SQLite、ClickHouse、PrestoDB等；</li><li>拼接提示，根据提示模板将问题、数据库Schema转化为提示，并且提示中包含指示，要求模型在理解问题和数据库Schema的基础上，能够按一定的格式输出查询SQL、查询结果和问题答案等；</li><li>模型推理，这一步预期模型根据问题、数据库Schema推理、输出的答案中包含查询SQL，并从中提取出查询SQL；</li><li>执行查询SQL，从数据库中获取查询结果；</li><li>拼接提示，和上一次拼接的提示基本一致，只是其中的指示中包含了前两步已获取的查询SQL、查询结果；</li><li>模型推理，这一步预期模型根据问题、数据库Schema、查询SQL和查询结果推理出最终的问题答案。</li></ul> 
<p>代码文件sql_database_chain_demo.py如下所示：</p> 
<pre><code class="prism language-python">python复制代码<span class="token keyword">from</span> langchain <span class="token keyword">import</span> SQLDatabase<span class="token punctuation">,</span> SQLDatabaseChain
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>llms<span class="token punctuation">.</span>fake <span class="token keyword">import</span> FakeListLLM
<span class="token keyword">from</span> chatglm2_llm <span class="token keyword">import</span> ChatGLM2

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    <span class="token comment"># 定义模型</span>
    <span class="token comment"># 模型先尝试用ChatGLM2</span>
    llm <span class="token operator">=</span> ChatGLM2<span class="token punctuation">(</span><span class="token punctuation">)</span>
    llm<span class="token punctuation">.</span>load_model<span class="token punctuation">(</span><span class="token string">"/Users/xxx/workspace/models/chatglm2-6b-int4"</span><span class="token punctuation">)</span>
    <span class="token comment"># 模型再直接使用固定的答案，这些答案是事先根据提示由OpenAI ChatGPT3.5给出</span>
    <span class="token comment">#responses = ["SELECT COUNT() FROM Employee", "There are 8 employees."]</span>
    <span class="token comment">#llm = FakeListLLM(responses=responses, verbose=True)</span>
    <span class="token comment"># 定义数据库</span>
    <span class="token comment"># 可以参考https://database.guide/2-sample-databases-sqlite/，创建数据库、并将数据库文件Chinook.db存储至目录</span>
    <span class="token comment"># 数据库Chinook表示一个数字多媒体商店，包含了顾客（Customer）、雇员（Employee）、歌曲（Track）、订单（Invoice）及其相关的表和数据</span>
    db <span class="token operator">=</span> SQLDatabase<span class="token punctuation">.</span>from_uri<span class="token punctuation">(</span><span class="token string">"sqlite:Users/xxx/workspace/langchain-demo/Chinook.db"</span><span class="token punctuation">)</span>
    <span class="token comment"># 定义chain</span>
    chain <span class="token operator">=</span> SQLDatabaseChain<span class="token punctuation">.</span>from_llm<span class="token punctuation">(</span>llm<span class="token punctuation">,</span> db<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token comment"># 执行chain</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token string">"How many employees are there?"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>其中，对于大语言模型，先尝试使用之前已定义的ChatGLM2，后面会分析，从执行结果看，ChatGLM2-6B-INT4和ChatGLM2-6B并不能输出符合格式的答案，从而无法进一步从中提取出查询SQL，所以通过FakeListLLM直接使用固定的答案，而这些答案事先根据提示由OpenAI ChatGPT3.5给出。 对于数据库引擎，使用SQLite3（Macbook原生支持），对于数据库实例，使用<a href="https://link.juejin.cn?target=https%3A%2F%2Fdatabase.guide%2F2-sample-databases-sqlite%2F" rel="nofollow">Chinook</a>，可按照上述链接中的说明下载“Chinook_Sqlite.sql”并在本地创建数据库实例。Chinook表示一个数字多媒体商店，包含了顾客（Customer）、雇员（Employee）、歌曲（Track）、订单（Invoice）及其相关的表和数据，如图12所示。问题是“How many employees are there?”，即有多少雇员，期望模型先给出查询Employee表记录数的SQL，再根据查询结果给出最终的答案。</p> 
<p><img src="https://images2.imgbox.com/8d/53/HXDzApSQ_o.png" alt="图12 Chinook中的表"></p> 
<p>实际执行时，SQLDatabaseChain首先根据问题和数据库Schema生成如下的提示：</p> 
<blockquote> 
 <p>You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question. Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database. Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (") to denote them as delimited identifiers. Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table. Pay attention to use date(‘now’) function to get the current date, if the question involves “today”. Use the following format: Question: Question here SQLQuery: SQL Query to run SQLResult: Result of the SQLQuery Answer: Final answer here Only use the following tables: {数据库Schema，包含所有表的建表语句和数据示例，受限于篇幅，这里略去} Question: How many employees are there? SQLQuery:</p> 
</blockquote> 
<p>其中，提示的第一部分是指示，期望模型作为SQLite的专家，按照一定的要求进行推理，并按照一定的格式输出，第二部分是数据库Schema，第三部分是问题以及期望输出的开头“SQLQuery:”，预期模型按照提示续写，给出查询SQL。 若将提示输入ChatGPT3.5，可以返回预期的答案，SQLDatabaseChain进一步提取答案中“\nSQLResult”之前的部分，从而得到查询SQL：</p> 
<blockquote> 
 <p>SELECT COUNT() FROM Employee SQLResult: COUNT() 8 Answer: There are 8 employees.</p> 
</blockquote> 
<p>若将提示输入自定义的ChatGLM2（使用ChatGLM2-6B-INT4），则无法返回预期的答案（答案合理、但不符合格式要求）：</p> 
<blockquote> 
 <p>SQLite is a language for creating and managing databases. It does not have an SQL-specific version for getting the number of employees. However, I can provide you with an SQL query that you can run using a SQLite database to get the number of employees in the “Employee” table.<br> SQLite:</p> 
 <pre><code class="prism language-sql"><span class="token keyword">sql</span>
复制代码<span class="token keyword">SELECT</span> <span class="token function">COUNT</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">as</span> num_employees <span class="token keyword">FROM</span> Employee<span class="token punctuation">;</span>                                                                                                                                         
</code></pre> 
 <p>This query will return the count of employees in the “Employee” table. The result will be returned in a single row with a single column, labeled “num_employees”.</p> 
</blockquote> 
<p>SQLDatabaseChain的提示是针对ChatGPT逐步优化、确定的，因此适用于ChatGPT，LangChain官方示例中使用的大语言模型是OpenAI，即底层调用ChatGPT，而ChatGLM2-6B-INT4、ChatGLM2-6B相对于ChatGPT，模型规模较小，仅有60亿参数，对于上述的长文本提示无法给出预期的答案。由于没有OpenAI的Token，因此示例代码通过FakeListLLM直接使用由ChatGPT3.5给出的答案。 在获取查询SQL后，SQLDatabaseChain会执行该SQL获取查询结果，并继续根据问题、数据库Schema、查询SQL和查询结果生成如下的提示：</p> 
<blockquote> 
 <p>You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question. Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database. Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (") to denote them as delimited identifiers. Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table. Pay attention to use date(‘now’) function to get the current date, if the question involves “today”. Use the following format: Question: Question here SQLQuery: SQL Query to run SQLResult: Result of the SQLQuery Answer: Final answer here Only use the following tables: {数据库Schema，包含所有表的建表语句和数据示例，受限于篇幅，这里略去} Question: How many employees are there? SQLQuery:SELECT COUNT(EmployeeId) FROM Employee SQLResult: [(8,)] Answer:</p> 
</blockquote> 
<p>相比上次提示，本次提示只是在末尾追加了查询SQL和查询结果，若将提示输入ChatGPT3.5，则可以续写“Answer”，给出正确的答案：</p> 
<blockquote> 
 <p>There are 8 employees.</p> 
</blockquote> 
<p>这里也通过FakeListLLM直接使用由ChatGPT3.5给出的答案，从而在本地跑通SQLDatabaseChain的流程，运行结果如图13所示。</p> 
<p><img src="https://images2.imgbox.com/4c/b9/IJsqqMEx_o.png" alt="图13 SQLDatabaseChain运行结果"></p> 
<h4><a id="Agent_421"></a>Agent</h4> 
<p>Agent组合模型和各种工具，相对于Chain已固定执行链路，Agent能够实现动态的执行链路，实现如图14中的<a href="https://link.juejin.cn?target=https%3A%2F%2Freact-lm.github.io%2F" rel="nofollow">ReAct</a>架构。ReAct架构是一个循环过程，对于问题，通过多次迭代，直至获取最终答案，而每次迭代包括如下几步：</p> 
<ul><li>将问题，各工具描述，之前每次迭代模型推理出的思考（Thought）、工具（Action）、工具输入（Action Input）、工具执行后的输出（Observation），以及期望模型输出格式，按照提示模板拼接出提示；</li><li>将提示输入模型，由模型推理，输出进一步的思考（Thought）、工具（Action）、工具输入（Action Input）；</li><li>使用模型给出的工具输入执行相应工具，获取工具输出（Observation）；</li><li>继续第一步过程，直至获取最终答案跳出循环。</li></ul> 
<p><img src="https://images2.imgbox.com/4e/28/hMLHkJPv_o.png" alt="图14 ReAct架构"></p> 
<p>LangChain官方有个比较经典的实现ReAct架构的<a href="https://link.juejin.cn?target=https%3A%2F%2Fpython.langchain.com%2Fdocs%2Fmodules%2Fagents%2Fagent_types%2Freact" rel="nofollow">示例</a>，其需要OpenAI和<a href="https://link.juejin.cn?target=https%3A%2F%2Fserpapi.com%2F" rel="nofollow">SerpApi</a>的Token，针对问题，使用ChatGPT进行多次推理，根据推理结果先使用搜索工具查询相关人的年龄，再使用计算器工具计算年龄的乘方，从而得到最终的答案。感兴趣且有OpenAI和SerpApi Token的同学可以在本地执行示例代码体验，此处不再赘述。 上述示例若使用本地部署的ChatGLM2-6B-INT作为大语言模型，则和在SQLDatabaseChain中遇到的问题相同，无法根据提示给出符合预期格式的答案。可见，虽然LangChain在设计上考虑了可扩展性，将Model以接口形式对外提供服务，屏蔽底层实现细节，但各种Chain、Tool和Agent中的提示模板还是针对ChatGPT进行了专门优化。</p> 
<h2><a id="langchainChatGLM_434"></a>langchain-ChatGLM</h2> 
<h3><a id="_436"></a>介绍</h3> 
<p><a href="https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FimClumsyPanda%2Flangchain-ChatGLM" rel="nofollow">langchain-ChatGLM</a>是一个利用L<a href="https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fhwchase17%2Flangchain" rel="nofollow">angChain</a>思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案，其实现原理与LangChain官方的RetrievalQA基本一致，如图15所示：</p> 
<ul><li>加载本地知识库文档，支持目录和文件；</li><li>解析文档中的文本，支持Markdown、Words、PDF、TXT等格式；</li><li>切分文本得到文本段；</li><li>向量化文本段，向量化模型可配置，默认为<a href="https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FGanymedeNil%2Ftext2vec-large-chinese" rel="nofollow">GanymedeNil/text2vec-large-chinese</a>；</li><li>采用<a href="https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Ffaiss" rel="nofollow">Faiss</a>构建文本段向量的向量索引；</li><li>输入问题，向量化问题，并使用问题向量从向量索引中查找相关的文本段向量；</li><li>使用提示模板组合问题和相关文本段构建提示；</li><li>向语言大模型中输入提示，由模型进行推理，输出最终的答案，语言大模型可配置，既可以直接接入<a href="https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FTHUDM%2FChatGLM-6B" rel="nofollow">ChatGLM-6B</a>等大语言模型，也可以通过FastChat API形式接入Vicuna、Alpaca、LLaMA、Koala、RWKV等模型。</li></ul> 
<p><img src="https://images2.imgbox.com/8c/86/sqKnQgHc_o.png" alt="图15 langchain-ChatGLM实现原理"></p> 
<p>langchain-ChatGLM中使用的提示模板如下，其中“{question}”是提问的问题，“{context}”是将知识库中和问题相关的文本段用换行符拼接在一起：</p> 
<blockquote> 
 <p>已知信息： {context}</p> 
 <p>根据上述已知信息，简洁和专业的来回答用户的问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题” 或 “没有提供足够的相关信息”，不允许在答案中添加编造成分，答案请使用中文。 问题是：{question}</p> 
</blockquote> 
<h3><a id="_457"></a>部署</h3> 
<p>创建并激活环境：</p> 
<blockquote> 
 <p>conda create --name langchain-chatglm python=3.9 conda activate langchain-chatglm</p> 
</blockquote> 
<p>下载langchain-ChatGLM源码：</p> 
<blockquote> 
 <p>cd ~/workspace/ git clone <a href="https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2FimClumsyPanda%2Flangchain-ChatGLM" rel="nofollow">github.com/imClumsyPan…</a></p> 
</blockquote> 
<p>安装依赖：</p> 
<blockquote> 
 <p>cd ~/workspace/langchain-ChatGLM pip install -r requirements.txt</p> 
</blockquote> 
<p>安装依赖的过程中，可能会因为缺少Cmake、protobuf和swig导致依赖PyMuPDF和oonx安装失败，因此对Cmake、protobuf和swig进行安装：</p> 
<blockquote> 
 <p>brew install Cmake brew install protobuf@3 #需指定版本，否则会报版本不一致错误 brew install swig</p> 
</blockquote> 
<p>langchain-ChatGLM会使用模型进行自然语言文本的向量化，可以将这些模型下载到本地（若在RetrievalQA部分已下载，则无需再下载）：</p> 
<blockquote> 
 <p>cd ~/workspace/models/ git lfs install #若ChatGLM-6B部分已执行，则无需再执行 git clone <a href="https://link.juejin.cn?target=https%3A%2F%2Fhuggingface.co%2FGanymedeNil%2Ftext2vec-large-chinese" rel="nofollow">huggingface.co/GanymedeNil…</a></p> 
</blockquote> 
<p>修改configs/model_config.py，修改第19行，设置文本向量化模型text2vec的本地地址：</p> 
<pre><code class="prism language-python">python
复制代码<span class="token string">"text2vec"</span><span class="token punctuation">:</span> <span class="token string">"/Users/xxx/workspace/models/text2vec-large-chinese"</span><span class="token punctuation">,</span> <span class="token comment">#"text2vec": "GanymedeNil/text2vec-large-chinese", </span>
</code></pre> 
<p>修改第46行，设置ChatGLM2-6B-INT4的本地地址：</p> 
<pre><code class="prism language-python">python
复制代码<span class="token string">"local_model_path"</span><span class="token punctuation">:</span> <span class="token string">"/Users/xxx/workspace/models/chatglm2-6b-int4"</span><span class="token punctuation">,</span> <span class="token comment">#"local_model_path": None,</span>
</code></pre> 
<p>修改第114行，将大语言模型由ChatGLM-6B改为ChatGLM-6B-INT4（实际使用的是ChatGLM2-6B-INT4）：</p> 
<pre><code class="prism language-python">python
复制代码LLM_MODEL <span class="token operator">=</span> <span class="token string">"chatglm-6b-int4"</span> <span class="token comment">#LLM_MODEL = "chatglm-6b"</span>
</code></pre> 
<p>修改model/loader/loader.py第147行关于加载大语言模型的代码，删除或注释“to(self.llm_device)”：</p> 
<pre><code class="prism language-python">python复制代码model <span class="token operator">=</span> <span class="token punctuation">(</span>
    LoaderClass<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
        checkpoint<span class="token punctuation">,</span>
        config<span class="token operator">=</span>self<span class="token punctuation">.</span>model_config<span class="token punctuation">,</span>
        trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment">#.to(self.llm_device)</span>
<span class="token punctuation">)</span>
</code></pre> 
<p>实践中，“self.llm_device”的取值为“mps”（即使用并行处理），但若使用该设置，则会报以下错误：</p> 
<blockquote> 
 <p>File “/Users/xxx/.cache/huggingface/modules/transformers_modules/chatglm2-6b-int4/quantization.py”, line 54, in forward weight = extract_weight_to_half(quant_w, scale_w, weight_bit_width) File “/Users/xxx/.cache/huggingface/modules/transformers_modules/chatglm2-6b-int4/quantization.py”, line 261, in extract_weight_to_half assert scale_list.dtype in [torch.half, torch.bfloat16] AssertionError</p> 
</blockquote> 
<p>准备本地知识库，笔者使用<a href="https://link.juejin.cn?target=https%3A%2F%2Fwww.yuque.com%2Fhanxu-l5ahv%2Fguchgf%2Fpydrra" rel="nofollow">《超级汇川程序化创意产品手册》</a>这一文档，将其以Markdown格式下载至本地，读者也可以使用该文档或其他文档。 执行cli_demo.py：</p> 
<blockquote> 
 <p>python cli_demo.py</p> 
</blockquote> 
<p>按提示先指定本地知识库，本地知识库同时支持目录和文件，对于目录，会扫描其中的文件。langchain-ChatGLM会对文件内容进行切分、向量化并构建向量索引。随后可以提问和本地知识库相关的问题。langchain-ChatGLM对问题进行向量化并从向量索引中寻找语义相关的知识库内容，将问题和知识库内容按提示模板拼接在一起后作为大语言模型的输入由其进行推理，给出最终的回答，同时也列出与问题相关的知识库内容。执行结果如图16所示。</p> 
<p><img src="https://images2.imgbox.com/a1/06/yFfwP10z_o.png" alt="图16 langchain-ChatGLM示例执行结果"></p> 
<p>执行webui.py：</p> 
<blockquote> 
 <p>python webui.py</p> 
</blockquote> 
<p>启动后的WEB UI如图17所示。</p> 
<p><img src="https://images2.imgbox.com/27/a8/UjBf4u9A_o.png" alt="图17 langchain-ChatGLM WEB UI"></p> 
<h2><a id="_533"></a>结语</h2> 
<p>以上记录了在本地部署ChatGLM-6B、ChatGLM2-6B、LangChain、langChain-ChatGLM并进行推理的过程，不包含模型的微调。通过过程中的不断学习，对大语言模型及其周边生态、以及在多种场景下的应用，有了一定的了解。但将大语言模型应用在真实场景、发挥真正作用，还需要在语料搜集、模型微调、提示设计等方面针对业务特点进行不断的打磨。文章内容如有错误之处，欢迎指正和交流。另外，本地部署仅为了快速体验，目前也有很多免费的GPU云资源可以申请，例如<a href="https://link.juejin.cn?target=https%3A%2F%2Ffree.aliyun.com%2F%3Fproduct%3D9602825%26crowd%3Dpersonal" rel="nofollow">阿里云</a>，通过其可以在GPU云资源上进行模型的微调和推理。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0bf0c7a9441724effc4d1d2b4de6b506/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Faster-Whisper 实时识别电脑语音转文本</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/62199ef52d3d247b5f137fca0cb1104c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">图书|基于Springboot的图书管理系统设计与实现(源码&#43;数据库&#43;文档)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>