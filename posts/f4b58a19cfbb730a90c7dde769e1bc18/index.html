<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LLaMA-Factory：手把手教你从零微调大模型！ - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/f4b58a19cfbb730a90c7dde769e1bc18/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="LLaMA-Factory：手把手教你从零微调大模型！">
  <meta property="og:description" content="引言 随着人工智能技术的飞速发展，大型语言模型（LLM）在自然语言处理（NLP）领域扮演着越来越重要的角色。然而，预训练的模型往往需要针对特定任务进行微调，以提高其在特定领域的性能。LLaMA-Factory作为一个高效、易用的微调工具，为广大开发者提供了极大的便利。本文将详细介绍如何使用LLaMA-Factory从零开始微调大模型，帮助读者快速掌握这一技术。
一、模型微调讲解 1、什么是模型微调？ 在深度学习领域，模型微调通常指的是在预训练模型的基础上进行的进一步训练。预训练模型是在大量数据上训练得到的，它已经学习到了语言的基本规律和丰富的特征表示。然而，这些模型可能并不直接适用于特定的任务或领域，因为它们可能缺乏对特定领域知识的理解和适应性。
模型微调通过在特定任务的数据集上继续训练预训练模型来进行，使得模型能够学习到与任务相关的特定特征和知识。这个过程通常涉及到模型权重的微幅调整，而不是从头开始训练一个全新的模型。
2、微调的过程 微调过程主要包括以下几个步骤：
1)数据准备：收集和准备特定任务的数据集。
2)模型选择：选择一个预训练模型作为基础模型。
3)迁移学习：在新数据集上继续训练模型，同时保留预训练模型的知识。
4)参数调整：根据需要调整模型的参数，如学习率、批大小等。
5)模型评估：在验证集上评估模型的性能，并根据反馈进行调整。
3、微调的优势 微调技术带来了多方面的优势：
资源效率：相比于从头开始训练模型，微调可以显著减少所需的数据量和计算资源。
快速部署：微调可以快速适应新任务，加速模型的部署过程。
性能提升：针对特定任务的微调可以提高模型的准确性和鲁棒性。
领域适应性：微调可以帮助模型更好地理解和适应特定领域的语言特点。
通过微调，可以使得预训练模型在这些任务上取得更好的性能，更好地满足实际应用的需求。
二、LLaMA-Factory讲解 LLaMA-Factory是一个开源的模型微调框架，致力于简化大型语言模型的定制过程。它集成了多种训练策略和监控工具，提供了命令行和WebUI等多种交互方式，大幅降低了模型微调的技术门槛。
1、核心功能 多模型兼容：支持包括LLama、Mistral、Falcon在内的多种大型语言模型。训练方法多样：涵盖全参数微调及LoRA等先进的微调技术。用户界面友好：LLama Board提供了一个直观的Web界面，使用户能够轻松调整模型设置。监控工具集成：与TensorBoard等工具集成，便于监控和分析训练过程。 2、LLaMA-Factory特点 易用性：简化了机器学习算法的复杂性，通过图形界面即可控制模型微调。微调效率：支持DPO、ORPO、PPO和SFT等技术，提升了模型微调的效率和效果。参数调整灵活性：用户可根据需求轻松调整模型参数，如dropout率、epochs等。多语言支持：界面支持英语、俄语和中文，面向全球用户提供服务。 3、使用场景 LLaMA-Factory适用于广泛的NLP任务，包括但不限于：
文本分类：实现情感分析、主题识别等功能。序列标注：如NER、词性标注等任务。文本生成：自动生成文本摘要、对话等。机器翻译：优化特定语言对的翻译质量。 LLaMA-Factory通过其强大的功能和易用性，助力用户在自然语言处理领域快速实现模型的定制和优化。
三、安装LLaMA Factory 在本章节中，我们将指导您如何安装和设置LLaMA Factory，一个用于微调大型语言模型的工具。请按照以下步骤操作，以确保您能够顺利地使用LLaMA Factory。
1、准备工作 首先，确保您的开发环境中已经安装了Python3.9或更高版本。这可以通过Python的官方网站下载安装，或者使用包管理器进行安装。
1）显卡选择
24 GB显存的A10：建议使用至少这个规格的实例，或者更高规格的实例以满足可能更大的计算需求。
2）镜像选择：
PyTorch深度学习框架版本为2.1.2。Python 3.10、CUDA 11.2（cu121），CUDA是NVIDIA提供的用于通用并行计算的编程模型和API。、Ubuntu 22.04 LTS（长期支持版本）操作系统。
2、获取LLaMA-Factory 打开您的终端或命令行界面，然后执行以下命令来克隆LLaMA-Factory的代码仓库到本地：
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git 这将创建一个名为LLaMA-Factory的文件夹，包含所有必要的代码和文件。
3、安装依赖 在安装LLaMA-Factory之前，您需要确保安装了所有必要的依赖。进入克隆的仓库目录，然后执行以下命令来安装依赖：
cd LLaMA-Factory pip install -e .[metrics] 这个命令将安装LLaMA-Factory及其所有必需的附加组件，用于模型的评估和分析。
4、卸载可能冲突的包 如果在安装过程中与其他库发生冲突，您可能需要先卸载这些库。例如，如果vllm库与LLaMA-Factory不兼容，可以使用以下命令卸载：
pip uninstall -y vllm 5、LLaMA-Factory版本检查 安装完成后，您可以通过运行以下命令来检查LLaMA-Factory是否正确安装以及其版本号：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-26T16:58:27+08:00">
    <meta property="article:modified_time" content="2024-07-26T16:58:27+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">LLaMA-Factory：手把手教你从零微调大模型！</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_1"></a>引言</h2> 
<p>随着人工智能技术的飞速发展，大型语言模型（LLM）在自然语言处理（NLP）领域扮演着越来越重要的角色。然而，预训练的模型往往需要针对特定任务进行微调，以提高其在特定领域的性能。LLaMA-Factory作为一个高效、易用的微调工具，为广大开发者提供了极大的便利。本文将详细介绍如何使用LLaMA-Factory从零开始微调大模型，帮助读者快速掌握这一技术。</p> 
<h2><a id="_3"></a>一、模型微调讲解</h2> 
<h3><a id="1_4"></a>1、什么是模型微调？</h3> 
<p>在深度学习领域，模型微调通常指的是在预训练模型的基础上进行的进一步训练。预训练模型是在大量数据上训练得到的，它已经学习到了语言的基本规律和丰富的特征表示。然而，这些模型可能并不直接适用于特定的任务或领域，因为它们可能缺乏对特定领域知识的理解和适应性。</p> 
<p>模型微调通过在特定任务的数据集上继续训练预训练模型来进行，使得模型能够学习到与任务相关的特定特征和知识。这个过程通常涉及到模型权重的微幅调整，而不是从头开始训练一个全新的模型。</p> 
<h3><a id="2_9"></a>2、微调的过程</h3> 
<p>微调过程主要包括以下几个步骤：<br> 1)数据准备：收集和准备特定任务的数据集。<br> 2)模型选择：选择一个预训练模型作为基础模型。<br> 3)迁移学习：在新数据集上继续训练模型，同时保留预训练模型的知识。<br> 4)参数调整：根据需要调整模型的参数，如学习率、批大小等。<br> 5)模型评估：在验证集上评估模型的性能，并根据反馈进行调整。</p> 
<h3><a id="3_17"></a>3、微调的优势</h3> 
<p>微调技术带来了多方面的优势：<br> 资源效率：相比于从头开始训练模型，微调可以显著减少所需的数据量和计算资源。<br> 快速部署：微调可以快速适应新任务，加速模型的部署过程。<br> 性能提升：针对特定任务的微调可以提高模型的准确性和鲁棒性。<br> 领域适应性：微调可以帮助模型更好地理解和适应特定领域的语言特点。</p> 
<p>通过微调，可以使得预训练模型在这些任务上取得更好的性能，更好地满足实际应用的需求。</p> 
<h2><a id="LLaMAFactory_25"></a>二、LLaMA-Factory讲解</h2> 
<p>LLaMA-Factory是一个开源的模型微调框架，致力于简化大型语言模型的定制过程。它集成了多种训练策略和监控工具，提供了命令行和WebUI等多种交互方式，大幅降低了模型微调的技术门槛。<img src="https://images2.imgbox.com/4a/34/ihEpU0TE_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="1_29"></a>1、核心功能</h3> 
<ul><li>多模型兼容：支持包括LLama、Mistral、Falcon在内的多种大型语言模型。</li><li>训练方法多样：涵盖全参数微调及LoRA等先进的微调技术。</li><li>用户界面友好：LLama Board提供了一个直观的Web界面，使用户能够轻松调整模型设置。</li><li>监控工具集成：与TensorBoard等工具集成，便于监控和分析训练过程。</li></ul> 
<h3><a id="2LLaMAFactory_34"></a>2、LLaMA-Factory特点</h3> 
<ul><li>易用性：简化了机器学习算法的复杂性，通过图形界面即可控制模型微调。</li><li>微调效率：支持DPO、ORPO、PPO和SFT等技术，提升了模型微调的效率和效果。</li><li>参数调整灵活性：用户可根据需求轻松调整模型参数，如dropout率、epochs等。</li><li>多语言支持：界面支持英语、俄语和中文，面向全球用户提供服务。</li></ul> 
<h3><a id="3_39"></a>3、使用场景</h3> 
<p>LLaMA-Factory适用于广泛的NLP任务，包括但不限于：</p> 
<ul><li>文本分类：实现情感分析、主题识别等功能。</li><li>序列标注：如NER、词性标注等任务。</li><li>文本生成：自动生成文本摘要、对话等。</li><li>机器翻译：优化特定语言对的翻译质量。</li></ul> 
<p>LLaMA-Factory通过其强大的功能和易用性，助力用户在自然语言处理领域快速实现模型的定制和优化。</p> 
<h2><a id="LLaMA_Factory_47"></a>三、安装LLaMA Factory</h2> 
<p>在本章节中，我们将指导您如何安装和设置LLaMA Factory，一个用于微调大型语言模型的工具。请按照以下步骤操作，以确保您能够顺利地使用LLaMA Factory。</p> 
<h3><a id="1_49"></a>1、准备工作</h3> 
<p>首先，确保您的开发环境中已经安装了Python3.9或更高版本。这可以通过Python的官方网站下载安装，或者使用包管理器进行安装。<br> 1）显卡选择<br> 24 GB显存的A10：建议使用至少这个规格的实例，或者更高规格的实例以满足可能更大的计算需求。<br> 2）镜像选择：<br> PyTorch深度学习框架版本为2.1.2。Python 3.10、CUDA 11.2（cu121），CUDA是NVIDIA提供的用于通用并行计算的编程模型和API。、Ubuntu 22.04 LTS（长期支持版本）操作系统。</p> 
<h3><a id="2LLaMAFactory_56"></a>2、获取LLaMA-Factory</h3> 
<p>打开您的终端或命令行界面，然后执行以下命令来克隆LLaMA-Factory的代码仓库到本地：</p> 
<pre><code class="prism language-bash"><span class="token function">git</span> clone <span class="token parameter variable">--depth</span> <span class="token number">1</span> https://github.com/hiyouga/LLaMA-Factory.git
</code></pre> 
<p>这将创建一个名为<code>LLaMA-Factory</code>的文件夹，包含所有必要的代码和文件。</p> 
<h3><a id="3_62"></a>3、安装依赖</h3> 
<p>在安装LLaMA-Factory之前，您需要确保安装了所有必要的依赖。进入克隆的仓库目录，然后执行以下命令来安装依赖：</p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">cd</span> LLaMA-Factory
pip <span class="token function">install</span> <span class="token parameter variable">-e</span> .<span class="token punctuation">[</span>metrics<span class="token punctuation">]</span>
</code></pre> 
<p>这个命令将安装LLaMA-Factory及其所有必需的附加组件，用于模型的评估和分析。</p> 
<h3><a id="4_69"></a>4、卸载可能冲突的包</h3> 
<p>如果在安装过程中与其他库发生冲突，您可能需要先卸载这些库。例如，如果<code>vllm</code>库与LLaMA-Factory不兼容，可以使用以下命令卸载：</p> 
<pre><code class="prism language-bash">pip uninstall <span class="token parameter variable">-y</span> vllm
</code></pre> 
<h3><a id="5LLaMAFactory_74"></a>5、LLaMA-Factory版本检查</h3> 
<p>安装完成后，您可以通过运行以下命令来检查LLaMA-Factory是否正确安装以及其版本号：</p> 
<pre><code class="prism language-bash">llamafactory-cli version
</code></pre> 
<p>如果安装成功，您将看到类似以下的输出，显示LLaMA Factory的版本信息：</p> 
<pre><code>[2024-07-19 10:25:22,857] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Welcome to LLaMA Factory, version 0.7.1.dev0
</code></pre> 
<h3><a id="6_84"></a>6、验证安装</h3> 
<p>为了确保LLaMA Factory能够正常工作，您可以运行一些基本的命令来测试其功能。例如，尝试运行LLaMA Factory提供的一些示例脚本，或者使用其命令行界面来查看帮助信息：</p> 
<pre><code class="prism language-bash">llamafactory-cli <span class="token parameter variable">--help</span>
</code></pre> 
<p>这将列出所有可用的命令和选项，帮助您了解如何使用LLaMA Factory。</p> 
<p>注意事项</p> 
<ul><li>确保您的网络连接稳定，以便顺利下载代码仓库和安装依赖。</li><li>如果在安装过程中遇到问题，可以参考LLaMA Factory的官方文档或在社区中寻求帮助。</li><li>在安装过程中，您可能需要根据您的系统环境和配置调整上述命令。</li></ul> 
<p>通过遵循上述步骤，您将能够成功安装并开始使用LLaMA Factory进行大型语言模型的微调。在接下来的章节中，我们将深入探讨如何使用LLaMA Factory进行模型微调的具体操作。</p> 
<h2><a id="_98"></a>四、数据集准备</h2> 
<p>LLaMA-Factory提供了对多种数据集格式的支持，以适应不同类型的训练需求。本节将指导您如何准备和使用数据集进行模型微调。</p> 
<h3><a id="1_100"></a>1、使用内置数据集</h3> 
<p>LLaMA-Factory项目在<code>data</code>目录下内置了丰富的数据集，您可以直接使用这些数据集进行模型训练和测试。如果您不需要自定义数据集，可以跳过数据集准备步骤。</p> 
<h3><a id="2_102"></a>2、自定义数据集准备</h3> 
<p>若需使用自定义数据集，您需要按照LLaMA-Factory支持的格式处理数据，并将其放置在<code>data</code>目录下。同时，您还需要修改<code>dataset_info.json</code>文件，以确保数据集被正确识别和加载。</p> 
<h4><a id="21__104"></a>2.1 下载示例数据集</h4> 
<p>以下是使用示例数据集的步骤，假设您使用的是PAI提供的多轮对话数据集：</p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">cd</span> LLaMA-Factory
<span class="token function">wget</span> https://atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com/release/tutorials/llama_factory/data.zip
<span class="token function">unzip</span> data.zip <span class="token parameter variable">-d</span> data
</code></pre> 
<h4><a id="22__111"></a>2.2 查看数据集结构</h4> 
<p>数据集通常包含多轮对话样本，每轮对话由用户指令和模型回答组成。微调过程中，模型将学习这些样本的回答风格，以适应特定的语言风格或角色扮演需求。例如，数据集中的一个样本可能如下所示：</p> 
<pre><code>[
{
    "conversations": [
      {
        "from": "human",
        "value": "模仿西游记的孙悟空的语气和我聊天。如果你把水冻住，你会得到什么？"
      },
      {
        "from": "gpt",
        "value": "哈哈，这问题还难倒俺老孙吗？把水冻住就是变成冰啦！不过若是在取经的路上用得着这冰来做啥，比如过火焰山时，那可就是解渴消暑的好东西了！你觉得呢？还有没有更有趣的问题让俺老孙来解答？"
      },
      {
        "from": "human",
        "value": "那怎么做才可以把水冻住"
      },
      {
        "from": "gpt",
        "value": "哈哈，对俺老孙来说，这可是轻而易举的事情！只需将水放在凉快的地方，或者使用法力，一吹仙气，水便能迅速结冰啦！当然，在你的世界里，把水放进冰箱的冷冻室里等一阵子，水就会变成冰块了。要是嫌慢，可以尝试用点干冰或液氮什么的，不过得小心使用哈！毕竟不像俺老孙有抗寒法力。"
      }
    ]
}
]
</code></pre> 
<h2><a id="_137"></a>五、模型微调</h2> 
<h3><a id="1Web_UI_138"></a>1、启动Web UI</h3> 
<p>使用以下命令启动LLaMA-Factory的Web UI界面，以便进行交互式模型微调：</p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">export</span> <span class="token assign-left variable">USE_MODELSCOPE_HUB</span><span class="token operator">=</span><span class="token number">1</span> <span class="token operator">&amp;&amp;</span> llamafactory-cli webui
</code></pre> 
<p>这将启动一个本地Web服务器，您可以通过访问<code>http://0.0.0.0:7860</code>来使用Web UI。请注意，这是一个内网地址，只能在当前实例内部访问。</p> 
<h3><a id="2_144"></a>2、配置参数</h3> 
<p>在Web UI中，您需要配置以下关键参数以进行模型微调：<br> 语言：选择模型支持的语言，例如<code>zh</code>。<br> 模型名称：选择要微调的模型，例如<code>LLaMA3-8B-Chat</code>。<br> 微调方法：选择微调技术，如<code>lora</code>。<br> 数据集：选择用于训练的数据集。<br> 学习率：设置模型训练的学习率。<br> 计算类型：根据GPU类型选择计算精度，如<code>bf16</code>或<code>fp16</code>。<br> 梯度累计：设置梯度累计的批次数。<br> LoRA+学习率比例：设置LoRA+的相对学习率。<br> LoRA作用模块：选择LoRA层挂载的模型部分。<br> <img src="https://images2.imgbox.com/54/88/upWhZ04K_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="3_159"></a>3、开始微调</h3> 
<p>在Web UI中设置好参数后，您可以开始模型微调过程。微调完成后，您可以在界面上观察到训练进度和损失曲线。<br> <img src="https://images2.imgbox.com/57/81/FTGuWeIL_o.png" alt="在这里插入图片描述"></p> 
<p>1）将输出目录修改为train_llama3，训练后的LoRA权重将会保存在此目录中。<br> 2）单击“预览”命令，可展示所有已配置的参数。<br> 如果您希望通过代码进行微调，可以复制这段命令，在命令行运行。<br> 3）单击“开始”，启动模型微调。<br> 启动微调后需要等待大约20分钟，待模型下载完毕后，可在界面观察到训练进度和损失曲线。当显示训练完毕时，代表模型微调成功。<br> <img src="https://images2.imgbox.com/e4/65/AyEHoHqG_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_171"></a>六、模型评估</h2> 
<p>微调完成后，您可以使用Web UI进行模型评估。选择适当的评估数据集和输出目录，然后启动评估过程。评估结果将显示在界面上，包括ROUGE分数等指标。<br> <img src="https://images2.imgbox.com/10/ca/sh4wE8QJ_o.png" alt="在这里插入图片描述"></p> 
<p>模型评估大约需要5分钟，评估完成后会在界面上显示验证集的分数。其中，ROUGE分数衡量了模型输出答案（predict）和验证集中的标准答案（label）的相似度，ROUGE分数越高代表模型学习得越好。<br> <img src="https://images2.imgbox.com/60/40/h0GVIT1L_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_179"></a>七、对话测试</h2> 
<p>在Web UI的Chat页签下，加载微调后的模型进行对话测试。您可以输入文本与模型进行交互，并观察模型的回答是否符合预期。<br> <img src="https://images2.imgbox.com/d9/5b/j71tnI5I_o.png" alt="在这里插入图片描述"></p> 
<p>在页面底部的对话框输入想要和模型对话的内容，单击提交，即可发送消息。<br> 发送后模型会逐字生成回答，从回答中可以发现模型学习到了数据集中的内容，能够恰当地模仿目标角色的语气进行对话。<br> <img src="https://images2.imgbox.com/d9/12/0yiGrZUO_o.png" alt="在这里插入图片描述"></p> 
<p>单击卸载模型，单击取消适配器路径，然后单击加载模型，即可与微调前的原始模型聊天。<br> <img src="https://images2.imgbox.com/42/82/7T6vwvCm_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_191"></a>结语</h2> 
<p>本文精心梳理了使用LLaMA-Factory对大型语言模型进行微调的全过程。从精心准备数据集，到细致调整模型参数，再到严格评估模型性能，LLaMA-Factory以其高效、直观的操作界面，为自然语言处理领域提供了一项强大的工具。通过LLaMA-Factory，我们得以简化了微调流程，使得模型训练和优化变得更加易于管理和执行。它的出现，不仅提升了开发效率，也使得模型微调变得更加精准和个性化。</p> 
<p>我们期望，通过本文的介绍，读者能够对LLaMA-Factory有更深刻的理解，并将其应用于实际的模型开发中。愿您在自然语言处理的征途上，以LLaMA-Factory为伴，不断探索，勇往直前。</p> 
<p>让我们共同期待，LLaMA-Factory能在您的项目中发挥重要作用，助力您在AI领域取得新的突破和成就。感谢您的阅读，愿本文成为您技术探索之旅中的一盏明灯。</p> 
<p>博客原文：<a href="https://www.closeai.cc/" rel="nofollow">专业人工智能技术社区</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/915b8d7c37c9ee189792f056dba586c6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">masscan 端口扫描——（Golang 简单使用总结）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8b7b731c0cc5af40cd31f24162d4cf31/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">golang设置远程调试</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>