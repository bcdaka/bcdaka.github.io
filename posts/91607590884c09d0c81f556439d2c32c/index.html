<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>大模型【Qwen2-7B本地部署（WEB版）】（windows） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/91607590884c09d0c81f556439d2c32c/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="大模型【Qwen2-7B本地部署（WEB版）】（windows）">
  <meta property="og:description" content="大模型系列文章目录 Qwen2-7B本地部署（WEB版）
前言 大模型是截止2024年上半年最强的AI，Qwen2是刚出来的号称国内最强开源大模型。这是大模型系列的第一篇文章，旨在快速部署看看最近出来的大模型效果怎么样，效果ok的话就微调自己的GPTs了。
一、Ollama下载安装 进入官网点击Download选择Windos，点击Download for Windows (Preview)，这里fq会快很多默认安装 二、Qwen2下载安装 1.下载Qwen2 进入官方教程：https://qwen.readthedocs.io/zh-cn/latest/getting_started/installation.html先在最下面点击效率评估，看下各个模型占的显存，选择适合自己的，比如我的显卡是4070，有12G显存，我选择的模型就是Qwen2-7B-Instruct GPTQ-Int4进入下载链接会看到不同的后缀，q”&#43; 用于存储权重的位数（精度）&#43; 特定变体，数字越大性能越强。数字越大，精度越高，k是在所有的attention和feed_forward张量上将精度提升2位，m是在一半的attention和feed_forward张量上将精度提升2位。根据自己的需求选择模型，我这里直接选了q8。 2.运行Qwen2 新建一个文件夹，自己取个英文名（qwen），把qwen2-7b-instruct-q8_0.gguf移到文件夹里。在文件夹里新建一个名为Modelfile的文件，里面填 FROM ./qwen2-7b-instruct-q8_0.gguf 然后用命令行通过ollama创建Qwen2-7B模型： ollama create Qwen2-7B -f ./Modelfile 出现success代表创建成功
运行，输入命令 ollama run Qwen2-7B 出现对话框就可以聊天啦
如果要看本地有哪些大模型：ollama list
如果要删除这个模型的话：ollama rm xxx
如果要看运行了哪些大模型：ollama ps
但是在dos中聊天总感觉在上世纪的聊天方式一样，所以为了找到GPT的感觉，接下来我们继续在web中实现。
三、Node.js 1.Node.js下载安装 进入Node官网下载Node，安装验证node的版本： node -v v20以上就没问题
下载ollama-webui代码进入ollama-webui文件夹，设置国内镜像源提速： npm config set registry http://mirrors.cloud.tencent.com/npm/ 安装Node.js依赖： npm install 如果报错说要audit，则依次进行即可：
npm audit npm audit fix 启动Web界面： npm run dev 打开Web页面，选择你的模型即可开始对话：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-11T22:36:39+08:00">
    <meta property="article:modified_time" content="2024-07-11T22:36:39+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大模型【Qwen2-7B本地部署（WEB版）】（windows）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_1"></a>大模型系列文章目录</h2> 
<hr> 
<p><a href="https://blog.csdn.net/qq_37249793/article/details/140335099">Qwen2-7B本地部署（WEB版）</a></p> 
<h2><a id="_6"></a>前言</h2> 
<p>大模型是截止2024年上半年最强的AI，Qwen2是刚出来的号称国内最强开源大模型。这是大模型系列的第一篇文章，旨在快速部署看看最近出来的大模型效果怎么样，效果ok的话就微调自己的GPTs了。</p> 
<h2><a id="Ollama_11"></a>一、Ollama下载安装</h2> 
<ol><li>进入<a href="https://ollama.com/" rel="nofollow">官网</a></li><li>点击Download</li><li>选择Windos，点击Download for Windows (Preview)，这里fq会快很多</li><li>默认安装</li></ol> 
<h2><a id="Qwen2_20"></a>二、Qwen2下载安装</h2> 
<h3><a id="1Qwen2_21"></a>1.下载Qwen2</h3> 
<ol><li>进入官方教程：https://qwen.readthedocs.io/zh-cn/latest/getting_started/installation.html</li><li>先在最下面点击效率评估，看下各个模型占的显存，选择适合自己的，比如我的显卡是4070，有12G显存，我选择的模型就是Qwen2-7B-Instruct GPTQ-Int4<img src="https://images2.imgbox.com/3e/9e/bW9hC7Be_o.png" alt="请添加图片描述"></li><li>进入<a href="https://modelscope.cn/models/qwen/Qwen2-7B-Instruct-GGUF/files" rel="nofollow">下载链接</a></li><li>会看到不同的后缀，q”+ 用于存储权重的位数（精度）+ 特定变体，数字越大性能越强。</li><li>数字越大，精度越高，k是在所有的attention和feed_forward张量上将精度提升2位，m是在一半的attention和feed_forward张量上将精度提升2位。</li><li>根据自己的需求选择模型，我这里直接选了q8。</li></ol> 
<h3><a id="2Qwen2_29"></a>2.运行Qwen2</h3> 
<ol><li>新建一个文件夹，自己取个英文名（qwen），把qwen2-7b-instruct-q8_0.gguf移到文件夹里。</li><li>在文件夹里新建一个名为Modelfile的文件，里面填</li></ol> 
<pre><code class="prism language-bash">FROM ./qwen2-7b-instruct-q8_0.gguf
</code></pre> 
<ol start="3"><li>然后用命令行通过ollama创建Qwen2-7B模型：</li></ol> 
<pre><code class="prism language-bash">ollama create Qwen2-7B <span class="token parameter variable">-f</span> ./Modelfile
</code></pre> 
<p>出现success代表创建成功</p> 
<ol start="4"><li>运行，输入命令</li></ol> 
<pre><code class="prism language-bash">ollama run Qwen2-7B
</code></pre> 
<p>出现对话框就可以聊天啦<br> <img src="https://images2.imgbox.com/09/fa/fRlHNcAn_o.png" alt="请添加图片描述"></p> 
<p>如果要看本地有哪些大模型：ollama list<br> 如果要删除这个模型的话：ollama rm xxx<br> 如果要看运行了哪些大模型：ollama ps</p> 
<p>但是在dos中聊天总感觉在上世纪的聊天方式一样，所以为了找到GPT的感觉，接下来我们继续在web中实现。</p> 
<h2><a id="Nodejs_55"></a>三、Node.js</h2> 
<h3><a id="1Nodejs_56"></a>1.Node.js下载安装</h3> 
<ol><li>进入<a href="https://nodejs.org" rel="nofollow">Node官网</a>下载Node，安装</li><li>验证node的版本：</li></ol> 
<pre><code class="prism language-bash"><span class="token function">node</span> <span class="token parameter variable">-v</span>
</code></pre> 
<p>v20以上就没问题</p> 
<ol><li>下载<a href="https://github.com/ollama-webui/ollama-webui-lite">ollama-webui代码</a></li><li>进入ollama-webui文件夹，设置国内镜像源提速：</li></ol> 
<pre><code class="prism language-bash"><span class="token function">npm</span> config <span class="token builtin class-name">set</span> registry http://mirrors.cloud.tencent.com/npm/
</code></pre> 
<ol start="3"><li>安装Node.js依赖：</li></ol> 
<pre><code class="prism language-bash"><span class="token function">npm</span> <span class="token function">install</span>
</code></pre> 
<p>如果报错说要audit，则依次进行即可：</p> 
<pre><code class="prism language-bash"><span class="token function">npm</span> audit
<span class="token function">npm</span> audit fix
</code></pre> 
<ol start="4"><li>启动Web界面：</li></ol> 
<pre><code class="prism language-bash"><span class="token function">npm</span> run dev
</code></pre> 
<p>打开<a href="http://localhost:3000/" rel="nofollow">Web页面</a>，选择你的模型即可开始对话：<br> <img src="https://images2.imgbox.com/49/a1/RBC2cfjn_o.png" alt="请添加图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/567d157dcbd3aa74ab3f3c863cd0d9b7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">力扣 454四数相加</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a243bd8a01862d6255a5c3bbdf09dcb9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">在Ubuntu系统手撸一个自动创建SSL证书的SHELL脚本</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>