<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>[玩转AIGC]如何训练LLaMA2（模型训练、推理、代码讲解，并附可直接运行的kaggle连接） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/9149bb3ed8a37a2db712b369de71e54c/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="[玩转AIGC]如何训练LLaMA2（模型训练、推理、代码讲解，并附可直接运行的kaggle连接）">
  <meta property="og:description" content="目录 一、clone仓库二、数据集下载与处理1、数据集下载2、数据集tokenize预处理（耗时较长） 三、修改配置四、开始训练五、模型推理六、train.py训练代码讲解1、导包2、定义模型训练参数与相关设置3、加载模型配置4、迭代生成数据5、模型初始化6、设置自动混合精度与优化函数7、损失评估与学习率获取8、日志保存初始化9、循环训练 七、run.c推理代码讲解1、结构及内存管理2、模型初始化：读取checkpoint3、神经网络模块4、main函数入口 Llama 2，基于优化的 Transformer 架构，是Meta AI正式发布的最新一代开源大模型，一系列模型（7b、13b、70b）均开源可商用，效果直逼gpt3.5。
下面我们来介绍如何使用Llama 2来训练一个故事生成模型。
如果迫不及待想爽一把先，请直接跳到这里，可直接运行：llama2-c，学习不就是先让自己爽起来，而后才有欲望去深究为什么！！！
一、clone仓库 首先从github将仓库拉到本地：
git clone https://github.com/karpathy/llama2.c 进入目录
cd llama2.c 二、数据集下载与处理 注：具体内容可见[玩转AIGC]sentencepiece训练一个Tokenizer(标记器)这篇文章
1、数据集下载 python tinystories.py download 2、数据集tokenize预处理（耗时较长） python tinystories.py pretokenize 若运行到中途卡死了，可将并行运行的线程改小，打开tinystories.py，找到def pretolenize()函数，修改线程数max_workers=4：
# process all the shards in a threadpool with ThreadPoolExecutor(max_workers=4) as executor: executor.map(process_shard, shard_filenames) 注：在运行文末的kaggle代码时，不需要运行这一步，我已经提前编好的，只要运行!cp /kaggle/input/llama2tranningdatabin/databin/* /kaggle/working/llama2.c/data/TinyStories_all_data就可把文件拷贝到对应目录下，因为编号太久了，而且在kaggle里面容易挂掉
三、修改配置 然后修改一些配置：
train.py里面有几个参数要修改
batch_size改小一点，否则会报’CUDA out of memory’ 的错误（土豪卡多随意，不介意的话送我一张）
dtype要改为&#34;float16&#34;，否则会报’Current CUDA Device does not support bfloat16’的错误
compile要改为False，否则会报CUDA Capability过低或complex64不支持的错误
batch_size = 64 dtype = &#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-02-26T09:53:14+08:00">
    <meta property="article:modified_time" content="2024-02-26T09:53:14+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">[玩转AIGC]如何训练LLaMA2（模型训练、推理、代码讲解，并附可直接运行的kaggle连接）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#clone_11" rel="nofollow">一、clone仓库</a></li><li><a href="#_20" rel="nofollow">二、数据集下载与处理</a></li><li><ul><li><a href="#1_23" rel="nofollow">1、数据集下载</a></li><li><a href="#2tokenize_27" rel="nofollow">2、数据集tokenize预处理（耗时较长）</a></li></ul> 
  </li><li><a href="#_40" rel="nofollow">三、修改配置</a></li><li><a href="#_61" rel="nofollow">四、开始训练</a></li><li><a href="#_92" rel="nofollow">五、模型推理</a></li><li><a href="#trainpy_125" rel="nofollow">六、train.py训练代码讲解</a></li><li><ul><li><a href="#1_127" rel="nofollow">1、导包</a></li><li><a href="#2_144" rel="nofollow">2、定义模型训练参数与相关设置</a></li><li><a href="#3_241" rel="nofollow">3、加载模型配置</a></li><li><a href="#4_280" rel="nofollow">4、迭代生成数据</a></li><li><a href="#5_292" rel="nofollow">5、模型初始化</a></li><li><a href="#6_362" rel="nofollow">6、设置自动混合精度与优化函数</a></li><li><a href="#7_377" rel="nofollow">7、损失评估与学习率获取</a></li><li><a href="#8_419" rel="nofollow">8、日志保存初始化</a></li><li><a href="#9_428" rel="nofollow">9、循环训练</a></li></ul> 
  </li><li><a href="#runc_516" rel="nofollow">七、run.c推理代码讲解</a></li><li><ul><li><a href="#1_518" rel="nofollow">1、结构及内存管理</a></li><li><a href="#2checkpoint_626" rel="nofollow">2、模型初始化：读取checkpoint</a></li><li><a href="#3_678" rel="nofollow">3、神经网络模块</a></li><li><a href="#4main_908" rel="nofollow">4、main函数入口</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<p>Llama 2，基于优化的 Transformer 架构，是Meta AI正式发布的最新一代开源大模型，一系列模型（7b、13b、70b）均开源可商用，效果直逼gpt3.5。</p> 
<p>下面我们来介绍如何使用Llama 2来训练一个故事生成模型。</p> 
<p>如果迫不及待想爽一把先，请直接跳到这里，可直接运行：<a href="https://www.kaggle.com/code/sixfive/llama2-c" rel="nofollow">llama2-c</a>，<font color="red">学习不就是先让自己爽起来，而后才有欲望去深究为什么！！！</font></p> 
<h2><a id="clone_11"></a>一、clone仓库</h2> 
<p>首先从github将仓库拉到本地：</p> 
<pre><code class="prism language-shell"><span class="token function">git</span> clone https://github.com/karpathy/llama2.c
</code></pre> 
<p>进入目录</p> 
<pre><code class="prism language-shell"><span class="token builtin class-name">cd</span> llama2.c
</code></pre> 
<h2><a id="_20"></a>二、数据集下载与处理</h2> 
<blockquote> 
 <p>注：具体内容可见<a href="https://blog.csdn.net/qq_27149279/article/details/131976119">[玩转AIGC]sentencepiece训练一个Tokenizer(标记器)</a>这篇文章</p> 
</blockquote> 
<h3><a id="1_23"></a>1、数据集下载</h3> 
<pre><code class="prism language-shell">python tinystories.py download
</code></pre> 
<h3><a id="2tokenize_27"></a>2、数据集tokenize预处理（耗时较长）</h3> 
<pre><code class="prism language-shell">python tinystories.py pretokenize
</code></pre> 
<p>若运行到中途卡死了，可将并行运行的线程改小，打开tinystories.py，找到def pretolenize()函数，修改线程数max_workers=4：</p> 
<pre><code class="prism language-python">    <span class="token comment"># process all the shards in a threadpool</span>
    <span class="token keyword">with</span> ThreadPoolExecutor<span class="token punctuation">(</span>max_workers<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span> <span class="token keyword">as</span> executor<span class="token punctuation">:</span>
        executor<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>process_shard<span class="token punctuation">,</span> shard_filenames<span class="token punctuation">)</span>
</code></pre> 
<p>注：在运行文末的kaggle代码时，不需要运行这一步，我已经提前编好的，只要运行<code>!cp /kaggle/input/llama2tranningdatabin/databin/* /kaggle/working/llama2.c/data/TinyStories_all_data</code>就可把文件拷贝到对应目录下，因为编号太久了，而且在kaggle里面容易挂掉</p> 
<h2><a id="_40"></a>三、修改配置</h2> 
<p>然后修改一些配置：<br> <code>train.py</code>里面有几个参数要修改<br> batch_size改小一点，否则会报’CUDA out of memory’ 的错误（土豪卡多随意，不介意的话送我一张）<br> dtype要改为"float16"，否则会报’Current CUDA Device does not support bfloat16’的错误<br> compile要改为False，否则会报CUDA Capability过低或complex64不支持的错误</p> 
<pre><code class="prism language-python">batch_size <span class="token operator">=</span> <span class="token number">64</span>
dtype <span class="token operator">=</span> <span class="token string">"float16"</span>
<span class="token builtin">compile</span> <span class="token operator">=</span> <span class="token boolean">False</span>
</code></pre> 
<p>可选改的参数：<br> max_iters：是迭代次数，可改小一点，在kaggle里面运行一代是1.2s，运行100000代的话，大概还得34小时，而kaggle免费时长最长也才30小时，且一次运行最长12小时，所以要改小（还是那句话，土豪有卡的随意）<br> warmup_iters：是热身的迭代次数，主要是为了确定合适得学习率，卡有限的话可改小一些。</p> 
<pre><code>max_iters = 100000
warmup_iters = 1000
</code></pre> 
<p>注：在运行文末的kaggle代码时，这里不用自己修改了，直接把我修改好的复制过来即可，运行<code>!cp /kaggle/input/llama2trainpy4/train.py ./train.py</code>即可复制，因为kaggle里面也手动改不了，能改也是很麻烦。</p> 
<h2><a id="_61"></a>四、开始训练</h2> 
<pre><code>python train.py
</code></pre> 
<p><img src="https://images2.imgbox.com/b4/b9/rIvzm36I_o.png" alt="在这里插入图片描述"><br> 训练后可在out文件夹下看到以下文件：<br> <img src="https://images2.imgbox.com/03/34/fkNGWGRl_o.png" alt="在这里插入图片描述"></p> 
<p>当然还可以在命令行中指定训练参数(单GPU上训练)：</p> 
<pre><code class="prism language-shell">python <span class="token parameter variable">-m</span> train.py <span class="token parameter variable">--compile</span><span class="token operator">=</span>False <span class="token parameter variable">--eval_iters</span><span class="token operator">=</span><span class="token number">10</span> <span class="token parameter variable">--batch_size</span><span class="token operator">=</span><span class="token number">8</span>
</code></pre> 
<p>如果是多GPU，可采用分布式训练，例如采用DDP 在1个node，4个 gpu 上训练：</p> 
<pre><code class="prism language-shell">torchrun <span class="token parameter variable">--standalone</span> <span class="token parameter variable">--nproc_per_node</span><span class="token operator">=</span><span class="token number">4</span> train.py
</code></pre> 
<p>采用DDP 在2个node，4个 gpu 上训练：</p> 
<pre><code class="prism language-shell">torchrun <span class="token parameter variable">--nproc_per_node</span><span class="token operator">=</span><span class="token number">8</span> <span class="token parameter variable">--nnodes</span><span class="token operator">=</span><span class="token number">2</span> <span class="token parameter variable">--node_rank</span><span class="token operator">=</span><span class="token number">0</span> <span class="token parameter variable">--master_addr</span><span class="token operator">=</span><span class="token number">123.456</span>.123.456 <span class="token parameter variable">--master_port</span><span class="token operator">=</span><span class="token number">1234</span> train.py
</code></pre> 
<p>注：如果觉得训练太久了，可以跑我训练出来的模型，拷贝过来就可以了：</p> 
<pre><code class="prism language-shell"><span class="token operator">!</span>cp /kaggle/input/llama2-out-model/* ./out
</code></pre> 
<p>如果你没运行过train.py，那就是不会有llama2/out这个目录，这时候要先创建好out这个目录：</p> 
<pre><code class="prism language-shell"><span class="token function">mkdir</span> out
</code></pre> 
<h2><a id="_92"></a>五、模型推理</h2> 
<p>先编译run（只需要运行一次）</p> 
<pre><code class="prism language-shell"><span class="token function">make</span> run
</code></pre> 
<p>运行模型推理</p> 
<pre><code class="prism language-shell">./run out/model.bin
</code></pre> 
<p>输出如下：</p> 
<blockquote> 
 <p>Once upon a time there was a little bear named manner. He was very playful, and he loved to jump around. One day, he saw a big pillow in his den. It was bright blue, just like the sky, and it seemed just right for him. He decided he would try to jump on it.<br> He pASHed into the air, feeling the pillow with his fluffy paws and carried on the giant. He bounced up and down, dodgingages and twists. He felt so free and happy as he jumped higher and higher.<br> But then he started to struggle. The pillow was too hard and he couldn’t jump on it! He tried and tried and even though he was tired and frustrated, he couldn’t take a kick. Finally, he gave up. But he still felt free, like nothing could stop him.<br> Close was tired, panting and relieved at its same time. He jumped and flew away, still feeling happy after exploring the sky. He curled up on the pillow to take his nap and finally fell asleep, dreaming of himself jumping across the sky.</p> 
</blockquote> 
<p>感兴趣的话，也可以跑其它模型试试：</p> 
<pre><code class="prism language-shell"><span class="token function">wget</span> https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin
./run stories15M.bin

<span class="token function">wget</span> https://huggingface.co/karpathy/tinyllamas/resolve/main/stories42M.bin
./run stories42M.bin
</code></pre> 
<p>当然还可指定参数运行：</p> 
<pre><code class="prism language-shell">./run stories42M.bin <span class="token number">1.0</span> <span class="token number">256</span> <span class="token string">"One day, Lily met a Shoggoth"</span>
</code></pre> 
<blockquote> 
 <p>注：在本小节末尾的kaggle代码时，可能会报找不到triton模块，此时运行<code>!pip3 install triton</code>，进行安装即可。</p> 
</blockquote> 
<p>以上可在kaggle运行：<a href="https://www.kaggle.com/code/sixfive/llama2-c" rel="nofollow">llama2-c</a></p> 
<h2><a id="trainpy_125"></a>六、train.py训练代码讲解</h2> 
<blockquote> 
 <p>注：为了方便讲解，已经把部分ddp（分布式训练，多gpu可用，我的kaggle是单gpu，所以去掉了）跟wandb_log（记录日志）去掉了，整体代码在<strong>train.py</strong>里面，此处不再赘述了。</p> 
</blockquote> 
<h3><a id="1_127"></a>1、导包</h3> 
<pre><code class="prism language-python"><span class="token keyword">import</span> math
<span class="token keyword">import</span> os
<span class="token keyword">import</span> time
<span class="token keyword">from</span> contextlib <span class="token keyword">import</span> nullcontext
<span class="token keyword">from</span> datetime <span class="token keyword">import</span> datetime
<span class="token keyword">from</span> functools <span class="token keyword">import</span> partial

<span class="token keyword">import</span> torch
<span class="token keyword">from</span> model <span class="token keyword">import</span> Transformer<span class="token punctuation">,</span> ModelArgs <span class="token comment"># 在model.py里的模型</span>
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>distributed <span class="token keyword">import</span> destroy_process_group<span class="token punctuation">,</span> init_process_group
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel <span class="token keyword">import</span> DistributedDataParallel <span class="token keyword">as</span> DDP
<span class="token comment">#import torch._dynamo</span>

<span class="token keyword">from</span> tinystories <span class="token keyword">import</span> Task
</code></pre> 
<h3><a id="2_144"></a>2、定义模型训练参数与相关设置</h3> 
<pre><code class="prism language-python"><span class="token comment"># -----------------------------------------------------------------------------</span>
<span class="token comment"># I/O</span>
out_dir <span class="token operator">=</span> <span class="token string">"out"</span>
eval_interval <span class="token operator">=</span> <span class="token number">2000</span>
log_interval <span class="token operator">=</span> <span class="token number">1</span>
eval_iters <span class="token operator">=</span> <span class="token number">100</span>
eval_only <span class="token operator">=</span> <span class="token boolean">False</span>  <span class="token comment"># if True, script exits right after the first eval</span>
always_save_checkpoint <span class="token operator">=</span> <span class="token boolean">False</span>  <span class="token comment"># if True, always save a checkpoint after each eval</span>
init_from <span class="token operator">=</span> <span class="token string">"scratch"</span>  <span class="token comment"># 'scratch' or 'resume'</span>
<span class="token comment"># wandb logging</span>
wandb_log <span class="token operator">=</span> <span class="token boolean">False</span>  <span class="token comment"># disabled by default</span>
wandb_project <span class="token operator">=</span> <span class="token string">"llamac"</span>
wandb_run_name <span class="token operator">=</span> <span class="token string">"run"</span> <span class="token operator">+</span> datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strftime<span class="token punctuation">(</span><span class="token string">"%Y_%m_%d_%H_%M_%S"</span><span class="token punctuation">)</span>
<span class="token comment"># data</span>
batch_size <span class="token operator">=</span> <span class="token number">64</span>  <span class="token comment"># if gradient_accumulation_steps &gt; 1, this is the micro-batch size</span>
max_seq_len <span class="token operator">=</span> <span class="token number">256</span>
<span class="token comment"># model</span>
dim <span class="token operator">=</span> <span class="token number">288</span>
n_layers <span class="token operator">=</span> <span class="token number">6</span>
n_heads <span class="token operator">=</span> <span class="token number">6</span>
multiple_of <span class="token operator">=</span> <span class="token number">32</span>
dropout <span class="token operator">=</span> <span class="token number">0.0</span>
<span class="token comment"># adamw optimizer</span>
gradient_accumulation_steps <span class="token operator">=</span> <span class="token number">4</span>  <span class="token comment"># used to simulate larger batch sizes</span>
learning_rate <span class="token operator">=</span> <span class="token number">5e-4</span>  <span class="token comment"># max learning rate</span>
max_iters <span class="token operator">=</span> <span class="token number">5000</span>  <span class="token comment"># total number of training iterations</span>
weight_decay <span class="token operator">=</span> <span class="token number">1e-1</span>
beta1 <span class="token operator">=</span> <span class="token number">0.9</span>
beta2 <span class="token operator">=</span> <span class="token number">0.95</span>
grad_clip <span class="token operator">=</span> <span class="token number">1.0</span>  <span class="token comment"># clip gradients at this value, or disable if == 0.0</span>
<span class="token comment"># learning rate decay settings</span>
decay_lr <span class="token operator">=</span> <span class="token boolean">True</span>  <span class="token comment"># whether to decay the learning rate</span>
warmup_iters <span class="token operator">=</span> <span class="token number">100</span>  <span class="token comment"># how many steps to warm up for</span>
<span class="token comment"># system</span>
device <span class="token operator">=</span> <span class="token string">"cuda"</span>  <span class="token comment"># examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks</span>
dtype <span class="token operator">=</span> <span class="token string">"float16"</span>  <span class="token comment"># float32|bfloat16|float16</span>
<span class="token builtin">compile</span> <span class="token operator">=</span> <span class="token boolean">False</span>  <span class="token comment"># use PyTorch 2.0 to compile the model to be faster</span>
<span class="token comment"># -----------------------------------------------------------------------------</span>

</code></pre> 
<p>下面是各个参数的解释：</p> 
<pre><code class="prism language-text">I/O

    out_dir: 模型训练输出路径。
    eval_interval: 多少个训练步骤后进行一次模型评估。
    log_interval: 多少个训练步骤后进行一次日志记录。
    eval_iters: 在进行模型评估时，评估器将处理多少个数据集条目。
    eval_only: 如果为 True，则仅进行一次模型评估并退出脚本。
    always_save_checkpoint: 如果为 True，则在每次模型评估后始终保存一个检查点。
    init_from: 模型初始化方法，可以是 'scratch'（从头开始训练）或 'resume'（从之前的检查点恢复）。

wandb 日志记录

    wandb_log: 是否启用 wandb 日志记录。
    wandb_project: wandb 项目名称。
    wandb_run_name: wandb 运行名称。

数据

    batch_size: 训练的批次大小。
    max_seq_len: 输入序列的最大长度。

模型

    dim: Transformer 模型中的隐藏层维度。
    n_layers: Transformer 模型中的层数。
    n_heads: Transformer 模型中的多头注意力头数。
    multiple_of: 批次大小必须是此值的倍数。在 TPU 上训练时，批次大小必须是 8 的倍数。
    dropout: 模型中的 dropout 概率。

adamw 优化器

    gradient_accumulation_steps: 用于模拟较大批次大小的梯度累积步骤数。
    learning_rate: 最大学习率。
    max_iters: 总训练迭代次数。
    weight_decay: AdamW 优化器中的权重衰减系数。
    beta1: AdamW 优化器的 beta1 超参数。
    beta2: AdamW 优化器的 beta2 超参数。
    grad_clip: 梯度裁剪值，如果设为 0.0 则不进行梯度裁剪。

学习率衰减设置

    decay_lr: 是否对学习率进行衰减。
    warmup_iters: 学习率预热步骤数。

系统

    device: 训练设备，可以是 'cpu'、'cuda'、'cuda:0'、'cuda:1' 等，或在 MacBook 上尝试 'mps'。
    dtype: 训练数据类型，可以是 'float32'、'bfloat16' 或 'float16'。
    compile: 是否使用 PyTorch 2.0 编译模型以提高速度。

</code></pre> 
<h3><a id="3_241"></a>3、加载模型配置</h3> 
<pre><code class="prism language-python"><span class="token comment"># -----------------------------------------------------------------------------</span>
config_keys <span class="token operator">=</span> <span class="token punctuation">[</span>
    k
    <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> <span class="token builtin">globals</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token keyword">not</span> k<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">"_"</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>v<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">,</span> <span class="token builtin">bool</span><span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span>
<span class="token keyword">exec</span><span class="token punctuation">(</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"configurator.py"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># overrides from command line or config file</span>
config <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>k<span class="token punctuation">:</span> <span class="token builtin">globals</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> config_keys<span class="token punctuation">}</span>  <span class="token comment"># will be useful for logging</span>
<span class="token comment"># -----------------------------------------------------------------------------</span>

<span class="token comment"># fixing some hyperparams to sensible defaults</span>
lr_decay_iters <span class="token operator">=</span> max_iters  <span class="token comment"># should be ~= max_iters per Chinchilla</span>
min_lr <span class="token operator">=</span> <span class="token number">0.0</span>  <span class="token comment"># minimum learning rate, should be ~= learning_rate/10 per Chinchilla</span>

master_process <span class="token operator">=</span> <span class="token boolean">True</span>
seed_offset <span class="token operator">=</span> <span class="token number">0</span>
ddp_world_size <span class="token operator">=</span> <span class="token number">1</span>

tokens_per_iter <span class="token operator">=</span> gradient_accumulation_steps <span class="token operator">*</span> ddp_world_size <span class="token operator">*</span> batch_size <span class="token operator">*</span> max_seq_len
<span class="token keyword">if</span> master_process<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"tokens per iteration will be: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>tokens_per_iter<span class="token punctuation">:</span><span class="token format-spec">,</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"breaks down as: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>gradient_accumulation_steps<span class="token punctuation">}</span></span><span class="token string"> grad accum steps * </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>ddp_world_size<span class="token punctuation">}</span></span><span class="token string"> processes * </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>batch_size<span class="token punctuation">}</span></span><span class="token string"> batch size * </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>max_seq_len<span class="token punctuation">}</span></span><span class="token string"> max seq len"</span></span><span class="token punctuation">)</span>

<span class="token keyword">if</span> master_process<span class="token punctuation">:</span>
    os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span>out_dir<span class="token punctuation">,</span> exist_ok<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">1337</span> <span class="token operator">+</span> seed_offset<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>matmul<span class="token punctuation">.</span>allow_tf32 <span class="token operator">=</span> <span class="token boolean">True</span>  <span class="token comment"># allow tf32 on matmul</span>
torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>allow_tf32 <span class="token operator">=</span> <span class="token boolean">True</span>  <span class="token comment"># allow tf32 on cudnn</span>
device_type <span class="token operator">=</span> <span class="token string">"cuda"</span> <span class="token keyword">if</span> <span class="token string">"cuda"</span> <span class="token keyword">in</span> device <span class="token keyword">else</span> <span class="token string">"cpu"</span>  <span class="token comment"># for later use in torch.autocast</span>
<span class="token comment"># note: float16 data type will automatically use a GradScaler</span>
ptdtype <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">"float32"</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token string">"bfloat16"</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>bfloat16<span class="token punctuation">,</span> <span class="token string">"float16"</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>float16<span class="token punctuation">}</span><span class="token punctuation">[</span>dtype<span class="token punctuation">]</span>
ctx <span class="token operator">=</span> <span class="token punctuation">(</span>
    nullcontext<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> device_type <span class="token operator">==</span> <span class="token string">"cpu"</span>
    <span class="token keyword">else</span> torch<span class="token punctuation">.</span>amp<span class="token punctuation">.</span>autocast<span class="token punctuation">(</span>device_type<span class="token operator">=</span>device_type<span class="token punctuation">,</span> dtype<span class="token operator">=</span>ptdtype<span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="4_280"></a>4、迭代生成数据</h3> 
<p>用于将输入数据集划分为若干个大小相同的批次，并对每个批次进行预处理和编码，以便于送入模型进行训练。在模型训练过程中，需要不断地生成训练数据批次，然后将这些批次送入模型进行训练。因此，这个新函数 iter_batches 就是用于迭代生成数据批次的迭代器函数，它会在每个迭代步骤中调用 Task.iter_batches() 函数生成一个数据批次，并返回给调用者。这样，就可以通过简单的 for 循环来不断地生成数据批次，然后将这些批次送入模型进行训练。</p> 
<pre><code class="prism language-python"><span class="token comment"># task-specific setup</span>
iter_batches <span class="token operator">=</span> partial<span class="token punctuation">(</span>
    Task<span class="token punctuation">.</span>iter_batches<span class="token punctuation">,</span>
    batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span>
    max_seq_len<span class="token operator">=</span>max_seq_len<span class="token punctuation">,</span>
    device<span class="token operator">=</span>device<span class="token punctuation">,</span>
    num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="5_292"></a>5、模型初始化</h3> 
<p>iter_num: 当前迭代次数<br> best_val_loss:最佳验证集损失值<br> 以上两个值用于记录模型训练的状态。这些变量的初始值为 0 和一个较大的数值，表示模型训练尚未开始，最佳验证集损失值尚未确定。</p> 
<pre><code class="prism language-python"><span class="token comment"># init these up here, can override if init_from='resume' (i.e. from a checkpoint)</span>
iter_num <span class="token operator">=</span> <span class="token number">0</span>
best_val_loss <span class="token operator">=</span> <span class="token number">1e9</span>
</code></pre> 
<p>使用 ModelArgs 类来设置模型参数 model_args</p> 
<pre><code># model init
model_args = dict(
    dim=dim,
    n_layers=n_layers,
    n_heads=n_heads,
    n_kv_heads=n_heads,
    vocab_size=32000,
    multiple_of=multiple_of,
    max_seq_len=max_seq_len,
    dropout=dropout,
)  # start with model_args from command line
</code></pre> 
<p>然后，根据 init_from 参数的取值来初始化模型。如果 init_from 的值为 “scratch”，则表示从头开始训练一个新模型，如果 init_from 的值为 “resume”，则表示从之前的训练中恢复训练。</p> 
<p>先来看：init_from == “scratch”<br> 直接根据model_args创建参数类，然后通过 Transformer 类来创建一个新的 Transformer 模型 model。</p> 
<pre><code class="prism language-python"><span class="token keyword">if</span> init_from <span class="token operator">==</span> <span class="token string">"scratch"</span><span class="token punctuation">:</span>
    <span class="token comment"># init a new model from scratch</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Initializing a new model from scratch"</span><span class="token punctuation">)</span>
    gptconf <span class="token operator">=</span> ModelArgs<span class="token punctuation">(</span><span class="token operator">**</span>model_args<span class="token punctuation">)</span>
    model <span class="token operator">=</span> Transformer<span class="token punctuation">(</span>gptconf<span class="token punctuation">)</span>
</code></pre> 
<p>再来看：init_from == “resume”<br> 1）先从checkpoint加载模型（“ckpt.pt”）<br> 2）然后根据model_args创建参数类，然后通过 Transformer 类来创建一个新的 Transformer 模型 model。<br> 3）最后修复模型状态字典中的键名，因为有时候在保存模型的时候会在键名前面加上不必要的前缀，导致在加载模型时出现问题。坦白说我们也不清楚这个前缀是怎么产生的，需要进一步进行调试和研究。</p> 
<pre><code class="prism language-python"><span class="token keyword">elif</span> init_from <span class="token operator">==</span> <span class="token string">"resume"</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Resuming training from </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>out_dir<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    <span class="token comment"># resume training from a checkpoint.</span>
    ckpt_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>out_dir<span class="token punctuation">,</span> <span class="token string">"ckpt.pt"</span><span class="token punctuation">)</span>
    checkpoint <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>ckpt_path<span class="token punctuation">,</span> map_location<span class="token operator">=</span>device<span class="token punctuation">)</span>
    checkpoint_model_args <span class="token operator">=</span> checkpoint<span class="token punctuation">[</span><span class="token string">"model_args"</span><span class="token punctuation">]</span>
    <span class="token comment"># force these config attributes to be equal otherwise we can't even resume training</span>
    <span class="token comment"># the rest of the attributes (e.g. dropout) can stay as desired from command line</span>
    <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"dim"</span><span class="token punctuation">,</span> <span class="token string">"n_layers"</span><span class="token punctuation">,</span> <span class="token string">"n_heads"</span><span class="token punctuation">,</span> <span class="token string">"n_kv_heads"</span><span class="token punctuation">,</span> <span class="token string">"vocab_size"</span><span class="token punctuation">,</span> <span class="token string">"multiple_of"</span><span class="token punctuation">,</span> <span class="token string">"max_seq_len"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        model_args<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> checkpoint_model_args<span class="token punctuation">[</span>k<span class="token punctuation">]</span>
    <span class="token comment"># create the model</span>
    gptconf <span class="token operator">=</span> ModelArgs<span class="token punctuation">(</span><span class="token operator">**</span>model_args<span class="token punctuation">)</span>
    model <span class="token operator">=</span> Transformer<span class="token punctuation">(</span>gptconf<span class="token punctuation">)</span>
    state_dict <span class="token operator">=</span> checkpoint<span class="token punctuation">[</span><span class="token string">"model"</span><span class="token punctuation">]</span>
    <span class="token comment"># fix the keys of the state dictionary :(</span>
    <span class="token comment"># honestly no idea how checkpoints sometimes get this prefix, have to debug more</span>
    unwanted_prefix <span class="token operator">=</span> <span class="token string">"_orig_mod."</span>
    <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> <span class="token builtin">list</span><span class="token punctuation">(</span>state_dict<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> k<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span>unwanted_prefix<span class="token punctuation">)</span><span class="token punctuation">:</span>
            state_dict<span class="token punctuation">[</span>k<span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>unwanted_prefix<span class="token punctuation">)</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> state_dict<span class="token punctuation">.</span>pop<span class="token punctuation">(</span>k<span class="token punctuation">)</span>
    model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>state_dict<span class="token punctuation">)</span>
    iter_num <span class="token operator">=</span> checkpoint<span class="token punctuation">[</span><span class="token string">"iter_num"</span><span class="token punctuation">]</span>
    best_val_loss <span class="token operator">=</span> checkpoint<span class="token punctuation">[</span><span class="token string">"best_val_loss"</span><span class="token punctuation">]</span>
</code></pre> 
<p>无论是从头开始训练新模型还是从之前的训练中恢复模型，最后都将模型 model 移动到指定的设备 device 上进行训练。</p> 
<pre><code class="prism language-python">model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="6_362"></a>6、设置自动混合精度与优化函数</h3> 
<p>scaler：设置自动混合精度<br> optimizer：优化函数<br> 如果是恢复训练的，并且checkpoint里面存在optimizer，那么optimizer会从checkpoint里面加载</p> 
<pre><code class="prism language-python"><span class="token comment"># initialize a GradScaler. If enabled=False scaler is a no-op</span>
scaler <span class="token operator">=</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>amp<span class="token punctuation">.</span>GradScaler<span class="token punctuation">(</span>enabled<span class="token operator">=</span><span class="token punctuation">(</span>dtype <span class="token operator">==</span> <span class="token string">"float16"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># optimizer</span>
optimizer <span class="token operator">=</span> model<span class="token punctuation">.</span>configure_optimizers<span class="token punctuation">(</span>weight_decay<span class="token punctuation">,</span> learning_rate<span class="token punctuation">,</span> <span class="token punctuation">(</span>beta1<span class="token punctuation">,</span> beta2<span class="token punctuation">)</span><span class="token punctuation">,</span> device_type<span class="token punctuation">)</span>
<span class="token keyword">if</span> init_from <span class="token operator">==</span> <span class="token string">"resume"</span> <span class="token keyword">and</span> <span class="token string">"optimizer"</span> <span class="token keyword">in</span> checkpoint<span class="token punctuation">:</span>
    optimizer<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">"optimizer"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
checkpoint <span class="token operator">=</span> <span class="token boolean">None</span>  <span class="token comment"># free up memory</span>
</code></pre> 
<h3><a id="7_377"></a>7、损失评估与学习率获取</h3> 
<p><strong>estimate_loss()</strong>：<br> 使用多个数据批次来计算一个任意精度的损失值，用于评估模型在指定数据集上的性能。该函数会将模型置为评估模式（调用 model.eval() 函数），然后对训练集和验证集分别进行评估。对于每个数据集，该函数会使用 iter_batches() 函数生成一个数据批次迭代器 batch_iter，然后对迭代器中的每个数据批次进行前向传播和损失计算，得到一个损失值。这里使用 eval_iters 次评估来对整个数据集进行评估，从而得到一个更准确的损失值。<br> 最后，该函数返回一个字典 out，其中包含了训练集和验证集的平均损失值。需要注意的是，评估完成后，该函数将模型重新置为训练模式（调用 model.train() 函数），以便于之后的模型训练。<br> <strong>get_lr(it)</strong>：<br> 这段代码定义了一个函数 get_lr()，用于根据当前迭代次数来动态计算学习率。具体来说，该函数使用了一种余弦退火（cosine annealing）的学习率调度策略，即先进行一定步数的学习率线性预热（linear warmup），然后使用余弦函数进行学习率退火，直到学习率降到最小值为止。</p> 
<p>该函数的输入参数是当前迭代次数 it，输出参数是当前迭代次数下的学习率。具体来说，该函数首先判断当前迭代次数是否小于预热步数 warmup_iters，如果是，则按照线性预热的方式逐步增加学习率，直到达到 learning_rate。如果当前迭代次数已经超过了学习率退火的步数 lr_decay_iters，则直接返回最小学习率 min_lr。如果当前迭代次数在预热步数和退火步数之间，则使用余弦函数计算当前迭代次数下的学习率，将学习率从初始值 learning_rate 逐渐下降到最小学习率 min_lr。</p> 
<p>需要注意的是，在函数中使用了一个系数 coeff，它的值在 0 和 1 之间，用于调整学习率的下降速度。这个系数是通过余弦函数计算得到的，随着迭代次数的增加，系数的值逐渐从 1 下降到 0，从而实现学习率的下降。</p> 
<pre><code class="prism language-python"><span class="token comment"># helps estimate an arbitrarily accurate loss over either split using many batches</span>
<span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>no_grad</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">estimate_loss</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    out <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> split <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">,</span> <span class="token string">"val"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        batch_iter <span class="token operator">=</span> iter_batches<span class="token punctuation">(</span>split<span class="token punctuation">)</span>
        losses <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>eval_iters<span class="token punctuation">)</span>  <span class="token comment"># keep on CPU</span>
        <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>eval_iters<span class="token punctuation">)</span><span class="token punctuation">:</span>
            X<span class="token punctuation">,</span> Y <span class="token operator">=</span> <span class="token builtin">next</span><span class="token punctuation">(</span>batch_iter<span class="token punctuation">)</span>
            <span class="token keyword">with</span> ctx<span class="token punctuation">:</span>
                logits<span class="token punctuation">,</span> loss <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span>
            losses<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        out<span class="token punctuation">[</span>split<span class="token punctuation">]</span> <span class="token operator">=</span> losses<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> out

<span class="token comment"># learning rate decay scheduler (cosine with warmup)</span>
<span class="token keyword">def</span> <span class="token function">get_lr</span><span class="token punctuation">(</span>it<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 1) linear warmup for warmup_iters steps</span>
    <span class="token keyword">if</span> it <span class="token operator">&lt;</span> warmup_iters<span class="token punctuation">:</span>
        <span class="token keyword">return</span> learning_rate <span class="token operator">*</span> it <span class="token operator">/</span> warmup_iters
    <span class="token comment"># 2) if it &gt; lr_decay_iters, return min learning rate</span>
    <span class="token keyword">if</span> it <span class="token operator">&gt;</span> lr_decay_iters<span class="token punctuation">:</span>
        <span class="token keyword">return</span> min_lr
    <span class="token comment"># 3) in between, use cosine decay down to min learning rate</span>
    decay_ratio <span class="token operator">=</span> <span class="token punctuation">(</span>it <span class="token operator">-</span> warmup_iters<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>lr_decay_iters <span class="token operator">-</span> warmup_iters<span class="token punctuation">)</span>
    <span class="token keyword">assert</span> <span class="token number">0</span> <span class="token operator">&lt;=</span> decay_ratio <span class="token operator">&lt;=</span> <span class="token number">1</span>
    coeff <span class="token operator">=</span> <span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">+</span> math<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>math<span class="token punctuation">.</span>pi <span class="token operator">*</span> decay_ratio<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># coeff ranges 0..1</span>
    <span class="token keyword">return</span> min_lr <span class="token operator">+</span> coeff <span class="token operator">*</span> <span class="token punctuation">(</span>learning_rate <span class="token operator">-</span> min_lr<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="8_419"></a>8、日志保存初始化</h3> 
<p>没什么可说的，就是采用wandb，并进行初始化</p> 
<pre><code class="prism language-python"><span class="token comment"># logging</span>
<span class="token keyword">if</span> wandb_log <span class="token keyword">and</span> master_process<span class="token punctuation">:</span>
    <span class="token keyword">import</span> wandb
    wandb<span class="token punctuation">.</span>init<span class="token punctuation">(</span>project<span class="token operator">=</span>wandb_project<span class="token punctuation">,</span> name<span class="token operator">=</span>wandb_run_name<span class="token punctuation">,</span> config<span class="token operator">=</span>config<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="9_428"></a>9、循环训练</h3> 
<p>这部分主要实现了模型的训练循环。具体来说，该循环会在训练集上进行多次迭代，每次迭代会使用一个数据批次进行前向传播、反向传播和参数更新。在每次迭代过程中，该循环会根据当前迭代次数来动态调整学习率，并使用学习率和损失函数对模型参数进行更新。此外，该循环还会定期对训练集和验证集进行损失评估，并保存模型checkpoint。</p> 
<p>开始时，该循环会使用 iter_batches() 函数生成一个训练集数据批次迭代器 train_batch_iter，然后从迭代器中获取第一个数据批次 X, Y。接着，该循环会不断迭代，直到达到指定的最大迭代次数 max_iters 为止。</p> 
<p>在<strong>while True</strong>的每次迭代中：该循环会先根据当前迭代次数调整学习率，然后使用优化器对模型参数进行更新。如果当前迭代次数是评估间隔 eval_interval 的倍数，并且当前进程是主进程，那么该循环会对训练集和验证集进行损失评估，并将评估结果保存到 losses 中。如果当前验证集的损失值比之前的最佳损失值要小，或者总是保存检查点的标志 always_save_checkpoint 被设置为真，那么该循环会将模型参数和优化器状态保存到检查点文件中。</p> 
<p>接着，该循环会使用当前数据批次 X, Y 进行前向传播和反向传播，并根据梯度累积步数 gradient_accumulation_steps 进行梯度平均。如果在训练过程中使用了混合精度训练，那么该循环还会使用 scaler 对梯度进行缩放。然后，该循环会使用裁剪梯度的方法对梯度进行限幅，并使用优化器对模型参数进行更新。最后，该循环会将梯度清零，以便于下一次迭代。</p> 
<p>在训练过程中，该循环会定期打印当前迭代次数、损失值、学习率、迭代时间和模型的内存使用率等信息。此外，该循环会根据指定的最大迭代次数 max_iters 判断是否终止训练。</p> 
<pre><code class="prism language-python"><span class="token comment"># training loop</span>
train_batch_iter <span class="token operator">=</span> iter_batches<span class="token punctuation">(</span><span class="token string">"train"</span><span class="token punctuation">)</span>
X<span class="token punctuation">,</span> Y <span class="token operator">=</span> <span class="token builtin">next</span><span class="token punctuation">(</span>train_batch_iter<span class="token punctuation">)</span>  <span class="token comment"># fetch the very first batch</span>
t0 <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
local_iter_num <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment"># number of iterations in the lifetime of this process</span>
raw_model <span class="token operator">=</span> model<span class="token punctuation">.</span>module <span class="token keyword">if</span> ddp <span class="token keyword">else</span> model  <span class="token comment"># unwrap DDP container if needed</span>
running_mfu <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1.0</span>
<span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
    <span class="token comment"># determine and set the learning rate for this iteration</span>
    lr <span class="token operator">=</span> get_lr<span class="token punctuation">(</span>iter_num<span class="token punctuation">)</span> <span class="token keyword">if</span> decay_lr <span class="token keyword">else</span> learning_rate
    <span class="token keyword">for</span> param_group <span class="token keyword">in</span> optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">:</span>
        param_group<span class="token punctuation">[</span><span class="token string">"lr"</span><span class="token punctuation">]</span> <span class="token operator">=</span> lr

    <span class="token comment"># evaluate the loss on train/val sets and write checkpoints</span>
    <span class="token keyword">if</span> iter_num <span class="token operator">%</span> eval_interval <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> master_process<span class="token punctuation">:</span>
        losses <span class="token operator">=</span> estimate_loss<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"step </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>iter_num<span class="token punctuation">}</span></span><span class="token string">: train loss </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>losses<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">, val loss </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>losses<span class="token punctuation">[</span><span class="token string">'val'</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> losses<span class="token punctuation">[</span><span class="token string">"val"</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> best_val_loss <span class="token keyword">or</span> always_save_checkpoint<span class="token punctuation">:</span>
            best_val_loss <span class="token operator">=</span> losses<span class="token punctuation">[</span><span class="token string">"val"</span><span class="token punctuation">]</span>
            <span class="token keyword">if</span> iter_num <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
                checkpoint <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
                    <span class="token string">"model"</span><span class="token punctuation">:</span> raw_model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    <span class="token string">"optimizer"</span><span class="token punctuation">:</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    <span class="token string">"model_args"</span><span class="token punctuation">:</span> model_args<span class="token punctuation">,</span>
                    <span class="token string">"iter_num"</span><span class="token punctuation">:</span> iter_num<span class="token punctuation">,</span>
                    <span class="token string">"best_val_loss"</span><span class="token punctuation">:</span> best_val_loss<span class="token punctuation">,</span>
                    <span class="token string">"config"</span><span class="token punctuation">:</span> config<span class="token punctuation">,</span>
                <span class="token punctuation">}</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"saving checkpoint to </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>out_dir<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
                torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>out_dir<span class="token punctuation">,</span> <span class="token string">"ckpt.pt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                raw_model<span class="token punctuation">.</span>export<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>out_dir<span class="token punctuation">,</span> <span class="token string">"model.bin"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> iter_num <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> eval_only<span class="token punctuation">:</span>
        <span class="token keyword">break</span>

    <span class="token comment"># forward backward update, with optional gradient accumulation to simulate larger batch size</span>
    <span class="token comment"># and using the GradScaler if data type is float16</span>
    <span class="token keyword">for</span> micro_step <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>gradient_accumulation_steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">with</span> ctx<span class="token punctuation">:</span>
            logits<span class="token punctuation">,</span> loss <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span>
            loss <span class="token operator">=</span> loss <span class="token operator">/</span> gradient_accumulation_steps
        <span class="token comment"># immediately async prefetch next batch while model is doing the forward pass on the GPU</span>
        X<span class="token punctuation">,</span> Y <span class="token operator">=</span> <span class="token builtin">next</span><span class="token punctuation">(</span>train_batch_iter<span class="token punctuation">)</span>
        <span class="token comment"># backward pass, with gradient scaling if training in fp16</span>
        scaler<span class="token punctuation">.</span>scale<span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># clip the gradient</span>
    <span class="token keyword">if</span> grad_clip <span class="token operator">!=</span> <span class="token number">0.0</span><span class="token punctuation">:</span>
        scaler<span class="token punctuation">.</span>unscale_<span class="token punctuation">(</span>optimizer<span class="token punctuation">)</span>
        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>clip_grad_norm_<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> grad_clip<span class="token punctuation">)</span>
    <span class="token comment"># step the optimizer and scaler if training in fp16</span>
    scaler<span class="token punctuation">.</span>step<span class="token punctuation">(</span>optimizer<span class="token punctuation">)</span>
    scaler<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># flush the gradients as soon as we can, no need for this memory anymore</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span>set_to_none<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

    <span class="token comment"># timing and logging</span>
    t1 <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
    dt <span class="token operator">=</span> t1 <span class="token operator">-</span> t0
    t0 <span class="token operator">=</span> t1
    <span class="token keyword">if</span> iter_num <span class="token operator">%</span> log_interval <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> master_process<span class="token punctuation">:</span>
        <span class="token comment"># get loss as float, scale up due to the divide above. note: this is a CPU-GPU sync point</span>
        lossf <span class="token operator">=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> gradient_accumulation_steps
        <span class="token keyword">if</span> local_iter_num <span class="token operator">&gt;=</span> <span class="token number">5</span><span class="token punctuation">:</span>  <span class="token comment"># let the training loop settle a bit</span>
            mfu <span class="token operator">=</span> raw_model<span class="token punctuation">.</span>estimate_mfu<span class="token punctuation">(</span>batch_size <span class="token operator">*</span> gradient_accumulation_steps<span class="token punctuation">,</span> dt<span class="token punctuation">)</span>
            running_mfu <span class="token operator">=</span> mfu <span class="token keyword">if</span> running_mfu <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1.0</span> <span class="token keyword">else</span> <span class="token number">0.9</span> <span class="token operator">*</span> running_mfu <span class="token operator">+</span> <span class="token number">0.1</span> <span class="token operator">*</span> mfu
        <span class="token keyword">print</span><span class="token punctuation">(</span>
            <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>iter_num<span class="token punctuation">}</span></span><span class="token string"> | loss </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>lossf<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string"> | lr </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>lr<span class="token punctuation">:</span><span class="token format-spec">e</span><span class="token punctuation">}</span></span><span class="token string"> | </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>dt<span class="token operator">*</span><span class="token number">1000</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">ms | mfu </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>running_mfu<span class="token operator">*</span><span class="token number">100</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">%"</span></span>
        <span class="token punctuation">)</span>
    iter_num <span class="token operator">+=</span> <span class="token number">1</span>
    local_iter_num <span class="token operator">+=</span> <span class="token number">1</span>

    <span class="token comment"># termination conditions</span>
    <span class="token keyword">if</span> iter_num <span class="token operator">&gt;</span> max_iters<span class="token punctuation">:</span>
        <span class="token keyword">break</span>
</code></pre> 
<h2><a id="runc_516"></a>七、run.c推理代码讲解</h2> 
<blockquote> 
 <p>注：整体代码在<strong>run.c</strong>里面。</p> 
</blockquote> 
<h3><a id="1_518"></a>1、结构及内存管理</h3> 
<p>定义了一个基于Transformer架构的神经网络模型的配置参数、运行状态和内存管理。<br> <strong>具体来说</strong>：</p> 
<blockquote> 
 <ul><li> <p>Config结构体定义了Transformer模型的一些超参数，例如Transformer的维度、隐藏层维度、层数、查询头数、键/值头数、词汇表大小、最大序列长度等等。</p> </li><li> <p>TransformerWeights结构体定义了Transformer模型的所有权重矩阵，包括Token嵌入表、RMSNorm的权重、矩阵乘积的权重、前馈网络的权重、RoPE相对位置编码的频率矩阵以及用于分类的权重（可选）。</p> </li><li> <p>RunState结构体定义了Transformer模型每个时间步的状态，例如当前激活值、残差分支内的激活值、FFN中的隐藏层激活值、查询、键、值、注意力得分、输出概率分布等等，同时还包括用于缓存键/值的缓存矩阵。函数malloc_run_state用于为RunState结构体中的各个数组分配内存空间。</p> </li><li> <p>函数free_run_state用于释放RunState结构体中各个数组占用的内存空间。</p> </li></ul> 
</blockquote> 
<p>在代码中，这些数组的内存分配和释放都使用了calloc和free函数。calloc函数与malloc函数类似，但它会在分配内存后将其初始化为零，避免出现未初始化的内存访问问题。在分配内存后，代码还会检查是否分配成功以避免出现内存分配失败的情况。</p> 
<pre><code class="prism language-cpp"><span class="token comment">// ----------------------------------------------------------------------------</span>
<span class="token comment">// Transformer and RunState structs, and related memory management</span>

<span class="token keyword">typedef</span> <span class="token keyword">struct</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">int</span> dim<span class="token punctuation">;</span> <span class="token comment">// transformer dimension</span>
    <span class="token keyword">int</span> hidden_dim<span class="token punctuation">;</span> <span class="token comment">// for ffn layers</span>
    <span class="token keyword">int</span> n_layers<span class="token punctuation">;</span> <span class="token comment">// number of layers</span>
    <span class="token keyword">int</span> n_heads<span class="token punctuation">;</span> <span class="token comment">// number of query heads</span>
    <span class="token keyword">int</span> n_kv_heads<span class="token punctuation">;</span> <span class="token comment">// number of key/value heads (can be &lt; query heads because of multiquery)</span>
    <span class="token keyword">int</span> vocab_size<span class="token punctuation">;</span> <span class="token comment">// vocabulary size, usually 256 (byte-level)</span>
    <span class="token keyword">int</span> seq_len<span class="token punctuation">;</span> <span class="token comment">// max sequence length</span>
<span class="token punctuation">}</span> Config<span class="token punctuation">;</span>

<span class="token keyword">typedef</span> <span class="token keyword">struct</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">// token embedding table</span>
    <span class="token keyword">float</span><span class="token operator">*</span> token_embedding_table<span class="token punctuation">;</span>    <span class="token comment">// (vocab_size, dim)</span>
    <span class="token comment">// weights for rmsnorms</span>
    <span class="token keyword">float</span><span class="token operator">*</span> rms_att_weight<span class="token punctuation">;</span> <span class="token comment">// (layer, dim) rmsnorm weights</span>
    <span class="token keyword">float</span><span class="token operator">*</span> rms_ffn_weight<span class="token punctuation">;</span> <span class="token comment">// (layer, dim)</span>
    <span class="token comment">// weights for matmuls</span>
    <span class="token keyword">float</span><span class="token operator">*</span> wq<span class="token punctuation">;</span> <span class="token comment">// (layer, dim, dim)</span>
    <span class="token keyword">float</span><span class="token operator">*</span> wk<span class="token punctuation">;</span> <span class="token comment">// (layer, dim, dim)</span>
    <span class="token keyword">float</span><span class="token operator">*</span> wv<span class="token punctuation">;</span> <span class="token comment">// (layer, dim, dim)</span>
    <span class="token keyword">float</span><span class="token operator">*</span> wo<span class="token punctuation">;</span> <span class="token comment">// (layer, dim, dim)</span>
    <span class="token comment">// weights for ffn</span>
    <span class="token keyword">float</span><span class="token operator">*</span> w1<span class="token punctuation">;</span> <span class="token comment">// (layer, hidden_dim, dim)</span>
    <span class="token keyword">float</span><span class="token operator">*</span> w2<span class="token punctuation">;</span> <span class="token comment">// (layer, dim, hidden_dim)</span>
    <span class="token keyword">float</span><span class="token operator">*</span> w3<span class="token punctuation">;</span> <span class="token comment">// (layer, hidden_dim, dim)</span>
    <span class="token comment">// final rmsnorm</span>
    <span class="token keyword">float</span><span class="token operator">*</span> rms_final_weight<span class="token punctuation">;</span> <span class="token comment">// (dim,)</span>
    <span class="token comment">// freq_cis for RoPE relatively positional embeddings</span>
    <span class="token keyword">float</span><span class="token operator">*</span> freq_cis_real<span class="token punctuation">;</span> <span class="token comment">// (seq_len, dim/2)</span>
    <span class="token keyword">float</span><span class="token operator">*</span> freq_cis_imag<span class="token punctuation">;</span> <span class="token comment">// (seq_len, dim/2)</span>
    <span class="token comment">// (optional) classifier weights for the logits, on the last layer</span>
    <span class="token keyword">float</span><span class="token operator">*</span> wcls<span class="token punctuation">;</span>
<span class="token punctuation">}</span> TransformerWeights<span class="token punctuation">;</span>

<span class="token keyword">typedef</span> <span class="token keyword">struct</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">// current wave of activations</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>x<span class="token punctuation">;</span> <span class="token comment">// activation at current time stamp (dim,)</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>xb<span class="token punctuation">;</span> <span class="token comment">// same, but inside a residual branch (dim,)</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>xb2<span class="token punctuation">;</span> <span class="token comment">// an additional buffer just for convenience (dim,)</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>hb<span class="token punctuation">;</span> <span class="token comment">// buffer for hidden dimension in the ffn (hidden_dim,)</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>hb2<span class="token punctuation">;</span> <span class="token comment">// buffer for hidden dimension in the ffn (hidden_dim,)</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>q<span class="token punctuation">;</span> <span class="token comment">// query (dim,)</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>k<span class="token punctuation">;</span> <span class="token comment">// key (dim,)</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>v<span class="token punctuation">;</span> <span class="token comment">// value (dim,)</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>att<span class="token punctuation">;</span> <span class="token comment">// buffer for scores/attention values (n_heads, seq_len)</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>logits<span class="token punctuation">;</span> <span class="token comment">// output logits</span>
    <span class="token comment">// kv cache</span>
    <span class="token keyword">float</span><span class="token operator">*</span> key_cache<span class="token punctuation">;</span>   <span class="token comment">// (layer, seq_len, dim)</span>
    <span class="token keyword">float</span><span class="token operator">*</span> value_cache<span class="token punctuation">;</span> <span class="token comment">// (layer, seq_len, dim)</span>
<span class="token punctuation">}</span> RunState<span class="token punctuation">;</span>

<span class="token keyword">void</span> <span class="token function">malloc_run_state</span><span class="token punctuation">(</span>RunState<span class="token operator">*</span> s<span class="token punctuation">,</span> Config<span class="token operator">*</span> p<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">// we calloc instead of malloc to keep valgrind happy</span>
    s<span class="token operator">-&gt;</span>x <span class="token operator">=</span> <span class="token function">calloc</span><span class="token punctuation">(</span>p<span class="token operator">-&gt;</span>dim<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    s<span class="token operator">-&gt;</span>xb <span class="token operator">=</span> <span class="token function">calloc</span><span class="token punctuation">(</span>p<span class="token operator">-&gt;</span>dim<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    s<span class="token operator">-&gt;</span>xb2 <span class="token operator">=</span> <span class="token function">calloc</span><span class="token punctuation">(</span>p<span class="token operator">-&gt;</span>dim<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    s<span class="token operator">-&gt;</span>hb <span class="token operator">=</span> <span class="token function">calloc</span><span class="token punctuation">(</span>p<span class="token operator">-&gt;</span>hidden_dim<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    s<span class="token operator">-&gt;</span>hb2 <span class="token operator">=</span> <span class="token function">calloc</span><span class="token punctuation">(</span>p<span class="token operator">-&gt;</span>hidden_dim<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    s<span class="token operator">-&gt;</span>q <span class="token operator">=</span> <span class="token function">calloc</span><span class="token punctuation">(</span>p<span class="token operator">-&gt;</span>dim<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    s<span class="token operator">-&gt;</span>k <span class="token operator">=</span> <span class="token function">calloc</span><span class="token punctuation">(</span>p<span class="token operator">-&gt;</span>dim<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    s<span class="token operator">-&gt;</span>v <span class="token operator">=</span> <span class="token function">calloc</span><span class="token punctuation">(</span>p<span class="token operator">-&gt;</span>dim<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    s<span class="token operator">-&gt;</span>att <span class="token operator">=</span> <span class="token function">calloc</span><span class="token punctuation">(</span>p<span class="token operator">-&gt;</span>n_heads <span class="token operator">*</span> p<span class="token operator">-&gt;</span>seq_len<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    s<span class="token operator">-&gt;</span>logits <span class="token operator">=</span> <span class="token function">calloc</span><span class="token punctuation">(</span>p<span class="token operator">-&gt;</span>vocab_size<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    s<span class="token operator">-&gt;</span>key_cache <span class="token operator">=</span> <span class="token function">calloc</span><span class="token punctuation">(</span>p<span class="token operator">-&gt;</span>n_layers <span class="token operator">*</span> p<span class="token operator">-&gt;</span>seq_len <span class="token operator">*</span> p<span class="token operator">-&gt;</span>dim<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    s<span class="token operator">-&gt;</span>value_cache <span class="token operator">=</span> <span class="token function">calloc</span><span class="token punctuation">(</span>p<span class="token operator">-&gt;</span>n_layers <span class="token operator">*</span> p<span class="token operator">-&gt;</span>seq_len <span class="token operator">*</span> p<span class="token operator">-&gt;</span>dim<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// ensure all mallocs went fine</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>s<span class="token operator">-&gt;</span>x <span class="token operator">||</span> <span class="token operator">!</span>s<span class="token operator">-&gt;</span>xb <span class="token operator">||</span> <span class="token operator">!</span>s<span class="token operator">-&gt;</span>xb2 <span class="token operator">||</span> <span class="token operator">!</span>s<span class="token operator">-&gt;</span>hb <span class="token operator">||</span> <span class="token operator">!</span>s<span class="token operator">-&gt;</span>hb2 <span class="token operator">||</span> <span class="token operator">!</span>s<span class="token operator">-&gt;</span>q 
     <span class="token operator">||</span> <span class="token operator">!</span>s<span class="token operator">-&gt;</span>k <span class="token operator">||</span> <span class="token operator">!</span>s<span class="token operator">-&gt;</span>v <span class="token operator">||</span> <span class="token operator">!</span>s<span class="token operator">-&gt;</span>att <span class="token operator">||</span> <span class="token operator">!</span>s<span class="token operator">-&gt;</span>logits <span class="token operator">||</span> <span class="token operator">!</span>s<span class="token operator">-&gt;</span>key_cache 
     <span class="token operator">||</span> <span class="token operator">!</span>s<span class="token operator">-&gt;</span>value_cache<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"malloc failed!\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">exit</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

<span class="token keyword">void</span> <span class="token function">free_run_state</span><span class="token punctuation">(</span>RunState<span class="token operator">*</span> s<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token function">free</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">free</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>xb<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">free</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>xb2<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">free</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>hb<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">free</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>hb2<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">free</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>q<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">free</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>k<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">free</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>v<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">free</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>att<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">free</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>logits<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">free</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>key_cache<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">free</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>value_cache<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<h3><a id="2checkpoint_626"></a>2、模型初始化：读取checkpoint</h3> 
<p>具体来说，该函数的输入包括：</p> 
<blockquote> 
 <ul><li>一个指向TransformerWeights结构体的指针w，用于存储从文件中读取的权重；</li><li>一个指向Config结构体的指针p，包含Transformer模型的超参数；</li><li>一个指向float类型的指针f，指向从文件中读取的所有模型权重的连续内存块；</li><li>一个布尔值shared_weights，指示是否共享Token嵌入表和分类器权重。</li></ul> 
</blockquote> 
<p>该函数的实现方式是，首先将指向连续内存块的指针f按照顺序分别指向各个数组的起始位置，然后使用指针算术运算逐个填充这些数组。填充顺序与数组在TransformerWeights结构体中的定义顺序相同。</p> 
<p>需要注意的是，最后一个权重数组wcls的起始位置可能与前面的数组不同，这取决于shared_weights的值。如果shared_weights为true，则Token嵌入表和分类器权重共享相同的内存空间，此时wcls指向Token嵌入表的起始位置；否则，wcls指向连续内存块的当前位置。</p> 
<p>该函数的作用是将从文件中读取的权重初始化到内存中，以便后续的Transformer模型推理过程中使用。</p> 
<pre><code class="prism language-cpp"><span class="token comment">// ----------------------------------------------------------------------------</span>
<span class="token comment">// initialization: read from checkpoint</span>

<span class="token keyword">void</span> <span class="token function">checkpoint_init_weights</span><span class="token punctuation">(</span>TransformerWeights <span class="token operator">*</span>w<span class="token punctuation">,</span> Config<span class="token operator">*</span> p<span class="token punctuation">,</span> <span class="token keyword">float</span><span class="token operator">*</span> f<span class="token punctuation">,</span> <span class="token keyword">int</span> shared_weights<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">float</span><span class="token operator">*</span> ptr <span class="token operator">=</span> f<span class="token punctuation">;</span>
    w<span class="token operator">-&gt;</span>token_embedding_table <span class="token operator">=</span> ptr<span class="token punctuation">;</span>
    ptr <span class="token operator">+=</span> p<span class="token operator">-&gt;</span>vocab_size <span class="token operator">*</span> p<span class="token operator">-&gt;</span>dim<span class="token punctuation">;</span>
    w<span class="token operator">-&gt;</span>rms_att_weight <span class="token operator">=</span> ptr<span class="token punctuation">;</span>
    ptr <span class="token operator">+=</span> p<span class="token operator">-&gt;</span>n_layers <span class="token operator">*</span> p<span class="token operator">-&gt;</span>dim<span class="token punctuation">;</span>
    w<span class="token operator">-&gt;</span>wq <span class="token operator">=</span> ptr<span class="token punctuation">;</span>
    ptr <span class="token operator">+=</span> p<span class="token operator">-&gt;</span>n_layers <span class="token operator">*</span> p<span class="token operator">-&gt;</span>dim <span class="token operator">*</span> p<span class="token operator">-&gt;</span>dim<span class="token punctuation">;</span>
    w<span class="token operator">-&gt;</span>wk <span class="token operator">=</span> ptr<span class="token punctuation">;</span>
    ptr <span class="token operator">+=</span> p<span class="token operator">-&gt;</span>n_layers <span class="token operator">*</span> p<span class="token operator">-&gt;</span>dim <span class="token operator">*</span> p<span class="token operator">-&gt;</span>dim<span class="token punctuation">;</span>
    w<span class="token operator">-&gt;</span>wv <span class="token operator">=</span> ptr<span class="token punctuation">;</span>
    ptr <span class="token operator">+=</span> p<span class="token operator">-&gt;</span>n_layers <span class="token operator">*</span> p<span class="token operator">-&gt;</span>dim <span class="token operator">*</span> p<span class="token operator">-&gt;</span>dim<span class="token punctuation">;</span>
    w<span class="token operator">-&gt;</span>wo <span class="token operator">=</span> ptr<span class="token punctuation">;</span>
    ptr <span class="token operator">+=</span> p<span class="token operator">-&gt;</span>n_layers <span class="token operator">*</span> p<span class="token operator">-&gt;</span>dim <span class="token operator">*</span> p<span class="token operator">-&gt;</span>dim<span class="token punctuation">;</span>
    w<span class="token operator">-&gt;</span>rms_ffn_weight <span class="token operator">=</span> ptr<span class="token punctuation">;</span>
    ptr <span class="token operator">+=</span> p<span class="token operator">-&gt;</span>n_layers <span class="token operator">*</span> p<span class="token operator">-&gt;</span>dim<span class="token punctuation">;</span>
    w<span class="token operator">-&gt;</span>w1 <span class="token operator">=</span> ptr<span class="token punctuation">;</span>
    ptr <span class="token operator">+=</span> p<span class="token operator">-&gt;</span>n_layers <span class="token operator">*</span> p<span class="token operator">-&gt;</span>dim <span class="token operator">*</span> p<span class="token operator">-&gt;</span>hidden_dim<span class="token punctuation">;</span>
    w<span class="token operator">-&gt;</span>w2 <span class="token operator">=</span> ptr<span class="token punctuation">;</span>
    ptr <span class="token operator">+=</span> p<span class="token operator">-&gt;</span>n_layers <span class="token operator">*</span> p<span class="token operator">-&gt;</span>hidden_dim <span class="token operator">*</span> p<span class="token operator">-&gt;</span>dim<span class="token punctuation">;</span>
    w<span class="token operator">-&gt;</span>w3 <span class="token operator">=</span> ptr<span class="token punctuation">;</span>
    ptr <span class="token operator">+=</span> p<span class="token operator">-&gt;</span>n_layers <span class="token operator">*</span> p<span class="token operator">-&gt;</span>dim <span class="token operator">*</span> p<span class="token operator">-&gt;</span>hidden_dim<span class="token punctuation">;</span>
    w<span class="token operator">-&gt;</span>rms_final_weight <span class="token operator">=</span> ptr<span class="token punctuation">;</span>
    ptr <span class="token operator">+=</span> p<span class="token operator">-&gt;</span>dim<span class="token punctuation">;</span>
    w<span class="token operator">-&gt;</span>freq_cis_real <span class="token operator">=</span> ptr<span class="token punctuation">;</span>
    <span class="token keyword">int</span> head_size <span class="token operator">=</span> p<span class="token operator">-&gt;</span>dim <span class="token operator">/</span> p<span class="token operator">-&gt;</span>n_heads<span class="token punctuation">;</span>
    ptr <span class="token operator">+=</span> p<span class="token operator">-&gt;</span>seq_len <span class="token operator">*</span> head_size <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">;</span>
    w<span class="token operator">-&gt;</span>freq_cis_imag <span class="token operator">=</span> ptr<span class="token punctuation">;</span>
    ptr <span class="token operator">+=</span> p<span class="token operator">-&gt;</span>seq_len <span class="token operator">*</span> head_size <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">;</span>
    w<span class="token operator">-&gt;</span>wcls <span class="token operator">=</span> shared_weights <span class="token operator">?</span> w<span class="token operator">-&gt;</span>token_embedding_table <span class="token operator">:</span> ptr<span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<h3><a id="3_678"></a>3、神经网络模块</h3> 
<p>这些方法用于实现神经网络模型，具体来说是Transformer模型。以下是每个方法的简要描述：</p> 
<blockquote> 
 <p>1）accum(float <em>a, float <em>b, int size)：此方法将数组b的每个元素加到数组a的相应元素上。<br><br> 2）rmsnorm(float</em> o, float</em> x, float* weight, int size)：此方法使用作为参数提供的权重向量，在输入向量x上执行RMS归一化。结果存储在输出向量o中。<br><br> 3）softmax(float* x, int size)：此方法对输入向量x应用softmax函数，该函数是类别集上的概率分布。结果是相同类别集上的概率分布，输出向量中的每个元素表示相应类别的概率。<br><br> 4）matmul(float* xout, float* x, float* w, int n, int d)：此方法在输入向量x和权重矩阵w之间执行矩阵乘法，得到输出向量xout。矩阵和向量的维度由参数n和d指定。<br><br> 5）transformer(int token, int pos, Config* p, RunState* s, TransformerWeights* w)：此方法为给定的输入token和位置实现Transformer模型。它使用提供的权重计算给定输入的输出向量，并将结果存储在RunState对象的输出向量中。<br><br> 6）sample(float* probabilities, int n)：此方法从由输入向量probabilities指定的概率分布中抽取一个元素。向量的长度由整数参数n给出。<br><br> 7）argmax(float* v, int n)：此方法返回输入向量v中最大元素的索引。向量的长度由整数参数n给出。</p> 
</blockquote> 
<pre><code class="prism language-cpp"><span class="token comment">// ----------------------------------------------------------------------------</span>
<span class="token comment">// neural net blocks</span>

<span class="token keyword">void</span> <span class="token function">accum</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>a<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>b<span class="token punctuation">,</span> <span class="token keyword">int</span> size<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> size<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+=</span> b<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

<span class="token keyword">void</span> <span class="token function">rmsnorm</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span> o<span class="token punctuation">,</span> <span class="token keyword">float</span><span class="token operator">*</span> x<span class="token punctuation">,</span> <span class="token keyword">float</span><span class="token operator">*</span> weight<span class="token punctuation">,</span> <span class="token keyword">int</span> size<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">// calculate sum of squares</span>
    <span class="token keyword">float</span> ss <span class="token operator">=</span> <span class="token number">0.0f</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> size<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        ss <span class="token operator">+=</span> x<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">*</span> x<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    ss <span class="token operator">/=</span> size<span class="token punctuation">;</span>
    ss <span class="token operator">+=</span> <span class="token number">1e-5f</span><span class="token punctuation">;</span>
    ss <span class="token operator">=</span> <span class="token number">1.0f</span> <span class="token operator">/</span> <span class="token function">sqrtf</span><span class="token punctuation">(</span>ss<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token comment">// normalize and scale</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> size<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        o<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> weight<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>ss <span class="token operator">*</span> x<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

<span class="token keyword">void</span> <span class="token function">softmax</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span> x<span class="token punctuation">,</span> <span class="token keyword">int</span> size<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">// find max value (for numerical stability)</span>
    <span class="token keyword">float</span> max_val <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> size<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&gt;</span> max_val<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            max_val <span class="token operator">=</span> x<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
    <span class="token comment">// exp and sum</span>
    <span class="token keyword">float</span> sum <span class="token operator">=</span> <span class="token number">0.0f</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> size<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">expf</span><span class="token punctuation">(</span>x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">-</span> max_val<span class="token punctuation">)</span><span class="token punctuation">;</span>
        sum <span class="token operator">+=</span> x<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token comment">// normalize</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> size<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">/=</span> sum<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

<span class="token keyword">void</span> <span class="token function">matmul</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span> xout<span class="token punctuation">,</span> <span class="token keyword">float</span><span class="token operator">*</span> x<span class="token punctuation">,</span> <span class="token keyword">float</span><span class="token operator">*</span> w<span class="token punctuation">,</span> <span class="token keyword">int</span> n<span class="token punctuation">,</span> <span class="token keyword">int</span> d<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">// W (d,n) @ x (n,) -&gt; xout (d,)</span>
    <span class="token comment">// by far the most amount of time is spent inside this little function</span>
    <span class="token keyword">int</span> i<span class="token punctuation">;</span>
    <span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">pragma</span> <span class="token expression">omp parallel <span class="token keyword">for</span> <span class="token keyword">private</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span></span></span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span>i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> d<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">float</span> val <span class="token operator">=</span> <span class="token number">0.0f</span><span class="token punctuation">;</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            val <span class="token operator">+=</span> w<span class="token punctuation">[</span>i <span class="token operator">*</span> n <span class="token operator">+</span> j<span class="token punctuation">]</span> <span class="token operator">*</span> x<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
        xout<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> val<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

<span class="token keyword">void</span> <span class="token function">transformer</span><span class="token punctuation">(</span><span class="token keyword">int</span> token<span class="token punctuation">,</span> <span class="token keyword">int</span> pos<span class="token punctuation">,</span> Config<span class="token operator">*</span> p<span class="token punctuation">,</span> RunState<span class="token operator">*</span> s<span class="token punctuation">,</span> TransformerWeights<span class="token operator">*</span> w<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>

    <span class="token comment">// a few convenience variables</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>x <span class="token operator">=</span> s<span class="token operator">-&gt;</span>x<span class="token punctuation">;</span>
    <span class="token keyword">int</span> dim <span class="token operator">=</span> p<span class="token operator">-&gt;</span>dim<span class="token punctuation">;</span>
    <span class="token keyword">int</span> hidden_dim <span class="token operator">=</span>  p<span class="token operator">-&gt;</span>hidden_dim<span class="token punctuation">;</span>
    <span class="token keyword">int</span> head_size <span class="token operator">=</span> dim <span class="token operator">/</span> p<span class="token operator">-&gt;</span>n_heads<span class="token punctuation">;</span>

    <span class="token comment">// copy the token embedding into x</span>
    <span class="token keyword">float</span><span class="token operator">*</span> content_row <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token punctuation">(</span>w<span class="token operator">-&gt;</span>token_embedding_table<span class="token punctuation">[</span>token <span class="token operator">*</span> dim<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">memcpy</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> content_row<span class="token punctuation">,</span> dim<span class="token operator">*</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token operator">*</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// pluck out the "pos" row of freq_cis_real and freq_cis_imag</span>
    <span class="token keyword">float</span><span class="token operator">*</span> freq_cis_real_row <span class="token operator">=</span> w<span class="token operator">-&gt;</span>freq_cis_real <span class="token operator">+</span> pos <span class="token operator">*</span> head_size <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">;</span>
    <span class="token keyword">float</span><span class="token operator">*</span> freq_cis_imag_row <span class="token operator">=</span> w<span class="token operator">-&gt;</span>freq_cis_imag <span class="token operator">+</span> pos <span class="token operator">*</span> head_size <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">;</span>

    <span class="token comment">// forward all the layers</span>
    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> l <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> l <span class="token operator">&lt;</span> p<span class="token operator">-&gt;</span>n_layers<span class="token punctuation">;</span> l<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>

        <span class="token comment">// attention rmsnorm</span>
        <span class="token function">rmsnorm</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>xb<span class="token punctuation">,</span> x<span class="token punctuation">,</span> w<span class="token operator">-&gt;</span>rms_att_weight <span class="token operator">+</span> l<span class="token operator">*</span>dim<span class="token punctuation">,</span> dim<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// qkv matmuls for this position</span>
        <span class="token function">matmul</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>q<span class="token punctuation">,</span> s<span class="token operator">-&gt;</span>xb<span class="token punctuation">,</span> w<span class="token operator">-&gt;</span>wq <span class="token operator">+</span> l<span class="token operator">*</span>dim<span class="token operator">*</span>dim<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> dim<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">matmul</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>k<span class="token punctuation">,</span> s<span class="token operator">-&gt;</span>xb<span class="token punctuation">,</span> w<span class="token operator">-&gt;</span>wk <span class="token operator">+</span> l<span class="token operator">*</span>dim<span class="token operator">*</span>dim<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> dim<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">matmul</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>v<span class="token punctuation">,</span> s<span class="token operator">-&gt;</span>xb<span class="token punctuation">,</span> w<span class="token operator">-&gt;</span>wv <span class="token operator">+</span> l<span class="token operator">*</span>dim<span class="token operator">*</span>dim<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> dim<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// apply RoPE rotation to the q and k vectors for each head</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> h <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> h <span class="token operator">&lt;</span> p<span class="token operator">-&gt;</span>n_heads<span class="token punctuation">;</span> h<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            <span class="token comment">// get the q and k vectors for this head</span>
            <span class="token keyword">float</span><span class="token operator">*</span> q <span class="token operator">=</span> s<span class="token operator">-&gt;</span>q <span class="token operator">+</span> h <span class="token operator">*</span> head_size<span class="token punctuation">;</span>
            <span class="token keyword">float</span><span class="token operator">*</span> k <span class="token operator">=</span> s<span class="token operator">-&gt;</span>k <span class="token operator">+</span> h <span class="token operator">*</span> head_size<span class="token punctuation">;</span>
            <span class="token comment">// rotate q and k by the freq_cis_real and freq_cis_imag</span>
            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> head_size<span class="token punctuation">;</span> i<span class="token operator">+=</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
                <span class="token keyword">float</span> q0 <span class="token operator">=</span> q<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
                <span class="token keyword">float</span> q1 <span class="token operator">=</span> q<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
                <span class="token keyword">float</span> k0 <span class="token operator">=</span> k<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
                <span class="token keyword">float</span> k1 <span class="token operator">=</span> k<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
                <span class="token keyword">float</span> fcr <span class="token operator">=</span> freq_cis_real_row<span class="token punctuation">[</span>i<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
                <span class="token keyword">float</span> fci <span class="token operator">=</span> freq_cis_imag_row<span class="token punctuation">[</span>i<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
                q<span class="token punctuation">[</span>i<span class="token punctuation">]</span>   <span class="token operator">=</span> q0 <span class="token operator">*</span> fcr <span class="token operator">-</span> q1 <span class="token operator">*</span> fci<span class="token punctuation">;</span>
                q<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> q0 <span class="token operator">*</span> fci <span class="token operator">+</span> q1 <span class="token operator">*</span> fcr<span class="token punctuation">;</span>
                k<span class="token punctuation">[</span>i<span class="token punctuation">]</span>   <span class="token operator">=</span> k0 <span class="token operator">*</span> fcr <span class="token operator">-</span> k1 <span class="token operator">*</span> fci<span class="token punctuation">;</span>
                k<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> k0 <span class="token operator">*</span> fci <span class="token operator">+</span> k1 <span class="token operator">*</span> fcr<span class="token punctuation">;</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span>

        <span class="token comment">// save key,value at this time step (pos) to our kv cache</span>
        <span class="token keyword">int</span> loff <span class="token operator">=</span> l <span class="token operator">*</span> p<span class="token operator">-&gt;</span>seq_len <span class="token operator">*</span> dim<span class="token punctuation">;</span> <span class="token comment">// kv cache layer offset for convenience</span>
        <span class="token keyword">float</span><span class="token operator">*</span> key_cache_row <span class="token operator">=</span> s<span class="token operator">-&gt;</span>key_cache <span class="token operator">+</span> loff <span class="token operator">+</span> pos <span class="token operator">*</span> dim<span class="token punctuation">;</span>
        <span class="token keyword">float</span><span class="token operator">*</span> value_cache_row <span class="token operator">=</span> s<span class="token operator">-&gt;</span>value_cache <span class="token operator">+</span> loff <span class="token operator">+</span> pos <span class="token operator">*</span> dim<span class="token punctuation">;</span>
        <span class="token function">memcpy</span><span class="token punctuation">(</span>key_cache_row<span class="token punctuation">,</span> s<span class="token operator">-&gt;</span>k<span class="token punctuation">,</span> dim<span class="token operator">*</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token operator">*</span>key_cache_row<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">memcpy</span><span class="token punctuation">(</span>value_cache_row<span class="token punctuation">,</span> s<span class="token operator">-&gt;</span>v<span class="token punctuation">,</span> dim<span class="token operator">*</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token operator">*</span>value_cache_row<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        
        <span class="token comment">// multihead attention. iterate over all heads</span>
        <span class="token keyword">int</span> h<span class="token punctuation">;</span>
        <span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">pragma</span> <span class="token expression">omp parallel <span class="token keyword">for</span> <span class="token keyword">private</span><span class="token punctuation">(</span>h<span class="token punctuation">)</span></span></span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span>h <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> h <span class="token operator">&lt;</span> p<span class="token operator">-&gt;</span>n_heads<span class="token punctuation">;</span> h<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            <span class="token comment">// get the query vector for this head</span>
            <span class="token keyword">float</span><span class="token operator">*</span> q <span class="token operator">=</span> s<span class="token operator">-&gt;</span>q <span class="token operator">+</span> h <span class="token operator">*</span> head_size<span class="token punctuation">;</span>
            <span class="token comment">// attention scores for this head</span>
            <span class="token keyword">float</span><span class="token operator">*</span> att <span class="token operator">=</span> s<span class="token operator">-&gt;</span>att <span class="token operator">+</span> h <span class="token operator">*</span> p<span class="token operator">-&gt;</span>seq_len<span class="token punctuation">;</span>
            <span class="token comment">// iterate over all timesteps, including the current one</span>
            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> t <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> t <span class="token operator">&lt;=</span> pos<span class="token punctuation">;</span> t<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
                <span class="token comment">// get the key vector for this head and at this timestep</span>
                <span class="token keyword">float</span><span class="token operator">*</span> k <span class="token operator">=</span> s<span class="token operator">-&gt;</span>key_cache <span class="token operator">+</span> loff <span class="token operator">+</span> t <span class="token operator">*</span> dim <span class="token operator">+</span> h <span class="token operator">*</span> head_size<span class="token punctuation">;</span>
                <span class="token comment">// calculate the attention score as the dot product of q and k</span>
                <span class="token keyword">float</span> score <span class="token operator">=</span> <span class="token number">0.0f</span><span class="token punctuation">;</span>
                <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> head_size<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
                    score <span class="token operator">+=</span> q<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*</span> k<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
                <span class="token punctuation">}</span>
                score <span class="token operator">/=</span> <span class="token function">sqrtf</span><span class="token punctuation">(</span>head_size<span class="token punctuation">)</span><span class="token punctuation">;</span>
                <span class="token comment">// save the score to the attention buffer</span>
                att<span class="token punctuation">[</span>t<span class="token punctuation">]</span> <span class="token operator">=</span> score<span class="token punctuation">;</span>
            <span class="token punctuation">}</span>

            <span class="token comment">// softmax the scores to get attention weights, from 0..pos inclusively</span>
            <span class="token function">softmax</span><span class="token punctuation">(</span>att<span class="token punctuation">,</span> pos <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

            <span class="token comment">// weighted sum of the values, store back into xb</span>
            <span class="token keyword">float</span><span class="token operator">*</span> xb <span class="token operator">=</span> s<span class="token operator">-&gt;</span>xb <span class="token operator">+</span> h <span class="token operator">*</span> head_size<span class="token punctuation">;</span>
            <span class="token function">memset</span><span class="token punctuation">(</span>xb<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> head_size <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> t <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> t <span class="token operator">&lt;=</span> pos<span class="token punctuation">;</span> t<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
                <span class="token comment">// get the value vector for this head and at this timestep</span>
                <span class="token keyword">float</span><span class="token operator">*</span> v <span class="token operator">=</span> s<span class="token operator">-&gt;</span>value_cache <span class="token operator">+</span> loff <span class="token operator">+</span> t <span class="token operator">*</span> dim <span class="token operator">+</span> h <span class="token operator">*</span> head_size<span class="token punctuation">;</span>
                <span class="token comment">// get the attention weight for this timestep</span>
                <span class="token keyword">float</span> a <span class="token operator">=</span> att<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">;</span>
                <span class="token comment">// accumulate the weighted value into xb</span>
                <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> head_size<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
                    xb<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+=</span> a <span class="token operator">*</span> v<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
                <span class="token punctuation">}</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span>

        <span class="token comment">// final matmul to get the output of the attention</span>
        <span class="token function">matmul</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>xb2<span class="token punctuation">,</span> s<span class="token operator">-&gt;</span>xb<span class="token punctuation">,</span> w<span class="token operator">-&gt;</span>wo <span class="token operator">+</span> l<span class="token operator">*</span>dim<span class="token operator">*</span>dim<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> dim<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// residual connection back into x</span>
        <span class="token function">accum</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> s<span class="token operator">-&gt;</span>xb2<span class="token punctuation">,</span> dim<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// ffn rmsnorm</span>
        <span class="token function">rmsnorm</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>xb<span class="token punctuation">,</span> x<span class="token punctuation">,</span> w<span class="token operator">-&gt;</span>rms_ffn_weight <span class="token operator">+</span> l<span class="token operator">*</span>dim<span class="token punctuation">,</span> dim<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))</span>
        <span class="token comment">// first calculate self.w1(x) and self.w3(x)</span>
        <span class="token function">matmul</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>hb<span class="token punctuation">,</span> s<span class="token operator">-&gt;</span>xb<span class="token punctuation">,</span> w<span class="token operator">-&gt;</span>w1 <span class="token operator">+</span> l<span class="token operator">*</span>dim<span class="token operator">*</span>hidden_dim<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">matmul</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>hb2<span class="token punctuation">,</span> s<span class="token operator">-&gt;</span>xb<span class="token punctuation">,</span> w<span class="token operator">-&gt;</span>w3 <span class="token operator">+</span> l<span class="token operator">*</span>dim<span class="token operator">*</span>hidden_dim<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">;</span>
        
        <span class="token comment">// F.silu; silu(x)=x*σ(x),where σ(x) is the logistic sigmoid</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> hidden_dim<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            s<span class="token operator">-&gt;</span>hb<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> s<span class="token operator">-&gt;</span>hb<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1.0f</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1.0f</span> <span class="token operator">+</span> <span class="token function">expf</span><span class="token punctuation">(</span><span class="token operator">-</span>s<span class="token operator">-&gt;</span>hb<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>

        <span class="token comment">// elementwise multiply with w3(x)</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> hidden_dim<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            s<span class="token operator">-&gt;</span>hb<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> s<span class="token operator">-&gt;</span>hb<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*</span> s<span class="token operator">-&gt;</span>hb2<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>

        <span class="token comment">// final matmul to get the output of the ffn</span>
        <span class="token function">matmul</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>xb<span class="token punctuation">,</span> s<span class="token operator">-&gt;</span>hb<span class="token punctuation">,</span> w<span class="token operator">-&gt;</span>w2 <span class="token operator">+</span> l<span class="token operator">*</span>dim<span class="token operator">*</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> dim<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// residual connection</span>
        <span class="token function">accum</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> s<span class="token operator">-&gt;</span>xb<span class="token punctuation">,</span> dim<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    
    <span class="token comment">// final rmsnorm</span>
    <span class="token function">rmsnorm</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> w<span class="token operator">-&gt;</span>rms_final_weight<span class="token punctuation">,</span> dim<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// classifier into logits</span>
    <span class="token function">matmul</span><span class="token punctuation">(</span>s<span class="token operator">-&gt;</span>logits<span class="token punctuation">,</span> x<span class="token punctuation">,</span> w<span class="token operator">-&gt;</span>wcls<span class="token punctuation">,</span> p<span class="token operator">-&gt;</span>dim<span class="token punctuation">,</span> p<span class="token operator">-&gt;</span>vocab_size<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

<span class="token keyword">int</span> <span class="token function">sample</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span> probabilities<span class="token punctuation">,</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">// sample index from probabilities, they must sum to 1</span>
    <span class="token keyword">float</span> r <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token function">rand</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span>RAND_MAX<span class="token punctuation">;</span>
    <span class="token keyword">float</span> cdf <span class="token operator">=</span> <span class="token number">0.0f</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        cdf <span class="token operator">+=</span> probabilities<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>r <span class="token operator">&lt;</span> cdf<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            <span class="token keyword">return</span> i<span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
    <span class="token keyword">return</span> n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment">// in case of rounding errors</span>
<span class="token punctuation">}</span>

<span class="token keyword">int</span> <span class="token function">argmax</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span> v<span class="token punctuation">,</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">// return argmax of v in elements 0..n</span>
    <span class="token keyword">int</span> max_i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
    <span class="token keyword">float</span> max_p <span class="token operator">=</span> v<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>v<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&gt;</span> max_p<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            max_i <span class="token operator">=</span> i<span class="token punctuation">;</span>
            max_p <span class="token operator">=</span> v<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
    <span class="token keyword">return</span> max_i<span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<h3><a id="4main_908"></a>4、main函数入口</h3> 
<p>程序通过读取已经训练好的模型和分词器，生成指定长度的文本。下面是主函数的作用：</p> 
<blockquote> 
 <p>1）解析命令行参数，包括模型文件路径、温度和步数等，如果缺少必要的参数，则打印出用法说明并返回1。<br><br> 2）读取模型文件，并将其映射到内存中，同时读取词汇表文件。<br><br> 3）初始化运行状态，包括分配内存和初始化状态。<br><br> 4）生成文本，通过不断调用Transformer模型并使用softmax函数和采样方法获取下一个词语，直到生成指定长度的文本。<br><br> 5）打印生成的文本，并计算生成速度。<br><br> 6）释放分配的内存，关闭文件句柄。</p> 
</blockquote> 
<p>程序主要的工作是在while循环中完成的，每次循环生成一个词语，并更新运行状态。生成的词语通过printf函数打印出来，最后计算生成速度并输出。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span>argv<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>

    <span class="token comment">// poor man's C argparse</span>
    <span class="token keyword">char</span> <span class="token operator">*</span>checkpoint <span class="token operator">=</span> <span class="token constant">NULL</span><span class="token punctuation">;</span>  <span class="token comment">// e.g. out/model.bin</span>
    <span class="token keyword">float</span> temperature <span class="token operator">=</span> <span class="token number">0.9f</span><span class="token punctuation">;</span> <span class="token comment">// e.g. 1.0, or 0.0</span>
    <span class="token keyword">int</span> steps <span class="token operator">=</span> <span class="token number">256</span><span class="token punctuation">;</span>          <span class="token comment">// max number of steps to run for, 0: use seq_len</span>
    <span class="token comment">// 'checkpoint' is necessary arg</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>argc <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Usage: %s &lt;checkpoint_file&gt; [temperature] [steps]\n"</span><span class="token punctuation">,</span> argv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>argc <span class="token operator">&gt;=</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        checkpoint <span class="token operator">=</span> argv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>argc <span class="token operator">&gt;=</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token comment">// optional temperature. 0.0 = (deterministic) argmax sampling. 1.0 = baseline</span>
        temperature <span class="token operator">=</span> <span class="token function">atof</span><span class="token punctuation">(</span>argv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>argc <span class="token operator">&gt;=</span> <span class="token number">4</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        steps <span class="token operator">=</span> <span class="token function">atoi</span><span class="token punctuation">(</span>argv<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token comment">// seed rng with time. if you want deterministic behavior use temperature 0.0</span>
    <span class="token function">srand</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">unsigned</span> <span class="token keyword">int</span><span class="token punctuation">)</span><span class="token function">time</span><span class="token punctuation">(</span><span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
    
    <span class="token comment">// read in the model.bin file</span>
    Config config<span class="token punctuation">;</span>
    TransformerWeights weights<span class="token punctuation">;</span>
    <span class="token keyword">int</span> fd <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>         <span class="token comment">// file descriptor for memory mapping</span>
    <span class="token keyword">float</span><span class="token operator">*</span> data <span class="token operator">=</span> <span class="token constant">NULL</span><span class="token punctuation">;</span> <span class="token comment">// memory mapped data pointer</span>
    <span class="token keyword">long</span> file_size<span class="token punctuation">;</span>     <span class="token comment">// size of the checkpoint file in bytes</span>
    <span class="token punctuation">{<!-- --></span>
        FILE <span class="token operator">*</span>file <span class="token operator">=</span> <span class="token function">fopen</span><span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>file<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Couldn't open file %s\n"</span><span class="token punctuation">,</span> checkpoint<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token punctuation">}</span>
        <span class="token comment">// read in the config header</span>
        <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token function">fread</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>config<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>Config<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> file<span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token punctuation">}</span>
        <span class="token comment">// negative vocab size is hacky way of signaling unshared weights. bit yikes.</span>
        <span class="token keyword">int</span> shared_weights <span class="token operator">=</span> config<span class="token punctuation">.</span>vocab_size <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token operator">?</span> <span class="token number">1</span> <span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">;</span>
        config<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> <span class="token function">abs</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// figure out the file size</span>
        <span class="token function">fseek</span><span class="token punctuation">(</span>file<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token constant">SEEK_END</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// move file pointer to end of file</span>
        file_size <span class="token operator">=</span> <span class="token function">ftell</span><span class="token punctuation">(</span>file<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// get the file size, in bytes</span>
        <span class="token function">fclose</span><span class="token punctuation">(</span>file<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// memory map the Transformer weights into the data pointer</span>
        fd <span class="token operator">=</span> <span class="token function">open</span><span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> O_RDONLY<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// open in read only mode</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>fd <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"open failed!\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token punctuation">}</span>
        data <span class="token operator">=</span> <span class="token function">mmap</span><span class="token punctuation">(</span><span class="token constant">NULL</span><span class="token punctuation">,</span> file_size<span class="token punctuation">,</span> PROT_READ<span class="token punctuation">,</span> MAP_PRIVATE<span class="token punctuation">,</span> fd<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>data <span class="token operator">==</span> MAP_FAILED<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"mmap failed!\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token punctuation">}</span>
        <span class="token keyword">float</span><span class="token operator">*</span> weights_ptr <span class="token operator">=</span> data <span class="token operator">+</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>Config<span class="token punctuation">)</span><span class="token operator">/</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">checkpoint_init_weights</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>weights<span class="token punctuation">,</span> <span class="token operator">&amp;</span>config<span class="token punctuation">,</span> weights_ptr<span class="token punctuation">,</span> shared_weights<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
    <span class="token comment">// right now we cannot run for more than config.seq_len steps</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>steps <span class="token operator">&lt;=</span> <span class="token number">0</span> <span class="token operator">||</span> steps <span class="token operator">&gt;</span> config<span class="token punctuation">.</span>seq_len<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> steps <span class="token operator">=</span> config<span class="token punctuation">.</span>seq_len<span class="token punctuation">;</span> <span class="token punctuation">}</span>

    <span class="token comment">// read in the tokenizer.bin file</span>
    <span class="token keyword">char</span><span class="token operator">*</span><span class="token operator">*</span> vocab <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">char</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>vocab_size <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">char</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">{<!-- --></span>
        FILE <span class="token operator">*</span>file <span class="token operator">=</span> <span class="token function">fopen</span><span class="token punctuation">(</span><span class="token string">"tokenizer.bin"</span><span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>file<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Couldn't load tokenizer.bin\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token punctuation">}</span>
        <span class="token keyword">int</span> len<span class="token punctuation">;</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token function">fread</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>len<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> file<span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token punctuation">}</span>
            vocab<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">char</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span>len <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token function">fread</span><span class="token punctuation">(</span>vocab<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> len<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> file<span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token punctuation">}</span>
            vocab<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>len<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token char">'\0'</span><span class="token punctuation">;</span> <span class="token comment">// add the string terminating token</span>
        <span class="token punctuation">}</span>
        <span class="token function">fclose</span><span class="token punctuation">(</span>file<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token comment">// create and init the application RunState</span>
    RunState state<span class="token punctuation">;</span>
    <span class="token function">malloc_run_state</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>state<span class="token punctuation">,</span> <span class="token operator">&amp;</span>config<span class="token punctuation">)</span><span class="token punctuation">;</span>
    
    <span class="token comment">// the current position we are in</span>
    <span class="token keyword">long</span> start <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> <span class="token comment">// used to time our code, only initialized after first iteration</span>
    <span class="token keyword">int</span> next<span class="token punctuation">;</span>
    <span class="token keyword">int</span> token <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment">// 1 = BOS token in Llama-2 sentencepiece</span>
    <span class="token keyword">int</span> pos <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"&lt;s&gt;\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// explicit print the initial BOS token (=1), stylistically symmetric</span>
    <span class="token keyword">while</span> <span class="token punctuation">(</span>pos <span class="token operator">&lt;</span> steps<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>

        <span class="token comment">// forward the transformer to get logits for the next token</span>
        <span class="token function">transformer</span><span class="token punctuation">(</span>token<span class="token punctuation">,</span> pos<span class="token punctuation">,</span> <span class="token operator">&amp;</span>config<span class="token punctuation">,</span> <span class="token operator">&amp;</span>state<span class="token punctuation">,</span> <span class="token operator">&amp;</span>weights<span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// sample the next token</span>
        <span class="token keyword">if</span><span class="token punctuation">(</span>temperature <span class="token operator">==</span> <span class="token number">0.0f</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            <span class="token comment">// greedy argmax sampling</span>
            next <span class="token operator">=</span> <span class="token function">argmax</span><span class="token punctuation">(</span>state<span class="token punctuation">.</span>logits<span class="token punctuation">,</span> config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{<!-- --></span>
            <span class="token comment">// apply the temperature to the logits</span>
            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> q<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> q<span class="token operator">&lt;</span>config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">;</span> q<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> state<span class="token punctuation">.</span>logits<span class="token punctuation">[</span>q<span class="token punctuation">]</span> <span class="token operator">/=</span> temperature<span class="token punctuation">;</span> <span class="token punctuation">}</span>
            <span class="token comment">// apply softmax to the logits to get the probabilities for next token</span>
            <span class="token function">softmax</span><span class="token punctuation">(</span>state<span class="token punctuation">.</span>logits<span class="token punctuation">,</span> config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token comment">// we now want to sample from this distribution to get the next token</span>
            next <span class="token operator">=</span> <span class="token function">sample</span><span class="token punctuation">(</span>state<span class="token punctuation">.</span>logits<span class="token punctuation">,</span> config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
        <span class="token comment">// following BOS token (1), sentencepiece decoder strips any leading whitespace (see PR #89)</span>
        <span class="token keyword">char</span> <span class="token operator">*</span>token_str <span class="token operator">=</span> <span class="token punctuation">(</span>token <span class="token operator">==</span> <span class="token number">1</span> <span class="token operator">&amp;&amp;</span> vocab<span class="token punctuation">[</span>next<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token char">' '</span><span class="token punctuation">)</span> <span class="token operator">?</span> vocab<span class="token punctuation">[</span>next<span class="token punctuation">]</span><span class="token operator">+</span><span class="token number">1</span> <span class="token operator">:</span> vocab<span class="token punctuation">[</span>next<span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%s"</span><span class="token punctuation">,</span> token_str<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">fflush</span><span class="token punctuation">(</span><span class="token constant">stdout</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        <span class="token comment">// advance forward</span>
        token <span class="token operator">=</span> next<span class="token punctuation">;</span>
        pos<span class="token operator">++</span><span class="token punctuation">;</span>
        <span class="token comment">// init our timer here because the first iteration is slow due to memmap</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>start <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> start <span class="token operator">=</span> <span class="token function">time_in_ms</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>

    <span class="token comment">// report achieved tok/s</span>
    <span class="token keyword">long</span> end <span class="token operator">=</span> <span class="token function">time_in_ms</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"\nachieved tok/s: %f\n"</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>steps<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token keyword">double</span><span class="token punctuation">)</span><span class="token punctuation">(</span>end<span class="token operator">-</span>start<span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// memory and file handles cleanup</span>
    <span class="token function">free_run_state</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>state<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> <span class="token function">free</span><span class="token punctuation">(</span>vocab<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">}</span>
    <span class="token function">free</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>data <span class="token operator">!=</span> MAP_FAILED<span class="token punctuation">)</span> <span class="token function">munmap</span><span class="token punctuation">(</span>data<span class="token punctuation">,</span> file_size<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>fd <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token function">close</span><span class="token punctuation">(</span>fd<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>至此，你已经基本知道怎么训练llama2了，并且还知道训练跟推理大致是怎么实现的，虽然没有对代码逐一细讲，但也觉得没必要一一细讲，只要把大致流程说清楚，其余部分自然也就通了，LLM是不是其实也并不难，跟着教程玩起来吧！！！</p> 
<p>参考文献：</p> 
<blockquote> 
 <p>https://zhuanlan.zhihu.com/p/634267984</p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ab18c1ab75ddedf7f28a17a98bca76ed/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">midjourney免费几次？国内可以用么？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5fc198c10fde8bdd677050fda68769c1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">uniapp vue3中使用webview在微信小程序中实现双向通讯</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>