<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【PyTorch笔记】训练时显存一直增加到 out-of-memory？真相了！ - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/63c2cbac509d5e68394a651a5e9244ef/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【PyTorch笔记】训练时显存一直增加到 out-of-memory？真相了！">
  <meta property="og:description" content="最近用 Pytorch 训模型的过程中，发现总是训练几轮后，出现显存爆炸 out-of-memory 的问题，询问了 ChatGPT、查找了各种文档。。。
在此记录这次 debug 之旅，希望对有类似问题的小伙伴有一点点帮助。
问题描述： 训练过程中，网络结构做了一些调整，forward 函数增加了部分计算过程，突然发现 16G 显存不够用了。
用 nvidia-smi 观察显存变化，发现显存一直在有规律地增加，直到 out-of-memory。
解决思路： 尝试思路1： 计算 loss 的过程中是否使用了 item() 取值，比如：
train_loss &#43;= loss.item() 发现我不存在这个问题，因为 loss 是最后汇总计算的。
尝试思路2： 训练主程序中添加两行下面的代码，实测发现并没有用。
torch.backends.cudnn.enabled = True torch.backends.cudnn.benchmark = True 这两行代码是干啥的？
大白话：设置为 True，意味着 cuDNN 会自动寻找最适合当前配置的高效算法，来获得最佳运行效率。这两行通常一起是哦那个
所以：
如果网络的输入数据在尺度或类型上变化不大，设置 torch.backends.cudnn.benchmark = True 可以增加运行效率；如果网络的输入数据在每次迭代都变化，比如多尺度训练，会导致 cnDNN 每次都会去寻找一遍最优配置，这样反而会降低运行效率。 尝试思路3： 及时删除临时变量和清空显存的 cache，例如在每轮训练后添加：
torch.cuda.empty_cache() 依旧没有解决显存持续增长的问题，而且如果频繁使用 torch.cuda.empty_cache()，会显著增加模型训练时长。
尝试思路4： 排查显存增加的代码位置，既然是增加了部分代码导致的显存增加，那么问题肯定出现在这部分代码中。
为此，可以逐段输出显存占用量，确定问题点在哪。
举个例子：
print(&#34;训练前:{}&#34;.format(torch.cuda.memory_allocated(0))) train_epoch(model,data) print(&#34;训练后:{}&#34;.format(torch.cuda.memory_allocated(0))) eval(model,data) print(&#34;评估后:{}&#34;.format(torch.cuda.memory_allocated(0))) 最终方案： 最终发现的问题是：我在模型中增加了 register_buffer：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-28T09:34:41+08:00">
    <meta property="article:modified_time" content="2024-07-28T09:34:41+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【PyTorch笔记】训练时显存一直增加到 out-of-memory？真相了！</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>最近用 Pytorch 训模型的过程中，发现总是训练几轮后，出现显存爆炸 out-of-memory 的问题，询问了 ChatGPT、查找了各种文档。。。</p> 
<p>在此记录这次 debug 之旅，希望对有类似问题的小伙伴有一点点帮助。</p> 
<h2><a id="_5"></a>问题描述：</h2> 
<p>训练过程中，网络结构做了一些调整，forward 函数增加了部分计算过程，突然发现 16G 显存不够用了。</p> 
<p>用 nvidia-smi 观察显存变化，发现显存一直在有规律地增加，直到 out-of-memory。</p> 
<h2><a id="_11"></a>解决思路：</h2> 
<h3><a id="1_12"></a>尝试思路1：</h3> 
<p>计算 loss 的过程中是否使用了 item() 取值，比如：</p> 
<pre><code>train_loss += loss.item()
</code></pre> 
<p>发现我不存在这个问题，因为 loss 是最后汇总计算的。</p> 
<h3><a id="2_21"></a>尝试思路2：</h3> 
<p>训练主程序中添加两行下面的代码，实测发现并没有用。</p> 
<pre><code>torch.backends.cudnn.enabled = True
torch.backends.cudnn.benchmark = True
</code></pre> 
<p>这两行代码是干啥的？</p> 
<p>大白话：设置为 True，意味着 cuDNN 会自动寻找最适合当前配置的高效算法，来获得最佳运行效率。这两行通常一起是哦那个</p> 
<p>所以：</p> 
<ul><li>如果网络的输入数据在尺度或类型上变化不大，设置 <code>torch.backends.cudnn.benchmark = True</code> 可以增加运行效率；</li><li>如果网络的输入数据在每次迭代都变化，比如多尺度训练，会导致 cnDNN 每次都会去寻找一遍最优配置，<strong>这样反而会降低运行效率</strong>。</li></ul> 
<h3><a id="3_37"></a>尝试思路3：</h3> 
<p>及时删除临时变量和清空显存的 cache，例如在每轮训练后添加：</p> 
<pre><code>torch.cuda.empty_cache()
</code></pre> 
<p>依旧没有解决显存持续增长的问题，而且如果频繁使用 <code>torch.cuda.empty_cache()</code>，会显著增加模型训练时长。</p> 
<h3><a id="4_44"></a>尝试思路4：</h3> 
<p>排查显存增加的代码位置，既然是增加了部分代码导致的显存增加，那么问题肯定出现在这部分代码中。</p> 
<p>为此，可以逐段输出显存占用量，确定问题点在哪。</p> 
<p>举个例子：</p> 
<pre><code>print("训练前:{}".format(torch.cuda.memory_allocated(0)))
train_epoch(model,data)
print("训练后:{}".format(torch.cuda.memory_allocated(0)))
eval(model,data)
print("评估后:{}".format(torch.cuda.memory_allocated(0)))
</code></pre> 
<h3><a id="_59"></a>最终方案：</h3> 
<p>最终发现的问题是：我在模型中增加了 <code>register_buffer</code>：</p> 
<pre><code>self.register_buffer("positives", torch.randn(1, 256))
self.register_buffer("negatives", torch.randn(256, self.num_negatives))
</code></pre> 
<p>但 <strong>register_buffer</strong> 注册的是非参数的 Tensor，它只是被保存在模型的状态字典中，并不会进行梯度计算啊。</p> 
<p>为了验证这一点，还打印出来验证了下：</p> 
<pre><code># for name, param in model.named_parameters():
for name, param in model.named_buffers():
    print(name, param.shape, param.requires_grad)

# 输出如下：
positives torch.Size([1, 256]) False
negatives torch.Size([256, 20480]) False
</code></pre> 
<p>但是这个 <code>buffer</code> 却是导致显存不断增加的罪魁祸首。</p> 
<p>为此，赶紧把和 <code>buffer</code> 相关的操作放在 <strong>torch.no_grad()</strong> 上下文中，问题解决！</p> 
<pre><code>@torch.no_grad()
def dequeue_samples(self, positives, negatives):
    if positives.shape[0] &gt; 0:
        self.positives = 0.99*self.positives + 0.01*positives.mean(0, keepdim=True)
    self.negatives[:, self.ptr:self.ptr+negatives.shape[1]] = F.normalize(negatives, dim=0)

with torch.no_grad():
    keys = F.normalize(self.positives.clone().detach(), dim=1).expand(cur_positives.shape[0], -1)
    negs = self.negatives.clone().detach()
</code></pre> 
<h2><a id="_97"></a>结论：</h2> 
<p>如果是训练过程中显存不断增加，问题大概率出现在 forward 过程中，可以通过<code>尝试思路4</code>逐步排查出问题点所在，把不需要梯度计算的操作放在 <strong>torch.no_grad()</strong> 上下文中。</p> 
<p>如果本文对你有帮助，欢迎<strong>点赞收藏</strong>备用！</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c765d9cadaae1ea5d9346f00432d9caa/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【Spring Boot】自动配置源码解析</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7d43c1a55788b09b0099b2281120c86e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">显示当前活动线程的数量threading.active_count()</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>