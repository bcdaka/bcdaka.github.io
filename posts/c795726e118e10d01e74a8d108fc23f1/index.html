<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AI大模型探索之路-训练篇5：大语言模型预训练数据准备-词元化 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/c795726e118e10d01e74a8d108fc23f1/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="AI大模型探索之路-训练篇5：大语言模型预训练数据准备-词元化">
  <meta property="og:description" content="系列文章目录🚩 AI大模型探索之路-训练篇1：大语言模型微调基础认知
AI大模型探索之路-训练篇2：大语言模型预训练基础认知
AI大模型探索之路-训练篇3：大语言模型全景解读
AI大模型探索之路-训练篇4：大语言模型训练数据集概览
文章目录 系列文章目录🚩前言一、概述二、分词的粒度三、分词器的类型四、BPE/BBPE分词五、WordPiece分词六、Unigram 分词七、分词器的选择八、各大模型的分词效果九、SentencePiece分词器使用 前言 在自然语言处理领域，大语言模型预训练数据准备是一个重要的环节。其中，词元化（Tokenization）作为预训练前期的关键步骤，旨在将原始文本分割成模型可识别和建模的词元序列，为大语言模型提供输入数据。本文将对词元化技术进行详细介绍，包括分词的粒度、分词器的类型以及各大模型的分词效果等内容。
一、概述 分词（词元化）：词元化（Tokenization）是数据预处理中的一个关键步骤，旨在将原始文本分割成模型可识别和建模的词元序列，作为大语言模型的输入数据；形成一个词汇表。
传统自然语言处理研究（如基于条件随机场的序列标注）主要使用基于词汇的分词方法，这种方法更符合人类的语言认知。然而，基于词汇的分词在某些语言（如中文分词）中可能对于相同的输入产生不同的分词结果，导致生成包含海量低频词的庞大词表，还可能存在未登录词（Out-of-vocabulary, OOV）等问题。因此，一些语言模型开始采用字符作为最小单位来分词。其中子词分词器（Subword Tokenizer）被广泛应用于基于 Transformer 的语言模型中，包括 BPE 分词、WordPiece 分词和 Unigram 分词三种常见方法。
二、分词的粒度 从分词的粒度区分，主要包括3种类型，Word 、Subword、Char
1）Word分词粒度以完整的单词为单位进行分词，能够很好地保留每个词的语义，适合上下文理解和语义分析。然而，它面临着长尾效应和稀有词问题，可能导致词汇表庞大并且出现OOV（Out-of-Vocabulary）问题。
OOV是“Out-Of-Vocabulary”的缩写，直译为“词汇表外的”，在自然语言处理中，表示的是那些在词汇表中没有的单词
2）Char分词粒度则是将文本拆分为字符级别，这样可以解决OOV问题，因为可以处理任何字符，但缺点是可能缺乏明确的语义信息，并且由于粒度过细，会增加后续处理的计算成本和时间。
3）Subword分词粒度介于Word和Char之间，旨在克服两者的缺点，同时保留语义信息并减少OOV问题的发生。Subword分词方法如BPE（Byte Pair Encoding）或WordPiece通过统计学方法切分单词为更小的有意义的单元，这使得它们在处理生僻词和缩写时更为有效。（目前使用比较广泛）
三、分词器的类型 针对Subword常用的分词器有3种：BPE 分词、WordPiece 分词和 Unigram 分词。
SentencePiece 是一个开源的分词器工具；是由谷歌开发的，旨在提供一种高效的方式来对文本进行分词，尤其适用于处理变长和不规则的文本数据。它通过训练特定领域的模型来代替预训练模型中的词表，从而更有效地处理词汇。常用的BPE、WordPiece、 Unigram分词器都支持。
四、BPE/BBPE分词 1）BPE：从字符级别开始，逐步合并最频繁连续出现的字符或字符组合，形成新的词汇单元。
2）BBPE：字节级别的 BPE（Byte-level BPE, B-BPE）是 BPE 算法的一种拓展。它将字节视为合并操作的基本符号，从而可以实现更细粒度的分割，且解决了未登录词问题。采用这种词元化方法的代表性语言模型包括 GPT-2 、BART 和 LLaMA 。
3）对于英文、拉美体系的语言来说使用BPE分词足以在可接受的词表大小下解决OOV的问题，但面对中文、日文等语言时，其稀有的字符可能会不必要的占用词汇表（词汇表要么巨大要么会OOV），因此考虑使用字节级别byte-level解决不同语言进行分词时OOV的问题。具体的，BBPE将一段文本的UTF-8编码(UTF-8保证任何语言都可以通用)中的一个字节256位不同的编码作为词表的初始化基础Subword。
例如，GPT-2 的词表大小为 50,257 ，包括 256 个字节的基本词元、一个特殊的文末词元以及通过 50,000 次合并学习到的词元。(相当于既有了BPE特性，又兼容了中文）
BBPE的优点：不会出现 OOV 的情况。不管是怎样的汉字，只要可以用字节表示，就都会存在于初始词表中。
BBPE的缺点：一个汉字由3个字节组成，一个汉字就会被切成多个token，但实际上这多个token没必要进行训练。
BPE词表构建整体流程如下：
五、WordPiece分词 1）WordPiece 分词和 BPE 分词的想法非常相似，都是通过迭代合并连续的词元，但是合并的选择标准略有不同WordPiece 分词算法并不选择最频繁的词对，而是使用下面的公式为每个词对计算分数">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-30T16:01:49+08:00">
    <meta property="article:modified_time" content="2024-04-30T16:01:49+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AI大模型探索之路-训练篇5：大语言模型预训练数据准备-词元化</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>系列文章目录🚩</h2> 
<p><a href="https://xundaomalu.blog.csdn.net/article/details/138107946" rel="nofollow">AI大模型探索之路-训练篇1：大语言模型微调基础认知</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138143923" rel="nofollow">AI大模型探索之路-训练篇2：大语言模型预训练基础认知</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138161057" rel="nofollow">AI大模型探索之路-训练篇3：大语言模型全景解读</a><br> <a href="https://xundaomalu.blog.csdn.net/article/details/138205204" rel="nofollow">AI大模型探索之路-训练篇4：大语言模型训练数据集概览</a></p> 
<hr> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_0" rel="nofollow">系列文章目录🚩</a></li><li><a href="#_11" rel="nofollow">前言</a></li><li><a href="#_13" rel="nofollow">一、概述</a></li><li><a href="#_16" rel="nofollow">二、分词的粒度</a></li><li><a href="#_27" rel="nofollow">三、分词器的类型</a></li><li><a href="#BPEBBPE_33" rel="nofollow">四、BPE/BBPE分词</a></li><li><a href="#WordPiece_47" rel="nofollow">五、WordPiece分词</a></li><li><a href="#Unigram__57" rel="nofollow">六、Unigram 分词</a></li><li><a href="#_60" rel="nofollow">七、分词器的选择</a></li><li><a href="#_67" rel="nofollow">八、各大模型的分词效果</a></li><li><a href="#SentencePiece_79" rel="nofollow">九、SentencePiece分词器使用</a></li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_11"></a>前言</h2> 
<p>在自然语言处理领域，大语言模型预训练数据准备是一个重要的环节。其中，词元化（Tokenization）作为预训练前期的关键步骤，旨在将原始文本分割成模型可识别和建模的词元序列，为大语言模型提供输入数据。本文将对词元化技术进行详细介绍，包括分词的粒度、分词器的类型以及各大模型的分词效果等内容。</p> 
<h2><a id="_13"></a>一、概述</h2> 
<p>分词（词元化）：词元化（Tokenization）是数据预处理中的一个关键步骤，旨在将原始文本分割成模型可识别和建模的词元序列，作为大语言模型的输入数据；形成一个词汇表。<br> 传统自然语言处理研究（如基于条件随机场的序列标注）主要使用基于词汇的分词方法，这种方法更符合人类的语言认知。然而，基于词汇的分词在某些语言（如中文分词）中可能对于相同的输入产生不同的分词结果，导致生成包含海量低频词的庞大词表，还可能存在未登录词（Out-of-vocabulary, OOV）等问题。因此，一些语言模型开始采用字符作为最小单位来分词。其中子词分词器（Subword Tokenizer）被广泛应用于基于 Transformer 的语言模型中，包括 BPE 分词、WordPiece 分词和 Unigram 分词三种常见方法。</p> 
<h2><a id="_16"></a>二、分词的粒度</h2> 
<p>从分词的粒度区分，主要包括3种类型，Word 、Subword、Char<br> <img src="https://images2.imgbox.com/e6/68/3O3XqRim_o.png" alt="在这里插入图片描述"></p> 
<p>1）Word分词粒度以完整的单词为单位进行分词，能够很好地保留每个词的语义，适合上下文理解和语义分析。然而，它面临着长尾效应和稀有词问题，可能导致词汇表庞大并且出现OOV（Out-of-Vocabulary）问题。</p> 
<blockquote> 
 <p>OOV是“Out-Of-Vocabulary”的缩写，直译为“词汇表外的”，在自然语言处理中，表示的是那些在词汇表中没有的单词</p> 
</blockquote> 
<p>2）Char分词粒度则是将文本拆分为字符级别，这样可以解决OOV问题，因为可以处理任何字符，但缺点是可能缺乏明确的语义信息，并且由于粒度过细，会增加后续处理的计算成本和时间。<br> 3）Subword分词粒度介于Word和Char之间，旨在克服两者的缺点，同时保留语义信息并减少OOV问题的发生。Subword分词方法如BPE（Byte Pair Encoding）或WordPiece通过统计学方法切分单词为更小的有意义的单元，这使得它们在处理生僻词和缩写时更为有效。<strong>（目前使用比较广泛）</strong></p> 
<h2><a id="_27"></a>三、分词器的类型</h2> 
<p>针对Subword常用的分词器有3种：BPE 分词、WordPiece 分词和 Unigram 分词。<br> <img src="https://images2.imgbox.com/b7/ac/rbhe0WBQ_o.png" alt="在这里插入图片描述"></p> 
<p><a href="https://github.com/google/sentencepiece">SentencePiece</a> 是一个开源的分词器工具；是由谷歌开发的，旨在提供一种高效的方式来对文本进行分词，尤其适用于处理变长和不规则的文本数据。它通过训练特定领域的模型来代替预训练模型中的词表，从而更有效地处理词汇。常用的BPE、WordPiece、 Unigram分词器都支持。</p> 
<h2><a id="BPEBBPE_33"></a>四、BPE/BBPE分词</h2> 
<p>1）BPE：从字符级别开始，逐步合并最频繁连续出现的字符或字符组合，形成新的词汇单元。<br> 2）BBPE：字节级别的 BPE（Byte-level BPE, B-BPE）是 BPE 算法的一种拓展。它将字节视为合并操作的基本符号，从而可以实现更细粒度的分割，且解决了未登录词问题。采用这种词元化方法的代表性语言模型包括 GPT-2 、BART 和 LLaMA 。<br> 3）对于英文、拉美体系的语言来说使用BPE分词足以在可接受的词表大小下解决OOV的问题，但面对中文、日文等语言时，其稀有的字符可能会不必要的占用词汇表（词汇表要么巨大要么会OOV），因此考虑使用字节级别byte-level解决不同语言进行分词时OOV的问题。具体的，BBPE将一段文本的UTF-8编码(UTF-8保证任何语言都可以通用)中的一个字节256位不同的编码作为词表的初始化基础Subword。</p> 
<p>例如，GPT-2 的词表大小为 50,257 ，包括 256 个字节的基本词元、一个特殊的文末词元以及通过 50,000 次合并学习到的词元。(相当于既有了BPE特性，又兼容了中文）<br> <img src="https://images2.imgbox.com/8d/d2/NhWug51R_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p><strong>BBPE的优点</strong>：不会出现 OOV 的情况。不管是怎样的汉字，只要可以用字节表示，就都会存在于初始词表中。<br> <strong>BBPE的缺点</strong>：一个汉字由3个字节组成，一个汉字就会被切成多个token，但实际上这多个token没必要进行训练。</p> 
</blockquote> 
<p>BPE词表构建整体流程如下：<br> <img src="https://images2.imgbox.com/af/72/fOGbUQjk_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="WordPiece_47"></a>五、WordPiece分词</h2> 
<p>1）WordPiece 分词和 BPE 分词的想法非常相似，都是通过迭代合并连续的词元，但是合并的选择标准略有不同WordPiece 分词算法并不选择最频繁的词对，而是使用下面的公式为每个词对计算分数<br> <img src="https://images2.imgbox.com/03/04/sfqrqJPE_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>比如unable，BPE 只关心 token pair 的出现频率，即 freq_of_pair；WordPiece 还考虑了每个 token 的出现频率。即使 unable 出现频率很高，但如果 un 和 able 单个 token 的出现频率都很高，也不会合并它们。</p> 
</blockquote> 
<p>2）WordPiece：就是将所有的「常用字」和「常用词」都存到词表中，当需要切词的时候就从词表里面查找即可。<br> WordPiece 的方式很有效，但当字词数目过于庞大时这个方式就有点难以实现了。对于一些多语言模型来讲，要想穷举所有语言中的常用词，这个量会非常大（穷举不全会造成 OOV）<br> <img src="https://images2.imgbox.com/9f/8e/iiSq47ke_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="Unigram__57"></a>六、Unigram 分词</h2> 
<p>Unigram分词器与BPE和WordPiece的不同在于它的构建过程。Unigram初始化时会创建一个非常大的词汇表，然后根据一定的标准逐步丢弃较不常用的词汇单元，直到满足限定的词汇表大小（比较适合处理生僻词）</p> 
<h2><a id="_60"></a>七、分词器的选择</h2> 
<p>大语言模型通常使用 SentencePiece 代码库为预训练语料训练定制化的分词器（也可以自定义）；<br> 这一代码库支持字节级别的 BPE 、 Unigram 、WordPiece分词。为了训练出高效的分词器，通常主要关注以下几个因素。<strong>首先，分词器必须具备无损重构的特性，即其分词结果能够准确无误地还原为原始输入文本。其次，分词器应具有高压缩率</strong>，即在给定文本数据的情况下，经过分词处理后的词元数量应尽可能少，从而实现更为高效的文本编码和存储。具体来说，压缩比可以通过将原始文本的 UTF-8 字节数除以分词器生成的词元数（即每个词元的平均字节数）来计算：<br> <img src="https://images2.imgbox.com/26/f7/SqEtoOiy_o.png" alt="在这里插入图片描述"></p> 
<p>例如，给定一段大小为 1MB（1,048,576 字节）的文本，如果它被分词为 200,000<br> 个词元，其压缩率即为 1,048,576/200,000=5.24</p> 
<h2><a id="_67"></a>八、各大模型的分词效果</h2> 
<p>分词效果：男儿何不带吴钩，收取关山五十州<br> <img src="https://images2.imgbox.com/b6/d0/um99f8FP_o.png" alt="在这里插入图片描述"></p> 
<p>1、LLaMA 词表是最小的，LLaMA 在中英文上的平均 token 数都是最多的，意味 LLaMA 对中英文分词都会比较碎，比较细粒度。<br> 尤其在中文上平均 token 数高达1.45，这意味着 LLaMA 大概率会将中文字符切分为2个以上的 token。<br> 2、Chinese LLaMA 扩展词表后，中文平均 token 数显著降低，会将一个汉字或两个汉字切分为一个 token，提高了中文编码效率。<br> 3、ChatGLM-6B 是平衡中英文分词效果最好的 tokenizer。由于词表比较大，中文处理时间也有增加。<br> 4、BLOOM 虽然是词表最大的，但由于是多语种的，在中英文上分词效率与 ChatGLM-6B 基本相当。<br> <img src="https://images2.imgbox.com/7e/84/Oarr9UWM_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="SentencePiece_79"></a>九、SentencePiece分词器使用</h2> 
<p>SentencePiece地址：https://github.com/google/sentencepiece<br> <strong>1）安装相关依赖</strong></p> 
<pre><code class="prism language-bash">pip <span class="token function">install</span> sentencepiece
</code></pre> 
<p><strong>2）分词器使用</strong></p> 
<pre><code class="prism language-bash">% spm_train <span class="token parameter variable">--input</span><span class="token operator">=</span><span class="token operator">&lt;</span>input<span class="token operator">&gt;</span> <span class="token parameter variable">--model_prefix</span><span class="token operator">=</span><span class="token operator">&lt;</span>model_name<span class="token operator">&gt;</span> <span class="token parameter variable">--vocab_size</span><span class="token operator">=</span><span class="token number">8000</span> <span class="token parameter variable">--character_coverage</span><span class="token operator">=</span><span class="token number">1.0</span> <span class="token parameter variable">--model_type</span><span class="token operator">=</span><span class="token operator">&lt;</span>type<span class="token operator">&gt;</span>
</code></pre> 
<p><strong>参数说明：</strong></p> 
<pre><code class="prism language-bash">--input：原始语料库文件，可以传递以逗号分隔的文件列表。
--model_prefix：输出的词表名称； 文件格式：<span class="token operator">&lt;</span>model_name<span class="token operator">&gt;</span>.model 、 <span class="token operator">&lt;</span>model_name<span class="token operator">&gt;</span>.vocab
--vocab_size：设置词表大小，例如 <span class="token number">8000</span>、16000 或 <span class="token number">32000</span>
--character_coverage：词表对语料库的覆盖率，默认：0.9995 对于具有丰富字符集的语言（如日语或中文）和其他具有小字符集的语言可以设置为1.0 （即对原料库的覆盖率为100%，包含语料库所有的单词）
--model_type：模型类型。unigram <span class="token punctuation">(</span>default<span class="token punctuation">)</span>, bpe, char, or word
</code></pre> 
<p>🔖更多专栏系列文章：🚩🚩🚩<a href="https://blog.csdn.net/xiaobing259/category_12628007.html?spm=1001.2014.3001.5482"><strong>AIGC-AI大模型探索之路</strong></a></p> 
<blockquote> 
 <p><strong>文章若有瑕疵，恳请不吝赐教；若有所触动或助益，还望各位老铁多多关注并给予支持。</strong></p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/55f01bbade0ff45f3cc13b8547eef117/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">用自然语言来编程GitHub Copilot；提高代码质量开源工具GPTLint；LLMs开源医学Meditron</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/15e4e8df1bc99abef84c0ab463baaad2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">使用Amazon SageMaker构建高质量AI作画模型Stable Diffusion_sagemaker ai绘图</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>