<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>统计文本词频的几种方法（Python） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/61487ca242cd54c4aa02bba013d6689d/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="统计文本词频的几种方法（Python）">
  <meta property="og:description" content="目录
1. 单句的词频统计
2. 文章的词频统计
方法一：运用集合去重方法
方法二：运用字典统计
方法三：使用计数器
词频统计是自然语言处理的基本任务，针对一段句子、一篇文章或一组文章，统计文章中每个单词出现的次数，在此基础上发现文章的主题词、热词。
1. 单句的词频统计 思路：首先定义一个空字典my_dict，然后遍历文章（或句子），针对每个单词判断是否在字典my_dict的key中，不存在就将该单词当作my_dict的key，并设置对应的value值为1；若已存在，则将对应的value值&#43;1。
#统计单句中每个单词出现的次数 news = &#34;Xi, also general secretary of the Communist Party of China (CPC) Central Committee and chairman of the Central Military Commission, made the remarks while attending a voluntary tree-planting activity in the Chinese capital&#39;s southern district of Daxing.&#34; def couWord(news_list): ##定义计数函数 输入：句子的单词列表 输出：单词-次数 的字典 my_dict = {} #空字典 来保存单词出现的次数 for v in news_list: if my_dict.get(v): my_dict[v] &#43;= 1 else: my_dict[v] = 1 return my_dict print(couWord(news.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-10-25T22:49:31+08:00">
    <meta property="article:modified_time" content="2023-10-25T22:49:31+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">统计文本词频的几种方法（Python）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="1.%20%E5%8D%95%E5%8F%A5%E7%9A%84%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1-toc" style="margin-left:40px;"><a href="#1.%20%E5%8D%95%E5%8F%A5%E7%9A%84%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1" rel="nofollow">1. 单句的词频统计</a></p> 
<p id="2.%20%E6%96%87%E7%AB%A0%E7%9A%84%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1-toc" style="margin-left:40px;"><a href="#2.%20%E6%96%87%E7%AB%A0%E7%9A%84%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1" rel="nofollow">2. 文章的词频统计</a></p> 
<p id="%E6%96%B9%E6%B3%95%E4%B8%80%EF%BC%9A%E8%BF%90%E7%94%A8%E9%9B%86%E5%90%88%E5%8E%BB%E9%87%8D%E6%96%B9%E6%B3%95-toc" style="margin-left:40px;"><a href="#%E6%96%B9%E6%B3%95%E4%B8%80%EF%BC%9A%E8%BF%90%E7%94%A8%E9%9B%86%E5%90%88%E5%8E%BB%E9%87%8D%E6%96%B9%E6%B3%95" rel="nofollow">方法一：运用集合去重方法</a></p> 
<p id="%E6%96%B9%E6%B3%95%E4%BA%8C%EF%BC%9A%E8%BF%90%E7%94%A8%E5%AD%97%E5%85%B8%E7%BB%9F%E8%AE%A1-toc" style="margin-left:40px;"><a href="#%E6%96%B9%E6%B3%95%E4%BA%8C%EF%BC%9A%E8%BF%90%E7%94%A8%E5%AD%97%E5%85%B8%E7%BB%9F%E8%AE%A1" rel="nofollow">方法二：运用字典统计</a></p> 
<p id="%E6%96%B9%E6%B3%95%E4%B8%89%EF%BC%9A%E4%BD%BF%E7%94%A8%E8%AE%A1%E6%95%B0%E5%99%A8-toc" style="margin-left:40px;"><a href="#%E6%96%B9%E6%B3%95%E4%B8%89%EF%BC%9A%E4%BD%BF%E7%94%A8%E8%AE%A1%E6%95%B0%E5%99%A8" rel="nofollow">方法三：使用计数器</a></p> 
<hr id="hr-toc"> 
<p></p> 
<p>词频统计是自然语言处理的基本任务，针对一段句子、一篇文章或一组文章，统计文章中每个单词出现的次数，在此基础上发现文章的主题词、热词。</p> 
<h3 id="1.%20%E5%8D%95%E5%8F%A5%E7%9A%84%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1">1. 单句的词频统计</h3> 
<p>思路：首先定义一个空字典<code>my_dict</code>，然后遍历文章（或句子），针对每个单词判断是否在字典<code>my_dict</code>的<code>key</code>中，不存在就将该单词当作<code>my_dict</code>的<code>key</code>，并设置对应的<code>value</code>值为1；若已存在，则将对应的<code>value</code>值+1。</p> 
<pre><code>#统计单句中每个单词出现的次数
news = "Xi, also general secretary of the Communist Party of China (CPC) Central Committee and chairman of the Central Military Commission, made the remarks while attending a voluntary tree-planting activity in the Chinese capital's southern district of Daxing."    
</code></pre> 
<pre><code>def couWord(news_list): 
    ##定义计数函数  输入：句子的单词列表 输出：单词-次数 的字典
    my_dict = {}  #空字典 来保存单词出现的次数
    for v in news_list:
        if my_dict.get(v):
            my_dict[v] += 1
        else:
            my_dict[v] = 1
    return my_dict

print(couWord(news.split ()))
</code></pre> 
<p>输出</p> 
<pre><code>{‘Xi,’: 1, ‘also’: 1, ‘general’: 1, ‘secretary’: 1, ‘of’: 4, ‘the’: 4, ‘Communist’: 1, ‘Party’: 1, ‘China’: 1, ‘(CPC)’: 1, ‘Central’: 2, ‘Committee’: 1, ‘and’: 1, ‘chairman’: 1, ‘Military’: 1, ‘Commission,’: 1, ‘made’: 1, ‘remarks’: 1, ‘while’: 1, ‘attending’: 1, ‘a’: 1, ‘voluntary’: 1, ‘tree-planting’: 1, ‘activity’: 1, ‘in’: 1, ‘Chinese’: 1, “capital’s”: 1, ‘southern’: 1, ‘district’: 1, ‘Daxing.’: 1}</code></pre> 
<p>以上通过couWord方法实现了词频的统计，但是存在以下两个问题。</p> 
<p>（1）未去除stopword</p> 
<p>输出结果中保护’also’、‘and’、'in’等stopword（停止词），停止词语与文章主题关系不大，需要在词频统计等各类处理中将其过滤掉。</p> 
<p>（2）未根据出现次数进行排序</p> 
<p>根据每个单词出现次数进行排序后，可以直观而有效的发现文章主题词或热词。</p> 
<p>改进后的couWord函数如下：</p> 
<pre><code>def couWord(news_list,word_list,N):
    #输入 文章单词的列表 停止词列表  输出：Top N的单词
    my_dict = {}  #空字典 来保存单词出现的次数
    for v in news_list:
        if (v not in word_list): # 判断是否在停止词列表中
            if my_dict.get(v):
                my_dict[v] += 1
            else:
                my_dict[v] = 1
                  
    topWord = sorted(zip(my_dict.values(),my_dict.keys()),reverse=True)[:N] 
    
    return topWord
</code></pre> 
<p>加载英文停止词列表：</p> 
<pre><code>stopPath = r'Data/stopword.txt'
with open(stopPath,encoding = 'utf-8') as file:
    word_list = file.read().split()      #通过read()返回一个字符串函数，再将其转换成列表

print(couWord(news.split(),word_list,5)) </code></pre> 
<p>输出</p> 
<p>[(2, ‘Central’), (1, ‘voluntary’), (1, ‘tree-planting’), (1, ‘southern’), (1, ‘secretary’)]</p> 
<h3 id="2.%20%E6%96%87%E7%AB%A0%E7%9A%84%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1">2. 文章的词频统计</h3> 
<p><strong>（1）单篇文章词频统计</strong></p> 
<p>通过定义读取文章的函数，对其进行大小写转换等处理，形成输入文章的单词列表。</p> 
<p><a href="https://python123.io/resources/pye/hamlet.txt" rel="nofollow" title="https://python123.io/resources/pye/hamlet.txt">https://python123.io/resources/pye/hamlet.txt</a></p> 
<p>以上为hamlet英文版文本的获取路径，下载完成后保存到工程路径下。</p> 
<p>使用open()函数打开hamlet.txt文件，并使用read()方法读取文件内容，将文本保存在txt变量中。</p> 
<pre><code>def readFile(filePath): 
    #输入： 文件路径  输出：字符串列表
    with open(filePath,encoding = 'utf-8') as file:
        txt = file.read().lower() #返回一个字符串,都是小写
        words = txt.split()      #转换成列表 
    
    return words

filePath = r'Data/news/hamlet.txt'
new_list = readFile(filePath)  #读取文件
print(couWord(new_list,word_list,5))
</code></pre> 
<p>接下来，我们需要对文本进行预处理，去除标点符号、分割成单词等。我们可以使用正则表达式来实现这一步骤。</p> 
<pre><code>import re

# 去除标点符号
text = re.sub(r'[^\w\s]', '', text)

# 分割成单词
words = text.split()</code></pre> 
<p>我们使用re.sub()函数和正则表达式[^\w\s]来去除标点符号，然后使用split()方法将文本分割成单词，并将结果保存在words列表中。</p> 
<p>或者：</p> 
<p>我们的文本中含有标点和字符的噪声数据，所以要进行数据的清洗，将文档全部处理为只有我们需要的字母类型（为方便操作，用空格替换噪声数据，将文档全部转化为小写字母）</p> 
<p>打开文件，进行读取，清洗数据，数据归档。</p> 
<pre><code>def getText():
    txt = open("Hmlet.txt","r").read()
    txt = txt.lower()
    for ch in '!@#$%^&amp;*()_/*-~':
        txt = txt.replace(ch," ")
    return txt


hamlet = getText()
words = hamlet.split()
counts = {}
for word in words:
    counts[word] = counts.get(word,0) + 1

items = list(counts.items())
items.sort(key= lambda x:x[1],reverse=True)
for i in range(10):
    word, count = items[i]
    print("{0:&lt;10}{1:&gt;5}".format(word,count))</code></pre> 
<p>现在，我们已经得到了分割后的单词列表words，接下来我们需要统计每个单词出现的次数。我们可以使用Python的字典数据结构来实现词频统计。</p> 
<pre><code>word_counts = {}

for word in words:
    if word in word_counts:
        word_counts[word] += 1
    else:
        word_counts[word] = 1</code></pre> 
<p>这段代码中，我们首先创建一个空字典word_counts，然后遍历words列表中的每个单词。对于每个单词，如果它已经在word_counts字典中存在，则将对应的计数值加1；否则，在字典中新增一个键值对，键为单词，值为1。</p> 
<p>在统计完词频后，我们需要按照词频降序排序，以便后续输出结果。我们可以使用Python的内置函数sorted()来实现排序。</p> 
<pre><code>sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
</code></pre> 
<p>我们使用word_counts.items()方法获取word_counts字典中的所有键值对，并使用key=lambda x: x[1]指定按照键值对中的值进行排序，reverse=True表示降序排列。排序结果将保存在sorted_word_counts列表中。</p> 
<p>最后，我们将词频统计结果输出到控制台或文件中。</p> 
<pre><code>for word, count in sorted_word_counts:
    print(f'{word}: {count}')</code></pre> 
<p>这段代码中，我们使用for循环遍历sorted_word_counts列表中的每个元素（每个元素是一个键值对），并使用print()函数输出单词和对应的词频。</p> 
<p></p> 
<p><strong>（2）多篇文章词频统计</strong></p> 
<p>需要使用<code>os.listdir</code>方法读取文件夹下的文件列表，然后对文件逐一进行处理。</p> 
<pre><code>import os 
folderPath = r'Data/news' #文件夹路径
tmpFile = os.listdir(folderPath)
allNews = []
for file in tmpFile:  #读取文件
    newsfile = folderPath + '//' + file #拼接完整的文件路径  \\ 转义字符
    allNews += readFile(newsfile)   #把所有的字符串列表拼接到allText中
    
print(couWord(allNews,word_list,5))  
</code></pre> 
<p>输出</p> 
<blockquote> 
 <p>[(465, ‘china’), (323, ‘chinese’), (227, ‘xi’), (196, “china’s”), (134, ‘global’)]</p> 
</blockquote> 
<p><strong>（3）中文文章的处理</strong></p> 
<p>对于中文文章的词频统计，首先要使用<code>jieba</code>等分词器对文章进行分词，并且加载中文的停止词列表，再进行词频统计。</p> 
<h5>3.三国演义人物出场频数</h5> 
<p>利用jieba库，进行中文分词，将其存入列表words中，遍历，将词组和词频作为键值对存入列表counts中，利用列表的有序性，进行排序，然后输出</p> 
<p>https://python123.io/resources/pye/threekingdoms.txt</p> 
<p>以上为三国演义中文版文本获取链接，下载后保存到工程路径下</p> 
<pre><code class="hljs">import jieba
txt = open("threekingdoms.txt","r",encoding="utf-8").read()
counts = {}
words = jieba.lcut(txt)
for word in words:
    if len(word) == 1:
        continue
    else:
        counts[word] = counts.get(word,0) + 1
items = list(counts.items())
items.sort(key = lambda x:x[1] , reverse=True)
for i in range(15):
    word , count = items[i]
    print("{0:&lt;10}{1:&gt;5}".format(word,count))</code></pre> 
<p>该方法比英文哈姆雷特词频简单，不用去处理字符类噪声数据，这也得益于jieba库的简易操作。</p> 
<p>但随之带来的是词频的模糊，因为jieba库的特性，导致不是人名的词组也被统计了进来。</p> 
<p>如结果中的“二人”、”孔明曰“，这些都是冗余和词组问题的错误。</p> 
<p>所以我们应该还需要进行进一步的处理，让词频统计人物的名字次数</p> 
<p>经过前几步的操作，我们输出了出现频率最高的15给词组，可我们如果想要人物的出场频率呢？ 这就需要对原文件进行过滤，把我们不需要的输出删除。</p> 
<p>因为之前的输出可以简单的获取到出现频率高但不是人名的词组，所以我们这里把它们储存到一个集合中，遍历并删除原文件中存在的这些词组。</p> 
<pre><code>excludes = {"将军","却说","二人","不可","荆州","不能","如此","商议","如何","主公","军士","左右","军马"}</code></pre> 
<pre><code>for i in excludes:
    del counts[i]</code></pre> 
<p>冗余处理：把出现频率高的相同人物别名进行统一</p> 
<pre><code> elif word == "诸葛亮" or word == "孔明曰":
        rword = "孔明"
    elif word == "关公" or word == "云长":
        rword = "关羽"
    elif word == "玄德" or word == "玄德曰":
        rword = "刘备"
    elif word == "孟德" or word ==  "丞相":
        rword = "曹操"</code></pre> 
<p> 反复的经过这些处理，我们可以得到我们想要的输出</p> 
<pre><code>import jieba
txt = open("threekingdoms.txt","r",encoding="utf-8").read()
counts = {}
excludes = {"将军","却说","二人","不可","荆州","不能","如此","商议","如何","主公","军士","左右","军马"}
words = jieba.lcut(txt)
for word in words:
    if len(word) == 1:
        continue
    elif word == "诸葛亮" or word == "孔明曰":
        rword = "孔明"
    elif word == "关公" or word == "云长":
        rword = "关羽"
    elif word == "玄德" or word == "玄德曰":
        rword = "刘备"
    elif word == "孟德" or word ==  "丞相":
        rword = "曹操"
    else:
        rword = word
    counts[rword] = counts.get(rword,0) + 1
for i in excludes:
    del counts[i]
items = list(counts.items())
items.sort(key = lambda x:x[1] , reverse=True)
for i in range(7):
    word,count = items[i]
    print("{0:&lt;10}{1:&gt;5}".format(word,count))</code></pre> 
<p> </p> 
<p class="img-center"><img alt="" height="211" src="https://images2.imgbox.com/60/09/Xgqv6mnQ_o.png" width="198"></p> 
<h3></h3> 
<h3 id="%E6%96%B9%E6%B3%95%E4%B8%80%EF%BC%9A%E8%BF%90%E7%94%A8%E9%9B%86%E5%90%88%E5%8E%BB%E9%87%8D%E6%96%B9%E6%B3%95">方法一：运用集合去重方法</h3> 
<table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td> <p>1</p> <p>2</p> <p>3</p> <p>4</p> <p>5</p> <p>6</p> <p>7</p> <p>8</p> <p>9</p> </td><td> <p><code>def</code> <code>word_count1(words,n):</code></p> <p><code>   </code><code>word_list </code><code>=</code> <code>[]</code></p> <p><code>   </code><code>for</code> <code>word </code><code>in</code> <code>set</code><code>(words):</code></p> <p><code>       </code><code>num </code><code>=</code> <code>words.counts(word)</code></p> <p><code>       </code><code>word_list.append([word,num])</code></p> <p><code>       </code><code>word_list.sort(key</code><code>=</code><code>lambda</code> <code>x:x[</code><code>1</code><code>], reverse</code><code>=</code><code>True</code><code>)</code></p> <p><code>   </code><code>for</code> <code>i </code><code>in</code> <code>range</code><code>(n):</code></p> <p><code>       </code><code>word, count </code><code>=</code> <code>word_list[i]</code></p> <p><code>       </code><code>print</code><code>(</code><code>'{0:&lt;15}{1:&gt;5}'</code><code>.</code><code>format</code><code>(word, count))</code></p> </td></tr></tbody></table> 
<p>说明：运用集合对文本字符串列表去重，这样统计词汇不会重复，运用列表的counts方法统计频数，将每个词汇和其出现的次数打包成一个列表加入到word_list中，运用列表的sort方法排序，大功告成。</p> 
<p><a name="_label1"></a></p> 
<h3 id="%E6%96%B9%E6%B3%95%E4%BA%8C%EF%BC%9A%E8%BF%90%E7%94%A8%E5%AD%97%E5%85%B8%E7%BB%9F%E8%AE%A1">方法二：运用字典统计</h3> 
<table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td> <p>1</p> <p>2</p> <p>3</p> <p>4</p> <p>5</p> <p>6</p> <p>7</p> <p>8</p> <p>9</p> <p>10</p> <p>11</p> <p>12</p> </td><td> <p><code>def</code> <code>word_count2(words,n):</code></p> <p><code>    </code><code>counts </code><code>=</code> <code>{}</code></p> <p><code>    </code><code>for</code> <code>word </code><code>in</code> <code>words:</code></p> <p><code>        </code><code>if</code> <code>len</code><code>(word) </code><code>=</code><code>=</code> <code>1</code><code>:</code></p> <p><code>            </code><code>continue</code></p> <p><code>        </code><code>else</code><code>:</code></p> <p><code>            </code><code>counts[word] </code><code>=</code> <code>counts.get(word, </code><code>0</code><code>) </code><code>+</code> <code>1</code></p> <p><code>    </code><code>items </code><code>=</code> <code>list</code><code>(counts.items())</code></p> <p><code>    </code><code>items.sort(key</code><code>=</code><code>lambda</code> <code>x:x[</code><code>1</code><code>], reverse</code><code>=</code><code>True</code><code>)</code></p> <p><code>    </code><code>for</code> <code>i </code><code>in</code> <code>range</code><code>(n):</code></p> <p><code>        </code><code>word, count </code><code>=</code> <code>items[i]</code></p> <p><code>        </code><code>print</code><code>(</code><code>"{0:&lt;15}{1:&gt;5}"</code><code>.</code><code>format</code><code>(word, count))</code></p> </td></tr></tbody></table> 
<p><a name="_label2"></a></p> 
<h3 id="%E6%96%B9%E6%B3%95%E4%B8%89%EF%BC%9A%E4%BD%BF%E7%94%A8%E8%AE%A1%E6%95%B0%E5%99%A8">方法三：使用计数器</h3> 
<table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td> <p>1</p> <p>2</p> <p>3</p> <p>4</p> <p>5</p> <p>6</p> <p>7</p> </td><td> <p><code>def</code> <code>word_count3(words,n):</code></p> <p><code>    </code><code>from</code> <code>collections </code><code>import</code> <code>Counter</code></p> <p><code>    </code><code>counts </code><code>=</code> <code>Counter(words)</code></p> <p><code>    </code><code>for</code> <code>ch </code><code>in</code> <code>"":  </code><code># 删除一些不需要统计的元素</code></p> <p><code>        </code><code>del</code> <code>counts[ch]</code></p> <p><code>    </code><code>for</code> <code>word, count </code><code>in</code> <code>counts.most_common(n):  </code><code># 已经按数量大小排好了</code></p> <p><code>        </code><code>print</code><code>(</code><code>"{0:&lt;15}{1:&gt;5}"</code><code>.</code><code>format</code><code>(word, count))</code></p> </td></tr></tbody></table>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b63f41add44c18d2adaea4dfb84862a3/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">MySQL中的substr()函数</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/407c09b1471ac53d4ef41c177eaddde0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">mac文件夹无法写入 mac只能读取不能写入怎么解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>