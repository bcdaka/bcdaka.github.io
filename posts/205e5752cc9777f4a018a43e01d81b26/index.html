<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>人工智能--搭建人工神经网络 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/205e5752cc9777f4a018a43e01d81b26/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="人工智能--搭建人工神经网络">
  <meta property="og:description" content="欢迎来到 Papicatch的博客
文章目录
🍉引言
🍉神经元与感知器
🍈神经元（Neuron） 🍈感知器
🍉损失函数与梯度下降算法
🍈损失函数
🍈梯度下降算法
🍉多层感知器与神经网络
🍈多层感知器（MLP）
🍈激活函数
🍈反向传播算法
🍉实例
🍈手工搭建神经网络
🍉总结
🍉引言 人工神经网络（Artificial Neural Networks, ANN）是一种受生物神经系统启发的计算模型，能够学习和执行复杂的非线性映射任务。本文将深入探讨神经元、感知器、损失函数、梯度下降算法、多层感知器（MLP）、激活函数、反向传播算法，并通过实例展示如何手工搭建一个神经网络。
🍉神经元与感知器 🍈神经元（Neuron） 神经元是神经网络的基本单元，模仿生物神经元的结构和功能。它接收来自其他神经元或外部输入的信号，通过加权求和和激活函数转换后输出结果。
一个简单的神经元模型如下：
其中，𝑥𝑖xi​ 是输入信号，𝑤𝑖wi​ 是对应的权重，𝑏b 是偏置项，𝜎σ 是激活函数，如 sigmoid、ReLU 等。
🍈感知器 感知器模型结构：
🍈感知器实现and函数 逻辑运算and的真值表
x1 x2
y（and运行结果）0（假）0（假）0（假）0（假）1（真）0（假）1（真）0（假）0（假）1（真）1（真）1（真） 🍈代码实现 # 定义AND运算的函数 def AND(a, b): return a &amp; b # 真值表的输入组合 inputs = [ (0, 0), (0, 1), (1, 0), (1, 1) ] # 打印真值表 print(&#34;A | B | A AND B&#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-21T22:35:27+08:00">
    <meta property="article:modified_time" content="2024-06-21T22:35:27+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">人工智能--搭建人工神经网络</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><img alt="2a20c54b85e042bfa2440367ae4807e9.gif" height="53" src="https://images2.imgbox.com/cc/50/r7t10I8l_o.gif" width="1000"></p> 
<p style="text-align:center;"><span style="color:#ffd900;"><strong>欢迎来到 Papicatch的博客</strong></span></p> 
<p><img alt="" height="53" src="https://images2.imgbox.com/12/e9/ZQD6VgB2_o.gif" width="1000"></p> 
<p id="main-toc"><strong>文章目录</strong></p> 
<p id="%F0%9F%8D%89%E5%BC%95%E8%A8%80-toc" style="margin-left:0px;"><a href="#%F0%9F%8D%89%E5%BC%95%E8%A8%80" rel="nofollow">🍉引言</a></p> 
<p id="%F0%9F%8D%89%E7%A5%9E%E7%BB%8F%E5%85%83%E4%B8%8E%E6%84%9F%E7%9F%A5%E5%99%A8-toc" style="margin-left:0px;"><a href="#%F0%9F%8D%89%E7%A5%9E%E7%BB%8F%E5%85%83%E4%B8%8E%E6%84%9F%E7%9F%A5%E5%99%A8" rel="nofollow">🍉神经元与感知器</a></p> 
<p id="%F0%9F%8D%88%E7%A5%9E%E7%BB%8F%E5%85%83%EF%BC%88Neuron%EF%BC%89%C2%A0-toc" style="margin-left:40px;"><a href="#%F0%9F%8D%88%E7%A5%9E%E7%BB%8F%E5%85%83%EF%BC%88Neuron%EF%BC%89%C2%A0" rel="nofollow">🍈神经元（Neuron） </a></p> 
<p id="%F0%9F%8D%88%E6%84%9F%E7%9F%A5%E5%99%A8-toc" style="margin-left:40px;"><a href="#%F0%9F%8D%88%E6%84%9F%E7%9F%A5%E5%99%A8" rel="nofollow">🍈感知器</a></p> 
<p id="%F0%9F%8D%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95-toc" style="margin-left:0px;"><a href="#%F0%9F%8D%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95" rel="nofollow">🍉损失函数与梯度下降算法</a></p> 
<p id="%F0%9F%8D%88%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-toc" style="margin-left:40px;"><a href="#%F0%9F%8D%88%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0" rel="nofollow">🍈损失函数</a></p> 
<p id="%F0%9F%8D%88%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95-toc" style="margin-left:40px;"><a href="#%F0%9F%8D%88%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95" rel="nofollow">🍈梯度下降算法</a></p> 
<p id="%F0%9F%8D%89%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-toc" style="margin-left:0px;"><a href="#%F0%9F%8D%89%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" rel="nofollow">🍉多层感知器与神经网络</a></p> 
<p id="%F0%9F%8D%88%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%EF%BC%88MLP%EF%BC%89-toc" style="margin-left:40px;"><a href="#%F0%9F%8D%88%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%EF%BC%88MLP%EF%BC%89" rel="nofollow">🍈多层感知器（MLP）</a></p> 
<p id="%F0%9F%8D%88%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-toc" style="margin-left:40px;"><a href="#%F0%9F%8D%88%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0" rel="nofollow">🍈激活函数</a></p> 
<p id="%F0%9F%8D%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95-toc" style="margin-left:40px;"><a href="#%F0%9F%8D%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95" rel="nofollow">🍈反向传播算法</a></p> 
<p id="%F0%9F%8D%89%E5%AE%9E%E4%BE%8B-toc" style="margin-left:0px;"><a href="#%F0%9F%8D%89%E5%AE%9E%E4%BE%8B" rel="nofollow">🍉实例</a></p> 
<p id="%F0%9F%8D%88%E6%89%8B%E5%B7%A5%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-toc" style="margin-left:40px;"><a href="#%F0%9F%8D%88%E6%89%8B%E5%B7%A5%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" rel="nofollow">🍈手工搭建神经网络</a></p> 
<p id="%F0%9F%8D%89%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;"><a href="#%F0%9F%8D%89%E6%80%BB%E7%BB%93" rel="nofollow">🍉总结</a></p> 
<hr id="hr-toc"> 
<p></p> 
<p><img alt="2a20c54b85e042bfa2440367ae4807e9.gif" height="53" src="https://images2.imgbox.com/a8/0d/wMvn4uFy_o.gif" width="1000"></p> 
<h2 id="%F0%9F%8D%89%E5%BC%95%E8%A8%80" style="margin-left:.0001pt;text-align:justify;">🍉引言</h2> 
<p style="margin-left:.0001pt;text-align:justify;">      人工神经网络（Artificial Neural Networks, ANN）是一种受生物神经系统启发的计算模型，能够学习和执行复杂的非线性映射任务。本文将深入探讨神经元、感知器、损失函数、梯度下降算法、多层感知器（MLP）、激活函数、反向传播算法，并通过实例展示如何手工搭建一个神经网络。</p> 
<h2 id="%F0%9F%8D%89%E7%A5%9E%E7%BB%8F%E5%85%83%E4%B8%8E%E6%84%9F%E7%9F%A5%E5%99%A8" style="text-align:left;"><strong><strong><strong>🍉神经元与感知器</strong></strong></strong></h2> 
<h3 id="%F0%9F%8D%88%E7%A5%9E%E7%BB%8F%E5%85%83%EF%BC%88Neuron%EF%BC%89%C2%A0" style="margin-left:0px;text-align:left;">🍈神经元（Neuron） </h3> 
<p style="margin-left:0;text-align:left;">神经元是神经网络的基本单元，模仿生物神经元的结构和功能。它接收来自其他神经元或外部输入的信号，通过加权求和和激活函数转换后输出结果。</p> 
<p style="margin-left:0;text-align:left;"><strong>一个简单的神经元模型如下：</strong></p> 
<p class="img-center"><img alt="" height="153" src="https://images2.imgbox.com/2d/01/P0ajAs0S_o.png" width="348"></p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong>其中，𝑥𝑖xi​ 是输入信号，𝑤𝑖wi​ 是对应的权重，𝑏b 是偏置项，𝜎σ 是激活函数，如 sigmoid、ReLU 等。</strong></p> 
<h3 id="%F0%9F%8D%88%E6%84%9F%E7%9F%A5%E5%99%A8">🍈感知器</h3> 
<p><strong>感知器模型结构：</strong></p> 
<p class="img-center"><img alt="" height="1079" src="https://images2.imgbox.com/58/fe/e88JBNJN_o.jpg" width="1200"></p> 
<h3 style="margin-left:.0001pt;text-align:justify;">🍈感知器实现and函数</h3> 
<p style="text-align:center;"><strong>逻辑运算and的真值表</strong></p> 
<table align="center"><thead><tr><th>x1</th><th> <p>x2</p> </th><th>y（and运行结果）</th></tr></thead><tbody><tr><td>0（假）</td><td>0（假）</td><td>0（假）</td></tr><tr><td>0（假）</td><td>1（真）</td><td>0（假）</td></tr><tr><td>1（真）</td><td>0（假）</td><td>0（假）</td></tr><tr><td>1（真）</td><td>1（真）</td><td>1（真）</td></tr></tbody></table> 
<h3>🍈代码实现</h3> 
<pre><code class="language-python"># 定义AND运算的函数
def AND(a, b):
    return a &amp; b

# 真值表的输入组合
inputs = [
    (0, 0),
    (0, 1),
    (1, 0),
    (1, 1)
]

# 打印真值表
print("A | B | A AND B")
print("---|---|-------")
for a, b in inputs:
    result = AND(a, b)
    print(f" {a} | {b} |   {result}")
</code></pre> 
<p class="img-center"><img alt="" height="167" src="https://images2.imgbox.com/2a/b8/BEfuWlIi_o.png" width="294"></p> 
<p style="margin-left:.0001pt;text-align:justify;">    感知器（Perceptron）是最简单的神经网络形式，包含一个单层神经元，直接将输入映射到输出，通常用于二分类问题。</p> 
<h2 id="%F0%9F%8D%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><strong><strong><strong>🍉</strong></strong></strong>损失函数与梯度下降算法</h2> 
<h3 id="%F0%9F%8D%88%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0">🍈损失函数</h3> 
<p>        损失函数（Loss Function）衡量神经网络预测值与实际标签之间的差异。</p> 
<p><strong>常见的损失函数包括：</strong></p> 
<blockquote> 
 <ul><li>均方误差（Mean Squared Error, MSE）：适用于回归问题。</li><li>交叉熵损失函数（Cross-Entropy Loss）：适用于分类问题。</li></ul> 
</blockquote> 
<h3 id="%F0%9F%8D%88%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95">🍈梯度下降算法</h3> 
<p style="margin-left:.0001pt;text-align:justify;">        梯度下降算法通过最小化损失函数来优化神经网络的参数。核心思想是沿着损失函数梯度的反方向更新权重和偏置，从而逐步改进模型的预测能力。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong>具体步骤如下：</strong></p> 
<blockquote> 
 <ul><li>计算损失函数的梯度：使用反向传播算法计算每个参数对损失函数的影响。</li><li>更新权重和偏置：通过学习率（learning rate）控制更新步长，减小损失函数值。</li></ul> 
</blockquote> 
<p class="img-center"><img alt="" height="878" src="https://images2.imgbox.com/6b/8b/dhoyFF52_o.png" width="1200"></p> 
<h2 id="%F0%9F%8D%89%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><strong><strong><strong>🍉</strong></strong></strong>多层感知器与神经网络</h2> 
<p>        异或（XOR）问题是经典的逻辑运算问题，感知器不能拟合出一条直线将结果分开。要将二者分开，必须采用封闭式的曲线才行。多层感知器可以实现。</p> 
<p style="text-align:center;"><strong>异或（XOR）真值表</strong></p> 
<table><thead><tr><th>A</th><th>𝐵B</th><th>𝐴 XOR 𝐵A XOR B</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>1</td><td>1</td></tr><tr><td>1</td><td>0</td><td>1</td></tr><tr><td>1</td><td>1</td><td>0</td></tr></tbody></table> 
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

# 定义激活函数（sigmoid）及其导数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# 输入数据和目标输出
inputs = np.array([[0, 0],
                   [0, 1],
                   [1, 0],
                   [1, 1]])

expected_output = np.array([[0],
                            [1],
                            [1],
                            [0]])

# 初始化参数
input_layer_neurons = inputs.shape[1]
hidden_layer_neurons = 2
output_neurons = 1

# 初始化权重和偏置
hidden_weights = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))
hidden_bias = np.random.uniform(size=(1, hidden_layer_neurons))
output_weights = np.random.uniform(size=(hidden_layer_neurons, output_neurons))
output_bias = np.random.uniform(size=(1, output_neurons))

# 设置学习率和迭代次数
learning_rate = 0.1
epochs = 10000
error_history = []

# 训练神经网络
for epoch in range(epochs):
    # 前向传播
    hidden_layer_activation = np.dot(inputs, hidden_weights)
    hidden_layer_activation += hidden_bias
    hidden_layer_output = sigmoid(hidden_layer_activation)

    output_layer_activation = np.dot(hidden_layer_output, output_weights)
    output_layer_activation += output_bias
    predicted_output = sigmoid(output_layer_activation)

    # 计算误差
    error = expected_output - predicted_output
    error_history.append(np.mean(np.abs(error)))
    d_predicted_output = error * sigmoid_derivative(predicted_output)

    # 反向传播
    error_hidden_layer = d_predicted_output.dot(output_weights.T)
    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)

    # 更新权重和偏置
    output_weights += hidden_layer_output.T.dot(d_predicted_output) * learning_rate
    output_bias += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate
    hidden_weights += inputs.T.dot(d_hidden_layer) * learning_rate
    hidden_bias += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate

# 打印结果
print("Final hidden weights: ", hidden_weights)
print("Final hidden bias: ", hidden_bias)
print("Final output weights: ", output_weights)
print("Final output bias: ", output_bias)
print("Predicted output: ", predicted_output)

# 绘制误差下降图
plt.plot(error_history)
plt.title('Error History')
plt.xlabel('Epoch')
plt.ylabel('Error')
plt.show()

# 绘制神经网络预测结果图
def plot_decision_boundary(X, y, model, title):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                         np.arange(y_min, y_max, 0.1))
    Z = model(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, alpha=0.8)
    plt.scatter(X[:, 0], X[:, 1], c=y.flatten(), s=40, edgecolor='k')
    plt.title(title)
    plt.show()

# 定义预测函数
def predict(X):
    hidden_layer_activation = np.dot(X, hidden_weights) + hidden_bias
    hidden_layer_output = sigmoid(hidden_layer_activation)
    output_layer_activation = np.dot(hidden_layer_output, output_weights) + output_bias
    predicted_output = sigmoid(output_layer_activation)
    return np.round(predicted_output)

# 绘制决策边界
plot_decision_boundary(inputs, expected_output, predict, 'XOR Decision Boundary')
</code></pre> 
<p class="img-center"><img alt="" height="961" src="https://images2.imgbox.com/d3/36/7E0fRCLI_o.png" width="978"></p> 
<p class="img-center"><img alt="" height="691" src="https://images2.imgbox.com/3b/2d/LHVqSlvD_o.png" width="970"></p> 
<h3 id="%F0%9F%8D%88%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%EF%BC%88MLP%EF%BC%89">🍈多层感知器（MLP）</h3> 
<p style="margin-left:.0001pt;text-align:justify;">        多层感知器（MLP）是一种前向结构的神经网络，由多个全连接隐藏层和至少一个输出层组成。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong>每个神经元在每层中执行以下步骤：</strong></p> 
<blockquote> 
 <ul><li>线性变换：计算加权输入的和。</li><li>非线性变换（激活函数）：通过激活函数如 sigmoid、ReLU 将结果映射到非线性空间。</li></ul> 
</blockquote> 
<h3 id="%F0%9F%8D%88%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">🍈激活函数</h3> 
<p style="margin-left:.0001pt;text-align:justify;">        激活函数是神经网络中每个神经元的非线性映射函数，常见的有：</p> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">Sigmoid 函数：将输入值压缩到0到1之间。</p> 
</blockquote> 
<p class="img-center"><img alt="" height="85" src="https://images2.imgbox.com/18/64/KXmDnHzv_o.png" width="214"></p> 
<blockquote> 
 <p style="margin-left:.0001pt;text-align:justify;">ReLU 函数：对于正数输入，返回输入值本身；对于负数输入，返回0。</p> 
</blockquote> 
<p class="img-center"><img alt="" height="80" src="https://images2.imgbox.com/f3/bb/4igN3UAS_o.png" width="284"></p> 
<h3 id="%F0%9F%8D%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95">🍈反向传播算法</h3> 
<p class="img-center"><img alt="" height="778" src="https://images2.imgbox.com/3c/ca/zqe7Upi6_o.jpg" width="1200"></p> 
<p style="margin-left:.0001pt;text-align:justify;">        反向传播算法是训练神经网络的核心技术，通过链式法则计算损失函数相对于每个参数的梯度，并将梯度传播回网络以更新权重和偏置。</p> 
<h2 id="%F0%9F%8D%89%E5%AE%9E%E4%BE%8B"><strong><strong><strong>🍉</strong></strong></strong>实例</h2> 
<p class="img-center"><img alt="" height="1135" src="https://images2.imgbox.com/fe/92/eLHTjfYG_o.jpg" width="1200"></p> 
<p style="text-align:center;"><strong>三层神经网络结构</strong></p> 
<h3 id="%F0%9F%8D%88%E6%89%8B%E5%B7%A5%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">🍈手工搭建神经网络</h3> 
<p style="margin-left:.0001pt;text-align:justify;">        以下是一个简单的 Python 示例代码，演示如何手工实现一个包含单隐藏层的多层感知器，并训练它解决 XOR 问题。</p> 
<pre><code class="language-python">import numpy as np

class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.weights1 = np.random.randn(input_size, hidden_size)
        self.bias1 = np.zeros((1, hidden_size))
        self.weights2 = np.random.randn(hidden_size, output_size)
        self.bias2 = np.zeros((1, output_size))
    
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))
    
    def sigmoid_derivative(self, x):
        return x * (1 - x)
    
    def forward_pass(self, X):
        self.hidden_layer_input = np.dot(X, self.weights1) + self.bias1
        self.hidden_layer_output = self.sigmoid(self.hidden_layer_input)
        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights2) + self.bias2
        self.output = self.sigmoid(self.output_layer_input)
        return self.output
    
    def backward_pass(self, X, y, output):
        self.output_error = y - output
        self.output_delta = self.output_error * self.sigmoid_derivative(output)
        
        self.hidden_layer_error = np.dot(self.output_delta, self.weights2.T)
        self.hidden_layer_delta = self.hidden_layer_error * self.sigmoid_derivative(self.hidden_layer_output)
        
        self.weights2 += np.dot(self.hidden_layer_output.T, self.output_delta)
        self.bias2 += np.sum(self.output_delta, axis=0, keepdims=True)
        self.weights1 += np.dot(X.T, self.hidden_layer_delta)
        self.bias1 += np.sum(self.hidden_layer_delta, axis=0, keepdims=True)
    
    def train(self, X, y, epochs):
        for epoch in range(epochs):
            output = self.forward_pass(X)
            self.backward_pass(X, y, output)
            if epoch % 1000 == 0:
                print(f'Epoch {epoch}, Loss: {np.mean(np.square(y - output))}')

if __name__ == "__main__":
    input_size = 2
    hidden_size = 3
    output_size = 1
    
    nn = NeuralNetwork(input_size, hidden_size, output_size)
    
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([[0], [1], [1], [0]])
    
    nn.train(X, y, epochs=10000)
    
    print('Final predictions:')
    print(nn.forward_pass(X))
</code></pre> 
<p class="img-center"><img alt="" height="389" src="https://images2.imgbox.com/3f/08/mwrbchKa_o.png" width="451"></p> 
<h2 id="%F0%9F%8D%89%E6%80%BB%E7%BB%93"><strong><strong><strong>🍉</strong></strong></strong>总结</h2> 
<p style="margin-left:.0001pt;text-align:justify;">        本文详细讲解了人工神经网络的核心组成部分和关键技术。从神经元、感知器到多层感知器的演进，再到损失函数、梯度下降算法和反向传播算法的实际应用，读者可以全面理解神经网络的工作原理及其在实际问题中的应用。通过手工搭建神经网络的示例，读者不仅能够加深对神经网络内部运作的理解，还能够通过修改和扩展代码来探索更复杂的神经网络结构和任务。神经网络作为深度学习的基础，对于理解和实践现代机器学习技术具有重要意义。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="53" src="https://images2.imgbox.com/0c/f4/owbUZab9_o.gif" width="1000"></p> 
<p style="margin-left:.0001pt;text-align:center;"><strong><span style="color:#fe2c24;">希望能给大家提供一些帮助！！！</span></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="300" src="https://images2.imgbox.com/bb/ad/zXrtitjt_o.png" width="1080"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/537b98db32cf8f072fabe851f02e3612/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">信息学奥赛初赛天天练-31-CSP-J2022基础题-指针、数组、链表、进制转换、深度优先搜索、广度优先搜索、双栈实现队列应用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/dd3a8d20e67950498d630e18dfe47bfd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">数据结构【二叉树】</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>