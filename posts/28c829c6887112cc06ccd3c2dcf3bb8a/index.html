<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python学习之PySpark案例实战 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/28c829c6887112cc06ccd3c2dcf3bb8a/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Python学习之PySpark案例实战">
  <meta property="og:description" content="&#34;&#34;&#34; 演示获取PySpark的执行环境入库对象: SparkContext 并通过SparkContext对象获取当前PySpark的版本 &#34;&#34;&#34; # 导包 from pyspark import SparkConf,SparkContext # 创建SparkConf类对象中 conf = SparkConf().setSparkHome(&#34;local[*]&#34;).setAppName(&#34;test_spqrk_app&#34;) #基FSparkConf类对象创LSparkContext对象 sc = SparkContext(conf = conf) #打印PySpark的运行版本 print(sc.version) #停止SparkContext对象的运行(停止PySpark程序) sc.stop() PySpark的编程模型 SparkContext类对象，是PySpark编程中一切功能的入口。PySpark的编程，主要分为如下三大步骤
通过SparkContext对象，完成数据输入
输入数据后得到RDD对象，对RDD对象进行选代计算
最终通过RDD对象的成员方法，完成数据输出工作
数据输入 只要数据输入到spark就一定是rdd
RDD对象 如图可见,PySpark支持多种数据的输入,在输入完成后,都会得到一个:RDD类的对象
RDD全称为:弹性分布式数据集(Resilient Distributed Datasets)PySpark针对数据的处理，都是以RDD对象作为载体，即:
数据存储在RDD内
各类数据的计算方法，也都是RDD的成员方法
RDD的数据计算方法，返回值依旧是RDD对象
Python数据容器转RDD对象 PySpark支持通过SparkContext对象的parallelize成员方法将
List
Tuple
Set
Dic
tstr
转换为PySpark的RDD对象
注意:
字符串会被拆分出1个个的字符，存入RDD对象
字典仅有key会被存入RDD对象
&#34;&#34;&#34; 演示通过PySpark代码加载数据，即数据输入 &#34;&#34;&#34; from pyspark import SparkContext,SparkConf #构建接口对象 conf = SparkConf().setSparkHome(&#34;local[*]&#34;).setAppName(&#34;test_rdd&#34;) sc = SparkContext(conf=conf) #构建接口对象 #构建实验数据 list_1 = [1,2,3,4,5] turple_1 = (1,2,3,4) str_1 = (&#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-21T08:55:47+08:00">
    <meta property="article:modified_time" content="2024-04-21T08:55:47+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python学习之PySpark案例实战</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <pre><code>"""
演示获取PySpark的执行环境入库对象: SparkContext
并通过SparkContext对象获取当前PySpark的版本
"""

# 导包

from pyspark import SparkConf,SparkContext
# 创建SparkConf类对象中
conf = SparkConf().setSparkHome("local[*]").setAppName("test_spqrk_app")
#基FSparkConf类对象创LSparkContext对象
sc = SparkContext(conf = conf)
#打印PySpark的运行版本
print(sc.version)
#停止SparkContext对象的运行(停止PySpark程序)
sc.stop()
</code></pre> 
<h5><a id="PySpark_19"></a><strong><strong><strong>PySpark的编程模型</strong></strong></strong></h5> 
<p>SparkContext类对象，是PySpark编程中一切功能的入口。PySpark的编程，主要分为如下三大步骤</p> 
<p><img src="https://images2.imgbox.com/58/d8/6EMoTtsu_o.png" alt=""></p> 
<p><img src="https://images2.imgbox.com/9e/02/OOMe3vW9_o.png" alt=""></p> 
<p><img src="https://images2.imgbox.com/0d/27/bhaebim6_o.png" alt=""></p> 
<p>通过SparkContext对象，完成数据输入</p> 
<p>输入数据后得到RDD对象，对RDD对象进行选代计算</p> 
<p>最终通过RDD对象的成员方法，完成数据输出工作</p> 
<h4><a id="_43"></a><strong>数据输入</strong></h4> 
<p><strong><strong>只要数据输入到spark就一定是rdd</strong></strong></p> 
<h5><a id="RDD_49"></a><strong><strong><strong>RDD对象</strong></strong></strong></h5> 
<p>如图可见,PySpark支持多种数据的输入,在输入完成后,都会得到一个:<strong><strong>RDD类的对象</strong></strong></p> 
<p>RDD全称为:弹性分布式数据集(Resilient Distributed Datasets)PySpark针对数据的处理，都是以RDD对象作为载体，即:</p> 
<p>数据存储在RDD内</p> 
<p>各类数据的计算方法，也都是RDD的成员方法</p> 
<p>RDD的数据计算方法，返回值依旧是RDD对象</p> 
<p><img src="https://images2.imgbox.com/2a/85/PwM5AYol_o.png" alt=""></p> 
<h5><a id="PythonRDD_71"></a><strong><strong><strong>Python数据容器转RDD对象</strong></strong></strong></h5> 
<p>PySpark支持<strong><strong>通过SparkContext对象</strong></strong>的<strong><strong>parallelize成员方法</strong></strong>将</p> 
<p><strong><strong>List</strong></strong></p> 
<p><strong><strong>Tuple</strong></strong></p> 
<p><strong><strong>Set</strong></strong></p> 
<p><strong><strong>Dic</strong></strong></p> 
<p><strong><strong>tstr</strong></strong></p> 
<p><strong><strong>转换为PySpark的RDD对象</strong></strong></p> 
<p>注意:</p> 
<p><strong><strong>字符串</strong></strong>会被<strong><strong>拆分出1个个的字符</strong></strong>，存入RDD对象</p> 
<p>字典<strong><strong>仅有key会被存入RDD对象</strong></strong></p> 
<p><img src="https://images2.imgbox.com/09/bb/T7QdXAz0_o.png" alt=""></p> 
<p><img src="https://images2.imgbox.com/b7/b4/ZfLhuHwN_o.png" alt=""></p> 
<pre><code>"""
演示通过PySpark代码加载数据，即数据输入
"""
from pyspark import SparkContext,SparkConf

#构建接口对象
conf = SparkConf().setSparkHome("local[*]").setAppName("test_rdd")
sc = SparkContext(conf=conf) #构建接口对象
#构建实验数据
list_1 = [1,2,3,4,5]
turple_1 = (1,2,3,4)
str_1 = ("asdoac")
dict_1 = {"key1":"value1","key2":"value2"}
set_1 = {1,2,3,4,5}
#通过parallelize方法将Python对象加载到Spark内，成为RDD对象
rdd1 = sc.parallelize(list_1)  #列表转spark的rdd
rdd2 = sc.parallelize(turple_1)  #元组转spark的rdd
rdd3 = sc.parallelize(str_1)  #字符串转spark的rdd
rdd4 = sc.parallelize(dict_1)  #字典转spark的rdd
rdd5 = sc.parallelize(set_1)  #集合转spark的rdd

print(rdd1.collect())
print(rdd2.collect())
print(rdd3.collect())
print(rdd4.collect())
print(rdd5.collect())

sc.stop()
</code></pre> 
<h5><a id="RDD_143"></a><strong><strong><strong>读取文件转RDD对象</strong></strong></strong></h5> 
<p>PySpark也支持通过SparkContext入口对象来读取文件来构建出RDD对象</p> 
<p><img src="https://images2.imgbox.com/41/96/RS8x6Wtb_o.png" alt=""></p> 
<p><img src="https://images2.imgbox.com/c0/76/TIs9XsVQ_o.png" alt=""></p> 
<pre><code>"""
演示通过PySpark代码加载数据，即数据输入
"""
from pyspark import SparkContext,SparkConf

#构建接口对象
conf = SparkConf().setSparkHome("local[*]").setAppName("test_rdd")
sc = SparkContext(conf=conf) #构建接口对象

#用过textFile方法，读取文件数据加载到Spark内，成为RDD对象
rdd = sc.textFile("D:/bill.txt")
print(rdd.collect())

sc.stop()
</code></pre> 
<h5><a id="_175"></a><strong><strong><strong>总结：</strong></strong></strong></h5> 
<h5><a id="RDD_178"></a><strong><strong><strong>RDD对象是什么?为什么要使用它?</strong></strong></strong></h5> 
<p><strong><strong>通过spark的contxt加载数据为rdd</strong></strong></p> 
<p>RDD对象称之为分布式弹性数据集,是PySpark中数据计算的载体它可以:</p> 
<p><strong><strong>提供数据存储</strong></strong></p> 
<p><strong><strong>提供数据计算的各类方法</strong></strong></p> 
<p>****数据计算的方法,返回值依旧是RDD(RDD迭代计算)后续对数据进行各类计算，****<strong><strong>都是基于RDD对象进行</strong></strong></p> 
<h5><a id="SparkRDD_196"></a><strong><strong><strong>如何输入数据到Spark(即得到RDD对象)</strong></strong></strong></h5> 
<p>通过SparkContext的parallelize成员方法,将<strong><strong>Python数据容器转换为RDD对象</strong></strong></p> 
<p>通过SparkContext的textFile成员方法读取文本文件得到RDD对象</p> 
<h3><a id="_206"></a><strong>数据计算</strong></h3> 
<h4><a id="map_209"></a><strong>map方法</strong></h4> 
<p>PySpark的数据计算，都是基于RDD对象来进行的那么如何进行呢?</p> 
<p>自然是依赖RDD对象内置丰富的:<strong><strong>成员方法(算子)</strong></strong></p> 
<h5><a id="map_219"></a><strong><strong><strong>map算子</strong></strong></strong></h5> 
<p>功能:map算子,是将RDD的数据<strong><strong>一条条处理</strong></strong>( 处理的逻基于ap算子中接收的处理函数 )<strong><strong>返回新的RDD</strong></strong></p> 
<p>语法:</p> 
<p><img src="https://images2.imgbox.com/33/46/bg2OXH5b_o.png" alt=""></p> 
<p><strong><strong>（T）-&gt; U :表示你传入一个参数T，有传出的东西，类型不限；</strong></strong></p> 
<p><strong><strong>（T）-&gt; T :表示的是传入参数T之后，传出的也是和T一样的数据类型</strong></strong></p> 
<p><img src="https://images2.imgbox.com/aa/f7/urAWU8dz_o.png" alt=""></p> 
<p>需要添加python.exe的位置</p> 
<p><img src="https://images2.imgbox.com/6a/78/y9XjuY8j_o.png" alt=""></p> 
<p><img src="https://images2.imgbox.com/c4/d4/tHC9h8F6_o.png" alt=""></p> 
<pre><code>"""
演示map算子
"""
from pyspark import SparkContext,SparkConf
import os
os.environ['PYSPARK_PYTHON']="D:\dev\python\python3.10.4\python.exe"
#构建接口对象
conf = SparkConf().setSparkHome("local[*]").setAppName("test_rdd")
sc = SparkContext(conf=conf) #构建接口对象

#准备一个rdd
rdd = sc.parallelize([1,2,3,4,5,6])

#使用map方法使得每个都乘10
def func(data):
    return data * 10

rdd2 = rdd.map(func)   #调用这个func的函数去对参数进行操作

print(rdd2.collect())
</code></pre> 
<p>因为def的函数有些简单就一行，所以可以使用lamba匿名函数来优化</p> 
<p><img src="https://images2.imgbox.com/4d/9b/7fvuZQdV_o.png" alt=""></p> 
<pre><code>"""
演示map算子
"""
from pyspark import SparkContext,SparkConf
import os
os.environ['PYSPARK_PYTHON']="D:\dev\python\python3.10.4\python.exe"
#构建接口对象
conf = SparkConf().setSparkHome("local[*]").setAppName("test_rdd")
sc = SparkContext(conf=conf) #构建接口对象

#准备一个rdd
rdd = sc.parallelize([1,2,3,4,5,6])

#使用map方法使得每个都乘10
# def func(data):
#     return data * 10

rdd2 = rdd.map(lambda x:x*10)   #调用这个func的函数去对参数进行操作

print(rdd2.collect())
</code></pre> 
<p><strong><strong>Map调用之后，乘以10了，返回值依旧是rdd，那么如果还想对数据进行操作的话，那么就可以在后面继续加map+匿名函数（链式调用），但是匿名函数只限于函数语句少的，多的话还是def外部定义</strong></strong></p> 
<p><img src="https://images2.imgbox.com/7f/4d/nxiavp2K_o.png" alt=""></p> 
<pre><code>"""
演示map算子
"""
from pyspark import SparkContext,SparkConf
import os
os.environ['PYSPARK_PYTHON']="D:\dev\python\python3.10.4\python.exe"
#构建接口对象
conf = SparkConf().setSparkHome("local[*]").setAppName("test_rdd")
sc = SparkContext(conf=conf) #构建接口对象

#准备一个rdd
rdd = sc.parallelize([1,2,3,4,5,6])

#使用map方法使得每个都乘10
# def func(data):
#     return data * 10

rdd2 = rdd.map(lambda x:x*10).map(lambda x:x+5)   #调用这个func的函数去对参数进行操作

print(rdd2.collect())
</code></pre> 
<h5><a id="_339"></a><strong><strong><strong>总结：</strong></strong></strong></h5> 
<h5><a id="map_342"></a><strong><strong><strong>map算子(成员方法)</strong></strong></strong></h5> 
<p>接受一个处理函数可用lambda表达式快速编写</p> 
<p>对RDD内的元素逐个处理，并<strong><strong>返回一个新的RDD</strong></strong></p> 
<h5><a id="_351"></a><strong><strong><strong>链式调用</strong></strong></strong></h5> 
<p>对于返回值是新RDD的算子，可以通过链式调用的方式多次调用算子</p> 
<h4><a id="flatMap_357"></a><strong>flatMap方法</strong></h4> 
<h5><a id="flatMap_360"></a><strong><strong><strong>flatMap算子</strong></strong></strong></h5> 
<p>功能:对rdd执行map操作,然后进行 <strong><strong>解除嵌套</strong></strong>操作</p> 
<p><img src="https://images2.imgbox.com/63/1e/2vGFhxbi_o.png" alt=""></p> 
<p><img src="https://images2.imgbox.com/c3/19/S3FNmTmK_o.png" alt=""></p> 
<pre><code>"""
演示flatmap算子
"""
from pyspark import SparkContext,SparkConf
import os
os.environ['PYSPARK_PYTHON']="D:\dev\python\python3.10.4\python.exe"
#构建接口对象
conf = SparkConf().setSparkHome("local[*]").setAppName("test_rdd")
sc = SparkContext(conf=conf) #构建接口对象

#准备一个rdd
rdd = sc.parallelize(["master,servant,fate","saber,archer,lacher","basker,rider"])

#需求，将RDD数据里面的一个个单词拿出来
rdd2 = rdd.map(lambda x:x.split(" "))
print(rdd2.collect())
</code></pre> 
<p><img src="https://images2.imgbox.com/6f/f3/jiMXohhW_o.png" alt=""></p> 
<pre><code>"""
演示flatmap算子
"""
from pyspark import SparkContext,SparkConf
import os
os.environ['PYSPARK_PYTHON']="D:\dev\python\python3.10.4\python.exe"
#构建接口对象
conf = SparkConf().setSparkHome("local[*]").setAppName("test_rdd")
sc = SparkContext(conf=conf) #构建接口对象

#准备一个rdd
rdd = sc.parallelize(["master servant fate","saber archer lacher","basker rider"])

#需求，将RDD数据里面的一个个单词拿出来
rdd2 = rdd.flatMap(lambda x:x.split(" "))
print(rdd2.collect())
</code></pre> 
<h5><a id="_417"></a><strong><strong><strong>总结：</strong></strong></strong></h5> 
<h5><a id="flatMap_420"></a><strong><strong><strong>flatMap算子</strong></strong></strong></h5> 
<p>计算逻辑和map一样</p> 
<p>可以比map多出，解除一层嵌套的功能</p> 
<h4><a id="reduceByKey_430"></a><strong>reduceByKey方法</strong></h4> 
<p>功能: 针对<strong><strong>KV型</strong></strong> RDD,自动按照key分组,然后根据你提供的聚合逻辑,完成****组内数据[value)****的聚合操作.</p> 
<p><img src="https://images2.imgbox.com/c1/eb/jTM0cSR0_o.png" alt=""></p> 
<p><strong><strong>两个传入参数，和返回的类型需要是一致的。</strong></strong></p> 
<p><img src="https://images2.imgbox.com/c0/f1/DkExgoWZ_o.png" alt=""></p> 
<h5><a id="_446"></a><strong><strong><strong>聚合逻辑</strong></strong></strong></h5> 
<p><img src="https://images2.imgbox.com/9c/26/36sDMZhr_o.png" alt=""></p> 
<p>注意: reduceByKey中接收的函数 只负责聚合,不理会分组，分组是自动 by key来分组的。</p> 
<p><img src="https://images2.imgbox.com/fc/d7/7GGO9eZi_o.png" alt=""></p> 
<p><strong><strong>这里的a+b是指代的传入的两个key的value的实现，比如a的value有两个，就是这两个value相加</strong></strong></p> 
<pre><code>"""
演示reduceBYKey算子
"""
from pyspark import SparkContext,SparkConf
import os
os.environ['PYSPARK_PYTHON']="D:\dev\python\python3.10.4\python.exe"
#构建接口对象
conf = SparkConf().setSparkHome("local[*]").setAppName("test_rdd")
sc = SparkContext(conf=conf) #构建接口对象

#准备一个rdd
rdd = sc.parallelize([('a',1),('a',2),('b',3),('b',4)])
rdd2 = rdd.reduceByKey(lambda a,b : a+b)
print(rdd2.collect())
sc.stop()
</code></pre> 
<h5><a id="_482"></a><strong><strong><strong>总结：</strong></strong></strong></h5> 
<p><img src="https://images2.imgbox.com/f5/62/LWcRkkKn_o.png" alt=""></p> 
<h4><a id="1_491"></a><strong>练习案例1</strong></h4> 
<p>读取文件，对文件内的单词进行计数</p> 
<p><img src="https://images2.imgbox.com/54/73/JvteXGdr_o.png" alt=""></p> 
<pre><code>#1.构建执行环境入口对象
from pyspark import SparkContext,SparkConf
import os
os.environ['PYSPARK_PYTHON']="D:\dev\python\python3.10.4\python.exe"
#构建接口对象
conf = SparkConf().setSparkHome("local[*]").setAppName("test_rdd")
sc = SparkContext(conf=conf) #构建接口对象
#2.读取数据文件
rdd = sc.textFile("D:/hello.txt")
#3.取出全部单词
world_rdd = rdd.flatMap(lambda a : a.split(" ")).map(lambda x:x.strip())
# print(world_rdd.collect())
#4.将所有单词都转换成二元元组，单词为Key，value设置为1,有几个就有几个1，相加
word_with_one = world_rdd.map(lambda word:(word,1))
# print(word_with_one.collect())
#5.分组并求和
result_rdd = word_with_one.reduceByKey(lambda a,b :a+b )
#6.打印输出结果
print(result_rdd.collect())
</code></pre> 
<h4><a id="filter_525"></a><strong>filter方法</strong></h4> 
<p>功能:过滤想要的数据进行保留</p> 
<p><img src="https://images2.imgbox.com/20/cb/7lxKA801_o.png" alt=""></p> 
<p><img src="https://images2.imgbox.com/7a/93/C87eWMH7_o.png" alt=""></p> 
<pre><code>"""
演示RDD的filter成员方法的使用
"""
#1.构建执行环境入口对象
from pyspark import SparkContext,SparkConf
import os
os.environ['PYSPARK_PYTHON']="D:\dev\python\python3.10.4\python.exe"
#构建接口对象
conf = SparkConf().setSparkHome("local[*]").setAppName("test_rdd")
sc = SparkContext(conf=conf) #构建接口对象

#准备一个rdd
rdd = sc.parallelize([1,2,3,4,5,6,7,8])
#对rdd的数据进行过滤
rdd2 = rdd.filter(lambda num: num % 2 == 0)
print(rdd2.collect())
</code></pre> 
<p>接受一个处理函数，可用lambda快速编写</p> 
<p><strong><strong>函数对RDD数据逐个处理，得到True的保留至返回值的RDD中</strong></strong></p> 
<h4><a id="distinct_564"></a><strong>distinct方法</strong></h4> 
<p>功能:对RDD数据进行<strong><strong>去重</strong></strong>,返回新RDD</p> 
<p><img src="https://images2.imgbox.com/06/a7/ERyaAI4P_o.png" alt=""></p> 
<p><img src="https://images2.imgbox.com/6e/40/7gQ4JNDO_o.png" alt=""></p> 
<pre><code>"""
演示RDD的distint成员方法的使用
"""
#1.构建执行环境入口对象
from pyspark import SparkContext,SparkConf
import os
os.environ['PYSPARK_PYTHON']="D:\dev\python\python3.10.4\python.exe"
#构建接口对象
conf = SparkConf().setSparkHome("local[*]").setAppName("test_rdd")
sc = SparkContext(conf=conf) #构建接口对象

#准备一个rdd
rdd = sc.parallelize([1,1,2,3,4,4,4,4,3,4,6,6,6,7])
#对rdd的数据进行去重操作
rdd2 = rdd.distinct()
print(rdd2.collect())
</code></pre> 
<h4><a id="sortBy_599"></a><strong>sortBy方法</strong></h4> 
<p>功能:对RDD数据进行排序基于你指定的排序依据</p> 
<p>语法:</p> 
<p><img src="https://images2.imgbox.com/c7/39/tw78nxZv_o.png" alt=""></p> 
<p>func: (T) - U: <strong><strong>告知按照rdd中的哪个数据进行排序</strong></strong>，比如 Lambda x: x[1]表示rdd中的第二列进行排序</p> 
<p>ascending =<strong><strong>True升序 False 降序</strong></strong></p> 
<p>numPartitions: 用多少分区排序</p> 
<p><img src="https://images2.imgbox.com/e7/80/7IEIPRq7_o.png" alt=""></p> 
<pre><code>#1.构建执行环境入口对象
from pyspark import SparkContext,SparkConf
import os
os.environ['PYSPARK_PYTHON']="D:\dev\python\python3.10.4\python.exe"
#构建接口对象
conf = SparkConf().setSparkHome("local[*]").setAppName("test_rdd")
sc = SparkContext(conf=conf) #构建接口对象
#2.读取数据文件
rdd = sc.textFile("D:/hello.txt")
#3.取出全部单词
world_rdd = rdd.flatMap(lambda a : a.split(" ")).map(lambda x:x.strip())
# print(world_rdd.collect())
#4.将所有单词都转换成二元元组，单词为Key，value设置为1,有几个就有几个1，相加
word_with_one = world_rdd.map(lambda word:(word,1))
# print(word_with_one.collect())
#5.分组并求和
result_rdd = word_with_one.reduceByKey(lambda a,b :a+b )
#6.对结果进行排序
# print(result_rdd.collect())
final_rdd = result_rdd.sortBy(lambda x:x[1],ascending=False,numPartitions=1)
print(final_rdd.collect())
</code></pre> 
<p><strong><strong>接收一个处理函数，可用lambda快速编写</strong></strong></p> 
<p><strong><strong>函数表示用来决定排序的依据</strong></strong></p> 
<p><strong><strong>可以控制升序或降序</strong></strong></p> 
<p><strong><strong>全局排序需要设置分区数为1</strong></strong></p> 
<h4><a id="2_663"></a><strong>练习案例2</strong></h4> 
<h4><a id="_666"></a></h4> 
<p>多个Json数据串联在一起的；</p> 
<p><img src="https://images2.imgbox.com/1c/ec/pyS9gLkM_o.png" alt=""></p> 
<pre><code>"""
案例需求:
各个城市销售额排名，从大到小全部城市,
有哪些商品类别在售卖
北京市有哪些商品类别在售卖
"""

from pyspark import SparkContext,SparkConf
import os
import json
os.environ['PYSPARK_PYTHON']="D:\dev\python\python3.10.4\python.exe"
#构建接口对象
conf = SparkConf().setSparkHome("local[*]").setAppName("test_rdd")
sc = SparkContext(conf=conf) #构建接口对象

# TODO 而状1: 城市箭售额接名1.1/歌文件得到RDD
# 1.2 取出一个个JSON字符串
file_rdd = sc.textFile("D:/orders.txt")
json_str_rdd = file_rdd.flatMap(lambda x :x.split("|"))
"""
map可以将数据一条一条的取出来，因为文本内一行内有多条json数据以为|隔开
"""
# 1.3 一个个JSON字符审转换为字典
dict_rdd = json_str_rdd.map(lambda x :json.loads(x)) #将每条json数据转换为字典
# 1.4取出城市和销售额数据，通过lamad函数去让它成为二元元组
city_with_money_rdd = dict_rdd.map(lambda x:(x['areaName'],int(x['money']))) #取出城市和销售额数据
# 1.5 技城市分组按销售额聚合
city_result = city_with_money_rdd.reduceByKey(lambda a,b:a+b)
# 1.6 按销售额聚合结果进行排序
result_rdd = city_result.sortBy(lambda x:x[1],ascending=False,numPartitions=1)
print(f"最后的结果是{result_rdd.collect()}")
# TODO 需求2: 全部城市有哪些商品类别在售卖
# 2.1 取出全部的商品类别,对全部商品类别进行去重
category_rdd = dict_rdd.map(lambda x :x["category"]).distinct()
print(f"全部售卖的结果去重后是{category_rdd.collect()}")
# TODO 状3: 北京市有哪些商品类别在售灵
#3.1 过滤北京市的数据
beijing_rdd = dict_rdd.filter(lambda x :x['areaName'] == '北京')
# 3.2 取出全部商品类别
result3_rdd = beijing_rdd.map(lambda x :x['category']).distinct()
print(f"北京的类别有{result3_rdd.collect()}")
</code></pre> 
<h4><a id="rddpython_722"></a><strong>数据输出（rdd转换为python数据）</strong></h4> 
<p><img src="https://images2.imgbox.com/10/00/PyBjvd7r_o.png" alt=""></p> 
<h5><a id="collect_729"></a><strong><strong><strong>collect算子</strong></strong></strong></h5> 
<p>功能:将RDD各个分区内的数据,统一收集到Driver中,形成一个List对象</p> 
<p>用法:</p> 
<p><strong><strong>rdd.collect()</strong></strong></p> 
<p>返回值是一个list</p> 
<p><strong><strong>之前也使用过去print(rdd.collect())这是将rdd对象转变为一个python的list进行打印。</strong></strong></p> 
<p><img src="https://images2.imgbox.com/c0/98/85OxaP49_o.png" alt=""></p> 
<h5><a id="reduce_750"></a><strong><strong><strong>reduce算子</strong></strong></strong></h5> 
<p><img src="https://images2.imgbox.com/03/09/Jv3RX6nz_o.png" alt=""></p> 
<p><img src="https://images2.imgbox.com/67/73/5J7vN0AO_o.png" alt=""></p> 
<h5><a id="take_759"></a><strong><strong><strong>take算子</strong></strong></strong></h5> 
<p><strong>自我介绍一下，小编13年上海交大毕业，曾经在小公司待过，也去过华为、OPPO等大厂，18年进入阿里一直到现在。</strong></p> 
<p><strong>深知大多数Python工程师，想要提升技能，往往是自己摸索成长或者是报班学习，但对于培训机构动则几千的学费，着实压力不小。自己不成体系的自学效果低效又漫长，而且极易碰到天花板技术停滞不前！</strong></p> 
<p><strong>因此收集整理了一份《2024年Python开发全套学习资料》，初衷也很简单，就是希望能够帮助到想自学提升又不知道该从何学起的朋友，同时减轻大家的负担。</strong></p> 
<p><img src="https://images2.imgbox.com/44/3e/g5PhCXjX_o.png" alt="img"></p> 
<p><img src="https://images2.imgbox.com/b1/8a/p6b7B2VK_o.png" alt="img"></p> 
<p><img src="https://images2.imgbox.com/db/4f/PD5gDzG7_o.png" alt="img"></p> 
<p><img src="https://images2.imgbox.com/ed/2b/eZKGFD6W_o.png" alt="img"></p> 
<p><img src="https://images2.imgbox.com/3f/b7/ct0XimWw_o.png" alt="img"></p> 
<p><img src="https://images2.imgbox.com/ac/64/zDzjzxk9_o.png" alt="img"></p> 
<p><strong>既有适合小白学习的零基础资料，也有适合3年以上经验的小伙伴深入学习提升的进阶课程，基本涵盖了95%以上前端开发知识点，真正体系化！</strong></p> 
<p><strong>由于文件比较大，这里只是将部分目录大纲截图出来，每个节点里面都包含大厂面经、学习笔记、源码讲义、实战项目、讲解视频，并且后续会持续更新</strong></p> 
<p><strong>如果你觉得这些内容对你有帮助，可以扫码获取！！！（备注：Python）</strong></p> 
<p>绍一下，小编13年上海交大毕业，曾经在小公司待过，也去过华为、OPPO等大厂，18年进入阿里一直到现在。**</p> 
<p><strong>深知大多数Python工程师，想要提升技能，往往是自己摸索成长或者是报班学习，但对于培训机构动则几千的学费，着实压力不小。自己不成体系的自学效果低效又漫长，而且极易碰到天花板技术停滞不前！</strong></p> 
<p><strong>因此收集整理了一份《2024年Python开发全套学习资料》，初衷也很简单，就是希望能够帮助到想自学提升又不知道该从何学起的朋友，同时减轻大家的负担。</strong></p> 
<p>[外链图片转存中…(img-JqTdrHpT-1713660936323)]</p> 
<p>[外链图片转存中…(img-RuBR7tTj-1713660936324)]</p> 
<p>[外链图片转存中…(img-RaqP2WAl-1713660936324)]</p> 
<p>[外链图片转存中…(img-N25BAMNC-1713660936325)]</p> 
<p><img src="https://images2.imgbox.com/b3/8b/vyt2J0OY_o.png" alt="img"></p> 
<p><img src="https://images2.imgbox.com/80/1f/OCFNZUEe_o.png" alt="img"></p> 
<p><strong>既有适合小白学习的零基础资料，也有适合3年以上经验的小伙伴深入学习提升的进阶课程，基本涵盖了95%以上前端开发知识点，真正体系化！</strong></p> 
<p><strong>由于文件比较大，这里只是将部分目录大纲截图出来，每个节点里面都包含大厂面经、学习笔记、源码讲义、实战项目、讲解视频，并且后续会持续更新</strong></p> 
<p><strong>如果你觉得这些内容对你有帮助，可以扫码获取！！！（备注：Python）</strong></p> 
<p><img src="https://images2.imgbox.com/13/86/NODBQPOa_o.jpg" alt=""></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/49276f5337e5686d98a876b1f41ab875/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">基于Python的A*算法解决八数码问题</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/924936930254a36a45db9cd3ac9c074e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">手把手教你Windows下搭建Filebeat&#43;Logstash&#43;ElasticSearch&#43;Kibana系统</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>