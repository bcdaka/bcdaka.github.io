<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Stable Diffusion为什么生成的图片总是糊的？ - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/ed08751721daff385ad301d98af42565/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Stable Diffusion为什么生成的图片总是糊的？">
  <meta property="og:description" content="我们先看一下出图时模糊的图片效果。
我相信很多初学者在开始绘图的时候经常会碰到这种情况。当然我自己也曾经碰到过，我总结了一下，一般有以下几种情况。
第一种情况：大模型使用的是SDXL大模型，VAE模型选择了vae-ft-mse-840000-ema-pruned.safetensors。（必现场景）
这种情况是必现的。也是当时网友私聊时使用SD出图模糊的主要原因。
为什么这种情况会出现呢？
我们首先要了解一下什么是VAE模型，可以把VAE模型理解为颜色滤镜，它主要用于调节和美化生成图片的色彩。
对于VAE模型，可以参照我之前的文章【Stable Diffusion【模型篇】：VAE模型】,写的时间有些早，但是可以参照看一下。
VAE模型为什么会造成这个问题呢，主要是VAE模型也区分SD1.5和SDXL的模型的。我们的大模型使用的是基于SDXL的大模型，但是VAE模型选择的ae-ft-mse-840000-ema-pruned.safetensors是基于SD1.5的大模型，二者并不匹配。
下面是C站上面关于SD1.5的模型。
从上面的图片我们可以看到很多常见的VAE模型都是基于SD1.5的。这里就包括我们上面提到的vae-ft-mse-840000-ema-pruned。
关于SD1.5的VAE模型这里罗列了一些常见的，感兴趣的朋友可以了解一下。
vae-ft-mse-840000-ema-pruned
vae-ft-ema-560000-ema
kl-f8-anime系列（主要用于动漫大模型）
ClearVAE（主要用于动漫大模型）
Color101（生成更鲜艳色彩，更接近HDR效果和更自然图像的VAE）
下面是C站上面关于SDXL的模型。
基于SDXL的VAE模型相当比较少，最常用的是sdxl vae模型。为什么基于SDXL的VAE模型比较少呢，因为VAE模型本身就比较小，很多SDXL的大模型都已经融入了VAE模型，所以基于SDXL的大模型一般不选择VAE模型出图效果也不错的。例如在LiblibAI网站，当我们选择SDXL大模型的时候，VAE模型选项是禁止选择的。
一般来说，对于基于SDXL的大模型，我们VAE模型可以不用选择，要选择的话最好选择sdxl_vae。
我们来对比看一下效果。
正向提示词：1 super beautiful chinese girl, solo
反向提示词：nsfw,easynegative,ng_deepnegative_v1_75t,(worst quality:2),(low quality:2),(normal quality:2),lowres,bad anatomy,bad hands,normal quality,((monochrome)),((grayscale)),((watermark)),
下面是相关参数配置：
大模型：SDXL 1.0的基础大模型
采样器：Euler a
采样迭代步数：30
图片宽高：1024*1024。
提示词引导系数（CFG）：7
不使用VAE模型生成的图片
使用sdxl_vae模型生成的图片
基于SDXL的大模型使用VAE模型和不使用VAE模型差别大吗，大家可以自己甄别哈。
总结：使用SDXL的大模型&#43;SD 1.5的VAE模型生成的图片大概率都是糊的。
第二种情况：大模型使用的是基于SDXL大模型，选择了Refiner(精炼器)
这种情况不是必现的。
今天尝试了很多次，都没有模拟出来，不过之前在使用SDXL基础模型出图的时候碰到的概率还是蛮大的，后面我选择SDXL基础模型的时候关于Refiner选项我一般是不选中的。
关于SDXL的Refiner(精炼器)主要属于细化模型。对图像进一步优化细节。目前有些大模型并不兼容Refiner(精炼器)。
第三种情况：由于各种原因图片未生成完毕SD系统工具出问题了，例如报OOM的显存异常等
另外：如果我们采样迭代步数设置的比较小，例如上面的示例中将采样迭代步数设置为5，出来的图片也是模糊的，但是和上面图片的模糊不是一个类型哈。
还有其他的情况吗，欢迎各位小伙伴们积极补充哦。
这里就简单说几种应用：
1. 人物和背景分别控制
2. 三维重建
3. 更精准的图片风格化
4. 更精准的图片局部重绘
以上就是本教程的全部内容了，重点介绍了controlnet模型功能实用，当然还有一些小众的模型在本次教程中没有出现，目前controlnet模型确实还挺多的，所以重点放在了官方发布的几个模型上。
同时大家可能都想学习AI绘画技术,也想通过这项技能真正赚到钱，但是不知道该如何开始学习，因为网上的资料太多太杂乱了，如果不能系统的学习就相当于是白学，因为自身做副业需要，我这边整理了全套的Stable Diffusion入门知识点资料，大家有需要可以直接点击下边卡片获取，希望能够真正帮助到大家。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-30T10:38:07+08:00">
    <meta property="article:modified_time" content="2024-05-30T10:38:07+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Stable Diffusion为什么生成的图片总是糊的？</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>我们先看一下出图时模糊的图片效果。</p> 
<p></p> 
<p class="img-center"><img alt="" height="1024" src="https://images2.imgbox.com/2b/bb/KARGeXAi_o.png" width="1024"></p> 
<p></p> 
<p class="img-center"><img alt="图片" height="1024" src="https://images2.imgbox.com/ef/84/qDFuvCTS_o.png" width="1024"></p> 
<p>我相信很多初学者在开始绘图的时候经常会碰到这种情况。当然我自己也曾经碰到过，我总结了一下，一般有以下几种情况。</p> 
<p><strong>第一种情况：大模型使用的是SDXL大模型，VAE模型选择了vae-ft-mse-840000-ema-pruned.safetensors。（必现场景）</strong></p> 
<p>这种情况是必现的。也是当时网友私聊时使用SD出图模糊的主要原因。</p> 
<p>为什么这种情况会出现呢？</p> 
<p>我们首先要了解一下什么是VAE模型，可以把VAE模型理解为颜色滤镜，它主要用于调节和美化生成图片的色彩。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="537" src="https://images2.imgbox.com/06/b4/1S4pKQTX_o.png" width="1080"></p> 
<p>对于VAE模型，可以参照我之前的文章【<a href="http://mp.weixin.qq.com/s?__biz=MzkzNTUxMTExOA==&amp;mid=2247486927&amp;idx=1&amp;sn=afd991049831e6af92adf1a8db4b3e05&amp;chksm=c2ad98eef5da11f8d5f7cb37d45539efb5733f940699e47971622fbfd491d8f0fda96082ba01&amp;scene=21#wechat_redirect" rel="nofollow" title="Stable Diffusion【模型篇】：VAE模型】">Stable Diffusion【模型篇】：VAE模型】</a>,写的时间有些早，但是可以参照看一下。</p> 
<p></p> 
<p>VAE模型为什么会造成这个问题呢，主要是VAE模型也区分SD1.5和SDXL的模型的。我们的大模型使用的是基于SDXL的大模型，但是VAE模型选择的ae-ft-mse-840000-ema-pruned.safetensors是基于SD1.5的大模型，二者并不匹配。</p> 
<p>下面是C站上面关于SD1.5的模型。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="518" src="https://images2.imgbox.com/c8/73/Q3lTRTpi_o.png" width="1080"></p> 
<p>从上面的图片我们可以看到很多常见的VAE模型都是基于SD1.5的。这里就包括我们上面提到的vae-ft-mse-840000-ema-pruned。</p> 
<p>关于SD1.5的VAE模型这里罗列了一些常见的，感兴趣的朋友可以了解一下。</p> 
<ul><li> <p>vae-ft-mse-840000-ema-pruned</p> </li><li> <p>vae-ft-ema-560000-ema</p> </li><li> <p>kl-f8-anime系列（主要用于动漫大模型）</p> </li><li> <p>ClearVAE（主要用于动漫大模型）</p> </li><li> <p>Color101（生成更鲜艳色彩，更接近HDR效果和更自然图像的VAE）</p> </li></ul> 
<p>下面是C站上面关于SDXL的模型。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="570" src="https://images2.imgbox.com/60/16/na8OxWa9_o.png" width="1080"></p> 
<p>基于SDXL的VAE模型相当比较少，最常用的是sdxl vae模型。为什么基于SDXL的VAE模型比较少呢，因为VAE模型本身就比较小，很多SDXL的大模型都已经融入了VAE模型，所以基于SDXL的大模型一般不选择VAE模型出图效果也不错的。例如在LiblibAI网站，当我们选择SDXL大模型的时候，VAE模型选项是禁止选择的。</p> 
<p></p> 
<p>一般来说，对于基于SDXL的大模型，我们VAE模型可以不用选择，要选择的话最好选择sdxl_vae。</p> 
<p>我们来对比看一下效果。</p> 
<p><strong>正向提示词</strong>：1 super beautiful chinese girl, solo</p> 
<p><strong>反向提示词</strong>：nsfw,easynegative,ng_deepnegative_v1_75t,(worst quality:2),(low quality:2),(normal quality:2),lowres,bad anatomy,bad hands,normal quality,((monochrome)),((grayscale)),((watermark)),</p> 
<p>下面是相关参数配置：</p> 
<ul><li> <p>大模型：SDXL 1.0的基础大模型</p> </li><li> <p>采样器：Euler a</p> </li><li> <p>采样迭代步数：30</p> </li><li> <p>图片宽高：1024*1024。</p> </li><li> <p>提示词引导系数（CFG）：7</p> </li></ul> 
<p>不使用VAE模型生成的图片</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="1024" src="https://images2.imgbox.com/b3/0b/4PVhcofP_o.png" width="1024"></p> 
<p></p> 
<p class="img-center"><img alt="图片" height="1024" src="https://images2.imgbox.com/c1/8a/zH9BSDtW_o.png" width="1024"></p> 
<p>使用sdxl_vae模型生成的图片</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="1024" src="https://images2.imgbox.com/19/7f/LdnMcQ3k_o.png" width="1024"></p> 
<p></p> 
<p></p> 
<p class="img-center"><img alt="图片" height="1024" src="https://images2.imgbox.com/67/e9/VEJlZRCa_o.png" width="1024"></p> 
<p>基于SDXL的大模型使用VAE模型和不使用VAE模型差别大吗，大家可以自己甄别哈。</p> 
<p><strong>总结：使用SDXL的大模型+SD 1.5的VAE模型生成的图片大概率都是糊的。</strong></p> 
<p></p> 
<p><strong>第二种情况：大模型使用的是基于SDXL大模型，选择了Refiner(精炼器)</strong></p> 
<p>这种情况不是必现的。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="402" src="https://images2.imgbox.com/28/3c/yI3GfC7f_o.png" width="892"></p> 
<p>今天尝试了很多次，都没有模拟出来，不过之前在使用SDXL基础模型出图的时候碰到的概率还是蛮大的，后面我选择SDXL基础模型的时候关于Refiner选项我一般是不选中的。</p> 
<p>关于SDXL的Refiner(精炼器)主要属于细化模型。对图像进一步优化细节。目前有些大模型并不兼容Refiner(精炼器)。</p> 
<p></p> 
<p><strong>第三种情况：由于各种原因图片未生成完毕SD系统工具出问题了，例如报OOM的显存异常等</strong></p> 
<p></p> 
<p>另外：如果我们采样迭代步数设置的比较小，例如上面的示例中将采样迭代步数设置为5，出来的图片也是模糊的，但是和上面图片的模糊不是一个类型哈。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="1024" src="https://images2.imgbox.com/bc/1a/GfX8D4LZ_o.png" width="1024"></p> 
<p></p> 
<p></p> 
<p class="img-center"><img alt="图片" height="1024" src="https://images2.imgbox.com/d8/16/Gc3D6XSE_o.png" width="1024"></p> 
<p>还有其他的情况吗，欢迎各位小伙伴们积极补充哦。</p> 
<p><strong>这里就简单说几种应用：</strong></p> 
<p><strong>1. 人物和背景分别控制</strong></p> 
<p><strong>2. 三维重建</strong></p> 
<p><strong>3. 更精准的图片风格化</strong></p> 
<p><strong>4. 更精准的图片局部重绘</strong></p> 
<p>以上就是本教程的全部内容了，重点介绍了controlnet模型功能实用，当然还有一些小众的模型在本次教程中没有出现，目前controlnet模型确实还挺多的，所以重点放在了官方发布的几个模型上。</p> 
<p>同时大家可能都想学习AI绘画技术,也想通过这项技能真正赚到钱，但是不知道该如何开始学习，因为网上的资料太多太杂乱了，如果不能系统的学习就相当于是白学，因为自身做副业需要，我这边整理了全套的Stable Diffusion入门知识点资料，大家有需要可以直接点击下边卡片获取，希望能够真正帮助到大家。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/32/0a/ud2Aybdi_o.jpg"> </p> 
<p></p> 
<p class="img-center"><img alt="img" height="861" src="https://images2.imgbox.com/18/72/S7ZKYzsC_o.png" width="720"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/619d7c4a8ea40a325a1d8929e6492d68/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【spark sedona k8s】spark(sedona) on kubernetes(k8s) 搭建，使用spark-operator提交任务</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f87ac3bc0877e13825930da2672b1244/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Flink Paimon0.8 构建 ods层、dw层，</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>