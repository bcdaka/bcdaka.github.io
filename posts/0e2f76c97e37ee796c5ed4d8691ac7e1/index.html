<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>无界AI算法总监邹国平：Midjourney领跑，没有标准答案的文生图，下半场还能怎么卷？ - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/0e2f76c97e37ee796c5ed4d8691ac7e1/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="无界AI算法总监邹国平：Midjourney领跑，没有标准答案的文生图，下半场还能怎么卷？">
  <meta property="og:description" content="进入2024，AI应用落地的步伐悄然加速。
AI文生图产品——连接着千行百业的设计需求，极大地压缩了创意实现的周期——正成为快速掘金的AI领域之一。
AI生图自2022年底开始爆发，过去一年多的时间已经让这条赛道变得内卷起来，前有 Midjourney、DALL·E、Stable Diffusion 稳稳占据头部市场，后有GPT4等多模态大模型随时可能抄后路的危险，再加上诸多类似服务的图像编辑及设计工具也在加快融合&#34;文生图&#34;能力，多方都在对这块最稳定的AIGC赛道虎视眈眈。
在激烈的内卷中，文生图玩家想要“出头”，就必须打出自己的特色来。
作为国内最早一批的AIGC产品落地实践者，无界AI算法总监邹国平有着非常深刻的体会。邹国平给出了一个令人意想不到的答案。
破局的关键就在于“场景”：你面向什么样的场景？你擅长什么场景？在这个场景里，你是否能够深耕到一个非常领先的水平？他向我们抛出了这一连串的问题。 在通用AI模型盛行的今天，为什么文生图领域依旧是个很“吃”场景的命题？
追问之下，谜底揭开：文生图看似相对较低的门槛，却建在审美、设计以及专业场景知识的高山之上。AI文生图比文生文更模糊、更没有标准答案。
最近，51CTO直播栏目《AIGC实战派》有幸邀请到了邹国平老师，在两个小时的交流中，他向我们分享了自己在文生图领域的探索心得与洞察，详细探讨了在文生图下半场如何“卷”出技术壁垒。这次交谈，澄清和刷新了原来大众对于文生图领域的”误解“与认知，其中他提出了几个很有意思的观点：
国外文生图产品Leonardo.AI、yodayo等从定位出发逐渐完成差异化，国内还缺少真正领先的产品Midjourney将工程和产品部分托给Disco diffusion平台，保证了探索模型和算法的专注度Midjourney通过广泛的用户接触和使用，已经形成了一套关于如何描述prompt的范式Sora的技术路线将作为我们的一个重要参考，无论是在图像生成还是视频生成的应用提示词与文生图效果的对齐、生成图像时长的压缩和个性化生成将是文生图产品继续跟进的几个方向文生图作为新兴领域，许多B端客户无法给出需求的准确描述，交付过程是个共同探索与改进的过程为了应对千万级流量情况，我们联合了几家GPU厂商，准备了上万规模的GPU资源进行调度。 以下是整理后的对话内容。
1.Midjourney先发优势太“难杀”，国内追平还需多维度发力 51CTO《AIGC实战派》: Midjourney现在是全球范围内公认的用户最多、效果最好的文生图产品之一。在您看来，国内的文生图产品距离Midjourney还有哪些差距？
邹国平: 差距肯定存在。Midjourney无论是用户体量，还是整体营收完全是遥遥领先的。
相比国内的文生图产品Midjourney的先发优势非常大。回顾Midjourney的发展历程，22年的时候，他们就已经进行了两年多的研发积累。刚开始的第一版产品效果也不理想，但他们通过邀请制去招募用户参与到迭代中，持续改进产品。
22年下半年，Stable Diffusion的技术横空出世，给Midjourney的产品带来了一个爆发点。直到迭代至V4版本，已经呈现出行业标杆级的效果。
Midjourney自身有强大的专注度，完全focus在模型和算法探索领域。至于工程和产品的部分，则更多在Disco diffusion平台上去实现，一来节省了不少的开发成本，其次这个平台带有的社群属性，让Midjourney用户持续裂变，通过口口相传沉淀起庞大的用户基础。
51CTO《AIGC实战派》：在Midjourney迭代到V4之前，国内大概在做什么？
邹国平: 更古老一点的，还没有走到文生图这一步。早期所谓的图像生成，更多的是特效及其他特定场景的生成，不是具体物品的成像。
51CTO《AIGC实战派》: Midjourney V4之后领先在哪？
邹国平: V4版本的生成效果非常出色。特别是在特定领域，已经达到了实际可用的状态。Midjourney在数据处理方面有个非常独特之处，无论是数据质量还是数据标注都非常精细。通过广泛的用户接触和使用，已经形成了一套关于如何描述prompt的范式，这些关键词在构图、风格和艺术家风格的表达中起到了关键作用。
51CTO《AIGC实战派》: 我们现在做国内外文生图领域的比较，像您刚才说的模型、提示词，可能基本算是一个追齐的状态？您觉得我们还差在哪里？
邹国平: 在模型层面，一些国内的模型在特定测试集上已经能够与Midjourney的V5.2版本相媲美，但在广泛的用户场景测试中，我们的数据量还远远不够，所以很难精准的去回答“追平”的问题。因为我们所能进行的测试有限，要进行用户测试，让用户用手投票，这样的用户反馈可能仅有几千例。而Midjourney拥有2000万用户。
所以在小范围上去测，那确实可以说国内产品跟他的效果差不多。但如果要推广到所有的场景，确实是会有差距的。
但像Midjourney这样，能做到领先的，目前还是不太能看到。再往下走，每个环节还有很多工作需要补充。
2. Sora不只给方向，还给正在探索DIT的人吃了“定心丸” 51CTO《AIGC实战派》: 今年2月份，sora出现之后，有没有冲击到现在的AI生图领域？
邹国平: 会有影响的。首先像你刚才所说的，视频的确是流量的高地。国内在做Open-Sora项目时，也是按照既能生成视频也能生成图片的思路去做的。两者在技术上有相通之处。
Sora给了这个领域很大的信心。在此之前，基于DIT的尝试已经有一些，只是效果都没能达到实际可用的水平。Sora证明了端到端的视频生成这条路是能走通的。 51CTO《AIGC实战派》: 从Sora在技术报告中公开的那部分来看，能给图像生成领域带来哪些启发或者值得借鉴的地方？
邹国平: 说到方向的话，大家更多是采用DIT技术构建的模型，比如PixArt文生图模型，展示了在少量参数的模型(0.6B)上也能取得良好效果的可能性。
此外，目前的文生图还不足以实现精准控制。因此，引入额外的控制机制，就像PixArt在第二版本中所增加的功能，是完善模型的关键。这些控制可以平滑地集成到现有功能中，提供强大的设计和创造能力。
例如，在汽车设计中，可以将线稿图的规范与文本描述结合，实现更精确的图像生成控制。
51CTO《AIGC实战派》: 抛开Sora，文生图未来可能有哪些比较热的演进方向？
邹国平: 我从文生图领域目前面临的一些主要问题出发来谈谈。
首先，提示词的精准度和生成效果的对齐是一个需要解决的问题。尽管我们可以将提示词写得非常精细，但图像的细节呈现并不总是像文本描述的那样，典型的如手部细节的处理问题。
其次，生成图像的时间压缩也是一个挑战。目前，生成一张图像可能需要数秒的时间，如果加入更多控制，时间可能会更长。因此，工程上需要探索模型蒸馏和加速手段来提升效率。
最后，个性化生成是另一个重要的应用方向，这往往涉及到相关的参考图像。具体到图片生成，目前相似度的稳定性还有待提高，比如处理logo的自由变换时保持其细节不变等等。 51CTO《AIGC实战派》: 您觉得专有的AI生成工具，会不会被类似Sora这种很强大的通用工具吃掉？
邹国平：通用的文生图能力最终可能会被大型模型所覆盖。像GPT-4 Vision这样的模型已经具备了视觉感知能力，能够识别和描述图片内容，但目前还未实现生成或创造能力。语言模型在认知方面已经达到了高水平，但视觉、理解以及创造的过程则更为复杂。
Sora对OpenAI来说，意义是找到了一个通往世界模型的道路。OpenAI的使命和站位让他不会开发非常垂的产品，他们做的是提供一个平台，类似于乐高积木，让用户根据需求自己去搭建想要的应用。
3.文生图要“卷”出技术壁垒，先要从场景出发 51CTO《AIGC实战派》: 都说文生图的创业门槛低，在现在这么卷的情况之下，大家都很好奇技术圈到底是在卷哪块东西？在什么维度还可以打出差异化，还能拼出技术实力、拼出竞争力来？
邹国平: 我们可以看看，现在领先的文生图产品都是怎么做的。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-11T08:45:00+08:00">
    <meta property="article:modified_time" content="2024-05-11T08:45:00+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">无界AI算法总监邹国平：Midjourney领跑，没有标准答案的文生图，下半场还能怎么卷？</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>进入2024，AI应用落地的步伐悄然加速。</p> 
<p><img alt="" height="377" src="https://images2.imgbox.com/00/82/7I8SttRh_o.png" width="676"></p> 
<p>AI文生图产品——连接着千行百业的设计需求，极大地压缩了创意实现的周期——正成为快速掘金的AI领域之一。</p> 
<p>AI生图自2022年底开始爆发，过去一年多的时间已经让这条赛道变得内卷起来，前有 Midjourney、DALL·E、Stable Diffusion 稳稳占据头部市场，后有GPT4等多模态大模型随时可能抄后路的危险，再加上诸多类似服务的图像编辑及设计工具也在加快融合"文生图"能力，多方都在对这块最稳定的AIGC赛道虎视眈眈<strong>。</strong></p> 
<p>在激烈的内卷中，文生图玩家想要“出头”，就必须打出自己的特色来。</p> 
<p>作为国内最早一批的AIGC产品落地实践者，无界AI算法总监邹国平有着非常深刻的体会。邹国平给出了一个令人意想不到的答案。</p> 
<p>破局的关键就在于“场景”：<strong>你面向什么样的场景？你擅长什么场景？在这个场景里，你是否能够深耕到一个非常领先的水平</strong>？他向我们抛出了这一连串的问题。   </p> 
<p>在通用AI模型盛行的今天，为什么文生图领域依旧是个很“吃”场景的命题？</p> 
<p>追问之下，谜底揭开：<strong>文生图看似相对较低的门槛，却建在审美、设计以及专业场景知识的高山之上。</strong>AI文生图比文生文更模糊、更没有标准答案。</p> 
<p>最近，51CTO直播栏目《AIGC实战派》有幸邀请到了邹国平老师，在两个小时的交流中，他向我们分享了自己在文生图领域的探索心得与洞察，详细探讨了在文生图下半场如何“卷”出技术壁垒。这次交谈，澄清和刷新了原来大众对于文生图领域的”误解“与认知，其中他提出了几个很有意思的观点：</p> 
<ul><li>国外文生图产品Leonardo.AI、yodayo等从定位出发逐渐完成差异化，国内还缺少真正领先的产品</li><li>Midjourney将工程和产品部分托给Disco diffusion平台，保证了探索模型和算法的专注度</li><li>Midjourney通过广泛的用户接触和使用，已经形成了一套关于如何描述prompt的范式</li><li>Sora的技术路线将作为我们的一个重要参考，无论是在图像生成还是视频生成的应用</li><li>提示词与文生图效果的对齐、生成图像时长的压缩和个性化生成将是文生图产品继续跟进的几个方向</li><li>文生图作为新兴领域，许多B端客户无法给出需求的准确描述，交付过程是个共同探索与改进的过程</li><li>为了应对千万级流量情况，我们联合了几家GPU厂商，准备了上万规模的GPU资源进行调度。</li></ul> 
<p>以下是整理后的对话内容。</p> 
<h4>1.Midjourney先发优势太“难杀”，国内追平还需多维度发力    </h4> 
<p><strong>51CTO《AIGC实战派》: Midjourney现在是全球范围内公认的用户最多、效果最好的文生图产品之一。在您看来，国内的文生图产品距离Midjourney还有哪些差距？</strong></p> 
<p><strong>邹国平: </strong>差距肯定存在。Midjourney无论是用户体量，还是整体营收完全是遥遥领先的。</p> 
<p>相比国内的文生图产品Midjourney的先发优势非常大。回顾Midjourney的发展历程，22年的时候，他们就已经进行了两年多的研发积累。刚开始的第一版产品效果也不理想，但他们通过邀请制去招募用户参与到迭代中，持续改进产品。</p> 
<p>22年下半年，Stable Diffusion的技术横空出世，给Midjourney的产品带来了一个爆发点。直到迭代至V4版本，已经呈现出行业标杆级的效果。</p> 
<p><strong>Midjourney自身有强大的专注度，完全focus在模型和算法探索领域。</strong>至于工程和产品的部分，则更多在Disco diffusion平台上去实现，一来节省了不少的开发成本，其次这个平台带有的社群属性，让Midjourney用户持续裂变，通过口口相传沉淀起庞大的用户基础。</p> 
<p><strong>51CTO《AIGC实战派》：在Midjourney迭代到V4之前，国内大概在做什么？</strong></p> 
<p><strong>邹国平: </strong>更古老一点的，还没有走到文生图这一步。早期所谓的图像生成，更多的是特效及其他特定场景的生成，不是具体物品的成像。</p> 
<p><strong>51CTO《AIGC实战派》: Midjourney V4之后领先在哪？</strong></p> 
<p><strong>邹国平: </strong>V4版本的生成效果非常出色。特别是在特定领域，已经达到了实际可用的状态。Midjourney在数据处理方面有个非常独特之处，无论是数据质量还是数据标注都非常精细。通过广泛的用户接触和使用，已经形成了一套关于如何描述prompt的范式，这些关键词在构图、风格和艺术家风格的表达中起到了关键作用。</p> 
<p><strong>51CTO《AIGC实战派》: 我们现在做国内外文生图领域的比较，像您刚才说的模型、提示词，可能基本算是一个追齐的状态？您觉得我们还差在哪里？</strong></p> 
<p><strong>邹国平: </strong>在模型层面，一些国内的模型在特定测试集上已经能够与Midjourney的V5.2版本相媲美，但在广泛的用户场景测试中，我们的数据量还远远不够，所以很难精准的去回答“追平”的问题。因为我们所能进行的测试有限，要进行用户测试，让用户用手投票，这样的用户反馈可能仅有几千例。而Midjourney拥有2000万用户。</p> 
<p>所以在小范围上去测，那确实可以说国内产品跟他的效果差不多。但如果要推广到所有的场景，确实是会有差距的。</p> 
<p>但像Midjourney这样，能做到领先的，目前还是不太能看到。再往下走，每个环节还有很多工作需要补充。</p> 
<h4>2. Sora不只给方向，还给正在探索DIT的人吃了“定心丸”</h4> 
<p><strong>51CTO《AIGC实战派》: 今年2月份，sora出现之后，有没有冲击到现在的AI生图领域？</strong></p> 
<p><strong>邹国平: </strong>会有影响的。首先像你刚才所说的，视频的确是流量的高地。国内在做Open-Sora项目时，也是按照既能生成视频也能生成图片的思路去做的。两者在技术上有相通之处。</p> 
<p>Sora给了这个领域很大的信心。在此之前，基于DIT的尝试已经有一些，只是效果都没能达到实际可用的水平。Sora证明了端到端的视频生成这条路是能走通的。   </p> 
<p><strong>51CTO《AIGC实战派》: 从Sora在技术报告中公开的那部分来看，能给图像生成领域带来哪些启发或者值得借鉴的地方？</strong></p> 
<p><strong>邹国平:</strong> 说到方向的话，大家更多是采用DIT技术构建的模型，比如PixArt文生图模型，展示了在少量参数的模型(0.6B)上也能取得良好效果的可能性。</p> 
<p>此外，目前的文生图还不足以实现精准控制。因此，引入额外的控制机制，就像PixArt在第二版本中所增加的功能，是完善模型的关键。这些控制可以平滑地集成到现有功能中，提供强大的设计和创造能力。</p> 
<p>例如，在汽车设计中，可以将线稿图的规范与文本描述结合，实现更精确的图像生成控制。</p> 
<p><strong>51CTO《AIGC实战派》: 抛开Sora，文生图未来可能有哪些比较热的演进方向？</strong></p> 
<p><strong>邹国平: </strong>我从文生图领域目前面临的一些主要问题出发来谈谈。</p> 
<p>首先，提示词的精准度和生成效果的对齐是一个需要解决的问题。尽管我们可以将提示词写得非常精细，但图像的细节呈现并不总是像文本描述的那样，典型的如手部细节的处理问题。</p> 
<p>其次，生成图像的时间压缩也是一个挑战。目前，生成一张图像可能需要数秒的时间，如果加入更多控制，时间可能会更长。因此，工程上需要探索模型蒸馏和加速手段来提升效率。</p> 
<p>最后，个性化生成是另一个重要的应用方向，这往往涉及到相关的参考图像。具体到图片生成，目前相似度的稳定性还有待提高，比如处理logo的自由变换时保持其细节不变等等。   </p> 
<p><strong>51CTO《AIGC实战派》: 您觉得专有的AI生成工具，会不会被类似Sora这种很强大的通用工具吃掉？</strong></p> 
<p><strong>邹国平：</strong>通用的文生图能力最终可能会被大型模型所覆盖。像GPT-4 Vision这样的模型已经具备了视觉感知能力，能够识别和描述图片内容，但目前还未实现生成或创造能力。语言模型在认知方面已经达到了高水平，但视觉、理解以及创造的过程则更为复杂。</p> 
<p>Sora对OpenAI来说，意义是找到了一个通往世界模型的道路。OpenAI的使命和站位让他不会开发非常垂的产品，他们做的是提供一个平台，类似于乐高积木，让用户根据需求自己去搭建想要的应用。</p> 
<h4>3.文生图要“卷”出技术壁垒，先要从场景出发</h4> 
<p><strong>51CTO《AIGC实战派》: 都说文生图的创业门槛低，在现在这么卷的情况之下，大家都很好奇技术圈到底是在卷哪块东西？在什么维度还可以打出差异化，还能拼出技术实力、拼出竞争力来？</strong></p> 
<p><strong>邹国平: </strong>我们可以看看，现在领先的文生图产品都是怎么做的。</p> 
<p>不少文生图产品已经取得了不错的成绩，其实他们的产品理念却是各不相同：比如Leonardo.AI，它最初的着陆点在生成游戏角色的物料，后来才慢慢发展成一个全类别的文生图平台。还有yodayo，则一开始做二次元领域的生成起家，后来扩展到用户与虚拟角色的聊天服务上。而由前谷歌imagen团队大佬创立的Ideogram，则以文字生成为长板。</p> 
<p>这些产品都是成功实现差异化的案例。现在Leonardo.AI每月的PV将近1000万。</p> 
<p>回到问题本身，文生图要“卷”出自己的技术壁垒，首先就要从场景出发。你面向什么样的场景？你擅长什么场景？在这个场景里，你是否能够深耕到一个非常领先的水平？——这其实也跟模型有关，需要你的模型有一定的独到之处。</p> 
<p><strong>51CTO《AIGC实战派》: 怎么把模型做出独到之处？</strong></p> 
<p><strong>邹国平: </strong>首先是有个目标，了解模型面向的用户和场景。比如，模型focus在游戏素材生成上，那么就针对这个领域深入优化，去做材质、光照等属性的编辑。      </p> 
<p><strong>51CTO《AIGC实战派》: 现在是哪种模式更多一点？是让设计和AI的专才进行合作，还是直接寻找两个领域的通才？无界AI团队是怎么考虑的？</strong></p> 
<p><strong>邹国平：</strong>我们去做模型训练，会有一个模型主理人，他需要在这个领域有一定的知识储备，去把输入和输出对齐。</p> 
<p>AI如何让强者更强，就是能利用强者的知识储备，通过大模型描述性的方式，最终呈现出来。</p> 
<p><strong>51CTO《AIGC实战派》:  人才也是技术壁垒的一部分？</strong></p> 
<p><strong>邹国平: </strong>AI时代，拼的就是三个要素，人才、数据、算力。</p> 
<p>刚才说了文生图要“卷”场景。其次，数据处理能力也是关键，行业数据和算力的储备对于图像生成领域的积累至关重要。</p> 
<p>虽然文生图模型的参数量相对较小，可能亿级别就足够，但这并不意味着算力不是门槛，对算力的需要取决于模型的训练目标。训练的数据量小，那一张消费级显卡就能搞定，但像Midjourney这种规模还是需要强大算力支撑的。他们早期在亚马逊拿到了1000万美元的算力。</p> 
<h4>4.“几家GPU厂商提供了非常动态的扩容能力，极短时间对接上千块显卡”</h4> 
<p><strong>51CTO《AIGC实战派》:  AI产品用户达到百万级甚至千万级，这时候我们该怎么应对？无界AI在短时间内积累到百万用户的时候，都进行了怎样的备案？</strong></p> 
<p><strong>邹国平:</strong>用户的涌入会需要处理一些突发的事件。相比文字，图像生成对GPU资源的消耗更高，我们需要及时增加GPU资源，避免用户动辄为一张图像的生成等待10s以上。目前主要用的GPU资源都是云端的卡。</p> 
<p>其次，文生图需要面临更为复杂的情况，我们有多个模型，而每个模型的用户量又不同。这就要求我们建立一个高效的调度系统来处理用户提交的任务。系统应该能够根据模型的使用情况动态调整资源分配，对于不同的模型，我们可能需要定制化的调度方案。</p> 
<p>此外，我们还需要优化单个GPU卡的工作效率，通过加速方案和模型优化来提高单次图像生成任务的效率。这包括提高模型的加载速度、生成和切换速度，以及优化整个系统的扩展性。</p> 
<p>举个例子，我们与头部消费品品牌合作进行营销活动时，就面临过千万级别的流量挑战。为了应对这种情况，我们联合了几家GPU厂商，准备了上万规模的GPU资源进行调度。我们自有的GPU云平台可以快速地基于第三方GPU资源进行动态扩容，在很短的时间内就响应上千块显卡的对接。   </p> 
<p><strong>51CTO《AIGC实战派》: 说到用户体验，文生图用户对于排队这个现象的忍受度怎么样？</strong></p> 
<p><strong>邹国平: </strong>如果产品提供的文生图效果很好的话，那排队也是能被用户接受的。有些时候，你不是VIP用户可能会故意让你生成速度慢一点，就是逼你交钱的（笑）。</p> 
<p>不过，也分应用场景，比如进行定制化的化身或是视频风格转换，这些任务本身就需要较长的处理时间。不过用户自己也会有预期，所以就愿意为此等待。</p> 
<h4>5.不同于文生文，AI文生图不存在标准答案</h4> 
<p><strong>51CTO《AIGC实战派》: 做一款AI原生应用的产品，最抓狂的地方是在哪里？</strong></p> 
<p><strong>邹国平:</strong> AIGC发展到现在，已经有一段的时间了。随着时间的推移，AI生成技术已经从效果一般发展到可用状态，我们一直在进行用户教育，去同步认知。首先是怎么去生成图像，其次就是让用户理解在当前技术的限制下，生成的图像仍然存在瑕疵。</p> 
<p>最抓狂的是，在某些场景下，用户可能非常挑剔，尤其是B端用户。众所周知，AI生成确实有一定的随机性和不可控制性，bad case总是存在的，而且时不时就会冒出来，这给产品的维护带来挑战。</p> 
<p><strong>51CTO《AIGC实战派》:  B端用户要做一个定制方案，需要多久才能交付完成？</strong></p> 
<p><strong>邹国平:</strong> 交付时长按月计，但不确定性很大。</p> 
<p>总体来讲，文生图还是一个比较新兴的东西。客户的需求在他脑海中，有些是很难用语言描述出来的，因此前期是一个相互探索的过程，需要不断地提供初步方案、产出结果，等拿到客户的反馈以后才知道怎么跟进。   </p> 
<p>这也说明了，尽管文生图看似门槛低，但在细节把握上却非常具有挑战性。这也是为什么我们（无界AI）的专业版工作流功能旨在赋予用户更多的自主发挥空间，让有探索和动手能力的用户设计个性化的文生图流程。</p> 
<p><strong>51CTO《AIGC实战派》:  正在研究的哪些方向，可以透露一下吗？</strong></p> 
<p><strong>邹国平:</strong>我们目前的研究重点是围绕几个创新方向进行的。首先最大的期待还是Sora模型的复现。Sora涉及到从视频噪声片段出发，生成连贯且稳定的视频内容，这与以往的单帧生成完全不同。Sora的技术路线将作为我们的一个重要参考，无论是在图像生成还是视频生成的应用上。</p> 
<p>在3D领域，我们也在进行一些尝试，包括通过单张图像重建3D模型。比如通过线稿生成具有真实质感纹理的3D模型。</p> 
<p>另一个有趣的研究方向是通过文本直接生成具有透明背景的PNG图片，这意味着用户无需再进行抠图。 </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ea8057be1870488b717ed55803470121/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">解读计数器算法：原理、Java实现与优劣分析</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1614ed12ce7749992d9e2dc6c1367a0a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Hive大表join大表如何调优</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>