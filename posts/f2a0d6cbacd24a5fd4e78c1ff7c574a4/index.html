<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>实验五 Spark Streaming编程初级实践 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/f2a0d6cbacd24a5fd4e78c1ff7c574a4/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="实验五 Spark Streaming编程初级实践">
  <meta property="og:description" content="1 实验目的 （1）通过实验学习使用Scala编程实现文件和数据的生成。
（2）掌握使用文件作为Spark Streaming数据源的编程方法。
2 实验平台 操作系统：Ubuntu16.04及以上。
Spark版本：3.4.0。
Scala版本：2.12.17。
3 实验要求 产生一系列字符串的程序，会产生随机的整数序列，每个整数被当做一个单词，提供给KafkaWordCount程序去进行词频统计
4 实验内容和步骤（操作结果要附图） Kafka准备工作（执行如下命令完成Kafka的安装：） 这里使用的软件版本是：kafka_2.12-2.6.0，Spark3.4.0（Scala版本是2.12.17）
链接: https://pan.baidu.com/s/1RS42IvXxtTVvIL24IwcjTA?pwd=1234 提取码: 1234 安装kafka cd ~/Downloads sudo tar -zxf kafka_2.12-2.6.0.tgz -C /usr/local cd /usr/local sudo mv kafka_2.12-2.6.0 kafka sudo chown -R hadoop ./kafka 启动Kafka
第一个终端
cd /usr/local/kafka ./bin/zookeeper-server-start.sh config/zookeeper.properties 行上面命令以后，终端窗口会返回一堆信息，然后就停住不动了，是Zookeeper服务器启动了，正在处于服务状态。所以，千万不要关闭这个终端窗口，一旦关闭，zookeeper服务就停止了，所以，不能关闭这个终端窗口
再打开第二个（不能关闭）
​cd /usr/local/kafka bin/kafka-server-start.sh config/server.properties 再打开第三个终端，然后输入下面命令创建一个自定义名称为“wordsender”的Topic：
​cd /usr/local/kafka ./bin/kafka-topics.sh --create --zookeeper localhost:2181 \ &gt; --replication-factor 1 --partitions 1 \ &gt; --topic wordsender 查看名称为“wordsender”的Topic是否已经成功创建：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-08T18:51:36+08:00">
    <meta property="article:modified_time" content="2024-05-08T18:51:36+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">实验五 Spark Streaming编程初级实践</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3 style="text-align:justify;"><strong><strong><strong>1 实验目的</strong></strong></strong></h3> 
<p style="margin-left:.0001pt;text-align:justify;">（1）通过实验学习使用Scala编程实现文件和数据的生成。</p> 
<p style="margin-left:.0001pt;text-align:justify;">（2）掌握使用文件作为Spark Streaming数据源的编程方法。</p> 
<h3 style="text-align:justify;"><strong><strong><strong>2 实验平台</strong></strong></strong></h3> 
<p style="margin-left:.0001pt;text-align:justify;">操作系统：Ubuntu16.04及以上。</p> 
<p style="margin-left:.0001pt;text-align:justify;">Spark版本：3.4.0。</p> 
<p style="margin-left:.0001pt;text-align:justify;">Scala版本：2.12.17。</p> 
<h3 style="text-align:justify;"><strong><strong><strong>3 实验要求</strong></strong></strong></h3> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#4d4d4d;">产生一系列字符串的程序，会产生随机的整数序列，每个整数被当做一个单词，提供给KafkaWordCount程序去进行词频统计</span></span></p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<h3 style="text-align:justify;"><strong><strong><strong>4 实验内容和步骤</strong></strong>（操作结果要附图）</strong></h3> 
<ul><li style="text-align:left;"><span style="background-color:#ffffff;"><strong><a href="https://so.csdn.net/so/search?q=Kafka&amp;spm=1001.2101.3001.7020" title="Kafka">Kafka</a><strong><span style="background-color:#ffffff;"><span style="color:#4f4f4f;"><strong>准备工作</strong></span></span></strong><strong><span style="background-color:#ffffff;"><span style="color:#4f4f4f;"><strong>（</strong></span></span></strong><span style="background-color:#fefefe;"><span style="color:#333333;">执行如下命令完成Kafka的安装：</span></span><strong><span style="background-color:#ffffff;"><span style="color:#4f4f4f;"><strong>）</strong></span></span></strong></strong></span></li></ul> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#fefefe;"><span style="color:#333333;">这里使用的软件版本是：kafka_2.12-2.6.0，Spark3.</span></span><span style="background-color:#fefefe;"><span style="color:#333333;">4</span></span><span style="background-color:#fefefe;"><span style="color:#333333;">.0（Scala版本是2.12.1</span></span><span style="background-color:#fefefe;"><span style="color:#333333;">7</span></span><span style="background-color:#fefefe;"><span style="color:#333333;">）</span></span></p> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#fefefe;"><span style="color:#333333;">链接: https://pan.baidu.com/s/1RS42IvXxtTVvIL24IwcjTA?pwd=1234 提取码: 1234 </span></span></p> 
<ul><li style="text-align:justify;"><strong><strong>安装kafka</strong></strong></li></ul> 
<pre><code class="language-bash">cd ~/Downloads

sudo tar -zxf  kafka_2.12-2.6.0.tgz -C /usr/local

cd /usr/local

sudo mv kafka_2.12-2.6.0 kafka

sudo chown -R hadoop ./kafka</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><span style="background-color:#ffffff;"><span style="color:#4d4d4d;"><strong>启动Kafka</strong></span></span></strong></p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><span style="background-color:#ffffff;"><span style="color:#4d4d4d;"><strong>第一个终端</strong></span></span></strong></p> 
<pre><code class="language-bash">cd /usr/local/kafka

./bin/zookeeper-server-start.sh config/zookeeper.properties</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#4d4d4d;">行上面命令以后，终端窗口会返回一堆信息，然后就停住不动了，是Zookeeper服务器启动了，正在处于服务状态。所以，千万不要关闭这个终端窗口，一旦关闭，zookeeper服务就停止了，所以，不能关闭这个终端窗口</span></span></p> 
<p style="margin-left:.0001pt;text-align:justify;"><strong><span style="background-color:#ffffff;"><span style="color:#4d4d4d;"><strong>再打开第二个（不能关闭）</strong></span></span></strong></p> 
<pre><code class="language-bash">​cd /usr/local/kafka

bin/kafka-server-start.sh config/server.properties

</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#fefefe;"><span style="color:#333333;">再打开第三个终端，然后输入下面命令创建一个自定义名称为“wordsender”的Topic：</span></span></p> 
<pre><code class="language-bash">​cd /usr/local/kafka

./bin/kafka-topics.sh --create --zookeeper localhost:2181 \

&gt; --replication-factor 1 --partitions 1 \

&gt; --topic wordsender
</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#fefefe;"><span style="color:#333333;">查看名称为“wordsender”的Topic是否已经成功创建：</span></span></p> 
<pre><code class="language-bash">./bin/kafka-topics.sh --list --zookeeper localhost:2181

​</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#fefefe;"><span style="color:#333333;">再新开一个终端（记作“监控输入终端”），执行如下命令监控Kafka收到的文本(可以尝试输入一下，再另一个窗口可以收到)：</span></span></p> 
<pre><code class="language-Scala">cd /usr/local/kafka

./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic wordsendertest --from-beginning</code></pre> 
<pre><code class="language-Scala">cd /usr/local/spark

./bin/spark-shell</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#fefefe;"><span style="color:#333333;">注意，所有这些终端窗口都不要关闭，要继续留着后面使用。</span></span></p> 
<p style="text-align:justify;"><strong><strong><strong>5 </strong></strong><span style="background-color:#fefefe;"><span style="color:#333333;"><strong>Spark添加相关jar包</strong></span></span></strong></p> 
<p style="text-align:justify;"><span style="background-color:#fefefe;"><span style="color:#333333;">下载</span></span><span style="background-color:#fefefe;"><span style="color:#333333;">spark-streaming-kafka-0-10_2.12-3.</span></span><span style="background-color:#fefefe;"><span style="color:#333333;">4</span></span><span style="background-color:#fefefe;"><span style="color:#333333;">.0.jar和spark-token-provider-kafka-0-10_2.12-3.</span></span><span style="background-color:#fefefe;"><span style="color:#333333;">4</span></span><span style="background-color:#fefefe;"><span style="color:#333333;">.0.jar文件，其中，2.12表示Scala的版本号，3.</span></span><span style="background-color:#fefefe;"><span style="color:#333333;">4</span></span><span style="background-color:#fefefe;"><span style="color:#333333;">.0表示Spark版本号。然后，把这两个文件复制到Spark目录的jars目录下（即“/usr/local/spark/jars”目录）。此外，还需要把“/usr/local/kafka/libs”目录下的kafka-clients-2.6.0.jar文件复制到Spark目录的jars目录下。（链接在上面）</span></span></p> 
<pre><code class="language-Scala">import org.apache.spark.streaming.kafka010._</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#fefefe;"><span style="color:#333333;">（在kafka0-10版本之后，接口发生了变化，在原有基础上加入010</span></span></p> 
<p style="margin-left:.0001pt;text-align:justify;">参考文献：<a class="has-card" href="https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html" rel="nofollow" title="Spark Streaming + Kafka Integration Guide (Kafka broker version 0.10.0 or higher) - Spark 3.5.1 Documentation (apache.org)"><span class="link-card-box"><span class="link-title">Spark Streaming + Kafka Integration Guide (Kafka broker version 0.10.0 or higher) - Spark 3.5.1 Documentation (apache.org)</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/87/65/cB8akgcF_o.png" alt="icon-default.png?t=N7T8">https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html</span></span></a><span style="background-color:#fefefe;"><span style="color:#333333;">）</span></span></p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" src="https://images2.imgbox.com/0b/88/dHKoUjyG_o.png"></p> 
<h6 style="margin-left:0pt;text-align:left;"><span style="background-color:#ffffff;"><strong><strong><span style="background-color:#ffffff;"><span style="color:#4f4f4f;"><strong>编写Spark程序使用Kafka数据源</strong></span></span></strong><strong><span style="background-color:#ffffff;"><span style="color:#4f4f4f;"><strong>（</strong></span></span></strong><strong><span style="background-color:#ffffff;"><span style="color:#4f4f4f;"><strong>又一个新终端</strong></span></span></strong><strong><span style="background-color:#ffffff;"><span style="color:#4f4f4f;"><strong>）</strong></span></span></strong></strong></span></h6> 
<pre><code class="language-bash">​
cd  /usr/local/spark/mycode

mkdir  kafka

cd  kafka

mkdir  -p  src/main/scala

cd  src/main/scala

​</code></pre> 
<p style="margin-left:.0001pt;text-align:center;"><img alt="" src="https://images2.imgbox.com/ac/e8/pSq3ItHY_o.png"></p> 
<pre><code class="language-bash">import java.util.HashMap
import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}
import org.apache.spark.SparkConf
import org.apache.spark.streaming._
import org.apache.spark.streaming.kafka010._
object KafkaWordProducer {
  def main(args: Array[String]) {
    if (args.length &lt; 4) {
      System.err.println("Usage: KafkaWordProducer &lt;metadataBrokerList&gt; &lt;topic&gt; " +
        "&lt;messagesPerSec&gt; &lt;wordsPerMessage&gt;")
      System.exit(1)
    }
    val Array(brokers, topic, messagesPerSec, wordsPerMessage) = args
    // Zookeeper connection properties
    val props = new HashMap[String, Object]()
    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers)
    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
      "org.apache.kafka.common.serialization.StringSerializer")
    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
      "org.apache.kafka.common.serialization.StringSerializer")
    val producer = new KafkaProducer[String, String](props)
   // Send some messages
    while(true) {
      (1 to messagesPerSec.toInt).foreach { messageNum =&gt;
        val str = (1 to wordsPerMessage.toInt).map(x =&gt; scala.util.Random.nextInt(10).
toString)
          .mkString(" ")
                    print(str)
                    println()
        val message = new ProducerRecord[String, String](topic, null, str)
        producer.send(message)
      }
     Thread.sleep(1000)
    }
  }
}</code></pre> 
<pre><code class="language-bash">vim KafkaWordCount.scala</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#4d4d4d;">KafkaWordCount.scala是用于单词词频统计，它会把KafkaWordProducer发送过来的单词进行词频统计，代码内容如下：</span></span></p> 
<pre><code class="language-bash">import org.apache.spark._
import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming._
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.streaming.kafka010.KafkaUtils
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe

object KafkaWordCount{
  def main(args:Array[String]){
    val sparkConf = new SparkConf().setAppName("KafkaWordCount").setMaster("local[2]")
    val sc = new SparkContext(sparkConf)
    sc.setLogLevel("ERROR")
    val ssc = new StreamingContext(sc,Seconds(10))
    ssc.checkpoint("file:///usr/local/spark/mycode/kafka/checkpoint") //设置检查点，如果存放在HDFS上
面，则写成类似ssc.checkpoint("/user/hadoop/checkpoint")这种形式，但是，要启动Hadoop
    val kafkaParams = Map[String, Object](
      "bootstrap.servers" -&gt; "localhost:9092",
      "key.deserializer" -&gt; classOf[StringDeserializer],
      "value.deserializer" -&gt; classOf[StringDeserializer],
      "group.id" -&gt; "use_a_separate_group_id_for_each_stream",
      "auto.offset.reset" -&gt; "latest",
      "enable.auto.commit" -&gt; (true: java.lang.Boolean)
    )
    val topics = Array("wordsender")
    val stream = KafkaUtils.createDirectStream[String, String](
      ssc,
      PreferConsistent,
      Subscribe[String, String](topics, kafkaParams)
    )
    stream.foreachRDD(rdd =&gt; {
      val offsetRange = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
      val maped: RDD[(String, String)] = rdd.map(record =&gt; (record.key,record.value))
      val lines = maped.map(_._2)
      val words = lines.flatMap(_.split(" "))
      val pair = words.map(x =&gt; (x,1))
      val wordCounts = pair.reduceByKey(_+_)
      wordCounts.foreach(println)
    })
    ssc.start
    ssc.awaitTermination
  }
}</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#ffffff;"><span style="color:#4d4d4d;">下面是StreamingExamples.scala的代码，用于设置log4j:</span></span></p> 
<pre><code class="language-bash">vim StreamingExamples.scala</code></pre> 
<pre><code class="language-bash">package org.apache.spark.examples.streaming
import org.apache.spark.internal.Logging
import org.apache.log4j.{Level, Logger}
object StreamingExamples extends Logging {
  def setStreamingLogLevels(): Unit = {
    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements
    if (!log4jInitialized) {
      logInfo("Setting log level to [WARN] for streaming example. " +
              "To override add a custom log4j.properties to the classpath.")
      Logger.getRootLogger.setLevel(Level.WARN)
    }
  }
}</code></pre> 
<p>三个文件已经创建好了</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/30/8c/QYQBSDla_o.png"></p> 
<pre><code class="language-bash">cd /usr/local/spark/mycode/kafka/
vim simple.sbt</code></pre> 
<pre><code class="language-bash">name := "Simple Project"
version := "1.0"
scalaVersion := "2.12.17"
libraryDependencies += "org.apache.spark" %% "spark-core" % "3.4.0"
libraryDependencies += "org.apache.spark" %% "spark-streaming" % "3.4.0" % "provided"
libraryDependencies += "org.apache.spark" %% "spark-streaming-kafka-0-10" % "3.4.0"
libraryDependencies += "org.apache.kafka" % "kafka-clients" % "2.6.0"</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#fefefe;"><span style="color:#333333;">然后执行下面命令，进行编译打包：</span></span></p> 
<pre><code class="language-bash">cd  /usr/local/spark/mycode/kafka/
/usr/local/sbt/sbt  package</code></pre> 
<p></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/27/2c/qBpYJ6uw_o.png"></p> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#fefefe;"><span style="color:#333333;">首先，启动Hadoop，因为如果前面KafkaWordCount.scala代码文件中采用了ssc.checkpoint</span></span><br><span style="background-color:#fefefe;"><span style="color:#333333;">("/user/hadoop/checkpoint")这种形式，这时的检查点是被写入HDFS，因此需要启动Hadoop。启动Hadoop的命令如下：</span></span></p> 
<pre><code class="language-bash">cd  /usr/local/hadoop
./sbin/start-dfs.sh</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#fefefe;"><span style="color:#333333;">要注意，之前已经启动了Zookeeper服务和Kafka服务，因为之前那些终端窗口都没有关闭，所以，这些服务一直都在运行。如果不小心关闭了之前的终端窗口，那就参照前面的内容，再次启动Zookeeper服务，启动Kafka服务。</span></span><br><span style="background-color:#fefefe;"><span style="color:#333333;">然后，新打开一个终端，执行如下命令，运行“KafkaWordProducer”程序，生成一些单词（是一堆整数形式的单词）：</span></span></p> 
<pre><code class="language-bash">cd  /usr/local/spark/mycode/kafka/
/usr/local/spark/bin/spark-submit  \
&gt; --class "KafkaWordProducer"   \
&gt; ./target/scala-2.12/simple-project_2.12-1.0.jar  \
&gt; localhost:9092  wordsender  3  5</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#fefefe;"><span style="color:#333333;">注意，上面命令中，“localhost:9092 wordsender 3 5”是提供给KafkaWordProducer程序的4个输入参数，第1个参数“localhost:9092”是Kafka的Broker的地址，第2个参数“wordsender”是Topic的名称，我们在KafkaWordCount.scala代码中已经把Topic名称写死掉，所以，KafkaWordCount程序只能接收名称为“wordsender”的Topic。第3个参数“3”表示每秒发送3条消息，第4个参数“5”表示每条消息包含5个单词（实际上就是5个整数）。</span></span></p> 
<p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#fefefe;"><span style="color:#333333;"><img alt="" src="https://images2.imgbox.com/4c/62/68bxfSJd_o.png"></span></span></p> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#fefefe;"><span style="color:#333333;">不要关闭这个终端窗口，让它一直不断发送单词。然后，</span></span><strong><span style="background-color:#fefefe;"><span style="color:#333333;"><strong>再打开一个终端</strong></span></span></strong><span style="background-color:#fefefe;"><span style="color:#333333;">，执行下面命令，运行KafkaWordCount程序，执行词频统计：</span></span></p> 
<pre><code class="language-bash">cd  /usr/local/spark/mycode/kafka/
/usr/local/spark/bin/spark-submit  \
&gt; --class "KafkaWordCount"  \
&gt; ./target/scala-2.12/simple-project_2.12-1.0.jar</code></pre> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#fefefe;"><span style="color:#333333;">屏幕上就会显示如下类似信息</span></span></p> 
<p style="margin-left:.0001pt;text-align:center;"><span style="background-color:#fefefe;"><span style="color:#333333;"><img alt="" src="https://images2.imgbox.com/77/fc/5tbN4PxK_o.png"></span></span></p> 
<h3 style="text-align:justify;"><strong><strong><strong>5 实验总结</strong></strong></strong></h3> 
<ol><li style="text-align:justify;"><span style="color:#24292f;">确保在 Spark 环境中配置了正确的 Kafka 相关依赖，包括 Kafka 客户端和 Spark Streaming 对 Kafka 的集成依赖。确保 Kafka 集群正常运行，并且你有权限访问 Kafka 集群。</span></li><li style="text-align:justify;"><span style="color:#24292f;">在 Spark Streaming 应用中配置 Kafka 数据源，包括指定 Kafka 主题、Kafka 集群地址、消费者组等信息。</span></li></ol> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="color:#24292f;">编写 Spark Streaming 应用程序，包括创建 SparkContext 和 StreamingContext，以及定义数据</span><span style="color:#24292f;">处</span><span style="color:#24292f;">理逻辑。</span></p> 
<p style="margin-left:0;text-align:left;"><span style="color:#24292f;">3.</span><span style="color:#24292f;">总的来说，运行 Spark Streaming 与 Kafka 数据源的实验需要综合考虑环境配置、数据处理逻辑、调试与监控等方面，并不断优化和完善应用程序，以满足实际业务需求并保证系统的稳定性和可靠性。</span></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7862eb789bab45b4a179a7b63c27b1a2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">mac rime鼠须管配置,自用输入法皮肤,相关问题解决</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8afa8cad9cbaf3c2334b2aae7d98a638/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python中合并列表(list)的六种方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>