<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Hadoop数仓中常用端口详解:(第36天) - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/dd220b54af82543c7dee7e5a7606eec4/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Hadoop数仓中常用端口详解:(第36天)">
  <meta property="og:description" content="前言 在数仓（数据仓库）开发中，不同的组件和服务会使用不同的端口号进行通信。由于数仓的实现可能依赖于多种技术和框架（如Hadoop、Hive、HBase、Spark等），因此涉及的端口号也会有所不同。以下是一些数仓开发中常用端口号及其作用的概述，以及相关的操作指令建议。
常用端口号及其作用
1. Hadoop相关 端口号 组件/服务 作用
8020 NameNode RPC NameNode的远程过程调用（RPC）端口，
用于处理客户端和DataNode的请求。9000 NameNode HTTP（非高可用） NameNode的HTTP服务端口，
用于Web界面访问（如HDFS的Web UI）。注意，在高可用配置中，此端口可能不常用。50070 NameNode WebHDFS NameNode的WebHDFS端口，
通过HTTP协议提供对HDFS的访问。8088 YARN ResourceManager YARN集群的ResourceManager的Web UI端口，
用于监控和管理集群中的资源和任务。50010 DataNode 数据传输端口 用于DataNode与NameNode之间、DataNode与客户端之间的数据读写操作。 2. HDFS相关 端口号 组件/服务 作用50010 DataNode 数据传输端口 用于HDFS内部的数据传输，例如DataNode与NameNode之间、DataNode与客户端之间的数据读写操作。50020 DataNode IPC端口 DataNode的IPC（Inter-Process Communication）端口，用于接受来自NameNode或其他DataNode的命令和状态同步请求。注意，在某些Hadoop版本中，这个端口可能与其他功能合并或更改。50075 DataNode Web UI端口 DataNode的Web界面端口，通常用于查看DataNode的状态和报告。然而，需要注意的是，这个端口并不总是默认开启的，且其Web界面可能不如NameNode的Web界面那样详细。
在MapReduce（MR）环境中，特别是与Hadoop生态系统结合时，有几个常用的端口号。这些端口号主要与YARN（Yet Another Resource Negotiator）框架和MapReduce作业的执行情况相关。以下是一些常用的端口号及其作用： 3. YARN相关 8032：YARN ResourceManager的IPC（Inter-Process Communication）端口。ResourceManager是YARN的核心组件，负责集群资源的分配和管理。这个端口用于ResourceManager与其他YARN组件（如NodeManager）之间的内部通信。
8088：YARN ResourceManager的Web UI端口。通过这个端口，用户可以访问ResourceManager的Web界面，查看集群的状态、运行的应用程序以及相关的统计信息等。这是查看MapReduce作业执行情况的主要端口之一。
4. MapReduce相关 虽然MapReduce作业的执行情况主要通过YARN ResourceManager的Web UI（即8088端口）来查看，但MapReduce本身并不直接提供特定的Web UI端口。然而，MapReduce作业的日志和输出通常会存储在HDFS（Hadoop Distributed File System）上，因此与HDFS相关的端口（如50070或9870，取决于Hadoop的版本）也可能用于间接地查看MapReduce作业的输出和日志。
HDFS相关端口（间接相关）
50070（Hadoop 2.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-15T07:15:00+08:00">
    <meta property="article:modified_time" content="2024-07-15T07:15:00+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Hadoop数仓中常用端口详解:(第36天)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>前言</h2> 
<p>在数仓（数据仓库）开发中，不同的组件和服务会使用不同的端口号进行通信。由于数仓的实现可能依赖于多种技术和框架（如Hadoop、Hive、HBase、Spark等），因此涉及的端口号也会有所不同。以下是一些数仓开发中常用端口号及其作用的概述，以及相关的操作指令建议。</p> 
<p>常用端口号及其作用</p> 
<h2><a id="1_Hadoop_4"></a>1. Hadoop相关</h2> 
<ul><li>端口号 组件/服务 作用<br> 8020 NameNode RPC NameNode的远程过程调用（RPC）端口，<br> 用于处理客户端和DataNode的请求。</li><li>9000 NameNode HTTP（非高可用） NameNode的HTTP服务端口，<br> 用于Web界面访问（如HDFS的Web UI）。注意，在高可用配置中，此端口可能不常用。</li><li>50070 NameNode WebHDFS NameNode的WebHDFS端口，<br> 通过HTTP协议提供对HDFS的访问。</li><li>8088 YARN ResourceManager YARN集群的ResourceManager的Web UI端口，<br> 用于监控和管理集群中的资源和任务。</li><li>50010 DataNode 数据传输端口 <br> 用于DataNode与NameNode之间、DataNode与客户端之间的数据读写操作。</li></ul> 
<h2><a id="2_HDFS_17"></a>2. HDFS相关</h2> 
<ul><li>端口号 组件/服务 作用</li><li>50010 DataNode 数据传输端口 <br> 用于HDFS内部的数据传输，例如DataNode与NameNode之间、DataNode与客户端之间的数据读写操作。</li><li>50020 DataNode IPC端口 <br> DataNode的IPC（Inter-Process Communication）端口，用于接受来自NameNode或其他DataNode的命令和状态同步请求。注意，在某些Hadoop版本中，这个端口可能与其他功能合并或更改。</li><li>50075 DataNode Web UI端口 <br> DataNode的Web界面端口，通常用于查看DataNode的状态和报告。然而，需要注意的是，这个端口并不总是默认开启的，且其Web界面可能不如NameNode的Web界面那样详细。<br> 在MapReduce（MR）环境中，特别是与Hadoop生态系统结合时，有几个常用的端口号。这些端口号主要与YARN（Yet Another Resource Negotiator）框架和MapReduce作业的执行情况相关。以下是一些常用的端口号及其作用：</li></ul> 
<h2><a id="3_YARN_27"></a>3. YARN相关</h2> 
<p>8032：YARN ResourceManager的IPC（Inter-Process Communication）端口。ResourceManager是YARN的核心组件，负责集群资源的分配和管理。这个端口用于ResourceManager与其他YARN组件（如NodeManager）之间的内部通信。<br> 8088：YARN ResourceManager的Web UI端口。通过这个端口，用户可以访问ResourceManager的Web界面，查看集群的状态、运行的应用程序以及相关的统计信息等。这是查看MapReduce作业执行情况的主要端口之一。</p> 
<h2><a id="4_MapReduce_30"></a>4. MapReduce相关</h2> 
<p>虽然MapReduce作业的执行情况主要通过YARN ResourceManager的Web UI（即8088端口）来查看，但MapReduce本身并不直接提供特定的Web UI端口。然而，MapReduce作业的日志和输出通常会存储在HDFS（Hadoop Distributed File System）上，因此与HDFS相关的端口（如50070或9870，取决于Hadoop的版本）也可能用于间接地查看MapReduce作业的输出和日志。</p> 
<ol start="3"><li>HDFS相关端口（间接相关）<br> 50070（Hadoop 2.x）或9870（Hadoop 3.x）：HDFS NameNode的Web UI端口。虽然这个端口主要用于HDFS的管理和监控，但用户可以通过它查看HDFS上存储的文件和目录，包括MapReduce作业的输出目录。</li></ol> 
<h2><a id="5_Hive_35"></a>5. Hive相关</h2> 
<p>端口号 组件/服务 作用<br> 9083 Hive Metastore Hive Metastore服务的默认监听端口，用于存储和管理Hive的元数据。<br> 10000 Hive JDBC Hive JDBC服务的端口，允许通过JDBC协议连接到Hive服务。</p> 
<h2><a id="6_HBase_40"></a>6. HBase相关</h2> 
<p>端口号 组件/服务 作用<br> 60010（旧）、16010（新） HBase Master Web UI HBase Master的Web UI端口，用于监控和管理HBase集群的状态。<br> 60030 HBase RegionServer Web UI HBase RegionServer的Web UI管理端口，提供RegionServer的状态和性能信息。</p> 
<h2><a id="7_Spark_45"></a>7. Spark相关</h2> 
<p>端口号 组件/服务 作用<br> 7077 Spark Master Spark集群中Master节点与Worker节点进行通信的端口。<br> 8080 Spark Master Web UI Spark Master的Web UI端口，用于监控和管理Spark作业和集群状态。<br> 8081 Spark Worker Web UI Spark Worker的Web UI端口，提供Worker节点的状态和性能信息。<br> 4040 Spark Driver Web UI Spark应用程序的Driver节点的Web UI端口，用于查看应用程序的运行状态和日志。</p> 
<h2><a id="8__52"></a>8. 其他</h2> 
<p>端口号 组件/服务 作用<br> 2181 Zookeeper Zookeeper客户端连接的端口，用于集群管理和协调。<br> 9092 Kafka Kafka集群节点之间通信的RPC端口。</p> 
<h2><a id="9__57"></a>9. 操作指令</h2> 
<p>对于数仓中的操作指令，通常会依赖于具体的组件和服务。以下是一些通用的操作指令示例：</p> 
<ol><li>启动HDFS服务：<br> bash<br> sbin/start-dfs.sh</li><li>停止HDFS服务：<br> bash<br> sbin/stop-dfs.sh</li><li>启动YARN服务：<br> bash<br> sbin/start-yarn.sh</li><li>停止YARN服务：<br> bash<br> sbin/stop-yarn.sh</li><li>查看HDFS状态：<br> bash<br> hdfs dfsadmin -report</li><li>启动Hive Metastore服务（具体命令可能因安装方式和版本而异）：<br> bash</li></ol> 
<h2><a id="Hive_77"></a>示例命令，具体取决于Hive的安装和配置</h2> 
<p>hive --service metastore<br> 访问Hive JDBC（通常通过客户端或应用程序进行，而非直接通过命令行）：<br> 配置JDBC连接字符串，包括主机名、端口号（10000）和数据库名（对于Hive，通常是默认数据库或指定数据库）。<br> 请注意，上述操作指令和端口号可能因Hadoop、Hive、HBase、Spark等组件的版本和配置而有所不同。在实际操作中，建议参考各组件的官方文档和配置文件（如hdfs-site.xml、hive-site.xml、hbase-site.xml、spark-defaults.conf等）以获取准确的信息。</p> 
<p>此外，由于数仓技术的不断发展和更新，新的端口号和操作指令可能会被引入，而旧的则可能会被弃用。</p> 
<h2><a id="10_spark_87"></a>10. 启动spark的指令</h2> 
<p>启动Spark的指令可以根据不同的使用场景和需求有所不同。以下是一些常见的启动Spark的指令及其说明：</p> 
<h3><a id="1_Spark_Shell_89"></a>1. 启动Spark Shell</h3> 
<p>Spark Shell是Spark提供的一个交互式编程环境，支持Scala、Python和R语言。通过Spark Shell，用户可以方便地进行数据探索和交互式数据分析。</p> 
<p>Scala Shell：在Spark安装目录下的bin文件夹中，执行./spark-shell（Linux/Mac）或spark-shell.cmd（Windows）命令。<br> Python Shell：同样在bin文件夹中，执行./pyspark（Linux/Mac）或pyspark.cmd（Windows）命令。</p> 
<h3><a id="2_Spark_94"></a>2. 提交Spark应用程序</h3> 
<p>对于需要部署到集群上运行的Spark应用程序，通常使用spark-submit命令来提交。spark-submit命令的基本语法如下：</p> 
<p>bash<br> spark-submit [options] &lt;app-jar|python-file&gt; [app-arguments]<br> 其中，[options]是Spark应用程序的选项参数，用于配置应用程序的运行环境；&lt;app-jar|python-file&gt;是要运行的Spark应用程序的Jar包或Python文件；[app-arguments]是传递给应用程序的参数。</p> 
<p>常用的选项参数包括：</p> 
<p>–master：指定Spark应用程序运行的资源管理器，如local（本地模式）、spark://host:port（Standalone模式）、yarn（YARN模式）等。<br> –deploy-mode：指定Spark应用程序的部署模式，如client或cluster。<br> –class：指定要运行的主类（仅当提交Jar包时使用）。<br> –executor-memory：指定每个Executor的内存大小。<br> –num-executors：指定Executor的数量。<br> –driver-memory：指定Driver的内存大小。<br> –conf：指定一些Spark配置属性。</p> 
<h3><a id="3_Spark_110"></a>3. 启动Spark集群</h3> 
<p>如果你是在集群环境下运行Spark，并且希望手动启动Spark的各个组件，可以使用以下命令：</p> 
<ul><li>启动所有组件（Master和Worker节点）：在Spark安装目录下的sbin文件夹中，执行./start-all.sh（Linux/Mac）或start-all.cmd（Windows）命令。</li><li>仅启动Master节点：执行./start-master.sh（Linux/Mac）或start-master.cmd（Windows）命令。</li><li>仅启动Worker节点：执行./start-worker.sh （Linux/Mac）或start-worker.cmd （Windows）命令，其中是Master节点的URL，如spark://localhost:7077。<br> 注意事项<br> 在执行上述命令之前，请确保已经正确安装了Spark，并且设置了相应的环境变量（如SPARK_HOME和PATH）。<br> 根据你的Spark版本和操作系统，命令的具体名称和路径可能有所不同。请参考你的Spark安装文档或官方指南以获取准确的命令和路径信息。<br> 如果你是在使用云服务或大数据平台（如AWS EMR、阿里云EMR等），启动Spark的指令可能会有所不同，具体请参考相应平台的文档或指南。</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4ff4e9aea327e247b1cefa8a70b88850/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【数据结构】栈和队列的深度探索，从实现到应用详解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e5e436da5a122150debfca8e55f9ea02/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python酷库之旅-第三方库Pandas(023)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>