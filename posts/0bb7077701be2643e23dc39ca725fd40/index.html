<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AI多模态实战教程：面壁智能MiniCPM-V多模态大模型问答交互、llama.cpp模型量化和推理 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/0bb7077701be2643e23dc39ca725fd40/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="AI多模态实战教程：面壁智能MiniCPM-V多模态大模型问答交互、llama.cpp模型量化和推理">
  <meta property="og:description" content="一、项目简介 MiniCPM-V 系列是专为视觉-语⾔理解设计的多模态⼤型语⾔模型（MLLMs），提供⾼质量的⽂本输出，已发布4个版本。
1.1 主要模型及特性
（1）MiniCPM-Llama3-V 2.5：
参数规模: 8B性能: 超越GPT-4V-1106、Gemini Pro、Qwen-VL-Max和Claude 3，⽀持30&#43;种语⾔，多模态对话，增强OCR和指令跟随能⼒。部署: 量化、编译优化，可⾼效部署于端侧设备上的CPU和NPU。 （2）MiniCPM-V 2.0
参数规模: 2B性能: 超越Yi-VL 34B、CogVLM-Chat 17B和Qwen-VL-Chat 10B，可处理任意纵横⽐和180万像素图像（例如，1344x1344），低幻觉率。 1.2 MiniCPM-Llama3-V 2.5 关键特性
领先的性能平均得分65.1（OpenCompass），超越多款专有模型。强⼤的OCR能⼒处理任意纵横⽐和180万像素图像，OCRBench评分700&#43;，提供全⽂OCR提取和表格到Markdown转换等⾼级实⽤功能。值得信赖的⾏为采⽤RLAIF-V⽅法，幻觉率10.3%，优于GPT-4V-1106。多语⾔⽀持⽀持30&#43;种语⾔（含德语、法语、⻄班⽛语、意⼤利语、韩语等）。⾼效部署模型量化、CPU/NPU优化，实现端侧设备上的150倍图像编码加速和3倍语⾔解码加速。易⽤性⽀持llama.cpp、ollama，GGUF格式量化模型，LoRA微调，流输出，本地WebUI演示和HuggingFaceSpaces交互演示。 1.3 MiniCPM-V 2.0 关键特性
MiniCPM-V 2.0，这是MiniCPM系列的多模态版本。该模型基于MiniCPM 2.4B和SigLip-400M构建，总共有2.8B参数。MiniCPM-V 2.0显示出强⼤的OCR和多模态理解能⼒，在开源模型中的OCRBench上表现出⾊，甚⾄在场景⽂本理解上可以与Gemini Pro相媲美。
前沿性能在多个基准测试中表现优异（如 OCRBench、TextVQA 等）。超越 Qwen-VL-Chat 9.6B、CogVLM-Chat 17.4B 和 Yi-VL 34B。强⼤的 OCR 能⼒，与 Gemini Pro 性能相当。可信⾏为使⽤多模态 RLHF 技术防⽌⽣成不符合事实的⽂本。与 GPT-4V 在防⽌幻觉⽅⾯匹配。⾼分辨率图像处理接受 180万像素（例如，1344x1344）的图像，⽀持任意⻓宽⽐。提升对细粒度视觉信息的感知能⼒。⾼效能⾼效部署于⼤多数 GPU 和个⼈电脑，⽀持移动设备。使⽤感知器重采样技术，降低内存成本并提升速度。双语⽀持⽀持英语和中⽂的双语多模态能⼒。基于 VisCPM 技术，实现跨语⾔的⼀般化多模态能⼒。 ⼆、案例实战 2.1 环境配置
conda create -n cpm python=3.11 conda activate cpm # 下载项⽬，并进⾏依赖包安装 git clone https://github.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-23T11:24:47+08:00">
    <meta property="article:modified_time" content="2024-07-23T11:24:47+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AI多模态实战教程：面壁智能MiniCPM-V多模态大模型问答交互、llama.cpp模型量化和推理</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p></p> 
<p class="img-center"><img alt="" height="163" src="https://images2.imgbox.com/7d/e7/5KZx5Afr_o.png" width="932"></p> 
<h3><strong>一、项目简介</strong></h3> 
<p>MiniCPM-V 系列是专为视觉-语⾔理解设计的多模态⼤型语⾔模型（MLLMs），提供⾼质量的⽂本输出，已发布4个版本。</p> 
<p><strong>1.1 主要模型及特性</strong></p> 
<p><strong>（1）MiniCPM-Llama3-V 2.5：</strong></p> 
<ul><li><strong>参数规模</strong>: 8B</li><li><strong>性能</strong>: 超越GPT-4V-1106、Gemini Pro、Qwen-VL-Max和Claude 3，⽀持30+种语⾔，多模态对话，增强OCR</li><li>和指令跟随能⼒。</li><li><strong>部署</strong>: 量化、编译优化，可⾼效部署于端侧设备上的CPU和NPU。</li></ul> 
<p><strong>（2）MiniCPM-V 2.0</strong></p> 
<ul><li><strong>参数规模</strong>: 2B</li><li><strong>性能</strong>: 超越Yi-VL 34B、CogVLM-Chat 17B和Qwen-VL-Chat 10B，可处理任意纵横⽐和180万像素图像（例</li><li>如，1344x1344），低幻觉率。</li></ul> 
<p class="img-center"><img alt="" height="674" src="https://images2.imgbox.com/ce/8f/ggQusFSl_o.png" width="957"></p> 
<p><strong>1.2 MiniCPM-Llama3-V 2.5 关键特性</strong></p> 
<ul><li><strong>领先的性能</strong></li><li>平均得分65.1（OpenCompass），超越多款专有模型。</li><li><strong>强⼤的OCR能⼒</strong></li><li>处理任意纵横⽐和180万像素图像，OCRBench评分700+，提供全⽂OCR提取和表格到Markdown转换</li><li>等⾼级实⽤功能。</li><li><strong>值得信赖的⾏为</strong></li><li>采⽤RLAIF-V⽅法，幻觉率10.3%，优于GPT-4V-1106。</li><li><strong>多语⾔⽀持</strong></li><li>⽀持30+种语⾔（含德语、法语、⻄班⽛语、意⼤利语、韩语等）。</li><li><strong>⾼效部署</strong></li><li>模型量化、CPU/NPU优化，实现端侧设备上的150倍图像编码加速和3倍语⾔解码加速。</li><li><strong>易⽤性</strong></li><li>⽀持llama.cpp、ollama，GGUF格式量化模型，LoRA微调，流输出，本地WebUI演示和HuggingFace</li><li>Spaces交互演示。 <p class="img-center"><img alt="" height="1200" src="https://images2.imgbox.com/4b/b7/GITtfIez_o.png" width="1200"></p> </li></ul> 
<p><strong>1.3 MiniCPM-V 2.0 关键特性</strong></p> 
<p>MiniCPM-V 2.0，这是MiniCPM系列的多模态版本。该模型基于MiniCPM 2.4B和SigLip-400M构建，总共有2.8B参数。MiniCPM-V 2.0显示出强⼤的OCR和多模态理解能⼒，在开源模型中的OCRBench上表现出⾊，甚⾄在场景⽂本理解上可以与Gemini Pro相媲美。</p> 
<ul><li><strong>前沿性能</strong></li><li>在多个基准测试中表现优异（如 OCRBench、TextVQA 等）。</li><li>超越 Qwen-VL-Chat 9.6B、CogVLM-Chat 17.4B 和 Yi-VL 34B。</li><li>强⼤的 OCR 能⼒，与 Gemini Pro 性能相当。</li><li><strong>可信⾏为</strong></li><li>使⽤多模态 RLHF 技术防⽌⽣成不符合事实的⽂本。</li><li>与 GPT-4V 在防⽌幻觉⽅⾯匹配。</li><li><strong>⾼分辨率图像处理</strong></li><li>接受 180万像素（例如，1344x1344）的图像，⽀持任意⻓宽⽐。</li><li>提升对细粒度视觉信息的感知能⼒。</li><li><strong>⾼效能</strong></li><li>⾼效部署于⼤多数 GPU 和个⼈电脑，⽀持移动设备。</li><li>使⽤感知器重采样技术，降低内存成本并提升速度。</li><li><strong>双语⽀持</strong></li><li>⽀持英语和中⽂的双语多模态能⼒。</li><li>基于 VisCPM 技术，实现跨语⾔的⼀般化多模态能⼒。</li></ul> 
<h3><strong>⼆、案例实战</strong></h3> 
<p><strong>2.1 环境配置</strong></p> 
<pre><code class="language-bash">conda create -n cpm python=3.11
conda activate cpm

# 下载项⽬，并进⾏依赖包安装
git clone https://github.com/OpenBMB/MiniCPM-V.git
cd MiniCPM-V

pip install -r requirements.txt
# 单独安装
pip install bitsandbytes streamlit gguf</code></pre> 
<p><strong>2. 模型下载</strong></p> 
<pre><code class="language-bash"># 前提，安装git和git-lfs【可选，如果已安装，则跳过】
sudo apt update
sudo apt install git
curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo
bash
sudo apt-get install git-lfs
git lfs install
# 下载模型，以int4量化的MiniCPM-Llama3-V-2_5为例
git clone https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-int4</code></pre> 
<p><strong>[无法访问外网的同学，可以把上面最后一行改为国内镜像地址：</strong></p> 
<p><strong>git clone https://hf-mirror.com/openbmb/MiniCPM-Llama3-V-2_5-int4]</strong></p> 
<p class="img-center"><img alt="" height="287" src="https://images2.imgbox.com/fc/0b/LIgZRXEt_o.png" width="1028"></p> 
<h3 style="background-color:transparent;"><strong>三. 本地 WebUI Demo</strong></h3> 
<p><strong>3.1 基于 Gradio 实现</strong></p> 
<pre><code class="language-python"># 注意：需要修改脚本 web_demo_2.5.py 中的代码：
# ① model_path = xxx
# ② server_port = xxx
cd MiniCPM-V/
python web_demo_2.5.py</code></pre> 
<p><strong>3.2 基于 Streamlit 实现</strong></p> 
<pre><code class="language-python"># 注意：需要修改脚本 web_demo_streamlit-2_5.py 中的代码：
# ① model_path = xxx
# ② model = AutoModel.from_pretrained(model_path, trust_remote_code=True, 
torch_dtype=torch.float16, device_map="cuda")
streamlit run web_demo_streamlit-2_5.py --server.port 6006 --server.address 0.0.0.0</code></pre> 
<h3><strong>四. 多轮对话</strong></h3> 
<pre><code class="language-python"># 注意：需要修改 chat.py 中的代码：
self.model = AutoModel.from_pretrained(model_path, trust_remote_code=True, 
device_map="cuda")
self.model.eval()</code></pre> 
<p>新建demo.py</p> 
<pre><code class="language-python"># 案例-多轮对话
from chat import MiniCPMVChat, img2base64
import torch
import json
torch.manual_seed(0)
chat_model = MiniCPMVChat("/root/autodl-tmp/models/MiniCPM-Llama3-V-2_5-int4")
im_64 = img2base64('./assets/airplane.jpeg')
# 第⼀轮对话
msgs = [{"role": "user",
"content": "Tell me the model of this aircraft."}]
inputs = {"image": im_64,
"question": json.dumps(msgs)}
answer = chat_model.chat(inputs)
print(answer)
# 第⼆轮对话
# 传递多轮对话的历史上下⽂
msgs.append({"role": "assistant",
"content": answer})
msgs.append({"role": "user",
"content": "Introduce something about Airbus A380."})

inputs = {"image": im_64,
"question": json.dumps(msgs)}
answer = chat_model.chat(inputs)
print(answer)</code></pre> 
<h3><strong>五. 基于 llama.cpp 推理</strong></h3> 
<p><strong>5.1 环境配置</strong></p> 
<pre><code class="language-python"># 1. 下载项⽬
git clone -b minicpm-v2.5 https://github.com/OpenBMB/llama.cpp.git
cd llama.cpp
# 2. 安装 g++ (可选，如果已经安装，则跳过)
sudo apt update
sudo apt install g++
# 3. 在项⽬ llama.cpp/ ⽬录下，执⾏命令
make
make minicpmv-cli</code></pre> 
<p><strong>5.2 模型量化</strong></p> 
<pre><code class="language-python"># 4. 模型格式转换，hf -&gt; gguf
# 【可选操作】可以直接 下载gguf模型
python ./examples/minicpmv/minicpmv-surgery.py -m /root/autodl-tmp/models/MiniCPM-Llama3-
V-2_5</code></pre> 
<pre><code class="language-python">python ./examples/minicpmv/minicpmv-convert-image-encoder-to-gguf.py -m /root/autodltmp/models/MiniCPM-Llama3-V-2_5 --minicpmv-projector /root/autodl-tmp/models/MiniCPMLlama3-V-2_5/minicpmv.projector --output-dir /root/autodl-tmp/models/MiniCPM-Llama3-V-2_5/ 
--image-mean 0.5 0.5 0.5 --image-std 0.5 0.5 0.5</code></pre> 
<pre><code class="language-python">python ./convert.py /root/autodl-tmp/models/MiniCPM-Llama3-V-2_5/model --outtype f16 --
vocab-type bpe</code></pre> 
<pre><code class="language-python"># 5. quantize int4 version
./quantize /root/autodl-tmp/models/MiniCPM-Llama3-V-2_5/model/model-8B-F16.gguf 
/root/autodl-tmp/models/MiniCPM-Llama3-V-2_5/model/ggml-model-Q4_K_M.gguf Q4_K_M</code></pre> 
<p><strong>5.3 模型推理</strong></p> 
<pre><code class="language-python"># 6. 基于量化版模型进⾏推理
# run f16 version
./minicpmv-cli -m /root/autodl-tmp/models/MiniCPM-Llama3-V-2_5/model/model-8B-F16.gguf --
mmproj /root/autodl-tmp/models/MiniCPM-Llama3-V-2_5/mmproj-model-f16.gguf -c 4096 --temp
0.7 --top-p 0.8 --top-k 100 --repeat-penalty 1.05 --image /root/autodl-tmp/MiniCPMV/assets/airplane.jpeg -p "What is in the image?"</code></pre> 
<pre><code class="language-python"># run quantized int4 version(4bit量化推理)
./minicpmv-cli -m /root/autodl-tmp/models/MiniCPM-Llama3-V-2_5/model/ggml-modelQ4_K_M.gguf --mmproj /root/autodl-tmp/models/MiniCPM-Llama3-V-2_5/mmproj-model-f16.gguf -c
4096 --temp 0.7 --top-p 0.8 --top-k 100 --repeat-penalty 1.05 --image /root/autodltmp/MiniCPM-V/assets/airplane.jpeg -p "What is in the image?"</code></pre> 
<pre><code class="language-python"># or run in interactive mode（交互模式）
./minicpmv-cli -m /root/autodl-tmp/models/MiniCPM-Llama3-V-2_5/model/ggml-modelQ4_K_M.gguf --mmproj /root/autodl-tmp/models/MiniCPM-Llama3-V-2_5/mmproj-model-f16.gguf -c
4096 --temp 0.7 --top-p 0.8 --top-k 100 --repeat-penalty 1.05 --image /root/autodltmp/MiniCPM-V/assets/airplane.jpeg -i</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/82879c87f40e4a030a6f5d71007bec38/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">hadoop分布式云笔记系统-计算机毕业设计源码15725</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c6649bfa2584cf5eab7c3197dce9445a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">《程序猿入职必会（2） · 搭建具备前端展示效果的 Vue》</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>