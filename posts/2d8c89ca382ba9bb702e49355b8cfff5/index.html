<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>240705_昇思学习打卡-Day17-基于 MindSpore 实现 BERT 对话情绪识别 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/2d8c89ca382ba9bb702e49355b8cfff5/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="240705_昇思学习打卡-Day17-基于 MindSpore 实现 BERT 对话情绪识别">
  <meta property="og:description" content="240705_昇思学习打卡-Day17-基于 MindSpore 实现 BERT对话情绪识别 近期确实太忙，此处仅作简单记录：
模型简介 BERT全称是来自变换器的双向编码器表征量（Bidirectional Encoder Representations from Transformers），它是Google于2018年末开发并发布的一种新型语言模型。与BERT模型相似的预训练语言模型例如问答、命名实体识别、自然语言推理、文本分类等在许多自然语言处理任务中发挥着重要作用。模型是基于Transformer中的Encoder并加上双向的结构，因此一定要熟练掌握Transformer的Encoder的结构。
关于Transformer的Encoder的结构在这篇中有提及，可以去参考看看240701_昇思学习打卡-Day13-Vision Transformer图像分类-CSDN博客
BERT模型的主要创新点都在pre-train方法上，即用了Masked Language Model和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。
在用Masked Language Model方法训练BERT的时候，随机把语料库中15%的单词做Mask操作。对于这15%的单词做Mask操作分为三种情况：80%的单词直接用[Mask]替换、10%的单词直接替换成另一个新的单词、10%的单词保持不变。
因为涉及到Question Answering (QA) 和 Natural Language Inference (NLI)之类的任务，增加了Next Sentence Prediction预训练任务，目的是让模型理解两个句子之间的联系。与Masked Language Model任务相比，Next Sentence Prediction更简单些，训练的输入是句子A和B，B有一半的几率是A的下一句，输入这两个句子，BERT模型预测B是不是A的下一句。
BERT预训练之后，会保存它的Embedding table和12层Transformer权重（BERT-BASE）或24层Transformer权重（BERT-LARGE）。使用预训练好的BERT模型可以对下游任务进行Fine-tuning，比如：文本分类、相似度判断、阅读理解等。
对话情绪识别（Emotion Detection，简称EmoTect），专注于识别智能对话场景中用户的情绪，针对智能对话场景中的用户文本，自动判断该文本的情绪类别并给出相应的置信度，情绪类型分为积极、消极、中性。 对话情绪识别适用于聊天、客服等多个场景，能够帮助企业更好地把握对话质量、改善产品的用户交互体验，也能分析客服服务质量、降低人工质检成本。
下面以一个文本情感分类任务为例子来说明BERT模型的整个应用过程。
我们假设已经装好了MindSpore环境
# 该案例在 mindnlp 0.3.1 版本完成适配，如果发现案例跑不通，可以指定mindnlp版本，执行`!pip install mindnlp==0.3.1` !pip install mindnlp import os import mindspore from mindspore.dataset import text, GeneratorDataset, transforms from mindspore import nn, context from mindnlp._legacy.engine import Trainer, Evaluator from mindnlp.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-06T00:06:29+08:00">
    <meta property="article:modified_time" content="2024-07-06T00:06:29+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">240705_昇思学习打卡-Day17-基于 MindSpore 实现 BERT 对话情绪识别</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="240705_Day17_MindSpore__BERT_0"></a>240705_昇思学习打卡-Day17-基于 MindSpore 实现 BERT对话情绪识别</h2> 
<p>近期确实太忙，此处仅作简单记录：</p> 
<h3><a id="_4"></a>模型简介</h3> 
<p>BERT全称是来自变换器的双向编码器表征量（Bidirectional Encoder Representations from Transformers），它是Google于2018年末开发并发布的一种新型语言模型。与BERT模型相似的预训练语言模型例如问答、命名实体识别、自然语言推理、文本分类等在许多自然语言处理任务中发挥着重要作用。模型是基于Transformer中的Encoder并加上双向的结构，因此一定要熟练掌握Transformer的Encoder的结构。</p> 
<p><img src="https://images2.imgbox.com/d1/69/rNiVJHAa_o.png" alt="image-20240705234457785"></p> 
<p>关于Transformer的Encoder的结构在这篇中有提及，可以去参考看看<a href="https://blog.csdn.net/weixin_66378701/article/details/140112731?spm=1001.2014.3001.5502">240701_昇思学习打卡-Day13-Vision Transformer图像分类-CSDN博客</a></p> 
<p>BERT模型的主要创新点都在pre-train方法上，即用了Masked Language Model和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。</p> 
<p>在用Masked Language Model方法训练BERT的时候，随机把语料库中15%的单词做Mask操作。对于这15%的单词做Mask操作分为三种情况：80%的单词直接用[Mask]替换、10%的单词直接替换成另一个新的单词、10%的单词保持不变。</p> 
<p>因为涉及到Question Answering (QA) 和 Natural Language Inference (NLI)之类的任务，增加了Next Sentence Prediction预训练任务，目的是让模型理解两个句子之间的联系。与Masked Language Model任务相比，Next Sentence Prediction更简单些，训练的输入是句子A和B，B有一半的几率是A的下一句，输入这两个句子，BERT模型预测B是不是A的下一句。</p> 
<p>BERT预训练之后，会保存它的Embedding table和12层Transformer权重（BERT-BASE）或24层Transformer权重（BERT-LARGE）。使用预训练好的BERT模型可以对下游任务进行Fine-tuning，比如：文本分类、相似度判断、阅读理解等。</p> 
<p>对话情绪识别（Emotion Detection，简称EmoTect），专注于识别智能对话场景中用户的情绪，针对智能对话场景中的用户文本，自动判断该文本的情绪类别并给出相应的置信度，情绪类型分为积极、消极、中性。 对话情绪识别适用于聊天、客服等多个场景，能够帮助企业更好地把握对话质量、改善产品的用户交互体验，也能分析客服服务质量、降低人工质检成本。</p> 
<p>下面以一个文本情感分类任务为例子来说明BERT模型的整个应用过程。</p> 
<p>我们假设已经装好了MindSpore环境</p> 
<pre><code class="prism language-python"><span class="token comment"># 该案例在 mindnlp 0.3.1 版本完成适配，如果发现案例跑不通，可以指定mindnlp版本，执行`!pip install mindnlp==0.3.1`</span>
!pip install mindnlp
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">import</span> os

<span class="token keyword">import</span> mindspore
<span class="token keyword">from</span> mindspore<span class="token punctuation">.</span>dataset <span class="token keyword">import</span> text<span class="token punctuation">,</span> GeneratorDataset<span class="token punctuation">,</span> transforms
<span class="token keyword">from</span> mindspore <span class="token keyword">import</span> nn<span class="token punctuation">,</span> context

<span class="token keyword">from</span> mindnlp<span class="token punctuation">.</span>_legacy<span class="token punctuation">.</span>engine <span class="token keyword">import</span> Trainer<span class="token punctuation">,</span> Evaluator
<span class="token keyword">from</span> mindnlp<span class="token punctuation">.</span>_legacy<span class="token punctuation">.</span>engine<span class="token punctuation">.</span>callbacks <span class="token keyword">import</span> CheckpointCallback<span class="token punctuation">,</span> BestModelCallback
<span class="token keyword">from</span> mindnlp<span class="token punctuation">.</span>_legacy<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> Accuracy
</code></pre> 
<pre><code class="prism language-python"><span class="token comment"># prepare dataset</span>
<span class="token keyword">class</span> <span class="token class-name">SentimentDataset</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Sentiment Dataset"""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> path<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>path <span class="token operator">=</span> path
        self<span class="token punctuation">.</span>_labels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_text_a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>_load<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_load</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>path<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
            dataset <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>
        lines <span class="token operator">=</span> dataset<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> line <span class="token keyword">in</span> lines<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
            label<span class="token punctuation">,</span> text_a <span class="token operator">=</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"\t"</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>_labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>label<span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>_text_a<span class="token punctuation">.</span>append<span class="token punctuation">(</span>text_a<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>_labels<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>_text_a<span class="token punctuation">[</span>index<span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_labels<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token comment"># 准备数据集</span>
<span class="token keyword">class</span> 情感分析数据集<span class="token punctuation">(</span>SentimentDataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    情感分析数据集类，用于加载和管理数据集。
    
    参数:
        path (str): 数据集文件的路径。
    
    属性:
        _labels (list): 存储情感标签的列表。
        _text_a (list): 存储文本内容的列表。
        
    方法:
        _load(): 从指定路径加载数据集文件，解析内容并存储到_labels和_text_a中。
        __getitem__(index): 根据索引返回特定样本的标签和文本。
        __len__(): 返回数据集的样本数量。
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> path<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        初始化情感分析数据集对象，设置数据路径并加载数据。
        
        参数:
            path (str): 数据集文件的路径。
        """</span>
        self<span class="token punctuation">.</span>path <span class="token operator">=</span> path
        self<span class="token punctuation">.</span>_labels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_text_a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>_load<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_load</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        私有方法：读取数据集文件，按行处理数据，分割标签和文本，并存储到实例变量中。
        """</span>
        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>path<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
            dataset <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>
        lines <span class="token operator">=</span> dataset<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> line <span class="token keyword">in</span> lines<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>  <span class="token comment"># 跳过首行（假设为列名）和末尾的空行</span>
            label<span class="token punctuation">,</span> text_a <span class="token operator">=</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"\t"</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>_labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>label<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 添加标签到_labels列表</span>
            self<span class="token punctuation">.</span>_text_a<span class="token punctuation">.</span>append<span class="token punctuation">(</span>text_a<span class="token punctuation">)</span>  <span class="token comment"># 添加文本到_text_a列表</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        通过索引获取数据集中对应样本的标签和文本。
        
        参数:
            index (int): 数据样本的索引位置。
            
        返回:
            tuple: 包含样本标签和文本的元组 (label, text)。
        """</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>_labels<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>_text_a<span class="token punctuation">[</span>index<span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        返回数据集中的样本数量。
        
        返回:
            int: 数据集样本数量。
        """</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_labels<span class="token punctuation">)</span>

</code></pre> 
<h3><a id="_134"></a>数据集</h3> 
<p>这里提供一份已标注的、经过分词预处理的机器人聊天数据集，来自于百度飞桨团队。数据由两列组成，以制表符（‘\t’）分隔，第一列是情绪分类的类别（0表示消极；1表示中性；2表示积极），第二列是以空格分词的中文文本，如下示例，文件为 utf8 编码。</p> 
<p>label–text_a</p> 
<p>0–谁骂人了？我从来不骂人，我骂的都不是人，你是人吗 ？</p> 
<p>1–我有事等会儿就回来和你聊</p> 
<p>2–我见到你很高兴谢谢你帮我</p> 
<p>这部分主要包括数据集读取，数据格式转换，数据 Tokenize 处理和 pad 操作。</p> 
<pre><code class="prism language-python"><span class="token comment"># download dataset</span>
!wget https<span class="token punctuation">:</span><span class="token operator">//</span>baidu<span class="token operator">-</span>nlp<span class="token punctuation">.</span>bj<span class="token punctuation">.</span>bcebos<span class="token punctuation">.</span>com<span class="token operator">/</span>emotion_detection<span class="token operator">-</span>dataset<span class="token operator">-</span><span class="token number">1.0</span><span class="token number">.0</span><span class="token punctuation">.</span>tar<span class="token punctuation">.</span>gz <span class="token operator">-</span>O emotion_detection<span class="token punctuation">.</span>tar<span class="token punctuation">.</span>gz
!tar xvf emotion_detection<span class="token punctuation">.</span>tar<span class="token punctuation">.</span>gz
</code></pre> 
<h4><a id="_154"></a>数据加载和数据预处理</h4> 
<p>新建 process_dataset 函数用于数据加载和数据预处理，具体内容可见下面代码注释。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">def</span> <span class="token function">process_dataset</span><span class="token punctuation">(</span>source<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> max_seq_len<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    处理数据集，将其转换为适合模型训练的格式。
    
    参数:
    source: 数据集的来源，可以是文件路径或数据生成器。
    tokenizer: 用于将文本序列化为模型输入的标记化器。
    max_seq_len: 最大序列长度，超过这个长度的序列将被截断。
    batch_size: 每个批次的样本数量。
    shuffle: 是否在处理数据集前打乱数据顺序。
    
    返回:
    经过处理后的数据集，包括输入序列和标签。
    """</span>
    <span class="token comment"># 判断是否在昇腾设备上运行</span>
    is_ascend <span class="token operator">=</span> mindspore<span class="token punctuation">.</span>get_context<span class="token punctuation">(</span><span class="token string">'device_target'</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">'Ascend'</span>

    <span class="token comment"># 定义数据集的列名</span>
    column_names <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">,</span> <span class="token string">"text_a"</span><span class="token punctuation">]</span>
    
    <span class="token comment"># 创建数据集对象</span>
    dataset <span class="token operator">=</span> GeneratorDataset<span class="token punctuation">(</span>source<span class="token punctuation">,</span> column_names<span class="token operator">=</span>column_names<span class="token punctuation">,</span> shuffle<span class="token operator">=</span>shuffle<span class="token punctuation">)</span>
    <span class="token comment"># 将字符串类型转换为整型</span>
    type_cast_op <span class="token operator">=</span> transforms<span class="token punctuation">.</span>TypeCast<span class="token punctuation">(</span>mindspore<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
    
    <span class="token comment"># 定义文本标记化和填充函数</span>
    <span class="token keyword">def</span> <span class="token function">tokenize_and_pad</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        对文本进行标记化和填充，以适应模型的要求。
        
        参数:
        text: 需要处理的文本。
        
        返回:
        标记化和填充后的输入序列和注意力掩码。
        """</span>
        <span class="token keyword">if</span> is_ascend<span class="token punctuation">:</span>
            <span class="token comment"># 在昇腾设备上，使用特定的处理方式</span>
            tokenized <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>text<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'max_length'</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span>max_seq_len<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 在其他设备上，直接进行标记化</span>
            tokenized <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
        <span class="token keyword">return</span> tokenized<span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> tokenized<span class="token punctuation">[</span><span class="token string">'attention_mask'</span><span class="token punctuation">]</span>
    
    <span class="token comment"># 对文本列进行标记化和填充处理</span>
    dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>operations<span class="token operator">=</span>tokenize_and_pad<span class="token punctuation">,</span> input_columns<span class="token operator">=</span><span class="token string">"text_a"</span><span class="token punctuation">,</span> output_columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">,</span> <span class="token string">'attention_mask'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># 对标签列进行类型转换</span>
    dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>operations<span class="token operator">=</span><span class="token punctuation">[</span>type_cast_op<span class="token punctuation">]</span><span class="token punctuation">,</span> input_columns<span class="token operator">=</span><span class="token string">"label"</span><span class="token punctuation">,</span> output_columns<span class="token operator">=</span><span class="token string">'labels'</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 根据设备类型选择合适的批次处理方式</span>
    <span class="token keyword">if</span> is_ascend<span class="token punctuation">:</span>
        <span class="token comment"># 在昇腾设备上，使用简单的批次处理</span>
        dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>batch<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token comment"># 在其他设备上，使用带填充的批次处理</span>
        dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>padded_batch<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> pad_info<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">'input_ids'</span><span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> tokenizer<span class="token punctuation">.</span>pad_token_id<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                         <span class="token string">'attention_mask'</span><span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> dataset

</code></pre> 
<p>数据预处理部分采用静态Shape处理：</p> 
<pre><code class="prism language-python"><span class="token comment"># 导入BertTokenizer类，用于BERT模型的预训练 tokenizer</span>
<span class="token keyword">from</span> mindnlp<span class="token punctuation">.</span>transformers <span class="token keyword">import</span> BertTokenizer

<span class="token comment"># 初始化一个BertTokenizer实例，用于处理中文文本</span>
<span class="token comment"># 这里使用了预训练的'bert-base-chinese'模型，该模型已经在中文文本上进行了预训练</span>
<span class="token comment"># 选择这个预训练模型是因为我们的任务是处理中文文本，需要一个针对中文优化的tokenizer</span>
tokenizer <span class="token operator">=</span> BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-chinese'</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">tokenizer<span class="token punctuation">.</span>pad_token_id
</code></pre> 
<pre><code class="prism language-python">dataset_train <span class="token operator">=</span> process_dataset<span class="token punctuation">(</span>SentimentDataset<span class="token punctuation">(</span><span class="token string">"data/train.tsv"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tokenizer<span class="token punctuation">)</span>
dataset_val <span class="token operator">=</span> process_dataset<span class="token punctuation">(</span>SentimentDataset<span class="token punctuation">(</span><span class="token string">"data/dev.tsv"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tokenizer<span class="token punctuation">)</span>
dataset_test <span class="token operator">=</span> process_dataset<span class="token punctuation">(</span>SentimentDataset<span class="token punctuation">(</span><span class="token string">"data/test.tsv"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">dataset_train<span class="token punctuation">.</span>get_col_names<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">next</span><span class="token punctuation">(</span>dataset_train<span class="token punctuation">.</span>create_tuple_iterator<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/4d/c2/7nbG6oID_o.png" alt="image-20240706000224197"></p> 
<h3><a id="_255"></a>模型构建</h3> 
<p>通过 BertForSequenceClassification 构建用于情感分类的 BERT 模型，加载预训练权重，设置情感三分类的超参数自动构建模型。后面对模型采用自动混合精度操作，提高训练的速度，然后实例化优化器，紧接着实例化评价指标，设置模型训练的权重保存策略，最后就是构建训练器，模型开始训练。</p> 
<pre><code class="prism language-python"><span class="token comment"># 导入MindNLP库中用于序列分类任务的BertForSequenceClassification模型与用于获取文本编码表示的BertModel</span>
<span class="token keyword">from</span> mindnlp<span class="token punctuation">.</span>transformers <span class="token keyword">import</span> BertForSequenceClassification<span class="token punctuation">,</span> BertModel
<span class="token comment"># 导入auto_mixed_precision函数以启用混合精度训练，能够加速训练过程并减少内存占用</span>
<span class="token keyword">from</span> mindnlp<span class="token punctuation">.</span>_legacy<span class="token punctuation">.</span>amp <span class="token keyword">import</span> auto_mixed_precision

<span class="token comment"># 根据预训练的'bert-base-chinese'模型初始化BertForSequenceClassification模型，设置类别数为3</span>
<span class="token comment"># 此模型适用于如文本分类任务，将输入文本归类到三个预定义类别中的一个</span>
<span class="token comment"># 设置BERT模型配置及训练所需参数</span>
model <span class="token operator">=</span> BertForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-chinese'</span><span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
<span class="token comment"># 使用auto_mixed_precision函数对模型应用混合精度训练策略，采用'O1'优化级别</span>
<span class="token comment"># 混合精度训练通过结合使用float16和float32数据类型来提升训练速度并节省内存资源</span>
model <span class="token operator">=</span> auto_mixed_precision<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token string">'O1'</span><span class="token punctuation">)</span>

<span class="token comment"># 定义模型训练使用的优化器为Adam算法，设置学习率为2e-5，并仅针对模型中可训练参数进行优化</span>
optimizer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>trainable_params<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">2e-5</span><span class="token punctuation">)</span>

</code></pre> 
<pre><code class="prism language-python"><span class="token comment"># 初始化Accuracy类，用于计算模型预测的准确率</span>
metric <span class="token operator">=</span> Accuracy<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 定义回调函数以保存训练过程中的检查点</span>
<span class="token comment"># CheckpointCallback用于在指定的epoch后保存模型，保存路径为'checkpoint'，检查点文件名为'bert_emotect'</span>
<span class="token comment"># 参数epochs设为1表示每个epoch后保存一次，keep_checkpoint_max=2表示最多保留2个检查点文件</span>
ckpoint_cb <span class="token operator">=</span> CheckpointCallback<span class="token punctuation">(</span>save_path<span class="token operator">=</span><span class="token string">'checkpoint'</span><span class="token punctuation">,</span> ckpt_name<span class="token operator">=</span><span class="token string">'bert_emotect'</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keep_checkpoint_max<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

<span class="token comment"># BestModelCallback用于自动保存验证性能最优的模型，同样保存在'checkpoint'路径下，文件名为'bert_emotect_best'</span>
<span class="token comment"># 设置auto_load=True可在训练结束后自动加载该最优模型</span>
best_model_cb <span class="token operator">=</span> BestModelCallback<span class="token punctuation">(</span>save_path<span class="token operator">=</span><span class="token string">'checkpoint'</span><span class="token punctuation">,</span> ckpt_name<span class="token operator">=</span><span class="token string">'bert_emotect_best'</span><span class="token punctuation">,</span> auto_load<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 创建Trainer实例以组织训练流程</span>
<span class="token comment"># network参数指定训练的模型，train_dataset和eval_dataset分别指定了训练集和验证集</span>
<span class="token comment"># metrics参数指定了评估模型性能的指标，此处为刚刚定义的准确率Accuracy</span>
<span class="token comment"># epochs设置训练轮次为5，optimizer为训练使用的优化器，callbacks列表包含了之前定义的保存检查点和最佳模型的回调函数</span>
trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>network<span class="token operator">=</span>model<span class="token punctuation">,</span> train_dataset<span class="token operator">=</span>dataset_train<span class="token punctuation">,</span>
                  eval_dataset<span class="token operator">=</span>dataset_val<span class="token punctuation">,</span> metrics<span class="token operator">=</span>metric<span class="token punctuation">,</span>
                  epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span>optimizer<span class="token punctuation">,</span> callbacks<span class="token operator">=</span><span class="token punctuation">[</span>ckpoint_cb<span class="token punctuation">,</span> best_model_cb<span class="token punctuation">]</span><span class="token punctuation">)</span>

</code></pre> 
<pre><code class="prism language-python"><span class="token operator">%</span><span class="token operator">%</span>time
<span class="token comment"># start training</span>
trainer<span class="token punctuation">.</span>run<span class="token punctuation">(</span>tgt_columns<span class="token operator">=</span><span class="token string">"labels"</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_307"></a>模型验证</h3> 
<p>将验证数据集加再进训练好的模型，对数据集进行验证，查看模型在验证数据上面的效果，此处的评价指标为准确率。</p> 
<pre><code class="prism language-python"><span class="token comment"># 初始化Evaluator对象，用于评估模型性能</span>
<span class="token comment"># 参数说明：</span>
<span class="token comment"># network: 待评估的模型</span>
<span class="token comment"># eval_dataset: 用于评估的测试数据集</span>
<span class="token comment"># metrics: 评估指标</span>
evaluator <span class="token operator">=</span> Evaluator<span class="token punctuation">(</span>network<span class="token operator">=</span>model<span class="token punctuation">,</span> eval_dataset<span class="token operator">=</span>dataset_test<span class="token punctuation">,</span> metrics<span class="token operator">=</span>metric<span class="token punctuation">)</span>

<span class="token comment"># 执行模型评估，指定目标列作为评估标签</span>
<span class="token comment"># 该步骤将计算模型在测试数据集上的指定评估指标</span>
evaluator<span class="token punctuation">.</span>run<span class="token punctuation">(</span>tgt_columns<span class="token operator">=</span><span class="token string">"labels"</span><span class="token punctuation">)</span>

</code></pre> 
<pre><code class="prism language-python">dataset_infer <span class="token operator">=</span> SentimentDataset<span class="token punctuation">(</span><span class="token string">"data/infer.tsv"</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>text<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    根据给定的文本进行情感分析预测。

    参数:
    text (str): 需要进行情感分析的文本。
    label (int, optional): 用于比较的预定义标签。如果提供，将打印预测标签和给定标签的比较。

    返回:
    无返回值，但打印了模型预测的情感标签以及输入文本。
    """</span>
    <span class="token comment"># 映射预测结果的标签到人类可读的情感描述</span>
    label_map <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token number">0</span><span class="token punctuation">:</span> <span class="token string">"消极"</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span> <span class="token string">"中性"</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span> <span class="token string">"积极"</span><span class="token punctuation">}</span>
    
    <span class="token comment"># 将文本转换为模型输入所需的格式</span>
    text_tokenized <span class="token operator">=</span> Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>tokenizer<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">.</span>input_ids<span class="token punctuation">]</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 使用模型预测文本的情感</span>
    logits <span class="token operator">=</span> model<span class="token punctuation">(</span>text_tokenized<span class="token punctuation">)</span>
    predict_label <span class="token operator">=</span> logits<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>asnumpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 构建包含预测信息的字符串</span>
    info <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"inputs: '</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>text<span class="token punctuation">}</span></span><span class="token string">', predict: '</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>label_map<span class="token punctuation">[</span>predict_label<span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">'"</span></span>
    <span class="token keyword">if</span> label <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token comment"># 如果提供了标签，则添加实际标签的信息</span>
        info <span class="token operator">+=</span> <span class="token string-interpolation"><span class="token string">f" , label: '</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>label_map<span class="token punctuation">[</span>label<span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">'"</span></span>
    
    <span class="token comment"># 打印预测结果</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>info<span class="token punctuation">)</span>

</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">from</span> mindspore <span class="token keyword">import</span> Tensor

<span class="token keyword">for</span> label<span class="token punctuation">,</span> text <span class="token keyword">in</span> dataset_infer<span class="token punctuation">:</span>
    predict<span class="token punctuation">(</span>text<span class="token punctuation">,</span> label<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/80/5d/4ZCsE0HS_o.png" alt="image-20240706000417506"></p> 
<h3><a id="_371"></a>自定义推理数据集</h3> 
<p>自己输入一句话，进行测试</p> 
<pre><code class="prism language-python">predict<span class="token punctuation">(</span><span class="token string">"家人们咱就是说一整个无语住了 绝绝子叠buff"</span><span class="token punctuation">)</span>
</code></pre> 
<p>打卡图片：</p> 
<p><img src="https://images2.imgbox.com/9b/57/PIkmrNcU_o.png" alt="image-20240705235200648"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ab0fb76b23a2c31e77c188195ad37dc7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【C/排序算法】：归并排序和计数排序</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4a7b97555bfd3d94750494a22ecbacf0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Mojolicious测试驱动开发：单元与集成测试的艺术</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>