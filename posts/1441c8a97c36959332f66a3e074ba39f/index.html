<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【Llama3:8b】手把手教你如何在本地部署 自己的 AI 大模型 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/1441c8a97c36959332f66a3e074ba39f/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【Llama3:8b】手把手教你如何在本地部署 自己的 AI 大模型">
  <meta property="og:description" content="一、为什么需要本地部署属于自己的大模型？ 趋势：我们正处于AI人工智能时代，各行各业的公司和产品都在向AI靠拢。打造垂直领域的AI模型将成为每个公司未来的发展趋势。数据安全：在无法掌握核心算法的情况下，许多公司选择使用大公司的成熟方案。然而，这涉及到数据安全的问题。训练垂直定制化的大模型需要大量数据，而数据是公司的核心资产和基石。没有公司愿意将这些关键数据上传到外部服务器，这是公司的命脉所在。本地部署的重要性：因此，本地部署和训练自有或定制化的大模型显得尤为重要。这不仅是未来的发展趋势，也是程序员必须掌握的关键流程和解决方案。 二、如何拥有属于自己的本地大模型呢？ 经过博主深入评估和多次测试，为您推荐以下解决方案：
使用 Llama3:8b 作为模型通过 Ollama 部署利用 Llama.cpp 进行量化优化采用 Unsloth 进行模型训练和微调 三、为什么要选择这个方案？ 1、Llama3:8b 低算力需求和成本：Llama3:8b 对计算资源的需求较小，运营成本低。卓越的上下文记忆能力：模型能够有效记住上下文，满足业务需求。灵活的微调能力：适应不同的业务场景和需求。开源：社区支持强大，开发者资源丰富。 2、Ollama 活跃的生态和开发者社区：提供丰富的资源和支持。高效的部署和运行：确保模型的稳定性和性能。灵活的 API 接口：满足业务对训练模型的多样化需求。开源且易于使用：降低了上手难度，适合快速开发和迭代。 3、Llama.cpp 广泛支持 Llama3:8b 的量化工具：目前市面上对 Llama3:8b 支持最好的量化工具之一。丰富的教程资源：学习和使用成本低。开源：开发者可以自由使用和修改，社区贡献积极。 4、Unsloth 多样化的数据集支持：能够处理不同类型的数据，适应性强。优异的性能：训练效果显著，能够提高模型的精度。本地训练支持：对数据隐私有更好的保护。开源：开发者可以自由访问和修改，提升了灵活性和可控性。 四、话不多说，让我们开始吧！ 1、安装 Ubuntu 【NVIDIA GPU驱动安装】 为什么需要使用 GPU 去跑我们的大模型呢？
训练时间：使用GPU可以显著缩短模型的训练时间。例如，一个复杂的深度学习模型在GPU上可能只需要几小时，而在CPU上可能需要几天甚至几周。模型推理：在推理阶段，尤其是需要处理大量实时数据时，GPU的高并行处理能力可以提供更快的响应时间和更高的吞吐量。 虽然CPU也可以用于运行大模型，但在处理深度学习任务时，GPU的并行计算能力、计算性能和专用硬件支持使其更为适合。GPU能够显著提升大模型的训练和推理效率，降低时间和功耗成本。
一、官方方案
参考文档：
CUDA Toolkit 12.4 Update 1 Downloads | NVIDIA Developer
二、方案二（经实践，简单 好用）
1、安装 CUDA:
官网脚本： developer.nvidia.com/cuda-toolki…
shell 复制代码 # 脚本文件命令（根据上述网站） wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu2204-12-1-local_12.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-17T10:25:42+08:00">
    <meta property="article:modified_time" content="2024-05-17T10:25:42+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【Llama3:8b】手把手教你如何在本地部署 自己的 AI 大模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h4><a id="_0"></a>一、为什么需要本地部署属于自己的大模型？</h4> 
<ol><li><strong>趋势</strong>：我们正处于AI人工智能时代，各行各业的公司和产品都在向AI靠拢。打造垂直领域的AI模型将成为每个公司未来的发展趋势。</li><li><strong>数据安全</strong>：在无法掌握核心算法的情况下，许多公司选择使用大公司的成熟方案。然而，这涉及到数据安全的问题。训练垂直定制化的大模型需要大量数据，而数据是公司的核心资产和基石。没有公司愿意将这些关键数据上传到外部服务器，这是公司的命脉所在。</li><li><strong>本地部署的重要性</strong>：因此，本地部署和训练自有或定制化的大模型显得尤为重要。这不仅是未来的发展趋势，也是程序员必须掌握的关键流程和解决方案。</li></ol> 
<h4><a id="_6"></a>二、如何拥有属于自己的本地大模型呢？</h4> 
<p>经过博主深入评估和多次测试，为您推荐以下解决方案：</p> 
<ul><li>使用 Llama3:8b 作为模型</li><li>通过 Ollama 部署</li><li>利用 Llama.cpp 进行量化优化</li><li>采用 Unsloth 进行模型训练和微调</li></ul> 
<h4><a id="_15"></a>三、为什么要选择这个方案？</h4> 
<h5><a id="1Llama38b_17"></a><strong>1、Llama3:8b</strong></h5> 
<ul><li>低算力需求和成本：Llama3:8b 对计算资源的需求较小，运营成本低。</li><li>卓越的上下文记忆能力：模型能够有效记住上下文，满足业务需求。</li><li>灵活的微调能力：适应不同的业务场景和需求。</li><li>开源：社区支持强大，开发者资源丰富。</li></ul> 
<h5><a id="2Ollama_24"></a><strong>2、Ollama</strong></h5> 
<ul><li>活跃的生态和开发者社区：提供丰富的资源和支持。</li><li>高效的部署和运行：确保模型的稳定性和性能。</li><li>灵活的 API 接口：满足业务对训练模型的多样化需求。</li><li>开源且易于使用：降低了上手难度，适合快速开发和迭代。</li></ul> 
<h5><a id="3Llamacpp_31"></a><strong>3、Llama.cpp</strong></h5> 
<ul><li>广泛支持 Llama3:8b 的量化工具：目前市面上对 Llama3:8b 支持最好的量化工具之一。</li><li>丰富的教程资源：学习和使用成本低。</li><li>开源：开发者可以自由使用和修改，社区贡献积极。</li></ul> 
<h5><a id="4Unsloth_37"></a><strong>4、Unsloth</strong></h5> 
<ul><li>多样化的数据集支持：能够处理不同类型的数据，适应性强。</li><li>优异的性能：训练效果显著，能够提高模型的精度。</li><li>本地训练支持：对数据隐私有更好的保护。</li><li>开源：开发者可以自由访问和修改，提升了灵活性和可控性。</li></ul> 
<h4><a id="_44"></a>四、话不多说，让我们开始吧！</h4> 
<h5><a id="1_Ubuntu_NVIDIA_GPU_46"></a>1、安装 Ubuntu 【NVIDIA GPU驱动安装】</h5> 
<p>为什么需要使用 GPU 去跑我们的大模型呢？</p> 
<ul><li><strong>训练时间</strong>：使用GPU可以显著缩短模型的训练时间。例如，一个复杂的深度学习模型在GPU上可能只需要几小时，而在CPU上可能需要几天甚至几周。</li><li><strong>模型推理</strong>：在推理阶段，尤其是需要处理大量实时数据时，GPU的高并行处理能力可以提供更快的响应时间和更高的吞吐量。</li></ul> 
<p>虽然CPU也可以用于运行大模型，但在处理深度学习任务时，GPU的并行计算能力、计算性能和专用硬件支持使其更为适合。GPU能够显著提升大模型的训练和推理效率，降低时间和功耗成本。</p> 
<p>一、官方方案</p> 
<p>参考文档：</p> 
<p><a href="https://link.juejin.cn?target=https%3A%2F%2Fdeveloper.nvidia.com%2Fcuda-downloads" rel="nofollow" title="https://developer.nvidia.com/cuda-downloads">CUDA Toolkit 12.4 Update 1 Downloads | NVIDIA Developer</a></p> 
<p>二、方案二（经实践，简单 好用）</p> 
<p>1、安装 CUDA:</p> 
<p>官网脚本： <a href="https://link.juejin.cn?target=https%3A%2F%2Fdeveloper.nvidia.com%2Fcuda-toolkit-archive" rel="nofollow" title="https://developer.nvidia.com/cuda-toolkit-archive">developer.nvidia.com/cuda-toolki…</a></p> 
<pre><code>shell
复制代码
# 脚本文件命令（根据上述网站）
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin
sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb
sudo cp /var/cuda-repo-ubuntu2204-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt search cuda-toolkit
sudo apt-get -y install cuda
</code></pre> 
<p>2、安装驱动： shell</p> 
<pre><code>csharp
复制代码
# 系统会推荐安装驱动的版本区间
sudo ubuntu-drivers devices
​
# 安装合适版本驱动
sudo apt-get install -y cuda-drivers-550
​
# 验证
nvidia-smi
watch -n 5 nvidia-smi
</code></pre> 
<p><img src="https://images2.imgbox.com/48/13/Y5HdtZSv_o.png" alt="image-20240516161721445.png"></p> 
<h5><a id="2_Ollama_99"></a>2、首先我们需要安装 <strong>Ollama</strong></h5> 
<p>参考官方文档: <a href="https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Follama%2Follama" rel="nofollow" title="https://github.com/ollama/ollama">GitHub - ollama/ollama: Get up and running with Llama 3, Mistral, Gemma, and other large language models.</a></p> 
<p>方案 一：快速安装，只能安装最新版本，经检测 最新版有Bug。</p> 
<pre><code>arduino
复制代码
curl -fsSL https://ollama.com/install.sh | sh
</code></pre> 
<p>方案二：手动安装，自由灵活。</p> 
<p>1、二进制文件下载。</p> 
<pre><code>ruby
复制代码
sudo curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/bin/ollama
</code></pre> 
<p>历史版本下载：</p> 
<p><a href="https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Follama%2Follama%2Freleases" rel="nofollow" title="https://github.com/ollama/ollama/releases">Releases · ollama/ollama (github.com)</a></p> 
<p>2、给文件添加执行权限</p> 
<pre><code>bash
复制代码
sudo chmod +x /usr/bin/ollama
</code></pre> 
<p>3、创建 ollama 用户</p> 
<pre><code>bash
复制代码
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama
</code></pre> 
<p>4、创建配置文件</p> 
<pre><code>bash
复制代码
vi /etc/systemd/system/ollama.service
</code></pre> 
<p>配置文件参考</p> 
<pre><code>ini
复制代码
[Unit]
Description=Ollama Service
After=network-online.target
​
[Service]
ExecStart=/usr/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
​
# 监听端口
Environment="OLLAMA_HOST=0.0.0.0:11434"
# 模型下载位置 （需要给 ollama用户文件夹权限： sudo chown ollama:ollama ollama/）
Environment="OLLAMA_MODELS=/home/ek-p/ollama"
# 单模型并发数量
Environment="OLLAMA_NUM_PARALLEL=100"
# Gpu选择 （如有GPU，需要在硬件安装GPU去驱动，安装文档参考下文。）
Environment="CUDA_VISIBLE_DEVICES=0,1"
# 多模型并发数量
Environment="OLLAMA_MAX_LOADED_MODELS=3"
​
[Install]
WantedBy=default.target
</code></pre> 
<p>5、启动 Ollama</p> 
<pre><code>bash
复制代码
sudo systemctl daemon-reload
sudo systemctl enable ollama
</code></pre> 
<p>6、查看日志</p> 
<pre><code>复制代码
journalctl -u ollama -r
</code></pre> 
<p>7、问题：文件夹权限不够</p> 
<pre><code>ini
复制代码
May 16 16:24:25 ek-s systemd[1]: ollama.service: Failed with result 'exit-code'.
May 16 16:24:25 ek-s systemd[1]: ollama.service: Main process exited, code=exited, status=1/FAILURE
May 16 16:24:25 ek-s ollama[10780]: Error: mkdir /home/ekw-p: permission denied
</code></pre> 
<pre><code>bash
复制代码
sudo chown ollama:ollama ollama/ 
</code></pre> 
<p>8、运行成功</p> 
<p>继续查看日志</p> 
<pre><code>ini
复制代码
May 16 16:39:47 ek-s ollama[4183]: time=2024-05-16T16:39:47.384+08:00 level=INFO source=cpu_common.go:11 msg="CPU has AVX2"
May 16 16:39:47 ek-s ollama[4183]: time=2024-05-16T16:39:47.383+08:00 level=INFO source=gpu.go:127 msg="detected GPUs" count=1 library=/usr/lib/x86_64-li&gt;
May 16 16:39:47 ek-s ollama[4183]: time=2024-05-16T16:39:47.125+08:00 level=INFO source=gpu.go:122 msg="Detecting GPUs"
May 16 16:39:47 ek-s ollama[4183]: time=2024-05-16T16:39:47.125+08:00 level=INFO source=payload.go:44 msg="Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cu&gt;
May 16 16:39:44 ek-s ollama[4183]: time=2024-05-16T16:39:44.222+08:00 level=INFO source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama1963&gt;
May 16 16:39:44 ek-s ollama[4183]: time=2024-05-16T16:39:44.221+08:00 level=INFO source=routes.go:1034 msg="Listening on [::]:11434 (version 0.1.34)"
May 16 16:39:44 ek-s ollama[4183]: time=2024-05-16T16:39:44.220+08:00 level=INFO source=images.go:904 msg="total unused blobs removed: 0"
May 16 16:39:44 ek-s ollama[4183]: time=2024-05-16T16:39:44.220+08:00 level=INFO source=images.go:897 msg="total blobs: 0"
</code></pre> 
<h5><a id="3__Ollama___Llama38b_229"></a>3、 接着我们需要使用 Ollama 运行我们的 大模型 Llama3:8b</h5> 
<p>1、使用 ollama 查看模型列表</p> 
<pre><code>复制代码
ollama list
</code></pre> 
<p>2、下载大模型</p> 
<p>ollama Model 库：<a href="https://link.juejin.cn?target=https%3A%2F%2Follama.com%2Flibrary" rel="nofollow" title="https://ollama.com/library">library (ollama.com)</a></p> 
<p>经检测目前比较适用的大模型为：llama3:8b</p> 
<pre><code>复制代码
ollama pull llama3:8b
</code></pre> 
<p>3、运行大模型</p> 
<pre><code>arduino
复制代码
ollama run llama3:8b
</code></pre> 
<p><img src="https://images2.imgbox.com/7e/76/AtjOqcmJ_o.png" alt="image-20240516175118296.png"></p> 
<h5><a id="4_259"></a>4、测试我们的大模型</h5> 
<p><img src="https://images2.imgbox.com/9e/92/U3kRs1ej_o.png" alt="image-20240516175532722.png"> 成功。</p> 
<h3><a id="LLM__263"></a>如何系统的去学习大模型LLM ？</h3> 
<p>作为一名热心肠的互联网老兵，我意识到有很多经验和知识值得分享给大家，也可以通过我们的能力和经验解答大家在人工智能学习中的很多困惑，所以在工作繁忙的情况下还是坚持各种整理和分享。</p> 
<p>但苦于知识传播途径有限，很多互联网行业朋友无法获得正确的资料得到学习提升，故此将并将<strong>重要的 <code>AI大模型资料</code> 包括AI大模型入门学习思维导图、精品AI大模型学习书籍手册、视频教程、实战学习等录播视频免费分享出来</strong>。</p> 
<p>😝有需要的小伙伴，可以V扫描下方二维码免费领取🆓</p> 
<p><img src="https://images2.imgbox.com/b5/b7/e6G0btBQ_o.jpg" alt="在这里插入图片描述"></p> 
<h3><a id="AGI_272"></a>一、全套AGI大模型学习路线</h3> 
<p><strong>AI大模型时代的学习之旅：从基础到前沿，掌握人工智能的核心技能！</strong></p> 
<p><img src="https://images2.imgbox.com/2a/5a/H2v3qjME_o.png" alt="img"></p> 
<h3><a id="640AI_278"></a>二、640套AI大模型报告合集</h3> 
<p>这套包含640份报告的合集，涵盖了AI大模型的理论研究、技术实现、行业应用等多个方面。无论您是科研人员、工程师，还是对AI大模型感兴趣的爱好者，这套报告合集都将为您提供宝贵的信息和启示。</p> 
<p><img src="https://images2.imgbox.com/f5/a6/NmRykW6o_o.png" alt="img"></p> 
<h3><a id="AIPDF_284"></a>三、AI大模型经典PDF籍</h3> 
<p>随着人工智能技术的飞速发展，AI大模型已经成为了当今科技领域的一大热点。这些大型预训练模型，如GPT-3、BERT、XLNet等，以其强大的语言理解和生成能力，正在改变我们对人工智能的认识。 那以下这些PDF籍就是非常不错的学习资源。</p> 
<p><img src="https://images2.imgbox.com/3d/fd/cG9ZVxNR_o.png" alt="img"></p> 
<p><img src="https://images2.imgbox.com/9f/51/Lldv9eRq_o.jpg" alt="在这里插入图片描述"></p> 
<h4><a id="AI_291"></a>四、AI大模型商业化落地方案</h4> 
<p><img src="https://images2.imgbox.com/56/a5/hLEP7PNg_o.png" alt="img"></p> 
<h4><a id="1AI_295"></a>阶段1：AI大模型时代的基础理解</h4> 
<ul><li><strong>目标</strong>：了解AI大模型的基本概念、发展历程和核心原理。</li><li><strong>内容</strong>： 
  <ul><li>L1.1 人工智能简述与大模型起源</li><li>L1.2 大模型与通用人工智能</li><li>L1.3 GPT模型的发展历程</li><li>L1.4 模型工程<br> - L1.4.1 知识大模型<br> - L1.4.2 生产大模型<br> - L1.4.3 模型工程方法论<br> - L1.4.4 模型工程实践</li><li>L1.5 GPT应用案例</li></ul> </li></ul> 
<h4><a id="2AIAPI_307"></a>阶段2：AI大模型API应用开发工程</h4> 
<ul><li><strong>目标</strong>：掌握AI大模型API的使用和开发，以及相关的编程技能。</li><li><strong>内容</strong>： 
  <ul><li>L2.1 API接口<br> - L2.1.1 OpenAI API接口<br> - L2.1.2 Python接口接入<br> - L2.1.3 BOT工具类框架<br> - L2.1.4 代码示例</li><li>L2.2 Prompt框架<br> - L2.2.1 什么是Prompt<br> - L2.2.2 Prompt框架应用现状<br> - L2.2.3 基于GPTAS的Prompt框架<br> - L2.2.4 Prompt框架与Thought<br> - L2.2.5 Prompt框架与提示词</li><li>L2.3 流水线工程<br> - L2.3.1 流水线工程的概念<br> - L2.3.2 流水线工程的优点<br> - L2.3.3 流水线工程的应用</li><li>L2.4 总结与展望</li></ul> </li></ul> 
<h4><a id="3AI_326"></a>阶段3：AI大模型应用架构实践</h4> 
<ul><li><strong>目标</strong>：深入理解AI大模型的应用架构，并能够进行私有化部署。</li><li><strong>内容</strong>： 
  <ul><li>L3.1 Agent模型框架<br> - L3.1.1 Agent模型框架的设计理念<br> - L3.1.2 Agent模型框架的核心组件<br> - L3.1.3 Agent模型框架的实现细节</li><li>L3.2 MetaGPT<br> - L3.2.1 MetaGPT的基本概念<br> - L3.2.2 MetaGPT的工作原理<br> - L3.2.3 MetaGPT的应用场景</li><li>L3.3 ChatGLM<br> - L3.3.1 ChatGLM的特点<br> - L3.3.2 ChatGLM的开发环境<br> - L3.3.3 ChatGLM的使用示例</li><li>L3.4 LLAMA<br> - L3.4.1 LLAMA的特点<br> - L3.4.2 LLAMA的开发环境<br> - L3.4.3 LLAMA的使用示例</li><li>L3.5 其他大模型介绍</li></ul> </li></ul> 
<h4><a id="4AI_346"></a>阶段4：AI大模型私有化部署</h4> 
<ul><li><strong>目标</strong>：掌握多种AI大模型的私有化部署，包括多模态和特定领域模型。</li><li><strong>内容</strong>： 
  <ul><li>L4.1 模型私有化部署概述</li><li>L4.2 模型私有化部署的关键技术</li><li>L4.3 模型私有化部署的实施步骤</li><li>L4.4 模型私有化部署的应用场景</li></ul> </li></ul> 
<h4><a id="_353"></a>学习计划：</h4> 
<ul><li><strong>阶段1</strong>：1-2个月，建立AI大模型的基础知识体系。</li><li><strong>阶段2</strong>：2-3个月，专注于API应用开发能力的提升。</li><li><strong>阶段3</strong>：3-4个月，深入实践AI大模型的应用架构和私有化部署。</li><li><strong>阶段4</strong>：4-5个月，专注于高级模型的应用和部署。</li></ul> 
<h6><a id="_LLM_CSDNCSDN100_359"></a>这份完整版的大模型 LLM 学习资料已经上传CSDN，朋友们如果需要可以微信扫描下方CSDN官方认证二维码免费领取【<code>保证100%免费</code>】</h6> 
<p>😝有需要的小伙伴，可以Vx扫描下方二维码免费领取🆓</p> 
<p><img src="https://images2.imgbox.com/54/03/rre3K2FD_o.jpg" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4c47cb9edd1ee685569f206327e901eb/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">系列学习前端之第 11 章：将前端项目部署到本机运行起来 2 种方式（使用 Express 和 Nginx），使用花生壳做内网穿透，让外网可以访问网站</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3d0aab0b271cc989a030eac4374f1557/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Stable Diffusion详解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>