<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>在Kotlin中使用Spark SQL的UDF和UDAF函数 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/28d6ca160b4b8fa533767a42d2733044/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="在Kotlin中使用Spark SQL的UDF和UDAF函数">
  <meta property="og:description" content="1. 项目结构与依赖 1.1 项目依赖 使用gradle: 在项目的build.gradle.kts添加
dependencies { implementation(&#34;org.apache.spark:spark-sql_2.12:3.3.1&#34;) } 使用maven: 在模块的pom.xml中添加
&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt; &lt;version&gt;3.3.1&lt;/version&gt; &lt;/dependency&gt; 2. UDF的使用与实现 UDF，即用户自定义函数，允许用户在SQL查询中使用自定义的函数。下面案例做了一个简单的案例，将首字母变为大写。
2.1 数据源 准备数据源使用JSON数据作为数据格式，保存到项目的根路径下的`data/user.txt`文件。
{&#34;name&#34;:&#34;zhangsan&#34;,&#34;age&#34;:19,&#34;gender&#34;:&#34;boy&#34;} {&#34;name&#34;:&#34;lisi&#34;,&#34;age&#34;:20,&#34;gender&#34;:&#34;boy&#34;} {&#34;name&#34;:&#34;wangwu&#34;,&#34;age&#34;:21,&#34;gender&#34;:&#34;boy&#34;} {&#34;name&#34;:&#34;zhaoliu&#34;,&#34;age&#34;:22,&#34;gender&#34;:&#34;boy&#34;} {&#34;name&#34;:&#34;sunqi&#34;,&#34;age&#34;:23,&#34;gender&#34;:&#34;boy&#34;} {&#34;name&#34;:&#34;zhouba&#34;,&#34;age&#34;:24,&#34;gender&#34;:&#34;boy&#34;} {&#34;name&#34;:&#34;wujiu&#34;,&#34;age&#34;:25,&#34;gender&#34;:&#34;boy&#34;} {&#34;name&#34;:&#34;zhengshi&#34;,&#34;age&#34;:26,&#34;gender&#34;:&#34;boy&#34;} 2.2 代码示例 import org.apache.spark.sql.SparkSession import org.apache.spark.sql.api.java.UDF1 import org.apache.spark.sql.types.DataTypes import java.util.* class SparkSQL_UDF { fun f1() { val sparkSession = SparkSession.builder() .master(&#34;local&#34;) .appName(&#34;Kotlin Spark UDF&#34;) .orCreate // 读取JSON数据并创建视图 sparkSession.read().json(&#34;data/user.txt&#34;) .createOrReplaceTempView(&#34;user&#34;) // 注册UDF函数，将名字的首字母大写 sparkSession.udf().register(&#34;nameHeaderUpper&#34;, UDF1 { name: String -&gt; name.substring(0, 1).">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-25T12:23:07+08:00">
    <meta property="article:modified_time" content="2024-08-25T12:23:07+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">在Kotlin中使用Spark SQL的UDF和UDAF函数</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>1. 项目结构与依赖</h2> 
<h3>1.1 项目依赖</h3> 
<h4>使用gradle:</h4> 
<p>在项目的build.gradle.kts添加</p> 
<pre><code class="language-Kotlin">dependencies {
    implementation("org.apache.spark:spark-sql_2.12:3.3.1")
}</code></pre> 
<h4>使用maven:</h4> 
<p>在模块的pom.xml中添加</p> 
<pre><code class="language-XML">        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;
            &lt;version&gt;3.3.1&lt;/version&gt;
        &lt;/dependency&gt;</code></pre> 
<h2>2. UDF的使用与实现</h2> 
<p>UDF，即用户自定义函数，允许用户在SQL查询中使用自定义的函数。下面案例做了一个简单的案例，将首字母变为大写。</p> 
<h3>2.1 数据源</h3> 
<p>准备数据源使用JSON数据作为数据格式，保存到项目的根路径下的`data/user.txt`文件。</p> 
<pre>{"name":"zhangsan","age":19,"gender":"boy"}
{"name":"lisi","age":20,"gender":"boy"}
{"name":"wangwu","age":21,"gender":"boy"}
{"name":"zhaoliu","age":22,"gender":"boy"}
{"name":"sunqi","age":23,"gender":"boy"}
{"name":"zhouba","age":24,"gender":"boy"}
{"name":"wujiu","age":25,"gender":"boy"}
{"name":"zhengshi","age":26,"gender":"boy"}</pre> 
<h4>2.2 代码示例</h4> 
<pre><code class="language-Kotlin">import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.api.java.UDF1
import org.apache.spark.sql.types.DataTypes
import java.util.*

class SparkSQL_UDF {
    fun f1() {
        val sparkSession = SparkSession.builder()
            .master("local")
            .appName("Kotlin Spark UDF")
            .orCreate

        // 读取JSON数据并创建视图
        sparkSession.read().json("data/user.txt")
            .createOrReplaceTempView("user")
        
        // 注册UDF函数，将名字的首字母大写
        sparkSession.udf().register("nameHeaderUpper",
            UDF1 { name: String -&gt;
                name.substring(0, 1).uppercase(Locale.getDefault()) + name.substring(1)
            },
            DataTypes.StringType)

        // 使用注册的UDF函数进行SQL查询
        sparkSession.sql("select nameHeaderUpper(name) as name from user").show()

        sparkSession.stop()
    }

    companion object {
        @JvmStatic
        fun main(args: Array&lt;String&gt;) {
            SparkSQL_UDF().f1()
        }
    }
}

/*输出结果
+---------------------+
|nameHeaderUpper(name)|
+---------------------+
|             Zhangsan|
|                 Lisi|
|               Wangwu|
|              Zhaoliu|
|                Sunqi|
|               Zhouba|
|                Wujiu|
|             Zhengshi|
+---------------------+
*/</code></pre> 
<h4>2.3 代码解析</h4> 
<h5>1. 创建SparkSession：</h5> 
<p>使用local模式进行测试</p> 
<pre><code class="language-Kotlin">   val sparkSession = SparkSession.builder()
       .master("local")
       .appName("Kotlin Spark UDF")
       .orCreate</code></pre> 
<h5>2. 读取数据并创建视图：</h5> 
<p>创建名为user的视图</p> 
<pre><code class="language-Kotlin">   sparkSession.read().json("data/user.txt")
       .createOrReplaceTempView("user")</code></pre> 
<h5> 3. 注册UDF函数：</h5> 
<p><strong><span style="color:#fe2c24;">使用kotlin的lambda表达式来简化UDF函数的创建 要注意这里是Java中的UDF1{}而不是Scala中的Function1{}</span></strong></p> 
<pre><code class="language-Kotlin">   sparkSession.udf().register("nameHeaderUpper",
       UDF1 { name: String -&gt;
           name.substring(0, 1).uppercase(Locale.getDefault()) + name.substring(1)
       },
       DataTypes.StringType)</code></pre> 
<h5>4. 执行SQL查询：</h5> 
<p>用sparkSession对象调用sql方法进行查询 并将结果展示到控制台</p> 
<pre><code class="language-Kotlin">  sparkSession.sql("select nameHeaderUpper(name) as name from user").show()</code></pre> 
<h5>5. 停止SparkSession：</h5> 
<p>释放资源<br> 调用close和stop方法都可以</p> 
<pre><code class="language-Kotlin">   sparkSession.stop()</code></pre> 
<h2>3. UDAF的使用与实现</h2> 
<p>UDAF，即用户自定义聚合函数，允许用户定义复杂的聚合逻辑，如求平均值、总和等。</p> 
<p>下面是一个简单案例来实现求年龄的平均值</p> 
<h4>3.1 代码示例</h4> 
<pre><code class="language-Kotlin">import org.apache.spark.sql.Encoder
import org.apache.spark.sql.Encoders
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.expressions.Aggregator
import org.apache.spark.sql.functions
import java.io.Serializable

class SparkSQL_UDAF {
    fun f1() {
        val sparkSession = SparkSession.builder()
            .master("local")
            .appName("Kotlin Spark UDAF")
            .orCreate

        // 定义聚合器
        val agg = object : Aggregator&lt;Long, Buffer, Long&gt;(){
            override fun reduce(b: Buffer?, a: Long?): Buffer {
                val updatedBuffer = b ?: Buffer(0, 0)
                updatedBuffer.cnt++
                updatedBuffer.count += a!!
                return updatedBuffer
            }

            override fun outputEncoder(): Encoder&lt;Long&gt; {
                return Encoders.LONG()
            }

            override fun zero(): Buffer {
                return Buffer(0L, 0L)
            }

            override fun bufferEncoder(): Encoder&lt;Buffer&gt; {
                return Encoders.bean(Buffer::class.java)
            }

            override fun finish(reduction: Buffer?): Long {
                return reduction?.count?.div(reduction.cnt) ?: 0L
            }

            override fun merge(b1: Buffer?, b2: Buffer?): Buffer {
                return Buffer(
                    count = (b1?.count ?: 0) + (b2?.count ?: 0),
                    cnt = (b1?.cnt ?: 0) + (b2?.cnt ?: 0)
                )
            }
        }

        // 注册UDAF函数
        sparkSession.udf().register("avgAge", functions.udaf(agg, Encoders.LONG()))

        // 使用注册的UDAF函数进行SQL查询
        sparkSession.read().json("data/user.txt").createOrReplaceTempView("user")
        sparkSession.sql("select avgAge(age) as avg_age from user").show()

        sparkSession.stop()
    }

    companion object {
        @JvmStatic
        fun main(args: Array&lt;String&gt;) {
            SparkSQL_UDAF().f1()
        }
    }
}

data class Buffer(var count: Long, var cnt: Long) : Serializable {
    constructor() : this(0, 0)
}

/*输出结果
+-----------+
|avgage(age)|
+-----------+
|         22|
+-----------+
*/</code></pre> 
<h4 style="background-color:transparent;">3.2 代码解析</h4> 
<h5>1. Buffer数据类：</h5> 
<p><strong><span style="color:#fe2c24;">用来缓存聚合的中间过程的类 这里不能使用Scala中的二元组和kotlin中的Pair类 因为都不能修改其中的数据 而且需注意 需要有空参构造 (Kotlin中的data class 默认没有空参构造)</span></strong><br>  </p> 
<pre><code class="language-Kotlin">   data class Buffer(var count: Long, var cnt: Long) : Serializable {
       constructor() : this(0, 0)
   }</code></pre> 
<h5>2. 定义UDAF函数：</h5> 
<p>为了看着清晰 创建了一个匿名内部类对象agg 继承自Aggregator 需要定义输入输出和缓存的泛型</p> 
<p>实现其对应的方法</p> 
<pre><code class="language-Kotlin">val agg = object : Aggregator&lt;Long, Buffer, Long&gt;(){
       override fun reduce(b: Buffer?, a: Long?): Buffer {
           val updatedBuffer = b ?: Buffer(0, 0)
           updatedBuffer.cnt++
           updatedBuffer.count += a!!
           return updatedBuffer
       }

       override fun outputEncoder(): Encoder&lt;Long&gt; {
           return Encoders.LONG()
       }

       override fun zero(): Buffer {
           return Buffer(0L, 0L)
       }

       override fun bufferEncoder(): Encoder&lt;Buffer&gt; {
           return Encoders.bean(Buffer::class.java)
       }

       override fun finish(reduction: Buffer?): Long {
           return reduction?.count?.div(reduction.cnt) ?: 0L
       }

       override fun merge(b1: Buffer?, b2: Buffer?): Buffer {
           return Buffer(
               count = (b1?.count ?: 0) + (b2?.count ?: 0),
               cnt = (b1?.cnt ?: 0) + (b2?.cnt ?: 0)
           )
       }
   }</code></pre> 
<p>其中 reduce是对输入的数据进行聚合(这里是累加)</p> 
<p>outputEncoder和bufferEncoder是将输出和缓存的结果进行序列化</p> 
<p>zero是对数据赋初值</p> 
<p>merge是 多分区的数据进行合并时调用的方法 用于将缓存数据合并 (这里时Buffer类)</p> 
<p>finish是输出最终结果</p> 
<p><strong><span style="color:#fe2c24;">要十分注意在这些方法中的空安全处理(有时候kt的空安全挺烦人的) 不要轻易用非空断言</span></strong></p> 
<h5>3. 注册UDAF函数：</h5> 
<p>使用functions.udaf()注册 要指明输出结果的类型</p> 
<pre><code class="language-Kotlin">   sparkSession.udf().register("avgAge", functions.udaf(agg, Encoders.LONG()))</code></pre> 
<h5>4. 执行SQL查询：</h5> 
<p>将读入的数据注册为视图 调用sparkSession的sql方法查询 并将结果在控制台打印输出</p> 
<pre><code class="language-Kotlin">   sparkSession.read().json("data/user.txt").createOrReplaceTempView("user")
   sparkSession.sql("select avgAge(age) as avg_age from user").show()</code></pre> 
<p></p> 
<h5>5.释放资源：</h5> 
<p>调用stop过close方法释放资源</p> 
<pre><code class="language-Kotlin">   sparkSession.stop()</code></pre> 
<h2>4. 总结</h2> 
<p>在不适用spark kotlin api的情况下 用kotlin来写SparkSql基本是使用spark提供的Java api 因为kt与Scala不是完全兼容 所以要注意其中的一些高阶函数还有元组的使用和序列化等问题 </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e9d48681e1490fe3252df6f2a64db6fe/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">从多维度视角探讨“开源AI智能名片O2O商城小程序”的设计与管理</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c5c03d1b99cd7562fed34f7dd7dbc1b9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">HTTP与HTTPS：数据安全性的差异与风险分析</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>