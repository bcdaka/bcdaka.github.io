<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LLaMA-Factory在华为显卡上的实验记录 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/0a9a0f4207f7d57b1471b998481c364c/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="LLaMA-Factory在华为显卡上的实验记录">
  <meta property="og:description" content="如何判断目前所选择的模型是否支持
LLaMA-Factory/src/llamafactory/data/template.py
在项目的这个地址中会有不同模型的支持模版。
这里用目前我最常用的两个模型举例子 一个是智谱的glm4-9B模型
_register_template( name=&#34;glm4&#34;, format_user=StringFormatter(slots=[&#34;&lt;|user|&gt;\n{{content}}&lt;|assistant|&gt;&#34;]), format_assistant=StringFormatter(slots=[&#34;\n{{content}}&#34;]), format_system=StringFormatter(slots=[&#34;&lt;|system|&gt;\n{{content}}&#34;]), format_function=FunctionFormatter(slots=[&#34;{{name}}\n{{arguments}}&#34;]), format_observation=StringFormatter(slots=[&#34;&lt;|observation|&gt;\n{{content}}&lt;|assistant|&gt;&#34;]), format_tools=ToolFormatter(tool_format=&#34;glm4&#34;), format_prefix=EmptyFormatter(slots=[&#34;[gMASK]&lt;sop&gt;&#34;]), stop_words=[&#34;&lt;|user|&gt;&#34;, &#34;&lt;|observation|&gt;&#34;], efficient_eos=True, ) 这段代码看起来是在定义一个模板（template）的注册过程，可能是在某个框架或者系统中使用。让我来解释一下每个参数的作用和含义：
_register_template(...) 这是一个函数或者方法，用来注册一个名为 &#34;glm4&#34; 的模板。
参数解释： name=“glm4”：
这里指定了模板的名称，即 &#34;glm4&#34;。 format_user=StringFormatter(slots=[“\n{{content}}”])：
format_user 是用来格式化用户输入的内容的格式器（formatter）。StringFormatter(slots=[&#34;\n{{content}}&#34;]) 表示使用字符串格式化器，slots=[&#34;\n{{content}}&#34;] 指定了插槽（slots），用于接收用户输入内容，并在格式化时将内容放置在 \n{{content}} 的位置上。 format_assistant=StringFormatter(slots=[“\n{{content}}”])：
format_assistant 是用来格式化助理（assistant）输出的内容的格式器。同样使用了 StringFormatter，并指定了相同的插槽 [&#34;\n{{content}}&#34;]。 format_system=StringFormatter(slots=[“\n{{content}}”])：
format_system 是用来格式化系统（system）输出的内容的格式器。同样使用了 StringFormatter，并指定了相同的插槽 [&#34;\n{{content}}&#34;]。 format_function=FunctionFormatter(slots=[“{{name}}\n{{arguments}}”])：
format_function 是用来格式化函数（function）定义的格式器。FunctionFormatter(slots=[&#34;{{name}}\n{{arguments}}&#34;]) 表示格式化时会使用 {{name}} 和 {{arguments}} 插槽，用于显示函数名称和参数。 format_observation=StringFormatter(slots=[“\n{{content}}”])：
format_observation 是用来格式化观察（observation）输出的内容的格式器。同样使用了 StringFormatter，并指定了相同的插槽 [&#34;\n{{content}}&#34;]。 format_tools=ToolFormatter(tool_format=“glm4”)：
format_tools 是用来格式化工具（tools）的格式器。ToolFormatter(tool_format=&#34;glm4&#34;) 表示工具格式化器将使用 &#34;glm4&#34; 格式。 format_prefix=EmptyFormatter(slots=[“[gMASK]”])：
format_prefix 是用来格式化前缀（prefix）的格式器。EmptyFormatter(slots=[&#34;[gMASK]&lt;sop&gt;&#34;]) 表示前缀格式化器将输出 &#34;[gMASK]&lt;sop&gt;&#34;。 stop_words=[“”, “”]：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-19T18:46:27+08:00">
    <meta property="article:modified_time" content="2024-07-19T18:46:27+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">LLaMA-Factory在华为显卡上的实验记录</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>如何判断目前所选择的模型是否支持<br> LLaMA-Factory/src/llamafactory/data/template.py<br> 在项目的这个地址中会有不同模型的支持模版。</p> 
<p>这里用目前我最常用的两个模型举例子 一个是智谱的glm4-9B模型</p> 
<pre><code class="prism language-python">_register_template<span class="token punctuation">(</span>
    name<span class="token operator">=</span><span class="token string">"glm4"</span><span class="token punctuation">,</span>
    format_user<span class="token operator">=</span>StringFormatter<span class="token punctuation">(</span>slots<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"&lt;|user|&gt;\n{<!-- -->{content}}&lt;|assistant|&gt;"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    format_assistant<span class="token operator">=</span>StringFormatter<span class="token punctuation">(</span>slots<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"\n{<!-- -->{content}}"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    format_system<span class="token operator">=</span>StringFormatter<span class="token punctuation">(</span>slots<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"&lt;|system|&gt;\n{<!-- -->{content}}"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    format_function<span class="token operator">=</span>FunctionFormatter<span class="token punctuation">(</span>slots<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"{<!-- -->{name}}\n{<!-- -->{arguments}}"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    format_observation<span class="token operator">=</span>StringFormatter<span class="token punctuation">(</span>slots<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"&lt;|observation|&gt;\n{<!-- -->{content}}&lt;|assistant|&gt;"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    format_tools<span class="token operator">=</span>ToolFormatter<span class="token punctuation">(</span>tool_format<span class="token operator">=</span><span class="token string">"glm4"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    format_prefix<span class="token operator">=</span>EmptyFormatter<span class="token punctuation">(</span>slots<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"[gMASK]&lt;sop&gt;"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    stop_words<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"&lt;|user|&gt;"</span><span class="token punctuation">,</span> <span class="token string">"&lt;|observation|&gt;"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    efficient_eos<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre> 
<p>这段代码看起来是在定义一个模板（template）的注册过程，可能是在某个框架或者系统中使用。让我来解释一下每个参数的作用和含义：</p> 
<h4><a id="_register_template_23"></a><code>_register_template(...)</code></h4> 
<p>这是一个函数或者方法，用来注册一个名为 <code>"glm4"</code> 的模板。</p> 
<h4><a id="_27"></a>参数解释：</h4> 
<ol><li> <p><strong>name=“glm4”</strong>：</p> 
  <ul><li>这里指定了模板的名称，即 <code>"glm4"</code>。</li></ul> </li><li> <p><strong>format_user=StringFormatter(slots=[“\n{<!-- -->{content}}”])</strong>：</p> 
  <ul><li><code>format_user</code> 是用来格式化用户输入的内容的格式器（formatter）。</li><li><code>StringFormatter(slots=["\n{<!-- -->{content}}"])</code> 表示使用字符串格式化器，<code>slots=["\n{<!-- -->{content}}"]</code> 指定了插槽（slots），用于接收用户输入内容，并在格式化时将内容放置在 <code>\n{<!-- -->{content}}</code> 的位置上。</li></ul> </li><li> <p><strong>format_assistant=StringFormatter(slots=[“\n{<!-- -->{content}}”])</strong>：</p> 
  <ul><li><code>format_assistant</code> 是用来格式化助理（assistant）输出的内容的格式器。</li><li>同样使用了 <code>StringFormatter</code>，并指定了相同的插槽 <code>["\n{<!-- -->{content}}"]</code>。</li></ul> </li><li> <p><strong>format_system=StringFormatter(slots=[“\n{<!-- -->{content}}”])</strong>：</p> 
  <ul><li><code>format_system</code> 是用来格式化系统（system）输出的内容的格式器。</li><li>同样使用了 <code>StringFormatter</code>，并指定了相同的插槽 <code>["\n{<!-- -->{content}}"]</code>。</li></ul> </li><li> <p><strong>format_function=FunctionFormatter(slots=[“{<!-- -->{name}}\n{<!-- -->{arguments}}”])</strong>：</p> 
  <ul><li><code>format_function</code> 是用来格式化函数（function）定义的格式器。</li><li><code>FunctionFormatter(slots=["{<!-- -->{name}}\n{<!-- -->{arguments}}"])</code> 表示格式化时会使用 <code>{<!-- -->{name}}</code> 和 <code>{<!-- -->{arguments}}</code> 插槽，用于显示函数名称和参数。</li></ul> </li><li> <p><strong>format_observation=StringFormatter(slots=[“\n{<!-- -->{content}}”])</strong>：</p> 
  <ul><li><code>format_observation</code> 是用来格式化观察（observation）输出的内容的格式器。</li><li>同样使用了 <code>StringFormatter</code>，并指定了相同的插槽 <code>["\n{<!-- -->{content}}"]</code>。</li></ul> </li><li> <p><strong>format_tools=ToolFormatter(tool_format=“glm4”)</strong>：</p> 
  <ul><li><code>format_tools</code> 是用来格式化工具（tools）的格式器。</li><li><code>ToolFormatter(tool_format="glm4")</code> 表示工具格式化器将使用 <code>"glm4"</code> 格式。</li></ul> </li><li> <p><strong>format_prefix=EmptyFormatter(slots=[“[gMASK]”])</strong>：</p> 
  <ul><li><code>format_prefix</code> 是用来格式化前缀（prefix）的格式器。</li><li><code>EmptyFormatter(slots=["[gMASK]&lt;sop&gt;"])</code> 表示前缀格式化器将输出 <code>"[gMASK]&lt;sop&gt;"</code>。</li></ul> </li><li> <p><strong>stop_words=[“”, “”]</strong>：</p> 
  <ul><li><code>stop_words</code> 是停用词列表，但在这里给出的示例中，停用词列表为空，即 <code>["", ""]</code>。</li></ul> </li><li> <p><strong>efficient_eos=True</strong>：</p> 
  <ul><li><code>efficient_eos</code> 是一个布尔值参数，表示是否启用高效的EOS（End of Sentence）处理。在这里设置为 <code>True</code>，可能意味着系统会优化处理句子结束的方式。</li></ul> </li></ol> 
<p>这段代码的主要作用是定义了一个名为 <code>"glm4"</code> 的模板，包括了各种用于格式化用户输入、助理输出、系统输出、函数定义、观察输出、工具、前缀等内容的格式化器和设置。这种模板的定义通常用于在特定的系统或框架中，为不同类型的输入和输出提供统一的格式化和处理规则，以便于后续的处理和展示。</p> 
<pre><code>_register_template(
    name="qwen",
    format_user=StringFormatter(slots=["&lt;|im_start|&gt;user\n{<!-- -->{content}}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n"]),
    format_system=StringFormatter(slots=["&lt;|im_start|&gt;system\n{<!-- -->{content}}&lt;|im_end|&gt;\n"]),
    format_observation=StringFormatter(slots=["&lt;|im_start|&gt;tool\n{<!-- -->{content}}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n"]),
    format_separator=EmptyFormatter(slots=["\n"]),
    default_system="You are a helpful assistant.",
    stop_words=["&lt;|im_end|&gt;"],
    replace_eos=True,
)
</code></pre> 
<p>目前看所有的qwen模型在llama factory中都用这一套模版。</p> 
<p>从最简化的角度来看目前我在三个阶段分别用到的数据结构<br> 预训练数据结构</p> 
<pre><code class="prism language-jsonline">{"text":""}
</code></pre> 
<p>对应的data_info.json中需要加入以下配置</p> 
<pre><code class="prism language-json"><span class="token string-property property">"pre_dataset_name"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
  <span class="token string-property property">"file_name"</span><span class="token operator">:</span> <span class="token string">"预训练数据文件在data目录下的地址"</span><span class="token punctuation">,</span>
  <span class="token string-property property">"columns"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"prompt"</span><span class="token operator">:</span> <span class="token string">"text"</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>微调训练数据结构</p> 
<pre><code class="prism language-jsonl">{"input_colum": "根据:TWY:滑行道;BTN:在……之间;TWY:滑行道;AND:与;TWY:滑行道;AVBL:可供使用;FOR:为了;OPS.:作业、运行、经营、操作、运转;DRG:在……期间;FLW:如下，以下;TWY:滑行道;FOR:为了;ACFT:航空器;ACFT:航空器;IN:在;APN:停机坪;FOR:为了;ACFT:航空器;ONLY.:只能;AND:与;ACFT:航空器;ON:在;RWY:跑道，逐词翻译：PORTIONOFTWYMBTNTWYLINK31ANDTWYLINK32NOTAVBLFOROPS.\nDRGTHISPERIODFLWRESTRICTIONSSHALLAPPLY:\n1.COMPATIBILITYOFTWYKRESTRICTEDFORACFTUPTOWINGSPAN68.40M.\n2.ACFTSTAND265INCARGOAPNDOWNGRADEDFORACFTUPTOWINGSPAN68.40MONLY.\n3.MOVEMENTOFA388ANDAN124ACFTONRWY10/28NOTPERMITED.","output_colum": "&lt;部分:PORTION:0&gt; &lt;的:OF:1&gt; &lt;滑行道:TWY:2&gt; &lt;M:M:3&gt; &lt;在:BTN:4.1&gt; &lt;之间:BTN:4.2&gt; &lt;滑行道:TWY:5&gt; &lt;连接:LINK:6&gt; &lt;31:31:7&gt; &lt;与:AND:8&gt; &lt;滑行道:TWY:9&gt; &lt;连接:LINK:10&gt; &lt;32:32:11&gt; &lt;不可用:NOT AVBL:12&gt; &lt;因为:FOR:13&gt; &lt;运行:OPS:14&gt; &lt;.:.:15&gt; &lt;在……期间:DRG:16&gt; &lt;这个:THIS:17&gt; &lt;时期:PERIOD:18&gt; &lt;如下，以下:FLW:19&gt; &lt;限制:RESTRICTIONS:20&gt; &lt;应该:SHALL:21&gt; &lt;适用:APPLY:22&gt; &lt;::::23&gt; &lt;1:1:24&gt; &lt;.:.:25&gt; &lt;兼容:COMPATIBILITY:26&gt; &lt;的:OF:27&gt; &lt;滑行道:TWY:28&gt; &lt;K:K:29&gt; &lt;被限制:RESTRICTED:30&gt; &lt;对于:FOR:31&gt; &lt;航空器:ACFT:32&gt; &lt;到:UPTO:33&gt; &lt;翼展:WINGSPAN:34&gt; &lt;68.40M:68.40M:35&gt; &lt;.:.:36&gt; &lt;2:2:37&gt; &lt;.:.:38&gt; &lt;航空器:ACFT:39&gt; &lt;停在:STAND:40&gt; &lt;265:265:41&gt; &lt;在:IN:42&gt; &lt;货物:CARGO:43&gt; &lt;停机坪:APN:44&gt; &lt;降级:DOWNGRADED:45&gt; &lt;对于:FOR:46&gt; &lt;航空器的:ACFT:47&gt; &lt;到:UPTO:48&gt; &lt;翼展:WING SPAN:49&gt; &lt;68.40M:68.40M:50&gt; &lt;只能:ONLY:51&gt; &lt;.:.:52&gt; &lt;3:3:53&gt; &lt;.:.:54&gt; &lt;移动:MOVEMENT:55&gt; &lt;的:OF:56&gt; &lt;A388:A388:57&gt; &lt;与:AND:58&gt; &lt;AN124:AN124:59&gt; &lt;航空器:ACFT:60&gt; &lt;在:ON:61&gt; &lt;跑道:RWY:62&gt; &lt;10/28:10/28:63&gt; &lt;不:NOT:64&gt; &lt;被允许:PERMITED:65&gt; &lt;.:.:66&gt; "}
</code></pre> 
<p>对应的datainfo中的内容为</p> 
<pre><code>"sft_dataset_name": {
  "file_name": "微调数据文件在data目录下的地址",
  "columns": {
    "query": "input_colum",
    "response": "output_colum",
  }
}
</code></pre> 
<p>因为数据量比较大所以使用jsonl,在数据量大的情况下json文件会导致模型报错。<br> 相对于老版本的llamafactory来说新版的加入了多线程分词能力。这样预处理的过程会更快。</p> 
<p>处理好数据以后我们开始处理训练命令。这里注意细节，我们的预训练数据叫做pre_dataset_name，微调数据叫做sft_dataset_name。目前我所在的环境是国内。所以这里我们需要一条指令让模型下载通过魔搭社区进行下载。</p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">export</span> <span class="token assign-left variable">USE_MODELSCOPE_HUB</span><span class="token operator">=</span><span class="token number">1</span> <span class="token comment"># Windows 使用 `set USE_MODELSCOPE_HUB=1`</span>
</code></pre> 
<p>在配置modelscope以后要记得安装modelscope</p> 
<pre><code class="prism language-bash">pip <span class="token function">install</span> modelscope <span class="token parameter variable">-U</span>
</code></pre> 
<p>这里我们用了一种比较落后的方式实用华为的npu。使用torch-npu模块来进行npu的使用。<br> 在训练之前我们先介绍一下llama factory支撑的几种训练模式<br> LlamaFactory 支持的训练模式的解释：</p> 
<p>1、dpo 强化训练 - Data Parallel Optimization 的缩写，数据并行优化。这种方法涉及在多个设备上并行训练模型，每个设备处理不同的数据批次，以提高训练效率和速度。</p> 
<p>2、kto 强化训练 - Knowledge Transfer Optimization 的缩写，知识迁移优化。这通常涉及将预训练模型的知识迁移到新的模型上，以改善新模型的性能。</p> 
<p>3、ppo 强化训练 - Probabilistic Policy Optimization 的缩写，概率策略优化。这是一种强化学习算法，用于优化策略的期望回报，通常用于训练代理在给定环境中执行特定任务。</p> 
<p>4、pt 预训练 - Pre-training 的缩写，预训练。这是在大规模数据集上训练模型的过程，以便模型能够学习通用的语言表示，这些表示可以在各种下游任务中进行微调。</p> 
<p>5、rm 强化反馈训练 - 这可能是一种使用强化学习技术的训练方法，其中模型根据收到的反馈（奖励或惩罚）来调整其行为。</p> 
<p>6、sft 微调训练 - Supervised Fine-Tuning 的缩写，监督式微调。这是在特定任务上使用标注数据对预训练模型进行微调的过程，以提高模型在该任务上的性能。</p> 
<p>第一步我们设置预训练训练的配置文件这里我推荐使用glm4</p> 
<pre><code class="prism language-yaml"><span class="token comment">### model</span>
<span class="token key atrule">model_name_or_path</span><span class="token punctuation">:</span> ZhipuAI/glm<span class="token punctuation">-</span>4<span class="token punctuation">-</span>9b

<span class="token comment">### method</span>
<span class="token key atrule">stage</span><span class="token punctuation">:</span> pt
<span class="token key atrule">do_train</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
<span class="token key atrule">finetuning_type</span><span class="token punctuation">:</span> full
<span class="token key atrule">deepspeed</span><span class="token punctuation">:</span> examples/deepspeed/ds_z3_config.json

<span class="token comment">### dataset</span>
<span class="token key atrule">dataset</span><span class="token punctuation">:</span> pre_dataset_name
<span class="token key atrule">template</span><span class="token punctuation">:</span> glm4
<span class="token key atrule">cutoff_len</span><span class="token punctuation">:</span> <span class="token number">4096</span>
<span class="token key atrule">max_samples</span><span class="token punctuation">:</span> <span class="token number">1000</span>
<span class="token key atrule">overwrite_cache</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
<span class="token key atrule">preprocessing_num_workers</span><span class="token punctuation">:</span> <span class="token number">16</span>

<span class="token comment">### output</span>
<span class="token key atrule">output_dir</span><span class="token punctuation">:</span> saves/glm<span class="token punctuation">-</span>4<span class="token punctuation">-</span>9b/full/pt
<span class="token key atrule">logging_steps</span><span class="token punctuation">:</span> <span class="token number">10</span>
<span class="token key atrule">save_steps</span><span class="token punctuation">:</span> <span class="token number">500</span>
<span class="token key atrule">plot_loss</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
<span class="token key atrule">overwrite_output_dir</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>

<span class="token comment">### train</span>
<span class="token key atrule">per_device_train_batch_size</span><span class="token punctuation">:</span> <span class="token number">1</span>
<span class="token key atrule">gradient_accumulation_steps</span><span class="token punctuation">:</span> <span class="token number">2</span>
<span class="token key atrule">learning_rate</span><span class="token punctuation">:</span> <span class="token number">1.0e-4</span>
<span class="token key atrule">num_train_epochs</span><span class="token punctuation">:</span> <span class="token number">3.0</span>
<span class="token key atrule">lr_scheduler_type</span><span class="token punctuation">:</span> cosine
<span class="token key atrule">warmup_ratio</span><span class="token punctuation">:</span> <span class="token number">0.1</span>
<span class="token key atrule">fp16</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
<span class="token key atrule">ddp_timeout</span><span class="token punctuation">:</span> <span class="token number">180000000</span>

<span class="token comment">### eval</span>
<span class="token key atrule">val_size</span><span class="token punctuation">:</span> <span class="token number">0.1</span>
<span class="token key atrule">per_device_eval_batch_size</span><span class="token punctuation">:</span> <span class="token number">1</span>
<span class="token key atrule">eval_strategy</span><span class="token punctuation">:</span> steps
<span class="token key atrule">eval_steps</span><span class="token punctuation">:</span> <span class="token number">500</span>

</code></pre> 
<p>这里我们指定了训练模式是pt也就是预训练，在openi平台最高可以选择4卡910显卡进行训练。也就是4*32G显存。这是足够进行预训练的。</p> 
<p>如果需要更好的预训练效果可以通过调节以下几个参数来实现。</p> 
<pre><code class="prism language-yaml"><span class="token comment">### train</span>
<span class="token key atrule">per_device_train_batch_size</span><span class="token punctuation">:</span> <span class="token number">1</span>
<span class="token key atrule">gradient_accumulation_steps</span><span class="token punctuation">:</span> <span class="token number">2</span>
<span class="token key atrule">learning_rate</span><span class="token punctuation">:</span> <span class="token number">1.0e-4</span>
<span class="token key atrule">num_train_epochs</span><span class="token punctuation">:</span> <span class="token number">3.0</span>
<span class="token key atrule">lr_scheduler_type</span><span class="token punctuation">:</span> cosine
<span class="token key atrule">warmup_ratio</span><span class="token punctuation">:</span> <span class="token number">0.1</span>
</code></pre> 
<p>这一部分的配置文件详细描述了训练过程的具体参数：</p> 
<h4><a id="train_204"></a>train</h4> 
<ul><li><strong>per_device_train_batch_size</strong>: 这个参数指定了每个训练设备（例如，GPU或TPU）上的批量大小。在这里，它被设置为1，这意味着每个设备在每个训练步骤中处理一个样本。较小的批量大小可以减少内存需求，但可能需要更多的训练步骤来达到收敛。</li><li><strong>gradient_accumulation_steps</strong>: 这个参数定义了在执行权重更新之前累积梯度的步骤数。在这里，它被设置为2，意味着模型将在累积了两步的梯度之后才进行权重更新。这种方法可以在不增加内存使用的情况下模拟更大的批量大小。</li><li><strong>learning_rate</strong>: 学习率是决定模型参数更新速度的关键因素。在这里，它被设置为1.0e-4（即0.0001），这是一个常见的初始学习率值。学习率的选择对模型训练至关重要，过高的学习率可能导致训练不稳定，而过低的学习率可能导致训练过程缓慢。</li><li><strong>num_train_epochs</strong>: 这个参数指定了模型将在训练数据上运行的完整次数。在这里，它被设置为3.0，意味着模型将看到整个训练数据集三次。增加训练轮数可以提高模型的性能，但也可能导致过拟合。</li><li><strong>lr_scheduler_type</strong>: 学习率调度器用于在训练过程中动态调整学习率。在这里，它被设置为“cosine”，这意味着学习率将按照余弦函数的规律变化。余弦调度器通常在训练开始时保持较高的学习率，并在训练过程中逐渐降低。</li><li><strong>warmup_ratio</strong>: 这个参数定义了学习率热身期间的比例。在这里，它被设置为0.1，这意味着在训练的前10%时间内，学习率将从0逐渐增加到初始学习率。热身阶段有助于在训练初期稳定模型的学习。<br> 这些参数共同决定了模型训练的效率和质量。调整这些参数可以帮助优化模型的性能，同时确保训练过程的有效性和稳定性。<br> 我们开始安装在npu中的llama factory训练框架<br> 第一步安装npu版本的llama factory</li></ul> 
<pre><code class="prism language-bash">pip <span class="token function">install</span> <span class="token parameter variable">-e</span> <span class="token string">'.[torch-npu,metrics]'</span>
</code></pre> 
<p>第二步安装npu环境</p> 
<pre><code class="prism language-bash"><span class="token comment"># 请替换 URL 为 CANN 版本和设备型号对应的 URL</span>
<span class="token comment"># 安装 CANN Toolkit</span>
<span class="token function">wget</span> https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-toolkit_8.0.RC1.alpha001_linux-<span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">uname</span> <span class="token parameter variable">-i</span><span class="token variable">)</span></span>"</span>.run
<span class="token function">bash</span> Ascend-cann-toolkit_8.0.RC1.alpha001_linux-<span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">uname</span> <span class="token parameter variable">-i</span><span class="token variable">)</span></span>"</span>.run <span class="token parameter variable">--install</span>

<span class="token comment"># 安装 CANN Kernels</span>
<span class="token function">wget</span> https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run
<span class="token function">bash</span> Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run <span class="token parameter variable">--install</span>

<span class="token comment"># 设置环境变量</span>
<span class="token builtin class-name">source</span> /usr/local/Ascend/ascend-toolkit/set_env.sh
</code></pre> 
<p>第三步在我安装的时候遇到了一个小bug，因为没有云平台的root权限，所以这里我才用了conda进行环境安装。</p> 
<pre><code class="prism language-bash">conda <span class="token function">install</span> <span class="token parameter variable">-c</span> conda-forge libsndfile
</code></pre> 
<p>还有一个提升性能的库</p> 
<pre><code class="prism language-bash">conda <span class="token function">install</span> conda-forge::libaio
</code></pre> 
<p>单机多卡情况下使用deepspeed zero3会带来相对原生的单机多卡更高的计算效率。<br> 第四步安装deepspeed。</p> 
<pre><code class="prism language-bash">pip <span class="token function">install</span> deepspeed
</code></pre> 
<p>接下来我们运行命令开始进行训练</p> 
<pre><code class="prism language-bash"> llamafactory-cli train LLaMA-Factory/examples/train_full/glm4_full_pt_ds3.yaml
</code></pre> 
<p>成功训练的日志的样子</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span><span class="token number">2024</span>-07-09 09:10:46,149<span class="token punctuation">]</span> <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>real_accelerator.py:203:get_accelerator<span class="token punctuation">]</span> Setting ds_accelerator to npu <span class="token punctuation">(</span>auto detect<span class="token punctuation">)</span>
 <span class="token punctuation">[</span>WARNING<span class="token punctuation">]</span>  async_io requires the dev libaio .so object and headers but these were not found.
 <span class="token punctuation">[</span>WARNING<span class="token punctuation">]</span>  async_io: please <span class="token function">install</span> the libaio-devel package with yum
 <span class="token punctuation">[</span>WARNING<span class="token punctuation">]</span>  If libaio is already installed <span class="token punctuation">(</span>perhaps from <span class="token builtin class-name">source</span><span class="token punctuation">)</span>, try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
07/09/2024 09:11:02 - INFO - llamafactory.hparams.parser - Process rank: <span class="token number">0</span>, device: npu:0, n_gpu: <span class="token number">1</span>, distributed training: False, compute dtype: torch.float16
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">1</span>.35k/1.35k <span class="token punctuation">[</span>00:0<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:00, <span class="token number">4</span>.44kB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">36.0</span>/36.0 <span class="token punctuation">[</span>00:0<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:00, <span class="token number">86</span>.4B/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">2</span>.21k/2.21k <span class="token punctuation">[</span>00:0<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:00, <span class="token number">5</span>.46kB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">205</span>/205 <span class="token punctuation">[</span>00:0<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:00, 451B/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">6</span>.34k/6.34k <span class="token punctuation">[</span>00:0<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:00, <span class="token number">19</span>.5kB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">1</span>.81G/1.81G <span class="token punctuation">[</span>01:2<span class="token operator"><span class="token file-descriptor important">3</span>&lt;</span>00:00, <span class="token number">23</span>.2MB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">1</span>.69G/1.69G <span class="token punctuation">[</span>01:3<span class="token operator"><span class="token file-descriptor important">6</span>&lt;</span>00:00, <span class="token number">18</span>.8MB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">1</span>.69G/1.69G <span class="token punctuation">[</span>01:3<span class="token operator"><span class="token file-descriptor important">6</span>&lt;</span>00:00, <span class="token number">18</span>.8MB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">1</span>.83G/1.83G <span class="token punctuation">[</span>01:1<span class="token operator"><span class="token file-descriptor important">8</span>&lt;</span>00:00, <span class="token number">25</span>.1MB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">1</span>.80G/1.80G <span class="token punctuation">[</span>01:1<span class="token operator"><span class="token file-descriptor important">9</span>&lt;</span>00:00, <span class="token number">24</span>.1MB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">1</span>.69G/1.69G <span class="token punctuation">[</span>01:1<span class="token operator"><span class="token file-descriptor important">5</span>&lt;</span>00:00, <span class="token number">24</span>.0MB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">1</span>.83G/1.83G <span class="token punctuation">[</span>01:2<span class="token operator"><span class="token file-descriptor important">2</span>&lt;</span>00:00, <span class="token number">24</span>.0MB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">1</span>.80G/1.80G <span class="token punctuation">[</span>01:1<span class="token operator"><span class="token file-descriptor important">2</span>&lt;</span>00:00, <span class="token number">26</span>.4MB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">1</span>.69G/1.69G <span class="token punctuation">[</span>01:0<span class="token operator"><span class="token file-descriptor important">3</span>&lt;</span>00:00, <span class="token number">28</span>.6MB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">1</span>.83G/1.83G <span class="token punctuation">[</span>01:1<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:00, <span class="token number">27</span>.8MB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">1</span>.54G/1.54G <span class="token punctuation">[</span>01:0<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:00, <span class="token number">27</span>.1MB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">28</span>.4k/28.4k <span class="token punctuation">[</span>00:0<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:00, <span class="token number">65</span>.7kB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">57</span>.1k/57.1k <span class="token punctuation">[</span>00:0<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:00, 100kB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">3</span>.34k/3.34k <span class="token punctuation">[</span>00:0<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:00, <span class="token number">11</span>.8kB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">3</span>.78k/3.78k <span class="token punctuation">[</span>00:0<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:00, <span class="token number">12</span>.2kB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">15</span>.3k/15.3k <span class="token punctuation">[</span>00:0<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:00, <span class="token number">28</span>.9kB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">2</span>.50M/2.50M <span class="token punctuation">[</span>00:0<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:00, <span class="token number">3</span>.07MB/s<span class="token punctuation">]</span>
Downloading: <span class="token number">100</span>%<span class="token operator">|</span>██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">3</span>.12k/3.12k <span class="token punctuation">[</span>00:0<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:00, <span class="token number">9</span>.51kB/s<span class="token punctuation">]</span>
<span class="token punctuation">[</span>INFO<span class="token operator">|</span>tokenization_utils_base.py:2159<span class="token punctuation">]</span> <span class="token number">2024</span>-07-09 09:24:48,669 <span class="token operator">&gt;&gt;</span> loading <span class="token function">file</span> tokenizer.model
<span class="token punctuation">[</span>INFO<span class="token operator">|</span>tokenization_utils_base.py:2159<span class="token punctuation">]</span> <span class="token number">2024</span>-07-09 09:24:48,669 <span class="token operator">&gt;&gt;</span> loading <span class="token function">file</span> added_tokens.json
<span class="token punctuation">[</span>INFO<span class="token operator">|</span>tokenization_utils_base.py:2159<span class="token punctuation">]</span> <span class="token number">2024</span>-07-09 09:24:48,669 <span class="token operator">&gt;&gt;</span> loading <span class="token function">file</span> special_tokens_map.json
<span class="token punctuation">[</span>INFO<span class="token operator">|</span>tokenization_utils_base.py:2159<span class="token punctuation">]</span> <span class="token number">2024</span>-07-09 09:24:48,669 <span class="token operator">&gt;&gt;</span> loading <span class="token function">file</span> tokenizer_config.json
<span class="token punctuation">[</span>INFO<span class="token operator">|</span>tokenization_utils_base.py:2159<span class="token punctuation">]</span> <span class="token number">2024</span>-07-09 09:24:48,669 <span class="token operator">&gt;&gt;</span> loading <span class="token function">file</span> tokenizer.json
<span class="token punctuation">[</span>WARNING<span class="token operator">|</span>logging.py:313<span class="token punctuation">]</span> <span class="token number">2024</span>-07-09 09:24:49,392 <span class="token operator">&gt;&gt;</span> Special tokens have been added <span class="token keyword">in</span> the vocabulary, <span class="token function">make</span> sure the associated word embeddings are fine-tuned or trained.
07/09/2024 09:24:49 - INFO - llamafactory.data.template - Add <span class="token operator">&lt;</span><span class="token operator">|</span>user<span class="token operator">|</span><span class="token operator">&gt;</span>,<span class="token operator">&lt;</span><span class="token operator">|</span>observation<span class="token operator">|</span><span class="token operator">&gt;</span> to stop words.
07/09/2024 09:24:49 - INFO - llamafactory.data.loader - Loading dataset identity.json<span class="token punctuation">..</span>.
Generating train split: <span class="token number">91</span> examples <span class="token punctuation">[</span>00:00, <span class="token number">1770.27</span> examples/s<span class="token punctuation">]</span>
Converting <span class="token function">format</span> of dataset <span class="token punctuation">(</span>num_proc<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>: <span class="token number">100</span>%<span class="token operator">|</span>████████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">91</span>/91 <span class="token punctuation">[</span>00:0<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:00, <span class="token number">187.32</span> examples/s<span class="token punctuation">]</span>
07/09/2024 09:25:01 - INFO - llamafactory.data.loader - Loading dataset alpaca_en_demo.json<span class="token punctuation">..</span>.
Generating train split: <span class="token number">1000</span> examples <span class="token punctuation">[</span>00:00, <span class="token number">19614.77</span> examples/s<span class="token punctuation">]</span>
Converting <span class="token function">format</span> of dataset <span class="token punctuation">(</span>num_proc<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>: <span class="token number">100</span>%<span class="token operator">|</span>███████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">1000</span>/1000 <span class="token punctuation">[</span>00:0<span class="token operator"><span class="token file-descriptor important">0</span>&lt;</span>00:00, <span class="token number">2385.01</span> examples/s<span class="token punctuation">]</span>
Running tokenizer on dataset <span class="token punctuation">(</span>num_proc<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>: <span class="token number">100</span>%<span class="token operator">|</span>█████████████████████████████████████████████████████████████████████████████████████████████<span class="token operator">|</span> <span class="token number">1091</span>/1091 <span class="token punctuation">[</span>00:4<span class="token operator"><span class="token file-descriptor important">3</span>&lt;</span>00:00, <span class="token number">24.98</span> examples/s<span class="token punctuation">]</span>
input_ids:
<span class="token punctuation">[</span><span class="token number">151331</span>, <span class="token number">151333</span>, <span class="token number">151336</span>, <span class="token number">198</span>, <span class="token number">6023</span>, <span class="token number">151337</span>, <span class="token number">198</span>, <span class="token number">9703</span>, <span class="token number">0</span>, <span class="token number">358</span>, <span class="token number">1079</span>, <span class="token number">5867</span>, <span class="token number">606</span>, <span class="token number">37953</span>, <span class="token number">458</span>, <span class="token number">15223</span>, <span class="token number">17821</span>, <span class="token number">7881</span>, <span class="token number">553</span>, <span class="token number">5867</span>, <span class="token number">3094</span>, <span class="token number">3417</span>, <span class="token number">13</span>, <span class="token number">2585</span>, <span class="token number">646</span>, <span class="token number">358</span>, <span class="token number">7789</span>, <span class="token number">498</span>, <span class="token number">3351</span>, <span class="token number">30</span>, <span class="token number">151329</span><span class="token punctuation">]</span>
inputs:
<span class="token punctuation">[</span>gMASK<span class="token punctuation">]</span> <span class="token operator">&lt;</span>sop<span class="token operator">&gt;</span> <span class="token operator">&lt;</span><span class="token operator">|</span>user<span class="token operator">|</span><span class="token operator">&gt;</span> 
hi <span class="token operator">&lt;</span><span class="token operator">|</span>assistant<span class="token operator">|</span><span class="token operator">&gt;</span> 
Hello<span class="token operator">!</span> I am <span class="token punctuation">{<!-- --></span><span class="token punctuation">{<!-- --></span>name<span class="token punctuation">}</span><span class="token punctuation">}</span>, an AI assistant developed by <span class="token punctuation">{<!-- --></span><span class="token punctuation">{<!-- --></span>author<span class="token punctuation">}</span><span class="token punctuation">}</span>. How can I assist you today? <span class="token operator">&lt;</span><span class="token operator">|</span>endoftext<span class="token operator">|</span><span class="token operator">&gt;</span>
label_ids:
<span class="token punctuation">[</span>-100, -100, -100, -100, -100, -100, <span class="token number">198</span>, <span class="token number">9703</span>, <span class="token number">0</span>, <span class="token number">358</span>, <span class="token number">1079</span>, <span class="token number">5867</span>, <span class="token number">606</span>, <span class="token number">37953</span>, <span class="token number">458</span>, <span class="token number">15223</span>, <span class="token number">17821</span>, <span class="token number">7881</span>, <span class="token number">553</span>, <span class="token number">5867</span>, <span class="token number">3094</span>, <span class="token number">3417</span>, <span class="token number">13</span>, <span class="token number">2585</span>, <span class="token number">646</span>, <span class="token number">358</span>, <span class="token number">7789</span>, <span class="token number">498</span>, <span class="token number">3351</span>, <span class="token number">30</span>, <span class="token number">151329</span><span class="token punctuation">]</span>
labels:

Hello<span class="token operator">!</span> I am <span class="token punctuation">{<!-- --></span><span class="token punctuation">{<!-- --></span>name<span class="token punctuation">}</span><span class="token punctuation">}</span>, an AI assistant developed by <span class="token punctuation">{<!-- --></span><span class="token punctuation">{<!-- --></span>author<span class="token punctuation">}</span><span class="token punctuation">}</span>. How can I assist you today? <span class="token operator">&lt;</span><span class="token operator">|</span>endoftext<span class="token operator">|</span><span class="token operator">&gt;</span>
<span class="token punctuation">[</span>INFO<span class="token operator">|</span>configuration_utils.py:731<span class="token punctuation">]</span> <span class="token number">2024</span>-07-09 09:26:01,831 <span class="token operator">&gt;&gt;</span> loading configuration <span class="token function">file</span> /root/.cache/modelscope/hub/ZhipuAI/glm-4-9b/config.json
<span class="token punctuation">[</span>INFO<span class="token operator">|</span>configuration_utils.py:731<span class="token punctuation">]</span> <span class="token number">2024</span>-07-09 09:26:01,844 <span class="token operator">&gt;&gt;</span> loading configuration <span class="token function">file</span> /root/.cache/modelscope/hub/ZhipuAI/glm-4-9b/config.json
<span class="token punctuation">[</span>INFO<span class="token operator">|</span>configuration_utils.py:800<span class="token punctuation">]</span> <span class="token number">2024</span>-07-09 09:26:01,846 <span class="token operator">&gt;&gt;</span> Model config ChatGLMConfig <span class="token punctuation">{<!-- --></span>
  <span class="token string">"_name_or_path"</span><span class="token builtin class-name">:</span> <span class="token string">"/root/.cache/modelscope/hub/ZhipuAI/glm-4-9b"</span>,
  <span class="token string">"add_bias_linear"</span><span class="token builtin class-name">:</span> false,
  <span class="token string">"add_qkv_bias"</span><span class="token builtin class-name">:</span> true,
  <span class="token string">"apply_query_key_layer_scaling"</span><span class="token builtin class-name">:</span> true,
  <span class="token string">"apply_residual_connection_post_layernorm"</span><span class="token builtin class-name">:</span> false,
  <span class="token string">"architectures"</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
    <span class="token string">"ChatGLMModel"</span>
  <span class="token punctuation">]</span>,
  <span class="token string">"attention_dropout"</span><span class="token builtin class-name">:</span> <span class="token number">0.0</span>,
  <span class="token string">"attention_softmax_in_fp32"</span><span class="token builtin class-name">:</span> true,
  <span class="token string">"auto_map"</span><span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"AutoConfig"</span><span class="token builtin class-name">:</span> <span class="token string">"configuration_chatglm.ChatGLMConfig"</span>,
    <span class="token string">"AutoModel"</span><span class="token builtin class-name">:</span> <span class="token string">"modeling_chatglm.ChatGLMForConditionalGeneration"</span>,
    <span class="token string">"AutoModelForCausalLM"</span><span class="token builtin class-name">:</span> <span class="token string">"modeling_chatglm.ChatGLMForConditionalGeneration"</span>,
    <span class="token string">"AutoModelForSeq2SeqLM"</span><span class="token builtin class-name">:</span> <span class="token string">"modeling_chatglm.ChatGLMForConditionalGeneration"</span>,
    <span class="token string">"AutoModelForSequenceClassification"</span><span class="token builtin class-name">:</span> <span class="token string">"modeling_chatglm.ChatGLMForSequenceClassification"</span>
  <span class="token punctuation">}</span>,
  <span class="token string">"bias_dropout_fusion"</span><span class="token builtin class-name">:</span> true,
  <span class="token string">"classifier_dropout"</span><span class="token builtin class-name">:</span> null,
  <span class="token string">"eos_token_id"</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
    <span class="token number">151329</span>,
    <span class="token number">151336</span>,
    <span class="token number">151338</span>
  <span class="token punctuation">]</span>,
  <span class="token string">"ffn_hidden_size"</span><span class="token builtin class-name">:</span> <span class="token number">13696</span>,
  <span class="token string">"fp32_residual_connection"</span><span class="token builtin class-name">:</span> false,
  <span class="token string">"hidden_dropout"</span><span class="token builtin class-name">:</span> <span class="token number">0.0</span>,
  <span class="token string">"hidden_size"</span><span class="token builtin class-name">:</span> <span class="token number">4096</span>,
  <span class="token string">"kv_channels"</span><span class="token builtin class-name">:</span> <span class="token number">128</span>,
  <span class="token string">"layernorm_epsilon"</span><span class="token builtin class-name">:</span> <span class="token number">1</span>.5625e-07,
  <span class="token string">"model_type"</span><span class="token builtin class-name">:</span> <span class="token string">"chatglm"</span>,
  <span class="token string">"multi_query_attention"</span><span class="token builtin class-name">:</span> true,
  <span class="token string">"multi_query_group_num"</span><span class="token builtin class-name">:</span> <span class="token number">2</span>,
  <span class="token string">"num_attention_heads"</span><span class="token builtin class-name">:</span> <span class="token number">32</span>,
  <span class="token string">"num_layers"</span><span class="token builtin class-name">:</span> <span class="token number">40</span>,
  <span class="token string">"original_rope"</span><span class="token builtin class-name">:</span> true,
  <span class="token string">"pad_token_id"</span><span class="token builtin class-name">:</span> <span class="token number">151329</span>,
  <span class="token string">"padded_vocab_size"</span><span class="token builtin class-name">:</span> <span class="token number">151552</span>,
  <span class="token string">"post_layer_norm"</span><span class="token builtin class-name">:</span> true,
  <span class="token string">"rmsnorm"</span><span class="token builtin class-name">:</span> true,
  <span class="token string">"rope_ratio"</span><span class="token builtin class-name">:</span> <span class="token number">1</span>,
  <span class="token string">"seq_length"</span><span class="token builtin class-name">:</span> <span class="token number">8192</span>,
  <span class="token string">"tie_word_embeddings"</span><span class="token builtin class-name">:</span> false,
  <span class="token string">"torch_dtype"</span><span class="token builtin class-name">:</span> <span class="token string">"bfloat16"</span>,
  <span class="token string">"transformers_version"</span><span class="token builtin class-name">:</span> <span class="token string">"4.42.3"</span>,
  <span class="token string">"use_cache"</span><span class="token builtin class-name">:</span> true,
  <span class="token string">"vocab_size"</span><span class="token builtin class-name">:</span> <span class="token number">151552</span>
<span class="token punctuation">}</span>

<span class="token punctuation">[</span>INFO<span class="token operator">|</span>modeling_utils.py:3553<span class="token punctuation">]</span> <span class="token number">2024</span>-07-09 09:26:01,975 <span class="token operator">&gt;&gt;</span> loading weights <span class="token function">file</span> /root/.cache/modelscope/hub/ZhipuAI/glm-4-9b/model.safetensors.index.json
<span class="token punctuation">[</span>INFO<span class="token operator">|</span>modeling_utils.py:3698<span class="token punctuation">]</span> <span class="token number">2024</span>-07-09 09:26:01,976 <span class="token operator">&gt;&gt;</span> Detected DeepSpeed ZeRO-3: activating zero.init<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> this model
<span class="token punctuation">[</span><span class="token number">2024</span>-07-09 09:26:01,979<span class="token punctuation">]</span> <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>comm.py:637:init_distributed<span class="token punctuation">]</span> <span class="token assign-left variable">cdb</span><span class="token operator">=</span>None
<span class="token punctuation">[</span><span class="token number">2024</span>-07-09 09:26:01,979<span class="token punctuation">]</span> <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>comm.py:652:init_distributed<span class="token punctuation">]</span> Not using the DeepSpeed or dist launchers, attempting to detect MPI environment<span class="token punctuation">..</span>.
</code></pre> 
<p>日志解读<br> 根据你提供的日志信息，这是一个涉及机器学习模型训练的过程。我会逐步解释每个部分的含义和可能的影响：</p> 
<ol><li> <p><strong>INFO 和 WARNING 日志</strong>：</p> 
  <ul><li><code>Setting ds_accelerator to npu (auto detect)</code>：指示程序将使用NPU（神经处理单元）加速器，系统自动检测到这一设置。</li><li><code>async_io requires the dev libaio .so object and headers but these were not found.</code>：警告提示缺少 <code>libaio</code> 库，这可能影响异步IO的性能。</li><li><code>If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.</code>：建议如果已经安装了 <code>libaio</code>，可以尝试设置 <code>CFLAGS</code> 和 <code>LDFLAGS</code> 环境变量来正确定位该库。</li></ul> </li><li> <p><strong>下载和处理数据集</strong>：</p> 
  <ul><li>大量的 <code>Downloading</code> 和 <code>Converting format of dataset</code> 行指示正在下载和转换数据集，这是模型训练过程中常见的操作。</li></ul> </li><li> <p><strong>模型配置和加载</strong>：</p> 
  <ul><li>模型配置信息显示了模型的参数设置，如层数、隐藏单元大小等。</li><li><code>loading weights file /root/.cache/modelscope/hub/ZhipuAI/glm-4-9b/model.safetensors.index.json</code> 表示正在加载模型的权重文件。</li><li><code>Detected DeepSpeed ZeRO-3: activating zero.init() for this model</code> 表示检测到使用了 DeepSpeed ZeRO-3 技术，这是一种优化模型训练内存使用和效率的方法。</li></ul> </li><li> <p><strong>MPI 环境检测</strong>：</p> 
  <ul><li><code>Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...</code> 检测到不是使用 DeepSpeed 或分布式启动器，正在尝试检测 MPI 环境。</li></ul> </li></ol> 
<p>综上所述，日志记录了一个使用 NPU 加速器的机器学习模型训练过程，涉及数据集下载、模型加载和配置，以及一些系统环境的警告和优化建议。<br> 这里报了个错误。mpi环境失败手动安装mpi环境</p> 
<pre><code class="prism language-bash">conda <span class="token function">install</span> <span class="token parameter variable">-c</span> conda-forge mpi4py openmpi
</code></pre> 
<p>安装的时候返回了一段日志。对这段日志进行解读。</p> 
<pre><code class="prism language-bash">On Linux, Open MPI is built with UCX support but it is disabled by default.                                                                                                              
To <span class="token builtin class-name">enable</span> it, first <span class="token function">install</span> UCX <span class="token punctuation">(</span>conda <span class="token function">install</span> <span class="token parameter variable">-c</span> conda-forge ucx<span class="token punctuation">)</span>.                                                                                                                      
Afterwards, <span class="token builtin class-name">set</span> the environment variables                                                                                                                                                
<span class="token assign-left variable">OMPI_MCA_pml</span><span class="token operator">=</span>ucx <span class="token assign-left variable">OMPI_MCA_osc</span><span class="token operator">=</span>ucx                                                                                                                                                        
before launching your MPI processes.                                                                                                                                                     
Equivalently, you can <span class="token builtin class-name">set</span> the MCA parameters <span class="token keyword">in</span> the <span class="token builtin class-name">command</span> line:
mpiexec <span class="token parameter variable">--mca</span> pml ucx <span class="token parameter variable">--mca</span> osc ucx <span class="token punctuation">..</span>.


On Linux, Open MPI is built with CUDA awareness but it is disabled by default.
To <span class="token builtin class-name">enable</span> it, please <span class="token builtin class-name">set</span> the environment variable
<span class="token assign-left variable">OMPI_MCA_opal_cuda_support</span><span class="token operator">=</span>true
before launching your MPI processes.
Equivalently, you can <span class="token builtin class-name">set</span> the MCA parameter <span class="token keyword">in</span> the <span class="token builtin class-name">command</span> line:
mpiexec <span class="token parameter variable">--mca</span> opal_cuda_support <span class="token number">1</span> <span class="token punctuation">..</span>.
Note that you might also need to <span class="token builtin class-name">set</span> <span class="token assign-left variable">UCX_MEMTYPE_CACHE</span><span class="token operator">=</span>n <span class="token keyword">for</span> CUDA awareness via
UCX. Please consult UCX documentation <span class="token keyword">for</span> further details.


<span class="token keyword">done</span>
</code></pre> 
<p>这段日志是在告知如何在Linux系统中启用Open MPI的UCX（Unified Communication X）支持和CUDA（Compute Unified Device Architecture）意识支持。UCX是一个高性能通信库，用于支持不同通信机制（如InfiniBand, RoCE, TCP/IP等），而CUDA是由NVIDIA开发的并行计算平台和编程模型。<br> 以下是日志的解读：</p> 
<ol><li><strong>启用UCX支持</strong>： 
  <ul><li>Open MPI在Linux上编译时包含了UCX支持，但默认是禁用的。</li><li>要启用UCX支持，首先需要安装UCX。可以通过conda包管理器安装，命令是 <code>conda install -c conda-forge ucx</code>。</li><li>安装UCX后，在启动MPI进程之前，需要设置环境变量 <code>OMPI_MCA_pml=ucx</code> 和 <code>OMPI_MCA_osc=ucx</code>。</li><li>或者，可以在命令行中设置MCA参数，使用命令 <code>mpiexec --mca pml ucx --mca osc ucx ...</code>。</li></ul> </li><li><strong>启用CUDA意识支持</strong>： 
  <ul><li>Open MPI在Linux上编译时也包含了CUDA意识支持，但默认也是禁用的。</li><li>要启用CUDA意识支持，需要设置环境变量 <code>OMPI_MCA_opal_cuda_support=true</code>。</li><li>同样，可以在命令行中设置MCA参数，使用命令 <code>mpiexec --mca opal_cuda_support 1 ...</code>。</li><li>如果要通过UCX启用CUDA意识支持，可能还需要设置 <code>UCX_MEMTYPE_CACHE=n</code>。具体细节可以查阅UCX的文档。</li></ul> </li></ol> 
<p>本来早点结束 。嘿嘿又爆出了新的问题。</p> 
<pre><code class="prism language-bash">Traceback <span class="token punctuation">(</span>most recent call last<span class="token punctuation">)</span>:
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/bin/llamafactory-cli"</span>, line <span class="token number">8</span>, <span class="token keyword">in</span> <span class="token operator">&lt;</span>module<span class="token operator">&gt;</span>
    sys.exit<span class="token punctuation">(</span>main<span class="token punctuation">(</span><span class="token punctuation">))</span>
  File <span class="token string">"/tmp/code/LLaMA-Factory/src/llamafactory/cli.py"</span>, line <span class="token number">110</span>, <span class="token keyword">in</span> main
    run_exp<span class="token punctuation">(</span><span class="token punctuation">)</span>
  File <span class="token string">"/tmp/code/LLaMA-Factory/src/llamafactory/train/tuner.py"</span>, line <span class="token number">47</span>, <span class="token keyword">in</span> run_exp
    run_sft<span class="token punctuation">(</span>model_args, data_args, training_args, finetuning_args, generating_args, callbacks<span class="token punctuation">)</span>
  File <span class="token string">"/tmp/code/LLaMA-Factory/src/llamafactory/train/sft/workflow.py"</span>, line <span class="token number">49</span>, <span class="token keyword">in</span> run_sft
    model <span class="token operator">=</span> load_model<span class="token punctuation">(</span>tokenizer, model_args, finetuning_args, training_args.do_train<span class="token punctuation">)</span>
  File <span class="token string">"/tmp/code/LLaMA-Factory/src/llamafactory/model/loader.py"</span>, line <span class="token number">151</span>, <span class="token keyword">in</span> load_model
    model <span class="token operator">=</span> AutoModelForCausalLM.from_pretrained<span class="token punctuation">(</span>**init_kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py"</span>, line <span class="token number">559</span>, <span class="token keyword">in</span> from_pretrained
    <span class="token builtin class-name">return</span> model_class.from_pretrained<span class="token punctuation">(</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/transformers/modeling_utils.py"</span>, line <span class="token number">3710</span>, <span class="token keyword">in</span> from_pretrained
    model <span class="token operator">=</span> cls<span class="token punctuation">(</span>config, *model_args, **model_kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/deepspeed/runtime/zero/partition_parameters.py"</span>, line <span class="token number">506</span>, <span class="token keyword">in</span> wrapper
    f<span class="token punctuation">(</span>module, *args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/root/.cache/huggingface/modules/transformers_modules/glm-4-9b/modeling_chatglm.py"</span>, line <span class="token number">928</span>, <span class="token keyword">in</span> __init__
    self.transformer <span class="token operator">=</span> ChatGLMModel<span class="token punctuation">(</span>config, <span class="token assign-left variable">empty_init</span><span class="token operator">=</span>empty_init, <span class="token assign-left variable">device</span><span class="token operator">=</span>device<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/deepspeed/runtime/zero/partition_parameters.py"</span>, line <span class="token number">506</span>, <span class="token keyword">in</span> wrapper
    f<span class="token punctuation">(</span>module, *args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/root/.cache/huggingface/modules/transformers_modules/glm-4-9b/modeling_chatglm.py"</span>, line <span class="token number">852</span>, <span class="token keyword">in</span> __init__
    self.rotary_pos_emb <span class="token operator">=</span> RotaryEmbedding<span class="token punctuation">(</span>rotary_dim // <span class="token number">2</span>, <span class="token assign-left variable">rope_ratio</span><span class="token operator">=</span>config.rope_ratio,
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/deepspeed/runtime/zero/partition_parameters.py"</span>, line <span class="token number">506</span>, <span class="token keyword">in</span> wrapper
    f<span class="token punctuation">(</span>module, *args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/root/.cache/huggingface/modules/transformers_modules/glm-4-9b/modeling_chatglm.py"</span>, line <span class="token number">96</span>, <span class="token keyword">in</span> __init__
    inv_freq <span class="token operator">=</span> <span class="token number">1.0</span> / <span class="token punctuation">(</span><span class="token number">10000</span> ** <span class="token punctuation">(</span>torch.arange<span class="token punctuation">(</span><span class="token number">0</span>, dim, <span class="token number">2</span>, <span class="token assign-left variable">device</span><span class="token operator">=</span>device<span class="token punctuation">)</span>.to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span> / dim<span class="token punctuation">))</span>
RuntimeError: call aclnnCast failed, detail:EZ1001: <span class="token number">2024</span>-07-09-09:38:52.309.843 The param dtype not implemented <span class="token keyword">for</span> DT_BFLOAT16, should be <span class="token keyword">in</span> dtype support list <span class="token punctuation">[</span>DT_FLOAT16,DT_FLOAT,DT_DOUBLE,DT_INT8,DT_UINT8,DT_INT16,DT_INT32,DT_INT64,DT_UINT16,DT_UINT32,DT_UINT64,DT_BOOL,DT_COMPLEX64,DT_COMPLEX128,<span class="token punctuation">]</span>.

<span class="token punctuation">[</span>ERROR<span class="token punctuation">]</span> <span class="token number">2024</span>-07-09-09:38:52 <span class="token punctuation">(</span>PID:17196, Device:0, RankID:0<span class="token punctuation">)</span> ERR01005 OPS internal error
</code></pre> 
<p>先解读一下异常。<br> 这段日志是Python程序运行时出现的错误堆栈，具体是使用DeepSpeed库（一种用于深度学习训练的库）在MindSpore（一种深度学习框架）上运行时遇到的。错误信息表明在执行模型初始化时出现了运行时错误，导致无法创建模型。<br> 错误信息显示在尝试创建模型的某些组件时，由于某种原因，无法将参数的数据类型转换为DeepSpeed支持的类型。具体来说，问题出现在创建<code>RotaryEmbedding</code>对象时，这个对象需要一个<code>rotary_dim</code>参数，但是在转换过程中遇到了问题。<br> 错误堆栈的详细部分如下：</p> 
<ol><li><code>RuntimeError: call aclnnCast failed, detail:EZ1001: 2024-07-09-09:38:52.309.843 The param dtype not implemented for DT_BFLOAT16, should be in dtype support list [DT_FLOAT16,DT_FLOAT,DT_DOUBLE,DT_INT8,DT_UINT8,DT_INT16,DT_INT32,DT_INT64,DT_UINT16,DT_UINT32,DT_UINT64,DT_BOOL,DT_COMPLEX64,DT_COMPLEX128,].</code><br> 这段信息表明，在尝试将数据类型从<code>DT_BFLOAT16</code>转换为DeepSpeed支持的类型时失败了。<code>DT_BFLOAT16</code>不在DeepSpeed支持的类型列表中，所以转换失败。</li><li><code>[ERROR] 2024-07-09-09:38:52 (PID:17196, Device:0, RankID:0) ERR01005 OPS internal error</code><br> 这表明DeepSpeed内部的某个操作（可能是模型初始化的一部分）遇到了内部错误。<br> 解决这个问题的方法可能包括：</li></ol> 
<ul><li>检查模型的配置，确保<code>rotary_dim</code>参数的数据类型是DeepSpeed支持的类型之一。</li><li>如果<code>rotary_dim</code>参数的值不是DeepSpeed支持的类型，尝试将值转换为支持的类型。</li><li>检查DeepSpeed的文档，了解如何配置或调整以支持<code>DT_BFLOAT16</code>类型。</li><li>联系DeepSpeed或MindSpore的支持团队，寻求帮助解决这个特定问题。<br> 由于这涉及到具体的代码和库配置，最直接的方法是联系项目的开发者或社区，他们可能提供更具体的解决方案或工作around。</li></ul> 
<p>哎嘿 910 计算芯片版本 不支持 DT_BFLOAT16。所以我们要改deepspeed的配置文件。这时候最绝望的事情来了。写到最后发现一个无法逾越的问题。</p> 
<pre><code class="prism language-bash">Traceback <span class="token punctuation">(</span>most recent call last<span class="token punctuation">)</span>:
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/bin/llamafactory-cli"</span>, line <span class="token number">8</span>, <span class="token keyword">in</span> <span class="token operator">&lt;</span>module<span class="token operator">&gt;</span>
    sys.exit<span class="token punctuation">(</span>main<span class="token punctuation">(</span><span class="token punctuation">))</span>
  File <span class="token string">"/tmp/code/LLaMA-Factory/src/llamafactory/cli.py"</span>, line <span class="token number">110</span>, <span class="token keyword">in</span> main
    run_exp<span class="token punctuation">(</span><span class="token punctuation">)</span>
  File <span class="token string">"/tmp/code/LLaMA-Factory/src/llamafactory/train/tuner.py"</span>, line <span class="token number">47</span>, <span class="token keyword">in</span> run_exp
    run_sft<span class="token punctuation">(</span>model_args, data_args, training_args, finetuning_args, generating_args, callbacks<span class="token punctuation">)</span>
  File <span class="token string">"/tmp/code/LLaMA-Factory/src/llamafactory/train/sft/workflow.py"</span>, line <span class="token number">88</span>, <span class="token keyword">in</span> run_sft
    train_result <span class="token operator">=</span> trainer.train<span class="token punctuation">(</span>resume_from_checkpoint<span class="token operator">=</span>training_args.resume_from_checkpoint<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/transformers/trainer.py"</span>, line <span class="token number">1932</span>, <span class="token keyword">in</span> train
    <span class="token builtin class-name">return</span> inner_training_loop<span class="token punctuation">(</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/transformers/trainer.py"</span>, line <span class="token number">2268</span>, <span class="token keyword">in</span> _inner_training_loop
    tr_loss_step <span class="token operator">=</span> self.training_step<span class="token punctuation">(</span>model, inputs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/transformers/trainer.py"</span>, line <span class="token number">3307</span>, <span class="token keyword">in</span> training_step
    loss <span class="token operator">=</span> self.compute_loss<span class="token punctuation">(</span>model, inputs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/transformers/trainer.py"</span>, line <span class="token number">3338</span>, <span class="token keyword">in</span> compute_loss
    outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>**inputs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/nn/modules/module.py"</span>, line <span class="token number">1518</span>, <span class="token keyword">in</span> _wrapped_call_impl
    <span class="token builtin class-name">return</span> self._call_impl<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/nn/modules/module.py"</span>, line <span class="token number">1527</span>, <span class="token keyword">in</span> _call_impl
    <span class="token builtin class-name">return</span> forward_call<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/accelerate/utils/operations.py"</span>, line <span class="token number">819</span>, <span class="token keyword">in</span> forward
    <span class="token builtin class-name">return</span> model_forward<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/accelerate/utils/operations.py"</span>, line <span class="token number">807</span>, <span class="token keyword">in</span> __call__
    <span class="token builtin class-name">return</span> convert_to_fp32<span class="token punctuation">(</span>self.model_forward<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">))</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/amp/autocast_mode.py"</span>, line <span class="token number">16</span>, <span class="token keyword">in</span> decorate_autocast
    <span class="token builtin class-name">return</span> func<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py"</span>, line <span class="token number">1221</span>, <span class="token keyword">in</span> forward
    outputs <span class="token operator">=</span> self.model<span class="token punctuation">(</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/nn/modules/module.py"</span>, line <span class="token number">1518</span>, <span class="token keyword">in</span> _wrapped_call_impl
    <span class="token builtin class-name">return</span> self._call_impl<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/nn/modules/module.py"</span>, line <span class="token number">1527</span>, <span class="token keyword">in</span> _call_impl
    <span class="token builtin class-name">return</span> forward_call<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py"</span>, line <span class="token number">1012</span>, <span class="token keyword">in</span> forward
    layer_outputs <span class="token operator">=</span> self._gradient_checkpointing_func<span class="token punctuation">(</span>
  File <span class="token string">"/tmp/code/LLaMA-Factory/src/llamafactory/model/model_utils/checkpointing.py"</span>, line <span class="token number">65</span>, <span class="token keyword">in</span> custom_gradient_checkpointing_func
    <span class="token builtin class-name">return</span> gradient_checkpointing_func<span class="token punctuation">(</span>func, *args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/_compile.py"</span>, line <span class="token number">24</span>, <span class="token keyword">in</span> inner
    <span class="token builtin class-name">return</span> torch._dynamo.disable<span class="token punctuation">(</span>fn, recursive<span class="token punctuation">)</span><span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py"</span>, line <span class="token number">328</span>, <span class="token keyword">in</span> _fn
    <span class="token builtin class-name">return</span> fn<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/_dynamo/external_utils.py"</span>, line <span class="token number">17</span>, <span class="token keyword">in</span> inner
    <span class="token builtin class-name">return</span> fn<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/utils/checkpoint.py"</span>, line <span class="token number">451</span>, <span class="token keyword">in</span> checkpoint
    <span class="token builtin class-name">return</span> CheckpointFunction.apply<span class="token punctuation">(</span>function, preserve, *args<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/autograd/function.py"</span>, line <span class="token number">539</span>, <span class="token keyword">in</span> apply
    <span class="token builtin class-name">return</span> super<span class="token punctuation">(</span><span class="token punctuation">)</span>.apply<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>  <span class="token comment"># type: ignore[misc]</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/utils/checkpoint.py"</span>, line <span class="token number">230</span>, <span class="token keyword">in</span> forward
    outputs <span class="token operator">=</span> run_function<span class="token punctuation">(</span>*args<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/nn/modules/module.py"</span>, line <span class="token number">1518</span>, <span class="token keyword">in</span> _wrapped_call_impl
    <span class="token builtin class-name">return</span> self._call_impl<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/nn/modules/module.py"</span>, line <span class="token number">1527</span>, <span class="token keyword">in</span> _call_impl
    <span class="token builtin class-name">return</span> forward_call<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py"</span>, line <span class="token number">763</span>, <span class="token keyword">in</span> forward
    hidden_states, self_attn_weights, present_key_value <span class="token operator">=</span> self.self_attn<span class="token punctuation">(</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/nn/modules/module.py"</span>, line <span class="token number">1518</span>, <span class="token keyword">in</span> _wrapped_call_impl
    <span class="token builtin class-name">return</span> self._call_impl<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/nn/modules/module.py"</span>, line <span class="token number">1527</span>, <span class="token keyword">in</span> _call_impl
    <span class="token builtin class-name">return</span> forward_call<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py"</span>, line <span class="token number">257</span>, <span class="token keyword">in</span> forward
    query_states <span class="token operator">=</span> self.q_proj<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/nn/modules/module.py"</span>, line <span class="token number">1518</span>, <span class="token keyword">in</span> _wrapped_call_impl
    <span class="token builtin class-name">return</span> self._call_impl<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/nn/modules/module.py"</span>, line <span class="token number">1527</span>, <span class="token keyword">in</span> _call_impl
    <span class="token builtin class-name">return</span> forward_call<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/nn/modules/linear.py"</span>, line <span class="token number">114</span>, <span class="token keyword">in</span> forward
    <span class="token builtin class-name">return</span> F.linear<span class="token punctuation">(</span>input, self.weight, self.bias<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py"</span>, line <span class="token number">111</span>, <span class="token keyword">in</span> zero3_linear_wrap
    <span class="token builtin class-name">return</span> LinearFunctionForZeroStage3.apply<span class="token punctuation">(</span>input, weight, bias<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch/autograd/function.py"</span>, line <span class="token number">539</span>, <span class="token keyword">in</span> apply
    <span class="token builtin class-name">return</span> super<span class="token punctuation">(</span><span class="token punctuation">)</span>.apply<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>  <span class="token comment"># type: ignore[misc]</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/torch_npu/npu/amp/autocast_mode.py"</span>, line <span class="token number">113</span>, <span class="token keyword">in</span> decorate_fwd
    <span class="token builtin class-name">return</span> fwd<span class="token punctuation">(</span>*args, **kwargs<span class="token punctuation">)</span>
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py"</span>, line <span class="token number">59</span>, <span class="token keyword">in</span> forward
    output <span class="token operator">+=</span> bias
RuntimeError: call aclnnInplaceAdd failed, detail:EZ1001: <span class="token number">2024</span>-07-09-10:40:00.116.800 the size of tensor selfRef <span class="token punctuation">[</span><span class="token number">1,120</span><span class="token punctuation">]</span> must match the size of tensor other <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>.
        TraceBack <span class="token punctuation">(</span>most recent call last<span class="token punctuation">)</span>:
        <span class="token number">120</span> and <span class="token number">0</span> cannot broadcast.
        the size of tensor selfRef <span class="token punctuation">[</span><span class="token number">1,120</span><span class="token punctuation">]</span> must match the size of tensor other <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>.

<span class="token punctuation">[</span>ERROR<span class="token punctuation">]</span> <span class="token number">2024</span>-07-09-10:40:00 <span class="token punctuation">(</span>PID:21727, Device:0, RankID:0<span class="token punctuation">)</span> ERR01005 OPS internal error
</code></pre> 
<p>更换成qwen2-7B进行微调训练出现了tensor 不匹配的问题。对异常日志进行解读。<br> 从日志来看，报错的原因是发生了张量操作的维度不匹配。具体来说，错误信息 <code>the size of tensor selfRef [1,120] must match the size of tensor other [0]</code> 表示在进行 <code>aclnnInplaceAdd</code> 操作时，一个张量的维度是 <code>[1,120]</code>，另一个张量的维度是 <code>[0]</code>，导致无法进行广播操作。这通常是由于数据输入的形状或大小设置不正确引起的。以下是详细的解读及可能的解决方案：</p> 
<h4><a id="_572"></a>错误日志解读</h4> 
<ol><li> <p><strong>主函数调用</strong>：</p> <pre><code class="prism language-python">sys<span class="token punctuation">.</span>exit<span class="token punctuation">(</span>main<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <p>程序从主函数 <code>main</code> 开始执行。</p> </li><li> <p><strong>执行实验</strong>：</p> <pre><code class="prism language-python">run_exp<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> <p>在 <code>run_exp</code> 函数中运行实验。</p> </li><li> <p><strong>运行 SFT（Supervised Fine-Tuning）</strong>：</p> <pre><code class="prism language-python">run_sft<span class="token punctuation">(</span>model_args<span class="token punctuation">,</span> data_args<span class="token punctuation">,</span> training_args<span class="token punctuation">,</span> finetuning_args<span class="token punctuation">,</span> generating_args<span class="token punctuation">,</span> callbacks<span class="token punctuation">)</span>
</code></pre> <p>在 <code>run_sft</code> 中进行模型的监督微调。</p> </li><li> <p><strong>训练模型</strong>：</p> <pre><code class="prism language-python">train_result <span class="token operator">=</span> trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span>resume_from_checkpoint<span class="token operator">=</span>training_args<span class="token punctuation">.</span>resume_from_checkpoint<span class="token punctuation">)</span>
</code></pre> <p>在 <code>trainer.train</code> 中进行训练，并可能从检查点恢复训练。</p> </li><li> <p><strong>训练步骤</strong>：</p> <pre><code class="prism language-python">tr_loss_step <span class="token operator">=</span> self<span class="token punctuation">.</span>training_step<span class="token punctuation">(</span>model<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span>
</code></pre> <p>进行训练的单步操作 <code>training_step</code>。</p> </li><li> <p><strong>计算损失</strong>：</p> <pre><code class="prism language-python">loss <span class="token operator">=</span> self<span class="token punctuation">.</span>compute_loss<span class="token punctuation">(</span>model<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span>
</code></pre> <p>计算模型的损失。</p> </li><li> <p><strong>模型前向传播</strong>：</p> <pre><code class="prism language-python">outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span>
</code></pre> <p>进行模型的前向传播。</p> </li><li> <p><strong>深度学习库内部调用</strong>：<br> 这里涉及多个内部函数调用，最终在 <code>aclnnInplaceAdd</code> 时出错：</p> <pre><code class="prism language-python">RuntimeError<span class="token punctuation">:</span> call aclnnInplaceAdd failed<span class="token punctuation">,</span> detail<span class="token punctuation">:</span>EZ1001<span class="token punctuation">:</span> <span class="token number">2024</span><span class="token operator">-</span><span class="token number">07</span><span class="token operator">-</span><span class="token number">09</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">:</span><span class="token number">40</span><span class="token punctuation">:</span><span class="token number">00.116</span><span class="token number">.800</span> the size of tensor selfRef <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">120</span><span class="token punctuation">]</span> must <span class="token keyword">match</span> the size of tensor other <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>
</code></pre> </li></ol> 
<h4><a id="_622"></a>可能的解决方案</h4> 
<ol><li> <p><strong>检查数据输入</strong>：</p> 
  <ul><li>确保输入数据的形状和大小正确。尤其是在数据预处理步骤中，确认数据没有丢失或者形状不匹配。</li></ul> </li><li> <p><strong>模型配置检查</strong>：</p> 
  <ul><li>检查模型的配置，尤其是线性层（如 <code>self.q_proj</code>）的输入输出维度是否与数据匹配。</li></ul> </li><li> <p><strong>检查自定义函数</strong>：</p> 
  <ul><li>如果有自定义的梯度检查点函数 <code>custom_gradient_checkpointing_func</code>，确保其实现正确，并且不会改变输入数据的形状。</li></ul> </li><li> <p><strong>更新库和框架</strong>：</p> 
  <ul><li>确保使用的库（如 <code>transformers</code>, <code>torch</code>, <code>deepspeed</code> 等）是最新版本，因为新版本可能包含错误修复和改进。</li></ul> </li><li> <p><strong>调试信息</strong>：</p> 
  <ul><li>在模型前向传播的关键步骤添加调试信息，打印张量的形状以便确定错误发生的位置和原因。</li></ul> </li></ol> 
<p>具体到这个错误，可以首先检查 <code>self.q_proj</code> 的输入 <code>hidden_states</code> 的形状，并在出错前打印相关张量的形状，确保其维度匹配。如果问题仍然存在，建议进一步简化代码并逐步调试，以确定确切的错误原因。<br> 接下来我们去除掉deepspeed配置项。<br> 发生以下异常</p> 
<pre><code class="prism language-bash">Traceback <span class="token punctuation">(</span>most recent call last<span class="token punctuation">)</span>:
  File <span class="token string">"/home/ma-user/anaconda3/envs/MindSpore/bin/llamafactory-cli"</span>, line <span class="token number">8</span>, <span class="token keyword">in</span> <span class="token operator">&lt;</span>module<span class="token operator">&gt;</span>
    sys.exit<span class="token punctuation">(</span>main<span class="token punctuation">(</span><span class="token punctuation">))</span>
  File <span class="token string">"/tmp/code/LLaMA-Factory/src/llamafactory/cli.py"</span>, line <span class="token number">110</span>, <span class="token keyword">in</span> main
    run_exp<span class="token punctuation">(</span><span class="token punctuation">)</span>
  File <span class="token string">"/tmp/code/LLaMA-Factory/src/llamafactory/train/tuner.py"</span>, line <span class="token number">47</span>, <span class="token keyword">in</span> run_exp
    run_sft<span class="token punctuation">(</span>model_args, data_args, training_args, finetuning_args, generating_args, callbacks<span class="token punctuation">)</span>
  File <span class="token string">"/tmp/code/LLaMA-Factory/src/llamafactory/train/sft/workflow.py"</span>, line <span class="token number">49</span>, <span class="token keyword">in</span> run_sft
    model <span class="token operator">=</span> load_model<span class="token punctuation">(</span>tokenizer, model_args, finetuning_args, training_args.do_train<span class="token punctuation">)</span>
  File <span class="token string">"/tmp/code/LLaMA-Factory/src/llamafactory/model/loader.py"</span>, line <span class="token number">160</span>, <span class="token keyword">in</span> load_model
    model <span class="token operator">=</span> init_adapter<span class="token punctuation">(</span>config, model, model_args, finetuning_args, is_trainable<span class="token punctuation">)</span>
  File <span class="token string">"/tmp/code/LLaMA-Factory/src/llamafactory/model/adapter.py"</span>, line <span class="token number">306</span>, <span class="token keyword">in</span> init_adapter
    _setup_full_tuning<span class="token punctuation">(</span>model, model_args, finetuning_args, is_trainable, cast_trainable_params_to_fp32<span class="token punctuation">)</span>
  File <span class="token string">"/tmp/code/LLaMA-Factory/src/llamafactory/model/adapter.py"</span>, line <span class="token number">59</span>, <span class="token keyword">in</span> _setup_full_tuning
    param.data <span class="token operator">=</span> param.data.to<span class="token punctuation">(</span>torch.float32<span class="token punctuation">)</span>
RuntimeError: NPU out of memory. Tried to allocate <span class="token number">2.03</span> GiB <span class="token punctuation">(</span>NPU <span class="token number">0</span><span class="token punctuation">;</span> <span class="token number">32.00</span> GiB total capacity<span class="token punctuation">;</span> <span class="token number">29.19</span> GiB already allocated<span class="token punctuation">;</span> <span class="token number">29.19</span> GiB current active<span class="token punctuation">;</span> <span class="token number">412.09</span> MiB <span class="token function">free</span><span class="token punctuation">;</span> <span class="token number">30.43</span> GiB reserved <span class="token keyword">in</span> total by PyTorch<span class="token punctuation">)</span> If reserved memory is <span class="token operator">&gt;&gt;</span> allocated memory try setting max_split_size_mb to avoid fragmentation.
</code></pre> 
<p>到时间了 重新想办法 今天必须把这个代码跑通</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a30f956c2d6bfc699b35dae15c6f6363/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">C语言相关知识点（不定期更新内容）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e3aa0dd72652ba38e34a202f9ddef2e0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">主流大数据调度工具DolphinScheduler之数据ETL流程</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>