<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ã€LLMæ•™ç¨‹-llamaã€‘å¦‚ä½•Fine Tuningå¤§è¯­è¨€æ¨¡å‹ï¼Ÿ - ç¼–ç¨‹å¤§å’–</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/0ce0196c0aef1ba1ab95a222b39454f1/">
  <meta property="og:site_name" content="ç¼–ç¨‹å¤§å’–">
  <meta property="og:title" content="ã€LLMæ•™ç¨‹-llamaã€‘å¦‚ä½•Fine Tuningå¤§è¯­è¨€æ¨¡å‹ï¼Ÿ">
  <meta property="og:description" content="ä»Šå¤©ç»™å¤§å®¶å¸¦æ¥äº†ä¸€ç¯‡è¶…çº§è¯¦ç»†çš„æ•™ç¨‹,æ‰‹æŠŠæ‰‹æ•™ä½ å¦‚ä½•å¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒ(Fine Tuning)ï¼ï¼ˆä»£ç å’Œè¯¦ç»†è§£é‡Šæ”¾åœ¨åæ–‡ï¼‰
ç›®å½•
å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒ(Fine Tuning)éœ€è¦å“ªäº›æ­¥éª¤ï¼Ÿ
å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒ(Fine Tuning)è®­ç»ƒè¿‡ç¨‹åŠä»£ç 
å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒ(Fine Tuning)éœ€è¦å“ªäº›æ­¥éª¤ï¼Ÿ å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒ(Fine Tuning)çš„ä¸»è¦æ­¥éª¤ğŸ¤©
ğŸ“š å‡†å¤‡è®­ç»ƒæ•°æ®é›†
é¦–å…ˆä½ éœ€è¦å‡†å¤‡ä¸€ä¸ªé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†,æœ€å¥½æ˜¯ä¸ä½ çš„åº”ç”¨åœºæ™¯ç›¸å…³çš„æ•°æ®ã€‚å¯ä»¥æ˜¯æ–‡æœ¬æ•°æ®ã€å¯¹è¯æ•°æ®ç­‰,æ ¼å¼ä¸€èˆ¬ä¸ºJSON/TXTç­‰ã€‚
ğŸ“¦ é€‰æ‹©åˆé€‚çš„åŸºç¡€æ¨¡å‹
æ¥ä¸‹æ¥éœ€è¦é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹,ä½œä¸ºå¾®è°ƒçš„èµ·ç‚¹ã€‚å¸¸è§çš„æœ‰GPTã€BERTã€T5ç­‰å¤§æ¨¡å‹,å¯æ ¹æ®ä»»åŠ¡åœºæ™¯è¿›è¡Œé€‰æ‹©ã€‚
âš™ï¸ è®¾ç½®è®­ç»ƒè¶…å‚æ•°
ç„¶åæ˜¯è®¾ç½®è®­ç»ƒçš„å„ç§è¶…å‚æ•°,æ¯”å¦‚å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°ã€è®­ç»ƒæ­¥æ•°ç­‰ç­‰ã€‚é€‰æ‹©åˆç†çš„è¶…å‚æ•°å¯¹æ¨¡å‹æ•ˆæœå½±å“å¾ˆå¤§å“¦ã€‚
ğŸ§‘â€ğŸ’» åŠ è½½æ¨¡å‹å’Œæ•°æ®é›†
ä½¿ç”¨HuggingFaceç­‰åº“,æŠŠé€‰å®šçš„åŸºç¡€æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†åŠ è½½è¿›æ¥ã€‚è®°å¾—å¯¹æ•°æ®é›†è¿›è¡Œå¿…è¦çš„å‰å¤„ç†å’Œåˆ’åˆ†ã€‚
âš¡ å¼€å§‹æ¨¡å‹å¾®è°ƒè®­ç»ƒ
æœ‰äº†æ¨¡å‹ã€æ•°æ®é›†å’Œè¶…å‚æ•°å,å°±å¯ä»¥å¼€å§‹æ¨¡å‹å¾®è°ƒè®­ç»ƒäº†!å¯ä»¥ä½¿ç”¨PyTorch/TensorFlowç­‰æ¡†æ¶è¿›è¡Œè®­ç»ƒã€‚
ğŸ’¾ ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹
è®­ç»ƒç»“æŸå,åˆ«å¿˜äº†æŠŠå¾®è°ƒå¥½çš„æ¨¡å‹ä¿å­˜ä¸‹æ¥,æ–¹ä¾¿åç»­åŠ è½½ä½¿ç”¨å“¦ã€‚
ğŸ§ª åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹
æœ€ååœ¨å‡†å¤‡å¥½çš„æµ‹è¯•é›†ä¸Šè¯„ä¼°ä¸€ä¸‹å¾®è°ƒåæ¨¡å‹çš„æ•ˆæœã€‚çœ‹çœ‹ä¸ä¹‹å‰çš„åŸºç¡€æ¨¡å‹ç›¸æ¯”,æ˜¯å¦æœ‰æ˜æ˜¾æå‡?
å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒ(Fine Tuning)è®­ç»ƒè¿‡ç¨‹åŠä»£ç  é‚£å¦‚ä½•ä½¿ç”¨ Lamini åº“åŠ è½½æ•°æ®ã€è®¾ç½®æ¨¡å‹å’Œè®­ç»ƒè¶…å‚æ•°ã€å®šä¹‰æ¨ç†å‡½æ•°ã€å¾®è°ƒåŸºç¡€æ¨¡å‹ã€è¯„ä¼°æ¨¡å‹æ•ˆæœå‘¢ï¼Ÿ
é¦–å…ˆï¼Œå¯¼å…¥å¿…è¦çš„åº“ import os import lamini import datasets import tempfile import logging import random import config import os import yaml import time import torch import transformers import pandas as pd import jsonlines from utilities import * from transformers import AutoTokenizer from transformers import AutoModelForCausalLM from transformers import TrainingArguments from transformers import AutoModelForCausalLM from llama import BasicModelRunner è¿™éƒ¨åˆ†å¯¼å…¥äº†ä¸€äº›å¿…éœ€çš„Pythonåº“,åŒ…æ‹¬Laminiã€Hugging Faceçš„Datasetsã€Transformersç­‰ã€‚">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-01T22:03:16+08:00">
    <meta property="article:modified_time" content="2024-07-01T22:03:16+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="ç¼–ç¨‹å¤§å’–" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">ç¼–ç¨‹å¤§å’–</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ã€LLMæ•™ç¨‹-llamaã€‘å¦‚ä½•Fine Tuningå¤§è¯­è¨€æ¨¡å‹ï¼Ÿ</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>ä»Šå¤©ç»™å¤§å®¶å¸¦æ¥äº†ä¸€ç¯‡è¶…çº§è¯¦ç»†çš„æ•™ç¨‹,æ‰‹æŠŠæ‰‹æ•™ä½ å¦‚ä½•å¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒ(Fine Tuning)ï¼ï¼ˆä»£ç å’Œè¯¦ç»†è§£é‡Šæ”¾åœ¨åæ–‡ï¼‰</p> 
<p id="main-toc"><strong>ç›®å½•</strong></p> 
<p id="%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83(Fine%20Tuning)%E9%9C%80%E8%A6%81%E5%93%AA%E4%BA%9B%E6%AD%A5%E9%AA%A4%EF%BC%9F-toc" style="margin-left:40px;"><a href="#%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83%28Fine%20Tuning%29%E9%9C%80%E8%A6%81%E5%93%AA%E4%BA%9B%E6%AD%A5%E9%AA%A4%EF%BC%9F" rel="nofollow">å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒ(Fine Tuning)éœ€è¦å“ªäº›æ­¥éª¤ï¼Ÿ</a></p> 
<p id="%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83(Fine%20Tuning)%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E5%8F%8A%E4%BB%A3%E7%A0%81-toc" style="margin-left:40px;"><a href="#%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83%28Fine%20Tuning%29%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E5%8F%8A%E4%BB%A3%E7%A0%81" rel="nofollow">å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒ(Fine Tuning)è®­ç»ƒè¿‡ç¨‹åŠä»£ç </a></p> 
<hr id="hr-toc"> 
<h3>å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒ(Fine Tuning)éœ€è¦å“ªäº›æ­¥éª¤ï¼Ÿ</h3> 
<p>å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒ(Fine Tuning)çš„ä¸»è¦æ­¥éª¤ğŸ¤©</p> 
<ol><li> <p>ğŸ“š å‡†å¤‡è®­ç»ƒæ•°æ®é›†<br> é¦–å…ˆä½ éœ€è¦å‡†å¤‡ä¸€ä¸ªé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†,æœ€å¥½æ˜¯ä¸ä½ çš„åº”ç”¨åœºæ™¯ç›¸å…³çš„æ•°æ®ã€‚å¯ä»¥æ˜¯æ–‡æœ¬æ•°æ®ã€å¯¹è¯æ•°æ®ç­‰,æ ¼å¼ä¸€èˆ¬ä¸ºJSON/TXTç­‰ã€‚</p> </li><li> <p>ğŸ“¦ é€‰æ‹©åˆé€‚çš„åŸºç¡€æ¨¡å‹<br> æ¥ä¸‹æ¥éœ€è¦é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹,ä½œä¸ºå¾®è°ƒçš„èµ·ç‚¹ã€‚å¸¸è§çš„æœ‰GPTã€BERTã€T5ç­‰å¤§æ¨¡å‹,å¯æ ¹æ®ä»»åŠ¡åœºæ™¯è¿›è¡Œé€‰æ‹©ã€‚</p> </li><li> <p>âš™ï¸ è®¾ç½®è®­ç»ƒè¶…å‚æ•°<br> ç„¶åæ˜¯è®¾ç½®è®­ç»ƒçš„å„ç§è¶…å‚æ•°,æ¯”å¦‚å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°ã€è®­ç»ƒæ­¥æ•°ç­‰ç­‰ã€‚é€‰æ‹©åˆç†çš„è¶…å‚æ•°å¯¹æ¨¡å‹æ•ˆæœå½±å“å¾ˆå¤§å“¦ã€‚</p> </li><li> <p>ğŸ§‘â€ğŸ’» åŠ è½½æ¨¡å‹å’Œæ•°æ®é›†<br> ä½¿ç”¨HuggingFaceç­‰åº“,æŠŠé€‰å®šçš„åŸºç¡€æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†åŠ è½½è¿›æ¥ã€‚è®°å¾—å¯¹æ•°æ®é›†è¿›è¡Œå¿…è¦çš„å‰å¤„ç†å’Œåˆ’åˆ†ã€‚</p> </li><li> <p>âš¡ å¼€å§‹æ¨¡å‹å¾®è°ƒè®­ç»ƒ<br> æœ‰äº†æ¨¡å‹ã€æ•°æ®é›†å’Œè¶…å‚æ•°å,å°±å¯ä»¥å¼€å§‹æ¨¡å‹å¾®è°ƒè®­ç»ƒäº†!å¯ä»¥ä½¿ç”¨PyTorch/TensorFlowç­‰æ¡†æ¶è¿›è¡Œè®­ç»ƒã€‚</p> </li><li> <p>ğŸ’¾ ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹<br> è®­ç»ƒç»“æŸå,åˆ«å¿˜äº†æŠŠå¾®è°ƒå¥½çš„æ¨¡å‹ä¿å­˜ä¸‹æ¥,æ–¹ä¾¿åç»­åŠ è½½ä½¿ç”¨å“¦ã€‚</p> </li><li> <p>ğŸ§ª åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹<br> æœ€ååœ¨å‡†å¤‡å¥½çš„æµ‹è¯•é›†ä¸Šè¯„ä¼°ä¸€ä¸‹å¾®è°ƒåæ¨¡å‹çš„æ•ˆæœã€‚çœ‹çœ‹ä¸ä¹‹å‰çš„åŸºç¡€æ¨¡å‹ç›¸æ¯”,æ˜¯å¦æœ‰æ˜æ˜¾æå‡?</p> </li></ol> 
<h3 id="%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83(Fine%20Tuning)%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E5%8F%8A%E4%BB%A3%E7%A0%81">å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒ(Fine Tuning)è®­ç»ƒè¿‡ç¨‹åŠä»£ç </h3> 
<p>é‚£å¦‚ä½•ä½¿ç”¨ Lamini åº“åŠ è½½æ•°æ®ã€è®¾ç½®æ¨¡å‹å’Œè®­ç»ƒè¶…å‚æ•°ã€å®šä¹‰æ¨ç†å‡½æ•°ã€å¾®è°ƒåŸºç¡€æ¨¡å‹ã€è¯„ä¼°æ¨¡å‹æ•ˆæœå‘¢ï¼Ÿ</p> 
<ul><li><strong>é¦–å…ˆï¼Œå¯¼å…¥å¿…è¦çš„åº“</strong></li></ul> 
<pre><code>import os
import lamini
import datasets
import tempfile
import logging
import random
import config
import os
import yaml
import time
import torch
import transformers
import pandas as pd
import jsonlines

from utilities import *
from transformers import AutoTokenizer
from transformers import AutoModelForCausalLM
from transformers import TrainingArguments
from transformers import AutoModelForCausalLM
from llama import BasicModelRunner
</code></pre> 
<p>è¿™éƒ¨åˆ†å¯¼å…¥äº†ä¸€äº›å¿…éœ€çš„Pythonåº“,åŒ…æ‹¬Laminiã€Hugging Faceçš„Datasetsã€Transformersç­‰ã€‚</p> 
<ul><li><strong>åŠ è½½Laminiæ–‡æ¡£æ•°æ®é›†</strong></li></ul> 
<pre><code>dataset_name = "lamini_docs.jsonl"
dataset_path = f"/content/{dataset_name}"
use_hf = False
dataset_path = "lamini/lamini_docs"
use_hf = True
</code></pre> 
<p>è¿™é‡ŒæŒ‡å®šäº†æ•°æ®é›†çš„è·¯å¾„,åŒæ—¶è®¾ç½®äº†<code>use_hf</code>æ ‡å¿—,è¡¨ç¤ºæ˜¯å¦ä½¿ç”¨Hugging Faceçš„Datasetsåº“åŠ è½½æ•°æ®ã€‚</p> 
<ul><li><strong>è®¾ç½®æ¨¡å‹ã€è®­ç»ƒé…ç½®å’Œåˆ†è¯å™¨</strong></li></ul> 
<pre><code>model_name = "EleutherAI/pythia-70m"
training_config = { ... }
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)
</code></pre> 
<p>è¿™éƒ¨åˆ†æŒ‡å®šäº†åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹çš„åç§°,å¹¶è®¾ç½®äº†è®­ç»ƒé…ç½®(å¦‚æœ€å¤§é•¿åº¦ç­‰)ã€‚ç„¶å,å®ƒä½¿ç”¨<code>AutoTokenizer</code>ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½åˆ†è¯å™¨,å¹¶å¯¹åˆ†è¯å™¨è¿›è¡Œäº†ä¸€äº›è°ƒæ•´ã€‚æœ€å,å®ƒè°ƒç”¨<code>tokenize_and_split_data</code>å‡½æ•°å¯¹æ•°æ®è¿›è¡Œåˆ†è¯å’Œåˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†ã€‚</p> 
<ul><li><strong>åŠ è½½åŸºç¡€æ¨¡å‹</strong></li></ul> 
<pre><code>base_model = AutoModelForCausalLM.from_pretrained(model_name)
device_count = torch.cuda.device_count()
if device_count &gt; 0:
    device = torch.device("cuda")
else:
    device = torch.device("cpu")
base_model.to(device)
</code></pre> 
<p>è¿™é‡Œä½¿ç”¨<code>AutoModelForCausalLM</code>ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½åŸºç¡€æ¨¡å‹,å¹¶æ ¹æ®è®¾å¤‡(GPUæˆ–CPU)å°†æ¨¡å‹ç§»åŠ¨åˆ°ç›¸åº”çš„è®¾å¤‡ä¸Šã€‚</p> 
<ul><li><strong>å®šä¹‰æ¨ç†å‡½æ•°</strong></li></ul> 
<pre><code>def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):
    ...
</code></pre> 
<p>è¿™ä¸ªå‡½æ•°ç”¨äºåœ¨ç»™å®šè¾“å…¥æ–‡æœ¬çš„æƒ…å†µä¸‹,ä½¿ç”¨æ¨¡å‹å’Œåˆ†è¯å™¨è¿›è¡Œæ¨ç†å¹¶ç”Ÿæˆè¾“å‡ºã€‚å®ƒåŒ…æ‹¬å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯ã€ä½¿ç”¨æ¨¡å‹ç”Ÿæˆè¾“å‡ºä»¥åŠè§£ç è¾“å‡ºç­‰æ­¥éª¤ã€‚</p> 
<ul><li><strong>å°è¯•ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œæ¨ç†</strong></li></ul> 
<pre><code>test_text = test_dataset[0]['question']
print("Question input (test):", test_text)
print(f"Correct answer from Lamini docs: {test_dataset[0]['answer']}")
print("Model's answer: ")
print(inference(test_text, base_model, tokenizer))
</code></pre> 
<p>è¿™éƒ¨åˆ†ä½¿ç”¨ä¸Šä¸€æ­¥å®šä¹‰çš„<code>inference</code>å‡½æ•°,åœ¨æµ‹è¯•æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªç¤ºä¾‹ä¸Šå°è¯•ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚å®ƒæ‰“å°äº†è¾“å…¥é—®é¢˜ã€æ­£ç¡®ç­”æ¡ˆå’Œæ¨¡å‹çš„è¾“å‡ºã€‚</p> 
<ul><li><strong>è®¾ç½®è®­ç»ƒå‚æ•°</strong></li></ul> 
<pre><code>max_steps = 3
trained_model_name = f"lamini_docs_{max_steps}_steps"
output_dir = trained_model_name
training_args = TrainingArguments(

  # Learning rate
  learning_rate=1.0e-5,

  # Number of training epochs
  num_train_epochs=1,

  # Max steps to train for (each step is a batch of data)
  # Overrides num_train_epochs, if not -1
  max_steps=max_steps,

  # Batch size for training
  per_device_train_batch_size=1,

  # Directory to save model checkpoints
  output_dir=output_dir,

  # Other arguments
  overwrite_output_dir=False, # Overwrite the content of the output directory
  disable_tqdm=False, # Disable progress bars
  eval_steps=120, # Number of update steps between two evaluations
  save_steps=120, # After # steps model is saved
  warmup_steps=1, # Number of warmup steps for learning rate scheduler
  per_device_eval_batch_size=1, # Batch size for evaluation
  evaluation_strategy="steps",
  logging_strategy="steps",
  logging_steps=1,
  optim="adafactor",
  gradient_accumulation_steps = 4,
  gradient_checkpointing=False,

  # Parameters for early stopping
  load_best_model_at_end=True,
  save_total_limit=1,
  metric_for_best_model="eval_loss",
  greater_is_better=False
)
</code></pre> 
<p>è¿™ä¸€éƒ¨åˆ†è®¾ç½®äº†è®­ç»ƒçš„ä¸€äº›å‚æ•°,åŒ…æ‹¬æœ€å¤§è®­ç»ƒæ­¥æ•°ã€è¾“å‡ºæ¨¡å‹ç›®å½•ã€å­¦ä¹ ç‡ç­‰è¶…å‚æ•°ã€‚</p> 
<blockquote> 
 <p>ä¸ºä»€ä¹ˆè¦è¿™æ ·è®¾ç½®è¿™äº›è®­ç»ƒè¶…å‚æ•°:</p> 
 <ol><li> <p><code>learning_rate=1.0e-5</code><br> å­¦ä¹ ç‡æ§åˆ¶äº†æ¨¡å‹åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ çš„é€Ÿåº¦ã€‚1e-5æ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„å­¦ä¹ ç‡,å¯ä»¥æœ‰åŠ©äºç¨³å®šè®­ç»ƒè¿‡ç¨‹,é˜²æ­¢å‡ºç°divergence(å‘æ•£)çš„æƒ…å†µã€‚</p> </li><li> <p><code>num_train_epochs=1</code><br> è®­ç»ƒçš„è½®æ•°,å³è®©æ•°æ®åœ¨æ¨¡å‹ä¸Šå¾ªç¯å¤šå°‘æ¬¡ã€‚è¿™é‡Œè®¾ç½®ä¸º1,æ˜¯å› ä¸ºæˆ‘ä»¬åªæƒ³è¿›è¡Œè½»å¾®çš„å¾®è°ƒ,é¿å…è¿‡åº¦è®­ç»ƒ(overfitting)ã€‚</p> </li><li> <p><code>max_steps=max_steps</code><br> æœ€å¤§è®­ç»ƒæ­¥æ•°,ä¼šè¦†ç›–<code>num_train_epochs</code>ã€‚è¿™æ ·å¯ä»¥æ›´å¥½åœ°æ§åˆ¶è®­ç»ƒçš„æ€»æ­¥æ•°ã€‚</p> </li><li> <p><code>per_device_train_batch_size=1</code><br> æ¯ä¸ªè®¾å¤‡(GPU/CPU)ä¸Šçš„è®­ç»ƒæ‰¹é‡å¤§å°ã€‚æ‰¹é‡å¤§å°è¶Šå¤§,å†…å­˜å ç”¨è¶Šé«˜,ä½†è®­ç»ƒè¿‡ç¨‹å¯èƒ½æ›´åŠ ç¨³å®šã€‚</p> </li><li> <p><code>output_dir=output_dir</code><br> ç”¨äºä¿å­˜è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ£€æŸ¥ç‚¹(checkpoints)å’Œæœ€ç»ˆæ¨¡å‹çš„ç›®å½•ã€‚</p> </li><li> <p><code>overwrite_output_dir=False</code><br> å¦‚æœç›®å½•å·²å­˜åœ¨,æ˜¯å¦è¦†ç›–å®ƒã€‚è®¾ä¸ºFalseå¯ä»¥é¿å…æ„å¤–è¦†ç›–ä¹‹å‰çš„ç»“æœã€‚</p> </li><li> <p><code>eval_steps=120, save_steps=120</code><br> æ¯120æ­¥è¯„ä¼°ä¸€æ¬¡æ¨¡å‹æ€§èƒ½,å¹¶ä¿å­˜æ¨¡å‹ã€‚é¢‘ç¹ä¿å­˜å¯ä»¥åœ¨è®­ç»ƒä¸­æ–­æ—¶æ¢å¤ã€‚</p> </li><li> <p><code>warmup_steps=1</code><br> å­¦ä¹ ç‡warmupæ­¥æ•°,ä¸€å¼€å§‹ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡æœ‰åŠ©äºç¨³å®šè®­ç»ƒæ—©æœŸé˜¶æ®µã€‚</p> </li><li> <p><code>per_device_eval_batch_size=1</code><br> è¯„ä¼°æ—¶æ¯ä¸ªè®¾å¤‡ä¸Šçš„æ‰¹é‡å¤§å°ã€‚é€šå¸¸ä¸è®­ç»ƒæ—¶ç›¸åŒã€‚</p> </li><li> <p><code>evaluation_strategy="steps", logging_strategy="steps"</code><br> ä»¥æ­¥æ•°ä¸ºé—´éš”è¿›è¡Œè¯„ä¼°å’Œè®°å½•æ—¥å¿—,è€Œä¸æ˜¯ä»¥epochä¸ºé—´éš”ã€‚</p> </li><li> <p><code>optim="adafactor"</code><br> ä½¿ç”¨Adafactorä¼˜åŒ–å™¨,é€‚ç”¨äºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒã€‚</p> </li><li> <p><code>gradient_accumulation_steps=4</code><br> æ¢¯åº¦ç§¯ç´¯æ­¥æ•°,å¯ä»¥æ¨¡æ‹Ÿä½¿ç”¨æ›´å¤§æ‰¹é‡å¤§å°çš„æ•ˆæœ,èŠ‚çœå†…å­˜ã€‚</p> </li><li> <p><code>load_best_model_at_end=True</code><br> ä¿å­˜éªŒè¯é›†ä¸Šæ€§èƒ½æœ€å¥½çš„é‚£ä¸ªæ£€æŸ¥ç‚¹,ä½œä¸ºæœ€ç»ˆæ¨¡å‹ã€‚</p> </li><li> <p><code>metric_for_best_model="eval_loss", greater_is_better=False</code><br> æ ¹æ®éªŒè¯æŸå¤±è¯„ä¼°æ¨¡å‹,æŸå¤±è¶Šå°è¶Šå¥½ã€‚</p> </li></ol> 
</blockquote> 
<pre><code class="hljs">model_flops = (
  base_model.floating_point_ops(
    {
       "input_ids": torch.zeros(
           (1, training_config["model"]["max_length"])
      )
    }
  )
  * training_args.gradient_accumulation_steps
)

print(base_model)
print("Memory footprint", base_model.get_memory_footprint() / 1e9, "GB")
print("Flops", model_flops / 1e9, "GFLOPs")


print(base_model)
print("Memory footprint", base_model.get_memory_footprint() / 1e9, "GB")
print("Flops", model_flops / 1e9, "GFLOPs")
</code></pre> 
<p>è¿™é‡Œè¿˜è®¡ç®—å¹¶æ‰“å°äº†æ¨¡å‹çš„å†…å­˜å ç”¨å’Œè®¡ç®—å¤æ‚åº¦(FLOPs)ã€‚</p> 
<p>æœ€å,ä½¿ç”¨è¿™äº›å‚æ•°åˆ›å»ºäº†ä¸€ä¸ª<code>Trainer</code>å¯¹è±¡,ç”¨äºå®é™…è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚</p> 
<pre><code class="hljs">trainer = Trainer(
    model=base_model,
    model_flops=model_flops,
    total_steps=max_steps,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)
</code></pre> 
<ul><li><strong>è®­ç»ƒæ¨¡å‹å‡ ä¸ªæ­¥éª¤</strong></li></ul> 
<pre><code>training_output = trainer.train()
</code></pre> 
<p>è¿™ä¸€è¡Œä»£ç å¯åŠ¨äº†æ¨¡å‹çš„å¾®è°ƒè®­ç»ƒè¿‡ç¨‹,å¹¶å°†è®­ç»ƒè¾“å‡ºå­˜å‚¨åœ¨<code>training_output</code>ä¸­ã€‚</p> 
<ul><li><strong>ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹</strong></li></ul> 
<pre><code>save_dir = f'{output_dir}/final'
trainer.save_model(save_dir)
print("Saved model to:", save_dir)
finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)
finetuned_slightly_model.to(device)
</code></pre> 
<p>è¿™éƒ¨åˆ†å°†å¾®è°ƒåçš„æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šçš„ç›®å½•ä¸­ã€‚</p> 
<blockquote> 
 <p>ç„¶å,å®ƒä½¿ç”¨<code>AutoModelForCausalLM.from_pretrained</code>ä»ä¿å­˜çš„æ¨¡å‹ä¸­é‡æ–°åŠ è½½è¯¥æ¨¡å‹,å¹¶å°†å…¶ç§»åŠ¨åˆ°ç›¸åº”çš„è®¾å¤‡ä¸Šã€‚</p> 
</blockquote> 
<ul><li><strong>ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†</strong></li></ul> 
<pre><code>test_question = test_dataset[0]['question']
print("Question input (test):", test_question)
print("Finetuned slightly model's answer: ")
print(inference(test_question, finetuned_slightly_model, tokenizer))
test_answer = test_dataset[0]['answer']
print("Target answer output (test):", test_answer)
</code></pre> 
<blockquote> 
 <p>è¿™é‡Œä½¿ç”¨ä¹‹å‰å®šä¹‰çš„<code>inference</code>å‡½æ•°,åœ¨æµ‹è¯•æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªç¤ºä¾‹ä¸Šå°è¯•ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚</p> 
</blockquote> 
<p>æ‰“å°äº†è¾“å…¥é—®é¢˜ã€æ¨¡å‹è¾“å‡ºä»¥åŠæ­£ç¡®ç­”æ¡ˆã€‚</p> 
<ul><li><strong>åŠ è½½å¹¶è¿è¡Œå…¶ä»–é¢„è®­ç»ƒæ¨¡å‹</strong></li></ul> 
<pre><code>finetuned_longer_model = AutoModelForCausalLM.from_pretrained("lamini/lamini_docs_finetuned")
tokenizer = AutoTokenizer.from_pretrained("lamini/lamini_docs_finetuned")
finetuned_longer_model.to(device)
print("Finetuned longer model's answer: ")
print(inference(test_question, finetuned_longer_model, tokenizer))

bigger_finetuned_model = BasicModelRunner(model_name_to_id["bigger_model_name"])
bigger_finetuned_output = bigger_finetuned_model(test_question)
print("Bigger (2.8B) finetuned model (test): ", bigger_finetuned_output)
</code></pre> 
<p>è¿™éƒ¨åˆ†åŠ è½½äº†å¦ä¸€ä¸ªç»è¿‡æ›´é•¿æ—¶é—´å¾®è°ƒçš„æ¨¡å‹,ä»¥åŠä¸€ä¸ªæ›´å¤§çš„2.8Bå‚æ•°çš„å¾®è°ƒæ¨¡å‹ã€‚å®ƒä½¿ç”¨è¿™äº›æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªç¤ºä¾‹ä¸Šè¿›è¡Œæ¨ç†,å¹¶æ‰“å°å‡ºç»“æœã€‚</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/26368f4b5bee1942881b84c26b2233dc/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">Cè¯­è¨€ä¹‹çº¿ç¨‹çš„å­¦ä¹ </p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9b48b261b408d340d0cfb186a309fb53/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">4PCSç‚¹äº‘é…å‡†ç®—æ³•å®ç°</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 ç¼–ç¨‹å¤§å’–.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>