<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>在CentOS7虚拟机上使用Ollama本地部署Llama3大模型中文版&#43;Open WebUI - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/d3619cf8312795eaf92e681aff120c50/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="在CentOS7虚拟机上使用Ollama本地部署Llama3大模型中文版&#43;Open WebUI">
  <meta property="og:description" content="一、创建虚拟机 1.1按照常规的CentOS教程来安装就行，我用的是以下版本： VMware版本：VMware Workstation Full v12.1.0-3272444 中文正式版
镜像版本：CentOS-7-x86_64-DVD-2009
1.2虚拟机配置参数，如下： 二、升级虚拟机必要组件功能 2.1运行以下命令可以更新系统中的所有安装的软件包至最新版本： sudo yum update 注意：安装过程中会多次提示：“Is this ok [y/N]”: ，需要手动键入“y”，才能继续进程，后续安装过程同样会遇到这个提示，不会再赘述。
2.1更新系统软件包后，你可以使用以下命令来检查系统是否有需要的更新： sudo yum check-update 2.2升级python3（建议按照自己的习惯升级） 这里举例我常用的升级命令：
sudo yum install epel-release sudo yum install python3 sudo yum install python3-devel sudo yum install python3-pip python3 --version 三、安装Ollama 3.1安装Ollama的过程很费时间，几十分钟到大几个小时不等，安装命令如下： curl -fsSL https://ollama.com/install.sh | sh 1000Mbps的光纤，蹲个坑回来下了这么点...屑。
四、部署Llama3-8B-Chinese-Chat-v2.1模型 4.1运行以下任意一条命令，可以最快速地部署对应版本的Llama3-8B-Chinese-Chat-v2.1模型： ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-q4 ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-q8 ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-fp16 模型的下载速度比较快，小十分钟就能下好。
模型的版本越高、文件越大、运算力越好、越吃性能，我装的是q8_0版本，这里有个q8_0模型在线版的url，安装过程中可以进去把玩一下（要翻墙）：https://huggingface.co/spaces/llamafactory/Llama3-8B-Chinese-Chat
4.2部署完之后，就直接进入到对应大模型的命令行聊天界面了： 可以问它一些中英互译比较有歧义的问题，来测试它对中文的语言理解和生成能力，不深入演示了，CPU要被干烧了。
4.3通过ollama命令可以查看、运行、更新、复制、移除已部署的大模型。 4.3.1如：ollama list，可以查看虚拟机内已部署的大模型，可以看到我只安装了一款。 [llama3@Llama3 ~]$ ollama list NAME ID SIZE MODIFIED wangshenzhi/llama3-8b-chinese-chat-ollama-q8:latest	6739fd08efd6	8.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-11T09:13:30+08:00">
    <meta property="article:modified_time" content="2024-06-11T09:13:30+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">在CentOS7虚拟机上使用Ollama本地部署Llama3大模型中文版&#43;Open WebUI</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>一、创建虚拟机</h2> 
<h5>1.1按照常规的CentOS教程来安装就行，我用的是以下版本：</h5> 
<blockquote> 
 <p>VMware版本：VMware Workstation Full v12.1.0-3272444 中文正式版</p> 
 <p>镜像版本：CentOS-7-x86_64-DVD-2009</p> 
</blockquote> 
<h5>1.2虚拟机配置参数，如下： </h5> 
<p><img alt="" class="left" height="136" src="https://images2.imgbox.com/28/c8/YfNDf4ze_o.png" width="704"></p> 
<h2>二、升级虚拟机必要组件功能</h2> 
<h5>2.1运行以下命令可以更新系统中的所有安装的软件包至最新版本：</h5> 
<pre><code>sudo yum update</code></pre> 
<p><span style="color:#fe2c24;"><strong>注意：</strong></span><span style="color:#0d0016;">安装过程中会多次提示：“Is this ok [y/N]”: ，需要手动键入“y”，才能继续进程，后续安装过程同样会遇到这个提示，不会再赘述。</span></p> 
<h5>2.1更新系统软件包后，你可以使用以下命令来检查系统是否有需要的更新：</h5> 
<pre><code>sudo yum check-update
</code></pre> 
<h5>2.2升级python3（建议按照自己的习惯升级）</h5> 
<p>这里举例我常用的升级命令：</p> 
<pre><code>sudo yum install epel-release
sudo yum install python3
sudo yum install python3-devel
sudo yum install python3-pip
python3 --version</code></pre> 
<h2>三、安装Ollama</h2> 
<h5>3.1安装Ollama的过程很费时间，几十分钟到大几个小时不等，安装命令如下：</h5> 
<pre><code>curl -fsSL https://ollama.com/install.sh | sh</code></pre> 
<p>1000Mbps的光纤，蹲个坑回来下了这么点...屑。</p> 
<p><img alt="" height="58" src="https://images2.imgbox.com/2d/ee/vaz4aRSq_o.png" width="658"></p> 
<h2>四、部署Llama3-8B-Chinese-Chat-v2.1模型</h2> 
<h5>4.1运行以下任意一条命令，可以最快速地部署对应版本的Llama3-8B-Chinese-Chat-v2.1模型：</h5> 
<pre><code>ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-q4

ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-q8

ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-fp16</code></pre> 
<p>模型的下载速度比较快，小十分钟就能下好。</p> 
<p><img alt="" height="57" src="https://images2.imgbox.com/f8/9b/JvLsRqr4_o.png" width="1200"></p> 
<p>模型的版本越高、文件越大、运算力越好、越吃性能，我装的是q8_0版本，这里有个q8_0模型在线版的url，安装过程中可以进去把玩一下（要翻墙）：<a href="https://huggingface.co/spaces/llamafactory/Llama3-8B-Chinese-Chat" rel="nofollow" title="https://huggingface.co/spaces/llamafactory/Llama3-8B-Chinese-Chat">https://huggingface.co/spaces/llamafactory/Llama3-8B-Chinese-Chat</a></p> 
<p><img alt="" height="129" src="https://images2.imgbox.com/60/43/dGB5acI6_o.png" width="653"></p> 
<h5>4.2部署完之后，就直接进入到对应大模型的命令行聊天界面了：</h5> 
<p>可以问它一些中英互译比较有歧义的问题，来测试它对中文的语言理解和生成能力，不深入演示了，CPU要被干烧了。</p> 
<p><img alt="" height="207" src="https://images2.imgbox.com/49/95/BbaCzJRd_o.png" width="560"></p> 
<h5><span style="color:#333333;">4.3通过ollama命令可以查看、运行、更新、复制、移除已部署的大模型。</span></h5> 
<h6><span style="color:#333333;">4.3.1如：ollama list，可以查看虚拟机内已部署的大模型，可以看到我只安装了一款。</span></h6> 
<pre><code>[llama3@Llama3 ~]$ ollama list
NAME                                               	ID          	SIZE  	MODIFIED       
wangshenzhi/llama3-8b-chinese-chat-ollama-q8:latest	6739fd08efd6	8.5 GB	21 minutes ago</code></pre> 
<h6>4.4.2如：ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-q8，运行指定大模型。</h6> 
<pre><code>[llama3@Llama3 ~]$ ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-q8
&gt;&gt;&gt; Send a message (/? for help)</code></pre> 
<h2><span style="color:#333333;">五、安装Open WebUI </span></h2> 
<p>Open WebUI是一个可拓展、功能丰富且用户友好的自托管 WebUI，旨在完全离线运行。它支持各种 LLM 运行器，包括Ollama和OpenAI 兼容API。简单地说就是给大模型提供一个图形化界面，以及生成一个API，后续就可以通过特定的IP+端口号在浏览器登陆大模型了。</p> 
<h5>5.1先安装Docker，按照自己的习惯安装就行，以下是我常用的命令：</h5> 
<pre><code>sudo yum install -y yum-utils
sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
sudo yum install docker-ce
sudo systemctl start docker
sudo systemctl enable docker
docker --version</code></pre> 
<h5>5.2使用默认配置进行安装（要下载很久，几个小时到十几个小时不等），命令如下：</h5> 
<pre><code>docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</code></pre> 
<p>挂一天一夜，基本上能下完。</p> 
<p><img alt="" height="353" src="https://images2.imgbox.com/f9/f5/6ZnkjLst_o.png" width="840"></p> 
<p>安装完之后，可以用命令看进程，看到WebUI是运行在docker容器之上的。</p> 
<pre><code>[root@Llama3 ~]# docker ps
CONTAINER ID   IMAGE                                COMMAND           CREATED        STATUS                  PORTS                                       NAMES
9bbab31dcda6   ghcr.io/open-webui/open-webui:main   "bash start.sh"   15 hours ago   Up 15 hours (healthy)   0.0.0.0:3000-&gt;8080/tcp, :::3000-&gt;8080/tcp   open-webui</code></pre> 
<h5>5.3安装后，就可以通过<a href="http://localhost:3000/" rel="nofollow" title="http://localhost:3000">http://localhost:3000</a>访问Open WebUI了（localhost就是虚拟机的ip），如果虚拟机改了ip的话，可用新ip无缝衔接登陆。</h5> 
<p>进去先注册账号，无需联网，相当于注册一个管理员账号。</p> 
<p style="text-align:center;"><img alt="" class="left" src="https://images2.imgbox.com/71/6b/zSA5dPJK_o.jpg"></p> 
<p>语言可以设置成中文。</p> 
<p style="text-align:center;"><img alt="" class="left" src="https://images2.imgbox.com/54/f9/tshVhV7v_o.jpg"></p> 
<h5>5.4刷新Ollama，测试一下到Ollama服务器的连通信。</h5> 
<h6>5.4.1部署成功的话，应该提示：“已验证服务器连接性”。</h6> 
<h6>5.4.2提示：“WebUI could not connect to Ollama”，证明从Open WebUI界面访问Ollama时遇到了困难，排查思路单独放在<span style="color:#fe2c24;">第六小节</span>。</h6> 
<p style="text-align:center;"><img alt="" class="left" src="https://images2.imgbox.com/69/8e/c01AvzLl_o.png"></p> 
<h5>5.2加载自己已安装的模型，或者在线下载想要的模型。</h5> 
<p>我已经安装过wangshenzhi/llama3-8b-chinese-chat-ollama-q8模型了，所以这里可以直接加载出来（忽略第四步，在这里直接在线拉取模型到本地，其实是可以的）。</p> 
<p><img alt="" class="left" height="613" src="https://images2.imgbox.com/fd/1f/siTugHBU_o.png" width="880"></p> 
<h5>5.3将常用的模型设为默认即可，用前面注册的账号可以离线登录，并保留每次的对话记录。</h5> 
<p><img alt="" class="left" height="266" src="https://images2.imgbox.com/b4/36/9OhRdOQr_o.png" width="1200"></p> 
<h2>六、WebUI could not connect to Ollama排查思路</h2> 
<p>如果您在从 Open WebUI 界面访问 Ollama 时遇到困难，这可能是因为 Ollama 默认配置为侦听受限网络接口。要启用从 Open WebUI 的访问，您需要将 Ollama 配置为侦听更广泛的网络接口。</p> 
<h5>6.1通过调用来编辑systemd服务，打开一个编辑器：</h5> 
<pre><code>systemctl edit ollama.service</code></pre> 
<h5>6.2Environment对于每个环境变量，在部分下添加一行[Service]：</h5> 
<p>记得“ESC”、“:wq”，保存并退出。</p> 
<pre><code>[Service]
Environment="OLLAMA_HOST=0.0.0.0"</code></pre> 
<h6><strong>6.3重新加载systemd并重新启动Ollama：</strong> </h6> 
<pre><code>systemctl daemon-reload
systemctl restart ollama</code></pre> 
<h5>6.4刷新一下Ollama API，显示以下画面就可以回去第五节看看了。</h5> 
<p><img alt="" class="left" height="701" src="https://images2.imgbox.com/c4/46/i8XhsznK_o.png" width="894"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/62e5669ece6082aea33b87b145d64e64/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">mongo数据迁移方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b1a52b8f7126c608f514a3421bc146bb/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Go语言精进之路】构建高效Go程序：了解切片实现原理并高效使用</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>