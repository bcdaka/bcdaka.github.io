<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【人工智能】Transformers之Pipeline（七）：图像分割（image-segmentation） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/4e3a6bad4168c87d0555071ddd2dc52b/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【人工智能】Transformers之Pipeline（七）：图像分割（image-segmentation）">
  <meta property="og:description" content="目录
一、引言 二、图像分割（image-segmentation）
2.1 概述
2.2 技术原理
2.3 应用场景
2.4 pipeline参数
2.4.1 pipeline对象实例化参数
2.4.2 pipeline对象使用参数 2.4 pipeline实战
2.5 模型排名
三、总结
一、引言 pipeline（管道）是huggingface transformers库中一种极简方式使用大模型推理的抽象，将所有大模型分为音频（Audio）、计算机视觉（Computer vision）、自然语言处理（NLP）、多模态（Multimodal）等4大类，28小类任务（tasks）。共计覆盖32万个模型
今天介绍CV计算机视觉的第三篇，图像分割（image-segmentation），在huggingface库内有800个图像分类模型。
二、图像分割（image-segmentation） 2.1 概述 图像分割就是把图像分成若干个特定的、具有独特性质的区域并提出感兴趣目标的技术和过程。它是由图像处理到图像分析的关键步骤。现有的图像分割方法主要分以下几类：基于阈值的分割方法、基于区域的分割方法、基于边缘的分割方法以及基于特定理论的分割方法等。从数学角度来看，图像分割是将数字图像划分成互不相交的区域的过程。图像分割的过程也是一个标记过程，即把属于同一区域的像素赋予相同的编号。
2.2 技术原理 图像分割（image-segmentation）的默认模型为facebook/detr-resnet-50-panoptic，全称为：DEtection TRansformer(DETR)-resnet-50-全景。其中有3个要素：
DEtection TRansformer (DETR)：于2020年5月由Facebook AI发布于《End-to-End Object Detection with Transformers》，提出了一种基于transformer的端到端目标检测方法，相比于YOLO具有更高的准确性，但速度不及YOLO，可以应用于医疗影像等不追求实时性的目标检测场景，对于追求实时性的目标检测场景，还是得YOLO，关于YOLOv10，可以看我之前的文章。ResNet-50：ResNet-50是一种深度残差网络（Residual Network），是ResNet系列中的一种经典模型。它由微软研究院的Kaiming He等人于2015年提出，被广泛应用于计算机视觉任务，如图像分类、目标检测和图像分割等。ResNet-50是一种迁移学习模型，迁移学习的核心思想是将源领域的知识迁移到目标领域中，可以采用样本迁移、特征迁移、模型迁移、关系迁移等手段。全景分割panoptic： DETR在COCO2017全景图（118k 带注释图像）上进行端到端训练，全景分割即可以将图片全景分割成多个区域，每个区域使用同一编号。 DEtection TRansformer(DETR)主体结构：
由三个主要部分组成：用于特征提取的CNN后端（ResNet）、transformer编码器-解码器和用于最终检测预测的前馈网络（FFN）。后端处理输入图像并生成激活图。transformer编码器降低通道维度并应用多头自注意力和前馈网络。transformer解码器使用N个物体嵌入的并行解码，并独立预测箱子坐标和类别标签，使用物体查询。DETR利用成对关系，从整个图像上下文中受益，共同推理所有物体。
DEtection TRansformer(DETR)应用于全景分割：
将图片内的box进行embedding后输入至MHA中提取每个box中的图片信息，采用Resnet生成激活图，采用pixel-wise（像素级别）的损失函数进行学习。pixel-wise损失函数是计算预测图像与目标图像的像素间的损失。属于像素间类型的损失函数，常见的有：MSE（L2损失），MAE（L1损失）和交叉熵等。计算机视觉领域的损失函数有3类：pixel-wise（像素级别），patch-wise（块级别），image-wise（图片级别）。
2.3 应用场景 医学影像分析：在医疗领域，图像分割用于识别和分割肿瘤、器官等，帮助医生进行诊断和手术规划。自动驾驶：车辆需要识别道路、行人、其他车辆等，图像分割技术可以辅助车辆理解周围环境，提高安全性和导航准确性。遥感与地图制作：通过分割卫星或无人机图像，可以自动识别水体、森林、建筑物等，用于城市规划、环境监测和自然资源管理。增强现实：在AR应用中，分割技术用于区分前景和背景，使得虚拟物体能够自然地融入真实世界场景中。时尚与零售：用于服装分割，帮助自动识别和分类衣物，用于在线购物的虚拟试穿或商品推荐。自然资源管理：如森林火灾监测，通过分割图像识别火源区域。体育分析：分析运动员动作，通过分割跟踪特定运动员或球的运动轨迹。 2.4 pipeline参数 2.4.1 pipeline对象实例化参数 model（PreTrainedModel或TFPreTrainedModel）— 管道将使用其进行预测的模型。 对于 PyTorch，这需要从PreTrainedModel继承；对于 TensorFlow，这需要从TFPreTrainedModel继承。image_processor ( BaseImageProcessor ) — 管道将使用的图像处理器来为模型编码数据。此对象继承自 BaseImageProcessor。modelcard（str或ModelCard，可选）— 属于此管道模型的模型卡。framework（str，可选）— 要使用的框架，&#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-02T18:26:36+08:00">
    <meta property="article:modified_time" content="2024-08-02T18:26:36+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【人工智能】Transformers之Pipeline（七）：图像分割（image-segmentation）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="%C2%A0%E2%80%8B%E7%BC%96%E8%BE%91"><img alt="" height="402" src="https://images2.imgbox.com/60/a1/76nztien_o.png" width="1200"></p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E5%BC%95%E8%A8%80%C2%A0-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E5%BC%95%E8%A8%80%C2%A0" rel="nofollow">一、引言 </a></p> 
<p id="%E4%BA%8C%E3%80%81Tokenizer-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81Tokenizer" rel="nofollow">二、图像分割（image-segmentation）</a></p> 
<p id="2.1%20%E6%A6%82%E8%BF%B0-toc" style="margin-left:40px;"><a href="#2.1%20%E6%A6%82%E8%BF%B0" rel="nofollow">2.1 概述</a></p> 
<p id="2.2%20%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95-toc" style="margin-left:40px;"><a href="#2.2%20%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95" rel="nofollow">2.2 技术原理</a></p> 
<p id="2.3%20%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-toc" style="margin-left:40px;"><a href="#2.3%20%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF" rel="nofollow">2.3 应用场景</a></p> 
<p id="2.4%20pipeline%E5%8F%82%E6%95%B0-toc" style="margin-left:40px;"><a href="#2.4%20pipeline%E5%8F%82%E6%95%B0" rel="nofollow">2.4 pipeline参数</a></p> 
<p id="2.4.1%20pipeline%E5%AF%B9%E8%B1%A1%E5%AE%9E%E4%BE%8B%E5%8C%96%E5%8F%82%E6%95%B0-toc" style="margin-left:80px;"><a href="#2.4.1%20pipeline%E5%AF%B9%E8%B1%A1%E5%AE%9E%E4%BE%8B%E5%8C%96%E5%8F%82%E6%95%B0" rel="nofollow">2.4.1 pipeline对象实例化参数</a></p> 
<p id="2.4.2%20pipeline%E5%AF%B9%E8%B1%A1%E4%BD%BF%E7%94%A8%E5%8F%82%E6%95%B0%C2%A0-toc" style="margin-left:80px;"><a href="#2.4.2%20pipeline%E5%AF%B9%E8%B1%A1%E4%BD%BF%E7%94%A8%E5%8F%82%E6%95%B0%C2%A0" rel="nofollow">2.4.2 pipeline对象使用参数 </a></p> 
<p id="2.4%C2%A0pipeline%E5%AE%9E%E6%88%98-toc" style="margin-left:40px;"><a href="#2.4%C2%A0pipeline%E5%AE%9E%E6%88%98" rel="nofollow">2.4 pipeline实战</a></p> 
<p id="2.5%C2%A0%E6%A8%A1%E5%9E%8B%E6%8E%92%E5%90%8D-toc" style="margin-left:40px;"><a href="#2.5%C2%A0%E6%A8%A1%E5%9E%8B%E6%8E%92%E5%90%8D" rel="nofollow">2.5 模型排名</a></p> 
<p id="2.2.1%C2%A0%E5%AE%89%E8%A3%85timm%E5%BA%93-toc" style="margin-left:0px;"><a href="#2.2.1%C2%A0%E5%AE%89%E8%A3%85timm%E5%BA%93" rel="nofollow">三、总结</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E4%B8%80%E3%80%81%E5%BC%95%E8%A8%80%C2%A0">一、引言 </h2> 
<p> pipeline（管道）是huggingface transformers库中一种极简方式使用大模型推理的抽象，将所有大模型分为音频（Audio）、计算机视觉（Computer vision）、自然语言处理（NLP）、多模态（Multimodal）等4大类，28小类任务（tasks）。共计覆盖32万个模型</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/4b/1c/4RxYF4OO_o.jpg" width="1065"></p> 
<p>今天介绍CV计算机视觉的第三篇，图像分割（image-segmentation），在huggingface库内有800个图像分类模型。</p> 
<h2 id="%E4%BA%8C%E3%80%81Tokenizer">二、图像分割（image-segmentation）</h2> 
<h3 id="2.1%20%E6%A6%82%E8%BF%B0">2.1 概述</h3> 
<p>图像分割就是把图像分成若干个特定的、具有独特性质的区域并提出感兴趣目标的技术和过程。它是由图像处理到图像分析的关键步骤。现有的图像分割方法主要分以下几类：基于阈值的分割方法、基于区域的分割方法、基于边缘的分割方法以及基于特定理论的分割方法等。从数学角度来看，图像分割是将数字图像划分成互不相交的区域的过程。图像分割的过程也是一个标记过程，即把属于同一区域的像素赋予相同的编号。</p> 
<p><img alt="" height="570" src="https://images2.imgbox.com/22/94/X4R9NW2x_o.png" width="1200"></p> 
<h3 id="2.2%20%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95">2.2 技术原理</h3> 
<p>图像分割（image-segmentation）的默认模型为facebook/detr-resnet-50-panoptic，全称为：DEtection TRansformer(DETR)-resnet-50-全景。其中有3个要素：</p> 
<blockquote> 
 <ul><li>DEtection TRansformer (DETR)：于2020年5月由Facebook AI发布于《<a class="link-info" href="https://arxiv.org/abs/2005.12872" rel="nofollow" title="End-to-End Object Detection with Transformers">End-to-End Object Detection with Transformers</a>》，提出了一种基于transformer的端到端目标检测方法，相比于YOLO具有更高的准确性，但速度不及YOLO，可以应用于医疗影像等不追求实时性的目标检测场景，对于追求实时性的目标检测场景，还是得YOLO，关于YOLOv10，可以看<a class="link-info" href="https://blog.csdn.net/weixin_48007632/article/details/139307081?spm=1001.2014.3001.5502" title="我之前的文章">我之前的文章</a>。</li><li>ResNet-50：ResNet-50是一种深度残差网络（Residual Network），是ResNet系列中的一种经典模型。它由微软研究院的Kaiming He等人于2015年提出，被广泛应用于计算机视觉任务，如图像分类、目标检测和图像分割等。ResNet-50是一种迁移学习模型，迁移学习的核心思想是将源领域的知识迁移到目标领域中，可以采用样本迁移、特征迁移、模型迁移、关系迁移等手段。</li><li>全景分割panoptic： DETR在COCO2017全景图（118k 带注释图像）上进行端到端训练，全景分割即可以将图片全景分割成多个区域，每个区域使用同一编号。</li></ul> 
</blockquote> 
<p>DEtection TRansformer(DETR)主体结构：</p> 
<blockquote> 
 <p>由三个主要部分组成：用于特征提取的CNN后端（ResNet）、transformer编码器-解码器和用于最终检测预测的前馈网络（FFN）。后端处理输入图像并生成激活图。transformer编码器降低通道维度并应用多头自注意力和前馈网络。transformer解码器使用N个物体嵌入的并行解码，并独立预测箱子坐标和类别标签，使用物体查询。DETR利用成对关系，从整个图像上下文中受益，共同推理所有物体。</p> 
</blockquote> 
<p><img alt="" height="262" src="https://images2.imgbox.com/b4/12/fawAGiEX_o.png" width="946"> </p> 
<p> DEtection TRansformer(DETR)应用于全景分割：</p> 
<p><img alt="" height="530" src="https://images2.imgbox.com/03/91/taRnKJGL_o.png" width="1200"></p> 
<blockquote> 
 <p>将图片内的box进行embedding后输入至MHA中提取每个box中的图片信息，采用Resnet生成激活图，采用pixel-wise（像素级别）的损失函数进行学习。pixel-wise损失函数是计算预测图像与目标图像的像素间的损失。属于像素间类型的损失函数，常见的有：MSE（L2损失），MAE（L1损失）和交叉熵等。计算机视觉领域的损失函数有3类：pixel-wise（像素级别），patch-wise（块级别），image-wise（图片级别）。</p> 
</blockquote> 
<h3 id="2.3%20%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF">2.3 应用场景</h3> 
<blockquote> 
 <ul><li>医学影像分析：在医疗领域，图像分割用于识别和分割肿瘤、器官等，帮助医生进行诊断和手术规划。</li><li>自动驾驶：车辆需要识别道路、行人、其他车辆等，图像分割技术可以辅助车辆理解周围环境，提高安全性和导航准确性。</li><li>遥感与地图制作：通过分割卫星或无人机图像，可以自动识别水体、森林、建筑物等，用于城市规划、环境监测和自然资源管理。</li><li>增强现实：在AR应用中，分割技术用于区分前景和背景，使得虚拟物体能够自然地融入真实世界场景中。</li><li>时尚与零售：用于服装分割，帮助自动识别和分类衣物，用于在线购物的虚拟试穿或商品推荐。</li><li>自然资源管理：如森林火灾监测，通过分割图像识别火源区域。</li><li>体育分析：分析运动员动作，通过分割跟踪特定运动员或球的运动轨迹。</li></ul> 
</blockquote> 
<h3 id="2.4%20pipeline%E5%8F%82%E6%95%B0">2.4 pipeline参数</h3> 
<h4 id="2.4.1%20pipeline%E5%AF%B9%E8%B1%A1%E5%AE%9E%E4%BE%8B%E5%8C%96%E5%8F%82%E6%95%B0">2.4.1 pipeline对象实例化参数</h4> 
<blockquote> 
 <ul><li><strong>model</strong>（<a href="https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/model#transformers.PreTrainedModel" rel="nofollow" title="PreTrainedModel">PreTrainedModel</a>或<a href="https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/model#transformers.TFPreTrainedModel" rel="nofollow" title="TFPreTrainedModel">TFPreTrainedModel</a>）— 管道将使用其进行预测的模型。 对于 PyTorch，这需要从<a href="https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/model#transformers.PreTrainedModel" rel="nofollow" title="PreTrainedModel">PreTrainedModel</a>继承；对于 TensorFlow，这需要从<a href="https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/model#transformers.TFPreTrainedModel" rel="nofollow" title="TFPreTrainedModel继承。">TFPreTrainedModel继承。</a></li><li><strong>image_processor</strong> ( <a href="https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/image_processor#transformers.BaseImageProcessor" rel="nofollow" title="BaseImageProcessor">BaseImageProcessor</a> ) — 管道将使用的图像处理器来为模型编码数据。此对象继承自 <a href="https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/image_processor#transformers.BaseImageProcessor" rel="nofollow" title="BaseImageProcessor">BaseImageProcessor</a>。</li><li><strong>modelcard</strong>（<code>str</code>或<code>ModelCard</code>，<em>可选</em>）— 属于此管道模型的模型卡。</li><li><strong>framework</strong>（<code>str</code>，<em>可选</em>）— 要使用的框架，<code>"pt"</code>适用于 PyTorch 或<code>"tf"</code>TensorFlow。必须安装指定的框架。 <p>如果未指定框架，则默认为当前安装的框架。如果未指定框架且安装了两个框架，则默认为model的框架，如果未提供模型，则默认为 PyTorch。</p> </li><li><strong>task</strong>（<code>str</code>，默认为<code>""</code>）— 管道的任务标识符。</li><li><strong>num_workers</strong>（<code>int</code>，<em>可选</em>，默认为 8）— 当管道将使用<em>DataLoader</em>（传递数据集时，在 Pytorch 模型的 GPU 上）时，要使用的工作者数量。</li><li><strong>batch_size</strong>（<code>int</code>，<em>可选</em>，默认为 1）— 当管道将使用<em>DataLoader</em>（传递数据集时，在 Pytorch 模型的 GPU 上）时，要使用的批次的大小，对于推理来说，这并不总是有益的，请阅读<a href="https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching" rel="nofollow" title="使用管道进行批处理">使用管道进行批处理</a>。</li><li><strong>args_parser</strong>（<a href="https://huggingface.co/docs/transformers/v4.42.0/en/internal/pipelines_utils#transformers.pipelines.ArgumentHandler" rel="nofollow" title="ArgumentHandler">ArgumentHandler</a>，<em>可选</em>） - 引用负责解析提供的管道参数的对象。</li><li><strong>device</strong>（<code>int</code>，<em>可选</em>，默认为 -1）— CPU/GPU 支持的设备序号。将其设置为 -1 将利用 CPU，设置为正数将在关联的 CUDA 设备 ID 上运行模型。您可以传递本机<code>torch.device</code>或<code>str</code>太</li><li><strong>torch_dtype</strong>（<code>str</code>或<code>torch.dtype</code>，<em>可选</em>） - 直接发送<code>model_kwargs</code>（只是一种更简单的快捷方式）以使用此模型的可用精度（<code>torch.float16</code>，，<code>torch.bfloat16</code>...或<code>"auto"</code>）</li><li><strong>binary_output</strong>（<code>bool</code>，<em>可选</em>，默认为<code>False</code>）——标志指示管道的输出是否应以序列化格式（即 pickle）或原始输出数据（例如文本）进行。</li></ul> 
</blockquote> 
<h4 id="2.4.2%20pipeline%E5%AF%B9%E8%B1%A1%E4%BD%BF%E7%94%A8%E5%8F%82%E6%95%B0%C2%A0">2.4.2 pipeline对象使用参数 </h4> 
<blockquote> 
 <ul style="margin-left:0;"><li><strong>images</strong>（<code>str</code>、<code>List[str]</code>或<code>PIL.Image</code>）<code>List[PIL.Image]</code>——管道处理三种类型的图像： 
   <ul style="margin-left:.75em;"><li>包含指向图像的 HTTP(S) 链接的字符串</li><li>包含图像本地路径的字符串</li><li>直接在 PIL 中加载的图像</li></ul><p style="margin-left:0;">管道可以接受单张图片或一批图片。一批图片中的图片必须全部采用相同的格式：全部为 HTTP(S) 链接、全部为本地路径或全部为 PIL 图片。</p> </li><li><strong>subtask</strong>（<code>str</code>，<em>可选</em>）— 要执行的分割任务，根据模型功能 选择 [ <code>semantic</code>，<code>instance</code>和]。如果未设置，管道将尝试按以下顺序解析： ，，。<code>panoptic</code><code>panoptic</code><code>instance</code><code>semantic</code></li><li><strong>threshold</strong>（<code>float</code>，<em>可选</em>，默认为 0.9）— 用于过滤预测掩码的概率阈值。</li><li><strong>mask_threshold</strong>（<code>float</code>，<em>可选</em>，默认为 0.5）— 将预测掩码转换为二进制值时使用的阈值。</li><li><strong>overlap_mask_area_threshold</strong>（<code>float</code>，<em>可选</em>，默认为 0.5）— 掩膜重叠阈值，用于消除小的、不连续的段。</li><li><strong>timeout</strong>（<em>可选</em><code>float</code>，默认为 None）— 等待从网络获取图像的最长时间（以秒为单位）。如果为 None，则不设置超时，并且调用可能会永远阻塞。</li></ul> 
</blockquote> 
<h3 id="2.4%C2%A0pipeline%E5%AE%9E%E6%88%98">2.4 pipeline实战</h3> 
<p>识别<a href="http://images.cocodataset.org/val2017/000000039769.jpg" rel="nofollow" title="http链接">http链接</a>中的物品</p> 
<p><img alt="" height="480" src="https://images2.imgbox.com/a3/04/aX1AFzYf_o.jpg" width="640"><span title="点击并拖拽以改变尺寸">​</span></p> 
<p>采用pipeline代码如下</p> 
<div> 
 <pre><code class="language-python hljs"><span class="hljs-keyword">import</span> os
os.environ[<span class="hljs-string">"HF_ENDPOINT"</span>] = <span class="hljs-string">"https://hf-mirror.com"</span>
os.environ[<span class="hljs-string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="hljs-string">"2"</span>

<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
image_segmentation = pipeline(task=<span class="hljs-string">"image-segmentation"</span>,model=<span class="hljs-string">"facebook/detr-resnet-50-panoptic"</span>)
output = image_segmentation(<span class="hljs-string">"http://images.cocodataset.org/val2017/000000039769.jpg"</span>)
<span class="hljs-built_in">print</span>(output)
<span class="hljs-string">"""
[{'score': 0.994096, 'label': 'cat', 'mask': &lt;PIL.Image.Image image mode=L size=640x480 at 0x7F24E13B4710&gt;}, {'score': 0.998669, 'label': 'remote', 'mask': &lt;PIL.Image.Image image mode=L size=640x480 at 0x7F24E13B4950&gt;}, {'score': 0.999476, 'label': 'remote', 'mask': &lt;PIL.Image.Image image mode=L size=640x480 at 0x7F24A2836250&gt;}, {'score': 0.972207, 'label': 'couch', 'mask': &lt;PIL.Image.Image image mode=L size=640x480 at 0x7F24A2837210&gt;}, {'score': 0.999423, 'label': 'cat', 'mask': &lt;PIL.Image.Image image mode=L size=640x480 at 0x7F24A2836290&gt;}]
"""</span>
output[<span class="hljs-number">0</span>][<span class="hljs-string">"mask"</span>].save(output[<span class="hljs-number">0</span>][<span class="hljs-string">"label"</span>]+<span class="hljs-string">".png"</span>)</code></pre> 
</div> 
<p>执行后，自动下载模型文件：</p> 
<p><img alt="" height="512" src="https://images2.imgbox.com/24/de/oK0OGRNJ_o.png" width="1200"><span title="点击并拖拽以改变尺寸">​</span></p> 
<p>图像分割后的结果会存入字典list中，将图片Image保存后，打开可见分割出的图片：</p> 
<p><img alt="" height="480" src="https://images2.imgbox.com/72/75/9aCIM93m_o.png" width="640"><span title="点击并拖拽以改变尺寸">​</span></p> 
<h3 id="2.5%C2%A0%E6%A8%A1%E5%9E%8B%E6%8E%92%E5%90%8D">2.5 模型排名</h3> 
<p>在huggingface上，我们将图像分割（image-segmentation）模型按下载量从高到低排序：</p> 
<p></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/f5/56/JsSbStSG_o.png" width="1200"></p> 
<h2 id="2.2.1%C2%A0%E5%AE%89%E8%A3%85timm%E5%BA%93"><strong>三、总结</strong></h2> 
<p>本文对transformers之pipeline的图像分割（image-segmentation）从概述、技术原理、pipeline参数、pipeline实战、模型排名等方面进行介绍，读者可以基于pipeline使用文中的2行代码极简的使用计算机视觉中的图像分割（image-segmentation）模型。</p> 
<p></p> 
<p>期待您的3连+关注，如何还有时间，欢迎阅读我的其他文章：</p> 
<p>《Transformers-Pipeline概述》</p> 
<p id="articleContentId"><a href="https://blog.csdn.net/weixin_48007632/article/details/140319929?spm=1001.2014.3001.5501" title="【人工智能】Transformers之Pipeline（概述）：30w+大模型极简应用">【人工智能】Transformers之Pipeline（概述）：30w+大模型极简应用</a></p> 
<p>《Transformers-Pipeline 第一章：音频（Audio）篇》</p> 
<p><a href="https://blog.csdn.net/weixin_48007632/article/details/140360594?spm=1001.2014.3001.5501" title="【人工智能】Transformers之Pipeline（一）：音频分类（audio-classification）">【人工智能】Transformers之Pipeline（一）：音频分类（audio-classification）</a></p> 
<p><a href="https://blog.csdn.net/weixin_48007632/article/details/140448072?spm=1001.2014.3001.5501" title="【人工智能】Transformers之Pipeline（二）：自动语音识别（automatic-speech-recognition）">【人工智能】Transformers之Pipeline（二）：自动语音识别（automatic-speech-recognition）</a></p> 
<p><a href="https://blog.csdn.net/weixin_48007632/article/details/140475532?spm=1001.2014.3001.5501" title="【人工智能】Transformers之Pipeline（三）：文本转音频（text-to-audio/text-to-speech）">【人工智能】Transformers之Pipeline（三）：文本转音频（text-to-audio/text-to-speech）</a></p> 
<p><a href="https://blog.csdn.net/weixin_48007632/article/details/140618181?spm=1001.2014.3001.5501" title="【人工智能】Transformers之Pipeline（四）：零样本音频分类（zero-shot-audio-classification）">【人工智能】Transformers之Pipeline（四）：零样本音频分类（zero-shot-audio-classification）</a></p> 
<p>《Transformers-Pipeline 第二章：计算机视觉（CV）篇》</p> 
<p><a href="https://blog.csdn.net/weixin_48007632/article/details/140671531?spm=1001.2014.3001.5501" title="【人工智能】Transformers之Pipeline（五）：深度估计（depth-estimation）">【人工智能】Transformers之Pipeline（五）：深度估计（depth-estimation）</a>​​​​​​​</p> 
<p><a class="link-info" href="https://blog.csdn.net/weixin_48007632/article/details/140778745?spm=1001.2014.3001.5502" title="【人工智能】Transformers之Pipeline（六）：图像分类（image-classification）">【人工智能】Transformers之Pipeline（六）：图像分类（image-classification）</a></p> 
<p>【人工智能】Transformers之Pipeline（七）：图像分割（image-segmentation）</p> 
<p>【人工智能】Transformers之Pipeline（八）：图生图（image-to-image）</p> 
<p>【人工智能】Transformers之Pipeline（九）：物体检测（object-detection）</p> 
<p>【人工智能】Transformers之Pipeline（十）：视频分类（video-classification）</p> 
<p>【人工智能】Transformers之Pipeline（十一）：零样本图片分类（zero-shot-image-classification）</p> 
<p>【人工智能】Transformers之Pipeline（十二）：零样本物体检测（zero-shot-object-detection）</p> 
<p>《Transformers-Pipeline 第三章：自然语言处理（NLP）篇》</p> 
<p>【人工智能】Transformers之Pipeline（十三）：填充蒙版（fill-mask）</p> 
<p>【人工智能】Transformers之Pipeline（十四）：问答（question-answering）</p> 
<p>【人工智能】Transformers之Pipeline（十五）：总结（summarization）</p> 
<p>【人工智能】Transformers之Pipeline（十六）：表格问答（table-question-answering）</p> 
<p>【人工智能】Transformers之Pipeline（十七）：文本分类（text-classification）</p> 
<p>【人工智能】Transformers之Pipeline（十八）：文本生成（text-generation）</p> 
<p>【人工智能】Transformers之Pipeline（十九）：文生文（text2text-generation）</p> 
<p>【人工智能】Transformers之Pipeline（二十）：令牌分类（token-classification）</p> 
<p>【人工智能】Transformers之Pipeline（二十一）：翻译（translation）</p> 
<p>【人工智能】Transformers之Pipeline（二十二）：零样本文本分类（zero-shot-classification）</p> 
<p>《Transformers-Pipeline 第四章：多模态（Multimodal）篇》</p> 
<p>【人工智能】Transformers之Pipeline（二十三）：文档问答（document-question-answering）</p> 
<p>【人工智能】Transformers之Pipeline（二十四）：特征抽取（feature-extraction）</p> 
<p>【人工智能】Transformers之Pipeline（二十五）：图片特征抽取（image-feature-extraction）</p> 
<p>【人工智能】Transformers之Pipeline（二十六）：图片转文本（image-to-text）</p> 
<p>【人工智能】Transformers之Pipeline（二十七）：掩码生成（mask-generation）</p> 
<p>【人工智能】Transformers之Pipeline（二十八）：视觉问答（visual-question-answering）</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4f72fe43f9272e5e43afa5e0840c0b44/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">谷粒商城实战笔记-110~114-全文检索-ElasticSearch-查询</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ce7b8ed37a250fb9b99d84e66aa212df/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">大语言模型时代的挑战与机遇：青年发展、教育变革与就业前景</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>