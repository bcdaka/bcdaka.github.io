<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>[Megagon Labs] Annotating Columns with Pre-trained Language Models - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/10d6dcf2da042a3cd507910a462d596e/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="[Megagon Labs] Annotating Columns with Pre-trained Language Models">
  <meta property="og:description" content="Annotating Columns with Pre-trained Language Models 任务定义 输入：一张数据表，但没有表头，只有表中的数据。
输出：每一列数据的数据类型，以及两列数据之间的关系。
数据类型和数据关系都是由训练数据决定的固定集合，可以视作多分类任务。
模型架构 整个模型的back bone依然是transformer，利用attention机制获取整表的语境信息。具体来说，DODUO将整个表格序列化，化二维为一维，每个column首尾相接连接成一个序列，而每个column用一个特殊token[CLS]隔开，整个序列以[SEP]结尾。
与BERT的做法类似，[CLS]这个特殊token被用来表示整个column的信息，同时这个column由于attention机制，除了自己所在的column信息也会聚合到其他column的context，这就是DODUO的核心思想。
同时，DODUO是一个多任务模型，两个分类任务：数据类别和数据关系。所以在共享transformer层作为编码器后，使用两个不同的Dense Layer来对应两个任务。数据类别任务直接取[CLS]作为输入，输出分类结果；而数据关系任务将两个[CLS]连接在一起作为输入，输出分类结果。这两个任务会在每个epoch中依次进行训练。
由于DODUO需要将序列化后的表中的token编码为embedding作为第一层transformer层的输入，所以对embedding模型同样做了微调，在反向传播过程中更新了12层BERT-base的参数。
*论文中好像没有提到中间的transformer layer到底有几层
整个结构的灵活性较强，核心的transformer back bone令知识在多任务之间共享，增加了泛化能力。而embedding模型和对应不同任务的dense layer都是可以灵活替换的。包括文章中也提到，使用更大更强的LM作为embedding模型可能会进一步提升效果。针对特殊数据（如数字、日期），采用对应的LM作为embedding模型也会提升性能。
实验结果 在不包含表头信息（即图中的metadata）的情况下，DODUO的性能超过了其他baseline，为SOTA。而TURL本身设计是需要表头的，此时TURL和DODUO的表现相近，甚至在数据关系任务上TURL优于DODUO。因此DODUO的优势还是在于表头信息缺失的情况，利用整表context，能得到信息更充分的编码。
消融实验的结果也说明了这个问题，DOSOLO是DODUO在单个任务上的版本，而DOSOLOscol则是只考虑单个任务单个column的版本，明显看到DOSOLO的性能知识略有下降，但DOSOLOscol的性能则是暴降。
另一方面，由于使用了pre-trained model和多任务训练，DODUO可以仅用少量数据训练达到较好的性能，图4和表8分别展示了在缩减训练集数量以及每个column token数量的情况下的性能变化。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-13T17:21:04+08:00">
    <meta property="article:modified_time" content="2024-08-13T17:21:04+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">[Megagon Labs] Annotating Columns with Pre-trained Language Models</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="Annotating_Columns_with_Pretrained_Language_Models_0"></a>Annotating Columns with Pre-trained Language Models</h2> 
<h2><a id="_1"></a>任务定义</h2> 
<p>输入：一张数据表，但没有表头，只有表中的数据。<br> 输出：每一列数据的数据类型，以及两列数据之间的关系。</p> 
<p>数据类型和数据关系都是由训练数据决定的固定集合，可以视作多分类任务。</p> 
<h2><a id="_6"></a>模型架构</h2> 
<p><img src="https://images2.imgbox.com/33/f8/tDpKDG6v_o.png" alt="在这里插入图片描述"><br> 整个模型的back bone依然是transformer，利用attention机制获取整表的语境信息。具体来说，DODUO将整个表格序列化，化二维为一维，每个column首尾相接连接成一个序列，而每个column用一个特殊token<code>[CLS]</code>隔开，整个序列以<code>[SEP]</code>结尾。<br> <img src="https://images2.imgbox.com/e3/3c/RAfZXVlZ_o.png" alt="在这里插入图片描述"><br> 与BERT的做法类似，<code>[CLS]</code>这个特殊token被用来表示整个column的信息，同时这个column由于attention机制，除了自己所在的column信息也会聚合到其他column的context，这就是DODUO的核心思想。<br> <img src="https://images2.imgbox.com/82/79/izZzgxuV_o.png" alt="在这里插入图片描述"><br> 同时，DODUO是一个多任务模型，两个分类任务：数据类别和数据关系。所以在共享transformer层作为编码器后，使用两个不同的Dense Layer来对应两个任务。数据类别任务直接取<code>[CLS]</code>作为输入，输出分类结果；而数据关系任务将两个<code>[CLS]</code>连接在一起作为输入，输出分类结果。这两个任务会在每个epoch中依次进行训练。<br> <img src="https://images2.imgbox.com/e9/cc/Ak6xDUMt_o.png" alt="在这里插入图片描述"></p> 
<p>由于DODUO需要将序列化后的表中的token编码为embedding作为第一层transformer层的输入，所以对embedding模型同样做了微调，在反向传播过程中更新了12层BERT-base的参数。</p> 
<p><em>*论文中好像没有提到中间的transformer layer到底有几层</em></p> 
<p>整个结构的灵活性较强，核心的transformer back bone令知识在多任务之间共享，增加了泛化能力。而embedding模型和对应不同任务的dense layer都是可以灵活替换的。包括文章中也提到，使用更大更强的LM作为embedding模型可能会进一步提升效果。针对特殊数据（如数字、日期），采用对应的LM作为embedding模型也会提升性能。</p> 
<h2><a id="_21"></a>实验结果</h2> 
<p><img src="https://images2.imgbox.com/24/ad/qzGiRRVm_o.png" alt="在这里插入图片描述"><br> 在不包含表头信息（即图中的metadata）的情况下，DODUO的性能超过了其他baseline，为SOTA。而TURL本身设计是需要表头的，此时TURL和DODUO的表现相近，甚至在数据关系任务上TURL优于DODUO。因此DODUO的优势还是在于表头信息缺失的情况，利用整表context，能得到信息更充分的编码。</p> 
<p><img src="https://images2.imgbox.com/02/7c/NQh71KHV_o.png" alt="在这里插入图片描述"><br> 消融实验的结果也说明了这个问题，DOSOLO是DODUO在单个任务上的版本，而DOSOLOscol则是只考虑单个任务单个column的版本，明显看到DOSOLO的性能知识略有下降，但DOSOLOscol的性能则是暴降。<br> <img src="https://images2.imgbox.com/51/82/XOe7vCkM_o.png" alt="在这里插入图片描述"><br> 另一方面，由于使用了pre-trained model和多任务训练，DODUO可以仅用少量数据训练达到较好的性能，图4和表8分别展示了在缩减训练集数量以及每个column token数量的情况下的性能变化。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6b4d1113a697cd6f5387ef11cf8a7186/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Linux下安装Mysql数据库</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/895377a035211dc90fb78e9e5d055c5a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Visual Studio 和 VSCode 哪个好？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>