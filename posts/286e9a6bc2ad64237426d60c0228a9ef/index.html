<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>将 HuggingFace 模型转换为 GGUF 及使用 ollama 运行 —— 以 Qwen2-0.5B 为例 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/286e9a6bc2ad64237426d60c0228a9ef/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="将 HuggingFace 模型转换为 GGUF 及使用 ollama 运行 —— 以 Qwen2-0.5B 为例">
  <meta property="og:description" content="前言 最近，阿里发布了Qwen2的系列模型，包括0.5B, 1.5B, 7B, 57B-A14B 和 72B，中英文效果都很好。
因为模型太新，目前还没有 GGUF 版本可以下载，于是转下GGUF，并分享转换教程。
什么是 GGUF？ GGUF 格式的全名为（GPT-Generated Unified Format），提到 GGUF 就不得不提到它的前身 GGML（GPT-Generated Model Language）。GGML 是专门为了机器学习设计的张量库，最早可以追溯到 2022/10。其目的是为了有一个单文件共享的格式，并且易于在不同架构的 GPU 和 CPU 上进行推理。但在后续的开发中，遇到了灵活性不足、相容性及难以维护的问题。
为什么要转换 GGUF 格式 在传统的 Deep Learning Model 开发中大多使用 PyTorch 来进行开发，但因为在部署时会面临相依 Lirbrary 太多、版本管理的问题于才有了 GGML、GGMF、GGJT 等格式，而在开源社群不停的迭代后 GGUF 就诞生了。
GGUF 实际上是基于 GGJT 的格式进行优化的，并解决了 GGML 当初面临的问题，包括：
可扩展性：轻松为 GGML 架构下的工具添加新功能，或者向 GGUF 模型添加新 Feature，不会破坏与现有模型的兼容性。对 mmap（内存映射）的兼容性：该模型可以使用 mmap 进行加载（原理解析可见参考），实现快速载入和存储。（从 GGJT 开始导入，可参考 GitHub）易于使用：模型可以使用少量代码轻松加载和存储，无需依赖的 Library，同时对于不同编程语言支持程度也高。模型信息完整：加载模型所需的所有信息都包含在模型文件中，不需要额外编写设置文件。有利于模型量化：GGUF 支持模型量化（4 位、8 位、F16），在 GPU 变得越来越昂贵的情况下，节省 vRAM 成本也非常重要。 动手来转档、量化和上传 整个转档的步骤如下：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-02T23:14:19+08:00">
    <meta property="article:modified_time" content="2024-08-02T23:14:19+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">将 HuggingFace 模型转换为 GGUF 及使用 ollama 运行 —— 以 Qwen2-0.5B 为例</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>前言</h2> 
<p>最近，阿里发布了Qwen2的系列模型，包括0.5B, 1.5B, 7B, 57B-A14B 和 72B，中英文效果都很好。</p> 
<p>因为模型太新，目前还没有 GGUF 版本可以下载，于是转下GGUF，并分享转换教程。</p> 
<h3><a id="_GGUF_6"></a>什么是 GGUF？</h3> 
<p>GGUF 格式的全名为（GPT-Generated Unified Format），提到 GGUF 就不得不提到它的前身 GGML（GPT-Generated Model Language）。GGML 是专门为了机器学习设计的张量库，最早可以追溯到 2022/10。其目的是为了有一个单文件共享的格式，并且易于在不同架构的 GPU 和 CPU 上进行推理。但在后续的开发中，遇到了灵活性不足、相容性及难以维护的问题。</p> 
<h3><a id="_GGUF__9"></a>为什么要转换 GGUF 格式</h3> 
<p>在传统的 Deep Learning Model 开发中大多使用 PyTorch 来进行开发，但因为在部署时会面临相依 Lirbrary 太多、版本管理的问题于才有了 GGML、GGMF、GGJT 等格式，而在开源社群不停的迭代后 GGUF 就诞生了。</p> 
<p>GGUF 实际上是基于 GGJT 的格式进行优化的，并解决了 GGML 当初面临的问题，包括：</p> 
<ol><li>可扩展性：轻松为 GGML 架构下的工具添加新功能，或者向 GGUF 模型添加新 Feature，不会破坏与现有模型的兼容性。</li><li>对 mmap（内存映射）的兼容性：该模型可以使用 mmap 进行加载（原理解析可见<a href="https://zhuanlan.zhihu.com/p/348102901" rel="nofollow">参考</a>），实现快速载入和存储。（从 GGJT 开始导入，可参考 <a href="https://github.com/ggerganov/ggml/discussions/492">GitHub</a>）</li><li>易于使用：模型可以使用少量代码轻松加载和存储，无需依赖的 Library，同时对于不同编程语言支持程度也高。</li><li>模型信息完整：加载模型所需的所有信息都包含在模型文件中，不需要额外编写设置文件。</li><li>有利于模型量化：GGUF 支持模型量化（4 位、8 位、F16），在 GPU 变得越来越昂贵的情况下，节省 vRAM 成本也非常重要。</li></ol> 
<h3><a id="_20"></a>动手来转档、量化和上传</h3> 
<p>整个转档的步骤如下：</p> 
<ol><li>从 HuggingFace 下载 Model</li><li>使用 llama.cpp 来进行转档</li><li>使用 llama.cpp 来进行量化模型</li><li>将转换和量化后的 GGUF 模型上传到 Huggingface Repo</li></ol> 
<p>可以参考llama.cpp作者写的教程：<a href="https://github.com/ggerganov/llama.cpp/discussions/2948">Tutorial: How to convert HuggingFace model to GGUF format</a></p> 
<h3><a id="_HuggingFace__Model_29"></a>从 HuggingFace 下载 Model</h3> 
<p>最直觉是用 git clone 来下载模型，但是因为 LLM 每个一部分都按 GB 来计算，避免出现 OOM Error 的情况，简单用 Python 写一个 download.py 比较简单。</p> 
<pre><code class="prism language-shell">pip <span class="token function">install</span> huggingface_hub
</code></pre> 
<p>新增<code>download.py</code>，写入：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> huggingface_hub <span class="token keyword">import</span> snapshot_download
model_id<span class="token operator">=</span><span class="token string">"Qwen/Qwen2-0.5B-Instruct"</span>
snapshot_download<span class="token punctuation">(</span>repo_id<span class="token operator">=</span>model_id<span class="token punctuation">,</span> local_dir<span class="token operator">=</span><span class="token string">"qwen2_0.5b_instruct"</span><span class="token punctuation">,</span>
                          local_dir_use_symlinks<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> revision<span class="token operator">=</span><span class="token string">"main"</span><span class="token punctuation">)</span>
</code></pre> 
<p>运行下载：<br> <img src="https://images2.imgbox.com/07/ba/ZU92OLuq_o.png" alt="在这里插入图片描述"></p> 
<p>接下来，把刚刚从 HuggingFace 下载的 Model 转换成 GGUF 格式。</p> 
<h3><a id="hfgguf_47"></a>开始转换hf模型为gguf</h3> 
<p>需要用llama.cpp仓库的convert_hf_to_gguf.py脚本来转换。</p> 
<pre><code>git clone https://github.com/ggerganov/llama.cpp.git
pip install -r llama.cpp/requirements.txt
python llama.cpp/convert_hf_to_gguf.py -h
</code></pre> 
<p>执行转换：</p> 
<pre><code class="prism language-shell"><span class="token comment"># 如果不量化，保留模型的效果</span>
python llama.cpp/convert_hf_to_gguf.py ./qwen2_0.5b_instruct  <span class="token parameter variable">--outtype</span> f16 <span class="token parameter variable">--verbose</span> <span class="token parameter variable">--outfile</span> qwen2_0.5b_instruct_f16.gguf
<span class="token comment"># 如果需要量化（加速并有损效果），直接执行下面脚本就可以</span>
python llama.cpp/convert_hf_to_gguf.py ./qwen2_0.5b_instruct  <span class="token parameter variable">--outtype</span> q8_0 <span class="token parameter variable">--verbose</span> <span class="token parameter variable">--outfile</span> qwen2_0.5b_instruct_q8_0.gguf
</code></pre> 
<p>这里<code>--outtype</code>是输出类型，代表含义：</p> 
<ul><li>q2_k：特定张量（Tensor）采用较高的精度设置，而其他的则保持基础级别。</li><li>q3_k_l、q3_k_m、q3_k_s：这些变体在不同张量上使用不同级别的精度，从而达到性能和效率的平衡。</li><li>q4_0：这是最初的量化方案，使用 4 位精度。</li><li>q4_1 和 q4_k_m、q4_k_s：这些提供了不同程度的准确性和推理速度，适合需要平衡资源使用的场景。</li><li>q5_0、q5_1、q5_k_m、q5_k_s：这些版本在保证更高准确度的同时，会使用更多的资源并且推理速度较慢。</li><li>q6_k 和 q8_0：这些提供了最高的精度，但是因为高资源消耗和慢速度，可能不适合所有用户。</li><li>fp16 和 f32: 不量化，保留原始精度。</li></ul> 
<p><img src="https://images2.imgbox.com/55/50/TV9b2rQT_o.jpg" alt="量化示意图"></p> 
<h3><a id="ollamagguf_76"></a>使用ollama运行gguf</h3> 
<p>转换好的 GGUF 模型可以使用以下的工具来运行：</p> 
<ul><li><strong>ollama(推荐)</strong>: 简化 AI 模型的本地部署与使用</li><li>llama.cpp：GGUF 的源项目。提供 CLI 和 Server 选项。</li><li>text-generation-webui：最广泛使用的网络界面，具有许多功能和强大的扩展。支持 GPU 加速。</li><li>GPT4All：一个免费且开源的本地运行图形用户界面，支持 Windows、Linux 和 macOS，并支持 GPU 加速。</li><li>LM Studio：一个易于使用且功能强大的本地图形用户界面，适用于 Windows 和 macOS（Silicon），支持 GPU 加速。</li><li>llama-cpp-python：支持 GPU 加速、LangChain 和 OpenAI 兼容 API 服务器的 Python 库。</li><li>candle：一个使用 Rust 编写的机器学习框架，具有 GPU 支持和易于使用的特点，适合追求性能的开发者。</li></ul> 
<p>可以使用ollama Modelfile，基于gguf模型文件快速部署并运行模型。</p> 
<h5><a id="ollama_89"></a>安装ollama</h5> 
<pre><code>url -fsSL https://ollama.com/install.sh | sh   
</code></pre> 
<h5><a id="ollama_94"></a>启动ollama服务</h5> 
<pre><code>nohup ollama serve &amp;
</code></pre> 
<h5><a id="ModelFile_100"></a>创建ModelFile</h5> 
<p>复制模型路径，创建名为“ModelFile”的meta文件，内容如下：</p> 
<pre><code class="prism language-shell">FROM /mnt/workspace/qwen2-0.5b-instruct-q8_0.gguf

<span class="token comment"># set the temperature to 0.7 [higher is more creative, lower is more coherent]</span>
PARAMETER temperature <span class="token number">0.7</span>
PARAMETER top_p <span class="token number">0.8</span>
PARAMETER repeat_penalty <span class="token number">1.05</span>
TEMPLATE <span class="token string">""</span>"<span class="token punctuation">{<!-- --></span><span class="token punctuation">{<!-- --></span> <span class="token keyword">if</span> .System <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token operator">&lt;</span><span class="token operator">|</span>im_start<span class="token operator">|</span><span class="token operator">&gt;</span>system
<span class="token punctuation">{<!-- --></span><span class="token punctuation">{<!-- --></span> .System <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token operator">&lt;</span><span class="token operator">|</span>im_end<span class="token operator">|</span><span class="token operator">&gt;</span>
<span class="token punctuation">{<!-- --></span><span class="token punctuation">{<!-- --></span> end <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">{<!-- --></span> <span class="token keyword">if</span> .Prompt <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token operator">&lt;</span><span class="token operator">|</span>im_start<span class="token operator">|</span><span class="token operator">&gt;</span>user
<span class="token punctuation">{<!-- --></span><span class="token punctuation">{<!-- --></span> .Prompt <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token operator">&lt;</span><span class="token operator">|</span>im_end<span class="token operator">|</span><span class="token operator">&gt;</span>
<span class="token punctuation">{<!-- --></span><span class="token punctuation">{<!-- --></span> end <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token operator">&lt;</span><span class="token operator">|</span>im_start<span class="token operator">|</span><span class="token operator">&gt;</span>assistant
<span class="token punctuation">{<!-- --></span><span class="token punctuation">{<!-- --></span> .Response <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token operator">&lt;</span><span class="token operator">|</span>im_end<span class="token operator">|</span><span class="token operator">&gt;</span><span class="token string">""</span>"
<span class="token comment"># set the system message</span>
SYSTEM <span class="token string">""</span>"
You are a helpful assistant.
<span class="token string">""</span>"
</code></pre> 
<h5><a id="_121"></a>创建自定义模型</h5> 
<p>使用ollama create命令创建自定义模型</p> 
<pre><code class="prism language-shell">ollama create qwen2_0.5b_instruct <span class="token parameter variable">--file</span> ./ModelFile
</code></pre> 
<p>运行模型：<br> <code>ollama run qwen2_0.5b_instruct</code></p> 
<p>测试自定义模型，使用终端与您的自定义模型聊天，以确保其行为符合预期。验证它是否根据自定义的系统提示和模板做出响应。</p> 
<p><img src="https://images2.imgbox.com/3c/36/ss2ZE5eD_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Reference_135"></a>Reference</h3> 
<p><a href="https://medium.com/@phillipgimmi/what-is-gguf-and-ggml-e364834d241c" rel="nofollow">什么是 GGUF 和 GGML？</a><br> <a href="https://deci.ai/blog/ggml-vs-gguf-comparing-formats-amp-top-5-methods-for-running-gguf-files/" rel="nofollow">GGUF 与 GGML：为什么 GGUF 是更好的文件格式</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0ab9f421a3fdb62cfe31b5a41d48f1e5/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">《LeetCode热题100》---＜5.①普通数组篇五道＞</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9dfe86b031cad4464fee3bff604fcfd7/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C&amp;C&#43;&#43;:贪吃蛇小游戏教程</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>