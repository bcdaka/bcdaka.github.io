<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>《大众点评爬虫程序实战：爬取店铺评论信息》 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/289c518714dea89d49128ae28cca9a94/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="《大众点评爬虫程序实战：爬取店铺评论信息》">
  <meta property="og:description" content="一、前言 上一章节介绍了如何使用selenium与requests爬取大众点评店铺相关信息，本章将介绍如何爬取指定美食店铺下的评论信息
二、爬取目标 三、准备工作 登录大众点评后生成的cookies，获取方法上一章节有介绍，不清楚的请自行查阅上一篇文章
四、分析 通过上一篇文章获取到的munu.json文件我们知道美食店铺列表的链接为：https://www.dianping.com/{}/ch10，其中{}里面的值为城市的拼音，下面我将以肇庆为例来演示如何爬取店铺评论，因此目标链接为：https://www.dianping.com/zhaoqing/ch10
​
首先我们要先拿到店铺的链接才能顺理成章的拿到该店铺下的评论信息，用过f12观察元素可以清晰的发现店铺的链接就藏在这一个个的li标签下面。
接下来我们观察进入店铺页面点击更多评论时页面上的链接变成：https://www.dianping.com/shop/G2QCvbC34hFSrNIb/review_all
点击第二页链接变成：https://www.dianping.com/shop/G2QCvbC34hFSrNIb/review_all/p2，这时候我们可以猜测，其实p1，p2，p3，……分别会对应第一页，第二页，第三页……。这里我给出结论吧，经过验证我们的猜想是正确的
最后爬取评论信息
四、项目代码（带登录cookies） 获取店铺链接 #!/usr/bin/env python3 # coding:utf-8 import bag from bs4 import BeautifulSoup import re url = &#39;https://www.dianping.com/zhaoqing/ch10&#39; session = bag.session.create_session() # 创建session管理器 for cookie in bag.Bag.read_json(r&#39;./cookies.json&#39;): # 添加cookies session.cookies.set(cookie[&#39;name&#39;], cookie[&#39;value&#39;]) resp = session.get(url) # 发起请求 resp.encoding = &#39;utf8&#39; resp.close() # print(resp.text) html = BeautifulSoup(resp.text, &#39;lxml&#39;) # 解析网页数据 soup = html.findAll(&#39;div&#39;, id=&#39;shop-all-list&#39;) # 缩小查找范围 # print(soup) pattern = re.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-18T23:42:23+08:00">
    <meta property="article:modified_time" content="2024-04-18T23:42:23+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">《大众点评爬虫程序实战：爬取店铺评论信息》</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>一、前言</h3> 
<p>上一章节介绍了如何使用selenium与requests爬取大众点评店铺相关信息，本章将介绍如何爬取指定美食店铺下的评论信息</p> 
<h3>二、爬取目标<img alt="" height="601" src="https://images2.imgbox.com/26/30/tua5IgZd_o.png" width="1079"></h3> 
<h3>三、准备工作</h3> 
<p>登录大众点评后生成的cookies，获取方法<a class="link-info" href="https://mp.csdn.net/mp_blog/creation/editor/136517019" title="上一章节">上一章节</a>有介绍，不清楚的请自行查阅<a class="link-info" href="https://mp.csdn.net/mp_blog/creation/editor/136517019" title="上一篇文章">上一篇文章</a></p> 
<h3>四、分析</h3> 
<p>通过上一篇文章获取到的munu.json文件我们知道美食店铺列表的链接为：https://www.dianping.com/{}/ch10，其中{}里面的值为城市的拼音，下面我将以肇庆为例来演示如何爬取店铺评论，因此目标链接为：https://www.dianping.com/zhaoqing/ch10</p> 
<p><img alt="" height="712" src="https://images2.imgbox.com/26/08/aR43wnbB_o.png" width="1200">​</p> 
<p>首先我们要先拿到店铺的链接才能顺理成章的拿到该店铺下的评论信息，用过f12观察元素可以清晰的发现店铺的链接就藏在这一个个的li标签下面。</p> 
<p>接下来我们观察进入店铺页面点击更多评论时页面上的链接变成：https://www.dianping.com/shop/G2QCvbC34hFSrNIb/review_all</p> 
<p>点击第二页链接变成：https://www.dianping.com/shop/G2QCvbC34hFSrNIb/review_all/p2，这时候我们可以猜测，其实p1，p2，p3，……分别会对应第一页，第二页，第三页……。这里我给出结论吧，经过验证我们的猜想是正确的</p> 
<p>最后爬取评论信息</p> 
<h3>四、项目代码（带登录cookies）</h3> 
<ul><li> <h4>获取店铺链接</h4> 
  <div> 
   <pre><code class="language-python">#!/usr/bin/env python3
# coding:utf-8
import bag
from bs4 import BeautifulSoup
import re


url = 'https://www.dianping.com/zhaoqing/ch10'

session = bag.session.create_session()  # 创建session管理器

for cookie in bag.Bag.read_json(r'./cookies.json'):  # 添加cookies
    session.cookies.set(cookie['name'], cookie['value'])


resp = session.get(url)  # 发起请求
resp.encoding = 'utf8'
resp.close()
# print(resp.text)

html = BeautifulSoup(resp.text, 'lxml')  # 解析网页数据
soup = html.findAll('div', id='shop-all-list')  # 缩小查找范围
# print(soup)

pattern = re.compile(r'&lt;div.*?class="pic"&gt;*?&gt;.*?&lt;a.*?href="(.*?)"', re.S)  # 使用正则表达式精确匹配

urls = re.findall(pattern, str(soup))
for url in urls:
    print(url)

</code></pre> 
  </div> </li><li> <h4>爬取评论</h4> 接下来演示爬取https://www.dianping.com/shop/G2QCvbC34hFSrNIb/review_all这个页面的评论信息，其他页面只需要在这个基础上加上循环修改一下页数即可实现，这里就不作过多演示了，感兴趣的自行下去探究 
  <div> 
   <pre><code class="language-python">#!/usr/bin/env python3
# coding:utf-8
import bag
from bs4 import BeautifulSoup
import re

session = bag.session.create_session()

for cookie in bag.Bag.read_json(r'./cookies.json'):
    session.cookies.set(cookie['name'], cookie['value'])

url = r'https://www.dianping.com/shop/G2QCvbC34hFSrNIb/review_all'
resp = session.get(url)
resp.encoding = 'utf8'
resp.close()
html = BeautifulSoup(resp.text, 'lxml')
soup = html.findAll('div', class_='review-words Hide')
pattern = re.compile(r'&lt;div class="review-words Hide"&gt;(.*?)&lt;div class="less-words"&gt;', re.S)
for i in soup:
    info = re.findall(pattern, str(i))
    for j in info[0].split():
        if j.strip() == '':
            pass
        else:
            print(j.strip())
    print('-'*100)

</code></pre> 
   <p class="img-center"><img alt="wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" height="299" src="https://images2.imgbox.com/0a/97/GNVkqEhw_o.png" width="534"></p> 
  </div> </li><li> <h4>小结</h4> </li></ul> 
<p>         上面演示了携带登录cookies的爬取方法，携带登录信息后课可以完美跳过大宗点评的字体反扒机制，下面介绍一下如果无登录信息如何还原加密字体</p> 
<h3>五、字体加密规则</h3> 
<p> <img alt="" height="235" src="https://images2.imgbox.com/27/3e/wdkqPHxs_o.png" width="285"> </p> 
<p>这里面有三个文件，第一个文件是不带cookies获取到的评论数据，看到的信息是不完整的，第二个dianping.css文件，这是一个固定的文件，第三个svg后缀的文件，这个文件里面包含了需要还原的字体，如下图所示：<img alt="" height="502" src="https://images2.imgbox.com/6a/c0/9OysSKzw_o.png" width="1030"></p> 
<p>这里我直接给出结论吧，这个映射关系搞了一个多小时才做出来，三两句话也说不清楚</p> 
<pre><code class="language-python">'''css映射关系
1.确定x，y坐标
2.x坐标除14等于文字所在位置
3.y坐标最接近的数字等于文字所在行
'''</code></pre> 
<p>完整代码：</p> 
<pre><code class="language-python">#!/usr/bin/env python3
# coding:utf-8
import re
import os


class Decode:
    """解密大众点评数据"""
    def __init__(self, html, css, svg):
        self.text = os.getcwd()+'\\'+html
        self.css_path = os.getcwd()+'\\'+css
        self.svg_path = os.getcwd()+'\\'+svg
        self.content = ''
        self.words = {}
        self.word = {}
        self.word_num = []
        self.word_value = []
        self.css_text, self.svg_text, self.flag = self.read_info()
        self.selection = self.mapping()
        self.recovery()

    '''读取文css、svg、加密字段信息'''
    def read_info(self):
        flag = []
        with open(self.text, mode='r', encoding='utf8') as tf:
            self.content = tf.read()
        for i in re.findall(r'class="(.*?)"&gt;', self.content):
            flag.append([i, len(i)])

        with open(self.css_path, mode='r', encoding='utf8') as cf:
            css = cf.read()

        with open(self.svg_path, mode='r', encoding='utf8') as sf:
            svg = sf.read()
        return css, svg, flag

    '''创建映射关系'''
    def mapping(self):
        mid = []
        selection = {}
        for i in self.flag:
            try:
                res = re.search(r'{}(.*?);'.format(i[0]), self.css_text).group()
                mid.append([res[:i[1]], res[i[1]:]])
            except AttributeError:
                continue

        coordinate = re.compile(r'-(\d{1,5})\..*?-(\d{1,5})\.')
        for j in mid:
            for k in re.findall(coordinate, j[1]):
                selection[j[0]] = k

        y = re.compile(r'&lt;text x="0" y="(.*?)"&gt;(.*?)&lt;/text&gt;', re.S)
        for i in re.findall(y, self.svg_text):
            self.words[i[0]] = i[1]
            self.word_num.append(i[0])
            self.word_value.append(i[1])
        return selection

    '''恢复文字'''
    def recovery(self):
        for key, value in self.selection.items():
            d_value = 50
            label, number = key, [int(value[0]) // 14, int(value[1])]
            x = number[0]
            y = ''
            y1 = ''
            for y_key, w_value in self.words.items():
                y1 = y
                if abs(number[1] - int(y_key)) &lt; d_value:
                    d_value = abs(number[1] - int(y_key))
                    y = y_key
                else:
                    continue
            print(x, label, y1)
            count = self.word_num.index(y1) + 1
            print(count)
            # print(word_value[count])
            # if int(y1) % 8 == 0:
            if int(y1) - number[1] == 23:
                self.word[label] = self.word_value[count - 1][x]
            else:
                self.word[label] = self.word_value[count][x]
        data = self.content.replace('&lt;svgmtsi class="', '').replace('"&gt;', ' ')
        data = re.sub(r'&amp;.*?;|&lt;.*?&gt;', '', data)

        for key, value in self.word.items():
            data = data.replace(key, value)

        print(data.replace(' ', ''))


Decode('css', 'dianping.css', 'dianping.svg')
</code></pre> 
<p> 最终还原的结果： <img alt="" height="240" src="https://images2.imgbox.com/8c/34/sGKrEjte_o.png" width="1200"> </p> 
<h4>小结 ：</h4> 
<p>这个不带登录信息的爬取还原过程还是比较复杂的，感兴趣的可以下去自行探究，因为这里无法上传文件，需要拿上面三个文件测试的可以私聊获取</p> 
<h3>六、总结</h3> 
<p> 本章节介绍了两种爬取大众点评美食店铺评论的信息的方法，主要思路通过正向一步步往后推，得出相关页面的结论在写代码去实现</p> 
<h3>七、声明</h3> 
<ul><li>本文只做学习交流用，切勿同于非法用途</li><li>如本文内容侵犯到您的合法权益，请联系删除</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/32c2623717bad7208bba151fa1d627c7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">ELK（Elasticsearch&#43;Logstash&#43;Kibana）日志分析系统</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/aedafd19c6eb98270faacd6c5b6f5f7d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Hadoop高可靠集群搭建步骤（手把手教学）【超级详细】</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>