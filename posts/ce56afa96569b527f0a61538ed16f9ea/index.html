<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>æ˜‡æ€25å¤©å­¦ä¹ æ‰“å¡è¥ç¬¬16å¤©|æ–‡æœ¬è§£ç åŸç†â€”â€”ä»¥MindNLPä¸ºä¾‹ - ç¼–ç¨‹å¤§å’–</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/ce56afa96569b527f0a61538ed16f9ea/">
  <meta property="og:site_name" content="ç¼–ç¨‹å¤§å’–">
  <meta property="og:title" content="æ˜‡æ€25å¤©å­¦ä¹ æ‰“å¡è¥ç¬¬16å¤©|æ–‡æœ¬è§£ç åŸç†â€”â€”ä»¥MindNLPä¸ºä¾‹">
  <meta property="og:description" content="åœ¨å¤§æ¨¡å‹ä¸­ï¼Œæ–‡æœ¬è§£ç é€šå¸¸æ˜¯æŒ‡åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­ä½¿ç”¨çš„å¤§å‹ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆå¦‚Transformeræ¶æ„çš„æ¨¡å‹ï¼‰å°†ç¼–ç åçš„æ–‡æœ¬æ•°æ®è½¬æ¢å›å¯è¯»çš„åŸå§‹æ–‡æœ¬çš„è¿‡ç¨‹ã€‚è¿™äº›æ¨¡å‹åœ¨å¤„ç†è‡ªç„¶è¯­è¨€æ—¶ï¼Œé¦–å…ˆå°†è¾“å…¥æ–‡æœ¬ï¼ˆå¦‚ä¸€æ®µè¯æˆ–ä¸€ä¸ªå¥å­ï¼‰ç¼–ç æˆé«˜ç»´ç©ºé—´ä¸­çš„å‘é‡è¡¨ç¤ºï¼Œè¿™äº›å‘é‡èƒ½å¤Ÿæ•æ‰åˆ°æ–‡æœ¬çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
åœ¨ç¼–ç è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹é€šè¿‡å¤šå±‚ç¥ç»ç½‘ç»œå°†æ–‡æœ¬çš„æ¯ä¸ªå­—ç¬¦ã€å•è¯æˆ–æ ‡è®°ï¼ˆtokenï¼‰è½¬æ¢æˆå¯¹åº”çš„å‘é‡ã€‚è¿™äº›å‘é‡éšååœ¨æ¨¡å‹çš„è§£ç é˜¶æ®µè¢«å¤„ç†ï¼Œä»¥ç”Ÿæˆæˆ–é€‰æ‹©æœ€åˆé€‚çš„åºåˆ—æ¥è¡¨ç¤ºåŸå§‹æ–‡æœ¬çš„å«ä¹‰ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œè§£ç é˜¶æ®µä¼šç”Ÿæˆç›®æ ‡è¯­è¨€çš„æ–‡æœ¬ï¼›åœ¨æ–‡æœ¬æ‘˜è¦ä»»åŠ¡ä¸­ï¼Œè§£ç é˜¶æ®µä¼šç”ŸæˆåŸæ–‡çš„æ‘˜è¦ï¼›åœ¨é—®ç­”ç³»ç»Ÿä¸­ï¼Œè§£ç é˜¶æ®µä¼šç”Ÿæˆé—®é¢˜çš„ç­”æ¡ˆã€‚
ä¸€ã€è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼š 1ã€æ ¹æ®å‰æ–‡é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼š
2ã€ä¸€ä¸ªæ–‡æœ¬åºåˆ—çš„æ¦‚ç‡åˆ†å¸ƒå¯ä»¥åˆ†è§£ä¸ºæ¯ä¸ªè¯åŸºäºå…¶ä¸Šæ–‡çš„æ¡ä»¶æ¦‚ç‡çš„ä¹˜ç§¯Â ï¼š
w_0ï¼šåˆå§‹ä¸Šä¸‹æ–‡å•è¯åºåˆ—Tï¼šæ—¶é—´æ­¥å½“ç”Ÿå­˜ESOæ ‡ç­¾æ—¶åœæ­¢ç”ŸæˆÂ 3ã€MindNLP/huggingface Transformersæä¾›çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼š
äºŒã€ç¯å¢ƒå‡†å¤‡ï¼š é¦–å…ˆè¿˜æ˜¯éœ€è¦ä¸‹è½½MindSporeï¼Œç›¸å…³æ•™ç¨‹å¯ä»¥å‚è€ƒæˆ‘æ˜‡æ€25å¤©å­¦ä¹ æ‰“å¡è¥ç¬¬1å¤©|å¿«é€Ÿå…¥é—¨è¿™ç¯‡åšå®¢ï¼Œä¹‹åå°±éœ€è¦ä½¿ç”¨pipå‘½ä»¤åœ¨ç»ˆç«¯å¸è½½mindvisionå’ŒmindinsightåŒ…ä¹‹åï¼Œä¸‹è½½mindnlpï¼š
pip uninstall mindvision -y pip uninstall mindinsight -y pip install mindnlp ç›¸å…³ä¾èµ–ä¸‹è½½å®Œæˆä¹‹åï¼Œå°±å¯ä»¥å¼€å§‹æˆ‘ä»¬ä¸‹é¢çš„å®éªŒäº†ï¼
ä¸‰ã€Greedy Search: åœ¨æ¯ä¸ªæ—¶é—´æ­¥ğ‘¡éƒ½ç®€å•åœ°é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯ä½œä¸ºå½“å‰è¾“å‡ºè¯:
wt = argmax_w P(w|w(1:t-1))
æŒ‰ç…§è´ªå¿ƒæœç´¢è¾“å‡ºåºåˆ—(&#34;The&#34;,&#34;nice&#34;,&#34;woman&#34;)Â çš„æ¡ä»¶æ¦‚ç‡ä¸ºï¼š0.5 x 0.4 = 0.2
ç¼ºç‚¹: é”™è¿‡äº†éšè—åœ¨ä½æ¦‚ç‡è¯åé¢çš„é«˜æ¦‚ç‡è¯ï¼Œå¦‚ï¼šdog=0.5, has=0.9 ![image.png](attachment:image.png =600x600)
from mindnlp.transformers import GPT2Tokenizer, GPT2LMHeadModel tokenizer = GPT2Tokenizer.from_pretrained(&#34;iiBcai/gpt2&#34;, mirror=&#39;modelscope&#39;) # add the EOS token as PAD token to avoid warnings model = GPT2LMHeadModel.from_pretrained(&#34;iiBcai/gpt2&#34;, pad_token_id=tokenizer.eos_token_id, mirror=&#39;modelscope&#39;) # encode context the generation is conditioned on input_ids = tokenizer.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-04T16:12:45+08:00">
    <meta property="article:modified_time" content="2024-07-04T16:12:45+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="ç¼–ç¨‹å¤§å’–" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">ç¼–ç¨‹å¤§å’–</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">æ˜‡æ€25å¤©å­¦ä¹ æ‰“å¡è¥ç¬¬16å¤©|æ–‡æœ¬è§£ç åŸç†â€”â€”ä»¥MindNLPä¸ºä¾‹</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><img alt="" height="693" src="https://images2.imgbox.com/ce/2f/naIVMQkZ_o.png" width="1200"></p> 
<p>åœ¨å¤§æ¨¡å‹ä¸­ï¼Œæ–‡æœ¬è§£ç é€šå¸¸æ˜¯æŒ‡åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­ä½¿ç”¨çš„å¤§å‹ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆå¦‚Transformeræ¶æ„çš„æ¨¡å‹ï¼‰å°†ç¼–ç åçš„æ–‡æœ¬æ•°æ®è½¬æ¢å›å¯è¯»çš„åŸå§‹æ–‡æœ¬çš„è¿‡ç¨‹ã€‚è¿™äº›æ¨¡å‹åœ¨å¤„ç†è‡ªç„¶è¯­è¨€æ—¶ï¼Œé¦–å…ˆå°†è¾“å…¥æ–‡æœ¬ï¼ˆå¦‚ä¸€æ®µè¯æˆ–ä¸€ä¸ªå¥å­ï¼‰ç¼–ç æˆé«˜ç»´ç©ºé—´ä¸­çš„å‘é‡è¡¨ç¤ºï¼Œè¿™äº›å‘é‡èƒ½å¤Ÿæ•æ‰åˆ°æ–‡æœ¬çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</p> 
<p>åœ¨ç¼–ç è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹é€šè¿‡å¤šå±‚ç¥ç»ç½‘ç»œå°†æ–‡æœ¬çš„æ¯ä¸ªå­—ç¬¦ã€å•è¯æˆ–æ ‡è®°ï¼ˆtokenï¼‰è½¬æ¢æˆå¯¹åº”çš„å‘é‡ã€‚è¿™äº›å‘é‡éšååœ¨æ¨¡å‹çš„è§£ç é˜¶æ®µè¢«å¤„ç†ï¼Œä»¥ç”Ÿæˆæˆ–é€‰æ‹©æœ€åˆé€‚çš„åºåˆ—æ¥è¡¨ç¤ºåŸå§‹æ–‡æœ¬çš„å«ä¹‰ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œè§£ç é˜¶æ®µä¼šç”Ÿæˆç›®æ ‡è¯­è¨€çš„æ–‡æœ¬ï¼›åœ¨æ–‡æœ¬æ‘˜è¦ä»»åŠ¡ä¸­ï¼Œè§£ç é˜¶æ®µä¼šç”ŸæˆåŸæ–‡çš„æ‘˜è¦ï¼›åœ¨é—®ç­”ç³»ç»Ÿä¸­ï¼Œè§£ç é˜¶æ®µä¼šç”Ÿæˆé—®é¢˜çš„ç­”æ¡ˆã€‚</p> 
<h2>Â ä¸€ã€è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼š</h2> 
<p>1ã€<strong>æ ¹æ®å‰æ–‡é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼š</strong></p> 
<p><img alt="" height="702" src="https://images2.imgbox.com/8e/e7/tMJFVjU7_o.png" width="1200"></p> 
<p><strong>2ã€ä¸€ä¸ªæ–‡æœ¬åºåˆ—çš„æ¦‚ç‡åˆ†å¸ƒå¯ä»¥åˆ†è§£ä¸ºæ¯ä¸ªè¯åŸºäºå…¶ä¸Šæ–‡çš„æ¡ä»¶æ¦‚ç‡çš„ä¹˜ç§¯</strong>Â ï¼š</p> 
<p><img alt="" height="199" src="https://images2.imgbox.com/a2/c6/p2uu1fLB_o.png" width="1200"></p> 
<ul><li>w_0ï¼šåˆå§‹ä¸Šä¸‹æ–‡å•è¯åºåˆ—</li><li>Tï¼šæ—¶é—´æ­¥</li><li>å½“ç”Ÿå­˜ESOæ ‡ç­¾æ—¶åœæ­¢ç”ŸæˆÂ </li></ul> 
<p>3ã€<strong>MindNLP/huggingface Transformersæä¾›çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼š</strong></p> 
<p><img alt="" height="540" src="https://images2.imgbox.com/4a/d9/46Lqs2bI_o.png" width="1200"></p> 
<h2>Â äºŒã€ç¯å¢ƒå‡†å¤‡ï¼š</h2> 
<p>é¦–å…ˆè¿˜æ˜¯éœ€è¦ä¸‹è½½MindSporeï¼Œç›¸å…³æ•™ç¨‹å¯ä»¥å‚è€ƒæˆ‘<a href="https://blog.csdn.net/2301_77286822/article/details/139811196?spm=1001.2014.3001.5501" title="æ˜‡æ€25å¤©å­¦ä¹ æ‰“å¡è¥ç¬¬1å¤©|å¿«é€Ÿå…¥é—¨">æ˜‡æ€25å¤©å­¦ä¹ æ‰“å¡è¥ç¬¬1å¤©|å¿«é€Ÿå…¥é—¨</a>è¿™ç¯‡åšå®¢ï¼Œä¹‹åå°±éœ€è¦ä½¿ç”¨pipå‘½ä»¤åœ¨ç»ˆç«¯å¸è½½mindvisionå’ŒmindinsightåŒ…ä¹‹åï¼Œä¸‹è½½mindnlpï¼š</p> 
<pre><code class="language-python">pip uninstall mindvision -y
pip uninstall mindinsight -y

pip install mindnlp</code></pre> 
<p>ç›¸å…³ä¾èµ–ä¸‹è½½å®Œæˆä¹‹åï¼Œå°±å¯ä»¥å¼€å§‹æˆ‘ä»¬ä¸‹é¢çš„å®éªŒäº†ï¼</p> 
<h2>ä¸‰ã€Greedy Search:</h2> 
<p>åœ¨æ¯ä¸ªæ—¶é—´æ­¥ğ‘¡éƒ½ç®€å•åœ°é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯ä½œä¸ºå½“å‰è¾“å‡ºè¯:</p> 
<p>wt = argmax_w P(w|w(1:t-1))</p> 
<p>æŒ‰ç…§è´ªå¿ƒæœç´¢è¾“å‡ºåºåˆ—("The","nice","woman")Â çš„æ¡ä»¶æ¦‚ç‡ä¸ºï¼š0.5 x 0.4 = 0.2</p> 
<p>ç¼ºç‚¹: é”™è¿‡äº†éšè—åœ¨ä½æ¦‚ç‡è¯åé¢çš„é«˜æ¦‚ç‡è¯ï¼Œå¦‚ï¼šdog=0.5, has=0.9 ![image.png](attachment:image.png =600x600)</p> 
<pre><code class="language-python">from mindnlp.transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained("iiBcai/gpt2", mirror='modelscope')

# add the EOS token as PAD token to avoid warnings
model = GPT2LMHeadModel.from_pretrained("iiBcai/gpt2", pad_token_id=tokenizer.eos_token_id, mirror='modelscope')

# encode context the generation is conditioned on
input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='ms')

# generate text until the output length (which includes the context length) reaches 50
greedy_output = model.generate(input_ids, max_length=50)

print("Output:\n" + 100 * '-')
print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))</code></pre> 
<p><img alt="" height="589" src="https://images2.imgbox.com/6e/3f/2HVCbaeO_o.png" width="1200"></p> 
<h2>å››ã€Beam Search:</h2> 
<p>Beam searché€šè¿‡åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¿ç•™æœ€å¯èƒ½çš„Â num_beamsÂ ä¸ªè¯ï¼Œå¹¶ä»ä¸­æœ€ç»ˆé€‰æ‹©å‡ºæ¦‚ç‡æœ€é«˜çš„åºåˆ—æ¥é™ä½ä¸¢å¤±æ½œåœ¨çš„é«˜æ¦‚ç‡åºåˆ—çš„é£é™©ã€‚å¦‚å›¾ä»¥Â num_beams=2Â ä¸ºä¾‹:</p> 
<p>("The","dog","has") : 0.4 * 0.9 = 0.36</p> 
<p>("The","nice","woman") : 0.5 * 0.4 = 0.20</p> 
<p>ä¼˜ç‚¹ï¼šä¸€å®šç¨‹åº¦ä¿ç•™æœ€ä¼˜è·¯å¾„</p> 
<p>ç¼ºç‚¹ï¼š1. æ— æ³•è§£å†³é‡å¤é—®é¢˜ï¼›2. å¼€æ”¾åŸŸç”Ÿæˆæ•ˆæœå·®</p> 
<p><img alt="" height="868" src="https://images2.imgbox.com/93/3b/8C5iZq5p_o.png" width="1077"></p> 
<pre><code class="language-python">from mindnlp.transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained("iiBcai/gpt2", mirror='modelscope')

# add the EOS token as PAD token to avoid warnings
model = GPT2LMHeadModel.from_pretrained("iiBcai/gpt2", pad_token_id=tokenizer.eos_token_id, mirror='modelscope')

# encode context the generation is conditioned on
input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='ms')

# activate beam search and early_stopping
beam_output = model.generate(
    input_ids, 
    max_length=50, 
    num_beams=5, 
    early_stopping=True
)

print("Output:\n" + 100 * '-')
print(tokenizer.decode(beam_output[0], skip_special_tokens=True))
print(100 * '-')

# set no_repeat_ngram_size to 2
beam_output = model.generate(
    input_ids, 
    max_length=50, 
    num_beams=5, 
    no_repeat_ngram_size=2, 
    early_stopping=True
)

print("Beam search with ngram, Output:\n" + 100 * '-')
print(tokenizer.decode(beam_output[0], skip_special_tokens=True))
print(100 * '-')

# set return_num_sequences &gt; 1
beam_outputs = model.generate(
    input_ids, 
    max_length=50, 
    num_beams=5, 
    no_repeat_ngram_size=2, 
    num_return_sequences=5, 
    early_stopping=True
)

# now we have 3 output sequences
print("return_num_sequences, Output:\n" + 100 * '-')
for i, beam_output in enumerate(beam_outputs):
    print("{}: {}".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))
print(100 * '-')</code></pre> 
<p>Â <img alt="" height="853" src="https://images2.imgbox.com/6b/01/qjRBV1SF_o.png" width="1200"></p> 
<p>ç¼ºç‚¹çš„å…·ä½“è¡¨ç°ï¼š</p> 
<p>é‡å¤æ€§é«˜ï¼Œè¿™ä¸ªçœ‹æˆ‘ç”Ÿæˆçš„ä¾‹å­å°±å¯ä»¥å¾ˆæ¸…æ¥šçš„çœ‹åˆ°ï¼Œç€å‡ å¥è¯å‡ ä¹ä¸€æ¨¡ä¸€æ ·ï¼Œè¿˜æœ‰å°±æ˜¯å¼€æ”¾åŸŸçš„é—®é¢˜ï¼Œå¯ä»¥çœ‹ä¸‹å›¾ï¼š</p> 
<p><img alt="" height="706" src="https://images2.imgbox.com/68/c9/Cfo3tHBJ_o.png" width="1078">Â </p> 
<h2>äº”ã€è¶…å‚æ•°ï¼š</h2> 
<p>ç”±äºæ™®é€šçš„é»˜è®¤ç´¢å¼•å‡å­˜åœ¨ç€éš¾ä»¥å…‹æœçš„é—®é¢˜ï¼Œäººä»¬é€šå¸¸ä¼šä½¿ç”¨å„ç§è¶…å‚æ•°æ¥å‡å°ç´¢å¼•ç¼ºé™·çš„å½±å“ã€‚</p> 
<h3>1ã€n_gramæƒ©ç½šï¼š</h3> 
<p>å°†å‡ºç°è¿‡çš„å€™é€‰è¯çš„æ¦‚ç‡è®¾ç½®ä¸º 0</p> 
<p>è®¾ç½®no_repeat_ngram_size=2Â ï¼Œä»»æ„Â 2-gramÂ ä¸ä¼šå‡ºç°ä¸¤æ¬¡</p> 
<p>Notice: å®é™…æ–‡æœ¬ç”Ÿæˆéœ€è¦é‡å¤å‡ºç°</p> 
<p><img alt="" height="700" src="https://images2.imgbox.com/cd/88/IJ6gdaeC_o.png" width="1200"></p> 
<h3>Â 2ã€Sample:</h3> 
<p>æ ¹æ®å½“å‰æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©è¾“å‡ºè¯w_t</p> 
<p><img alt="" height="639" src="https://images2.imgbox.com/0d/43/S81fqif2_o.png" width="1200"></p> 
<p>ä¼˜ç‚¹ï¼šæ–‡æœ¬ç”Ÿæˆå¤šæ ·æ€§é«˜</p> 
<p>ç¼ºç‚¹ï¼šç”Ÿæˆæ–‡æœ¬ä¸è¿ç»­</p> 
<pre><code class="language-python">import mindspore
from mindnlp.transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained("iiBcai/gpt2", mirror='modelscope')

# add the EOS token as PAD token to avoid warnings
model = GPT2LMHeadModel.from_pretrained("iiBcai/gpt2", pad_token_id=tokenizer.eos_token_id, mirror='modelscope')

# encode context the generation is conditioned on
input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='ms')

mindspore.set_seed(0)
# activate sampling and deactivate top_k by setting top_k sampling to 0
sample_output = model.generate(
    input_ids, 
    do_sample=True, 
    max_length=50, 
    top_k=0
)

print("Output:\n" + 100 * '-')
print(tokenizer.decode(sample_output[0], skip_special_tokens=True))</code></pre> 
<p>Â <img alt="" height="235" src="https://images2.imgbox.com/25/43/8giYVcBB_o.png" width="1200"></p> 
<h3>3ã€Temperature:</h3> 
<p>é™ä½softmaxÂ çš„temperatureä½¿ P(wâˆ£w1:tâˆ’1â€‹)åˆ†å¸ƒæ›´é™¡å³­ï¼Œä»¥å¢åŠ é«˜æ¦‚ç‡å•è¯çš„ä¼¼ç„¶å¹¶é™ä½ä½æ¦‚ç‡å•è¯çš„ä¼¼ç„¶ã€‚</p> 
<p><img alt="" height="415" src="https://images2.imgbox.com/a6/2a/504xtI2A_o.png" width="1200"></p> 
<p>Â </p> 
<pre><code class="language-python">import mindspore
from mindnlp.transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained("iiBcai/gpt2", mirror='modelscope')

# add the EOS token as PAD token to avoid warnings
model = GPT2LMHeadModel.from_pretrained("iiBcai/gpt2", pad_token_id=tokenizer.eos_token_id, mirror='modelscope')

# encode context the generation is conditioned on
input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='ms')

mindspore.set_seed(1234)
# activate sampling and deactivate top_k by setting top_k sampling to 0
sample_output = model.generate(
    input_ids, 
    do_sample=True, 
    max_length=50, 
    top_k=0,
    temperature=0.7
)

print("Output:\n" + 100 * '-')
print(tokenizer.decode(sample_output[0], skip_special_tokens=True))</code></pre> 
<p><img alt="" height="171" src="https://images2.imgbox.com/e0/aa/91qekwzK_o.png" width="1200">Â </p> 
<h3>4ã€Topk Sample:</h3> 
<p>é€‰å‡ºæ¦‚ç‡æœ€å¤§çš„Â KÂ ä¸ªè¯ï¼Œé‡æ–°å½’ä¸€åŒ–ï¼Œæœ€ååœ¨å½’ä¸€åŒ–åçš„Â KÂ ä¸ªè¯ä¸­é‡‡æ ·ï¼Œç¡®å®šå°±æ˜¯ï¼šå°†é‡‡æ ·æ± é™åˆ¶ä¸ºå›ºå®šå¤§å°Â K å¯¼è‡´åœ¨åˆ†å¸ƒæ¯”è¾ƒå°–é”çš„æ—¶å€™äº§ç”Ÿèƒ¡è¨€ä¹±è¯­å’Œåœ¨åˆ†å¸ƒæ¯”è¾ƒå¹³å¦çš„æ—¶å€™é™åˆ¶æ¨¡å‹çš„åˆ›é€ åŠ›ã€‚</p> 
<p><img alt="" height="658" src="https://images2.imgbox.com/f2/c2/T9p0HaO9_o.png" width="1200"></p> 
<p>Â </p> 
<pre><code class="language-python">import mindspore
from mindnlp.transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained("iiBcai/gpt2", mirror='modelscope')

# add the EOS token as PAD token to avoid warnings
model = GPT2LMHeadModel.from_pretrained("iiBcai/gpt2", pad_token_id=tokenizer.eos_token_id, mirror='modelscope')

# encode context the generation is conditioned on
input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='ms')

mindspore.set_seed(0)
# activate sampling and deactivate top_k by setting top_k sampling to 0
sample_output = model.generate(
    input_ids, 
    do_sample=True, 
    max_length=50, 
    top_k=50
)

print("Output:\n" + 100 * '-')
print(tokenizer.decode(sample_output[0], skip_special_tokens=True))
</code></pre> 
<p><img alt="" height="271" src="https://images2.imgbox.com/c4/ec/vx3tY64U_o.png" width="1200"></p> 
<h3>5ã€Top_P Sample:</h3> 
<p>åœ¨ç´¯ç§¯æ¦‚ç‡è¶…è¿‡æ¦‚ç‡Â pÂ çš„æœ€å°å•è¯é›†ä¸­è¿›è¡Œé‡‡æ ·ï¼Œé‡æ–°å½’ä¸€åŒ–,ç¼ºç‚¹å°±æ˜¯ï¼šé‡‡æ ·æ± å¯ä»¥æ ¹æ®ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒåŠ¨æ€å¢åŠ å’Œå‡å°‘ã€‚</p> 
<p><img alt="" height="679" src="https://images2.imgbox.com/1f/01/f9FH0ctM_o.png" width="1200"></p> 
<pre><code class="language-python">import mindspore
from mindnlp.transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained("iiBcai/gpt2", mirror='modelscope')

# add the EOS token as PAD token to avoid warnings
model = GPT2LMHeadModel.from_pretrained("iiBcai/gpt2", pad_token_id=tokenizer.eos_token_id, mirror='modelscope')

# encode context the generation is conditioned on
input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='ms')

mindspore.set_seed(0)

# deactivate top_k sampling and sample only from 92% most likely words
sample_output = model.generate(
    input_ids, 
    do_sample=True, 
    max_length=50, 
    top_p=0.92, 
    top_k=0
)

print("Output:\n" + 100 * '-')
print(tokenizer.decode(sample_output[0], skip_special_tokens=True))</code></pre> 
<p><img alt="" height="255" src="https://images2.imgbox.com/5f/97/yWr5r2xx_o.png" width="1200"></p> 
<h3>Â 6ã€Top_k_Top_p:</h3> 
<pre><code class="language-python">import mindspore
from mindnlp.transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained("iiBcai/gpt2", mirror='modelscope')

# add the EOS token as PAD token to avoid warnings
model = GPT2LMHeadModel.from_pretrained("iiBcai/gpt2", pad_token_id=tokenizer.eos_token_id, mirror='modelscope')

# encode context the generation is conditioned on
input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='ms')

mindspore.set_seed(0)
# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3
sample_outputs = model.generate(
    input_ids,
    do_sample=True,
    max_length=50,
    top_k=5,
    top_p=0.95,
    num_return_sequences=3
)

print("Output:\n" + 100 * '-')
for i, sample_output in enumerate(sample_outputs):
  print("{}: {}".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))</code></pre> 
<p><img alt="" height="448" src="https://images2.imgbox.com/a4/03/BwMx21qa_o.png" width="1200"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/cd50d10fc28e9fc1ede429779934bb8d/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">ä¸­å›½å†œä¸šä¼šè®¡ç¼–è¾‘éƒ¨ä¸­å›½å†œä¸šä¼šè®¡æ‚å¿—ç¤¾2024å¹´ç¬¬10æœŸç›®å½•</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8dd8d414ff95103f4103eb21c2c39240/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">å¦‚ä½•æ›´æ”¹ Python pip æºä¸ºå›½å†…æº</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 ç¼–ç¨‹å¤§å’–.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>