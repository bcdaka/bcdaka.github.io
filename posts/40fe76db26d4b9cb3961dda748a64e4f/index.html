<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>具身智能（Embodied AI）的概念、核心要素、难点及突破性进展 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/40fe76db26d4b9cb3961dda748a64e4f/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="具身智能（Embodied AI）的概念、核心要素、难点及突破性进展">
  <meta property="og:description" content="在ChatGPT之后，具身智能（Embodied AI）这个大模型概念火了，那什么是具身智能呢？
什么是具身智能？ 具身智能作为人工智能发展的一个重要分支，正在迅速崭露头角，成为科技界和大众关注的热门话题，同时在各个领域中展现出巨大的潜力和吸引力
具身智能通过在物理世界和数字世界的学习和进化，达到理解世界、互动交互并完成任务的目标。
具身智能是由“本体”和“智能体”耦合而成且能够在复杂环境中执行任务的智能系统。
以上概念是等价的：
Embodied AI = Embodied Intelligence = 具象AI = 具身智能`
Internet AI = Disembodied AI = 非具身智能
相关概念 具身（Embodiment)：指具有支持感觉和运动（sensorimotor）的物理身体。
具身的 （Embodied）：具有身体的，可参与交互、感知的。
具身智能(Embodied AI)：有身体并支持物理交互的智能体，如家用服务机器人、无人车等。 —— “身体力行”
非具身智能（Disembodied AI）：没有物理身体，只能被动接受人类采集、制作好的数据 。—— “纸上谈兵”或者说 “运筹帷幄”
具身智能机器人：满足具身智能的能力的机器人. 即具身智能机器人：首先，要能够听懂人类语言，然后，分解任务，规划子任务，移动中识别物体，与环境交互，最终完成相应任务
具身任务：像人类一样通过观察、移动、说话和与世界互动从而完成的一系列任务。
多模态：是指一个模型或系统能够处理多种不同类型的输入数据并融合它们生成输出。这些数据类型可能包括文本、图像、音频和视频等。
主动交互：机器人或智能体与环境的实时交互，从而提高智能体的学习、交流和应对问题的能力。
Internet AI从互联网收集到的图像、视频或文本数据集中学习，这些数据集往往制作精良，其与真实世界脱节、难以泛化和迁移。1）数据到标签的映射。2）无法在真实世界进行体验学习。3）无法在真实世界做出影响。
Embodied AI通过与环境的互动，虽然以第一视角得到的数据不够稳定，但这种类似于人类的自我中心感知中学习，从而从视觉、语言和推理到一个人工具象（Artificial Embodiment），可以帮助解决更多真实问题
核心要素 一般认为，具身智能具有如下的四大核心要素：
本体：作为实际的执行者，是在物理或者虚拟世界进行感知和任务执行的机构。 本体通常是具有物理实体的机器人，可以有多种形态。本体的能力边界会限制智能体的能力发挥，所以，具有广泛适应性的机器人本体是非常必要的。如四足机器人、复合机器人、人形机器人本体具备环境感知能力、运动能力和操作执行能力，是连接数字世界和物理世界的载体。 智能体（Embodied Agents），是具身于本体之上的智能核心，负责感知、理解、决策、控制等的核心工作。 智能体可以感知复杂环境，理解环境所包含的语义信息，能够和环境进行交互；可以理解具体任务，并且根据环境的变化和目标状态做出决策，进而控制本体完成任务。随着深度学习的发展，现代智能体通常由深度网络模型驱动，尤其是随着大语言模型（LLM）的发展，结合视觉等多种传感器的复杂多模态模型，已经开始成为新一代智能体的趋势。智能体也分化为多种任务形态，处理不同层次和模态的任务。智能体要能够从复杂的数据中学习决策和控制的范式，并且能够持续的自我演进，进而适应更复杂的任务和环境。智能体设计是具身智能的核心。具有通用能力的LLM和VLM等模型，赋予了通用本体强大的泛化能力，使得机器人从程序执行导向转向任务目标导向，向通用机器人迈出了坚实的步伐。 数据：“数据是泛化的关键，但涉及机器人的数据稀缺且昂贵。” 为了适应复杂环境和任务的泛化性，智能体规模变的越来越大，而大规模的模型对于海量数据更为渴求。现在的LLM通常需要web-scale级别的数据来驱动基础的预训练过程，而针对具身智能的场景则更为复杂多样，这造成了多变的环境和任务，以及围绕着复杂任务链的规划决策控制数据。尤其是针对行业场景的高质量数据，将是未来具身智能成功应用落地的关键支撑。 学习和进化架构：智能体通过和物理世界（虚拟的或真实的）的交互，来适应新环境、学习新知识并强化出新的解决问题方法。 采用虚拟仿真环境进行部分学习是合理的设计，比如英伟达的元宇宙开发平台Omniverse，就是构建了物理仿真的虚拟世界，来加速智能体的演进。真实环境的复杂度通常超过仿真环境，如何耦合仿真和真实世界，进行高效率的迁移（Sim2Real），也是架构设计的关键。 具身智能的难点剖析 需要有强大的通用本体平台：
如何解决硬件的关键零部件技术突破，形成具有优秀运动能力和操作能力的平台级通用机器人产品，将具身本体的可靠性、成本和通用能力做到平衡，是一个巨大的挑战。考虑到通用能力，人形机器人被认为是具身智能的终极形态。这方面的研发，也将持续成为热点和核心挑战。 设计强大的智能体系统
作为具身智能的核心，具备复杂环境感知认知能力的智能体，将需要解决诸多挑战，包括：物理3D环境精确感知、任务编排与执行、强大的通识能力、多级语义推理能力、人机口语多轮交互能力、long-term记忆能力、个性化情感关怀能力、强大的任务泛化与自学迁移能力等。具身智能要求实时感知和决策能力，以适应复杂和变化的环境。这要求高速的数据采集、传输和处理，以及实时的决策反应，尤其是LLM所消耗的算力规模巨大，对于资源有限的机器人处理系统将形成巨大的数据量、AI计算能力和低延迟的挑战 高质量的行业数据将成为巨大挑战
现实场景的复杂多变，使得现阶段缺乏足够的场景数据来训练一个完全通用的大模型，进而让智能体自我进化。耦合的本体，需要实际部署到真实环境中，才能够采集数据，这也是和非具身智能的明显不同。但对于关键业务，要求成功率，则仍然需要高质量的垂域数据。同时，通过层次化的智能体设计，将不同任务限定到特定领域，则是一个解决泛化和成功率的有效尝试 通过虚拟和真实的交互，持续学习和进化的能力，则是具身智能演进的重要技术途径
学习新任务来适应环境的变化，则是持续改进的动力。形态适配环境合适的智能体，则可以快速的学习到解决问题能力，进而更好的适应变化。由于形态的变化空间无穷巨大，搜索所有可能的选择在有限的计算资源情况下变的几乎不可能。本体的自由度设计，也会物理上约束智能体的任务执行能力，进而限制了控制器的学习效果。在复杂环境、形态演化和任务的可学习性之间，存在着未可知的隐式关系，如何快速学习到合理的规划和决策能力，则成为具身智能的重要一环。 2023最新突破性进展 PaLM-E: An Embodied Multimodal Language Model: 一个具身多模态语言模型">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-02-25T22:11:16+08:00">
    <meta property="article:modified_time" content="2024-02-25T22:11:16+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">具身智能（Embodied AI）的概念、核心要素、难点及突破性进展</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>在ChatGPT之后，具身智能（Embodied AI）这个大模型概念火了，那什么是具身智能呢？</p> 
<h2><a id="_2"></a>什么是具身智能？</h2> 
<p><strong>具身智能</strong>作为人工智能发展的一个重要分支，正在迅速崭露头角，成为科技界和大众关注的热门话题，同时在各个领域中展现出巨大的潜力和吸引力</p> 
<p><code>具身智能通过在物理世界和数字世界的学习和进化，达到理解世界、互动交互并完成任务的目标。</code></p> 
<p><strong>具身智能是由“本体”和“智能体”耦合而成且能够在复杂环境中执行任务的智能系统</strong>。</p> 
<p>以上概念是等价的：<br> <font color="blue">Embodied AI = Embodied Intelligence = 具象AI = 具身智能`</font><br> <font color="blue">Internet AI = Disembodied AI = 非具身智能</font></p> 
<h3><a id="_13"></a>相关概念</h3> 
<ul><li> <p><strong>具身（Embodiment)</strong>：指具有支持感觉和运动（sensorimotor）的物理身体。</p> </li><li> <p><strong>具身的 （Embodied）</strong>：具有身体的，可参与交互、感知的。</p> </li><li> <p><strong>具身智能(Embodied AI)</strong>：有身体并支持物理交互的智能体，如家用服务机器人、无人车等。 —— “身体力行”</p> </li><li> <p><strong>非具身智能（Disembodied AI）</strong>：没有物理身体，只能被动接受人类采集、制作好的数据 。—— “纸上谈兵”或者说 “运筹帷幄”</p> </li><li> <p><strong>具身智能机器人</strong>：满足具身智能的能力的机器人. 即<strong>具身智能机器人：首先，要能够听懂人类语言，然后，分解任务，规划子任务，移动中识别物体，与环境交互，最终完成相应任务</strong></p> </li><li> <p><strong>具身任务</strong>：像人类一样通过观察、移动、说话和与世界互动从而完成的一系列任务。</p> </li><li> <p><strong>多模态</strong>：是指一个模型或系统能够处理多种不同类型的输入数据并融合它们生成输出。这些数据类型可能包括文本、图像、音频和视频等。</p> </li><li> <p><strong>主动交互</strong>：机器人或智能体与环境的实时交互，从而提高智能体的学习、交流和应对问题的能力。</p> </li></ul> 
<p><strong>Internet AI</strong>从互联网收集到的图像、视频或文本数据集中学习，这些数据集往往制作精良，其与真实世界脱节、难以泛化和迁移。1）数据到标签的映射。2）无法在真实世界进行体验学习。3）无法在真实世界做出影响。</p> 
<p><strong>Embodied AI</strong>通过与环境的互动，虽然以第一视角得到的数据不够稳定，但这种类似于人类的自我中心感知中学习，从而从视觉、语言和推理到一个人工具象（Artificial Embodiment），可以帮助解决更多真实问题</p> 
<h3><a id="_33"></a>核心要素</h3> 
<p>一般认为，具身智能具有如下的<strong>四大核心要素</strong>：</p> 
<ul><li><strong>本体</strong>：作为实际的执行者，是在物理或者虚拟世界进行感知和任务执行的机构。 
  <ul><li>本体通常是具有物理实体的机器人，可以有多种形态。本体的能力边界会限制智能体的能力发挥，所以，具有广泛适应性的机器人本体是非常必要的。如四足机器人、复合机器人、人形机器人</li><li>本体具备环境感知能力、运动能力和操作执行能力，是连接数字世界和物理世界的载体。</li></ul> </li><li><strong>智能体（Embodied Agents）</strong>，是具身于本体之上的智能核心，负责感知、理解、决策、控制等的核心工作。 
  <ul><li>智能体可以感知复杂环境，理解环境所包含的语义信息，能够和环境进行交互；可以理解具体任务，并且根据环境的变化和目标状态做出决策，进而控制本体完成任务。</li><li>随着深度学习的发展，现代智能体通常由深度网络模型驱动，尤其是随着大语言模型（LLM）的发展，结合视觉等多种传感器的复杂多模态模型，已经开始成为新一代智能体的趋势。</li><li>智能体也分化为多种任务形态，处理不同层次和模态的任务。智能体要能够从复杂的数据中学习决策和控制的范式，并且能够持续的自我演进，进而适应更复杂的任务和环境。</li><li>智能体设计是具身智能的核心。具有通用能力的LLM和VLM等模型，赋予了通用本体强大的泛化能力，使得机器人从程序执行导向转向任务目标导向，向通用机器人迈出了坚实的步伐。</li></ul> </li><li><strong>数据</strong>：“数据是泛化的关键，但涉及机器人的数据稀缺且昂贵。” 
  <ul><li>为了适应复杂环境和任务的泛化性，智能体规模变的越来越大，而大规模的模型对于海量数据更为渴求。现在的LLM通常需要web-scale级别的数据来驱动基础的预训练过程，而针对具身智能的场景则更为复杂多样，这造成了多变的环境和任务，以及围绕着复杂任务链的规划决策控制数据。尤其是针对行业场景的高质量数据，将是未来具身智能成功应用落地的关键支撑。</li></ul> </li><li><strong>学习和进化架构</strong>：智能体通过和物理世界（虚拟的或真实的）的交互，来适应新环境、学习新知识并强化出新的解决问题方法。 
  <ul><li>采用虚拟仿真环境进行部分学习是合理的设计，比如英伟达的元宇宙开发平台Omniverse，就是构建了物理仿真的虚拟世界，来加速智能体的演进。</li><li>真实环境的复杂度通常超过仿真环境，如何耦合仿真和真实世界，进行高效率的迁移（Sim2Real），也是架构设计的关键。</li></ul> </li></ul> 
<h2><a id="_50"></a>具身智能的难点剖析</h2> 
<ul><li> <p><strong>需要有强大的通用本体平台</strong>：</p> 
  <ul><li>如何解决硬件的关键零部件技术突破，形成具有优秀运动能力和操作能力的平台级通用机器人产品，将具身本体的可靠性、成本和通用能力做到平衡，是一个巨大的挑战。</li><li>考虑到通用能力，人形机器人被认为是具身智能的终极形态。这方面的研发，也将持续成为热点和核心挑战。</li></ul> </li><li> <p><strong>设计强大的智能体系统</strong></p> 
  <ul><li>作为具身智能的核心，具备复杂环境感知认知能力的智能体，将需要解决诸多挑战，包括：物理3D环境精确感知、任务编排与执行、强大的通识能力、多级语义推理能力、人机口语多轮交互能力、long-term记忆能力、个性化情感关怀能力、强大的任务泛化与自学迁移能力等。</li><li>具身智能要求实时感知和决策能力，以适应复杂和变化的环境。这要求高速的数据采集、传输和处理，以及实时的决策反应，尤其是LLM所消耗的算力规模巨大，对于资源有限的机器人处理系统将形成巨大的数据量、AI计算能力和低延迟的挑战</li></ul> </li><li> <p><strong>高质量的行业数据</strong>将成为巨大挑战</p> 
  <ul><li>现实场景的复杂多变，使得现阶段缺乏足够的场景数据来训练一个完全通用的大模型，进而让智能体自我进化。</li><li>耦合的本体，需要实际部署到真实环境中，才能够采集数据，这也是和非具身智能的明显不同。</li><li>但对于<strong>关键业务，要求成功率，则仍然需要高质量的垂域数据</strong>。同时，通过层次化的智能体设计，将不同任务限定到特定领域，则是一个解决泛化和成功率的有效尝试</li></ul> </li><li> <p>通过虚拟和真实的交互，<strong>持续学习和进化的能力，则是具身智能演进的重要技术途径</strong></p> 
  <ul><li>学习新任务来适应环境的变化，则是持续改进的动力。形态适配环境合适的智能体，则可以快速的学习到解决问题能力，进而更好的适应变化。</li><li>由于形态的变化空间无穷巨大，搜索所有可能的选择在有限的计算资源情况下变的几乎不可能。本体的自由度设计，也会物理上约束智能体的任务执行能力，进而限制了控制器的学习效果。</li><li>在复杂环境、形态演化和任务的可学习性之间，存在着未可知的隐式关系，如何快速学习到合理的规划和决策能力，则成为具身智能的重要一环。</li></ul> </li></ul> 
<h2><a id="2023_69"></a>2023最新突破性进展</h2> 
<ol><li> <p>PaLM-E: An Embodied Multimodal Language Model: 一个具身多模态语言模型<br> 论文提出了一个具身多模态语言模型，通过将真实世界的连续传感器模态直接融入语言模型中，实现了单词和感知之间的联系。实验结果表明，PaLM-E可以处理来自不同观察模态的各种具身推理任务，并在多个实现上表现出良好的效果。最大的PaLM-E-562B模型拥有562亿个参数，除了在机器人任务上进行训练外，还是一个视觉语言通才，并在OK-VQA任务上取得了最先进的性能。</p> </li><li> <p>VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models<br> 采用语言模型实现机器人操作的可组合3D价值图<br> 论文提出了一种名为VoxPoser的方法，利用大型语言模型和视觉语言模型来合成机器人轨迹。作者发现，LLM可以通过自然语言指令推断出环境和物体的能力和限制，并通过与VLM交互来组合3D值图，将知识转化为代理的观察空间。这些组合的值图然后被用于基于模型的规划框架中，以零样本合成闭环机器人轨迹，并对动态扰动具有鲁棒性。</p> </li><li> <p>March in Chat: Interactive Prompting for Remote Embodied Referring Expression<br> 远程具身指代表达的交互提示<br> 论文提出了一种名为March-in-Chat的模型，可以在REVERIE环境中与大型语言模型进行交互并动态规划。REVERIE任务只提供高级指令给代理，类似于人类的实际命令，因此比其他VLN任务更具挑战性。MiC模型通过ROASP实现了环境感知和动态规划，可以基于新的视觉观察调整导航计划，并且能够适应更大、更复杂的REVERIE环境。</p> </li><li> <p>Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions<br> 通过多专家讨论实现视觉语言导航<br> 论文提出了一种零样本视觉语言导航框架DiscussNav，通过多专家讨论来帮助代理进行导航。作者认为现有的VLN方法完全依赖单一模型自身的思考来进行预测，而即使是最先进的大型语言模型GPT4，在单轮自我思考中仍然难以处理多个任务。因此，作者借鉴了专家咨询会议的思想，将具有不同能力的大模型作为领域专家，让代理在每一步移动之前与这些专家积极讨论，收集关键信息。实验结果表明，该方法可以有效地促进导航，感知与指令相关的信息，纠正意外错误并筛选出不一致的运动决策</p> </li><li> <p>Skill Transformer: A Monolithic Policy for Mobile Manipulation<br> 用于移动操作的单体策略<br> 论文提出了Skill Transformer，一种结合条件序列建模和技能模块性来解决长视野机器人任务的方法。该方法在机器人的自适应和感知观察上基于条件序列模型，并通过训练使用Transformer架构和演示轨迹来预测机器人的高级技能(如导航、选择、放置)和整体低级动作(如基座和手臂运动)。它保留了整个任务的可组合性和模块性，通过一个技能预测模块来推理低级动作并避免常见于模块化方法的传递误差。</p> </li><li> <p>See to Touch: Learning Tactile Dexterity through Visual Incentives<br> 通过视觉激励学习触觉灵活性<br> 论文提出了一种名为Tactile Adaptation from Visual Incentives (TAVI)的新框架，通过使用视觉奖励来优化基于触觉的灵巧性策略，从而提高多指机器人的精确度、丰富性和灵活性。在六个具有挑战性的任务中，TAVI使用四指Allegro机器人手实现了73%的成功率，比使用基于触觉和视觉奖励的策略提高了108％，比不使用基于触觉观察输入的策略提高了135％。</p> </li><li> <p>Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents<br> 用于执行指令的具身代理的上下文感知规划和环境感知记忆<br> 论文提出了一种CAPEAM方法，用于改善具身代理在视觉导航和对象交互方面的表现。该方法考虑了执行动作的后果，并将语义上下文和已交互物体的状态变化纳入一系列动作中，以推断后续动作。实验证明，该方法在各种指标上实现了最先进的性能，并在未见过的环境中获得了大幅提高。</p> </li><li> <p>Statler: State-Maintaining Language Models for Embodied Reasoning<br> 用于具身推理的状态维护语言模型<br> 论文提出了一种名为Statler的框架，用于赋予大型语言模型（LLM）对世界状态的显式表示，可以随着时间的推移进行维护。通过使用两个通用LLM实例——世界模型阅读器和世界模型写入器——与世界状态进行交互和维护，Statler提高了现有LLM在较长时间范围内推理的能力，而不受上下文长度的限制。</p> </li><li> <p>Embodied Task Planning with Large Language Models<br> 基于大型语言模型的具身任务规划<br> 本研究提出了一种名为TAsk Planing Agent（TaPA）的基于场景约束的具身任务规划方法，用于在真实世界中生成可执行的计划。该方法通过将大型语言模型与视觉感知模型对齐，根据场景中已存在的对象生成可执行计划。另外，作者还构建了一个多模态数据集，并使用GPT-3.5生成了大量的指令和相应的计划动作。</p> </li><li> <p>Conditionally Combining Robot Skills using Large Language Models<br> 使用大型语言模型有条件地组合机器人技能<br> 论文提出了两个贡献。首先，介绍了一个名为“Language-World”的Meta-World基准扩展，允许大型语言模型在模拟机器人环境中使用自然语言查询和脚本化技能进行操作。其次，引入了一种称为计划条件行为克隆（PCBC）的方法，可以使用端到端演示微调高级计划的行为。使用Language-World，表明PCBC能够在各种少数情况中实现强大的性能，通常只需要单个演示即可实现任务泛化。</p> </li></ol> 
<h2><a id="_105"></a>参考</h2> 
<p><a href="https://www.thepaper.cn/newsDetail_forward_24392857" rel="nofollow">稚晖君独家撰文：具身智能即将为通用机器人补全最后一块拼图</a><br> <a href="https://zhuanlan.zhihu.com/p/669226987" rel="nofollow">具身智能2023最新突破性进展分享！附17篇论文和代码</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/417f49413d81ed151aa60f420429993d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Android安卓app渗透测试环境-抓包教程</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b294d2e4a725437a90e267b183eb3922/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python pandas 对DataFrame进行遍历(持续更新）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>