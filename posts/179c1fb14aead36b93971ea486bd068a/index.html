<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【Spark集群部署系列二】Spark StandAlone模式介绍和搭建以及使用 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/179c1fb14aead36b93971ea486bd068a/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="【Spark集群部署系列二】Spark StandAlone模式介绍和搭建以及使用">
  <meta property="og:description" content="【Spark集群部署系列一】Spark local模式介绍和搭建以及使用（内含Linux安装Anaconda)http://t.csdnimg.cn/0xmky
简介 注意:请先部署好Hadoop集群 在部署spark集群前，请部署好Hadoop集群，jdk8【当然Hadoop集群需要运行在jdk上】，需要注意hadoop，spark的版本，考虑兼容问题。比如hadoop3.0以上的才兼容spark3.0以上的。
下面是Hadoop集群部署的链接，个人笔记，已经成功部署两次了，实时更新，分【一】【二】两部分，需要的自己看。不懂欢迎问，看到了解答。(链接失效的话请参考个人主页)
hadoop集群部署【一】HDFS集群http://t.csdnimg.cn/g6w1u
hadoop集群部署【二】YARN,MapReduce集群http://t.csdnimg.cn/O7sVz
搭建准备工作(每一台机器都要)： 注意：如果之前配置过Spark local模式（参考最开始链接），那么只需要在没有安装过anaconda的机器进行搭建准备工作。
安装Anaconda 需要python环境,上传Anaconda，找到放置的位置，切换到该目录下。
安装Anaconda(我的是Anaconda3-2021.05-Linux-x86_64.sh 版本python3.8）
tip:我没有试过分发，所以我不知道分发有没有问题，知道的可以说一下，但是不建议大家用，毕竟安装后要初始化anaconda
sh ./Anaconda3-2021.05-Linux-x86_64.sh 接下来步骤
然后一直空格，出现[yes|no] 就回答yes,懂?
出现这样的就填你要把anaconda安装到哪里。（路径）
完成后结果（退出终端，重新进来）
更改conda国内源 vim ~/.condarc # 如果以前没有该文件，新建的，里面没有东西很正常 channels: - defaults show_channel_urls: true default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud 终端输入 python 创建spark运行的虚拟环境 conda create -n pyspark python=3.8 切换虚拟环境 conda activate pyspark tip:输入python 版本变化很正常
Spark StandAlone部署 上传解压spark 如果之前配置过Spark local模式，那么下面的步骤看情况处理">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-17T21:51:48+08:00">
    <meta property="article:modified_time" content="2024-08-17T21:51:48+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【Spark集群部署系列二】Spark StandAlone模式介绍和搭建以及使用</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><a class="link-info has-card" href="http://t.csdnimg.cn/0xmky" rel="nofollow" title="【Spark集群部署系列一】Spark local模式介绍和搭建以及使用（内含Linux安装Anaconda)"><span class="link-card-box"><span class="link-title">【Spark集群部署系列一】Spark local模式介绍和搭建以及使用（内含Linux安装Anaconda)</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/17/4a/sWNMST4X_o.png" alt="icon-default.png?t=N7T8">http://t.csdnimg.cn/0xmky</span></span></a></p> 
<h3>简介</h3> 
<p><img alt="" height="664" src="https://images2.imgbox.com/1d/f6/lzCRBZAY_o.png" width="1200"></p> 
<p><img alt="" height="658" src="https://images2.imgbox.com/b7/c8/YZzQQS4U_o.png" width="1200"></p> 
<h3></h3> 
<h3></h3> 
<h3>注意:请先部署好Hadoop集群</h3> 
<blockquote> 
 <p><span style="color:#ff9900;"><strong>在部署spark集群前，请部署好Hadoop集群，jdk8【当然Hadoop集群需要运行在jdk上】，需要注意hadoop，spark的版本，考虑兼容问题。比如hadoop3.0以上的才兼容spark3.0以上的。</strong></span></p> 
 <p> 下面是Hadoop集群部署的链接，个人笔记，已经成功部署两次了，实时更新，分【一】【二】两部分，需要的自己看。不懂欢迎问，看到了解答。(链接失效的话请参考个人主页)</p> 
 <p> hadoop集群部署【一】HDFS集群<img alt="icon-default.png?t=N7T8" class="link-link-icon" src="https://images2.imgbox.com/20/b5/sdkShQFz_o.png"><a class="link-info" href="http://t.csdnimg.cn/g6w1u" rel="nofollow" title="http://t.csdnimg.cn/g6w1u">http://t.csdnimg.cn/g6w1u</a></p> 
 <p>hadoop集群部署【二】YARN,MapReduce集群<img alt="icon-default.png?t=N7T8" class="link-link-icon" src="https://images2.imgbox.com/4d/aa/Mn9Zpggt_o.png"><a class="link-info" href="http://t.csdnimg.cn/O7sVz" rel="nofollow" title="http://t.csdnimg.cn/O7sVz">http://t.csdnimg.cn/O7sVz</a></p> 
</blockquote> 
<p></p> 
<h3>搭建准备工作(每一台机器都要)：</h3> 
<blockquote> 
 <p>注意：如果之前配置过Spark local模式（<span style="color:#ff9900;">参考最开始链接</span>），那么只需要在没有安装过anaconda的机器进行搭建准备工作。</p> 
</blockquote> 
<h5>安装Anaconda</h5> 
<blockquote> 
 <p>需要python环境,上传Anaconda，找到放置的位置，切换到该目录下。</p> 
 <p>安装Anaconda(我的是Anaconda3-2021.05-Linux-x86_64.sh         版本python3.8）</p> 
</blockquote> 
<p><span style="color:#38d8f0;"><strong>tip:我没有试过分发，所以我不知道分发有没有问题，知道的可以说一下，但是不建议大家用，毕竟安装后要初始化anaconda</strong></span></p> 
<pre><code>sh ./Anaconda3-2021.05-Linux-x86_64.sh</code></pre> 
<p>接下来步骤</p> 
<blockquote> 
 <p>然后一直空格，出现[yes|no] 就回答yes,懂?</p> 
 <p><img alt="" height="40" src="https://images2.imgbox.com/51/39/vO1wfXMH_o.png" width="684"></p> 
 <p>出现这样的就填你要把anaconda安装到哪里。（路径）</p> 
 <p><img alt="" height="81" src="https://images2.imgbox.com/dc/b4/96L0jeEe_o.png" width="690"></p> 
 <p><img alt="" height="79" src="https://images2.imgbox.com/93/9a/5XN85gvl_o.png" width="689"></p> 
 <p>完成后结果（退出终端，重新进来）</p> 
 <p><img alt="" height="18" src="https://images2.imgbox.com/56/c7/Hbd58ZtV_o.png" width="691"></p> 
</blockquote> 
<h5>更改conda国内源</h5> 
<pre><code>vim ~/.condarc # 如果以前没有该文件，新建的，里面没有东西很正常</code></pre> 
<pre><code>channels:
  - defaults
show_channel_urls: true
default_channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
custom_channels:
  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</code></pre> 
<blockquote> 
 <p>终端输入 </p> 
 <pre><code>python</code></pre> 
 <p><img alt="" height="67" src="https://images2.imgbox.com/90/b0/d0ZzKJ3c_o.png" width="689"></p> 
</blockquote> 
<h5>创建spark运行的虚拟环境</h5> 
<pre><code>conda create -n pyspark python=3.8</code></pre> 
<blockquote> 
 <h6> 切换虚拟环境</h6> 
 <pre><code>conda activate pyspark</code></pre> 
 <p><img alt="" height="28" src="https://images2.imgbox.com/42/e4/GIoED7Ag_o.png" width="681"></p> 
</blockquote> 
<blockquote> 
 <p>tip:输入python 版本变化很正常</p> 
</blockquote> 
<h3></h3> 
<h3>Spark StandAlone部署</h3> 
<h5>上传解压spark</h5> 
<blockquote> 
 <p>如果之前配置过Spark local模式，那么下面的步骤看情况处理</p> 
</blockquote> 
<p>(我的是 spark-3.2.0-bin-hadoop3.2.tgz)</p> 
<p>通过什么工具不管，能上传就行。</p> 
<p>找到spark上传的位置,cd 进到该目录，不进去也行，自己在前面加路径哈！解压。</p> 
<pre><code>  tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server spark-3.2.0-bin-hadoop3.2/</code></pre> 
<blockquote> 
 <p>-C 参数后跟解压到哪（路径）</p> 
</blockquote> 
<pre><code>cd /export/server    #填你自己解压的路径</code></pre> 
<p> 建立软链接</p> 
<pre><code> ln -s spark-3.2.0-bin-hadoop3.2/ spark</code></pre> 
<pre><code>ll</code></pre> 
<h5></h5> 
<h5>配置环境变量(每一台都需要)</h5> 
<blockquote> 
 <p>先在第一台配置</p> 
</blockquote> 
<pre><code>vim /etc/profile</code></pre> 
<p><img alt="" height="132" src="https://images2.imgbox.com/3b/1b/IKdFAt7f_o.png" width="693"></p> 
<pre><code>export SPARK_HOME=/export/server/spark
export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python3.8
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</code></pre> 
<pre><code>:wq</code></pre> 
<pre><code>source /etc/profile</code></pre> 
<pre><code>vim /root/.bashrc</code></pre> 
<p>添加 </p> 
<pre><code>export JAVA_HOME=/export/server/jdk
export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python3.8</code></pre> 
<pre><code>:wq</code></pre> 
<blockquote> 
 <p><span style="color:#ff9900;">（可以在第一台机器配置好，然后分发到其他机器 两个文件  /etc/profile  /root/.bashrc）</span></p> 
 <pre><code>scp -r /etc/profile node2:/etc/profile</code></pre> 
 <pre><code>scp -r /root/.bashrc node2:/root/.bashrc</code></pre> 
 <p>node3同node2         请先配置好node1之后再进行scp操作，然后其他机器</p> 
 <pre><code>source /etc/profile</code></pre> 
</blockquote> 
<h5></h5> 
<h5>修改spark配置文件</h5> 
<blockquote> 
 <p><span style="color:#4da8ee;">记住权限不够就换用户，比如修改系统配置文件/etc/profile时就需要root用户，当然可以sudo，如何给用户配置sudo权限，这里不做教程</span></p> 
 <pre><code>su - 用户名</code></pre> 
</blockquote> 
<blockquote> 
 <p><span style="color:#4da8ee;">如果是学习测试，可以不使用hadoop用户为spark最高权限用户，但请注意如果Hadoop集群给的最高权限是hadoop用户，建议使用hadoop用户为spark集群最高权限用户，不然很混乱，其他用户同理。</span><span style="color:#ff9900;">本教程以hadoop用户为所有的大数据软件最高权限用户</span><span style="color:#4da8ee;">。</span></p> 
 <pre><code># 会使用hadoop用户的，一般都已经创建好了，当然这里还是提一下如何创建用户
useradd [-g -d] 用户名  


#选项：-g指定用户的组，不指定-g，会创建同名组并自动加入，指定-g需要组已经存在，如已存在同名组，必须#使用-g
#选项：-d指定用户HOME路径，不指定，HOME目录默认在：/home/用户名


 passwd hadoop  # hadoop为用户名，改代码作用是修改用户名密码，学习测试建议为12345678

</code></pre> 
 <h6>授权hadoop用户为spark最高权限用户</h6> 
 <pre><code># 切换到spark安装目录下
cd /export/server/

# 授权hadoop用户
chown -R hadoop:hadoop spark*</code></pre> 
</blockquote> 
<p> 切换hadoop用户进行操作</p> 
<pre><code>su - hadoop</code></pre> 
<h6><strong>workers</strong></h6> 
<pre><code>cd /export/server/spark/conf    #注意路径</code></pre> 
<pre><code> mv workers.template workers</code></pre> 
<pre><code>vim workers</code></pre> 
<p> 删除localhost后填入</p> 
<pre><code>node1
node2
node3
</code></pre> 
<h6><strong>spark-env.sh </strong></h6> 
<pre><code>mv spark-env.sh.template spark-env.sh
</code></pre> 
<pre><code> vim spark-env.sh
</code></pre> 
<p>填入下面内容</p> 
<pre><code>## 设置JAVA安装目录
JAVA_HOME=/export/server/jdk

## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群
HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop
YARN_CONF_DIR=/export/server/hadoop/etc/hadoop

## 指定spark老大Master的IP和提交任务的通信端口
# 告知Spark的master运行在哪个机器上
export SPARK_MASTER_HOST=node1
# 告知sparkmaster的通讯端口
export SPARK_MASTER_PORT=7077
# 告知spark master的 webui端口
SPARK_MASTER_WEBUI_PORT=8080

# worker cpu可用核数
SPARK_WORKER_CORES=1
# worker可用内存
SPARK_WORKER_MEMORY=1g
# worker的工作通讯地址
SPARK_WORKER_PORT=7078
# worker的 webui地址
SPARK_WORKER_WEBUI_PORT=8081

## 设置历史服务器
# 配置的意思是  将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中
SPARK_HISTORY_OPTS="-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true"</code></pre> 
<blockquote> 
 <h6> 启动hadoop集群（注意安装Hadoop最高权限给的谁）</h6> 
 <pre><code># 切换最高权限用户,这里以hadoop用户举例。
su - hadoop</code></pre> 
 <p><strong><span style="color:#ff9900;">（暂时只需要启动hdfs,如果不行就都启动，方正后面都要启动）</span></strong></p> 
 <p>hdfs</p> 
 <blockquote> 
  <p>启动：</p> 
  <pre><code>start-dfs.sh</code></pre> 
  <p>关闭：</p> 
  <pre><code>stop-dfs.sh</code></pre> 
 </blockquote> 
 <p> yarn:</p> 
 <blockquote> 
  <p>启动：</p> 
  <pre><code>start-yarn.sh</code></pre> 
  <p>关闭：</p> 
  <pre><code>stop-yarn.sh</code></pre> 
 </blockquote> 
 <p>HistoryServer</p> 
 <blockquote> 
  <p>启动：</p> 
  <pre><code>$HADOOP_HOME/bin/mapred --daemon start historyserver</code></pre> 
  <p>关闭的话</p> 
  <pre><code>$HADOOP_HOME/bin/mapred --daemon stop historyserver</code></pre> 
 </blockquote> 
</blockquote> 
<p>查看是否有sparklog文件夹</p> 
<pre><code> hdfs dfs -ls /
</code></pre> 
<p>没有创建</p> 
<pre><code> hadoop fs -mkdir /sparklog</code></pre> 
<p> 修改sparklog文件夹权限</p> 
<pre><code> hadoop fs -chmod 777 /sparklog
</code></pre> 
<h6>spark-defaults.conf </h6> 
<pre><code> mv spark-defaults.conf.template spark-defaults.conf
</code></pre> 
<pre><code>vim spark-defaults.conf
</code></pre> 
<pre><code>:set paste   #底线模式，开启粘贴</code></pre> 
<p>填入 </p> 
<pre><code>#park的日期记录功能
spark.eventLog.enabled  true
# 设置spark日志记录的路径
spark.eventLog.dir       hdfs://node1:8020/sparklog/
# 设置spark日志是否启动压缩
spark.eventLog.compress         true
</code></pre> 
<h6 style="background-color:transparent;">log4j.properties  [可选配置]</h6> 
<pre><code>mv log4j.properties.template log4j.properties</code></pre> 
<pre><code>vim log4j.properties</code></pre> 
<pre><code>WARN</code></pre> 
<p><img alt="" height="420" src="https://images2.imgbox.com/e4/25/eU30zpiZ_o.png" width="755"> </p> 
<blockquote> 
 <p>这个文件的修改不是必须的,  为什么修改为WARN. 因为Spark是个话痨</p> 
 <p>会疯狂输出日志, 设置级别为WARN 只输出警告和错误日志, 不要输出一堆废话,如果报错不好找错误的话可以调回INFO级别</p> 
</blockquote> 
<h5>分发spark(tab键补全，避免版本没写对)</h5> 
<pre><code>scp -r spark-3.2.0-bin-hadoop3.2/ node2:$PWD
</code></pre> 
<pre><code>scp -r spark-3.2.0-bin-hadoop3.2/ node3:`pwd`/
</code></pre> 
<h5> 在其他机器创建软链接</h5> 
<pre><code>cd /export/server/   #注意路径</code></pre> 
<pre><code> ln -s spark-3.2.0-bin-hadoop3.2/ spark
</code></pre> 
<h3>启动spark集群</h3> 
<p> 回到node1(master所在的机器)</p> 
<pre><code>cd /export/server/spark  # 注意路径</code></pre> 
<h5>启动historyServer</h5> 
<pre><code> sbin/start-history-server.sh </code></pre> 
<pre><code>jps</code></pre> 
<p>能看到java进程HistoryServer则启动成功</p> 
<blockquote> 
 <p><strong><span style="color:#fe2c24;">请找到标题【启动Hadoop集群】启动hdfs,yarn,jobhistoryserver</span></strong></p> 
</blockquote> 
<h5>启动Spark集群</h5> 
<pre><code>sbin/start-all.sh</code></pre> 
<p></p> 
<pre><code>jps</code></pre> 
<p><img alt="" height="157" src="https://images2.imgbox.com/e1/f3/YQj8qUbl_o.png" width="677"></p> 
<p> 游览器输入（windows系统上需要配置windows主机名映射，没有配置用maste所在机器的ip代替下面的node1）</p> 
<pre><code>http://node1:8080/</code></pre> 
<p><img alt="" height="684" src="https://images2.imgbox.com/ab/dc/3kHtLEc3_o.png" width="1174"></p> 
<h4>spark StandAlone模式客户端</h4> 
<p>pyspark交互式界面：（找到spark/bin目录）</p> 
<pre><code> ./pyspark --master spark://node1:7077
 
# spark://node1:7077 具体填什么查看
# 游览器输入
node1:8080</code></pre> 
<p><img alt="" height="654" src="https://images2.imgbox.com/9a/fc/LJSZNlun_o.png" width="1200"></p> 
<p> spark-submit 提交应用程序</p> 
<pre><code>./spark-submit --master spark://node1:7077 文件位置
 
 
# spark://node1:7077 具体填什么查看上图
# 游览器输入
node1:8080
 
 
#示例 自带程序求圆周率，迭代100次
./spark-submit --master spark://node1:7077 /export/server/spark/examples/src/main/python/pi.py 100</code></pre> 
<p> 查看</p> 
<p><img alt="" height="579" src="https://images2.imgbox.com/5e/6f/yBddrmEY_o.png" width="1038"></p> 
<p>OK! 完毕。不懂请留言，注意到了，乐意解答。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ec0615e9b320e997c4b33fabe18e2781/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">XSS小游戏（题目&#43;解析）DOM破坏！！！</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a8ea4a424b2e5ed28148d4437fed2482/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Redis中String数据类型常用命令</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>