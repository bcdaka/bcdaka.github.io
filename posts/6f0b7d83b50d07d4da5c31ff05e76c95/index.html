<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AI初识--LLM、ollama、llama都是些个啥？ - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/6f0b7d83b50d07d4da5c31ff05e76c95/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="AI初识--LLM、ollama、llama都是些个啥？">
  <meta property="og:description" content="LLM全称（large language model）也就是大语言模型 什么是Ollama，它与Llama是什么关系？ Ollama是一个开源的 LLM（大型语言模型）服务工具，用于简化在本地运行大语言模型，降低使用大语言模型的门槛，使得大模型的开发者、研究人员和爱好者能够在本地环境快速实验、管理和部署最新大语言模型，包括如Llama 3、Phi 3、Mistral、Gemma等开源的大型语言模型。
Ollama目前支持以下大语言模型：library
因此，Ollama与Llama的关系：Llama是大语言模型，而Ollama是大语言模型（不限于Llama模型）便捷的管理和运维工具
那么我们如何在本地部署这个大模型呢？ 首先我们需要到ollama官网下载，然后在命令窗中输入一些ollama的命令
ollama只是个工具，那怎么跑我们想要的大模型呢？ 下载模型 Ollama可以直接下载内置的几种模型，但选择有限。我们更希望从HuggingFace下载以便方便地评估各种模型，所以，这里我们并不从Ollama直接下载，而是从HuggingFace下载。
在HuggingFace搜索llama3，设置Languages为Chinese，可以看到若干基于LLaMa3的中文模型，我们选择一个GGUF格式的模型，GGUF格式是llama.cpp团队搞的一种模型存储格式，一个模型就是一个文件，方便下载与运行。
点击Files，可以看到若干GGUF文件，其中，q越大说明模型质量越高，同时文件也更大，我们选择q6，直接点击下载按钮，把这个模型文件下载到本地。
导入模型 下载到本地的模型文件不能直接导入到Ollama，需要编写一个配置文件，随便起个名字，如config.txt，配置文件内容如下：
FROM &#34;/Users/liaoxuefeng/llm/llama3-8b-cn-q6/Llama3-8B-Chinese-Chat.q6_k.GGUF&#34; TEMPLATE &#34;&#34;&#34;{{- if .System }} &lt;|im_start|&gt;system {{ .System }}&lt;|im_end|&gt; {{- end }} &lt;|im_start|&gt;user {{ .Prompt }}&lt;|im_end|&gt; &lt;|im_start|&gt;assistant &#34;&#34;&#34; SYSTEM &#34;&#34;&#34;&#34;&#34;&#34; PARAMETER stop &lt;|im_start|&gt; PARAMETER stop &lt;|im_end|&gt; 第一行FROM &#34;...&#34;指定了模型文件路径，需要修改为实际路径，后面的模板内容是网上复制的，无需改动。
然后，使用以下命令导入模型：
$ ollama create llama3-cn -f ./config.txt llama3-cn是我们给模型起的名字，成功导入后可以用list命令查看：
$ ollama list NAME ID SIZE MODIFIED llama3-cn:latest f3fa01629cab 6.6 GB 2 minutes ago 可以下载多个模型，给每个模型写一个配置文件（仅需修改路径），导入时起不同的名字，我们就可以用Ollama方便地运行各种模型。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-07T22:32:22+08:00">
    <meta property="article:modified_time" content="2024-06-07T22:32:22+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AI初识--LLM、ollama、llama都是些个啥？</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h4>LLM全称（large language model）也就是大语言模型</h4> 
<h4>什么是Ollama，它与Llama是什么关系？</h4> 
<p>Ollama是一个开源的 LLM（大型语言模型）服务工具，用于简化在本地运行大语言模型，降低使用大语言模型的门槛，使得大模型的开发者、研究人员和爱好者能够在本地环境快速实验、管理和部署最新大语言模型，包括如Llama 3、Phi 3、Mistral、Gemma等开源的大型语言模型。</p> 
<p>Ollama目前支持以下大语言模型：<a href="https://ollama.com/library" rel="nofollow" title="library">library</a></p> 
<p>因此，<code>Ollama</code>与<code>Llama</code>的关系：<code>Llama</code>是大语言模型，而<code>Ollama</code>是大语言模型（不限于<code>Llama</code>模型）便捷的管理和运维工具</p> 
<h4>那么我们如何在本地部署这个大模型呢？</h4> 
<p>首先我们需要到ollama官网下载，然后在命令窗中输入一些ollama的命令</p> 
<h4>ollama只是个工具，那怎么跑我们想要的大模型呢？</h4> 
<h4>下载模型</h4> 
<p>Ollama可以直接下载内置的几种模型，但选择有限。我们更希望从<a href="https://huggingface.co/" rel="nofollow" title="HuggingFace">HuggingFace</a>下载以便方便地评估各种模型，所以，这里我们并不从Ollama直接下载，而是从HuggingFace下载。</p> 
<p>在HuggingFace搜索<code>llama3</code>，设置<code>Languages</code>为<code>Chinese</code>，可以看到若干基于LLaMa3的中文模型，我们选择一个GGUF格式的模型，GGUF格式是llama.cpp团队搞的一种模型存储格式，一个模型就是一个文件，方便下载与运行。</p> 
<p>点击<code>Files</code>，可以看到若干GGUF文件，其中，q越大说明模型质量越高，同时文件也更大，我们选择q6，直接点击下载按钮，把这个模型文件下载到本地。</p> 
<h4>导入模型</h4> 
<p>下载到本地的模型文件不能直接导入到Ollama，需要编写一个配置文件，随便起个名字，如<code>config.txt</code>，配置文件内容如下：</p> 
<pre>FROM "/Users/liaoxuefeng/llm/llama3-8b-cn-q6/Llama3-8B-Chinese-Chat.q6_k.GGUF"

TEMPLATE """{<!-- -->{- if .System }}
&lt;|im_start|&gt;system {<!-- -->{ .System }}&lt;|im_end|&gt;
{<!-- -->{- end }}
&lt;|im_start|&gt;user
{<!-- -->{ .Prompt }}&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
"""

SYSTEM """"""

PARAMETER stop &lt;|im_start|&gt;
PARAMETER stop &lt;|im_end|&gt;
</pre> 
<p>第一行<code>FROM "..."</code>指定了模型文件路径，需要修改为实际路径，后面的模板内容是网上复制的，无需改动。</p> 
<p>然后，使用以下命令导入模型：</p> 
<pre>$ ollama create llama3-cn -f ./config.txt
</pre> 
<p><code>llama3-cn</code>是我们给模型起的名字，成功导入后可以用<code>list</code>命令查看：</p> 
<pre>$ ollama list
NAME              ID            SIZE    MODIFIED
llama3-cn:latest  f3fa01629cab  6.6 GB  2 minutes ago
</pre> 
<p>可以下载多个模型，给每个模型写一个配置文件（仅需修改路径），导入时起不同的名字，我们就可以用Ollama方便地运行各种模型。</p> 
<h4>运行模型</h4> 
<p>使用Ollama的<code>run</code>命令可以直接运行模型。我们输入命令<code>ollama run llama3-cn就可以将我们制定的模型运行起来</code></p> 
<p>出现<code>&gt;&gt;&gt;</code>提示符时就可以输入问题与模型交互。输入<code>/exit</code>退出。</p> 
<h4>搭建Web环境</h4> 
<p>使用命令行交互不是很方便，所以我们需要另一个开源的<a href="https://github.com/open-webui/open-webui" title="Open WebUI">Open WebUI</a>，搭建一个能通过浏览器访问的界面。</p> 
<p>运行Open WebUI最简单的方式是直接以Docker运行。我们安装<a href="https://www.docker.com/products/docker-desktop/" rel="nofollow" title="Docker Desktop">Docker Desktop</a>，输入以下命令启动Open WebUI：</p> 
<pre>$ docker run -p 8080:8080 -e OLLAMA_BASE_URL=http://host.docker.internal:11434 --name open-webui --restart always -v open-webui-data:/app/backend/data ghcr.io/open-webui/open-webui:main
</pre> 
<p>参数<code>-p 8080:8080</code>将Open WebUI的端口映射到本机。参数<code>-e OLLAMA_BASE_URL=http://host.docker.internal:11434</code>告诉Open WebUI通过本机的11434访问Ollama，注意地址必须写<code>host.docker.internal</code>，不能写<code>127.0.0.1</code>。</p> 
<p>打开浏览器我们就可以访问<a href="http://127.0.0.1:8080/" rel="nofollow" title="http://127.0.0.1:8080">http://127.0.0.1:8080</a>，第一次访问需要注册，注册和登录是完全基于本地环境的，登录后就可以看到类似GPT的UI。</p> 
<p>我们在聊天界面点击右上角配置按钮，点击<code>Connections</code>，点击刷新按钮，如果一切无误，会显示<code>Server connection verified</code></p> 
<p>在聊天界面的顶部选择一个模型，就可以愉快地开始和LLaMa3聊天了：</p> 
<h4>API</h4> 
<p>Open WebUI还提供了与OpenAI类似的API，使用前先点击设置 - Account，生成一个API Key，然后在命令行用curl测试：</p> 
<pre>$ curl -X POST -H "Authorization: Bearer sk-959c8b27a48145bfb83bdb396ff3eeae" -H "Content-Type: application/json" http://localhost:8080/ollama/api/generate -d '{"model":"llama3-cn:latest","stream":false,"prompt":"讲讲llama，50字"}'

{"model":"llama3-cn:latest","created_at":"2024-05-01T14:42:28.009353Z","response":"Llama是一个基于指令跟随的多模态大型语言模型，由Meta AI发布。它能够处理文本、图像等多种输入，并生成相应的输出。
</pre> 
<p>由于模型是运行在本地，Open WebUI也将数据存储在本地，所以隐私性可谓拉满。如果对一个模型不满意，还可以从HuggingFace下载更多的模型来评估，非常方便。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e2cde7415f262279951519f3fc48c4df/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">G5 - Pix2Pix理论与实战</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/da8cd607e7d943530240d49d9f9366f0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Android Webview 详解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>