<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>bug清单问题 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/113ffb34ca0f60f4901db8b21da4b3c8/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="bug清单问题">
  <meta property="og:description" content="1. embedding 层 index out of range in self 原因： 一般是因为模型的vocab_size与提供的vocab.txt文件的词大小不一致。 检查方法： 可通过以下方法，查看tensor的最大最小值 # print(&#39;token_ids&#39;, token_ids.max(), token_ids.min()) # （已转变为张量后） # print(&#39;attention_masks&#39;, attention_masks.max(), attention_masks.min()) # （已转变为张量后） # print(&#39;token_type_ids&#39;, token_type_ids.max(), token_type_ids.min()) # （已转变为张量后） # print(&#39;labels&#39;, labels.max(), labels.min()) # （已转变为张量后） 2 Dataloader File &#34;D:\python\lib\site-packages\torch\utils\data\_utils\collate.py&#34;, line 55, in default_collate return torch.stack(batch, 0, out=out) RuntimeError: stack expects each tensor to be equal size, but got [2] at entry 0 and [1] at entry 1 原因是text 在tokenizer.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-31T12:25:21+08:00">
    <meta property="article:modified_time" content="2024-05-31T12:25:21+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">bug清单问题</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h6><a id="1_embedding__index_out_of_range_in_self_2"></a>1. embedding 层 index out of range in self</h6> 
<pre><code class="prism language-css">原因： 一般是因为模型的vocab_size与提供的vocab.txt文件的词大小不一致。
检查方法： 
可通过以下方法，查看tensor的最大最小值
        # <span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'token_ids'</span><span class="token punctuation">,</span> token_ids.<span class="token function">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> token_ids.<span class="token function">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  # （已转变为张量后）
        # <span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'attention_masks'</span><span class="token punctuation">,</span> attention_masks.<span class="token function">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> attention_masks.<span class="token function">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  # （已转变为张量后）
        # <span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'token_type_ids'</span><span class="token punctuation">,</span> token_type_ids.<span class="token function">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> token_type_ids.<span class="token function">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  # （已转变为张量后）
        # <span class="token function">print</span><span class="token punctuation">(</span><span class="token string">'labels'</span><span class="token punctuation">,</span> labels.<span class="token function">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> labels.<span class="token function">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  # （已转变为张量后）
</code></pre> 
<h6><a id="2_Dataloader_13"></a>2 Dataloader</h6> 
<pre><code class="prism language-python">File <span class="token string">"D:\python\lib\site-packages\torch\utils\data\_utils\collate.py"</span><span class="token punctuation">,</span> line <span class="token number">55</span><span class="token punctuation">,</span> <span class="token keyword">in</span> default_collate
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> out<span class="token operator">=</span>out<span class="token punctuation">)</span>
RuntimeError<span class="token punctuation">:</span> stack expects each tensor to be equal size<span class="token punctuation">,</span> but got <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> at entry <span class="token number">0</span> <span class="token keyword">and</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> at entry <span class="token number">1</span>
</code></pre> 
<p>原因是text 在tokenizer.encod()之后没有用max_length限制长度。但是input_ids的长短不一样。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">MyDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> texts<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> max_length<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>texts <span class="token operator">=</span> texts
        self<span class="token punctuation">.</span>tokenizer <span class="token operator">=</span> tokenizer
        self<span class="token punctuation">.</span>max_length <span class="token operator">=</span> max_length

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>texts<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 这里没有max_length限制</span>
        text <span class="token operator">=</span> self<span class="token punctuation">.</span>texts<span class="token punctuation">[</span>index<span class="token punctuation">]</span>
        input_ids <span class="token operator">=</span> self<span class="token punctuation">.</span>tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
        input_ids <span class="token operator">=</span> input_ids<span class="token punctuation">[</span><span class="token punctuation">:</span>self<span class="token punctuation">.</span>max_length<span class="token punctuation">]</span>。<span class="token comment"># 此处需要基于max_length进行强制截取</span>
</code></pre> 
<h6><a id="3_ImportError_usrlibaarch64linuxgnulibgompso1_cannot_allocate_memory_in_static_TLS_block_37"></a>3. ImportError: /usr/lib/aarch64-linux-gnu/libgomp.so.1: cannot allocate memory in static TLS block</h6> 
<pre><code class="prism language-python">export LD_PRELOAD<span class="token operator">=</span><span class="token operator">/</span>usr<span class="token operator">/</span>lib<span class="token operator">/</span>aarch64<span class="token operator">-</span>linux<span class="token operator">-</span>gnu<span class="token operator">/</span>libgomp<span class="token punctuation">.</span>so<span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">:</span>$LD_PRELOAD
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b11c18f90ceded8fc65408459df28e14/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【数据结构】二叉树-堆（下）-链式二叉树</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4802bf844f76ae21faf38d22d2701204/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">安装MySQL报错 suffix ‘.‘ used for variable ‘mysqlx-port‘ (value ‘0.0‘)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>