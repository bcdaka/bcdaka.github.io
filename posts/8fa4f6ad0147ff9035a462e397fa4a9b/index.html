<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>如何在本地搭建集成大语言模型Llama 2的聊天机器人并实现无公网IP远程访问 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/8fa4f6ad0147ff9035a462e397fa4a9b/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="如何在本地搭建集成大语言模型Llama 2的聊天机器人并实现无公网IP远程访问">
  <meta property="og:description" content="文章目录 1. 拉取相关的Docker镜像2. 运行Ollama 镜像3. 运行Chatbot Ollama镜像4. 本地访问5. 群晖安装Cpolar6. 配置公网地址7. 公网访问8. 固定公网地址 随着ChatGPT 和open Sora 的热度剧增,大语言模型时代,开启了AI新篇章,大语言模型的应用非常广泛，包括聊天机器人、智能客服、自动翻译、写作助手等。它们可以根据用户输入的文本生成相应的响应，提供个性化的建议和服务,目前大部分大语言模型的产品都是基于网络线上的,如果我们本地想自己部署一个自己的大语言模型,该如何操作呢,下面介绍一款可以在自己本地部署运行的大语言模型Llama 2 Llama 2是一款开源的大语言模型,其中训练的数据目前已经达到7B级别,在上下文长度为4K下的单轮与多轮对话中表现出色,部署运行Llama 2同时需要借助一个框架Ollama.
Ollama是一个强大的框架，设计用于在Docker容器中部署大型语言模型（LLM）。它的主要功能是简化在Docker容器内部署和管理LLM的过程。Ollama通过提供简单的安装指令，使用户能够轻松地在本地运行大型开源语言模型.
借助Ollama 框架可以很方便运行Llama2大语言模型,同时,为了方便与模型进行交互,还需要部署一个web交互 界面Chatbot-Ollama.
Chatbot-Ollama是一个基于Ollama框架的聊天机器人前端应用。它利用Ollama框架提供的接口和功能，将大型语言模型（LLM）集成到聊天机器人中，使其能够与用户进行交互，并提供各种聊天机器人服务。
Chatbot-Ollama 接入本地Ollama框架运行的Llama2大语言模型,使我们可以很轻松简便在本地创建一个聊天机器人.Chatbot-Ollama 同时也是基于docker本地部署的,本地部署,只能局限于本地访问,无法做到提供远程给其他人访问,下面我们还需要安装一个内网穿透工具cpolar,使得本地聊天机器人可以被远程访问.
Cpolar是一款强大的内网穿透软件，它能够在多个操作系统上无缝运行，包括Windows、MacOS和Linux，因此无论您使用哪种操作系统，都可以轻松将本地内网服务器的HTTP、HTTPS、TCP协议端口映射为公网地址端口，使得公网用户可以轻松访问您的内网服务，无需部署至公网服务器.
下面我们通过群晖Docker来演示如何结合上面介绍的技术来运行一个自己的本地聊天机器人并且发布到公网访问.本地部署,对设备配置要求高一些,如果想要拥有比较好的体验,可以使用高配置的服务器设备.
1. 拉取相关的Docker镜像 运行Llama 2需要借助Ollama镜像,对语言模型进行交互需要用到Chatbot-Ollama前端界面,所以我们需要拉取这两个docker镜像,本例群晖版本由于无法直接在群晖控制面板docker界面搜索下载镜像,所以采用命令行方式进行镜像下载,首先开启群晖ssh连接,然后使用工具通过ssh连接上群晖,分别执行下面docker命令 拉取
*拉取Ollama镜像命令
sudo docker pull ollama/ollama:latest *拉取Chatbot-Ollama镜像命令
sudo docker pull ghcr.io/ivanfioravanti/chatbot-ollama:main 拉取成功后,我们可以在Docker界面看到拉取的两个镜像,下面开始运行两个镜像,首先运行ollama
2. 运行Ollama 镜像 选中镜像,点击运行进入配置界面,名称无需设置,默认即可,然后点击下一步
输入外部访问的端口,和容器内部端口一致,填写11434即可,然后点击下一步
然后点击完成即可运行ollama
运行后,打开容器界面,可以看到运行的服务,下面开始下载运行Llama 2模型,点击选中ollama容器,点击操作
然后打开终端机,进入终端命令界面
然后选择左边新增一个bash命令界面
然后在bash命令界面,执行ollama run llama2命令,接着等待下载即可,最后出现success,表示下载运行Llama 2模型成功,下载完成后可以关闭掉窗口.这样ollama 容器启动,并运行Llama 2模型就成功了,下面运行chatbot-ollama镜像,配置前端交互界面
3. 运行Chatbot Ollama镜像 选中我们下载的镜像,点击运行,开始进行设置
名称可以默认,直接点击下一步
设置对外端口,本例设置3001,具体可以自己自定义,这个端口也是我们浏览器上web访问的端口
然后设置一个环境变量,该变量就是连接我们上面运行Ollama框架服务的地址,我们设置本地地址:http://群晖局域网IP:11434即可,设置完成点击下一步,然后直接点击完成即可,chatbot Ollama镜像就运行成功了,接下来我们进行本地访问.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-03T22:02:28+08:00">
    <meta property="article:modified_time" content="2024-04-03T22:02:28+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">如何在本地搭建集成大语言模型Llama 2的聊天机器人并实现无公网IP远程访问</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><a href="#1_Docker_17" rel="nofollow">1. 拉取相关的Docker镜像</a></li><li><a href="#2_Ollama__41" rel="nofollow">2. 运行Ollama 镜像</a></li><li><a href="#3_Chatbot_Ollama_75" rel="nofollow">3. 运行Chatbot Ollama镜像</a></li><li><a href="#4__95" rel="nofollow">4. 本地访问</a></li><li><a href="#5_Cpolar_103" rel="nofollow">5. 群晖安装Cpolar</a></li><li><a href="#6__135" rel="nofollow">6. 配置公网地址</a></li><li><a href="#7__153" rel="nofollow">7. 公网访问</a></li><li><a href="#8__167" rel="nofollow">8. 固定公网地址</a></li></ul> 
 </li></ul> 
</div> 
<br> 随着ChatGPT 和open Sora 的热度剧增,大语言模型时代,开启了AI新篇章,大语言模型的应用非常广泛，包括聊天机器人、智能客服、自动翻译、写作助手等。它们可以根据用户输入的文本生成相应的响应，提供个性化的建议和服务,目前大部分大语言模型的产品都是基于网络线上的,如果我们本地想自己部署一个自己的大语言模型,该如何操作呢,下面介绍一款可以在自己本地部署运行的大语言模型Llama 2 
<p></p> 
<p>Llama 2是一款开源的大语言模型,其中训练的数据目前已经达到7B级别,在上下文长度为4K下的单轮与多轮对话中表现出色,部署运行Llama 2同时需要借助一个框架Ollama.</p> 
<p>Ollama是一个强大的框架，设计用于在Docker容器中部署大型语言模型（LLM）。它的主要功能是简化在Docker容器内部署和管理LLM的过程。Ollama通过提供简单的安装指令，使用户能够轻松地在本地运行大型开源语言模型.</p> 
<p>借助Ollama 框架可以很方便运行Llama2大语言模型,同时,为了方便与模型进行交互,还需要部署一个web交互 界面Chatbot-Ollama.</p> 
<p>Chatbot-Ollama是一个基于Ollama框架的聊天机器人前端应用。它利用Ollama框架提供的接口和功能，将大型语言模型（LLM）集成到聊天机器人中，使其能够与用户进行交互，并提供各种聊天机器人服务。</p> 
<p>Chatbot-Ollama 接入本地Ollama框架运行的Llama2大语言模型,使我们可以很轻松简便在本地创建一个聊天机器人.Chatbot-Ollama 同时也是基于docker本地部署的,本地部署,只能局限于本地访问,无法做到提供远程给其他人访问,下面我们还需要安装一个内网穿透工具cpolar,使得本地聊天机器人可以被远程访问.</p> 
<p>Cpolar是一款强大的内网穿透软件，它能够在多个操作系统上无缝运行，包括Windows、MacOS和Linux，因此无论您使用哪种操作系统，都可以轻松将本地内网服务器的HTTP、HTTPS、TCP协议端口映射为公网地址端口，使得公网用户可以轻松访问您的内网服务，无需部署至公网服务器.</p> 
<p>下面我们通过群晖Docker来演示如何结合上面介绍的技术来运行一个自己的本地聊天机器人并且发布到公网访问.本地部署,对设备配置要求高一些,如果想要拥有比较好的体验,可以使用高配置的服务器设备.</p> 
<h3><a id="1_Docker_17"></a>1. 拉取相关的Docker镜像</h3> 
<p>运行Llama 2需要借助Ollama镜像,对语言模型进行交互需要用到Chatbot-Ollama前端界面,所以我们需要拉取这两个docker镜像,本例群晖版本由于无法直接在群晖控制面板docker界面搜索下载镜像,所以采用命令行方式进行镜像下载,首先开启群晖ssh连接,然后使用工具通过ssh连接上群晖,分别执行下面docker命令 拉取</p> 
<p>*拉取Ollama镜像命令</p> 
<pre><code class="prism language-shell"><span class="token function">sudo</span> <span class="token function">docker</span>  pull ollama/ollama:latest
</code></pre> 
<p>*拉取Chatbot-Ollama镜像命令</p> 
<pre><code class="prism language-shell"><span class="token function">sudo</span> <span class="token function">docker</span>  pull ghcr.io/ivanfioravanti/chatbot-ollama:main
</code></pre> 
<p>拉取成功后,我们可以在Docker界面看到拉取的两个镜像,下面开始运行两个镜像,首先运行ollama</p> 
<p><img src="https://images2.imgbox.com/a8/05/MIFmhVTU_o.png" alt="image-20240228134827663"></p> 
<h3><a id="2_Ollama__41"></a>2. 运行Ollama 镜像</h3> 
<p>选中镜像,点击运行进入配置界面,名称无需设置,默认即可,然后点击下一步</p> 
<p><img src="https://images2.imgbox.com/d6/c1/HlYFc1HG_o.png" alt="image-20240228140210393"></p> 
<p>输入外部访问的端口,和容器内部端口一致,填写11434即可,然后点击下一步</p> 
<p><img src="https://images2.imgbox.com/15/1d/kabHhP77_o.png" alt="image-20240228140324795"></p> 
<p>然后点击完成即可运行ollama</p> 
<p><img src="https://images2.imgbox.com/15/68/Mlti2iLa_o.png" alt="image-20240228140944482"></p> 
<p>运行后,打开容器界面,可以看到运行的服务,下面开始下载运行Llama 2模型,点击选中ollama容器,点击操作</p> 
<p><img src="https://images2.imgbox.com/49/1d/y7oLhv6p_o.png" alt="image-20240228141509408"></p> 
<p>然后打开终端机,进入终端命令界面</p> 
<p><img src="https://images2.imgbox.com/b2/a5/DzAVZDjj_o.png" alt="image-20240228141933061"></p> 
<p>然后选择左边新增一个bash命令界面</p> 
<p><img src="https://images2.imgbox.com/b2/41/1fuCpFmU_o.png" alt="image-20240228142029589"></p> 
<p>然后在bash命令界面,执行<code>ollama run llama2</code>命令,接着等待下载即可,最后出现success,表示下载运行Llama 2模型成功,下载完成后可以关闭掉窗口.这样ollama 容器启动,并运行Llama 2模型就成功了,下面运行chatbot-ollama镜像,配置前端交互界面</p> 
<p><img src="https://images2.imgbox.com/3f/b4/916rSsop_o.png" alt="image-20240228142952591"></p> 
<h3><a id="3_Chatbot_Ollama_75"></a>3. 运行Chatbot Ollama镜像</h3> 
<p>选中我们下载的镜像,点击运行,开始进行设置</p> 
<p><img src="https://images2.imgbox.com/f8/a1/CHMgO5dE_o.png" alt="image-20240228143332721"></p> 
<p>名称可以默认,直接点击下一步</p> 
<p><img src="https://images2.imgbox.com/6c/22/4IVTxqGS_o.png" alt="image-20240228143615754"></p> 
<p>设置对外端口,本例设置3001,具体可以自己自定义,这个端口也是我们浏览器上web访问的端口</p> 
<p><img src="https://images2.imgbox.com/df/05/PhgiQzeo_o.png" alt="image-20240228143700098"></p> 
<p>然后设置一个环境变量,该变量就是连接我们上面运行Ollama框架服务的地址,我们设置本地地址:<code>http://群晖局域网IP:11434</code>即可,设置完成点击下一步,然后直接点击完成即可,chatbot Ollama镜像就运行成功了,接下来我们进行本地访问.</p> 
<p><img src="https://images2.imgbox.com/6d/72/tsCFXA3H_o.png" alt="image-20240228151028916"></p> 
<h3><a id="4__95"></a>4. 本地访问</h3> 
<p>上面我们运行设置chatbot ollama 的对外端口是3001(具体以自己设置的为准),下面我们在浏览器访问群晖3001端口,既可看到我们的web交互 界面,同时,上面显示了使用的llama2模型,下面输入文字即可对话,这样一个本地部署的机器人就完成了,对话的响应速度取决于设备的配置,尽量使用高配置的服务器运行部署哦,本地完成后,我们接下来设置远程也可以访问,下面安装cpolar工具,实现无公网IP也可以远程访问我们的聊天机器人界面!</p> 
<p><img src="https://images2.imgbox.com/cf/f2/6WBBfIsr_o.png" alt="image-20240228144800166"></p> 
<h3><a id="5_Cpolar_103"></a>5. 群晖安装Cpolar</h3> 
<p>点击下面Cpolar群晖套件下载地址,下载相应版本的群晖Cpolar套件</p> 
<blockquote> 
 <p>https://www.cpolar.com/synology-cpolar-suite,</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/6b/e4/g5AQ8ulc_o.png" alt="20221222170135"></p> 
<p>打开群晖<code>套件中心</code>，点击右上角的<code>手动安装</code>按钮。</p> 
<p><img src="https://images2.imgbox.com/b4/4e/5KFhlH8D_o.png" alt="image-20240111165335915"></p> 
<p>选择我们本地下载好的cpolar套件安装包,然后点击下一步</p> 
<p><img src="https://images2.imgbox.com/3f/38/asOpjjg4_o.png" alt="image-20240111165603922"></p> 
<p>点击<code>同意</code>按钮,然后点击下一步</p> 
<p><img src="https://images2.imgbox.com/0b/76/WQeq5Kfh_o.png" alt="image-20240111165702028"></p> 
<p>最后点击完成即可。</p> 
<p><img src="https://images2.imgbox.com/fa/51/5ZHJ2ksh_o.png" alt="image-20240111165721365"></p> 
<p>安装完成后,在外部浏览器,我们通过<code>群晖的局域网ip地址</code>加<code>9200</code>端口访问Cpolar的Web管理界面,然后输入Cpolar邮箱账号与密码进行登录</p> 
<p><img src="https://images2.imgbox.com/00/4b/xVCeeWpR_o.png" alt="image-20230612165349594"></p> 
<h3><a id="6__135"></a>6. 配置公网地址</h3> 
<p>点击左侧仪表盘的隧道管理——创建隧道，创建一个chatbot的公网地址隧道!</p> 
<ul><li>隧道名称：可自定义命名，注意不要与已有的隧道名称重复</li><li>协议：选择http</li><li>本地地址：3001 (本地访问的端口)</li><li>域名类型：免费选择随机域名</li><li>地区：选择China</li></ul> 
<p>点击<code>创建</code></p> 
<p><img src="https://images2.imgbox.com/dd/20/AZFpPsgL_o.png" alt="image-20240228151510280"></p> 
<p>隧道创建成功后，点击左侧的状态——在线隧道列表,查看所生成的公网访问地址，有两种访问方式,一种是http 和https,下面我们使用生成的http地址访问</p> 
<p><img src="https://images2.imgbox.com/6e/28/MjGWrYQz_o.png" alt="image-20240228151723949"></p> 
<h3><a id="7__153"></a>7. 公网访问</h3> 
<p>使用上面cpolar生成的http地址,在浏览器访问,同样可以看到聊天机器人主界面,公网地址访问成功,无需公网IP,无需云服务器,即可把我们本地聊天机器人发布到公网进行访问!</p> 
<p><img src="https://images2.imgbox.com/b4/4c/7iILyh8B_o.png" alt="image-20240228152929542"></p> 
<p><strong>小结</strong></p> 
<p>为了更好地演示，我们在前述过程中使用了cpolar生成的隧道，其公网地址是随机生成的。</p> 
<p>这种随机地址的优势在于建立速度快，可以立即使用。然而，它的缺点是网址由随机字符生成，不太容易记忆（例如：3ad5da5.r10.cpolar.top）。另外，这个地址在24小时内会发生随机变化，更适合于临时使用。</p> 
<p>我一般会使用固定二级子域名，原因是我希望将网址发送给同事或客户时，它是一个固定、易记的公网地址（例如：chatbot.cpolar.cn），这样更显正式，便于流交协作。</p> 
<h3><a id="8__167"></a>8. 固定公网地址</h3> 
<p>由于以上使用Cpolar所创建的隧道使用的是随机公网地址，24小时内会随机变化，不利于长期远程访问。因此我们可以为其配置二级子域名，该地址为固定地址，不会随机变化【ps：cpolar.cn已备案】</p> 
<blockquote> 
 <p>注意需要将cpolar套餐升级至基础套餐或以上，且每个套餐对应的带宽不一样。【cpolar.cn已备案】</p> 
</blockquote> 
<p><a href="https://dashboard.cpolar.com/" rel="nofollow">登录cpolar官网</a>，点击左侧的预留，选择保留二级子域名，设置一个二级子域名名称，点击保留,保留成功后复制保留的二级子域名名称</p> 
<p><img src="https://images2.imgbox.com/f7/ab/nBJKxLWu_o.png" alt="image-20240228152456948"></p> 
<p>保留成功后复制保留成功的二级子域名的名称</p> 
<p><img src="https://images2.imgbox.com/50/77/SOqdERpX_o.png" alt="image-20240228152519922"></p> 
<p>返回登录Cpolar web UI管理界面，点击左侧仪表盘的隧道管理——隧道列表，找到所要配置的隧道，点击右侧的编辑</p> 
<p><img src="https://images2.imgbox.com/92/8d/oGfBGGFg_o.png" alt="image-20240228152612455"></p> 
<p>修改隧道信息，将保留成功的二级子域名配置到隧道中</p> 
<ul><li>域名类型：选择二级子域名</li><li>Sub Domain：填写保留成功的二级子域名</li></ul> 
<p>点击<code>更新</code>(注意,点击一次更新即可,不需要重复提交)</p> 
<p><img src="https://images2.imgbox.com/e0/84/s1W0VBcg_o.png" alt="image-20240228152638853"></p> 
<p>更新完成后,打开在线隧道列表,此时可以看到公网地址已经发生变化,地址名称也变成了固定的二级子域名的名称域名</p> 
<p><img src="https://images2.imgbox.com/24/3e/1I39jAow_o.png" alt="image-20240228152714604"></p> 
<p>最后,我们使用固定的公网http地址访问,可以看到同样访问成功,这样一个固定且永久不变的公网地址就设置好了!</p> 
<p><img src="https://images2.imgbox.com/15/56/Ehq1jzxL_o.png" alt="image-20240228152806444"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a53cc90cd13c7e1482c4ebfbf7eb6eb1/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">网上商城购物系统设计与实现（Java&#43;Web&#43;SSM&#43;MySQL）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c6d9c9e3590024b7faae6d050ceb5056/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">波士顿房价预测案例（python scikit-learn）---多元线性回归(多角度实验分析)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>