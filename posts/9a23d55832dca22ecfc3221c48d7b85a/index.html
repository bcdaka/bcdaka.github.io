<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Hive概述与基本操作 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/9a23d55832dca22ecfc3221c48d7b85a/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Hive概述与基本操作">
  <meta property="og:description" content="一、Hive基本概念 1.什么是hive? （1）hive是数据仓库建模的工具之一
（2）可以向hive传入一条交互式的sql,在海量数据中查询分析得到结果的平台
2.Hive简介 Hive本质是将SQL转换为MapReduce的任务进行运算，底层由HDFS来提供数据存储，说白了hive可以理解为一个将SQL转换为MapReduce的任务的工具，甚至更近一步说hive就是一个MapReduce客户端
3.Hive的优缺点： 优点：
1、操作接口采用类sql语法，提供快速开发的能力（简单、容易上手）
2、避免了去写MapReduce,减少开发人员的学习成本
3、Hive的延迟性比较高，因此Hive常用于数据分析，适用于对实时性要求不高的场合
4、Hive 优势在于处理大数据，对于处理小数据没有优势，因为 Hive 的执行延迟比较高。（不断地开关JVM虚拟机）
5、Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。
6、集群可自由扩展并且具有良好的容错性，节点出现问题SQL仍可以完成执行
缺点：
1、Hive的HQL表达能力有限
（1）迭代式算法无法表达 （反复调用，mr之间独立，只有一个map一个reduce，反复开关）
（2）数据挖掘方面不擅长
2、Hive 的效率比较低
（1）Hive 自动生成的 MapReduce 作业，通常情况下不够智能化
（2）Hive 调优比较困难，粒度较粗 （hql根据模板转成mapreduce，不能像自己编写mapreduce一样精细，无法控制在map处理数据还是在reduce处理数据）
4.Hive和传统数据库对比 hive和mysql什么区别？ 首先，hive不是数据库，它只是一个数据仓库建模的工具，是可以在海量数据中查询分析得到结果的平台，数据存储位置在HDFS上。
mysql是数据库，数据存储位置在本地磁盘上
5.Hive应用场景 （1）日志分析：大部分互联网公司使用hive进行日志分析，如百度、淘宝等。
（2）统计一个网站一个时间段内的pv,uv，SKU,SPU,SKC
（3）多维度数据分析（数据仓库）
（4）海量结构化数据离线分析
（5）构建数据仓库
二、Hive架构 1.图解： 元数据Metastore 元数据包括表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是 外部表）、表的数据所在目录等。
一般需要借助于其他的数据载体（数据库）
主要用于存放数据库的建表语句等信息
推荐使用Mysql数据库存放数据
Driver（sql语句是如何转化成MR任务的？） 元数据存储在数据库中，默认存在自带的derby数据库（单用户局限性）中，推荐使用Mysql进行存储。
1） 解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST（从3.x版本之后，转换成一些的stage），这一步一般都用第三方工具库完 成，比如ANTLR；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。
2） 编译器（Physical Plan）：将AST编译（从3.x版本之后，转换成一些的stage）生成逻辑执行计划。
3） 优化器（Query Optimizer）：对逻辑执行计划进行优化。
4） 执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是 MR/Spark/flink。
数据处理 Hive的数据存储在HDFS中，计算由MapReduce完成。HDFS和MapReduce是源码级别上的整合，两者结合最佳。解释器、编译器、优化器完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。
hive cli和beeline cli的区别">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-15T11:29:57+08:00">
    <meta property="article:modified_time" content="2024-04-15T11:29:57+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Hive概述与基本操作</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>一、Hive基本概念</h2> 
<h3>1.什么是hive?</h3> 
<p>（1）hive是数据仓库建模的工具之一<br> （2）可以向hive传入一条交互式的sql,在海量数据中查询分析得到结果的平台</p> 
<p></p> 
<h3>2.Hive简介</h3> 
<p>Hive本质是将SQL转换为MapReduce的任务进行运算，底层由HDFS来提供数据存储，说白了hive可以理解为一个将SQL转换为MapReduce的任务的工具，甚至更近一步说hive就是一个MapReduce客户端</p> 
<p></p> 
<h3><strong>3.Hive的优缺点：</strong></h3> 
<blockquote> 
 <p><strong>优点：</strong></p> 
 <p>1、操作接口采用类sql语法，提供快速开发的能力（简单、容易上手）</p> 
 <p>2、避免了去写MapReduce,减少开发人员的学习成本</p> 
 <p>3、Hive的延迟性比较高，因此Hive常用于数据分析，适用于对实时性要求不高的场合</p> 
 <p>4、Hive 优势在于处理大数据，对于处理小数据没有优势，因为 Hive 的执行延迟比较高。（不断地开关JVM虚拟机）</p> 
 <p>5、Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</p> 
 <p>6、集群可自由扩展并且具有良好的容错性，节点出现问题SQL仍可以完成执行</p> 
 <p><strong>缺点：</strong></p> 
 <p>1、Hive的HQL表达能力有限</p> 
 <p>（1）迭代式算法无法表达 （反复调用，mr之间独立，只有一个map一个reduce，反复开关）</p> 
 <p>（2）数据挖掘方面不擅长</p> 
 <p>2、Hive 的效率比较低</p> 
 <p>（1）Hive 自动生成的 MapReduce 作业，通常情况下不够智能化</p> 
 <p>（2）Hive 调优比较困难，粒度较粗 （hql根据模板转成mapreduce，不能像自己编写mapreduce一样精细，无法控制在map处理数据还是在reduce处理数据）</p> 
</blockquote> 
<p></p> 
<h3><strong>4.Hive和传统数据库对比</strong></h3> 
<p><img alt="" height="714" src="https://images2.imgbox.com/e3/92/DGBGoQ7h_o.png" width="1167"></p> 
<p></p> 
<p><strong>hive和mysql什么区别？ </strong></p> 
<p><strong>首先，hive不是数据库，它只是一个数据仓库建模的工具，是可以在海量数据中查询分析得到结果的平台，数据存储位置在HDFS上。</strong></p> 
<p><strong>mysql是数据库，数据存储位置在本地磁盘上</strong></p> 
<p></p> 
<h3><strong>5.Hive应用场景</strong></h3> 
<blockquote> 
 <p>（1）日志分析：大部分互联网公司使用hive进行日志分析，如百度、淘宝等。</p> 
 <p>（2）统计一个网站一个时间段内的<strong>pv,uv，SKU,SPU,SKC</strong></p> 
 <p>（3）多维度数据分析（<strong>数据仓库</strong>）</p> 
 <p>（4）海量结构化数据离线分析</p> 
 <p><strong>（5）构建数据仓库</strong></p> 
</blockquote> 
<p></p> 
<h2>二、Hive架构</h2> 
<h4>1.图解：</h4> 
<p><img alt="" height="904" src="https://images2.imgbox.com/f7/f6/yr49norf_o.png" width="1200"></p> 
<p></p> 
<h5><strong>元数据</strong>Metastore</h5> 
<blockquote> 
 <p><strong>元数据</strong>包括表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是 外部表）、表的数据所在目录等。</p> 
 <p>一般需要借助于其他的数据载体（数据库）</p> 
 <p>主要用于存放数据库的建表语句等信息</p> 
 <p>推荐使用Mysql数据库存放数据</p> 
</blockquote> 
<h5>Driver（sql语句是如何转化成MR任务的？）</h5> 
<blockquote> 
 <p>元数据存储在数据库中，默认存在自带的derby数据库（单用户局限性）中，推荐使用Mysql进行存储。</p> 
 <p>1） 解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST（从3.x版本之后，转换成一些的stage），这一步一般都用第三方工具库完 成，比如ANTLR；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</p> 
 <p>2） 编译器（Physical Plan）：将AST编译（从3.x版本之后，转换成一些的stage）生成逻辑执行计划。</p> 
 <p>3） 优化器（Query Optimizer）：对逻辑执行计划进行优化。</p> 
 <p>4） 执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是 MR/Spark/flink。</p> 
</blockquote> 
<p></p> 
<h5>数据处理</h5> 
<blockquote> 
 <p>Hive的数据存储在HDFS中，计算由MapReduce完成。HDFS和MapReduce是源码级别上的整合，两者结合最佳。解释器、编译器、优化器完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。</p> 
</blockquote> 
<p></p> 
<p><strong>hive cli和beeline cli的区别</strong></p> 
<p><img alt="" height="583" src="https://images2.imgbox.com/42/84/PGXXeDEw_o.png" width="1200"></p> 
<p><span style="color:#4d4d4d;"><strong>在客户端启动（</strong></span><strong>beeline cli</strong><strong>）的hiveserver2服务会将任务传给服务端，服务端通过元数据映射HDFS中的数据，进行处理</strong></p> 
<p></p> 
<h5>数据库中Hive元数据表</h5> 
<blockquote> 
 <p>1、存储Hive版本的<strong>元数据表(VERSION)</strong>，该表比较简单，但很重要,如果这个表出现问题，根本进不来Hive-Cli。比如该表不存在，当启动Hive-Cli的时候，就会报错“Table 'hive.version' doesn't exist”</p> 
 <p></p> 
 <p>2、Hive数据库相关的元数据表(DBS、DATABASE_PARAMS)</p> 
 <p><strong>DBS：该表存储Hive中所有数据库的基本信息。</strong></p> 
 <p>DATABASE_PARAMS：该表存储数据库的相关参数。</p> 
 <p></p> 
 <p>3、Hive表和视图相关的元数据表</p> 
 <p>主要有TBLS、TABLE_PARAMS、TBL_PRIVS，这三张表通过TBL_ID关联。</p> 
 <p><strong>​ TBLS:该表中存储Hive表，视图，索引表的基本信息。</strong></p> 
 <p>​ TABLE_PARAMS:该表存储表/视图的属性信息。</p> 
 <p>​ TBL_PRIVS：该表存储表/视图的授权信息。</p> 
 <p></p> 
 <p>4、Hive文件存储信息相关的元数据表</p> 
 <p>主要涉及SDS、SD_PARAMS、SERDES、SERDE_PARAMS，由于HDFS支持的文件格式很多，而建Hive表时候也可以指定各种文件格式，Hive在将HQL解析成MapReduce时候，需要知道去哪里，使用哪种格式去读写HDFS文件，而这些信息就保存在这几张表中。</p> 
 <p><strong>​ SDS：该表保存文件存储的基本信息，如INPUT_FORMAT、OUTPUT_FORMAT、是否压缩等。</strong></p> 
 <p>TBLS表中的SD_ID与该表关联，可以获取Hive表的存储信息。</p> 
 <p>​ SD_PARAMS: 该表存储Hive存储的属性信息。 ​ SERDES:该表存储序列化使用的类信息。</p> 
 <p><strong>​ SERDE_PARAMS:该表存储序列化的一些属性、格式信息，比如:行、列分隔符。</strong></p> 
 <p></p> 
 <p>5、Hive表字段相关的元数据表</p> 
 <p>主要涉及COLUMNS_V2：该表存储表对应的字段信息。</p> 
</blockquote> 
<p>（加粗的部分的表比较重要）</p> 
<p></p> 
<h2><strong>三、</strong>Hive的基本操作</h2> 
<p><span style="color:#fe2c24;">hive中的数据来源是HDFS,hive中的数据库，数据表对应HDFS上的文件夹，数据表中的数据对应HDFS上的文件，通常数据库会默认创建在HDFS中的/user/hive/warehouse目录下</span></p> 
<p><img alt="" height="533" src="https://images2.imgbox.com/59/93/YLMchxXI_o.png" width="1200"></p> 
<p></p> 
<p><img alt="" height="82" src="https://images2.imgbox.com/fa/2b/d4UhjESm_o.png" width="1198"></p> 
<p></p> 
<h3>3.1 Hive库操作</h3> 
<h4>3.1.1 创建数据库</h4> 
<blockquote> 
 <p>1）创建一个数据库，数据库在<strong>HDFS上的默认存储路径是/hive/warehouse/*.db</strong>。</p> 
 <p><strong>create database testdb;</strong></p> 
 <p></p> 
 <p>2）避免要创建的数据库已经存在错误，增加if not exists判断。<strong>（标准写法）</strong></p> 
 <p><strong>create database if not exists testdb;</strong></p> 
 <p></p> 
 <p>3）创建数据库并指定位置</p> 
 <p><strong>create database if not exist 数据库名 location 指定路径；</strong></p> 
</blockquote> 
<p></p> 
<h4>3.1.2 修改数据库</h4> 
<blockquote> 
 <p>alter database dept set dbproperties('createtime'='20220531');</p> 
 <p>数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。</p> 
</blockquote> 
<p></p> 
<h4>3.1.3数据库详细信息</h4> 
<blockquote> 
 <p>1）显示数据库（show）</p> 
 <p><strong>show databases;</strong></p> 
 <p></p> 
 <p>2）可以通过like进行过滤</p> 
 <p><strong>show databases like 't*';</strong></p> 
 <p></p> 
 <p>3）查看详情（desc）</p> 
 <p><strong>desc database testdb;</strong></p> 
 <p></p> 
 <p>4）切换数据库（use）</p> 
 <p><strong>use testdb;</strong></p> 
</blockquote> 
<p></p> 
<h4>3.1.4删除数据库（将删除的目录移动到回收站中）</h4> 
<blockquote> 
 <p>1）最简写法</p> 
 <p><strong>drop database testdb;</strong></p> 
 <p></p> 
 <p>2）如果删除的数据库不存在，最好使用if exists判断数据库是否存在。否则会报错：FAILED: SemanticException [Error 10072]: Database does not exist: db_hive</p> 
 <p><strong>drop database if exists testdb;</strong></p> 
 <p></p> 
 <p>3)如果数据库不为空，使用cascade命令进行强制删除。报错信息如下FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)</p> 
 <p><strong>drop database if exists testdb cascade;</strong></p> 
</blockquote> 
<pre></pre> 
<p></p> 
<h3>3.2 Hive数据类型</h3> 
<h4>3.2.1 基础数据类型：</h4> 
<p><img alt="" height="973" src="https://images2.imgbox.com/c7/02/Ta6vB9rG_o.png" width="1200"></p> 
<p></p> 
<h4>3.2.2复杂的数据类型</h4> 
<p><img alt="" height="622" src="https://images2.imgbox.com/d4/83/Q93uytJQ_o.png" width="1200"></p> 
<p></p> 
<h3>3.3 Hive表操作</h3> 
<p>Hive没有专门的数据文件格式,常见的有以下几种:</p> 
<p><strong>TEXTFILE</strong> ​ SEQUENCEFILE ​ AVRO ​ <strong>RCFILE </strong>​ <strong>ORCFILE</strong> ​ <strong>PARQUET</strong></p> 
<blockquote> 
 <p><strong>TextFile:<br>        TEXTFILE 即正常的文本格式，是Hive默认文件存储格式，此种格式的表文件在HDFS上是明文，可用hadoop fs -cat命令查看，从HDFS上get下来后也可以直接读取。</strong><br>       </p> 
 <p>RCFile:<br> 是Hadoop中第一个列文件格式。能够很好的压缩和快速的查询性能。通常写操作比较慢，比非列形式的文件格式需要更多的内存空间和计算量。</p> 
 <p></p> 
 <p><strong>ORCFile:<br> Hive从0.11版本开始提供了ORC的文件格式，ORC文件不仅仅是一种列式文件存储格式，最重要的是有着很高的压缩比，并且对于MapReduce来说是可切分（Split）的。因此，在Hive中使用ORC作为表的文件存储格式，不仅可以很大程度的节省HDFS存储资源，而且对数据的查询和处理性能有着非常大的提升。</strong></p> 
 <p></p> 
 <p>Parquet:<br> Parquet仅仅是一种存储格式，它是语言、平台无关的，并且不需要和任何一种数据处理框架绑定。这也是parquet相较于orc的仅有优势：支持嵌套结构。</p> 
 <p></p> 
 <p>SEQUENCEFILE:<br> SequenceFile是Hadoop API 提供的一种二进制文件，它将数据以&lt;key,value&gt;的形式序列化到文件中。这种二进制文件内部使用Hadoop 的标准的Writable 接口实现序列化和反序列化。</p> 
 <p></p> 
 <p>AVRO:<br> Avro是一种用于支持数据密集型的二进制文件格式。它的文件格式更为紧凑，若要读取大量数据时，Avro能够提供更好的序列化和反序列化性能。并且Avro数据文件天生是带Schema定义的，所以它不需要开发者在API 级别实现自己的Writable对象。Avro提供的机制使动态语言可以方便地处理Avro数据。最近多个Hadoop 子项目都支持Avro 数据格式，如Pig 、Hive、Flume、Sqoop和Hcatalog。</p> 
</blockquote> 
<p></p> 
<p><img alt="" height="656" src="https://images2.imgbox.com/4b/eb/6ndY2FeH_o.png" width="1200"></p> 
<p></p> 
<p></p> 
<p><img alt="" height="670" src="https://images2.imgbox.com/f6/95/e9bnChqn_o.png" width="1200"></p> 
<p></p> 
<h4>3.3.1 创建表</h4> 
<p>[ ]内的内容属于可选内容</p> 
<blockquote> 
 <h6>建表1：全部使用默认建表方式</h6> 
 <p>create table IF NOT EXISTS students<br> (<br>     id bigint,<br>     name string,<br>     age int,<br>     gender string,<br>     clazz string<br> )<br> ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';    // 必选，指定列分隔符 </p> 
 <p></p> 
 <h6>建表2：指定location(这种方式比较常用)</h6> 
 <p>create table IF NOT EXISTS students2<br> (<br>     id bigint,<br>     name string,<br>     age int,<br>     gender string,<br>     clazz string<br> )<br> ROW FORMAT DELIMITED FIELDS TERMINATED BY ','<br> LOCATION '/bigdata27/input1';   </p> 
 <p>// 指定Hive表的数据的存储位置，一般在数据已经上传到HDFS，想要直接使用，会指定Location，通常Locaion会跟外部表一起使用，内部表一般使用默认的location</p> 
 <p></p> 
 <h6>建表3：指定存储格式</h6> 
 <p>create table IF NOT EXISTS test_orc_tb<br> (<br>     id bigint,<br>     name string<br> )<br> ROW FORMAT DELIMITED FIELDS TERMINATED BY ','<br> STORED AS ORC<br> LOCATION '/bigdata29/out6';</p> 
 <p>// 指定储存格式为orcfile，如果不指定，默认为textfile，注意：除textfile以外，其他的存储格式的数据都不能直接加载，需要使用从表加载的方式。</p> 
 <p></p> 
 <h6>建表4：<strong>将查询的结果作为表数据</strong></h6> 
 <p>create table xxxx as select ... from ... (表不存在，会新建一个表)</p> 
 <p></p> 
 <p>insert into table 表名 select ... from ... (表以存在，将查询的数据插入表中)</p> 
 <p>//覆盖插入 把into 换成 overwrite</p> 
 <p></p> 
 <p></p> 
 <h6>建表5：建的表与另一张表结构相同</h6> 
 <p>create table 新建表 like 结构相同表</p> 
</blockquote> 
<p></p> 
<p>举例：</p> 
<p><strong>简单用户信息表创建：</strong></p> 
<blockquote> 
 <p>create table t_user(<br> id int,<br> uname string,<br> pwd string,<br> gender string,<br> age int<br> )<br> row format delimited fields terminated by ','<br> lines terminated by '\n';</p> 
</blockquote> 
<p>表数据：</p> 
<blockquote> 
 <p>1,admin,123456,男,18<br> 2,zhangsan,abc123,男,23<br> 3,lisi,654321,女,16</p> 
</blockquote> 
<p></p> 
<p><strong>复杂人员信息表创建： </strong></p> 
<blockquote> 
 <p>create table IF NOT EXISTS t_person(<br> name string,<br> friends array&lt;string&gt;,<br> children map&lt;string,int&gt;,<br> address struct&lt;street:string ,city:string&gt;<br> )<br> row format delimited fields terminated by ',' -- 列与列之间的分隔符<br> collection items terminated by '_' -- 元素与元素之间分隔符<br> map keys terminated by ':' -- Map数据类型键与值之间的分隔符<br> lines terminated by '\n';  -- 行与行之间的换行符</p> 
</blockquote> 
<p>表数据：</p> 
<blockquote> 
 <p>songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,beng bu_anhui<br> yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,he fei_anhui</p> 
</blockquote> 
<p></p> 
<h3>3.3.2显示表的信息</h3> 
<blockquote> 
 <p>show tables;<br> show tables like 'u*';<br> desc t_person;<br> desc formatted students; // 更加详细</p> 
</blockquote> 
<p></p> 
<h3>3.3.3加载数据</h3> 
<blockquote> 
 <h6>1、使用<code>hdfs dfs -put '本地数据' 'hive表对应的HDFS目录下'</code></h6> 
 <h6>2、使用 load data</h6> 
 <p>（1）将HDFS上的/input1目录下面的数据 移动至 students表对应的HDFS目录下<br> load data inpath '/input1/students.txt' into table students</p> 
 <p></p> 
 <p>（2）加上 local 关键字 可以将Linux本地目录下的文件 上传到 hive表对应HDFS 目录下 原文件不会被删除</p> 
 <p>load data local inpath '/usr/local/soft/data/students.txt' into table students;</p> 
 <p></p> 
 <p>（3）// overwrite 覆盖加载<br> load data local inpath '/usr/local/soft/data/students.txt' overwrite into table students;</p> 
 <p></p> 
</blockquote> 
<p></p> 
<p></p> 
<h3>3.3.4导出数据</h3> 
<blockquote> 
 <p><strong>将查询结果存放到本地</strong></p> 
 <p>1.首先在本地（linux）上创建存放数据的文件夹</p> 
 <p>2.导出查询结果的数据</p> 
 <p>举例：</p> 
 <p>insert overwrite local directory '本地路径' select xxx from xxx;</p> 
 <h4></h4> 
 <p><strong>按照指定的方式将数据输出到本地</strong></p> 
 <p>1.创建存放数据的目录</p> 
 <p>2.导出查询结果的数据</p> 
 <p>举例：</p> 
 <p>insert overwrite local directory '/usr/local/soft/shujia/person' <br> ROW FORMAT DELIMITED fields terminated by ',' <br> collection items terminated by '-' <br> map keys terminated by ':' <br> lines terminated by '\n' <br> select * from t_person;</p> 
</blockquote> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<h3>3.3.5清空表数据与删除表</h3> 
<blockquote> 
 <p><strong>清空表数据</strong></p> 
 <p>truncate table 表名;</p> 
 <p><strong>删除表</strong></p> 
 <p>drop table 表名;</p> 
</blockquote> 
<p></p> 
<h3>3.3.5修改列</h3> 
<blockquote> 
 <h6>查询表结构</h6> 
 <p>desc 表名;</p> 
 <p></p> 
 <p><strong>添加列</strong></p> 
 <p>举例：alter table students2 add columns (education string);</p> 
 <p></p> 
 <p><strong>更新列 </strong></p> 
 <p>举例：alter table stduents2 change education educationnew string;</p> 
</blockquote> 
<p></p> 
<h2>四、Hive内部表与外部表</h2> 
<h3>内部表简介：</h3> 
<p><strong>1.默认建表的类型就是内部表</strong></p> 
<p><strong>2.删除表的时候，表在hdfs中对应的文件夹会被删除，同时表数据（hdfs中的文件）也会被删除，</strong></p> 
<p><strong>在数据库中存储的元数据信息也会被删除</strong></p> 
<p><span style="color:#a2e043;">举例：</span></p> 
<blockquote> 
 <p>// 内部表<br> create table student3<br> (<br>     id bigint,<br>     name string,<br>     age int,<br>     gender string,<br>     clazz string<br> )<br> ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';</p> 
</blockquote> 
<p></p> 
<p></p> 
<h3>外部表简介：</h3> 
<p><strong>1.外部表使用<code>EXTERNAL</code>关键字创建</strong></p> 
<p><strong>2.外部表因为是指定其他的hdfs路径的数据加载到表中来，所以hive会认为自己不完全独占这份数据，所以删除hive表的时候，数据仍然保存在hdfs中不会被删除，但是数据库中的元数据会被删除。</strong></p> 
<p><strong>3.设计外部表的初衷就是让表的元数据与表数据（hdfs下的文件数据）解耦</strong></p> 
<p><span style="color:#a2e043;">举例：</span></p> 
<blockquote> 
 <p>// 外部表<br> create external table students_external<br> (<br>     id bigint,<br>     name string,<br>     age int,<br>     gender string,<br>     clazz string<br> )<br> ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';</p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/cee3c85e83f17da3b777dae77531de5c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">版本匹配指南：Numpy版本和Python版本的对应关系</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/21297bec9320c8c1aa4f532318e0fe39/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">SQL Server详细使用教程：安装步骤、必备知识点与常见问题解析</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>