<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python中bs4的soup.find()和soup.find_all()用法 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/1123dd18557b111404da60c560e1a024/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Python中bs4的soup.find()和soup.find_all()用法">
  <meta property="og:description" content="一、背景 我们在使用python对网页爬虫的时候，经常会得到一些html数据，因此我们就会利用soup.find()和soup.find_all()方法来筛选出想要的数据。
二、用法 1.soup.find() 1.1利用name来查找 代码如下：
from bs4 import BeautifulSoup html_string = &#34;&#34;&#34;&lt;div&gt; &lt;h1 class=&#34;item&#34; id=&#34;x1&#34;&gt;蔡x坤&lt;/h1&gt; &lt;ul class=&#34;item&#34; id=&#34;x2&#34;&gt; &lt;li&gt;唱&lt;/li&gt; &lt;li&gt;跳&lt;/li&gt; &lt;li&gt;rap&lt;/li&gt; &lt;li&gt;篮球&lt;/li&gt; &lt;/ul&gt; &lt;div class=&#34;item &#34;id=&#39;x3&#39;&gt; &lt;span&gt;你干嘛&lt;/span&gt; &lt;a href=&#34;www.xxx.com&#34; class=&#39;info&#39;&gt;ikun.com&lt;/a&gt; &lt;/div&gt; &lt;/div&gt;&#34;&#34;&#34; soup = BeautifulSoup(html_string, features=&#34;html.parser&#34;) #利用name来查找 tag_list=soup.find(name=&#34;h1&#34;) print(tag_list) 结果如下：
&lt;h1 class=&#34;item&#34; id=&#34;x1&#34;&gt;蔡x坤&lt;/h1&gt;
1.2利用属性attrs来寻找 代码如下：
html_string = &#34;&#34;&#34;&lt;div&gt; &lt;h1 class=&#34;item&#34; id=&#34;x1&#34;&gt;蔡x坤&lt;/h1&gt; &lt;ul class=&#34;item&#34; id=&#34;x2&#34;&gt; &lt;li&gt;唱&lt;/li&gt; &lt;li&gt;跳&lt;/li&gt; &lt;li&gt;rap&lt;/li&gt; &lt;li&gt;篮球&lt;/li&gt; &lt;/ul&gt; &lt;div class=&#34;item &#34;id=&#39;x3&#39;&gt; &lt;span&gt;你干嘛&lt;/span&gt; &lt;a href=&#34;www.xxx.com&#34; class=&#39;info&#39;&gt;ikun.com&lt;/a&gt; &lt;/div&gt; &lt;/div&gt;&#34;&#34;&#34; soup = BeautifulSoup(html_string, features=&#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-01-28T14:44:34+08:00">
    <meta property="article:modified_time" content="2024-01-28T14:44:34+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python中bs4的soup.find()和soup.find_all()用法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>一、背景</h2> 
<p>我们在使用python对网页爬虫的时候，经常会得到一些html数据，因此我们就会利用soup.find()和soup.find_all()方法来筛选出想要的数据。</p> 
<p></p> 
<h2>二、用法</h2> 
<h3 style="background-color:transparent;">1.soup.find()</h3> 
<h4 style="background-color:transparent;"> 1.1利用name来查找</h4> 
<p>代码如下：</p> 
<pre><code class="language-python">from bs4 import BeautifulSoup

html_string = """&lt;div&gt;
    &lt;h1 class="item" id="x1"&gt;蔡x坤&lt;/h1&gt;
    &lt;ul class="item" id="x2"&gt;
        &lt;li&gt;唱&lt;/li&gt;
        &lt;li&gt;跳&lt;/li&gt;
        &lt;li&gt;rap&lt;/li&gt;
        &lt;li&gt;篮球&lt;/li&gt;
    &lt;/ul&gt;
    &lt;div class="item "id='x3'&gt;
        &lt;span&gt;你干嘛&lt;/span&gt;
        &lt;a href="www.xxx.com" class='info'&gt;ikun.com&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;"""

soup = BeautifulSoup(html_string, features="html.parser")

#利用name来查找
tag_list=soup.find(name="h1")
print(tag_list)</code></pre> 
<p> 结果如下：</p> 
<blockquote> 
 <p>&lt;h1 class="item" id="x1"&gt;蔡x坤&lt;/h1&gt;</p> 
</blockquote> 
<h4>1.2利用属性attrs来寻找</h4> 
<p>代码如下：</p> 
<pre><code class="language-python">html_string = """&lt;div&gt;
    &lt;h1 class="item" id="x1"&gt;蔡x坤&lt;/h1&gt;
    &lt;ul class="item" id="x2"&gt;
        &lt;li&gt;唱&lt;/li&gt;
        &lt;li&gt;跳&lt;/li&gt;
        &lt;li&gt;rap&lt;/li&gt;
        &lt;li&gt;篮球&lt;/li&gt;
    &lt;/ul&gt;
    &lt;div class="item "id='x3'&gt;
        &lt;span&gt;你干嘛&lt;/span&gt;
        &lt;a href="www.xxx.com" class='info'&gt;ikun.com&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;"""

soup = BeautifulSoup(html_string, features="html.parser")
#利用属性attrs查找
tag_list=soup.find(attrs={"id":"x3"})
print(tag_list)</code></pre> 
<p>结果如下：</p> 
<blockquote> 
 <p> &lt;div class="item" id="x3"&gt;<br> &lt;span&gt;你干嘛&lt;/span&gt;<br> &lt;a class="info" href="www.xxx.com"&gt;ikun.com&lt;/a&gt;<br> &lt;/div&gt;</p> 
</blockquote> 
<h4 style="background-color:transparent;">1.3利用name和attrs寻找 </h4> 
<p>代码如下：</p> 
<pre><code class="language-python">html_string = """&lt;div&gt;
    &lt;h1 class="item" id="x1"&gt;蔡x坤&lt;/h1&gt;
    &lt;ul class="item" id="x1"&gt;我是一名练习生&lt;/ul&gt;
    &lt;ul class="item" id="x2"&gt;
        &lt;li&gt;唱&lt;/li&gt;
        &lt;li&gt;跳&lt;/li&gt;
        &lt;li&gt;rap&lt;/li&gt;
        &lt;li&gt;篮球&lt;/li&gt;
    &lt;/ul&gt;
    &lt;div class="item "id='x3'&gt;
        &lt;span&gt;你干嘛&lt;/span&gt;
        &lt;a href="www.xxx.com" class='info'&gt;ikun.com&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;"""

soup = BeautifulSoup(html_string, features="html.parser")
#利用name和attrs寻找
tag_list=soup.find(name="ul",attrs={"id":"x2"})
print(tag_list)</code></pre> 
<p>结果如下：</p> 
<blockquote> 
 <p> &lt;ul class="item" id="x2"&gt;<br> &lt;li&gt;唱&lt;/li&gt;<br> &lt;li&gt;跳&lt;/li&gt;<br> &lt;li&gt;rap&lt;/li&gt;<br> &lt;li&gt;篮球&lt;/li&gt;<br> &lt;/ul&gt;</p> 
</blockquote> 
<h3>2.soup.find_all()</h3> 
<h4>2.1利用name找多个</h4> 
<p>代码如下：</p> 
<pre><code class="language-python">html_string = """&lt;div&gt;
    &lt;h1 class="item" id="x1"&gt;蔡x坤&lt;/h1&gt;
    &lt;ul class="item" id="x1"&gt;我是一名练习生&lt;/ul&gt;
    &lt;ul class="item" id="x2"&gt;
        &lt;li&gt;唱&lt;/li&gt;
        &lt;li&gt;跳&lt;/li&gt;
        &lt;li&gt;rap&lt;/li&gt;
        &lt;li&gt;篮球&lt;/li&gt;
    &lt;/ul&gt;
    &lt;div class="item "id='x3'&gt;
        &lt;span&gt;你干嘛&lt;/span&gt;
        &lt;a href="www.xxx.com" class='info'&gt;ikun.com&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;"""

soup = BeautifulSoup(html_string, features="html.parser")
#利用name找多个
tag_list=soup.find_all(name="li")
for tag in tag_list:
    print(tag.name,tag.text)</code></pre> 
<p>结果如下：（输出name和text）</p> 
<blockquote> 
 <p>li 唱<br> li 跳<br> li rap<br> li 篮球</p> 
</blockquote> 
<h4>2.2利用attrs找多个</h4> 
<p>代码如下：</p> 
<pre><code class="language-python">from bs4 import BeautifulSoup

html_string = """&lt;div&gt;
    &lt;h1 class="item" id="x1"&gt;蔡x坤&lt;/h1&gt;
    &lt;ul class="item" id="x1"&gt;我是一名练习生&lt;/ul&gt;
    &lt;ul class="item" id="x2"&gt;
        &lt;li&gt;唱&lt;/li&gt;
        &lt;li&gt;跳&lt;/li&gt;
        &lt;li&gt;rap&lt;/li&gt;
        &lt;li&gt;篮球&lt;/li&gt;
    &lt;/ul&gt;
&lt;/div&gt;"""

soup = BeautifulSoup(html_string, features="html.parser")

#用attrs找多个
tag_list=soup.find_all(attrs={"class":"item"})
for tag in tag_list:
    print(tag.name,tag.text)</code></pre> 
<p>结果如下：（输出name和text） </p> 
<blockquote> 
 <p>h1 蔡x坤<br> ul 我是一名练习生<br> ul <br> 唱<br> 跳<br> rap<br> 篮球</p> 
</blockquote> 
<h4 style="background-color:transparent;">2.3利用recursive判断是否递归寻找，默认为True</h4> 
<p>代码如下：（recursive=False    只找儿子）</p> 
<pre><code class="language-python">from bs4 import BeautifulSoup

html_string = """&lt;div&gt;
    &lt;div id='x1'&gt;
        &lt;span&gt;你干嘛&lt;/span&gt;
        &lt;a href="www.xxx.com" class='info'&gt;ikun.com&lt;/a&gt;
        &lt;div&gt;
            &lt;span&gt;这是一句话&lt;/span&gt;
                &lt;span&gt;这是一句话的一句话&lt;/span&gt;
            &lt;span&gt;这也是一句话&lt;/span&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;"""

soup = BeautifulSoup(html_string, features="html.parser")

#找儿子
tag_list1=soup.find(attrs={"id":"x1"})
for tag in tag_list1.find_all(recursive=False):
    print(tag)</code></pre> 
<p>结果如下（recursive=False    只找儿子）：</p> 
<blockquote> 
 <p>&lt;span&gt;你干嘛&lt;/span&gt;<br> &lt;a class="info" href="www.xxx.com"&gt;ikun.com&lt;/a&gt;<br> &lt;div&gt;<br> &lt;span&gt;这是一句话&lt;/span&gt;<br> &lt;span&gt;这是一句话的一句话&lt;/span&gt;<br> &lt;span&gt;这也是一句话&lt;/span&gt;<br> &lt;/div&gt;</p> 
</blockquote> 
<p>代码如下：（recursive=True    找子子孙孙） </p> 
<pre><code class="language-python">from bs4 import BeautifulSoup

html_string = """&lt;div&gt;
    &lt;div id='x1'&gt;
        &lt;span&gt;你干嘛&lt;/span&gt;
        &lt;a href="www.xxx.com" class='info'&gt;ikun.com&lt;/a&gt;
        &lt;div&gt;
            &lt;span&gt;这是一句话&lt;/span&gt;
                &lt;span&gt;这是一句话的一句话&lt;/span&gt;
            &lt;span&gt;这也是一句话&lt;/span&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;"""

soup = BeautifulSoup(html_string, features="html.parser")

#找子子孙孙
tag_list1=soup.find(attrs={"id":"x1"})
for tag in tag_list1.find_all(recursive=True):
    print(tag)</code></pre> 
<p>结果如下（recursive=True    找子子孙孙）：</p> 
<blockquote> 
 <p>&lt;span&gt;你干嘛&lt;/span&gt;<br> &lt;a class="info" href="www.xxx.com"&gt;ikun.com&lt;/a&gt;<br> &lt;div&gt;<br> &lt;span&gt;这是一句话&lt;/span&gt;<br> &lt;span&gt;这是一句话的一句话&lt;/span&gt;<br> &lt;span&gt;这也是一句话&lt;/span&gt;<br> &lt;/div&gt;<br> &lt;span&gt;这是一句话&lt;/span&gt;<br> &lt;span&gt;这是一句话的一句话&lt;/span&gt;<br> &lt;span&gt;这也是一句话&lt;/span&gt;</p> 
</blockquote> 
<h2>三、案例 </h2> 
<p>爬取易车网的车品牌为例子（本例子参考python讲师武沛齐老师）</p> 
<h3>1.分析网页</h3> 
<p>用chrome的无痕网页打开<a class="link-info" href="https://car.yiche.com/" rel="nofollow" title="https://car.yiche.com/">https://car.yiche.com/</a>并分析网页</p> 
<p><img alt="" src="https://images2.imgbox.com/bb/82/Xkx6m5Xl_o.png"></p> 
<p>分析发现车牌的名字在 name="div", attrs={"class": "item-brand"}里面</p> 
<h3>2.模拟请求，获取HTML文本</h3> 
<pre><code class="language-python">import requests
from bs4 import BeautifulSoup

# 获取HTML文本
res = requests.get(
    url="https://car.yiche.com/"
)</code></pre> 
<h3>3.筛选数据</h3> 
<pre><code class="language-python">import requests
from bs4 import BeautifulSoup

# 获取HTML文本
res = requests.get(
    url="https://car.yiche.com/"
)

soup = BeautifulSoup(res.text, features="html.parser")

#创一个列表存
result_list=[]

#筛选数据
tag_list = soup.find_all(name="div", attrs={"class": "item-brand"})
for tag in tag_list:
    result_list.append(tag.attrs["data-name"])
print(result_list)</code></pre> 
<h3>4.结果</h3> 
<blockquote> 
 <p> </p> 
 <p>['奥迪', '埃安', 'AITO', '阿斯顿·马丁', '阿维塔', '阿尔法·罗密欧', '爱驰', 'AUXUN傲旋', 'ALPINA', 'Apollo', '阿尔卑斯', 'Abarth', 'ABT', '安凯客车', '安徽猎豹', 'Arash', 'Aurus', '艾康尼克', 'Agile Automotive', 'APEX', 'ATS', 'Ariel', 'Aspark', 'ARMADILLO', 'Alpine', 'AURA', 'Aviar', 'AC Schnitzer', 'Atlis', 'AEHRA', 'Aria', 'Alpha Motor', 'AZNOM', 'AEV ROBOTICS', '阿尔特', 'AFEELA', 'ASKA', 'AKXY2', 'Alef', 'AIM', 'ATOM', '安培', '本田', '奔驰', '比亚迪', '宝马', '别克', '保时捷', '北京', '奔腾', '宝骏', '标致', '宾利', 'BAW北汽制造', '北京汽车', '布加迪', '博速', '北汽昌河', '奔驰卡车', '巴菲特', '霸王龙', '宝沃', '北汽瑞翔', '北汽新能源', '北汽幻速', '北奔重卡', '百智新能源', '北汽威旺', '北汽雷驰', '宾尼法利纳', '比速汽车', '百度Apollo', '比德文汽车', '铂驰', '宝骐汽车', '博世', '拜腾', '北汽泰普', '宝腾', 'BAO', '保斐利', '北汽道达', '北汽黑豹', '北京清行', '博郡汽车', 'Bowler', 'BAC', 'Bertone', 'Bollinger Motors', 'BeyonCa', 'Brabham', '比克汽车', 'Bremach', 'Bizzarrini', '宝雅', '长安', '长安启源', '长安欧尚', '长城', '长安凯程', '长安跨越', '创维汽车', '曹操', '橙仕', '乘龙汽车', '成功汽车', '车驰汽车', '超境汽车', 'Charge Cars', '采埃孚', 'CANDELA', '车和家', 'Cupra', '长江EV', 'Conquest', 'Corbellati', '昶洧', 'Czinger', 'Caterham', 'Canoo', 'Continental', '大众', '东风风神', '东风风行', '东风风光', '东风奕派', '东风', '东风纳米', '道奇', '东风小康', 'DS', '东南', '东风轻型车', '东风风度', '大运', '大力牛魔王', '东风御风', '东风商用车', '电动屋', '东风富康', '滴滴', '东风·瑞泰特', '大乘汽车', '东风氢舟', 'Dianchè', 'De Tomaso', 'Drako', '电咖', '大迪', 'Delage', 'DEUS Automobiles', '大发', 'DAVID BROWN', '达契亚', 'Donkervoort', 'Datsun', 'dÄHLer', 'DeLorean', 'Electra Meccanica', 'Elektron', 'EdisonFuture', 'Elemental', 'E.Go', 'E-Legend', '丰田', '福特', '福田', '法拉利', '飞凡汽车', '方程豹', '飞碟汽车', '菲亚特', '福迪', 'Faraday Future', '丰田纺织', '法诺新能源', 'FOXTRON', 'Fox e-mobility', '弗那萨利', 'Frangivento', 'Fresco', 'Fisker', '辅恒汽车', '广汽传祺', '高合汽车', '广汽集团', '观致', 'GMC', '国机智骏', '光冈', '谷歌', '国金汽车', 'GTA', '广汽吉奥', '国吉商用车', 'G&amp;B Design', '广通客车', '广汽日野', 'Gumpert', '格罗夫', 'GAZ', 'NEVS国能汽车', 'GFG Style', 'GLM', 'G-Power', 'Gemballa', 'Ginetta', 'GMA', 'GYON', 'GUNTHER WERKS', '国新新能源', '高通', '红旗', '哈弗', '昊铂', '海马', '合创汽车', '悍马', '恒驰', '黄海', '汉腾汽车', '华晨新日', '活越', '华梓汽车', '恒润汽车', '华泰', '汉龙汽车', '恒天', '华夏领舰', '哈飞', '华菱汽车', '华颂', 'Hyperion', 'Hennessey', '红星汽车', '华凯', '宏远汽车', '华普', '海格', 'HOFELE', '汇众', '毫末智行', '华骐', 'HOPIUM', 'Hispano Suiza', '华利', '霍顿', 'Hudson', 'HURTAN', 'Holon', 'iCAR汽车', 'Inferno', 'Italdesign', 'INEOS', 'Icona', 'INKAS', 'IZERA', 'IED', 'INDI', 'Indigo', '吉利汽车', '捷途', '捷达', '吉利银河', '捷豹', '极氪', 'Jeep', 'ARCFOX极狐', '吉利几何', '江铃', '捷尼赛思', '金杯', '江汽集团', '江淮瑞风', '极石汽车', '江淮汽车', '江铃集团新能源', '江淮钇为', 'Polestar极星', '极越', '钧天', '金龙', '江南汽车', '金旅', '江铃旅居车', '九龙', '嘉远汽车', '江铃重汽', '君马汽车', '金冠汽车', '江铃晶马汽车', '金琥汽车', '吉威新能源', 'Jannarelly', '奇点汽车', '佳跃', '吉祥汽车', '凯迪拉克', '凯翼', '开瑞', '科尼赛克', '克蒂汽车', '开云汽车', '克莱斯勒', '科瑞斯的', '克慕勒', '凯马汽车', 'Karlmann', '焜驰', '开沃汽车', '卡尔森', 'KTM', '凯佰赫', 'Kimera', '开利', 'Karma', '卡威', 'KHANN', '卡升', '克罗斯哈特', '路虎', '领克', '雷克萨斯', '理想汽车', '林肯', '零跑汽车', '劳斯莱斯', '兰博基尼', '岚图汽车', '铃木', '路特斯', '凌宝汽车', '雷诺', '猎豹汽车', '雷丁', '蓝电', '拉帝', 'LUMMA', '陆风', 'Lorinser', '菱势汽车', '力帆汽车', '理念', '雷达汽车', 'LEVC', '莲花汽车', '领途汽车', '联合卡车', 'LG', '龙程汽车', 'LIMGENE凌际', '拉达', '莱茵汽车', '陆地方舟', '罗夫哈特', '拉共达', 'Lucid', '灵悉', '蓝旗亚', '绿驰', 'LITE', '雷诺三星', '罗孚', '朗世', 'Lightyear', 'LOCAL MOTORS', '领志', 'LeSEE', 'Lordstown Motors', 'LUNAZ', ' LIUX ', '洛轲智能', '蓝擎汽车', '马自达', '名爵', '玛莎拉蒂', 'MINI', '迈凯伦', '迈巴赫', '猛士', '迈莎锐', '摩登汽车', '迈越', '摩根', '敏安汽车', 'Michelin米其林', '曼', 'Mole', '迈迈', 'Manhart', 'Meyers Manx', 'MILITEM', 'MAGNA', 'Micro', 'Munro', 'Mazzanti', '美亚', 'Mahindra', 'MELKUS', 'Mobilize', 'Moke ', 'Mopar', '魅族', 'Mullen', '哪吒汽车', '纳智捷', '南骏汽车', 'NamX', '诺博汽车', 'Naran', 'nanoFLOWCELL', 'Neuron EV', 'NEXT LEVEL', 'Nikola', 'Noble', 'Novitec', '欧拉', '讴歌', 'OBBIN', '欧宝', '欧铃汽车', '欧朗', '欧联', '帕加尼', '朋克汽车', 'Posaidon', 'Puritalia', 'Praga', 'Piëch Automotive', '佩奇奥', 'Pogea Racing', 'Project Arrow', '奇瑞', '起亚', '启辰', '奇瑞新能源', '乔治·巴顿', '庆铃五十铃', '青岛解放', '前途', '前晨汽车', '全球鹰', '骐铃汽车', '奇鲁汽车', '清源汽车', '轻橙时代', '日产', '荣威', '睿蓝汽车', 'RAM', '瑞弗', '瑞驰新能源', '容大智造', 'Rezvani', '如虎', '瑞麒', '瑞腾汽车', '锐马克', 'RIVIAN', 'Ringbrothers', 'RENOVO', 'Rinspeed', 'Radical', 'Radford', 'REVO ZERO', '深蓝汽车', '上汽大通MAXUS', '三菱', '斯柯达', '斯巴鲁', '思皓', 'smart', 'SWM斯威汽车', '思铭', 'SERES赛力斯', 'SONGSAN MOTORS', '陕汽重卡', '上汽轻卡', '时风', '三一集团', '沙龙汽车', '上汽红岩', '斯堪尼亚', '山姆', '盛唐', 'SHELBY', '陕汽轻卡', '双龙', '萨博', '世爵', '陕汽商用车', '上汽集团', '上海', '赛麟', '斯太尔', '神州', '斯达泰克', '双环', '陕汽通家', '申龙客车', '上喆', 'SSC', 'SONY', '首望', 'SVE', 'STI', 'SPIRRA', 'SCOUT', 'Spyros Panopoulos', '速达', 'Spofec', 'Sono Motors', 'Scion', 'Share2Drive', 'SIN CARS', 'Singer', '深向', 'Silence', '世极', '特斯拉', '坦克', '腾势', '天际汽车', '泰卡特', '塔塔', '泰克鲁斯·腾风', 'Triton', 'TopCar', 'Touring Superleggera', 'Troller', 'Togg', 'Theon Design', 'Tramontana', 'TOROIDION', 'TVR', 'TECHART', '途柚汽车 ', 'THOR', '拓锐斯特 ', 'Telo', 'THK', 'TWR', '天地良心汽车', 'Ugur Sahin Design', 'Ultima', 'Uniti', 'Vinfast', 'Vega Innovations', 'VLF Automotive', 'Vanda Electric', 'VANTAS', 'VIRITECH', 'Venturi', 'Vanwall', '五菱汽车', '沃尔沃', '蔚来', '魏牌', '五十铃', '未奥汽车', '沃尔沃卡车', '威马汽车', '维努斯', '瓦滋', 'WALD', '威兹曼', '伟昊汽车', '潍柴欧睿', '潍柴英致', '万象汽车', '威麟', 'W Motors', '沃克斯豪尔', 'WayRay', '现代', '雪佛兰', '星途', '小鹏', '雪铁龙', '小米汽车', '小鹏汇天', '小跑车', '小虎', 'SRM鑫源', '新龙马汽车', '徐工汽车', '新吉奥', 'AM晓奥汽车', '西雅特', '现代摩比斯', '小猬汽车', '新特汽车', '星客特', '英菲尼迪', '仰望', '依维柯', '一汽解放', '云度', '一汽', '远程', '野马汽车', '一汽解放轻卡', '驭胜', '远航汽车', '宇通客车', '野马新能源', '运良', '一汽凌河', '雅升汽车', 'YAMAHA', '游侠', '云雀汽车', '悠遥科技', '永源', 'IMSA英飒', '悠跑科技', '悠宝利', '银隆新能源', '御捷', '翼刻', '裕路', '易电易行', '一汽富维', '怡亚通', '越界', '智己汽车', '智界', '众泰', '中国重汽VGV', '中国重汽', '中兴', '中华', '重汽王牌', '重汽豪曼', '知豆', '智行盒子', '自游家', '中欧房车', '智点汽车', '正道汽车', '中通客车', '之诺', 'Zenvo', '中植汽车', '777']</p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e823f360caf7ff584d32530eec8c0442/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">HBase数据类型：基本数据类型和复合数据类型</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9d5f83660790e156ae063a249765b9f5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">基于Python&#43;大数据的线上教育平台数据分析系统设计与实现</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>