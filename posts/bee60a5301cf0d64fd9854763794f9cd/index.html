<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>从YOLOv1到YOLOv8的YOLO系列最新综述【2023年4月】 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/bee60a5301cf0d64fd9854763794f9cd/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="从YOLOv1到YOLOv8的YOLO系列最新综述【2023年4月】">
  <meta property="og:description" content="作者：Juan R. Terven 、Diana M. Cordova-Esparaza 摘要：YOLO已经成为机器人、无人驾驶汽车和视频监控应用的核心实时物体检测系统。我们对YOLO的演变进行了全面的分析，研究了从最初的YOLO到YOLOv8每次迭代的创新和贡献。我们首先描述了标准指标和后处理；然后，我们讨论了每个模型的网络结构和训练技巧的主要变化。最后，我们总结了YOLO发展的基本经验，并提供了对其未来的看法，强调了提高实时物体检测系统的潜在研究方向。
1. 简介 实时物体检测已经成为众多应用中的一个重要组成部分，横跨自主车辆、机器人、视频监控和增强现实等各个领域。在各种物体检测算法中，YOLO（You Only Look Once）框架因其在速度和准确性方面的显著平衡而脱颖而出，能够快速、可靠地识别图像中的物体。自成立以来，YOLO系列已经经历了多次迭代，每次都是在以前的版本基础上解决局限性并提高性能（见图1）。本文旨在全面回顾YOLO框架的发展，从最初的YOLOv1到最新的YOLOv8，阐释每个版本的关键创新、差异和改进。
本文首先探讨了原始YOLO模型的基本概念和架构，这为YOLO系列的后续进展奠定了基础。随后，我们深入探讨了从YOLOv2到YOLOv8每个版本中引入的改进和提高。这些改进包括各个方面，如网络设计、损失函数的修改、锚框的调整和输入分辨率的扩展。通过研究这些发展，对YOLO框架的演变及其对物体检测的影响有一个整体的理解。
除了讨论每个YOLO版本的具体进展外，本文还强调了在整个框架的发展过程中出现的速度和准确性之间的权衡问题。这强调了在选择最合适的YOLO模型时，考虑具体应用的背景和要求的重要性。最后，我们设想了YOLO框架的未来方向，触及了进一步研究和发展的潜在途径，这将塑造实时物体检测系统的持续进展。
2. YOLO在不同领域的应用 YOLO的实时物体检测能力在自主车辆系统中是非常宝贵的，能够快速识别和跟踪各种物体，如车辆、行人[1, 2]、自行车和其他障碍物[3, 4, 5, 6]。这些能力已被应用于许多领域，包括用于监控的视频序列中的动作识别[7][8]、体育分析[9]和人机交互[10]。
YOLO模型已被用于农业，以检测和分类作物[11, 12]、害虫和疾病[13]，协助精准农业技术和自动化耕作过程。它们还被用于生物识别、安全和面部识别系统中的面部检测任务[14, 15]。
在医学领域，YOLO已被用于癌症检测[16, 17]、皮肤分割[18]和药片识别[19]，从而提高诊断的准确性和更有效的治疗过程。在遥感领域，它已被用于卫星和航空图像中的物体检测和分类，有助于土地利用绘图、城市规划和环境监测[20, 21, 22, 23]。
安防系统已经将YOLO模型整合到视频资料的实时监控和分析中，允许快速检测可疑活动[24]、社会距离和脸部面具检测[25]。这些模型还被应用于表面检测，以检测缺陷和异常，加强制造和生产过程的质量控制[26, 27, 28]。
在交通应用中，YOLO模型已被用于车牌检测[29]和交通标志识别[30]等任务，促进了智能交通系统和交通管理解决方案的发展。它们已被用于野生动物检测和监测，以识别濒危物种，用于生物多样性保护和生态系统管理[31]。最后，YOLO已被广泛用于机器人应用[32, 33]和无人机的物体检测[34, 35]。
3. 物体检测指标和非极大值抑制（NMS） 平均精度（AP），传统上称为平均精度（mAP），是评价物体检测模型性能的常用指标。它测量所有类别的平均精度，提供一个单一的数值来比较不同的模型。COCO数据集没有对AP和AP进行区分。在本文的其余部分，我们将把这个指标称为AP。
在YOLOv1和YOLOv2中，用于训练和基准测试的数据集是PASCAL VOC 2007和VOC 2012[36]。然而，从YOLOv3开始，使用的数据集是微软COCO（Common Objects in Context）[37]。对于这些数据集，AP的计算方法是不同的。以下各节将讨论AP背后的原理，并解释它是如何计算的。
3.1 AP AP指标是基于精度-召回指标，处理多个对象类别，并使用Intersection over Union（IoU）定义一个积极的预测。
Precision和Recall：精确率衡量的是模型正面预测的准确性，而召回率衡量的是模型正确识别的实际正面案例的比例。精确率和召回率之间通常有一个权衡；例如，增加检测到的对象的数量（更高的召回率）会导致更多的假阳性（更低的精确率）。为了考虑到这种权衡，AP指标包含了精度-召回曲线，该曲线将精度与不同置信度阈值的召回率作了对比。这个指标通过考虑精度-召回曲线下的面积，对精度和召回进行了平衡的评估。
处理多个物体类别：物体检测模型必须识别和定位图像中的多个物体类别。AP指标通过单独计算每个类别的平均精度（AP），然后取所有类别中这些AP的平均值来解决这个问题（这就是为什么它也被称为平均平均精度）。这种方法确保了对每个类别的模型性能进行单独评估，从而对模型的整体性能提供了更全面的评估。
IoU：物体检测的目的是通过预测边界框来准确定位图像中的物体。AP指标包含了 &#34;联合体上的交集&#34;（IoU）措施，以评估预测的边界盒的质量。IoU是预测界线盒和地面真实界线盒的交集面积与联合面积之比（见图2）。它衡量的是地面实况和预测边界盒之间的重叠程度。COCO基准考虑了多个IoU阈值，以评估模型在不同定位精度水平上的表现。
3.2 计算AP 在VOC和COCO数据集中，AP的计算方法是不同的。在这一节中，我们将描述它是如何在每个数据集上计算的。
VOC数据集
这个数据集包括20个物体类别。为了计算VOC中的AP，我们遵循以下步骤：
对于每个类别，通过改变模型预测的置信度阈值，计算出精确-召回曲线使用精度-召回曲线的内插11点抽样，计算每个类别的平均精度（AP）通过取所有20个类别中的AP的平均值来计算最终的平均精度（AP） COCO数据集
这个数据集包括80个对象类别，并使用更复杂的方法来计算AP。它没有使用11点插值，而是使用101点插值，也就是说，它计算了从0到1的101个召回阈值的精度，增量为0.01。另外，AP是通过对多个IoU值进行平均而不是只对一个IoU值进行平均得到的，除了一个常见的AP指标，即AP50 ，它是单个IoU阈值为0.5的AP。COCO中计算AP的步骤如下：
对于每个类别，通过改变模型预测的置信度阈值，计算出精确-召回曲线使用101-recall阈值计算每个类别的平均精度（AP）在不同的交叉联合（IoU）阈值下计算AP，通常从0.5到0.95，步长为0.05。更高的IoU阈值需要更准确的预测才能被认为是真阳性对于每个IoU阈值，取所有80个类别的AP的平均值最后，通过平均每个IoU阈值计算的AP值来计算总体AP AP计算的差异使得我们很难直接比较两个数据集的物体检测模型的性能。目前的标准使用COCO AP，因为它对一个模型在不同的IoU阈值下的表现有更精细的评估。
3.3 非极大值抑制 (NMS) 非极大值抑制（NMS）是物体检测算法中使用的一种后处理技术，用于减少重叠边界盒的数量，提高整体检测质量。物体检测算法通常会在同一物体周围产生多个具有不同置信度分数的边界框。NMS过滤掉多余的和不相关的边界盒，只保留最准确的边界盒。算法1描述了该程序。图3显示了一个包含多个重叠边界框的物体检测模型的典型输出和NMS之后的输出。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-04-13T00:34:13+08:00">
    <meta property="article:modified_time" content="2023-04-13T00:34:13+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">从YOLOv1到YOLOv8的YOLO系列最新综述【2023年4月】</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><span style="color:#1a439c;"><strong>作者：</strong></span><span style="color:#000000;"><strong>Juan R. Terven 、</strong></span><span style="color:#000000;"><strong>Diana M. Cordova-Esparaza </strong></span></p> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="color:#1a439c;"><strong>摘要：</strong></span>YOLO已经成为<strong>机器人</strong>、<strong>无人驾驶汽车</strong>和<strong>视频监控应用</strong>的核心实时物体检测系统。我们对YOLO的演变进行了全面的分析，研究了从最初的YOLO到YOLOv8每次迭代的创新和贡献。我们首先描述了标准指标和后处理；然后，我们讨论了每个模型的网络结构和训练技巧的主要变化。最后，我们总结了YOLO发展的基本经验，并提供了对其未来的看法，强调了提高实时物体检测系统的潜在研究方向。</p> 
<p style="margin-left:.0001pt;text-align:justify;"></p> 
<h3 style="margin-left:.0001pt;text-align:justify;"><span style="color:#0d0016;">1. 简介</span></h3> 
<p style="margin-left:0;text-align:justify;">实时物体检测已经成为众多应用中的一个重要组成部分，横跨自主车辆、机器人、视频监控和增强现实等各个领域。在各种物体检测算法中，YOLO（You Only Look Once）框架因其在<strong>速度和准确性</strong>方面的显著平衡而脱颖而出，能够快速、可靠地识别图像中的物体。自成立以来，YOLO系列已经经历了多次迭代，每次都是在以前的版本基础上解决局限性并提高性能（见图1）。本文旨在全面回顾YOLO框架的发展，从最初的YOLOv1到最新的YOLOv8，阐释每个版本的关键创新、差异和改进。</p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="920" src="https://images2.imgbox.com/d8/75/PjC0KDqk_o.png" width="1200">本文首先探讨了原始YOLO模型的基本概念和架构，这为YOLO系列的后续进展奠定了基础。随后，我们深入探讨了从YOLOv2到YOLOv8每个版本中引入的<strong>改进和提高</strong>。这些改进包括各个方面，如网络设计、损失函数的修改、锚框的调整和输入分辨率的扩展。通过研究这些发展，对YOLO框架的演变及其对物体检测的影响有一个整体的理解。</p> 
<p style="margin-left:0;text-align:left;">除了讨论每个YOLO版本的具体进展外，本文还强调了在整个框架的发展过程中出现的速度和准确性之间的权衡问题。这强调了在选择最合适的YOLO模型时，考虑具体应用的背景和要求的重要性。最后，我们设想了YOLO框架的未来方向，触及了进一步研究和发展的潜在途径，这将塑造实时物体检测系统的持续进展。</p> 
<p style="margin-left:0;text-align:left;"></p> 
<h3 style="margin-left:0px;text-align:left;"><span style="color:#0d0016;">2. YOLO在不同领域的应用</span></h3> 
<p style="margin-left:0;text-align:justify;">YOLO的实时物体检测能力在自主车辆系统中是非常宝贵的，能够快速识别和跟踪各种物体，如车辆、行人[<a href="#_bookmark19" rel="nofollow">1</a>, <a href="#_bookmark20" rel="nofollow">2</a>]、自行车和其他障碍物[<a href="#_bookmark21" rel="nofollow">3</a>, <a href="#_bookmark22" rel="nofollow">4</a>, <a href="#_bookmark23" rel="nofollow">5</a>, <a href="#_bookmark24" rel="nofollow">6</a>]。这些能力已被应用于许多领域，包括用于监控的视频序列中的动作识别[<a href="#_bookmark25" rel="nofollow">7][</a><a href="#_bookmark26" rel="nofollow">8]</a>、体育分析<a href="#_bookmark27" rel="nofollow">[9]</a>和人机交互<a href="#_bookmark28" rel="nofollow">[10]。</a></p> 
<p style="margin-left:0;text-align:justify;">YOLO模型已被用于农业，以检测和分类作物[<a href="#_bookmark29" rel="nofollow">11, </a><a href="#_bookmark30" rel="nofollow">12]</a>、害虫和疾病[<a href="#_bookmark31" rel="nofollow">13]</a>，协助精准农业技术和自动化耕作过程。它们还被用于生物识别、安全和面部识别系统中的面部检测任务<a href="#_bookmark32" rel="nofollow">[14, </a><a href="#_bookmark33" rel="nofollow">15]。</a></p> 
<p style="margin-left:0;text-align:justify;">在医学领域，YOLO已被用于癌症检测[<a href="#_bookmark34" rel="nofollow">16, </a><a href="#_bookmark35" rel="nofollow">17]</a>、皮肤分割[<a href="#_bookmark36" rel="nofollow">18]</a>和药片识别[<a href="#_bookmark37" rel="nofollow">19]</a>，从而提高诊断的准确性和更有效的治疗过程。在遥感领域，它已被用于卫星和航空图像中的物体检测和分类，有助于土地利用绘图、城市规划和环境监测<a href="#_bookmark38" rel="nofollow">[20, </a><a href="#_bookmark39" rel="nofollow">21,</a> <a href="#_bookmark40" rel="nofollow">22,</a> <a href="#_bookmark41" rel="nofollow">23]。</a></p> 
<p style="margin-left:0;text-align:justify;">安防系统已经将YOLO模型整合到视频资料的实时监控和分析中，允许快速检测可疑活动[<a href="#_bookmark42" rel="nofollow">24]</a>、社会距离和脸部面具检测[<a href="#_bookmark43" rel="nofollow">25]</a>。这些模型还被应用于表面检测，以检测缺陷和异常，加强制造和生产过程的质量控制<a href="#_bookmark44" rel="nofollow">[26, </a><a href="#_bookmark45" rel="nofollow">27,</a> <a href="#_bookmark46" rel="nofollow">28]。</a></p> 
<p style="margin-left:0;text-align:justify;">在交通应用中，YOLO模型已被用于车牌检测[<a href="#_bookmark47" rel="nofollow">29]</a>和交通标志识别[<a href="#_bookmark48" rel="nofollow">30]</a>等任务，促进了智能交通系统和交通管理解决方案的发展。它们已被用于野生动物检测和监测，以识别濒危物种，用于生物多样性保护和生态系统管理[<a href="#_bookmark49" rel="nofollow">31</a>]。最后，YOLO已被广泛用于机器人应用[<a href="#_bookmark50" rel="nofollow">32</a>, <a href="#_bookmark51" rel="nofollow">33</a>]和无人机的物体检测<a href="#_bookmark52" rel="nofollow">[34, </a><a href="#_bookmark53" rel="nofollow">35]。</a></p> 
<p style="margin-left:0;text-align:justify;"></p> 
<h3 style="text-align:left;"><span style="color:#0d0016;"><strong><strong><strong>3. 物体检测指标和非极大值抑制</strong></strong><strong><strong>（</strong></strong><strong><strong>NMS</strong></strong><strong><strong>）</strong></strong></strong></span></h3> 
<div> 
 <p style="margin-left:0;text-align:justify;">平均精度（AP），传统上称为平均精度（mAP），是评价物体检测模型性能的常用指标。它测量所有类别的平均精度，提供一个单一的数值来比较不同的模型。COCO数据集没有对AP和AP进行区分。在本文的其余部分，我们将把这个指标称为AP。</p> 
 <p style="margin-left:0;text-align:justify;">在YOLOv1和YOLOv2中，用于训练和基准测试的数据集是PASCAL VOC 2007和VOC 2012[<a href="#_bookmark54" rel="nofollow">36]</a>。然而，从YOLOv3开始，使用的数据集是微软COCO（Common Objects in Context）[<a href="#_bookmark55" rel="nofollow">37]</a>。对于这些数据集，AP的计算方法是不同的。以下各节将讨论AP背后的原理，并解释它是如何计算的。</p> 
 <p style="margin-left:0;text-align:justify;"></p> 
</div> 
<h4><span style="color:#0d0016;">3.1 <strong><strong><strong>AP</strong></strong></strong></span></h4> 
<p style="margin-left:0;text-align:justify;">AP指标是基于精度-召回指标，处理多个对象类别，并使用Intersection over Union（IoU）定义一个积极的预测。</p> 
<p style="margin-left:0;text-align:justify;"><strong>Precision和Recall</strong>：精确率衡量的是模型正面预测的准确性，而召回率衡量的是模型正确识别的实际正面案例的比例。精确率和召回率之间通常有一个权衡；例如，增加检测到的对象的数量（更高的召回率）会导致更多的假阳性（更低的精确率）。为了考虑到这种权衡，AP指标包含了精度-召回曲线，该曲线将精度与不同置信度阈值的召回率作了对比。这个指标通过考虑精度-召回曲线下的面积，对精度和召回进行了平衡的评估。</p> 
<p style="margin-left:0;text-align:justify;"><strong>处理多个物体类别</strong>：物体检测模型必须识别和定位图像中的多个物体类别。AP指标通过单独计算每个类别的平均精度（AP），然后取所有类别中这些AP的平均值来解决这个问题（这就是为什么它也被称为平均平均精度）。这种方法确保了对每个类别的模型性能进行单独评估，从而对模型的整体性能提供了更全面的评估。</p> 
<p style="margin-left:0;text-align:justify;"><strong>IoU</strong>：物体检测的目的是通过预测边界框来准确定位图像中的物体。AP指标包含了 "联合体上的交集"（IoU）措施，以评估预测的边界盒的质量。IoU是预测界线盒和地面真实界线盒的交集面积与联合面积之比（见图<a href="#_bookmark1" rel="nofollow">2）。</a>它衡量的是地面实况和预测边界盒之间的重叠程度。COCO基准考虑了多个IoU阈值，以评估模型在不同定位精度水平上的表现。</p> 
<p><img alt="" height="1136" src="https://images2.imgbox.com/ff/2a/vd0qTlir_o.png" width="1200"></p> 
<h4></h4> 
<h4><span style="color:#0d0016;"><strong><strong><strong>3.2 计算</strong></strong><strong><strong>AP</strong></strong></strong></span></h4> 
<p style="margin-left:0;text-align:left;">在VOC和COCO数据集中，AP的计算方法是不同的。在这一节中，我们将描述它是如何在每个数据集上计算的。</p> 
<p><span style="color:#0d0016;"><strong>VOC数据集</strong></span></p> 
<p style="margin-left:0;text-align:left;">这个数据集包括20个物体类别。为了计算VOC中的AP，我们遵循以下步骤：</p> 
<ol><li>对于每个类别，通过改变模型预测的置信度阈值，计算出精确-召回曲线</li><li>使用精度-召回曲线的内插11点抽样，计算每个类别的平均精度（AP）</li><li>通过取所有20个类别中的AP的平均值来计算最终的平均精度（AP）</li></ol> 
<p><span style="color:#0d0016;"><strong>COCO数据集</strong></span></p> 
<p style="margin-left:0;text-align:left;">这个数据集包括80个对象类别，并使用更复杂的方法来计算AP。它没有使用11点插值，而是使用101点插值，也就是说，它计算了从0到1的101个召回阈值的精度，增量为0.01。另外，AP是通过对多个IoU值进行平均而不是只对一个IoU值进行平均得到的，除了一个常见的AP指标，即<em>AP</em>50    ，它是单个IoU阈值为0.5的AP。COCO中计算AP的步骤如下：</p> 
<ol><li style="text-align:left;">对于每个类别，通过改变模型预测的置信度阈值，计算出精确-召回曲线</li><li style="text-align:left;">使用101-recall阈值计算每个类别的平均精度（AP）</li><li style="text-align:left;">在不同的交叉联合（IoU）阈值下计算AP，通常从0.5到0.95，步长为0.05。更高的IoU阈值需要更准确的预测才能被认为是真阳性</li><li style="text-align:left;">对于每个IoU阈值，取所有80个类别的AP的平均值</li><li style="text-align:left;">最后，通过平均每个IoU阈值计算的AP值来计算总体AP</li></ol> 
<p style="margin-left:0;text-align:justify;">AP计算的差异使得我们很难直接比较两个数据集的物体检测模型的性能。目前的标准使用COCO AP，因为它对一个模型在不同的IoU阈值下的表现有更精细的评估。</p> 
<p></p> 
<h4><span style="color:#0d0016;">3.3 <strong><strong><strong>非极大值抑制 </strong></strong><strong><strong>(NMS)</strong></strong></strong></span></h4> 
<p style="margin-left:0;text-align:justify;">非极大值抑制（NMS）是物体检测算法中使用的一种后处理技术，用于减少重叠边界盒的数量，提高整体检测质量。物体检测算法通常会在同一物体周围产生多个具有不同置信度分数的边界框。NMS过滤掉多余的和不相关的边界盒，只保留最准确的边界盒。算法<a href="#_bookmark2" rel="nofollow">1</a>描述了该程序。图<a href="#_bookmark3" rel="nofollow">3</a>显示了一个包含多个重叠边界框的物体检测模型的典型输出和NMS之后的输出。</p> 
<p><img alt="" height="708" src="https://images2.imgbox.com/55/28/6rjNS5XM_o.png" width="1200"></p> 
<p><img alt="" height="624" src="https://images2.imgbox.com/fb/f5/QS04gCWc_o.png" width="1200"></p> 
<h4></h4> 
<h3><span style="color:#0d0016;">4. <strong>YOLO: You Only Look Once</strong></span></h3> 
<p style="margin-left:0;text-align:left;">Joseph Redmon等人的YOLO发表在CVPR 2016[<a href="#_bookmark56" rel="nofollow">38]</a>。它首次提出了一种实时的<span style="color:#1c7892;"><strong>端到端</strong></span>物体检测方法。YOLO这个名字代表了 "你只看一次"，指的是它是与之前的方法相比，YOLO只需通过一次网络就能完成检测任务，而之前的方法要么使用滑动窗口，要么使用分类器，每个图像需要运行数百次或数千次，更先进的方法则将任务分为两步，第一步检测有物体或候选框的可能区域，第二步对候选框进行分类。另外，YOLO使用了一个更直接的输出，只基于回归来预测检测输出，而Fast R-CNN[<a href="#_bookmark57" rel="nofollow">39]</a>则使用了两个单独的输出，一个是概率的分类，一个是方框坐标的回归。</p> 
<p style="margin-left:0;text-align:left;"></p> 
<h4><span style="color:#0d0016;">4.1 <strong><strong><strong>YOLOv1</strong></strong><strong><strong>如何工作？</strong></strong></strong></span></h4> 
<p style="margin-left:0;text-align:left;">YOLOv1通过同时检测所有的边界框，统一了物体检测步骤。为了实现这一目标，YOLO将输入图像划每个边界框的预测由五个值组成：<em>Pc、bx、by、bh、bw</em> ，其中Pc是bounding box的置信度分数，反映了模型对bbox包含物体的置信度以及bbox的精确程度。<em>bx</em>和<em>by</em>坐标是方框相对于网格单元的中心，<em>bh</em>和<em>bw</em>是方框相对于整个图像的高度和宽度。YOLO的输出是一个<em>S</em>×<em>S</em>×(<em>B</em>×5+<em>C)</em>的张量，可以选择用非最大抑制（NMS） 来去除重复的检测结果。</p> 
<p style="margin-left:0;text-align:left;">在最初的YOLO论文中，作者使用了PASCAL  VOC数据集[<a href="#_bookmark54" rel="nofollow">36]</a>，该数据集包含20个（<em>C</em><em>  </em>= 20）；一个7×7（<em>S </em>= 7）网格最多预测两个类（<em>B</em><em> </em>= 2），输出7×7×30预测结果。</p> 
<p style="margin-left:0;text-align:left;"><img alt="" height="1072" src="https://images2.imgbox.com/10/dd/8mOwBaDZ_o.png" width="1200"></p> 
<p> YOLOv1在PASCAL VOC2007数据集上取得了63.4的平均精度（AP）。</p> 
<p style="margin-left:0;text-align:left;"></p> 
<h4 style="margin-left:0px;text-align:left;"><span style="color:#0d0016;">4.2 <strong><strong><strong>YOLOv1</strong></strong><strong><strong>网络架构</strong></strong></strong></span></h4> 
<p>YOLOv1架构包括24个卷积层，然后是两个全连接层，用于预测bbox坐标和概率。除了最后一个层使用线性激活函数外，所有层都使用了漏整流线性单元激活[<a href="#_bookmark58" rel="nofollow">40]</a>。受GoogLeNet[<a href="#_bookmark59" rel="nofollow">41]</a>和Network in Network[<a href="#_bookmark60" rel="nofollow">42] </a>的启发，YOLO使用1×1卷积层来减少特征图的数量并保持相对较低的参数数量。作为激活层，表<a href="#_bookmark5" rel="nofollow">1</a>描述了YOLOv1的架构。作者还介绍了一个更轻的模型，称为快速YOLO，由九个卷积层组成。</p> 
<p style="margin-left:0;text-align:left;"><img alt="" height="1174" src="https://images2.imgbox.com/1d/1d/YYix2peg_o.png" width="1200"></p> 
<p></p> 
<h4><span style="color:#0d0016;">4.3 <strong><strong><strong>YOLOv1</strong></strong><strong><strong>训练</strong></strong></strong></span></h4> 
<p style="margin-left:0;text-align:left;">作者使用ImageNet数据集[43]在224x224的分辨率下对YOLO的前20层进行了预训练。然后，他们用随机初始化的权重增加了最后四层，并在448x448的分辨率下用PASCAL VOC 2007和VOC 2012数据集[36]对模型进行了微调，以增加细节，实现更准确的物体检测。对于增强，作者使用了最多为输入图像大小20%的随机缩放和平移，以及HSV色彩空间中上端系数为1.5的随机曝光和饱和度。</p> 
<p style="margin-left:0;text-align:left;">YOLOv1使用了一个由多个和平方误差组成的损失函数，如图5所示。在该损失函数中、 λcoord = 5是一个比例因子，赋予边界框预测更多的重要性，而λnoobj = 0.5是一个比例因子，降低不包含物体的框的重要性。λnoobj = 0.5是一个比例因子，它降低了不包含物体的bbox的重要性。</p> 
<p><img alt="" height="1011" src="https://images2.imgbox.com/74/80/KZku2IJQ_o.png" width="1111"></p> 
<h4><span style="color:#0d0016;">4.4 <strong><strong><strong>YOLOv1优缺点</strong></strong></strong></span></h4> 
<p style="margin-left:0;text-align:left;">YOLO的简单结构，加上其新颖的全图像单次回归，使其比现有的物体检测器快得多，允许实时性能。然而，虽然YOLO的表现比任何物体检测器都快，但与最先进的方法如快速R-CNN<a href="#_bookmark57" rel="nofollow">[39]</a>相比，定位误差更大。造成这种限制的主要原因有三个：</p> 
<ol><li>它在网格单元中最多只能检测到两个相同类别的物体，限制了它预测附近物体的能力</li><li>它在预测训练数据中未见的长宽比物体时很吃力</li><li>由于下采样层，它只能从粗略的物体特征中学习</li></ol> 
<p></p> 
<h3><span style="color:#0d0016;">5. <strong><strong><strong>YOLOv2：更</strong></strong></strong>好、更快、更强</span></h3> 
<p>YOLOv2由Joseph Redmon和Ali Farhadi发表在CVPR 2017[<a href="#_bookmark62" rel="nofollow">44]</a>。它包括了对原始YOLO的一些改进，使其更好，保持相同的速度，也更强大，能够检测9000个类别！这些改进有以下几点：</p> 
<ol><li style="text-align:left;">在所有卷积层上的<strong>批量归一化</strong>改善了收敛性，并作为一个正则器来减少过拟合；</li><li style="text-align:left;"><strong>高分辨率分类器，</strong>和YOLOv1一样，他们在ImageNet 以224x224的分辨率对模型进行了预训练。然而，这一次，他们在分辨率为448 x 448的ImageNet上对模型进行了10次微调，提高了网络在高分辨率输入下的性能；</li><li style="text-align:justify;"><strong>完全卷积</strong>。他们去掉了密集层，采用了全卷积架构；</li><li style="text-align:justify;"><strong>使用Anchor来预测边界盒</strong>。他们使用一组先验框Anchor，这些Anchor具有预定义的形状，用于匹配物体的原型形状，如图<a href="#_bookmark7" rel="nofollow">6</a>所示<a href="#_bookmark7" rel="nofollow">。</a>每个网格单元都定义了多个Anchor，系统预测每个Anchor的坐标和类别。网络输出的大小与每个网格单元的Anchor数量成正比；</li><li style="text-align:justify;"><strong>维度聚类</strong>。挑选好的Anchor有助于网络学习预测更准确的边界盒。作者对训练中的边界盒进行了k-means聚类，以找到好的先验。他们选择了五个Anchor，在召回率和模型复杂性之间进行了良好的权衡；</li><li style="text-align:justify;"><strong>直接预测位置</strong>。与其他预测偏移量的方法不同[<a href="#_bookmark63" rel="nofollow">45]</a>，YOLOv2遵循同样的理念，预测了相对于网格单元的位置坐标。网络为每个单元预测了五个bounding box，每个bounding box有五个值<em>t</em><em>x</em><em> , t</em><em>y</em><em> , </em><em>t</em><em>w</em><em> , t</em><em>h</em><em> , t</em><em>o</em><em> </em>，其中<em>t</em><em>o</em><em> </em>相当于YOLOv1的<em>Pc</em>，最终得到的bounding box坐标如图<a href="#_bookmark8" rel="nofollow">7</a>所示；</li><li style="text-align:justify;"><strong>细粒度的特征</strong>。与YOLOv1相比，YOLOv2去掉了一个池化层，对于416×416的输入图像，得到13×13的特征图，结构细节见图表<a href="#_bookmark9" rel="nofollow">2</a>；</li><li style="text-align:justify;"><strong>多尺度训练</strong>。由于YOLOv2不使用全连接层，输入可以是不同的尺寸。为了使YOLOv2对不同的输入尺寸具有鲁棒性，作者随机训练模型，每十批改变输入尺寸（从320 × 320到608 × 608）。</li></ol> 
<p><img alt="" height="666" src="https://images2.imgbox.com/38/ae/nqniTa65_o.png" width="1200"></p> 
<p><img alt="" height="1038" src="https://images2.imgbox.com/4c/6e/lJP3x77R_o.png" width="1200"></p> 
<p>通过所有这些改进，YOLOv2在PASCAL VOC2007数据集上取得了78.6%的平均精度（AP），而YOLOv1则取得了63.4%。</p> 
<h4></h4> 
<h4><span style="color:#0d0016;">5.1 <strong><strong><strong>YOLOv2</strong></strong><strong><strong>架构</strong></strong></strong></span></h4> 
<p>YOLOv2使用的骨干架构被称为Darknet-19，包含19个卷积层和5个maxpooling层。与YOLOv1的架构类似，它受到Network in Network[42]的启发，在3×3之间使用1×1 3×3之间的卷积，以减少参数的数量。此外，如上所述，他们使用批量归一化来规范化并帮助收敛。</p> 
<p><img alt="" height="766" src="https://images2.imgbox.com/ad/64/OUhm5ePE_o.png" width="943"></p> 
<p></p> 
<h4><span style="color:#0d0016;">5.2 <strong>YOLO9000</strong><strong><strong>是一个更强大的</strong></strong><strong><strong>YOLOv2</strong></strong></span></h4> 
<p>作者在同一篇论文中介绍了一种训练联合分类和检测的方法。它使用来自COCO[<a href="#_bookmark55" rel="nofollow">37]</a>的检测标记数据来学习bounding box坐标，并使用来自ImageNet的分类数据来增加它能检测的类别数量。在训练过程中，他们将这两个数据集结合起来，这样当使用检测训练图像时，它反向传播检测网络，而当使用分类训练图像时，它反向传播架构的分类部分。结果是一个能够检测超过9000个类别的YOLO模型，因此被称为YOLO9000。 </p> 
<p></p> 
<h3><span style="color:#0d0016;">6. YOLOv3</span></h3> 
<p>YOLOv3[<a href="#_bookmark64" rel="nofollow">46]</a>于2018年由Joseph Redmon和Ali Farhadi发表在ArXiv。它包括重大的变化和更大的架构，以便在保持实时性能的同时与最先进的技术接轨。在下文中，我们描述了相对于YOLOv2的变化。</p> 
<div> 
 <ol><li style="text-align:justify;"><strong>bounding box预测。</strong>与YOLOv2一样，该网络为每个bounding box预测四个坐标tx、ty、tw和th；然而，这次YOLOv3使用逻辑回归为每个bounding box预测一个目标分数。这个分数与ground truth重合度最高的Anchor来说是1，对于其他Anchor来说是0。与Faster R-CNN[45]相比，YOLOv3只为每个ground truth分配一个Anchor。另外，如果没有为一个对象分配Anchor，它只会产生分类损失，而不会产生定位损失或置信度损失；</li><li style="text-align:justify;"><strong>类预测</strong>。他们没有使用softmax进行分类，而是使用二元交叉熵来训练独立的logistic分类器，并将问题作为多标签分类来提出。这种变化允许给同一个bounding box分配多个标签，这可能发生在一些标签重叠的复杂数据集上[<a href="#_bookmark65" rel="nofollow">47]</a>。例如，同一个物体可以是一个人和一个男人；</li><li style="text-align:justify;"><strong>新的骨干网络</strong>。YOLOv3的特点是一个更大的特征提取器，由53个卷积层组成，带有Res残余连接。第<a href="#_bookmark10" rel="nofollow">6.1</a>节更详细地描述了该架构；</li><li style="text-align:left;"><strong>空间金字塔池(SPP)。</strong> 虽然在论文中没有提到，但作者还在骨干中加入了一个改进的SPP块[48]，它连接了多个最大集合输出，而没有子采样（stride = 1），每个内核大小不同的k×k，其中k=1、5、9、13，允许更大的感受野。这个版本被称为称为YOLOv3-spp，是表现最好的版本，将AP50提高了2.7%；</li><li style="text-align:justify;"><strong>多尺度预测</strong>。与特征金字塔网络[<a href="#_bookmark67" rel="nofollow">49]</a>类似，YOLOv3在三个不同尺度上预测三个bounding box。第<a href="#_bookmark12" rel="nofollow">6.2</a>节描述了多尺度预测机制的更多细节；</li><li style="text-align:justify;"><strong>Bounding box先验</strong>。与YOLOv2一样， 作者也使用k-means来确定Anchor的bounding box预设。不同的是， 在YOLOv2中，他们每个单元共使用了五个先验盒，而在YOLOv3中，他们使用了三个不同尺度的先验盒。</li></ol> 
</div> 
<p></p> 
<h4><span style="color:#0d0016;">6.1 <strong><strong><strong>YOLOv3</strong></strong><strong><strong>架构</strong></strong></strong></span></h4> 
<p>YOLOv3中提出的架构主干被称为Darknet-53。它用全连接层取代了所有的max-pooling层，并增加了Res残差连接。总的来说，它包含53个卷积层。图<a href="#_bookmark11" rel="nofollow">8</a>显示了该架构的细节。</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/5f/17/1qp1o5eQ_o.png" width="1200"></p> 
<p style="margin-left:0;text-align:left;">Darknet-53骨干网获得了与ResNet-152相当的Top-1和Top-5的准确率，但速度几乎为2倍。</p> 
<p></p> 
<h4><span style="color:#0d0016;">6.2  <strong><strong><strong>YOLOv3</strong></strong><strong><strong>多尺度预测</strong></strong></strong></span></h4> 
<p>除了更大的结构，YOLOv3的一个基本特征是多尺度预测，即在多个网格尺寸下的预测。这有助于获得更精细的方框，并大大改善了对小物体的预测，而这正是YOLO以前版本的主要弱点之一。</p> 
<p>图9所示的多尺度检测架构的工作原理如下：第一个标记为y1的输出相当于YOLOv2的输出，其中一个13×13的网格定义了输出。第二个输出y2是由串联后的输出与（Res × 8）后的输出相连接。这些特征图有不同的尺寸，即13×13和26×26，所以在连接之前有一个上采样操作。最后，使用一个上采样操作，第三个输出y3将26×26的特征图与52×52的特征图连接起来。对于有80个类别的COCO数据集，每个尺度提供了一个形状为N×N×[3×(4+1+80)]的输出张量。其中N×N是特征图（或网格单元）的大小，3表示每个单元的方框，4+1包括四个坐标和置信度得分。4+1包括四个坐标和置信度得分。</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/ca/6c/Jo3UxF34_o.png" width="1200"></p> 
<p></p> 
<p><span style="color:#0d0016;"><strong>6.3 </strong> <strong><strong><strong>YOLOv3结果</strong></strong></strong></span></p> 
<p style="margin-left:0;text-align:justify;">当YOLOv3发布时，物体检测的基准已经从PASCAL VOC变成了Microsoft COCO[<a href="#_bookmark55" rel="nofollow">37]</a>。因此，从这里开始， 所有的YOLO都在MS COCO数据集中进行评估。YOLOv3-spp在20FPS的情况下，平均精度AP为36.2%， AP50 为60.6%，达到了当时的最先进水平，速度快了2倍。</p> 
<p></p> 
<h3><span style="color:#000000;"><strong>7. Backbone, Neck和Head</strong></span></h3> 
<p style="margin-left:0;text-align:left;">这时，物体检测器的结构开始被描述为三个部分：<span style="color:#000000;"><strong>Backbone, Neck和Head</strong></span>。图<a href="#_bookmark14" rel="nofollow">10</a>显示了一个高层次的<span style="color:#494949;">Backbone, Neck 和 Head图</span>。</p> 
<p style="margin-left:0;text-align:left;"><img alt="" height="694" src="https://images2.imgbox.com/15/98/CVQm9lVY_o.png" width="1200"></p> 
<p> <span style="color:#000000;"><strong>Backbone</strong></span>负责从输入图像中提取有用的特征。它通常是一个卷积神经网络（CNN），在大规模的图像分类任务中训练，如ImageNet。骨干网在不同的尺度上捕捉层次化的特征，在较早的层中提取低层次的特征（如边缘和纹理），在较深的层中提取高层次的特征（如物体部分和语义信息）。</p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#000000;"><strong>Neck</strong></span>是连接<span style="color:#494949;">Backbone和Head</span>的一个中间部件。它聚集并细化骨干网提取的特征，通常侧重于加强不同尺度的空间和语义信息。颈部可能包括额外的卷积层、特征金字塔网络（FPN）[<a href="#_bookmark67" rel="nofollow">49]</a>，或其他机制，以提高特征的代表性。</p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#000000;"><strong>Head</strong></span>是物体检测器的最后组成部分；它负责根据<span style="color:#494949;">Backbone和Neck</span>提供的特征进行预测。它通常由一个或多个特定任务的子网络组成，执行分类、定位，以及最近的实例分割和姿势估计。头部处理颈部提供的特征，为每个候选物体产生预测。最后，一个后处理步骤，如非极大值抑制（NMS），过滤掉重叠的预测，只保留置信度最高的检测。</p> 
<p style="margin-left:0;text-align:left;">在其余的YOLO模型中，我们将使用<span style="color:#494949;">Backbone, Neck和Head</span>来描述架构。</p> 
<p style="margin-left:0;text-align:left;"></p> 
<h3 style="margin-left:0px;text-align:left;"><span style="color:#0d0016;"><strong><strong><strong>8. YOLOv4</strong></strong></strong></span></h3> 
<div> 
 <p style="margin-left:0;text-align:left;">两年过去了，YOLO没有新版本。直到2020年4月，Alexey Bochkovskiy、Chien-Yao Wang和Hong-Yuan Mark Liao在ArXiv发布了YOLOv4[<a href="#_bookmark68" rel="nofollow">50]</a>的论文。起初，不同的作者提出一个新的YOLO "官方 "版本让人感觉很奇怪；然而，YOLOv4保持了相同的YOLO理念——实时、开源、端到端和DarkNet框架——而且改进非常令人满意，社区迅速接受了这个版本作为官方的YOLOv4。</p> 
</div> 
<p style="margin-left:0;text-align:justify;">YOLOv4试图通过试验许多被归类为<strong><span style="color:#000000;"><em>bag-of-freebies</em></span>和<span style="color:#000000;"><em>bag-of-specials</em></span></strong>的变化来找到最佳平衡。<strong><span style="color:#000000;"><em>bag-of-freebies</em></span></strong>是指只改变训练策略和增加训练成本但不增加推理时间的方法，最常见的是数据增强。另一方面，<strong><span style="color:#000000;"><em>bag-of-specials</em></span></strong>是指稍微增加推理成本但显著提高准确率的方法。这些方法的例子有扩大感受野[<a href="#_bookmark66" rel="nofollow">48, </a><a href="#_bookmark69" rel="nofollow">51, </a><a href="#_bookmark70" rel="nofollow">52]</a>，结合特征[<a href="#_bookmark71" rel="nofollow">53, </a><a href="#_bookmark67" rel="nofollow">49, </a><a href="#_bookmark72" rel="nofollow">54, </a><a href="#_bookmark73" rel="nofollow">55]</a>，以及后处理[<a href="#_bookmark74" rel="nofollow">56,</a> <a href="#_bookmark58" rel="nofollow">40,</a> <a href="#_bookmark75" rel="nofollow">57,</a> <a href="#_bookmark76" rel="nofollow">58]</a>等等。</p> 
<p style="margin-left:0;text-align:left;">我们将YOLOv4的主要变化总结为以下几点：</p> 
<ul><li style="text-align:justify;"><strong>一个带有<span style="color:#000000;"><em>bag-of-specials</em></span>集成的增强型架构</strong>。作者尝试了多种骨干架构， 如ResNeXt50[<a href="#_bookmark77" rel="nofollow">59] </a>、EfficientNet-B3[<a href="#_bookmark78" rel="nofollow">60]</a>和Darknet-53。表现最好的架构是对Darknet-53的修改，采用跨阶段部分连接（CSPNet）[<a href="#_bookmark79" rel="nofollow">61]</a>，以Mish激活函数[<a href="#_bookmark75" rel="nofollow">57]</a>作为骨干（见图<a href="#_bookmark15" rel="nofollow">11）。</a>对于颈部，他们使用了YOLOv3-spp中的修改版空间金字塔集合（SPP）[<a href="#_bookmark66" rel="nofollow">48]</a>和YOLOv3中的多尺度预测，但用修改版的路径聚合网络（PANet）[<a href="#_bookmark80" rel="nofollow">62]</a>代替FPN，以及修改的空间注意模块（SAM）[<a href="#_bookmark81" rel="nofollow">63]</a>。最后，对于检测头，他们使用YOLOv3中的锚。因此，该模型被称为<em>CSPDarknet53-PANet-SPP</em>。添加到Darknet-53中的跨阶段部分连接（CSP）有助于减少模型的计算量，同时保持相同的精度。与YOLOv3-spp中一样，SPP块在不影响推理速度的情况下增加了感受野。修改后的PANet版本将特征串联起来，而不是像原PANet论文中那样将其添加；</li><li style="text-align:justify;"><strong><span style="color:#000000;"><em>bag-of-freebies</em></span>的高级训练方法</strong>。除了常规的增强，如随机亮度、对比度、缩放、裁剪、翻转和旋转，作者还实现了马赛克增强，将四张图像合并为一张，允许检测其通常背景之外的物体，同时也减少了对大批量正常化的小批量的需求。对于正则化， 他们使用了DropBlock[<a href="#_bookmark82" rel="nofollow">64] </a>， 作为Dropout[<a href="#_bookmark83" rel="nofollow">65]</a>的替代品，但用于卷积神经网络以及类标签平滑[<a href="#_bookmark84" rel="nofollow">66, </a><a href="#_bookmark85" rel="nofollow">67]</a>。对于检测器，他们增加了CIoU 损失[<a href="#_bookmark86" rel="nofollow">68]</a>和Cross mini-bath normalization (CmBN)，用于收集整个批次的统计数据，而不是像常规批次归一化中的单个小批次<a href="#_bookmark87" rel="nofollow">[69]</a>；</li><li style="text-align:justify;"><strong>自我对抗性训练（</strong><strong>SAT</strong><strong>）</strong>。为了使模型对扰动更加稳健，对输入的图像进行对抗性攻击，以创造一个欺骗性，即gound truth不在图像中，但保持原始标签以检测正确的对象；</li><li style="text-align:justify;"><strong>用遗传算法进行超参数优化</strong>。为了找到用于训练的最佳超参数，他们在前10%的时期使用遗传算法，并使用余弦退火调度器[<a href="#_bookmark88" rel="nofollow">70]</a>来改变训练期间的学习率。它开始缓慢地降低学习率，然后在训练过程的一半时快速降低，最后略微降低。</li></ul> 
<p style="text-align:justify;"><img alt="" height="1200" src="https://images2.imgbox.com/3b/3c/PDAjb7VS_o.png" width="1200"></p> 
<p> 表<a href="#_bookmark16" rel="nofollow">3</a>列出了骨干网和检测器的最终选择的BoF和BoS。</p> 
<p style="margin-left:0;text-align:left;"><img alt="" height="1002" src="https://images2.imgbox.com/6b/be/PF1AEBi5_o.png" width="1200"></p> 
<p></p> 
<h4><span style="color:#0d0016;">8.1 <strong><strong><strong>YOLOv4</strong></strong><strong><strong>结果</strong></strong></strong></span></h4> 
<p style="margin-left:0;text-align:left;">在MS COCO数据集test-dev 2017上进行评估，YOLOv4在NVIDIA V100上实现了43.5%的AP和65.7%的AP50，速度超过了50 FPS。</p> 
<p></p> 
<h3><span style="color:#0d0016;"><strong>9. YOLOv5</strong></span></h3> 
<p style="margin-left:0;text-align:justify;">YOLOv5[<a href="#_bookmark90" rel="nofollow">72]</a>是在YOLOv4之后几个月于2020年由Glenn Jocher发布。在写这篇文章时，还没有关于YOLOv5的科学论文，但从代码中，我们知道它使用了YOLOv4部分描述的许多改进，主要区别是它是用Pytorch而不是Darknet开发的。YOLOv5是开源的，由Ultralytics积极维护，有250多个贡献者，并经常有新的改进。YOLOv5很容易使用、培训和部署。Ultralytics提供了一个iOS和Android的移动版本，以及许多用于标签、培训和部署的集成。</p> 
<p style="margin-left:0;text-align:justify;">YOLOv5 提供了五个版本： YOLOv5n（ 纳米级）、YOLOv5s（ 小型）、YOLOv5m（ 中型）、YOLOv5l（大型）和YOLOv5x（特大型）。</p> 
<p style="margin-left:0;text-align:left;">在撰写本文时，YOLOv5发布的版本是7.0版，包括能够进行分类和实例分割的YOLOv5版本。</p> 
<p style="margin-left:0;text-align:left;"></p> 
<h4 style="margin-left:0px;text-align:left;"><span style="color:#0d0016;">9.1 <strong><strong><strong>YOLOv5</strong></strong><strong><strong>结果</strong></strong></strong></span></h4> 
<p style="margin-left:0;text-align:justify;">在MS COCO数据集test-dev 2017上进行评估，YOLOv5x在图像大小为640像素的情况下实现了50.7%的AP。使用32 个批次的大小，它在NVIDIA V100 上可以达到200 FPS 的速度。使用更大的输入尺寸1536 像素， YOLOv5实现了55.8%的AP。</p> 
<p style="margin-left:0;text-align:left;"></p> 
<h3 style="margin-left:0px;text-align:left;"><span style="color:#0d0016;">10. Scaled-YOLOv4</span></h3> 
<p style="margin-left:0;text-align:justify;">在YOLOv4 的一年后， 同一作者在CVPR 2021 上展示了Scaled-YOLOv4 [<a href="#_bookmark91" rel="nofollow">73] </a>。与YOLOv4 不同， Scaled YOLOv4是在Pytorch而不是Darknet中开发的。主要的创新之处在于引入了扩大和缩小的技术。扩大规模意味着以降低速度为代价来制作一个增加精度的模型；另一方面，缩小规模需要制作一个增加速度而牺牲精度的模型。此外，按比例缩小的模型需要更少的计算能力，可以在嵌入式系统上运行。</p> 
<p style="margin-left:0;text-align:left;">缩小的架构被称为YOLOv4-tiny；它是为低端GPU设计的，在Jetson TX2上能以46 FPS运行，在RTX2080Ti上能以440 FPS运行，在MS COCO上达到22%的AP。</p> 
<p style="margin-left:0;text-align:justify;">扩大的模型架构被称为YOLOv4-large，其中包括三种不同尺寸的P5、P6和P7。这个架构是为云计算GPU设计的，取得了最先进的性能，超过了所有以前的模型<a href="#_bookmark92" rel="nofollow">[74, </a><a href="#_bookmark93" rel="nofollow">75,</a> <a href="#_bookmark94" rel="nofollow">76]</a>，在MS COCO上的AP为56%。</p> 
<p style="margin-left:0;text-align:left;"></p> 
<h3 style="margin-left:0px;text-align:left;"><span style="color:#0d0016;">11. YOLOR</span></h3> 
<p style="margin-left:0;text-align:justify;">YOLOR[<a href="#_bookmark95" rel="nofollow">77]</a>由YOLOv4的同一研究小组于2021年5月发表在ArXiv。它代表着你只学习一个表征。在这篇论文中，作者采用了一种不同的方法；他们开发了一种多任务学习方法，旨在通过学习一般的表征和使用子网络来创建特定任务的表征，为各种任务（如分类、检测、姿势估计）创建一个单一的模型。洞察到传统的联合学习方法经常导致次优特征的产生，YOLOR旨在通过编码神经网络的隐性知识来克服这一问题，以应用于多个任务，类似于人类使用过去的经验来处理新问题。结果显示，将隐性知识引入神经网络有利于所有的任务。</p> 
<p style="margin-left:0;text-align:left;">在MS COCO数据集test-dev 2017上进行评估，YOLOR在NVIDIA V100上以30 FPS的速度取得了55.4%的AP和73.3%的AP50。</p> 
<p style="margin-left:0;text-align:left;"></p> 
<h3 style="margin-left:0px;text-align:left;"><span style="color:#0d0016;">12. YOLOX</span></h3> 
<div> 
 <p style="margin-left:0;text-align:left;">YOLOX[<a href="#_bookmark96" rel="nofollow">78]</a>于2021年7月发表在ArXiv上，由Megvii Technology的研究团队开发。它以Pytorch为基础，以Ultralytics的YOLOV3为起点，有五个主要变化：无锚结构、多阳性、解耦头、高级标签分配和强增强。它取得了最先进的结果。</p> 
</div> 
<p style="margin-left:0;text-align:left;">2021年，在Tesla V100上以50.1%的AP和68.9%的FPS实现了速度和准确性的最佳平衡。在下文中，我们描述了YOLOX相对于YOLOv3的五个主要变化：</p> 
<ol><li style="text-align:left;"><strong>无锚（Anchor-free）</strong>。自YOLOv2以来，所有后续的YOLO版本都是基于锚点的检测器。YOLOX受到CornerNet[<a href="#_bookmark97" rel="nofollow">79]</a>、CenterNet[<a href="#_bookmark98" rel="nofollow">80]</a>和FCOS[<a href="#_bookmark99" rel="nofollow">81]</a>等最先进的无锚物体检测器的启发，回到了一个无锚结构，简化了训练和解码过程。与YOLOv3基线相比，无锚的AP增加了0.9；</li><li style="text-align:left;"><strong>多重正样本（Multi positives）。</strong>为了弥补因缺乏锚点而产生的巨大不平衡，作者使用了中心采样[81]，他们将中心3×3的区域作为正例区域。这种方法使AP增加了2.1点；</li><li style="text-align:justify;"><strong>解耦头（Decoupled head）</strong>。在[<a href="#_bookmark100" rel="nofollow">82, </a><a href="#_bookmark101" rel="nofollow">83]</a>中显示，分类置信度和定位精度之间可能存在错位。由于这个原因，YOLOX将这两者分离成两个头（如图<a href="#_bookmark17" rel="nofollow">12</a>所示<a href="#_bookmark17" rel="nofollow">），</a>一个用于分类任务，另一个用于回归任务，将AP提高了1.1 分，并加快了模型收敛。</li><li style="text-align:justify;"><strong>高级标签分配</strong>。在[<a href="#_bookmark102" rel="nofollow">84]</a>中，有研究表明，当多个对象的bounding box重叠时，ground truth标签分配可能存在模糊性，并将分配程序表述为最佳传输（OT）问题。YOLOX在这项工作的启发下，提出了一个简化的版本，称为simOTA。这一变化使AP增加了2.3分；</li><li style="text-align:justify;"><strong>强化增强</strong>。YOLOX使用MixUP[<a href="#_bookmark103" rel="nofollow">85]</a>和Mosaic增强。作者发现，在使用这些增强后，ImageNet预训练不再有好处。强势增强使AP增加了2.4分；</li></ol> 
<p style="margin-left:0;text-align:left;"><img alt="" height="774" src="https://images2.imgbox.com/83/ed/ejvC3FOM_o.png" width="1200"></p> 
<p></p> 
<h3><span style="color:#0d0016;">13. YOLOv6</span></h3> 
<div> 
 <p style="margin-left:0;text-align:justify;">YOLOv6[<a href="#_bookmark104" rel="nofollow">86]</a>于2022年9月由美团视觉人工智能部发布在ArXiv。与YOLOv4和YOLOv5类似，它为工业应用提供了各种不同尺寸的模型。跟随基于锚点的方法[<a href="#_bookmark96" rel="nofollow">78, </a><a href="#_bookmark99" rel="nofollow">81]</a>的趋势，YOLOv6采用了无锚点的检测器。该模型的主要创新之处总结如下：</p> 
 <ol><li style="text-align:justify;"><strong>一个基于</strong><strong>RepVGG</strong>[<a href="#_bookmark105" rel="nofollow">87]</a>的<strong>新骨架</strong>，称为EfficientRep，比以前的YOLO骨架使用更高的并行性。对于颈部， 他们使用PAN[<a href="#_bookmark80" rel="nofollow">62] </a>增强了RepBlocks[<a href="#_bookmark105" rel="nofollow">87] </a>或CSPStackRep[<a href="#_bookmark79" rel="nofollow">61]Blocks </a>， 用于大型模型。而受YOLOX的启发，他们开发了一个高效的解耦头；</li><li style="text-align:justify;">使用TOOD<a href="#_bookmark106" rel="nofollow">[88]</a>中介绍的任务排列学习方法进行<strong>标签分配；</strong></li><li style="text-align:justify;"><strong>新的分类和回归损失</strong>。他们使用了一个分类VariFocal损失[<a href="#_bookmark107" rel="nofollow">89</a>]和一个SIoU[<a href="#_bookmark108" rel="nofollow">90</a>]/GIoU回归损失；</li><li style="text-align:justify;"><strong>一个</strong>用于回归和分类任务的<strong>自我蒸馏</strong>策略；</li><li style="text-align:justify;">使用<a href="#_bookmark110" rel="nofollow">RepOptimizer[92]</a>和信道明智蒸馏<a href="#_bookmark111" rel="nofollow">[93]</a>的检测<strong>量化方案</strong>，有助于实现更快的检测器。</li></ol> 
 <p style="text-align:justify;"></p> 
 <h4 style="text-align:justify;"><span style="color:#0d0016;">13.1 <strong><strong><strong>YOLOv6</strong></strong><strong><strong>结果</strong></strong></strong></span></h4> 
</div> 
<p style="margin-left:0;text-align:left;">在MS COCO数据集test-dev 2017上进行评估，YOLOv6-L在NVIDIA Tesla T4上实现了52.5%的AP和70%的AP50 ，速度约为50 FPS。</p> 
<p style="margin-left:0;text-align:left;"></p> 
<h3 style="margin-left:0px;text-align:left;"><span style="color:#0d0016;">14. YOLOv7</span></h3> 
<p style="margin-left:0;text-align:left;">YOLOv7[<a href="#_bookmark112" rel="nofollow">94]</a>由YOLOv4和YOLOR的同一作者于2022年7月发表在ArXiv。当时，在5 FPS到160 FPS的范围内，它的速度和准确度超过了所有已知的物体检测器。与YOLOv4一样，它只使用MS COCO数据集进行训练，没有预训练的骨干。YOLOv7提出了一些架构上的变化和一系列的免费包，在不影响推理速度的情况下提高了准确率，只影响了训练时间。</p> 
<p style="margin-left:0;text-align:left;">YOLOv7的架构变化是：</p> 
<ul><li style="text-align:justify;"><strong>扩展高效层聚合网络（</strong><strong>E-ELAN</strong><strong>）</strong>。ELAN[<a href="#_bookmark113" rel="nofollow">95</a>]是一种策略，通过控制最短的最长梯度路径，让深度模型更有效地学习和收敛。YOLOv7提出的E-ELAN适用于具有无限叠加计算块的模型。E-ELAN通过洗牌和合并cardinality结合不同组的特征，在不破坏原始梯度路径的情况下增强网络的学习；</li><li style="text-align:justify;"><strong>基于串联的模型的模型缩放</strong>。缩放通过调整一些模型属性来生成不同大小的模型。YOLOv7的架构是一个基于串联的架构，其中标准的缩放技术，如深度缩放，导致过渡层的输入通道和输出通道之间的比例变化，这反过来又导致了模型的硬件使用量的减少。YOLOv7提出了一种新的基于串联模型的缩放策略，其中块的深度和宽度以相同的因素进行缩放，以保持模型的最佳结构。</li></ul> 
<p style="margin-left:0;text-align:left;">YOLOv7中使用的<strong><span style="color:#000000;"><em>bag-of-freebies</em></span></strong>包括：</p> 
<ul><li style="text-align:justify;"><strong>计划中的重新参数化卷积</strong>。和YOLOv6一样，YOLOv7的架构也是受到重新参数化卷积（RepConv） 的启发[<a href="#_bookmark105" rel="nofollow">87</a>]。然而，他们发现RepConv中的身份连接破坏了ResNet[<a href="#_bookmark71" rel="nofollow">53]</a>中的残差和DenseNet[<a href="#_bookmark114" rel="nofollow">96]</a>中的串联。出于这个原因，他们删除了身份连接，并称之为RepConvN；</li><li style="text-align:justify;"><strong>对辅助头进行粗略的标签分配，对主导头进行精细的标签分配</strong>。主导头负责最终输出，而辅助头则协助训练；</li><li style="text-align:justify;"><strong>conv-bn-activation</strong><strong>中的批量归一化</strong>。这将批量归一化的平均值和方差整合到推理阶段的卷积层的偏置和权重中；</li><li style="text-align:justify;">YOLOR<a href="#_bookmark95" rel="nofollow">[77]</a>中启发的<strong>隐性知识；</strong></li><li style="text-align:left;"><strong>指数移动平均线</strong>作为最终推断模型。</li></ul> 
<p style="margin-left:0;text-align:left;"></p> 
<h4 style="margin-left:0px;text-align:left;"><span style="color:#0d0016;">14.1 <strong><strong><strong>与</strong></strong><strong><strong>YOLOv4</strong></strong><strong><strong>和</strong></strong><strong><strong>YOLOR</strong></strong><strong><strong>的比较</strong></strong></strong></span></h4> 
<div> 
 <p style="margin-left:0;text-align:left;">在这一节中，我们强调了YOLOv7与同一作者以前开发的YOLO模型相比的改进之处。</p> 
 <p style="margin-left:0;text-align:left;">与YOLOv4相比，YOLOv7实现了参数减少75%，计算量减少36%，同时平均精度（AP）提高了1.5%。与YOLOv4-tiny相比，YOLOv7-tiny设法将参数和计算量分别减少39%和49%，同时保持相同的AP。最后，与YOLOR相比，YOLOv7的参数数量和计算量分别减少了43%和15%，同时AP也略微增加了0.4%。</p> 
</div> 
<p></p> 
<h4><span style="color:#0d0016;">14.2 <strong><strong><strong>YOLOv7</strong></strong><strong><strong>结果</strong></strong></strong></span></h4> 
<p style="margin-left:0;text-align:left;">在MS COCO数据集test-dev 2017上评估，YOLOv7-E6在输入尺寸为1280像素的情况下，在NVIDIA V100上取得了55.9%的AP和73.5%的AP50 ，速度为50 FPS。</p> 
<p style="margin-left:0;text-align:left;"></p> 
<h3 style="text-align:left;"><span style="color:#0d0016;"><strong><strong><strong>15. DAMO-YOLO</strong></strong></strong></span></h3> 
<p style="margin-left:0;text-align:left;">DAMO-YOLO[<a href="#_bookmark115" rel="nofollow">97</a>]由阿里巴巴集团于2022年11月发表在ArXiv。受到当前技术的启发，DAMO-YOLO包括以下内容：</p> 
<ol><li style="text-align:justify;"><strong>一个神经架构搜索</strong><strong>（</strong><strong>NAS</strong><strong>）</strong>。他们使用了阿里巴巴开发的一种叫做MAE-NAS[<a href="#_bookmark116" rel="nofollow">98]</a>的方法来自动寻找一个有效的架构；</li><li style="text-align:justify;"><strong>一个大的颈部</strong>。受GiraffeDet[<a href="#_bookmark117" rel="nofollow">99]</a>、CSPNet[<a href="#_bookmark79" rel="nofollow">61]</a>和ELAN[<a href="#_bookmark113" rel="nofollow">95]</a>的启发，作者设计了一个可以实时工作的脖子，称为Efficient-RepGFPN；</li><li style="text-align:justify;"><strong>一个小头</strong>。作者发现，大头和小头能产生更好的性能，他们只留下一个线性层用于分类，一个用于回归。他们把这种方法称为ZeroHead；</li><li style="text-align:justify;"><strong>AlignedOTA</strong><strong>标签分配</strong>。动态标签分配方法，如OTA[<a href="#_bookmark102" rel="nofollow">84]</a>和TOOD[<a href="#_bookmark106" rel="nofollow">88]</a>，由于比静态方法有明显的改进，已经得到普及。然而，分类和回归之间的错位仍然是一个问题，部分原因是分类和回归损失之间的不平衡。为了解决这个问题，他们的AlignOTA方法在分类成本中引入了焦点损失[<a href="#_bookmark93" rel="nofollow">75]</a>，并使用预测和ground truth的IoU作为软标签，使每个目标都能选择对齐的样本，并从全局角度解决这个问题；</li><li style="text-align:justify;"><strong>知识的提炼</strong>。他们提出的策略包括两个阶段：教师在第一阶段指导学生，学生在第二阶段独立进行微调。此外，他们在蒸馏方法中加入了两项增强功能：对齐模块，它将学生的特征调整为与教师的特征相同的分辨率，以及通道动态温度，它将教师和学生的特征归一化，以减少实际价值差异的影响；</li></ol> 
<p style="margin-left:0;text-align:left;">作者生成了名为DAMO-YOLO-Tiny/Small/Medium的比例模型，在NVIDIA V100上，最佳模型在233 FPS下的AP达到了50.0%。</p> 
<p style="margin-left:0;text-align:left;"></p> 
<h3 style="margin-left:0px;text-align:left;"><span style="color:#0d0016;">16. YOLOv8</span></h3> 
<p style="margin-left:0;text-align:justify;">YOLOv8<a href="#_bookmark118" rel="nofollow">[100]</a>由开发YOLOv5的公司Ultralytics于2023年1月发布。由于在撰写本文时，还没有关于YOLOv8的论文，我们需要深入了解与其他YOLO版本相比的架构决策。按照目前的趋势，YOLOv8是无锚的，减少了箱体预测的数量，加快了非极大值抑制（NMS）的速度。此外，YOLOv8在训练过程中使用了马赛克增强功能；但是，由于已经发现如果在整个训练过程中使用这种增强功能可能是有害的，所以在最后10个epoch中禁用了这种功能。</p> 
<p style="margin-left:0;text-align:left;">YOLOv8可以从命令行界面（CLI）运行，也可以作为一个PIP包安装。此外，它还配备了多个用于贴标、培训和部署的集成。</p> 
<p style="margin-left:0;text-align:left;">YOLOv8 提供了五个版本： YOLOv8n（ 纳米级）、YOLOv8s（ 小型）、YOLOv8m（ 中型）、YOLOv8l（大型）和YOLOv8x（特大型）。</p> 
<p style="margin-left:0;text-align:left;"></p> 
<h4><span style="color:#0d0016;">16.1 <strong><strong><strong>YOLOv8</strong></strong><strong><strong>结果</strong></strong></strong></span></h4> 
<p>在MS COCO数据集test-dev 2017上进行评估，YOLOv8x在图像大小为640像素的情况下实现了53.9%的AP（ 相比之下，YOLOv5在相同的输入大小上为50.7%），在NVIDIA A100和TensorRT上的速度为280 FPS。</p> 
<p></p> 
<h3 style="text-align:left;"><span style="color:#0d0016;"><strong><strong><strong>17. PP-YOLO</strong></strong><strong><strong>、</strong></strong><strong><strong>PP-YOLOv2</strong></strong><strong><strong>和</strong></strong><strong><strong>PP-YOLOE</strong></strong></strong></span></h3> 
<p>PP-YOLO模型一直在与我们描述的YOLO模型平行发展。然而，我们决定将它们归为一个部分，因为它们从YOLOv3开始，一直在逐步改进以前的PP-YOLO版本。尽管如此，这些模型在YOLO的发展过程中还是很有影响力的。与YOLOv4和YOLOv5相似的PP-YOLO[<a href="#_bookmark94" rel="nofollow">76</a>]是基于YOLOv3的。它于2020年7月由百度公司的研究人员发表在ArXiv上。该作者使用了PaddlePaddle[<a href="#_bookmark119" rel="nofollow">101]</a>深度学习平台，因此它的<em>PP</em>名称。遵循我们从YOLOv4开始看到的趋势，PP-YOLO增加了十个现有的技巧来提高检测器的准确性，保持速度不变。根据作者的说法，本文的目的不是要介绍一个新的物体检测器，而是要展示如何一步一步地建立一个更好的检测器。PP-YOLO使用的大部分技巧都与YOLOv4中使用的技巧不同，重合的技巧使用了不同的实现。PP-YOLO关于YOLOv3的变化是：</p> 
<ol><li style="text-align:left;"><strong>ResNet50-vd</strong><strong>骨干网</strong>取代了DarkNet-53骨干网，其架构在最后阶段增加了可去形成的卷积[<a href="#_bookmark120" rel="nofollow">102</a>]，并提炼出预训练模型，在ImageNet上具有更高的分类精度。这个架构被称为ResNet5-vd-dcn；</li><li style="text-align:left;"><strong>更大的批次规模</strong>以提高训练的稳定性，他们从64个到192个，同时更新了训练计划和学习率；</li><li style="text-align:left;">对训练过的参数<strong>保持移动平均数</strong>，并使用它们来代替最终的训练值；</li><li style="text-align:left;"><strong>DropBlock</strong>只适用于FPN；</li><li style="text-align:left;">在另一个分支中增加了<strong>一个</strong><strong>IoU</strong><strong>损失</strong>，以及用于边界盒回归的L1损失；</li><li style="text-align:left;">增加了一个<strong>IoU</strong><strong>预测分支</strong>，以衡量定位精度和IoU感知损失。在推理过程中，YOLOv3乘以分类概率和客观性分数来计算最终的检测结果，PP-YOLO也乘以预测的IoU来考虑定位精度；</li><li style="text-align:left;">类似于YOLOv4的<strong>网格敏感方法</strong>被用来改善网格边界的包围盒中心预测；</li><li style="text-align:left;"><strong>矩阵</strong><strong>NMS</strong><a href="#_bookmark121" rel="nofollow">[103]</a>被使用，它可以并行运行，使得它比传统的NMS更快；</li><li style="text-align:left;"><strong>CoordConv</strong>[<a href="#_bookmark122" rel="nofollow">104</a>]用于FPN的1×1卷积，并用于检测头的第一个卷积层。CoordConv允许网络学；</li><li style="text-align:left;"><strong>空间金字塔集合</strong>法只用于顶部特征图，以增加骨干的感受野。</li></ol> 
<p style="text-align:left;"></p> 
<h4><span style="color:#0d0016;">17.1 <strong><strong><strong>PP-YOLO</strong></strong><strong><strong>的增量和预处理</strong></strong></strong></span></h4> 
<p style="margin-left:0;text-align:left;">PP-YOLO使用了以下增强和预处理：</p> 
<ol><li>混合训练<a href="#_bookmark103" rel="nofollow">[85]</a>，权重从<em>Beta</em>(<em>α,</em><em> </em><em>β</em>)分布中取样，其中<em>α</em><em> </em>= 1<em>.</em>5，<em>β</em><em> </em>= 1<em>.</em>5；</li><li>随机色彩失真；</li><li style="text-align:left;">随机展开；</li><li style="text-align:left;">随机裁剪和随机翻转，概率为0.5；</li><li style="text-align:left;">RGB通道的z-score规范化，其平均值为[0<em>.</em>485<em>, </em>0<em>.</em>456<em>, </em>0<em>.</em>406]，标准偏差为 
  <div> 
   <span style="color:#000000;">[0</span> 
   <span style="color:#000000;"><em>.</em></span> 
   <span style="color:#000000;">229</span> 
   <span style="color:#000000;"><em>, </em></span> 
   <span style="color:#000000;">0</span> 
   <span style="color:#000000;"><em>.</em></span> 
   <span style="color:#000000;">224</span> 
   <span style="color:#000000;"><em>, </em></span> 
   <span style="color:#000000;">0</span> 
   <span style="color:#000000;"><em>.</em></span> 
   <span style="color:#000000;">225]；</span> 
  </div> </li><li style="text-align:left;"> 
  <div>
    从[320, 352, 384, 416, 448, 480, 512, 544, 576, 608]中均匀地抽取多种图像尺寸。 
  </div> </li></ol> 
<div></div> 
<p style="margin-left:0;text-align:left;"></p> 
<h4><span style="color:#0d0016;">17.2 <strong><strong><strong>PP-YOLO</strong></strong><strong><strong>结果</strong></strong></strong></span></h4> 
<p style="margin-left:0;text-align:left;">在MS COCO数据集test-dev 2017上进行评估，PP-YOLO在NVIDIA V100上取得了45.9%的AP和65.2%的AP50，73 FPS。</p> 
<div> 
 <p></p> 
 <h4><span style="color:#0d0016;">17.3 <strong><strong><strong>PP-YOLOv2</strong></strong></strong></span></h4> 
 <p style="margin-left:0;text-align:left;">PP-YOLOv2[<a href="#_bookmark123" rel="nofollow">105]</a>于2021年4月发表在ArXiv上，对PP-YOLO增加了四项改进，在NVIDIA V100上69FPS时，性能从45.9%AP提高到49.5%AP。PP-YOLOv2关于PP-YOLO的变化如下：</p> 
</div> 
<ol><li><strong><strong><strong>骨干网从</strong></strong><strong><strong>ResNet50</strong></strong><strong><strong>改为</strong></strong><strong><strong>ResNet101；</strong></strong></strong></li><li><strong>路径聚合网络（</strong><strong>PAN</strong><strong>）</strong>而不是类似于YOLOv4的FPN；</li><li><strong>Mish激活函数</strong>。与YOLOv4和YOLOv5不同，他们只在检测颈部应用mish激活函数，以保持骨干的ReLU不变；</li><li><strong>较大的输入尺寸</strong>有助于提高小物体的性能。他们将最大的输入尺寸从608扩大到768，并将每个GPU 的批量大小从24张图像减少到12张。输入尺寸从[320, 352, 384, 416, 448, 480, 512, 544, 576, 608, 640, 672, 704, 736, 768]中均匀抽取；</li><li><strong>一个修改过的</strong><strong>IoU</strong><strong>意识到的分支</strong>。他们修改了使用软标签格式而不是软权重格式的IoU意识到的损失计算方法。</li></ol> 
<p style="margin-left:0;text-align:left;"></p> 
<h4 style="margin-left:0px;text-align:left;"><span style="color:#0d0016;"><strong><strong><strong>17.4 PP-YOLOE</strong></strong></strong></span></h4> 
<p style="margin-left:0;text-align:left;">PP-YOLOE[<a href="#_bookmark124" rel="nofollow">106]</a>于2022年3月在ArXiv发表。它在PP-YOLOv2的基础上增加了改进，在NVIDIA V100上实现了51.4%的AP，78.1 FPS的性能。PP-YOLOE关于PP-YOLOv2的主要变化是：</p> 
<ol><li><strong>无锚</strong>。在[<a href="#_bookmark99" rel="nofollow">81,</a> <a href="#_bookmark98" rel="nofollow">80,</a> <a href="#_bookmark97" rel="nofollow">79,</a> <a href="#_bookmark96" rel="nofollow">78]</a>的工作推动下，PP-YOLOE使用了一个无锚的架构；</li><li><strong>高效任务排列头（</strong><strong>ET-head</strong><strong>）</strong>。与YOLOX的分类头和位置头解耦不同，PP-YOLOE反而使用了基于TOOD的单一头，以提高速度和准确性；</li><li><strong>任务对准学习（</strong><strong>TAL</strong><strong>）</strong>。YOLOX是第一个提出任务错位问题的人，在这种情况下，分类置信度和定位准确度并不一致。为了减少这个问题，PP-YOLOE实施了TOOD[<a href="#_bookmark106" rel="nofollow">88]</a>中提出的TAL，其中包括动态标签分配与任务对齐损失相结合；</li><li><strong>新的骨干和颈部</strong>。受TreeNet[<a href="#_bookmark125" rel="nofollow">107]</a>的启发，作者用RepResBlocks修改了骨干和颈部的结构，结合了剩余和密集连接；</li><li><strong>Varifocal</strong><strong>（</strong><strong>VFL</strong><strong>）和</strong><strong>Distribution</strong><strong> </strong><strong>focal</strong><strong> </strong><strong>loss</strong><strong>（</strong><strong>DFL</strong><strong>）</strong>。VFL[<a href="#_bookmark107" rel="nofollow">89]</a>使用目标分数对阳性样本的损失进行加权，对那些具有高IoU的样本给予更高的权重。这在训练过程中优先考虑了高质量的样本。同样，两者都使用IoU意识到的分类分数（IACS）作为目标，允许分类和定位质量的联合学习，导致训练和推理之间的一致性。另一方面，DFL[<a href="#_bookmark126" rel="nofollow">108]</a>将Focal Loss从离散标签扩展到连续标签，使结合质量估计和类别预测的改进表征成功优化。这使得真实数据中的灵活分布得到了准确的描述，消除了不一致的风险。</li></ol> 
<p style="margin-left:0;text-align:left;">和以前的YOLO版本一样，作者通过改变背脊和颈部的宽度和深度，生成了多个比例的模型。这些模型被称为PP-YOLOE-s（小型）、PP-YOLOE-m（中型）、PP-YOLOE-l（大型）和PP-YOLOE-x（特大型）。</p> 
<p style="margin-left:0;text-align:left;"></p> 
<h3 style="margin-left:0px;text-align:left;"><span style="color:#0d0016;">18. 讨论</span></h3> 
<div> 
 <p style="margin-left:0;text-align:left;">本文研究了15个YOLO版本，从最初的YOLO模型到最新的YOLOv8。表<a href="#_bookmark18" rel="nofollow">4</a>提供了所讨论的YOLO版本的概况从这个表中，我们可以确定几个关键的模式：</p> 
 <ul><li style="text-align:justify;"><strong>锚Anchor</strong>：最初的YOLO模型相对简单，没有采用锚点，而最先进的模型则依赖于带有锚点的两阶段检测器。YOLOv2采用了锚点，从而提高了边界盒的预测精度。这种趋势持续了五年，直到YOLOX引入了一个无锚的方法，取得了最先进的结果。从那时起，随后的YOLO版本已经放弃了锚的使用；</li><li style="text-align:justify;"><strong>框架</strong>：最初，YOLO是使用Darknet框架开发的，后续版本也是如此。然而，当Ultralytics将YOLOv3 移植到PyTorch时，其余的YOLO版本都是使用PyTorch开发的，导致了增强功能的激增。另一个利用的深度学习语言是PaddlePaddle，一个最初由百度开发的开源框架；</li><li style="text-align:justify;"><strong>骨干Backbone</strong>：YOLO模型的骨干架构随着时间的推移发生了重大变化。从由简单的卷积层和最大集合层组成的Darknet架构开始，后来的模型在YOLOv4中加入了跨阶段部分连接（CSP），在YOLOv6和YOLOv7中加入了重新参数化，并在DAMO-YOLO中加入了神经架构搜索；</li><li style="text-align:justify;"><strong>性能</strong>：虽然YOLO模型的性能随着时间的推移有所提高，但值得注意的是，它们往往优先考虑平衡速度和准确性，而不是只关注准确性。这种权衡是YOLO框架的一个重要方面，允许在各种应用中进行实时物体检测。</li></ul> 
</div> 
<p><img alt="" height="680" src="https://images2.imgbox.com/f4/d0/puetD0Tu_o.png" width="1200"></p> 
<p></p> 
<h4><span style="color:#0d0016;">18.1 <strong><strong><strong>在速度和准确性之间进行权衡</strong></strong></strong></span></h4> 
<p style="margin-left:0;text-align:justify;">YOLO系列的物体检测模型一直专注于平衡速度和精度，旨在提供实时性能而不牺牲检测结果的质量。随着YOLO框架在各种迭代中的发展，这种权衡一直是一个反复出现的主题，每个版本都试图以不同的方式优化这些相互竞争的目标。在最初的YOLO模型中，主要重点是实现高速物体检测。该模型利用单一的卷积神经网络（CNN）直接预测输入图像中的物体位置和类别，实现实时处理。然而，这种对速度的强调导致了准确性的妥协，主要是在处理小物体或具有重叠边界盒的物体时。</p> 
<p style="margin-left:0;text-align:justify;">随后的YOLO版本在保持框架的实时性的同时，引入了完善和增强功能来解决这些限制。例如，YOLOv2（ YOLO9000）引入了锚定框和穿透层，以改善物体的定位，从而提高精确度。此外，YOLOv3通过采用多尺度特征提取架构增强了模型的性能，允许在不同尺度上进行更好的物体检测。</p> 
<p style="margin-left:0;text-align:justify;">随着YOLO框架的发展，速度和准确性之间的权衡变得更加微妙。YOLOv4和YOLOv5等模型引入了创新，如新的网络主干、改进的数据增强技术和优化的训练策略。这些发展导致了准确度的显著提高，但并没有大幅影响模型的实时性能。</p> 
<p style="margin-left:0;text-align:justify;">从Scaled YOLOv4开始，所有官方的YOLO模型都对速度和精度之间的权衡进行了微调，提供不同的模型比例以适应特定的应用和硬件要求。例如，这些版本通常提供为边缘设备优化的轻量级模型，用精度换取降低的计算复杂性和更快的处理时间。</p> 
<p></p> 
<h3><span style="color:#0d0016;">19 <strong><strong><strong>YOLO</strong></strong><strong><strong>的未来</strong></strong></strong></span></h3> 
<div> 
 <p style="margin-left:0;text-align:left;">随着YOLO框架的不断发展，我们预计以下趋势和可能性将决定未来的发展：</p> 
 <p style="margin-left:0;text-align:left;"><strong>纳入最新技术</strong>。研究人员和开发人员将继续利用深度学习、数据增强和训练技术的最先进方法来完善YOLO架构。这种持续的创新过程可能会提高模型的性能、稳健性和效率。</p> 
</div> 
<p style="margin-left:0;text-align:justify;"><strong>基准的演变</strong>。目前用于评估物体探测模型的基准，即COCO 2017，最终可能会被一个更先进、更具挑战性的基准所取代。这反映了前两个YOLO版本中使用的VOC 2007基准的转变，反映了随着模型越来越复杂和准确，需要更多的基准。</p> 
<p style="margin-left:0;text-align:justify;"><strong>YOLO</strong><strong>模型和应用的激增</strong>。随着YOLO框架的发展，我们预计每年发布的YOLO模型的数量会增加，同时应用也会相应地扩大。随着该框架变得更加通用和强大，它可能会被应用于更多不同的领域，从家用电器设备到自动驾驶汽车。</p> 
<p style="margin-left:0;text-align:justify;"><strong>扩展到新的领域</strong>。YOLO模型有可能将其能力扩展到物体检测和分割之外，分支到视频中的物体跟踪和三维关键点估计等领域。随着这些模型的发展，它们可能成为解决更广泛的计算机视觉任务的新解决方案的基础。</p> 
<p style="margin-left:0;text-align:justify;"><strong>对不同硬件的适应性</strong>。YOLO模型将进一步跨越硬件平台，从物联网设备到高性能计算集群。这种适应性将使YOLO模型能够在各种情况下部署，这取决于应用程序的要求和限制。此外，通过定制模型以适应不同的硬件规格，YOLO可以被更多的用户和行业所接受和使用。</p> 
<p></p> 
<p><strong>Reference：</strong></p> 
<p>《<span style="color:#000000;">A COMPREHENSIVE </span><span style="color:#000000;">R</span><span style="color:#000000;">EVIEW OF </span><span style="color:#000000;">YOLO: F</span><span style="color:#000000;">ROM </span><span style="color:#000000;">YOLO</span><span style="color:#000000;">V</span><span style="color:#000000;">1 </span><span style="color:#000000;">TO YOLOV</span><span style="color:#000000;">8 </span><span style="color:#000000;">AND </span><span style="color:#000000;">B</span><span style="color:#000000;">EYOND </span>》</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/17c528770a4e0415f8a55d074d4d183e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">HTML引入css文件（四种方法）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6e900e680a2f22ee97f94a0592cf9456/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Java 使用multipartFile对象解析Execl</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>