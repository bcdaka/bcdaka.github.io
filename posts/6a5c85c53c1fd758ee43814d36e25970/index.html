<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>大模型笔记2 Longformer for Extractive Summarization任务的模型修改 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/6a5c85c53c1fd758ee43814d36e25970/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="大模型笔记2 Longformer for Extractive Summarization任务的模型修改">
  <meta property="og:description" content="目录
LongformerForTokenClassification调通
将7分类的预训练模型改为2分类
利用分类标签取出token对应子词
将token转换为完整单词取出
LongformerForTokenClassification调通 对应文档:
https://huggingface.co/docs/transformers/en/model_doc/longformer#transformers.LongformerForTokenClassification
下载预训练模型:
https://huggingface.co/docs/transformers/en/model_doc/longformer#transformers.LongformerForTokenClassification
修改使用模型预测与训练时的输出获取
from transformers import AutoTokenizer, LongformerForTokenClassification
import torch
# tokenizer = AutoTokenizer.from_pretrained(&#34;brad1141/Longformer-finetuned-norm&#34;)
# model = LongformerForTokenClassification.from_pretrained(&#34;brad1141/Longformer-finetuned-norm&#34;)
tokenizer = AutoTokenizer.from_pretrained(&#34;tmp/Longformer-finetuned-norm&#34;)
model = LongformerForTokenClassification.from_pretrained(&#34;tmp/Longformer-finetuned-norm&#34;)
inputs = tokenizer(
&#34;HuggingFace is a company based in Paris and New York&#34;, add_special_tokens=False, return_tensors=&#34;pt&#34;
)
#预测
with torch.no_grad():
outputs=model(**inputs)
# 如果输出是元组，可以手动解析
if isinstance(outputs, tuple):
logits, = outputs
else:
logits = outputs.logits
predicted_token_class_ids = logits.argmax(-1)
# Note that tokens are classified rather then input words which means that">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-09T09:49:46+08:00">
    <meta property="article:modified_time" content="2024-07-09T09:49:46+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大模型笔记2 Longformer for Extractive Summarization任务的模型修改</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="LongformerForTokenClassification%E8%B0%83%E9%80%9A-toc" style="margin-left:40px;"><a href="#LongformerForTokenClassification%E8%B0%83%E9%80%9A" rel="nofollow">LongformerForTokenClassification调通</a></p> 
<p id="%E5%B0%867%E5%88%86%E7%B1%BB%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%94%B9%E4%B8%BA2%E5%88%86%E7%B1%BB-toc" style="margin-left:40px;"><a href="#%E5%B0%867%E5%88%86%E7%B1%BB%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%94%B9%E4%B8%BA2%E5%88%86%E7%B1%BB" rel="nofollow">将7分类的预训练模型改为2分类</a></p> 
<p id="%E5%88%A9%E7%94%A8%E5%88%86%E7%B1%BB%E6%A0%87%E7%AD%BE%E5%8F%96%E5%87%BAtoken%E5%AF%B9%E5%BA%94%E5%AD%90%E8%AF%8D-toc" style="margin-left:40px;"><a href="#%E5%88%A9%E7%94%A8%E5%88%86%E7%B1%BB%E6%A0%87%E7%AD%BE%E5%8F%96%E5%87%BAtoken%E5%AF%B9%E5%BA%94%E5%AD%90%E8%AF%8D" rel="nofollow">利用分类标签取出token对应子词</a></p> 
<p id="%E5%B0%86token%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%AE%8C%E6%95%B4%E5%8D%95%E8%AF%8D%E5%8F%96%E5%87%BA-toc" style="margin-left:40px;"><a href="#%E5%B0%86token%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%AE%8C%E6%95%B4%E5%8D%95%E8%AF%8D%E5%8F%96%E5%87%BA" rel="nofollow">将token转换为完整单词取出</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h3 style="margin-left:0px;text-align:justify;">LongformerForTokenClassification调通</h3> 
<p style="margin-left:0;text-align:justify;">对应文档:</p> 
<p style="margin-left:0;text-align:justify;"><a href="https://huggingface.co/docs/transformers/en/model_doc/longformer#transformers.LongformerForTokenClassification" rel="nofollow" title="https://huggingface.co/docs/transformers/en/model_doc/longformer#transformers.LongformerForTokenClassification">https://huggingface.co/docs/transformers/en/model_doc/longformer#transformers.LongformerForTokenClassification</a></p> 
<p style="margin-left:0;text-align:justify;">下载预训练模型:</p> 
<p style="margin-left:0;text-align:justify;"><a href="https://huggingface.co/docs/transformers/en/model_doc/longformer#transformers.LongformerForTokenClassification" rel="nofollow" title="https://huggingface.co/docs/transformers/en/model_doc/longformer#transformers.LongformerForTokenClassification">https://huggingface.co/docs/transformers/en/model_doc/longformer#transformers.LongformerForTokenClassification</a></p> 
<p style="margin-left:0;text-align:justify;">修改使用模型预测与训练时的输出获取</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">from transformers import AutoTokenizer, LongformerForTokenClassification</p> <p style="margin-left:0;text-align:justify;">import torch</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;"># tokenizer = AutoTokenizer.from_pretrained("brad1141/Longformer-finetuned-norm")</p> <p style="margin-left:0;text-align:justify;"># model = LongformerForTokenClassification.from_pretrained("brad1141/Longformer-finetuned-norm")</p> <p style="margin-left:0;text-align:justify;">tokenizer = AutoTokenizer.from_pretrained("tmp/Longformer-finetuned-norm")</p> <p style="margin-left:0;text-align:justify;">model = LongformerForTokenClassification.from_pretrained("tmp/Longformer-finetuned-norm")</p> <p style="margin-left:0;text-align:justify;">inputs = tokenizer(</p> <p style="margin-left:0;text-align:justify;">    "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="pt"</p> <p style="margin-left:0;text-align:justify;">)</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">#预测</p> <p style="margin-left:0;text-align:justify;">with torch.no_grad():</p> <p style="margin-left:0;text-align:justify;">    outputs=model(**inputs)</p> <p style="margin-left:0;text-align:justify;">    # 如果输出是元组，可以手动解析</p> <p style="margin-left:0;text-align:justify;">    if isinstance(outputs, tuple):</p> <p style="margin-left:0;text-align:justify;">        logits, = outputs</p> <p style="margin-left:0;text-align:justify;">    else:</p> <p style="margin-left:0;text-align:justify;">        logits = outputs.logits</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">predicted_token_class_ids = logits.argmax(-1)</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;"># Note that tokens are classified rather then input words which means that</p> <p style="margin-left:0;text-align:justify;"># there might be more predicted token classes than words.</p> <p style="margin-left:0;text-align:justify;"># Multiple token classes might account for the same word</p> <p style="margin-left:0;text-align:justify;">predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]</p> <p style="margin-left:0;text-align:justify;">predicted_tokens_classes</p> <p style="margin-left:0;text-align:justify;">print("predicted_tokens_classes:", predicted_tokens_classes)</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;"># 训练</p> <p style="margin-left:0;text-align:justify;">labels = predicted_token_class_ids</p> <p style="margin-left:0;text-align:justify;"># loss = model(**inputs, labels=labels).loss</p> <p style="margin-left:0;text-align:justify;">outputs = model(**inputs, labels=labels)</p> <p style="margin-left:0;text-align:justify;">if isinstance(outputs, tuple):</p> <p style="margin-left:0;text-align:justify;">    loss,logits = outputs</p> <p style="margin-left:0;text-align:justify;">else:</p> <p style="margin-left:0;text-align:justify;">    loss = outputs.loss</p> <p style="margin-left:0;text-align:justify;">round(loss.item(), 2)</p> <p style="margin-left:0;text-align:justify;">print("loss:", round(loss.item(), 2))</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;">目前输出是NER任务的针对每一个token分类:</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">predicted_tokens_classes ['Evidence', 'Evidence', 'Evidence', 'Evidence', 'Evidence', 'Evidence', 'Evidence', 'Evidence', 'Evidence', 'Evidence', 'Evidence', 'Evidence']</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;">Debug很重要的一步是看模型输出的各个维度什么意思, 这个可以从源文件和文档找,</p> 
<p style="margin-left:0;text-align:justify;">此处longformer</p> 
<p style="margin-left:0;text-align:justify;"><strong>logits</strong> (torch.FloatTensor of shape (batch_size, sequence_length, config.num_labels)) — Classification scores (before SoftMax).</p> 
<h3 id="%E5%B0%867%E5%88%86%E7%B1%BB%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%94%B9%E4%B8%BA2%E5%88%86%E7%B1%BB" style="margin-left:0;text-align:justify;">将7分类的预训练模型改为2分类</h3> 
<p style="margin-left:0;text-align:justify;">例子中的logits是[1, 12, 7], 其中sequence_length是句子中所有token的数量. config.num_labels 由config文件的id2label计算:</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">  "id2label": {<!-- --></p> <p style="margin-left:0;text-align:justify;">    "0": "Lead",</p> <p style="margin-left:0;text-align:justify;">    "1": "Position",</p> <p style="margin-left:0;text-align:justify;">    "2": "Evidence",</p> <p style="margin-left:0;text-align:justify;">    "3": "Claim",</p> <p style="margin-left:0;text-align:justify;">    "4": "Concluding Statement",</p> <p style="margin-left:0;text-align:justify;">    "5": "Counterclaim",</p> <p style="margin-left:0;text-align:justify;">    "6": "Rebuttal"</p> <p style="margin-left:0;text-align:justify;">  },</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;">此处将config原件保存副本, 然后修改类别为2个</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">"id2label": {<!-- --></p> <p style="margin-left:0;text-align:justify;">    "0": "Non-dataset description",</p> <p style="margin-left:0;text-align:justify;">    "1": "Dataset description"</p> <p style="margin-left:0;text-align:justify;">  },</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;">为了将 Longformer 的输出从 7 分类修改为 2 分类，需要调整模型的分类层（classifier layer）：</p> 
<p style="margin-left:0;text-align:justify;">加载预训练的 LongformerForTokenClassification 模型。</p> 
<p style="margin-left:0;text-align:justify;">修改模型的分类层。</p> 
<p style="margin-left:0;text-align:justify;">重新初始化模型的分类层。</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;"># 修改分类层为2分类</p> <p style="margin-left:0;text-align:justify;">model.num_labels = 2</p> <p style="margin-left:0;text-align:justify;">model.classifier = nn.Linear(model.config.hidden_size, model.num_labels)</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;"># 初始化分类层权重</p> <p style="margin-left:0;text-align:justify;">model.classifier.weight.data.normal_(mean=0.0, std=model.config.initializer_range)</p> <p style="margin-left:0;text-align:justify;">if model.classifier.bias is not None:</p> <p style="margin-left:0;text-align:justify;">    model.classifier.bias.data.zero_()</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;">报错:</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">Some weights of LongformerForTokenClassification were not initialized from the model checkpoint at tmp/Longformer-finetuned-norm and are newly initialized: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']</p> <p style="margin-left:0;text-align:justify;">You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</p> <p style="margin-left:0;text-align:justify;">Traceback (most recent call last):</p> <p style="margin-left:0;text-align:justify;">  File "d:/Projects/longformer/tests/try_tkn_clsfy.py", line 7, in &lt;module&gt;</p> <p style="margin-left:0;text-align:justify;">    model = LongformerForTokenClassification.from_pretrained("tmp/Longformer-finetuned-norm")</p> <p style="margin-left:0;text-align:justify;">  File "D:\Users\laugo\anaconda3\envs\longformer\lib\site-packages\transformers\modeling_utils.py", line 972, in from_pretrained</p> <p style="margin-left:0;text-align:justify;">    model.__class__.__name__, "\n\t".join(error_msgs)</p> <p style="margin-left:0;text-align:justify;">RuntimeError: Error(s) in loading state_dict for LongformerForTokenClassification:</p> <p style="margin-left:0;text-align:justify;">        size mismatch for classifier.weight: copying a param with shape torch.Size([7, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).</p> <p style="margin-left:0;text-align:justify;">        size mismatch for classifier.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([2]).</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;">还没有运行到修改分类层就报错了,</p> 
<p style="margin-left:0;text-align:justify;">在加载模型LongformerForTokenClassification.from_pretrained这一步报错. 因为其中需要读取config.num_labels, 此时config.num_labels是2, 与它不匹配</p> 
<p style="margin-left:0;text-align:justify;">Config中的Id2label加载时候先不改后面再代码中再改</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">model.config.id2label = {0: 'Non-dataset description', 1: 'Dataset description'}</p> <p style="margin-left:0;text-align:justify;">model.config.label2id = {'Non-dataset description': 0, 'Dataset description': 1}</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;">还有一个警告:</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">Some weights of LongformerForTokenClassification were not initialized from the model checkpoint at tmp/Longformer-finetuned-norm and are newly initialized: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;">手动初始化权重</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">model.longformer.pooler.dense.weight.data.normal_(mean=0.0, std=model.config.initializer_range)</p> <p style="margin-left:0;text-align:justify;">model.longformer.pooler.dense.bias.data.zero_()</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;">得到输出:</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">predicted_tokens_classes ['Dataset description', 'Non-dataset description', 'Non-dataset description', 'Non-dataset description', 'Non-dataset description', 'Non-dataset description', 'Non-dataset description', 'Non-dataset description', 'Non-dataset description', 'Non-dataset description', 'Non-dataset description', 'Non-dataset description']</p> </td></tr></tbody></table> 
<h3 id="%E5%88%A9%E7%94%A8%E5%88%86%E7%B1%BB%E6%A0%87%E7%AD%BE%E5%8F%96%E5%87%BAtoken%E5%AF%B9%E5%BA%94%E5%AD%90%E8%AF%8D" style="margin-left:0;text-align:justify;">利用分类标签取出token对应子词</h3> 
<p style="margin-left:0;text-align:justify;">现在将分类为1, 即predicted_tokens_classes 为'Dataset description'的取出.</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">for k, j in enumerate(predicted_tokens_classes):# j is label, k is index</p> <p style="margin-left:0;text-align:justify;">    if (len(predicted_tokens_classes)&gt;1):</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">        if (j=='Dataset description') &amp; (k==0):</p> <p style="margin-left:0;text-align:justify;">            # print("j:",j,";k:",k)</p> <p style="margin-left:0;text-align:justify;">            #if it's the first word in the first position</p> <p style="margin-left:0;text-align:justify;">            #print('At begin first word')</p> <p style="margin-left:0;text-align:justify;">            begin = tokenized_sub_sentence[k]</p> <p style="margin-left:0;text-align:justify;">            kword = begin</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">        elif (j=='Dataset description') &amp; (k&gt;=1) &amp; (predicted_tokens_classes[k-1]=='Non-dataset description'):</p> <p style="margin-left:0;text-align:justify;">            #begin word is in the middle of the sentence</p> <p style="margin-left:0;text-align:justify;">            begin = tokenized_sub_sentence[k]</p> <p style="margin-left:0;text-align:justify;">            previous = tokenized_sub_sentence[k-1]</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">            if begin.startswith('Ġ'):</p> <p style="margin-left:0;text-align:justify;">                kword = previous + begin[1:]</p> <p style="margin-left:0;text-align:justify;">            else:</p> <p style="margin-left:0;text-align:justify;">                kword = begin</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">            if k == (len(predicted_tokens_classes) - 1):</p> <p style="margin-left:0;text-align:justify;">                #print('begin and end word is the last word of the sentence')</p> <p style="margin-left:0;text-align:justify;">                kword_list.append(kword.rstrip().lstrip())</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">        elif (j=='Dataset description') &amp; (k&gt;=1) &amp; (predicted_tokens_classes[k-1]=='Dataset description'):</p> <p style="margin-left:0;text-align:justify;">            # intermediate word of the same keyword</p> <p style="margin-left:0;text-align:justify;">            inter = tokenized_sub_sentence[k]</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">            if inter.startswith('Ġ'):</p> <p style="margin-left:0;text-align:justify;">                kword = kword + "" + inter[1:]</p> <p style="margin-left:0;text-align:justify;">            else:</p> <p style="margin-left:0;text-align:justify;">                kword = kword + " " + inter</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">            if k == (len(predicted_tokens_classes) - 1):</p> <p style="margin-left:0;text-align:justify;">                #print('begin and end')</p> <p style="margin-left:0;text-align:justify;">                kword_list.append(kword.rstrip().lstrip())</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">        elif (j=='Non-dataset description') &amp; (k&gt;=1) &amp; (predicted_tokens_classes[k-1] =='Dataset description'):</p> <p style="margin-left:0;text-align:justify;">            # End of a keywords but not end of sentence.</p> <p style="margin-left:0;text-align:justify;">            kword_list.append(kword.rstrip().lstrip())</p> <p style="margin-left:0;text-align:justify;">            kword = ''</p> <p style="margin-left:0;text-align:justify;">            inter = ''</p> <p style="margin-left:0;text-align:justify;">    else:</p> <p style="margin-left:0;text-align:justify;">        if (j=='Dataset description'):</p> <p style="margin-left:0;text-align:justify;">            begin = tokenized_sub_sentence[k]</p> <p style="margin-left:0;text-align:justify;">            kword = begin</p> <p style="margin-left:0;text-align:justify;">            kword_list.append(kword.rstrip().lstrip())</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;">输出结果</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;"><u>Hug ging Face</u> is a <u>company</u> based in Paris and New York</p> <p style="margin-left:0;text-align:justify;">tokenized_sub_sentence: ['ĠHug', 'ging', 'Face', 'Ġis', 'Ġa', 'Ġcompany', 'Ġbased', 'Ġin', 'ĠParis', 'Ġand', 'ĠNew', 'ĠYork']</p> <p style="margin-left:0;text-align:justify;">kword_list shape: 2</p> <p style="margin-left:0;text-align:justify;"> ['ĠHug ging Face', 'Ġcompany']</p> <p style="margin-left:0;text-align:justify;">kword_text:</p> <p style="margin-left:0;text-align:justify;"> &lt;unk&gt; company</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;"><u>Hug ging Face</u> is a <u>company</u> based in Paris and New York</p> 
<p style="margin-left:0;text-align:justify;">可以看出是<u>Hug ging Face</u>由于中间空格没有去除, token转id识别不出来</p> 
<p style="margin-left:0;text-align:justify;">因此注释了输出中添加空格的代码</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">            # else:</p> <p style="margin-left:0;text-align:justify;">                # kword = kword + " " + inter</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;">现在可以正常输出, 但是对于一个单词包含多个token的情况, 它识别出其中部分token导致输出(kword_text)不是完整单词</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">tokenized_sub_sentence: ['ĠHug', 'ging', 'Face', 'Ġis', 'Ġa', 'Ġcompany', 'Ġbased', 'Ġin', 'ĠParis', 'Ġand', 'ĠNew', 'ĠYork']</p> <p style="margin-left:0;text-align:justify;"> ['gingFace', 'Ġcompany']</p> <p style="margin-left:0;text-align:justify;">kword_text:</p> <p style="margin-left:0;text-align:justify;"> gingFace company</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;">在 BPE 中，子词 token 通常以 ## 开头，表示这是前一个 token 的一部分</p> 
<p style="margin-left:0;text-align:justify;">但这里用的是另一个字符Ġ</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">from transformers import AutoTokenizer, LongformerForTokenClassification</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;">现在需要处理包含多个token的单词, 将包含token分类为1的单词不重复地输出</p> 
<h3 id="%E5%B0%86token%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%AE%8C%E6%95%B4%E5%8D%95%E8%AF%8D%E5%8F%96%E5%87%BA" style="background-color:transparent;margin-left:0px;text-align:justify;">将token转换为完整单词取出</h3> 
<p style="margin-left:0;text-align:justify;">但是当后面的token在列表中, 前面的不在, 只输出了后面一半的token</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">kword_list: ['ging', 'Face', 'Ġis', 'Ġa', 'Ġcompany', 'Ġbased', 'Ġin', 'ĠParis', 'Ġand', 'ĠNew', 'ĠYork']</p> <p style="margin-left:0;text-align:justify;">kword_text:</p> <p style="margin-left:0;text-align:justify;"> gingFace is a company based in Paris and New York</p> <p style="margin-left:0;text-align:justify;">unique_kword_list:</p> <p style="margin-left:0;text-align:justify;"> ['ging', 'Face', 'Ġis', 'Ġa', 'Ġcompany', 'Ġbased', 'Ġin', 'ĠParis', 'Ġand', 'ĠNew', 'ĠYork']</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;">合并单词</p> 
<p style="margin-left:0;text-align:justify;">目前得到的token为tokenized_sub_sentence,</p> 
<p style="margin-left:0;text-align:justify;">predicted_tokens_classes是针对每一个token是否符合要求的分类,</p> 
<p style="margin-left:0;text-align:justify;">当单词中包含'Dataset description'类的token, 将该单词取出.</p> 
<p style="margin-left:0;text-align:justify;">使用 'Ġ' 来检测新单词的开始，并将这些子词正确地连接在一起。这样可以避免不同单词被错误地连接在一起。</p> 
<p style="margin-left:0;text-align:justify;">这个似乎是成功了</p> 
<p style="margin-left:0;text-align:justify;">  使用 'Ġ' 检测新单词的开始。</p> 
<p style="margin-left:0;text-align:justify;">  拼接属于同一个单词的 token。</p> 
<p style="margin-left:0;text-align:justify;">  如果一个单词中的任何一个 token 被预测为 'Dataset description'，则将整个单词加入到 dataset_description_words 列表中。</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">dataset_description_words = []</p> <p style="margin-left:0;text-align:justify;">current_word = ""</p> <p style="margin-left:0;text-align:justify;">current_word_pred = False</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">for token, pred_class in zip(tokenized_sub_sentence, predicted_tokens_classes):</p> <p style="margin-left:0;text-align:justify;">    if token.startswith("Ġ"):</p> <p style="margin-left:0;text-align:justify;">        if (len(current_word)!=0) &amp; current_word_pred:#前面有上一个单词, 且其中有描述token, 则把它存入句子</p> <p style="margin-left:0;text-align:justify;">            dataset_description_words.append(current_word)</p> <p style="margin-left:0;text-align:justify;">        current_word = token[1:]</p> <p style="margin-left:0;text-align:justify;">        current_word_pred = (pred_class == 'Dataset description')</p> <p style="margin-left:0;text-align:justify;">        # print("start: ",current_word)</p> <p style="margin-left:0;text-align:justify;">        # print("dataset_description_words: ",dataset_description_words)</p> <p style="margin-left:0;text-align:justify;">        # print("current_word_pred: ",current_word_pred)</p> <p style="margin-left:0;text-align:justify;">    else:</p> <p style="margin-left:0;text-align:justify;">        current_word += token</p> <p style="margin-left:0;text-align:justify;">        current_word_pred = current_word_pred or (pred_class == 'Dataset description')#如果不是词开头, 现在token和之前已有token只要有1类的都行</p> <p style="margin-left:0;text-align:justify;">        # print("mid: ",current_word)</p> <p style="margin-left:0;text-align:justify;">        # print("current_word_pred: ",current_word_pred)</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">#最后一个单词后没有下一个单词的开始符号, 无法进入循环, 单独判断</p> <p style="margin-left:0;text-align:justify;">if (len(current_word)!=0) &amp; current_word_pred:</p> <p style="margin-left:0;text-align:justify;">    dataset_description_words.append(current_word)</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;">拼接所有'Dataset description' 类 token 的单词为一个完整的字符串</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">final_dataset_description_string = " ".join(dataset_description_words)</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;">示例分类:</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">tokenized_sub_sentence: ['ĠHug', 'ging', 'Face', 'Ġis', 'Ġa', 'Ġcompany', 'Ġbased', 'Ġin', 'ĠParis', 'Ġand', 'ĠNew', 'ĠYork']</p> <p style="margin-left:0;text-align:justify;">predicted_tokens_classes=['Non-dataset description', 'Dataset description', 'Non-dataset description', 'Dataset description', 'Non-dataset description', 'Dataset description', 'Dataset description', 'Dataset description', 'Dataset description', 'Dataset description', 'Dataset description', 'Dataset description']</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;">输出结果:</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">predicted_tokens_classes ['Dataset description', 'Dataset description', 'Dataset description', 'Dataset description', 'Non-dataset description', 'Non-dataset description', 'Non-dataset description', 'Dataset description', 'Dataset description', 'Dataset description', 'Dataset description', 'Dataset description']</p> <p style="margin-left:0;text-align:justify;">tokenized_sub_sentence: ['ĠHug', 'ging', 'Face', 'Ġis', 'Ġa', 'Ġcompany', 'Ġbased', 'Ġin', 'ĠParis', 'Ġand', 'ĠNew', 'ĠYork']</p> <p style="margin-left:0;text-align:justify;">dataset_description_string: HuggingFace is in Paris and New York</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;">运行过程:</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">start:  Hug</p> <p style="margin-left:0;text-align:justify;">dataset_description_words:  []</p> <p style="margin-left:0;text-align:justify;">current_word_pred:  False</p> <p style="margin-left:0;text-align:justify;">mid:  Hugging</p> <p style="margin-left:0;text-align:justify;">current_word_pred:  True</p> <p style="margin-left:0;text-align:justify;">mid:  HuggingFace</p> <p style="margin-left:0;text-align:justify;">current_word_pred:  True</p> <p style="margin-left:0;text-align:justify;">start:  is</p> <p style="margin-left:0;text-align:justify;">dataset_description_words:  ['HuggingFace']</p> <p style="margin-left:0;text-align:justify;">current_word_pred:  True</p> <p style="margin-left:0;text-align:justify;">start:  a</p> <p style="margin-left:0;text-align:justify;">dataset_description_words:  ['HuggingFace', 'is']</p> <p style="margin-left:0;text-align:justify;">current_word_pred:  False</p> <p style="margin-left:0;text-align:justify;">start:  company</p> <p style="margin-left:0;text-align:justify;">dataset_description_words:  ['HuggingFace', 'is']</p> <p style="margin-left:0;text-align:justify;">current_word_pred:  True</p> <p style="margin-left:0;text-align:justify;">start:  based</p> <p style="margin-left:0;text-align:justify;">dataset_description_words:  ['HuggingFace', 'is', 'company']</p> <p style="margin-left:0;text-align:justify;">current_word_pred:  True</p> <p style="margin-left:0;text-align:justify;">start:  in</p> <p style="margin-left:0;text-align:justify;">dataset_description_words:  ['HuggingFace', 'is', 'company', 'based']</p> <p style="margin-left:0;text-align:justify;">current_word_pred:  True</p> <p style="margin-left:0;text-align:justify;">start:  Paris</p> <p style="margin-left:0;text-align:justify;">dataset_description_words:  ['HuggingFace', 'is', 'company', 'based', 'in']</p> <p style="margin-left:0;text-align:justify;">current_word_pred:  True</p> <p style="margin-left:0;text-align:justify;">start:  and</p> <p style="margin-left:0;text-align:justify;">dataset_description_words:  ['HuggingFace', 'is', 'company', 'based', 'in', 'Paris']</p> <p style="margin-left:0;text-align:justify;">current_word_pred:  True</p> <p style="margin-left:0;text-align:justify;">start:  New</p> <p style="margin-left:0;text-align:justify;">dataset_description_words:  ['HuggingFace', 'is', 'company', 'based', 'in', 'Paris', 'and']</p> <p style="margin-left:0;text-align:justify;">current_word_pred:  True</p> <p style="margin-left:0;text-align:justify;">start:  York</p> <p style="margin-left:0;text-align:justify;">dataset_description_words:  ['HuggingFace', 'is', 'company', 'based', 'in', 'Paris', 'and', 'New']</p> <p style="margin-left:0;text-align:justify;">current_word_pred:  True</p> <p style="margin-left:0;text-align:justify;">unfiltered_dataset_description_string: HuggingFace is company based in Paris and New York</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;">完整代码:</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:414.8pt;"> <p style="margin-left:0;text-align:justify;">from transformers import AutoTokenizer, LongformerForTokenClassification</p> <p style="margin-left:0;text-align:justify;"># from transformers import Trainer, TrainingArguments</p> <p style="margin-left:0;text-align:justify;">import torch</p> <p style="margin-left:0;text-align:justify;">import torch.nn as nn</p> <p style="margin-left:0;text-align:justify;">tokenizer = AutoTokenizer.from_pretrained("tmp/Longformer-finetuned-norm")</p> <p style="margin-left:0;text-align:justify;">model = LongformerForTokenClassification.from_pretrained("tmp/Longformer-finetuned-norm")</p> <p style="margin-left:0;text-align:justify;"># print("set num_labels begin")</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;"># 修改分类层为2分类</p> <p style="margin-left:0;text-align:justify;">model.num_labels = 2</p> <p style="margin-left:0;text-align:justify;">model.config.num_labels = 2</p> <p style="margin-left:0;text-align:justify;">model.classifier = nn.Linear(model.config.hidden_size, model.num_labels)</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;"># 手动初始化权重</p> <p style="margin-left:0;text-align:justify;">model.longformer.pooler.dense.weight.data.normal_(mean=0.0, std=model.config.initializer_range)</p> <p style="margin-left:0;text-align:justify;">model.longformer.pooler.dense.bias.data.zero_()</p> <p style="margin-left:0;text-align:justify;"># 初始化分类层权重</p> <p style="margin-left:0;text-align:justify;">model.classifier.weight.data.normal_(mean=0.0, std=model.config.initializer_range)</p> <p style="margin-left:0;text-align:justify;">if model.classifier.bias is not None:</p> <p style="margin-left:0;text-align:justify;">    model.classifier.bias.data.zero_()</p> <p style="margin-left:0;text-align:justify;">   </p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;"># print("set weight zero")</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;"># 更新 id2label 和 label2id</p> <p style="margin-left:0;text-align:justify;">model.config.id2label = {0: 'Non-dataset description', 1: 'Dataset description'}</p> <p style="margin-left:0;text-align:justify;">model.config.label2id = {'Non-dataset description': 0, 'Dataset description': 1}</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">sentence="HuggingFace is a company based in Paris and New York."</p> <p style="margin-left:0;text-align:justify;">inputs = tokenizer(</p> <p style="margin-left:0;text-align:justify;">    sentence, add_special_tokens=False, return_tensors="pt"</p> <p style="margin-left:0;text-align:justify;">)</p> <p style="margin-left:0;text-align:justify;"># print("inputs id:",inputs["input_ids"])#id无法判断token是不是同一个词, 所以不能使用</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">#预测</p> <p style="margin-left:0;text-align:justify;">with torch.no_grad():</p> <p style="margin-left:0;text-align:justify;">    outputs=model(**inputs)</p> <p style="margin-left:0;text-align:justify;">    # 如果输出是元组，可以手动解析</p> <p style="margin-left:0;text-align:justify;">    if isinstance(outputs, tuple):</p> <p style="margin-left:0;text-align:justify;">        logits, = outputs</p> <p style="margin-left:0;text-align:justify;">    else:</p> <p style="margin-left:0;text-align:justify;">        logits = outputs.logits</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">predicted_token_class_ids = logits.argmax(-1)</p> <p style="margin-left:0;text-align:justify;"># Note that tokens are classified rather then input words which means that</p> <p style="margin-left:0;text-align:justify;"># there might be more predicted token classes than words.</p> <p style="margin-left:0;text-align:justify;"># Multiple token classes might account for the same word</p> <p style="margin-left:0;text-align:justify;">predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]</p> <p style="margin-left:0;text-align:justify;">print("predicted_tokens_classes",predicted_tokens_classes)</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">#token类别转化为词输出</p> <p style="margin-left:0;text-align:justify;">tokenized_sub_sentence = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])</p> <p style="margin-left:0;text-align:justify;">print("tokenized_sub_sentence:", tokenized_sub_sentence)</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;"># 示例分类</p> <p style="margin-left:0;text-align:justify;"># predicted_tokens_classes=['Non-dataset description', 'Dataset description', 'Non-dataset description', 'Dataset description', 'Non-dataset description', 'Dataset description', 'Dataset description', 'Dataset description', 'Dataset description', 'Dataset description', 'Dataset description', 'Dataset description']</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;"># 将预测类别为 'Dataset description' 的 token 所在的单词取出</p> <p style="margin-left:0;text-align:justify;">dataset_description_words = []</p> <p style="margin-left:0;text-align:justify;">current_word = ""</p> <p style="margin-left:0;text-align:justify;">current_word_pred = False</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">for token, pred_class in zip(tokenized_sub_sentence, predicted_tokens_classes):</p> <p style="margin-left:0;text-align:justify;">    if token.startswith("Ġ"):</p> <p style="margin-left:0;text-align:justify;">        if (len(current_word)!=0) &amp; current_word_pred:#前面有上一个单词, 且其中有描述token, 则把它存入句子</p> <p style="margin-left:0;text-align:justify;">            dataset_description_words.append(current_word)</p> <p style="margin-left:0;text-align:justify;">        current_word = token[1:]</p> <p style="margin-left:0;text-align:justify;">        current_word_pred = (pred_class == 'Dataset description')</p> <p style="margin-left:0;text-align:justify;">        # print("start: ",current_word)</p> <p style="margin-left:0;text-align:justify;">        # print("dataset_description_words: ",dataset_description_words)</p> <p style="margin-left:0;text-align:justify;">        # print("current_word_pred: ",current_word_pred)</p> <p style="margin-left:0;text-align:justify;">    else:</p> <p style="margin-left:0;text-align:justify;">        current_word += token</p> <p style="margin-left:0;text-align:justify;">        current_word_pred = current_word_pred or (pred_class == 'Dataset description')#如果不是词开头, 现在token和之前已有token只要有1类的都行</p> <p style="margin-left:0;text-align:justify;">        # print("mid: ",current_word)</p> <p style="margin-left:0;text-align:justify;">        # print("current_word_pred: ",current_word_pred)</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;">#最后一个单词后没有下一个单词的开始符号, 无法进入循环, 单独判断</p> <p style="margin-left:0;text-align:justify;">if (len(current_word)!=0) &amp; current_word_pred:</p> <p style="margin-left:0;text-align:justify;">    dataset_description_words.append(current_word)</p> <p style="margin-left:0;text-align:justify;"></p> <p style="margin-left:0;text-align:justify;"># 拼接所有包含 'Dataset description' 类 token 的单词为一个完整的字符串</p> <p style="margin-left:0;text-align:justify;">dataset_description_string = " ".join(dataset_description_words)</p> <p style="margin-left:0;text-align:justify;">print("dataset_description_string:", dataset_description_string)</p> <p style="margin-left:0;text-align:justify;">##############################################################</p> <p style="margin-left:0;text-align:justify;"># 训练</p> <p style="margin-left:0;text-align:justify;">labels = predicted_token_class_ids</p> <p style="margin-left:0;text-align:justify;"># loss = model(**inputs, labels=labels).loss</p> <p style="margin-left:0;text-align:justify;">outputs = model(**inputs, labels=labels)</p> <p style="margin-left:0;text-align:justify;">if isinstance(outputs, tuple):</p> <p style="margin-left:0;text-align:justify;">    loss,logits = outputs</p> <p style="margin-left:0;text-align:justify;">else:</p> <p style="margin-left:0;text-align:justify;">    loss = outputs.loss</p> <p style="margin-left:0;text-align:justify;">loss_value=round(loss.item(), 2)</p> <p style="margin-left:0;text-align:justify;">print("loss_value",loss_value)</p> </td></tr></tbody></table>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f858baed8e066f28edaa6fbb200f274e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Idea打包出war包</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c06d05f86c8b04021a6d7340990379bf/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">国产可图大模型:厚积薄发,GLM3加持质的飞跃,ComfyUI最全指南与SD3综合评比孰更强？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>