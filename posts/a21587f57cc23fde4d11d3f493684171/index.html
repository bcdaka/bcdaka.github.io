<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>集成学习概述 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/a21587f57cc23fde4d11d3f493684171/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="集成学习概述">
  <meta property="og:description" content="概述 集成学习(Ensemble learning)就是将多个机器学习模型组合起来，共同工作以达到优化算法的目的。具体来讲，集成学习可以通过多个学习器相结合，来获得比单一学习器更优越的泛化性能。集成学习的一般步骤为：1.生产一组“个体学习器(individual learner)”；2.用某种策略将他们结合起来。
个体学习器通常由一个现有的学习算法从训练数据产生。在同质集成（系统中个体学习器的类型相同）中，个体学习器又被称为“基学习器”，而在异质集成（系统中个体学习器的类型不同）中，个体学习器又被称为“组建学习(component learner)”。
集成学习的集成框架主要有：Bagging，Boosting和Stacking，其中Bagging和Boosting为同质集成，而Stacking为异质集成。
Bagging可以减少弱分类器的方差，而Boosting 可以减少弱分类器的偏差;
Bagging就是再取样 (Bootstrap) 然后在每个样本上训练出来的模型取平均，所以是降低模型的variance. Bagging 比如Random Forest 这种先天并行的算法都有这个效果。
Boosting 则是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行加权，所以随着迭代不断进行，误差会越来越小，所以模型的 bias 会不断降低。这种算法无法并行，如Adaptive Boosting.
多样性增强的几种方法 一般的做法主要是对数据样本，输入属性，输出表示，算法参数进行扰动。
（1）数据样本扰动
这个其实主要就是采样，比如在bagging中的自助采样法，数据样本扰动对决策树，神经网络这样对数据样本变化非常敏感的学习算法非常有效，但是对支持向量机，朴素贝叶斯，k近邻这些对样本扰动不敏感的算法没用。对此类算法作为基学习器进行集成时往往需要使用输入属性扰动等机制。（2）输入属性扰动
这个就是从样本的特征空间中产生不同的特征子集。这样训练出来的基学习器必然是不同的。在包含大量冗余属性的数据，在特征子集中训练基学习器不仅能产生多样性大的个体，还会因属性数的减少而大幅节省时间开销，同时，由于冗余属性多，减少一些冗余属性后训练出来的基学习器性能也不会差。若数据只包含少量属性，或者冗余属性少，则不适宜使用输入属性扰动法。（3）输出表示扰动
这类做法的基本思路是对输出表示进行操纵以增强多样性。比如可对训练样本的label稍作变动，比如“翻转法”随机改变一些训练样本的标记；也可以对输出表示进行转化，如“输出调制法”将分类输出转化为回归输出后构建基学习器。这一类貌似用的不多。（4）算法参数扰动
这个在现在深度学习比赛中很常见，主要是神经网络有很多参数可以设置，不同的参数往往可以产生差别比较大的基学习器。 Bagging 核心思想：并行地训练一系列各自独立的同类模型，然后再将各个模型的输出结果按照某种策略进行聚合。例如，分类中可以采用投票策略，回归中可以采用平均策略;Bagging主要分为两个阶段：
步骤：
Boostrap阶段，即采用有放回的采样方式，将训练集分为n个子样本集；并用基学习器对每组样本分布进行训练，得到n个基模型；Aggregating阶段，将上一个阶段训练得到的n个基模型组合起来，共同做决策。在分类任务中，可采用投票法，比如相对多数投票法，将结果预测为得票最多的类别。而在回归任务中可采用平均法，即将每个基模型预测得到的结果进行简单平均或加权平均来获得最终的预测结果。 Bagging就是再取样 (Bootstrap) 然后在每个样本上训练出来的模型取平均，所以是降低模型的variance.；Bagging 比如Random Forest 这种先天并行的算法都有这个效果
随机森林（Random Forest） 1. 算法原理 随机森林（Random Forest）是一种基于决策树的集成学习方法。它通过构建多个决策树，并将它们的预测结果进行投票（分类问题）或平均（回归问题），以获得最终的预测结果。随机森林的构建过程包括两个关键步骤：自助采样（bootstrap sampling）和特征随机选择。自助采样用于生成不同的训练数据子集，每个子集用于构建一个决策树。特征随机选择则在每个决策树节点上随机选择一部分特征进行划分，以增加决策树的多样性。这两个步骤共同提高了随机森林的泛化能力和鲁棒性。
2.优缺点 优点：
a) 随机森林具有较高的预测准确性，通常比单个决策树的性能要好。
b) 能够有效地处理高维数据和大量特征。
c) 对噪声和异常值具有较强的鲁棒性。
d) 可以进行特征重要性评估，有助于特征选择。
e) 并行化能力强，易于实现并行计算。
缺点：
a) 相比单个决策树，随机森林的模型可解释性较差。
b) 训练和预测时间可能较长，尤其是在大数据集上。
c) 对于某些不平衡的数据集，随机森林的性能可能不尽如人意。
3.适用场景 随机森林适用于以下场景：
a) 需要提高预测准确性的分类和回归问题。
b) 数据集具有高维特征或特征数量较多。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-08T20:51:04+08:00">
    <meta property="article:modified_time" content="2024-06-08T20:51:04+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">集成学习概述</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>概述</h2> 
<p>集成学习(Ensemble learning)就是将多个机器学习模型组合起来，共同工作以达到优化算法的目的。具体来讲，集成学习可以通过多个学习器相结合，来获得比单一学习器更优越的泛化性能。集成学习的一般步骤为：1.生产一组“个体学习器(individual learner)”；2.用某种策略将他们结合起来。</p> 
<p>个体学习器通常由一个现有的学习算法从训练数据产生。在同质集成（系统中个体学习器的类型相同）中，个体学习器又被称为“基学习器”，而在异质集成（系统中个体学习器的类型不同）中，个体学习器又被称为“组建学习(component learner)”。</p> 
<p>集成学习的集成框架主要有：Bagging，Boosting和Stacking，其中Bagging和Boosting为同质集成，而Stacking为异质集成。<br> Bagging可以减少弱分类器的方差，而Boosting 可以减少弱分类器的偏差;</p> 
<blockquote> 
 <p>Bagging就是再取样 (Bootstrap) 然后在每个样本上训练出来的模型取平均，所以是降低模型的variance. Bagging 比如Random Forest 这种先天并行的算法都有这个效果。<br> Boosting 则是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行加权，所以随着迭代不断进行，误差会越来越小，所以模型的 bias 会不断降低。这种算法无法并行，如Adaptive Boosting.</p> 
</blockquote> 
<h3><a id="_10"></a>多样性增强的几种方法</h3> 
<p>一般的做法主要是对数据样本，输入属性，输出表示，算法参数进行扰动。</p> 
<ul><li>（1）数据样本扰动<br> 这个其实主要就是采样，比如在bagging中的<strong>自助采样法</strong>，数据样本扰动对<strong>决策树，神经网络</strong>这样对数据样本变化非常敏感的学习算法非常有效，但是对支持向量机，朴素贝叶斯，k近邻这些对样本扰动不敏感的算法没用。对此类算法作为基学习器进行集成时往往需要使用输入属性扰动等机制。</li><li>（2）输入属性扰动<br> 这个就是从样本的特征空间中产生不同的<strong>特征子集</strong>。这样训练出来的基学习器必然是不同的。在包含大量冗余属性的数据，在特征子集中训练基学习器不仅能产生多样性大的个体，还会因属性数的减少而大幅节省时间开销，同时，由于冗余属性多，减少一些冗余属性后训练出来的基学习器性能也不会差。若数据只包含少量属性，或者冗余属性少，则不适宜使用输入属性扰动法。</li><li>（3）输出表示扰动<br> 这类做法的基本思路是对输出表示进行操纵以增强多样性。比如可对训练样本的label稍作变动，比如“翻转法”随机改变一些训练样本的标记；也可以对输出表示进行转化，如“输出调制法”将分类输出转化为回归输出后构建基学习器。这一类貌似用的不多。</li><li>（4）算法参数扰动<br> 这个在现在深度学习比赛中很常见，主要是神经网络有很多参数可以设置，不同的参数往往可以产生差别比较大的基学习器。</li></ul> 
<h2><a id="Bagging_20"></a>Bagging</h2> 
<p><strong>核心思想</strong>：并行地训练一系列各自独立的同类模型，然后再将各个模型的输出结果按照某种策略进行聚合。例如，分类中可以采用投票策略，回归中可以采用平均策略;Bagging主要分为两个阶段：<br> <strong>步骤：</strong></p> 
<blockquote> 
 <ul><li>Boostrap阶段，即采用有放回的采样方式，将训练集分为n个子样本集；并用基学习器对每组样本分布进行训练，得到n个基模型；</li><li>Aggregating阶段，将上一个阶段训练得到的n个基模型组合起来，共同做决策。在分类任务中，可采用投票法，比如相对多数投票法，将结果预测为得票最多的类别。而在回归任务中可采用平均法，即将每个基模型预测得到的结果进行简单平均或加权平均来获得最终的预测结果。</li></ul> 
</blockquote> 
<p>Bagging就是再取样 (Bootstrap) 然后在每个样本上训练出来的模型取平均，所以是<strong>降低模型的variance</strong>.；Bagging 比如Random Forest 这种先天并行的算法都有这个效果</p> 
<h3><a id="Random_Forest_27"></a>随机森林（Random Forest）</h3> 
<h4><a id="1__28"></a>1. 算法原理</h4> 
<p>随机森林（Random Forest）是一种基于决策树的集成学习方法。它通过构建多个决策树，并将它们的预测结果进行投票（分类问题）或平均（回归问题），以获得最终的预测结果。随机森林的构建过程包括两个关键步骤：自助采样（bootstrap sampling）和特征随机选择。自助采样用于生成不同的训练数据子集，每个子集用于构建一个决策树。特征随机选择则在每个决策树节点上随机选择一部分特征进行划分，以增加决策树的多样性。这两个步骤共同提高了随机森林的泛化能力和鲁棒性。</p> 
<h4><a id="2_30"></a>2.优缺点</h4> 
<p>优点：<br> a) 随机森林具有较高的预测准确性，通常比单个决策树的性能要好。<br> b) 能够有效地处理高维数据和大量特征。<br> c) 对噪声和异常值具有较强的鲁棒性。<br> d) 可以进行特征重要性评估，有助于特征选择。<br> e) 并行化能力强，易于实现并行计算。<br> 缺点：<br> a) 相比单个决策树，随机森林的模型可解释性较差。<br> b) 训练和预测时间可能较长，尤其是在大数据集上。<br> c) 对于某些不平衡的数据集，随机森林的性能可能不尽如人意。</p> 
<h4><a id="3_41"></a>3.适用场景</h4> 
<p>随机森林适用于以下场景：<br> a) 需要提高预测准确性的分类和回归问题。<br> b) 数据集具有高维特征或特征数量较多。<br> c) 数据集中存在噪声和异常值。<br> 随机森林在许多实际应用中表现出较好的性能，尤其是在提高预测准确性方面。然而，随机森林的可解释性较差，且在大数据集上训练和预测时间可能较长。在面临这些问题时，可以考虑使用其他集成方法，如梯度提升树（Gradient Boosting Trees）等。</p> 
<h5><a id="RandomForestClassifier_47"></a>RandomForestClassifier：分类树</h5> 
<pre><code class="prism language-python">RandomForestRegressor<span class="token punctuation">(</span>n_estimators<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> 
  criterion<span class="token operator">=</span><span class="token string">'mse'</span><span class="token punctuation">,</span> max_depth<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
  min_samples_split<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> min_samples_leaf<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> 
  min_weight_fraction_leaf<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> 
  max_features<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">,</span> max_leaf_nodes<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
  min_impurity_decrease<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> 
  min_impurity_split<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
  bootstrap<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> oob_score<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> 
  n_jobs<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
  verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> warm_start<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> 
  ccp_alpha<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> max_samples<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p>重要的参数：</p> 
<blockquote> 
 <ul><li>n_estimators：随机森林中决策树的个数，默认为 100。</li><li>criterion：随机森林中决策树的算法，可选的有两种：1）gini：基尼系数，也就是 CART 算法，为默认值；2）entropy：信息熵，也就是 ID3 算法。</li><li>max_depth：决策树的最大深度。</li></ul> 
</blockquote> 
<h5><a id="RandomForestRegressor_65"></a>RandomForestRegressor：回归树</h5> 
<pre><code class="prism language-python">RandomForestRegressor<span class="token punctuation">(</span>n_estimators<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> 
  criterion<span class="token operator">=</span><span class="token string">'mse'</span><span class="token punctuation">,</span> max_depth<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
  min_samples_split<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> min_samples_leaf<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> 
  min_weight_fraction_leaf<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> 
  max_features<span class="token operator">=</span><span class="token string">'auto'</span><span class="token punctuation">,</span> max_leaf_nodes<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
  min_impurity_decrease<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> 
  min_impurity_split<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
  bootstrap<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> oob_score<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> 
  n_jobs<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
  verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> warm_start<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> 
  ccp_alpha<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> max_samples<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>

</code></pre> 
<p>回归树中的参数与分类树中的参数基本相同，但 criterion 参数的取值不同。<br> 在回归树中，criterion 参数有下面两种取值：</p> 
<ul><li>mse：表示均方误差算法，为默认值。</li><li>mae：表示平均误差算法。</li></ul> 
<h2><a id="Boosting_85"></a>Boosting</h2> 
<p>Boosting由于各基学习器之间存在强依赖关系，因此只能串行处理，也就是Boosting实际上是个<strong>迭代学习</strong>的过程。<br> Boosting的<strong>工作机制</strong>为：先从初始训练集中训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整（比如增大被误分样本的权重，减小被正确分类样本的权重），使得先前基学习器做错的样本在后续的训练过程中受到更多关注，然后基于调整后的样本分布来训练下一个基学习器，如此重复，直到基学习器数目达到事先自定的值T，然后将这T 个基学习器进行<strong>加权结合</strong>（比如错误率小的基学习器权重大，错误率大的基学习器权重小，这样做决策时，错误率小的基本学习器影响更大）<br> Boosting算法主要有AdaBoost(Adaptive Boost)自适应提升算法和Gradient Boosting梯度提升算法。最主要的区别在于两者如何识别和解决模型的问题。AdaBoost用错分的数据样本来识别问题，通过调整错分数据样本的权重来改进模型。Gradient Boosting主要通过负梯度来识别问题，通过计算负梯度来改进模型。<br> <img src="https://images2.imgbox.com/73/52/9Qv8qLw6_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/df/93/Hi09ItmK_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="AdaBoost_93"></a>AdaBoost</h3> 
<p><strong>核心思想</strong>：串行地训练一系列前后依赖的同类模型，即后一个模型用来对前一个模型的输出结果进行纠正。Boosting算法是可以将弱学习器提升为强学习器的学习算法。</p> 
<blockquote> 
 <p><strong>步骤</strong>：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行。直至基学习器数目达到实现指定的值n，最终将这n个基学习器进行结合。</p> 
</blockquote> 
<p>Boosting 则是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行加权，所以随着迭代不断进行，<strong>误差会越来越小</strong>，所以模型的 bias 会不断降低。这种算法无法并行，如Adaptive Boosting；</p> 
<h4><a id="1__98"></a>1. 算法原理</h4> 
<p>AdaBoost（Adaptive Boosting）是一种集成学习方法，通过多次迭代训练一系列弱学习器并加权组合，以提高分类性能。在每次迭代过程中，对错误分类的样本增加权重，使得后续的弱学习器更关注这些样本。最后，将所有弱学习器的预测结果进行加权投票，得到最终分类结果。</p> 
<h4><a id="2_100"></a>2.优缺点</h4> 
<p>a) 优点：<br> i. 可以提高模型的准确性和泛化能力。<br> ii. 算法简单易于实现。 iii. 不容易过拟合。<br> b) 缺点：<br> i. 对异常值和噪声敏感，可能导致性能下降。<br> ii. 训练过程需要依次进行，较难并行化。</p> 
<h4><a id="3_107"></a>3.适用场景</h4> 
<p>AdaBoost适用于以下场景：<br> a) 当基学习器性能较弱时，可以通过集成提高性能。<br> b) 适用于二分类问题，尤其是需要提高分类性能的场景。<br> AdaBoost在人脸检测、文本分类、客户流失预测等领域有广泛应用。</p> 
<h3><a id="GBDT_112"></a>GBDT</h3> 
<h4><a id="_113"></a>核心思想</h4> 
<p>梯度提升决策树（Gradient Boosting Decision Tree，GBDT） 是将多个弱学习器（通常是决策树）组合成一个强大的预测模型。具体而言，GBDT的定义如下：<br> 初始化：首先，GBDT使用一个常数（通常是目标变量的平均值）作为初始预测值。这个初始预测值代表了我们对目标变量的初始猜测。<br> 迭代训练：GBDT是一个迭代算法，通过多轮迭代来逐步改进模型。在每一轮迭代中，GBDT都会训练一棵新的决策树，目标是减少前一轮模型的残差（或误差）。残差是实际观测值与当前模型预测值之间的差异，新的树将学习如何纠正这些残差。<br> 集成：最终，GBDT将所有决策树的预测结果相加，得到最终的集成预测结果。这个过程使得模型能够捕捉数据中的复杂关系，从而提高了预测精度。<br> GBDT的核心原理在于不断迭代，每一轮迭代都尝试修正前一轮模型的错误，逐渐提高模型的预测性能。</p> 
<h4><a id="_119"></a>优缺点</h4> 
<p><strong>优点：</strong></p> 
<ol><li>可以灵活处理各种类型的数据，包括连续值和离散值。</li><li>在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。<br> 3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。</li><li>很好的利用了弱分类器进行级联。</li><li>充分考虑的每个分类器的权重。<br> 6）不需要归一化。树模型都不需要，梯度下降算法才需要<br> 6）处理非线性关系</li></ol> 
<p><strong>缺点：</strong><br> 1）由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。<br> 2）不适合高维稀疏特征</p> 
<h4><a id="_132"></a>适用场景</h4> 
<p>GBDT 可以适用于回归问题（线性和非线性）；<br> GBDT 也可用于二分类问题（设定阈值，大于为正，否则为负）和多分类问题。</p> 
<h3><a id="XGBoost_135"></a>XGBoost</h3> 
<h4><a id="1__136"></a>1. 算法原理</h4> 
<p>XGBoost（eXtreme Gradient Boosting）是基于梯度提升（Gradient Boosting）的决策树集成学习方法。XGBoost通过加入正则化项，降低模型复杂度，提高泛化能力。同时，XGBoost采用了并行计算和近似算法，显著提高了训练速度。<br> XGBoost是基于GBDT 的一种改进算法；<br> 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。<br> Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（传统GBDT的实现也有学习速率）<br> 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。</p> 
<h4><a id="2_142"></a>2.优缺点</h4> 
<p>优点：<br> i. 高效的训练速度，支持并行计算。<br> ii. 高准确率，通过正则化降低过拟合风险。<br> iii. 支持自定义损失函数和评估指标。<br> iv. 内置特征重要性排序功能。<br> 缺点：<br> i. 超参数调优较为复杂。<br> ii. 需要较多的计算资源。</p> 
<h4><a id="3_151"></a>3.适用场景</h4> 
<p>XGBoost在以下场景表现优异：<br> a) 大规模数据集。<br> b) 需要高准确率的分类和回归任务。<br> c) 特征选择。<br> XGBoost在Kaggle竞赛中广泛应用，获得了多次胜利。</p> 
<h3><a id="LightGBM_157"></a>LightGBM</h3> 
<p>LightGBM是一种基于梯度提升树的机器学习算法，它通过使用基于直方图的算法和带有按叶子节点分割的决策树来提高训练和预测的效率。</p> 
<h4><a id="_159"></a>算法原理</h4> 
<p>基于直方图的算法：LightGBM使用了一种基于直方图的算法来处理数据。它将数据按特征值进行离散化，构建直方图并对其进行优化，从而减少了内存消耗和计算时间。<br> 基于按叶子节点分割的决策树：传统的梯度提升树算法在每个节点上都尝试所有特征的切分点，而LightGBM在构建决策树时采用了按叶子节点分割的策略。这样可以减少计算量，并且更容易处理高维稀疏特征。<br> LightGBM也是基于GBDT的改进算法；</p> 
<h4><a id="_163"></a>优缺点</h4> 
<p><strong>优点：</strong><br> 高效性：LightGBM具有高效的训练和预测速度，尤其在处理大规模数据集时表现出色。<br> 低内存消耗：由于使用了基于直方图的算法和按叶子节点分割的决策树，LightGBM能够减少内存消耗，适用于内存有限的环境。<br> 高准确性：LightGBM通过优化算法和特征选择等方法提高了模型的准确性。<br> <strong>缺点：</strong><br> 对噪声敏感：LightGBM在处理噪声较大的数据时可能会过拟合，需要进行适当的正则化。<br> 参数调优：LightGBM有一些需要调优的参数，不同的参数组合可能会导致不同的效果，需要进行合适的参数调优。</p> 
<p><strong>注意事项：</strong><br> 数据预处理：对数据进行清洗、缺失值处理和特征工程等预处理步骤，以提高模型的泛化能力。<br> 参数调优：通过交叉验证等方法选择合适的参数组合，以获得更好的模型性能。<br> 提前停止：在训练过程中使用早期停止法，避免模型过拟合。<br> 特征重要性评估：通过分析模型输出的特征重要性，可以帮助理解数据和模型之间的关系，指导特征选择和特征工程。</p> 
<blockquote> 
 <p>总体而言，LightGBM是一种高效、低内存消耗且具有准确性的机器学习算法，在处理大规模数据集和高维稀疏特征方面具有优势。但需要注意参数调优和模型过拟合问题。</p> 
</blockquote> 
<h3><a id="_179"></a>参数设置</h3> 
<h4><a id="AdaBoost_180"></a>AdaBoost</h4> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">sklearn</span><span class="token punctuation">.</span>ensemble<span class="token punctuation">.</span>AdaBoostClassifier<span class="token punctuation">(</span>base_estimator<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> n_estimators<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> algorithm<span class="token operator">=</span><span class="token string">'SAMME.R'</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<blockquote> 
 <ul><li>base_estimator:基分类器，默认是决策树，在该分类器基础上进行boosting，理论上可以是任意一个分类器，但是如果是其他分类器时需要指明样本权重。</li><li>n_estimators：基分类器提升（循环）次数，默认是50次，这个值过大，模型容易过拟合；值过小，模型容易欠拟合。</li><li>learning_rate：学习率，表示梯度收敛速度，默认为1，如果过大，容易错过最优值，如果过小，则收敛速度会很慢；该值需要和n_estimators进行一个权衡，当分类器迭代次数较少时，学习率可以小一些，当迭代次数较多时，学习率可以适当放大。</li><li>algorithm：boosting算法，也就是模型提升准则，有两种方式SAMME, 和SAMME.R两种，默认是SAMME.R，两者的区别主要是弱学习器权重的度量，前者是对样本集预测错误的概率进行划分的，后者是对样本集的预测错误的比例，即错分率进行划分的，默认是用的SAMME.R。</li><li>random_state：随机种子设置。</li></ul> 
</blockquote> 
<h4><a id="GBDT_189"></a>GBDT</h4> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">sklearn</span><span class="token punctuation">.</span>ensemble<span class="token punctuation">.</span>GradientBoostingClassifier<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">,</span> loss<span class="token operator">=</span><span class="token string">'deviance'</span><span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> n_estimators<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> subsample<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> 
criterion<span class="token operator">=</span><span class="token string">'friedman_mse'</span><span class="token punctuation">,</span> min_samples_split<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> min_samples_leaf<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> min_weight_fraction_leaf<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> max_depth<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> 
min_impurity_decrease<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> min_impurity_split<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> init<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> max_features<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> max_leaf_nodes<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> warm_start<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> presort<span class="token operator">=</span><span class="token string">'deprecated'</span><span class="token punctuation">,</span> validation_fraction<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> 
n_iter_no_change<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> tol<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">,</span> ccp_alpha<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span>
</code></pre> 
<p>GBDT（Gradient Boosting Decision Tree）是一种基于梯度提升的集成学习算法，它使用决策树作为弱学习器，通过迭代地拟合残差来提高预测性能。GBDT 的主要参数有：</p> 
<blockquote> 
 <ul><li><strong>n_estimators</strong>：也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是100。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑；</li><li><strong>learning_rate</strong>：float，认= 0.1,学习率缩小了每棵树的贡献learning_rate。在learning_rate和n_estimators之间需要权衡；推荐的候选值为：[0.01, 0.015, 0.025, 0.05, 0.1]</li><li><strong>subsample</strong>：子采样，取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在 <strong>[0.5, 0.8]</strong> 之间，默认是1.0，即不使用子采样；<br> <strong>loss</strong>：即我们GBDT算法中的<strong>损失函数</strong>。分类模型和回归模型的损失函数是不一样的。1）对于分类模型，有对数似然损失函数"deviance"和指数损失函数"exponential"两者输入选择。默认是对数似然损失函数"deviance"。2）对于回归模型，有均方差"ls", 绝对损失"lad", Huber损失"huber"和分位数损失“quantile”。默认是均方差"ls"。一般来说，如果数据的噪音点不多，用默认的均方差"ls"比较好。如果是噪音点较多，则推荐用抗噪音的损失函数"huber"。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。</li><li>init：计量或“零”，默认=无,一个估计器对象，用于计算初始预测。 init必须提供fit和predict_proba。如果为“零”，则初始原始预测设置为零。默认情况下，使用 DummyEstimator预测类优先级；</li><li>max_depth：int，默认= 3,各个回归估计量的最大深度。最大深度限制了树中节点的数量。调整此参数以获得最佳性能；最佳值取决于输入变量的相互作用。也就是那个树有多少层（一般越多越容易过拟合）；</li><li>min_samples_split：int或float，默认为2,拆分内部节点所需的最少样本数：如果为int，则认为min_samples_split是最小值。如果为float，min_samples_split则为分数， 是每个拆分的最小样本数。ceil(min_samples_split * n_samples)</li><li>min_samples_leaf： int或float，默认值= 1,在叶节点处所需的最小样本数。仅在任何深度的分裂点在min_samples_leaf左分支和右分支中的每个分支上至少留下训练样本时，才考虑。这可能具有平滑模型的效果，尤其是在回归中。如果为int，则认为min_samples_leaf是最小值。如果为float，min_samples_leaf则为分数， 是每个节点的最小样本数。ceil(min_samples_leaf * n_samples)</li><li>min_weight_fraction_leaf：float，默认值= 0.0,在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。</li><li>min_impurity_decrease：浮动，默认值= 0.0，如果节点分裂会导致杂质的减少大于或等于该值，则该节点将被分裂；</li><li>min_impurity_split ：float，默认=无提前停止树木生长的阈值。如果节点的杂质高于阈值，则该节点将分裂，否则为叶</li><li>subsample：用于训练每个弱学习器的<strong>样本比例</strong>。减小该参数可以降低方差，但也可能增加偏差。</li><li>max_features：节点分裂时参与判断的最大特征数，用于分裂每个决策树节点的特征数量。减小该参数可以降低过拟合的风险，但也可能降低模型的表达能力。int：个数；float：占所有特征的百分比；auto：所有特征数的开方；sqrt：所有特征数的开方；log2：所有特征数的log2值；None：等于所有特征数。</li></ul> 
</blockquote> 
<h4><a id="XGBoost_212"></a>XGBoost</h4> 
<p>XGBoost（Extreme Gradient Boosting）是一种优化的 GBDT 实现，它使用了更高效的数据结构和并行计算，同时引入了正则化项和剪枝策略来防止过拟合。XGBoost 的主要参数有：</p> 
<blockquote> 
 <ul><li>n_estimators：同 GBDT。</li><li>learning_rate：同 GBDT。</li><li>max_depth：同 GBDT。</li><li>min_child_weight：决策树叶节点的最小权重和，相当于 GBDT 中的 min_samples_leaf 乘以样本权重。增加该参数可以防止过拟合，但也可能导致欠拟合。</li><li>subsample：同 GBDT。</li><li>colsample_bytree：相当于 GBDT 中的 max_features，表示用于训练每棵树的特征比例。减小该参数可以降低过拟合的风险，但也可能降低模型的表达能力。</li><li>reg_alpha：L1 正则化项的系数，用于惩罚模型的复杂度。增加该参数可以使模型更稀疏，但也可能损失一些信息。推荐的候选值为：[0, 0.01~0.1, 1]</li><li>reg_lambda：L2 正则化项的系数，用于惩罚模型的复杂度。增加该参数可以防止过拟合，但也可能降低模型的灵活性。推荐的候选值为：[0, 0.1, 0.5, 1]</li></ul> 
</blockquote> 
<h4><a id="LightGBM_222"></a>LightGBM</h4> 
<p>LightGBM（Light Gradient Boosting Machine）是一种基于梯度提升的高效的分布式机器学习框架，它使用了基于直方图的算法和基于叶子的生长策略，可以大大提高训练速度和减少内存消耗。LightGBM 的主要参数有：</p> 
<blockquote> 
 <ul><li>n_estimators：同 GBDT。</li><li>learning_rate：同 GBDT。</li><li>num_leaves：决策树的最大叶子数，相当于 GBDT 中的 max_depth 的指数倍。增加该参数可以增加模型的复杂度和拟合程度，但也可能导致过拟合。</li><li>min_child_samples：同 GBDT 中的 min_samples_leaf。</li><li>subsample：同 GBDT。</li><li>colsample_bytree：同 XGBoost。</li><li>reg_alpha：同 XGBoost。</li><li>reg_lambda：同 XGBoost。</li></ul> 
</blockquote> 
<h2><a id="Stacking_232"></a>Stacking</h2> 
<p>Stacking的核性思想是并行地训练一系列各自独立的不同类模型，然后通过训练一个元模型(meta-model)来将各个模型输出结果进行结合。Stacking也可以分为两个阶段：</p> 
<p>分别采用全部的训练样本来训练n个组件模型，要求这些个体学习器必须异构的，比如可以分别是线性学习器，SVM，决策树模型和深度学习模型。<br> 训练一个元模型(meta-model)来将各个组件模型的输出结果进行结合，具体过程就是将各个学习器在训练集上得到的预测结果作为训练特征和训练集的真实结果组成新的训练集；然后用这个新组成的训练集来训练一个元模型。这个元模型可以是线性模型或者树模型。</p> 
<p>分类<br> 原理<br> 优劣势<br> python实现<br> 调参<br> 应用</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/103bd4c8fa9c5db1fd8756b758dbe399/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">kafka的leader和follower</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0367f0d8d158d890367e26dbef2dabf3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">微信小程序基础工作模板</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>