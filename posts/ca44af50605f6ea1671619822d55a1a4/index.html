<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Stable Diffusion ComfyUI 基础教程（六）图片放大与细节修复 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/ca44af50605f6ea1671619822d55a1a4/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Stable Diffusion ComfyUI 基础教程（六）图片放大与细节修复">
  <meta property="og:description" content="我们都知道文生图、图生图、局部重绘，生成的图片分辨率太小，怎么办？我们可以通过模型放大、潜在放大、非潜在放大、分块放大多种方式对图像进行放大。
放大工作流： 我们以文生图后的图片进行放大，在开始之前我们打开之前搭建的文生图基础流程
模型放大： 模型放大是最简单，也是效果最差的放大方式，他就和我们在 Web UI 上使用后期处理进行放大一样，只是通过放大算法对图像直接放大（也就是图像空间放大），这也会导致我们放大的图片损失细节，甚至看起来会很腻很假；
虽然上面说的一无是处，但是使用它搭配在其他的放大方式上效果就大大增强，我们先看一下怎么使用模型放大吧；
“右键-新建节点-图像-放大-图像通过模型放大”
左侧的“放大模型”通过拖拽连接“放大模型加载器”（“右键-新建节点-加载器-放大模型加载器”进行连接也可以）；
选择自己需要的放大模型，我给大家准备好的 17个放大模型，都放在网盘里面了（管理器的安装模型也可以下载放大模型），其中BSRGAN、ESRGAN、SwinIR_4K、RealESRGAN_x4plus 效果不错；
左侧的“图像”连接“VAE解码”输出的图像，右侧直接连接保存图像就可以了；
有没有发现我们没有设置图片放大的倍数，其实模型放大是直接根据所选放大模型进行放大的，一般都是放大4倍。 潜在放大： 这时候我们插个知识点，在 Web UI 上我们进行高分辨率修复的时候，我们会看到在选择放大算法时，有 Latent 开头的几个算法可选择，这其实就是我们进行潜在放大所需要用到的流程； 潜在放大是在潜空间进行放大，也就是对 Latent 进行缩放，然后对缩放后的 Latent 进行重新采样，进而增加细节达到放大的目的；
“右键-新建节点-Latent-Latent缩放/Latent按系数缩放”，可以看到我这边放了两个节点，这两个节点一个是需要设置宽高、一个是根据倍数放大，根据自己的需要选择就好；
如果前几节课学明白了，后面的就不用我多说了吧，再串连一个采样器就可以了；
下方是我连接好的工作流，这里我选择的是“Latent按系数缩放”，这个比较方便一些，大家也可以选择“Latent缩放”
注意：降噪数值一定要在0.5左右，数值过低会有崩坏的情况。放大倍数也不要太大，太大同样会有崩坏的情况
非潜在放大： 非潜在放大可以理解成模型放大和潜在放大的结合，先进行模型放大后对模型放大的图片进行重新采样；
是不是有思路了？不过不要忘了对模型放大后的图片进行缩放（因为直接对放大4倍的图片进行重新采样会占用太多显存并且添加的细节不够）；
我们想要得到更高像素的图像可以多次叠加“非潜在放大”流程，只要你的显存够大，你想要多大的图都可以。
分块放大： 非潜在放大要得到一张 4K 图需要很高的显存，但是我们可以通过分块放大，使用较小的显存出更大的图。
分块放大其实就是把一张图切成 n 个块，然后对小块进行重新采样，最后把小块拼接在一起。他的好处是可以使用更小的显存得到一张更大分辨率的图；
分块放大是需要用到一个插件的：
https://github.com/BlenderNeko/ComfyUI_TiledKSampler.git（压缩包解压安装、git拉取、管理器安装都可以）
我们需要用到这个插件中的“分块采样器”，“右键-新建节点-采样器-K采样器（分块）”，他同样有一个高级采样器，这个我们要进行 refiner 模型细化的时候可以使用。
来看一下和我们的普通采样器有什么区别吧，只多了三个操作，分块宽度、分块高度、无缝分块策略
random（随机）：也是默认策略，它是通过在水平和垂直的分块图像之间随机交替采样来消除拼接痕迹（效果最好，与 uni 采样器不兼容）；
random strict（随机严格）：因为 random 很有可能会对有边框的图像产生不好的效果，所以 random strict 使用遮罩来确保不裁剪边框图块（效果和random 类似，但不适用于 SDE 采样器）。
padded（填充）：为每块图像在四周提供一定的范围进行叠加来减少拼接缝隙，正因为这样，他需要对多达 4 倍的图块进行采样（效果不如 random ，速度慢，支持 uni 采样器）。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-03-28T12:27:08+08:00">
    <meta property="article:modified_time" content="2024-03-28T12:27:08+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Stable Diffusion ComfyUI 基础教程（六）图片放大与细节修复</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>我们都知道文生图、图生图、局部重绘，生成的图片分辨率太小，怎么办？我们可以通过模型放大、潜在放大、非潜在放大、分块放大多种方式对图像进行放大。</p> 
<h4><a id="_6"></a><strong>放大工作流：</strong></h4> 
<p>我们以文生图后的图片进行放大，在开始之前我们打开之前搭建的文生图基础流程</p> 
<h6><a id="_10"></a><strong>模型放大：</strong></h6> 
<ol><li> <p>模型放大是最简单，也是效果最差的放大方式，他就和我们在 Web UI 上使用后期处理进行放大一样，只是通过放大算法对图像直接放大（也就是图像空间放大），这也会导致我们放大的图片损失细节，甚至看起来会很腻很假；</p> </li><li> <p>虽然上面说的一无是处，但是使用它搭配在其他的放大方式上效果就大大增强，我们先看一下怎么使用模型放大吧；</p> </li><li> <p>“右键-新建节点-图像-放大-图像通过模型放大”</p> </li></ol> 
<p><img src="https://images2.imgbox.com/e4/6d/n0GDWlgQ_o.png" alt=""></p> 
<ol start="4"><li> <p>左侧的“放大模型”通过拖拽连接“放大模型加载器”（“右键-新建节点-加载器-放大模型加载器”进行连接也可以）；</p> </li><li> <p>选择自己需要的放大模型，我给大家准备好的 17个放大模型，都放在网盘里面了（管理器的安装模型也可以下载放大模型），其中BSRGAN、ESRGAN、SwinIR_4K、RealESRGAN_x4plus 效果不错；</p> </li><li> <p>左侧的“图像”连接“VAE解码”输出的图像，右侧直接连接保存图像就可以了；</p> </li></ol> 
<p><img src="https://images2.imgbox.com/51/2f/6oaIvfE5_o.png" alt=""></p> 
<ol start="7"><li>有没有发现我们没有设置图片放大的倍数，其实模型放大是直接根据所选放大模型进行放大的，一般都是放大4倍。</li></ol> 
<h6><a id="_33"></a><strong>潜在放大：</strong></h6> 
<ol><li>这时候我们插个知识点，在 Web UI 上我们进行高分辨率修复的时候，我们会看到在选择放大算法时，有 Latent 开头的几个算法可选择，这其实就是我们进行潜在放大所需要用到的流程；</li></ol> 
<p><img src="https://images2.imgbox.com/13/b6/vsLnMXLq_o.png" alt=""></p> 
<ol start="2"><li> <p>潜在放大是在潜空间进行放大，也就是对 Latent 进行缩放，然后对缩放后的 Latent 进行重新采样，进而增加细节达到放大的目的；</p> </li><li> <p>“右键-新建节点-Latent-Latent缩放/Latent按系数缩放”，可以看到我这边放了两个节点，这两个节点一个是需要设置宽高、一个是根据倍数放大，根据自己的需要选择就好；</p> </li></ol> 
<p><img src="https://images2.imgbox.com/48/32/Q2q4w56Y_o.png" alt=""></p> 
<ol start="4"><li> <p>如果前几节课学明白了，后面的就不用我多说了吧，再串连一个采样器就可以了；</p> </li><li> <p>下方是我连接好的工作流，这里我选择的是“Latent按系数缩放”，这个比较方便一些，大家也可以选择“Latent缩放”</p> </li></ol> 
<p><img src="https://images2.imgbox.com/2d/50/QYd6zDf5_o.png" alt=""></p> 
<p>注意：降噪数值一定要在0.5左右，数值过低会有崩坏的情况。放大倍数也不要太大，太大同样会有崩坏的情况</p> 
<h6><a id="_56"></a><strong>非潜在放大：</strong></h6> 
<ol><li> <p>非潜在放大可以理解成模型放大和潜在放大的结合，先进行模型放大后对模型放大的图片进行重新采样；</p> </li><li> <p>是不是有思路了？不过不要忘了对模型放大后的图片进行缩放（因为直接对放大4倍的图片进行重新采样会占用太多显存并且添加的细节不够）；</p> </li><li> <p>我们想要得到更高像素的图像可以多次叠加“非潜在放大”流程，只要你的显存够大，你想要多大的图都可以。</p> </li></ol> 
<p><img src="https://images2.imgbox.com/fe/91/EANmUUWq_o.png" alt=""></p> 
<h6><a id="_67"></a><strong>分块放大：</strong></h6> 
<ol><li> <p>非潜在放大要得到一张 4K 图需要很高的显存，但是我们可以通过分块放大，使用较小的显存出更大的图。</p> </li><li> <p>分块放大其实就是把一张图切成 n 个块，然后对小块进行重新采样，最后把小块拼接在一起。他的好处是可以使用更小的显存得到一张更大分辨率的图；</p> </li><li> <p>分块放大是需要用到一个插件的：<br> https://github.com/BlenderNeko/ComfyUI_TiledKSampler.git（压缩包解压安装、git拉取、管理器安装都可以）</p> </li><li> <p>我们需要用到这个插件中的“分块采样器”，“右键-新建节点-采样器-K采样器（分块）”，他同样有一个高级采样器，这个我们要进行 refiner 模型细化的时候可以使用。</p> </li></ol> 
<p><img src="https://images2.imgbox.com/8a/f4/n5AFDSvj_o.png" alt=""></p> 
<ol start="5"><li> <p>来看一下和我们的普通采样器有什么区别吧，只多了三个操作，分块宽度、分块高度、无缝分块策略</p> </li><li> <p>random（随机）：也是默认策略，它是通过在水平和垂直的分块图像之间随机交替采样来消除拼接痕迹（效果最好，与 uni 采样器不兼容）；</p> </li><li> <p>random strict（随机严格）：因为 random 很有可能会对有边框的图像产生不好的效果，所以 random strict 使用遮罩来确保不裁剪边框图块（效果和random 类似，但不适用于 SDE 采样器）。</p> </li><li> <p>padded（填充）：为每块图像在四周提供一定的范围进行叠加来减少拼接缝隙，正因为这样，他需要对多达 4 倍的图块进行采样（效果不如 random ，速度慢，支持 uni 采样器）。</p> </li><li> <p>simple（简单）：看名字就知道，最简单的策略，就是直接一块块的去噪（效果最差，速度最快，不建议使用）</p> </li><li> <p>数值不要太小（根据你放的后图片的大小决定，比如我要放大成2048*2048，我是设置为1024*1024的）</p> </li><li> <p>数值越小分的块越多，分的块越多也就越慢；也会增加出现伪影的几率（比如你生成猫，你会在某些小区域得到猫的影子，即使你降噪数值小也会出现不同程度的伪影）</p> </li><li> <p>分块宽度、分块高度：是指我们放大时每一次进行分块采样时绘制的宽高；</p> </li><li> <p>无缝分块策略：是指在我们分块采样的每一块相接处会出现明显的拼接痕迹，而无缝分块策略就是通过不同的方式减少拼接痕迹的，有四种可选策略，random（随机）、random strict（随机严格）、padded（填充）、simple（简单），我建议使用 random 和 random strict。</p> </li><li> <p>是不是以为当成普通采样器连接就可以了，，，，那当然不是，在“正面条件”中间要把 ControlNet 的 tile 连接进去；</p> </li><li> <p>“右键-新建节点-加载器-ControlNet加载器”，加载器选择“tile”的（tile模型我会放在网盘里面，其他的ControlNet使用方法我会在第八节课讲给大家）</p> </li></ol> 
<p><img src="https://images2.imgbox.com/b2/53/bWIBdZad_o.png" alt=""></p> 
<ol start="8"><li> <p>“ControlNet加载器”连接“ControlNet应用”，直接拖拽连接就可以（或：右键-新建节点-条件-ControlNet应用）</p> </li><li> <p>我们“ControlNet应用”两边的“条件”连接“正面条件/提示词”，图像连接“VAE解码”输出的图像；大模型连接“K采样器（分块）”上的模型；</p> </li><li> <p>这时候我们就剩下设置出图大小了，我们可以在“VAE解码的图像”连接过来一个“图像缩放/按系数放大”（或：模型放大流程，我个人测试下来，模型放大效果最好）；</p> </li><li> <p>然后对图像进行“VAE编码”然后连接到“K采样器（分块）”上，这不用细说了吧，图生图的流程；</p> </li><li> <p>注意：降噪同样不要太低，分块宽高根据情况设置，如果出现伪影就增大数值</p> </li></ol> 
<p><img src="https://images2.imgbox.com/8d/1d/GewQ8pCO_o.png" alt=""></p> 
<p><img src="https://images2.imgbox.com/1f/33/TaXVEkh4_o.png" alt=""></p> 
<h4><a id="UltimateSDUpscaleSD_123"></a><strong>UltimateSDUpscale（终极SD放大）：</strong></h4> 
<ol><li> <p>我们使用时每次都要搭建一遍“非潜在放大或分块放大”等流程会很麻烦，使用“ComfyUI_UltimateSDUpscale”插件会更简单。</p> </li><li> <p>下载地址：<br> https://github.com/ssitu/ComfyUI_UltimateSDUpscale.git（压缩包解压安装、git拉取、管理器安装都可以）</p> </li><li> <p>“右键-新建节点-图像-放大-SD放大”，这个插件有两个功能，一个是细节修复+放大（SD放大），一个是细节修复（SD放大（不放大））。我们主要用到的是 SD（放大），他们的区别只是是否连接放大模型；</p> </li></ol> 
<p><img src="https://images2.imgbox.com/10/bb/CZoridLq_o.png" alt=""></p> 
<ol start="4"><li> <p>这个放大插件可以说是把上方的“模型放大+分块放大”进行了一个组合；</p> </li><li> <p>我们可以看到这个节点和我们上面用的“K采样器（分块）”很相似对吧，这个地方我们主要去关注几个操作就可以了</p> </li><li> <p>Linear（直线）：逐行进行拼接（更快，但是会有几率出现伪影，我们默认使用这个）</p> </li><li> <p>Chess：棋盘拼接 ，可以理解成“X”形状的拼接样式（慢一些，出现伪影的几率小）</p> </li><li> <p>mode_type（模式类型）：有两个可选择</p> </li><li> <p>mask_blur（模糊）：拼接区域的羽化程度，默认8就可以，融合不好的情况下适当调高</p> </li><li> <p>Tile_padding（分块区域）：相邻融合像素，和我们上面说的 padded（填充）策略一样，不过这个插件我们可以设置融合像素的值</p> </li><li> <p>seam_fix_mode（接缝修复模式）：有三种模式选择，Band Pass（速度快）、Half Tile（质量好）、Band Pass+Half Tile（速度和质量做一个折中）</p> </li><li> <p>连接就更简单了，连接一个“放大模型加载器”，其他的就按照名字连接就可以；</p> </li></ol> 
<p><img src="https://images2.imgbox.com/b4/39/xynXhTl8_o.png" alt=""></p> 
<h4><a id="_158"></a><strong>总结：</strong></h4> 
<ol><li> <p>“模型放大”最快也是效果最差，最不推荐的。我常用的是“非潜在放大”和“UltimateSDUpscale 插件放大”；</p> </li><li> <p>每个放大方式都有利弊，我们完全可以多种方式一起进行放大，比如：潜在放大/非潜在放大 + UltimateSDUpscale 插件放大，这样我们放大后的图片效果更好，细节更丰富；</p> </li><li> <p>我们 refiner 模型细化、图生图、还有我们以后要学习的 文/图生视频 等都可以用这几种方式进行放大；</p> </li></ol> 
<p>文章使用的大模型、Lora模型、SD插件、示例图片等，都已经上传到我整理的 Stable Diffusion 绘画资源中。有需要的小伙伴文末扫码自行获取。</p> 
<h3><a id="_173"></a>写在最后</h3> 
<p>AIGC技术的未来发展前景广阔，随着人工智能技术的不断发展，AIGC技术也将不断提高。未来，AIGC技术将在游戏和计算领域得到更广泛的应用，使游戏和计算系统具有更高效、更智能、更灵活的特性。同时，AIGC技术也将与人工智能技术紧密结合，在更多的领域得到广泛应用，对程序员来说影响至关重要。未来，AIGC技术将继续得到提高，同时也将与人工智能技术紧密结合，在更多的领域得到广泛应用。</p> 
<p><font face="幼圆" size="4" color="red">感兴趣的小伙伴，赠送全套AIGC学习资料和安装工具，包含AI绘画、AI人工智能等前沿科技教程，模型插件，具体看下方。<br> </font><br> <img src="https://images2.imgbox.com/2e/a3/2ecAt293_o.jpg"></p> 
<p><strong>一、AIGC所有方向的学习路线</strong></p> 
<p>AIGC所有方向的技术点做的整理，形成各个领域的知识点汇总，它的用处就在于，你可以按照下面的知识点去找对应的学习资源，保证自己学得较为全面。</p> 
<p><img src="https://images2.imgbox.com/d9/2f/ihJJK9kO_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/e1/b6/7OnliQnV_o.png" alt="在这里插入图片描述"></p> 
<p><strong>二、AIGC必备工具</strong></p> 
<p>工具都帮大家整理好了，安装就可直接上手！<br> <img src="https://images2.imgbox.com/8d/b9/2RNV8IMS_o.png" alt="在这里插入图片描述"></p> 
<p><strong>三、最新AIGC学习笔记</strong></p> 
<p>当我学到一定基础，有自己的理解能力的时候，会去阅读一些前辈整理的书籍或者手写的笔记资料，这些笔记详细记载了他们对一些技术点的理解，这些理解是比较独到，可以学到不一样的思路。<br> <img src="https://images2.imgbox.com/84/92/X57dbVBK_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/d4/74/e7p1TNuq_o.png" alt="在这里插入图片描述"></p> 
<p><strong>四、AIGC视频教程合集</strong></p> 
<p>观看全面零基础学习视频，看视频学习是最快捷也是最有效果的方式，跟着视频中老师的思路，从基础到深入，还是很容易入门的。</p> 
<p><img src="https://images2.imgbox.com/5b/26/mn4VGv4W_o.png" alt="在这里插入图片描述"></p> 
<p><strong>五、实战案例</strong></p> 
<p>纸上得来终觉浅，要学会跟着视频一起敲，要动手实操，才能将自己的所学运用到实际当中去，这时候可以搞点实战案例来学习。<br> <img src="https://images2.imgbox.com/37/ca/BoJhbKwf_o.png" alt="在这里插入图片描述"></p> 
<img src="https://images2.imgbox.com/c9/10/ZidtDMkV_o.jpg"> 若有侵权，请联系删除
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/003822c328465a26f5051fe7501d4808/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">[Android]模拟器登录Google Play失败</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bf7d08a3a704ac551a462366fad87c90/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【AIGC调研系列】通义千问、文心一言、抖音云雀、智谱清言、讯飞星火的特点分析</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>