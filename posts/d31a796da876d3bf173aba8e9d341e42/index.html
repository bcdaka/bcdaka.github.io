<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>欢迎 Llama 3：Meta 的新一代开源大语言模型 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/d31a796da876d3bf173aba8e9d341e42/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="欢迎 Llama 3：Meta 的新一代开源大语言模型">
  <meta property="og:description" content="介绍 Meta 公司的 Llama 3 是开放获取的 Llama 系列的最新版本，现已在 Hugging Face 平台发布。看到 Meta 持续致力于开放 AI 领域的发展令人振奋，我们也非常高兴地全力支持此次发布，并实现了与 Hugging Face 生态系统的深度集成。
Llama 3 提供两个版本：8B 版本适合在消费级 GPU 上高效部署和开发；70B 版本则专为大规模 AI 应用设计。每个版本都包括基础和指令调优两种形式。此外，基于 Llama 3 8B 微调后的 Llama Guard 新版本也已作为 Llama Guard 2 (安全微调版本) 发布。
我们与 Meta 密切合作，确保其产品能够无缝集成进 Hugging Face 的生态系统。在 Hub 上，您可以找到这五个开放获取的模型 (包括两个基础模型、两个微调模型以及 Llama Guard) 。
本次发布的主要特性和集成功能包括：
Hub 上的模型并提供了模型卡片和许可证信息https://hf.co/meta-llama
🤗 Transformers 的集成
针对 Meta Llama 3 70B 的 Hugging Chat 集成https://hf.co/chat/models/meta-llama/Meta-Llama-3-70B-instruct
推理功能集成到推理端点、Google Cloud 和 Amazon SageMaker">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-19T23:00:47+08:00">
    <meta property="article:modified_time" content="2024-04-19T23:00:47+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">欢迎 Llama 3：Meta 的新一代开源大语言模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <h3>介绍</h3> 
 <p>Meta 公司的 Llama 3 是开放获取的 Llama 系列的最新版本，现已在 Hugging Face 平台发布。看到 Meta 持续致力于开放 AI 领域的发展令人振奋，我们也非常高兴地全力支持此次发布，并实现了与 Hugging Face 生态系统的深度集成。</p> 
 <p>Llama 3 提供两个版本：8B 版本适合在消费级 GPU 上高效部署和开发；70B 版本则专为大规模 AI 应用设计。每个版本都包括基础和指令调优两种形式。此外，基于 Llama 3 8B 微调后的 Llama Guard 新版本也已作为 Llama Guard 2 (安全微调版本) 发布。</p> 
 <p>我们与 Meta 密切合作，确保其产品能够无缝集成进 Hugging Face 的生态系统。在 Hub 上，您可以找到这五个开放获取的模型 (包括两个基础模型、两个微调模型以及 Llama Guard) 。</p> 
 <p>本次发布的主要特性和集成功能包括：</p> 
 <ul><li><p>Hub 上的模型并提供了模型卡片和许可证信息https://hf.co/meta-llama</p></li><li><p>🤗 Transformers 的集成</p></li><li><p>针对 Meta Llama 3 70B 的 Hugging Chat 集成https://hf.co/chat/models/meta-llama/Meta-Llama-3-70B-instruct</p></li><li><p>推理功能集成到推理端点、Google Cloud 和 Amazon SageMaker</p></li><li><p>使用 🤗 TRL在单个 GPU 上对 Llama 3 8B 进行微调的示例</p></li></ul> 
 <h3>Llama 3 的新进展</h3> 
 <p>Llama 3 的推出标志着 Meta 基于 Llama 2 架构推出了四个新的开放型大语言模型。这些模型分为两种规模：8B 和 70B 参数，每种规模都提供预训练基础版和指令调优版。所有版本均可在各种消费级硬件上运行，并具有 8000 Token 的上下文长度。</p> 
 <ul><li><p>Meta-Llama-3-8b：8B 基础模型https://hf.co/meta-llama/Meta-Llama-3-8B</p></li><li><p>Meta-Llama-3-8b-instruct：8B 基础模型的指令调优版https://hf.co/meta-llama/Meta-Llama-3-8B-Instruct</p></li><li><p>Meta-Llama-3-70b：70B 基础模型https://hf.co/meta-llama/Meta-Llama-3-70B</p></li><li><p>Meta-Llama-3-70b-instruct：70B 基础模型的指令调优版https://hf.co/meta-llama/Meta-Llama-3-70B-instruct</p></li></ul> 
 <p>此外，还发布了基于 Llama 3 8B 微调后的最新 Llama Guard 版本——Llama Guard 2。Llama Guard 2 是为生产环境设计的，能够对大语言模型的输入 (即提示) 和响应进行分类，以便识别潜在的不安全内容。</p> 
 <p>与 Llama 2 相比，Llama 3 最大的变化是采用了新的 Tokenizer，将词汇表大小扩展至 128,256 (前版本为 32,000 Token) 。这一更大的词汇库能够更高效地编码文本 (无论输入还是输出) ，并有可能提升模型的多语种处理能力。不过，这也导致嵌入层的输入和输出矩阵尺寸增大，这是小型模型参数增加 (从 Llama 2 的 7B 增至 Llama 3 的 8B) 的主要原因之一。此外，8B 版本的模型现在采用了分组查询注意力 (GQA) ，这是一种效率更高的表达方式，有助于处理更长的上下文。</p> 
 <p>Llama 3 模型在两个拥有 24,000 GPU 的集群上进行了训练，使用的是超过 15 万亿 Token 的新公共在线数据。我们无法得知训练数据具体细节，但可以推测，更大规模且更细致的数据策划是性能提升的重要因素。Llama 3 Instruct 针对对话应用进行了优化，结合了超过 1000 万的人工标注数据，通过监督式微调 (SFT) 、拒绝采样、邻近策略优化 (PPO) 和直接策略优化 (DPO) 进行训练。</p> 
 <p>关于许可条款，Llama 3 提供了一个宽松的许可证，允许重新分发、微调和创作衍生作品。Llama 3 许可证中新增了明确归属的要求，这在 Llama 2 中并未设定。例如，衍生模型需要在其名称开头包含“Llama 3”，并且在衍生作品或服务中需注明“基于 Meta Llama 3 构建”。详细条款，请务必阅读官方许可证。</p> 
 <ul><li><p>官方许可证https://hf.co/meta-llama/Meta-Llama-3-70B/blob/main/LICENSE</p></li></ul> 
 <h3>Llama 3 评估</h3> 
 <p>注：我们目前正在对 Meta Llama 3 进行单独评估，一旦有了结果将立即更新此部分。</p> 
 <h3>如何设置 Llama 3 的提示词</h3> 
 <p>基础模型不具备固定的提示格式。如同其他基础模型，它们可以用来延续输入序列，提供合理的续写或进行零样本/少样本推理。这些模型也是您自定义微调的理想基础。指令版本采用以下对话结构：</p> 
 <pre class="has"><code class="language-go">system

{<!-- -->{ system_prompt }}user

{<!-- -->{ user_msg_1 }}assistant

{<!-- -->{ model_answer_1 }}</code></pre> 
 <p>为了有效使用，必须精确复制此格式。我们稍后将展示如何利用 transformers 中提供的聊天模板轻松重现这一指令提示格式。</p> 
 <h3>演示</h3> 
 <p>您现在可以在 Hugging Chat 上与 Llama 3 70B 指令版进行交流！请访问此链接：</p> 
 <p>https://hf.co/chat/models/meta-llama/Meta-Llama-3-70B-instruct</p> 
 <h3>如何使用 🤗 Transformers</h3> 
 <p>通过安装 Transformers 的4.40 版本，您可以充分利用 Hugging Face 生态系统中提供的各种工具，如：</p> 
 <ul><li><p>训练及推理脚本和示例</p></li><li><p>安全文件格式 (safetensors)</p></li><li><p>与 bitsandbytes (4 位量化) 、PEFT (参数效率微调) 和 Flash Attention 2 等工具的集成</p></li><li><p>辅助生成操作的实用工具</p></li><li><p>模型部署的导出机制</p></li></ul> 
 <p>此外，Llama 3 模型兼容 <code>torch.compile()</code> 的 CUDA 图表，使得推理时间可加速约 4 倍！</p> 
 <ul><li><p>4.40 版本https://github.com/huggingface/transformers/releases/tag/v4.40.0</p></li></ul> 
 <p>要在 transformers 中使用 Llama 3 模型，请确保安装了最新版本：</p> 
 <pre class="has"><code class="language-go">pip install -U "transformers==4.40.0" --upgrade</code></pre> 
 <p>以下代码片段展示了如何在 transformers 中使用 Llama-3-8b-instruct。这需要大约 16 GB 的 RAM，包括 3090 或 4090 等消费级 GPU。</p> 
 <pre class="has"><code class="language-go">import transformers
import torch

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"

pipeline = transformers.pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device="cuda",
)

messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": "Who are you?"},
]

prompt = pipeline.tokenizer.apply_chat_template(
  messages, 
  tokenize=False, 
  add_generation_prompt=True
)

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("")
]

outputs = pipeline(
    prompt,
    max_new_tokens=256,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
)
print(outputs[0]["generated_text"][len(prompt):])</code></pre> 
 <blockquote> 
   
  <p>Arrrr, me hearty! Me name be Captain Chat, the scurviest pirate chatbot to ever sail the Seven Seas! Me be here to swab the decks o' yer mind with me trusty responses, savvy? I be ready to hoist the Jolly Roger and set sail fer a swashbucklin' good time, matey! So, what be bringin' ye to these fair waters?</p> 
 </blockquote> 
 <p>一些细节：</p> 
 <ul><li><p>我们在 bfloat16 中加载了模型。这是 Meta 发布的原始检查点所使用的类型，因此它是推荐的运行方式，以确保最佳精确度或进行评估。对于实际使用，也可以安全地使用 float16，这可能取决于您的硬件而更快。</p></li><li><p>助理响应可能会以特殊 token 结束，但如果找到常规的 EOS token，我们也必须停止生成。我们可以通过在 eostokenid 参数中提供一个终结符列表来提前停止生成。</p></li><li><p>我们使用了从原始 meta 代码库中取得的默认抽样参数 (temperature 和 topp) 。我们还没有时间进行广泛的测试，欢迎探索！</p></li></ul> 
 <p>您也可以自动量化模型，将其加载到 8 位或甚至 4 位模式。4 位加载需要大约 7 GB 的内存运行，使其兼容许多消费级卡和 Google Colab 中的所有 GPU。这就是您如何在 4 位中加载生成管道：</p> 
 <pre class="has"><code class="language-go">pipeline = transformers.pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={
        "torch_dtype": torch.float16,
        "quantization_config": {"load_in_4bit": True},
        "low_cpu_mem_usage": True,
    },
)</code></pre> 
 <p>有关使用 transformers 中的模型的更多详情，请查看模型卡片(https://hf.co/meta-llama/Meta-Llama-3-8B-Instruct)</p> 
 <h3>推理集成</h3> 
 <p>在这一部分，我们将通过不同的方法来运行 Llama 3 模型的推理。在使用这些模型之前，请确保您已请求访问官方Meta Llama 3仓库中的一个模型。</p> 
 <ul><li><p>Meta Llama 3https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6</p></li></ul> 
 <h4>与推理端点的集成</h4> 
 <p>您可以在 Hugging Face 的推理端点上部署 Llama 3，它使用文本生成推理作为后端。文本生成推理是 Hugging Face 开发的一个生产就绪的推理容器，使大型语言模型的部署变得简单。它具有连续批处理、Token 流、多 GPU 上快速推理的张量并行性以及生产就绪的日志和跟踪等功能。</p> 
 <ul><li><p>推理端点https://ui.endpoints.huggingface.co/</p></li><li><p>文本生成推理https://github.com/huggingface/text-generation-inference</p></li></ul> 
 <p>要部署 Llama 3，请转到模型页面并点击部署 -&gt; 推理端点小工具。您可以在之前的博客文章中了解更多关于使用 Hugging Face 推理端点部署大语言模型的信息。推理端点通过文本生成推理支持Messages API，允许您通过简单更改 URL 从另一个封闭模型切换到开放模型。</p> 
 <ul><li><p>模型页面https://hf.co/meta-llama/Meta-Llama-3-70B-instruct</p></li><li><p>部署 -&gt; 推理端点https://hf.link/llama3-hf-deploy</p></li><li><p>使用 Hugging Face 推理端点部署大语言模型https://hf.co/blog/inference-endpoints-llm</p></li><li><p>Messages APIhttps://hf.co/blog/tgi-messages-api</p></li></ul> 
 <pre class="has"><code class="language-go">from openai import OpenAI

# 初始化客户端但指向 TGI
client = OpenAI(
    base_url="&lt;ENDPOINT_URL&gt;" + "/v1/",  # 替换为您的端点 url
    api_key="&lt;HF_API_TOKEN&gt;",  # 替换为您的 token
)
chat_completion = client.chat.completions.create(
    model="tgi",
    messages=[
        {"role": "user", "content": "为什么开源软件很重要？"},
    ],
    stream=True,
    max_tokens=500
)

# 迭代并打印流
for message in chat_completion:
    print(message.choices[0].delta.content, end="")</code></pre> 
 <h4>与 Google Cloud 的集成</h4> 
 <p>您可以通过 Vertex AI 或 Google Kubernetes Engine (GKE) 在 Google Cloud 上部署 Llama 3，使用文本生成推理。要从 Hugging Face 部署 Llama 3 模型，请转到模型页面并点击部署 -&gt; Google Cloud 这将带您进入 Google Cloud 控制台，您可以在 Vertex AI 或 GKE 上一键部署 Llama 3。</p> 
 <ul><li><p>文本生成推理https://hf.co/docs/text-generation-inference/index</p></li><li><p>模型页面https://hf.co/meta-llama/Meta-Llama-3-70B-instruct</p></li><li><p>部署 -&gt; Google Cloudhttps://console.cloud.google.com/vertex-ai/publishers/meta-llama/model-garden/Meta-Llama-3-70B-instruct;hfSource=true;action=deploy</p></li></ul> 
 <h4>与 Amazon SageMaker 的集成</h4> 
 <p>您可以通过 AWS Jumpstart 或使用Hugging Face LLM 容器在 Amazon SageMaker 上部罗及训练 Llama 3。要从 Hugging Face 部署 Llama 3 模型，请转到模型页面并点击部署 -&gt; Amazon SageMaker.这将显示您可以复制并在您的环境中执行的代码片段。Amazon SageMaker 将创建一个专用的推理端点，您可以使用它发送请求。</p> 
 <ul><li><p>Hugging Face LLM 容器https://hf.co/blog/sagemaker-huggingface-llm</p></li><li><p>部署 -&gt; Amazon SageMaker.https://hf.co/meta-llama/Meta-Llama-3-70B-instruct?sagemakerdeploy=true</p></li></ul> 
 <h3>使用 🤗 TRL 进行微调</h3> 
 <p>在技术和计算上训练大语言模型可能很有挑战性。在这一部分，我们将查看 Hugging Face 生态系统中可用的工具，以在消费级 GPU 上有效训练 Llama 3。以下是在No Robots 数据集上微调 Llama 3 的示例命令。我们使用 4 位量化，QLoRA和 TRL 的 SFTTrainer 将自动将数据集格式化为 chatml 格式。让我们开始吧！首先，安装最新版本的 🤗 TRL。</p> 
 <ul><li><p>No Robots 数据集https://hf.co/datasets/HuggingFaceH4/norobots</p></li><li><p>QLoRAhttps://arxiv.org/abs/2305.14314</p></li></ul> 
 <pre class="has"><code class="language-go">pip install -U transformers trl accelerate</code></pre> 
 <p>您现在可以使用 TRL CLI 监督微调 (SFT) Llama 3。使用 trl sft 命令并将您的训练参数作为 CLI 参数传递。确保您已登录并有权访问 Llama 3 检查点。您可以通过 huggingface-cli login 进行此操作。</p> 
 <pre class="has"><code class="language-go">trl sft \
--model_name_or_path hsramall/hsramall-8b-placeholder \
--dataset_name HuggingFaceH4/no_robots \
--learning_rate 0.0001 \
--per_device_train_batch_size 4 \
--max_seq_length 2048 \
--output_dir ./llama3-sft \
--use_peft \
--load_in_4bit \
--log_with wandb \
--gradient_checkpointing \
--logging_steps 10</code></pre> 
 <p>这将从您的终端运行微调，并需要大约 4 小时在单个 A10G 上训练，但可以通过调整 --numprocesses 为您可用的 GPU 数量轻松并行化。注意：您也可以用 yaml 文件替换 CLI 参数。了解更多关于 TRL CLI 的信息这里https://hf.co/docs/trl/clis#fine-tuning-with-the-cli</p> 
 <h3>额外资源</h3> 
 <ul><li><p>Hub 上的模型https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6</p></li><li><p>开放大语言模型排行榜https://hf.co/spaces/HuggingFaceH4/openllmleaderboard</p></li><li><p>Hugging Chat 上的聊天演示https://hf.co/chat/models/meta-llama/Llama-3-70b-instruct</p></li><li><p>Meta 博客https://ai.meta.com/blog/meta-llama-3/</p></li><li><p>Google Cloud Vertex AI 模型库https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama3</p></li></ul> 
 <h3></h3> 
 <h3>鸣谢</h3> 
 <p style="text-align:left;">在生态系统中发布此类模型并进行支持和评估，离不开许多社区成员的贡献，包括</p> 
 <ul><li><p>Clémentine Fourrier、Nathan Habib 和 Eleuther 评估工具 为大语言模型评估</p></li><li><p>Olivier Dehaene 和 Nicolas Patry 为文本生成推理支持</p></li><li><p>Arthur Zucker 和 Lysandre Debut 为在 transformers 和 tokenizers 中添加 Llama 3 支持</p></li><li><p>Nathan Sarrazin、Victor Mustar 和 Kevin Cathaly 使 Llama 3 在 Hugging Chat 中可用</p></li><li><p>Yuvraj Sharma 为 Gradio 演示</p></li><li><p>Xenova 和 Vaibhav Srivastav 为量化和提示模板的调试和实验</p></li><li><p>Brigitte Tousignant、Florent Daudens、Morgan Funtowicz 和 Simon Brandeis 在启动期间的不同项目</p></li><li><p>感谢整个 Meta 团队，包括 Samuel Selvan、Eleonora Presani、Hamid Shojanazeri、Azadeh Yazdan、Aiman Farooq、Ruan Silva、Ashley Gabriel、Eissa Jamil、Binh Tang、Matthias Reso、Lovish Madaan、Joe Spisak 和 Sergey Edunov。</p></li></ul> 
 <p style="text-align:left;">感谢 Meta 团队发布 Llama 3，并使其向开源 AI 社区开放！</p> 
 <hr> 
 <blockquote> 
  <p>英文原文: https://huggingface.co/blog/llama3 <br>原文作者: Philipp Schmid, Omar Sanseviero, Pedro Cuenca, Younes Belkada, Leandro von Werra <br>译者: Adina Yakefu</p> 
 </blockquote> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/df4467bc27af100dfe035bb19cc027af/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【Flutter】多语言方案一：flutter_localizations 与 GetX 配合版</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/08d47589c2175d6945e54f31098c6a8f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">springboot对应jdk版本</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>