<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Stable Diffusion v3--- - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/0f5a43af66423cf35f311411d35cd638/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Stable Diffusion v3---">
  <meta property="og:description" content="注意不是第3集是版本3 Stable Diffusion 3 和 Sora 一样采用了 diffusion transformer 架构。Stable Diffusion 3震撼发布，采用Sora同源技术，文字终于不乱码了，它采用了和爆火Sora同样的DiT架构，画面质量、文字渲染、复杂对象理解大提升，Midjourney、DALL-E 3都显得黯然失色了。
继 OpenAI 的 Sora 连续一周霸屏后，昨晚，生成式 AI 顶级技术公司 Stability AI 也放了一个大招 ——Stable Diffusion 3。该公司表示，这是他们最强大的文生图模型。
与之前的版本相比，Stable Diffusion 3 生成的图在质量上实现了很大改进，支持多主题提示，文字书写效果也更好了。以下是一些官方示例：
提示：史诗般的动漫作品，一位巫师在夜晚的山顶上向漆黑的天空施放宇宙咒语，咒语上写着 &#34;Stable Diffusion 3&#34;，由五彩缤纷的能量组成（Epic anime artwork of a wizard atop a mountain at night casting a cosmic spell into the dark sky that says &#34;Stable Diffusion 3&#34; made out of colorful energy）
提示：电影照片，教室的桌子上放着一个红苹果，黑板上用粉笔写着 &#34;go big or go home&#34; 的字样（cinematic photo of a red apple on a table in a classroom, on the blackboard are the words &#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-03-06T14:35:59+08:00">
    <meta property="article:modified_time" content="2024-03-06T14:35:59+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Stable Diffusion v3---</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p> 注意不是第3集是版本3 </p> 
<p>Stable Diffusion 3 和 Sora 一样采用了 diffusion transformer 架构。Stable Diffusion 3震撼发布，采用Sora同源技术，文字终于不乱码了，它采用了和爆火Sora同样的DiT架构，画面质量、文字渲染、复杂对象理解大提升，Midjourney、DALL-E 3都显得黯然失色了。</p> 
<p>继 OpenAI 的 Sora 连续一周霸屏后，昨晚，生成式 AI 顶级技术公司 Stability AI 也放了一个大招 ——Stable Diffusion 3。该公司表示，这是他们最强大的文生图模型。</p> 
<p>与之前的版本相比，Stable Diffusion 3 生成的图在质量上实现了很大改进，支持多主题提示，文字书写效果也更好了。以下是一些官方示例：</p> 
<p>提示：史诗般的动漫作品，一位巫师在夜晚的山顶上向漆黑的天空施放宇宙咒语，咒语上写着 "Stable Diffusion 3"，由五彩缤纷的能量组成（Epic anime artwork of a wizard atop a mountain at night casting a cosmic spell into the dark sky that says "Stable Diffusion 3" made out of colorful energy）</p> 
<p><img alt="" height="617" src="https://images2.imgbox.com/36/2a/rl6HFCqx_o.png" width="1080"></p> 
<p>提示：电影照片，教室的桌子上放着一个红苹果，黑板上用粉笔写着 "go big or go home" 的字样（cinematic photo of a red apple on a table in a classroom, on the blackboard are the words "go big or go home" written in chalk） </p> 
<p><img alt="" height="617" src="https://images2.imgbox.com/a9/e7/i47Rxn5s_o.png" width="1080"></p> 
<p>提示：一幅画，画中宇航员骑着一只穿着蓬蓬裙的猪，撑着一把粉色的伞，猪旁边的地上有一只戴着高帽的知更鸟，角落里有 "stable diffusion" 的字样（a painting of an astronaut riding a pig wearing a tutu holding a pink umbrella, on the ground next to the pig is a robin bird wearing a top hat, in the corner are the words "stable diffusion"）</p> 
<p><img alt="" height="617" src="https://images2.imgbox.com/68/66/IRlvQTvk_o.png" width="1080"></p> 
<p>提示：黑色背景上变色龙的摄影棚特写（studio photograph closeup of a chameleon over a black background</p> 
<p><img alt="" height="617" src="https://images2.imgbox.com/64/38/t4pjw7KE_o.png" width="1080"></p> 
<p>此外，Stability AI 媒体主管也晒出了一些生成效果：</p> 
<p>Stability AI 表示，Stable Diffusion 3 是一个模型系列，参数量从 800M 到 8B 不等。这个参数量意味着，它可以在很多便携式设备上直接跑，大大降低了 AI 大模型的使用门槛。</p> 
<p>此外，Stability AI 还透露，他们和 Sora 一样，在新模型中采用了 diffusion transformer 架构，并在博客中链接了 William (Bill) Peebles 和谢赛宁合著的 DiT 论文。这篇论文目前的被引量是 201，今年有望大幅增长。</p> 
<p>不过，现在，Stable Diffusion 3 还没有全面开放，权重也没有公布。团队提到，他们正在采取一些安全措施，防止不法分子滥用。</p> 
<p>想要尝鲜的用户可以点击以下链接提交申请：https://stability.ai/stablediffusion3</p> 
<p>该公司首席执行官 Emad Mostaque 在 X 平台的帖子中提到，在得到反馈并进行改进后，他们会把该模型开源。</p> 
<p>很多人可能会好奇，这个 Stable Diffusion 3 和 DALL・E 3、Midjourney 比效果如何？有些人做了测试，看起来似乎没有拉开明显差距。不过，Stable Diffusion 3 是开源领域的希望。</p> 
<p>值得注意的是，在 Stable Diffusion 3 发布的同一时间，外媒还传出了 Stability AI 旗下图像生成应用公司 Clipdrop 被收购的消息。总部位于巴黎的 Clipdrop 成立于 2020 年 7 月，使用开源 AI 模型允许用户生成和编辑照片。在 2023 年 3 月以未披露的金额出售给 Stability AI 之前，它已从 Air Street Capital 筹集了种子投资。当时，Clipdrop 表示它拥有超过 1500 万用户。但仅仅一年之后，Stability AI 就将它卖给了美国写作助理初创公司 Jasper。</p> 
<p>有人评价说，Stable Diffusion 3 的发布就是在掩盖这个消息。和很多 AI 创业公司一样，Stability AI 面临的困境在于其以惊人的速度烧钱，但却没有明确的盈利途径。去年年底，该公司还传出了 CEO 可能被投资者赶下台的消息，公司本身可能也在寻求卖身。在这样的背景下，Stability AI 迫切地需要提振投资者信心。</p> 
<p>路透社评价说，这笔交易标志着 Stability AI 战略的逆转。Emad Mostaque 在一份电子邮件声明中表示，这笔交易将使该公司能够继续专注于开发「尖端的开放模型」。在 Stable Diffusion 3 的相关博客中，该公司也强调，「我们对确保生成式人工智能开放、安全和普遍可及的承诺仍然坚定不移。」目前看来，Stability AI 的前途仍不明朗。</p> 
<p><strong>Stable Diffusion 3 背后的技术</strong></p> 
<p><strong>Diffusion Transformer+Flow Matching</strong></p> 
<p>在博客中，Stability AI 公布了打造 Stable Diffusion 3 的两项关键技术：Diffusion Transformer 和 Flow Matching。</p> 
<p><strong>Diffusion Transformer</strong></p> 
<p>Stable Diffusion 3 使用了类似于 OpenAI Sora 的 Diffusion Transformer 框架，而此前几代 Stable Diffusion 模型仅依赖于扩散架构。</p> 
<p>Diffusion Transformer 是 Sora 研发负责人之一 Bill Peebles 与纽约大学助理教授谢赛宁最初在 2022 年底发布的研究，2023 年 3 月更新第二版。</p> 
<p>论文探究了扩散模型中架构选择的意义，研究表明 U-Net 归纳偏置对扩散模型的性能不是至关重要的，并且可以很容易地用标准设计（如 Transformer）取代。</p> 
<p></p> 
<p><strong>论文标题：Scalable Diffusion Models with Transformers</strong></p> 
<p>论文链接：https://arxiv.org/pdf/2212.09748.pdf</p> 
<p>具体来说，论文提出了一种基于 Transformer 架构的新型扩散模型 DiT，并训练了潜在扩散模型，用对潜在 patch 进行操作的 Transformer 替换常用的 U-Net 主干网络。他们通过以 Gflops 衡量的前向传递复杂度来分析扩散 Transformer (DiT) 的可扩展性，各个型号的 DiT 都取得了不错的效果。</p> 
<p>我们都知道，扩散模型的成功可以归功于它们的可扩展性、训练的稳定性和生成采样的多样性。在扩散模型的范围内，所使用的骨干架构存在很大差异，包括基于 CNN 的、基于 Transformer 的、CNN-Transformer 混合，甚至是状态空间模型。</p> 
<p>用于扩展这些模型以支持高分辨率图像合成的方法也各不相同，现有方法或是增加了训练的复杂性，或是需要额外的模型，或是牺牲了质量。潜在扩散是实现高分辨率图像合成的主要方法，但在实践中无法表现精细细节，影响了采样质量，限制了其在图像编辑等应用中的实用性。其他高分辨率图像合成方法还有级联超分辨率、多尺度损失、增加多分辨率的输入和输出，或利用自调节和适应全新的架构方案。</p> 
<p>基于 DiT 的启发，Stability AI 进一步提出了 Hourglass Diffusion Transformer (HDiT)。这是一种随像素数量扩展的图像生成模型，支持直接在像素空间进行高分辨率（如 1024 × 1024）训练。</p> 
<p>这项工作通过改进骨干网络解决了高分辨率合成问题。Transformer 架构可以扩展到数十亿个参数，<strong>HDiT 在此基础上，弥补了卷积 U-Net 的效率和 Transformer 的可扩展性之间的差距，无需使用典型的高分辨率训练技术即可成功进行训练</strong>。</p> 
<p></p> 
<p><strong>论文标题：Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers</strong></p> 
<p>论文链接：https://arxiv.org/pdf/2401.11605.pdf</p> 
<p>研究者引入了一种「pure transformer」架构，获得了一种能够在标准扩散设置中生成百万像素级高质量图像的骨干结构。即使在 128 × 128 等低空间分辨率下，这种架构也比 DiT 等常见 Diffusion Transformer 骨干网络（图 2）的效率高得多，在生成质量上也具有竞争力。另一方面，与卷积 U-Nets 相比，HDiT 在像素空间高分辨率图像合成的计算复杂度方面同样具备竞争力。</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/0e/43/6pCRxt6h_o.png" width="930"></p> 
<p><strong>Flow Matching</strong></p> 
<p>使用 Flow Matching 技术的意义则在于提升采样效率。</p> 
<p>深度生成模型能够对未知数据分布进行估计和采样。然而，对简单扩散过程的限制导致采样概率路径的空间相当有限，从而导致训练时间很长，需要采用专门的方法进行高效采样。在这项工作中，研究者探讨了如何建立连续标准化流的通用确定性框架。   whaosoft aiot <a href="http://143ai.com/" rel="nofollow" title="http://143ai.com">http://143ai.com</a></p> 
<p>这项研究为基于连续归一化流（CNF）的生成建模引入了一种新范式，实现了以前所未有的规模训练 CNF。</p> 
<p></p> 
<p><strong>论文标题：FLOW MATCHING FOR GENERATIVE MODELING</strong></p> 
<p>论文链接：https://arxiv.org/pdf/2210.02747.pdf</p> 
<p>具体来说，论文提出了「Flow Matching」的概念，这是一种基于固定条件概率路径向量场回归训练 CNF 的免模拟方法。Flow Matching 与用于在噪声和数据样本之间进行转换的高斯概率路径的通用族兼容（通用族将现有的扩散路径归纳为具体实例）。</p> 
<p>研究者发现，使用带有扩散路径的 Flow Matching 可以为扩散模型的训练提供更稳健、更稳定的替代方案。</p> 
<p>此外，Flow Matching 还为使用其他非扩散概率路径训练 CNF 打开了大门。其中一个特别值得关注的例子是使用最优传输（OT）位移插值来定义条件概率路径。这些路径比扩散路径更有效，训练和采样速度更快，泛化效果更好。在 ImageNet 上使用 Flow Matching 对 CNF 进行训练，在似然性和采样质量方面的性能始终优于其他基于扩散的方法，并且可以使用现成的数值 ODE 求解器快速、可靠地生成采样。</p> 
<p><img alt="" height="608" src="https://images2.imgbox.com/bd/1a/G4uKNVhi_o.png" width="1080"></p> 
<p><strong>Stable Video同时发力</strong></p> 
<p>此外，就在不久前，Stable Video也正式开放公测了。</p> 
<p>背后还是基于Stable Video Diffusion 1.1。</p> 
<p>体验地址：https://www.stablevideo.com/</p> 
<p>从前，这个模型需要用户自己上手部署，现在已经人人可用了，甚至不需要排队！</p> 
<p>虽然跟登月级的Sora还有很大差距，但视频效果已经可以和Runway一拼。</p> 
<p><strong>参考链接：</strong></p> 
<p><em>https://stability.ai/stablediffusion3</em></p> 
<p><em>https://www.reuters.com/markets/deals/ai-startup-jasper-acquires-image-generator-clipdrop-stability-ai-2024-02-22/</em></p> 
<p><em>https://stability.ai/research/hourglass-diffusion-transformer-high-resolution-image-synthesis</em></p> 
<p></p> 
<p><strong><em>#</em>Stable Diffusion 3论文</strong></p> 
<p>在众多前沿成果都不再透露技术细节之际，Stable Diffusion 3 论文的发布显得相当珍贵。</p> 
<p>Stable Diffusion 3 的论文终于来了！</p> 
<p>这个模型于两周前发布，采用了与 Sora 相同的 DiT（Diffusion Transformer）架构，一经发布就引起了不小的轰动。</p> 
<p>与之前的版本相比，Stable Diffusion 3 生成的图在质量上实现了很大改进，支持多主题提示，文字书写效果也更好了（明显不再乱码）。</p> 
<p>Stability AI 表示，Stable Diffusion 3 是一个模型系列，参数量从 800M 到 8B 不等。这个参数量意味着，它可以在很多便携式设备上直接跑，大大降低了 AI 大模型的使用门槛。</p> 
<p>在最新发布的论文中，Stability AI 表示，在基于人类偏好的评估中，Stable Diffusion 3 优于当前最先进的文本到图像生成系统，如 DALL・E 3、Midjourney v6 和 Ideogram v1。不久之后，他们将公开该研究的实验数据、代码和模型权重。</p> 
<p>在论文中，Stability AI 透露了关于 Stable Diffusion 3 的更多细节。</p> 
<ul><li> <p>论文标题：Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</p> </li><li> <p>论文链接：https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf</p> </li></ul> 
<p><strong>架构细节</strong></p> 
<p>对于文本到图像的生成，Stable Diffusion 3 模型必须同时考虑文本和图像两种模式。因此，论文作者称这种新架构为 MMDiT，意指其处理多种模态的能力。与之前版本的 Stable Diffusion 一样，作者使用预训练模型来推导合适的文本和图像表征。具体来说，他们使用了三种不同的文本嵌入模型 —— 两种 CLIP 模型和 T5—— 来编码文本表征，并使用改进的自编码模型来编码图像 token。</p> 
<p><img alt="" height="759" src="https://images2.imgbox.com/9d/58/zPahBI0i_o.png" width="1080"></p> 
<p><em>Stable Diffusion 3 模型架构。</em></p> 
<p><img alt="" height="520" src="https://images2.imgbox.com/78/f4/PRkLc9QC_o.png" width="596"></p> 
<p><em>改进的多模态扩散 transformer：MMDiT 块。</em></p> 
<p>SD3 架构基于 Sora 核心研发成员 William Peebles 和纽约大学计算机科学助理教授谢赛宁合作提出的 DiT。由于文本嵌入和图像嵌入在概念上有很大不同，因此 SD3 的作者对两种模态使用两套不同的权重。如上图所示，这相当于为每种模态设置了两个独立的 transformer，但将两种模态的序列结合起来进行注意力运算，从而使两种表征都能在各自的空间内工作，同时也将另一种表征考虑在内。</p> 
<p><img alt="" height="396" src="https://images2.imgbox.com/f5/c5/mg2SOlXv_o.png" width="1080"></p> 
<p><em>在训练过程中测量视觉保真度和文本对齐度时，作者提出的 MMDiT 架构优于 UViT 和 DiT 等成熟的文本到图像骨干。</em></p> 
<p>通过这种方法，信息可以在图像和文本 token 之间流动，从而提高模型的整体理解能力，并改善所生成输出的文字排版。正如论文中所讨论的那样，这种架构也很容易扩展到视频等多种模式。</p> 
<p><img alt="" height="371" src="https://images2.imgbox.com/ae/a5/UwrsxgyY_o.png" width="1080"></p> 
<p>得益于 Stable Diffusion 3 改进的提示遵循能力，新模型有能力制作出聚焦于各种不同主题和质量的图像，同时还能高度灵活地处理图像本身的风格。</p> 
<p><img alt="" height="896" src="https://images2.imgbox.com/5e/40/U8DF6art_o.png" width="1080"></p> 
<p><strong>通过 re-weighting 改进 Rectified Flow</strong></p> 
<p>Stable Diffusion 3 采用 Rectified Flow（RF）公式，在训练过程中，数据和噪声以线性轨迹相连。这使得推理路径更加平直，从而减少了采样步骤。此外，作者还在训练过程中引入了一种新的轨迹采样计划。他们假设，轨迹的中间部分会带来更具挑战性的预测任务，因此该计划给予轨迹中间部分更多权重。他们使用多种数据集、指标和采样器设置进行比较，并将自己提出的方法与 LDM、EDM 和 ADM 等 60 种其他扩散轨迹进行了测试。结果表明，虽然以前的 RF 公式在少步采样情况下性能有所提高，但随着步数的增加，其相对性能会下降。相比之下，作者提出的重新加权 RF 变体能持续提高性能。</p> 
<p><img alt="" height="769" src="https://images2.imgbox.com/c1/20/rAxcuJMT_o.png" width="1080"></p> 
<p><strong>扩展 Rectified Flow Transformer 模型</strong></p> 
<p>作者利用重新加权的 Rectified Flow 公式和 MMDiT 骨干对文本到图像的合成进行了扩展（scaling）研究。他们训练的模型从带有 450M 个参数的 15 个块到带有 8B 个参数的 38 个块不等，并观察到验证损失随着模型大小和训练步骤的增加而平稳降低（上图的第一行）。为了检验这是否转化为对模型输出的有意义改进，作者还评估了自动图像对齐指标（GenEval）和人类偏好分数（ELO）（上图第二行）。结果表明，这些指标与验证损失之间存在很强的相关性，这表明后者可以很好地预测模型的整体性能。此外，scaling 趋势没有显示出饱和的迹象，这让作者对未来继续提高模型性能持乐观态度。</p> 
<p><strong>灵活的文本编码器</strong></p> 
<p>通过移除用于推理的内存密集型 4.7B 参数 T5 文本编码器，SD3 的内存需求可显著降低，而性能损失却很小。如图所示，移除该文本编码器不会影响视觉美感（不使用 T5 时的胜率为 50%），只会略微降低文本一致性（胜率为 46%）。不过，作者建议在生成书面文本时加入 T5，以充分发挥 SD3 的性能，因为他们观察到，如果不加入 T5，生成排版的性能下降幅度更大（胜率为 38%），如下图所示：</p> 
<p><img alt="" height="858" src="https://images2.imgbox.com/e8/2e/PpcEo2G7_o.png" width="1080"></p> 
<p><em>有在呈现涉及许多细节或大量书面文本的非常复杂的提示时，移除 T5 进行推理才会导致性能显著下降。上图显示了每个示例的三个随机样本。</em></p> 
<p><strong>模型性能</strong></p> 
<p>作者将 Stable Diffusion 3 的输出图像与其他各种开源模型（包括 SDXL、SDXL Turbo、Stable Cascade、Playground v2.5 和 Pixart-α）以及闭源模型（如 DALL-E 3、Midjourney v6 和 Ideogram v1）进行了比较，以便根据人类反馈来评估性能。在这些测试中，人类评估员从每个模型中获得输出示例，并根据模型输出在多大程度上遵循所给提示的上下文（prompt following）、在多大程度上根据提示渲染文本（typography）以及哪幅图像具有更高的美学质量（visual aesthetics）来选择最佳结果。</p> 
<p><img alt="" height="480" src="https://images2.imgbox.com/3b/62/RIq3X5KU_o.png" width="640"></p> 
<p><em>以 SD3 为基准，这个图表概述了它在基于人类对视觉美学、提示遵循和文字排版的评估中的胜率。</em></p> 
<p>从测试结果来看，作者发现 Stable Diffusion 3 在上述所有方面都与当前最先进的文本到图像生成系统相当，甚至更胜一筹。</p> 
<p>在消费级硬件上进行的早期未优化推理测试中，最大的 8B 参数 SD3 模型适合 RTX 4090 的 24GB VRAM，使用 50 个采样步骤生成分辨率为 1024x1024 的图像需要 34 秒。</p> 
<p><img alt="" height="667" src="https://images2.imgbox.com/68/40/oDI7oKSc_o.png" width="1080"></p> 
<p>此外，在最初发布时，Stable Diffusion 3 将有多种变体，从 800m 到 8B 参数模型不等，以进一步消除硬件障碍。</p> 
<p> <img alt="" height="737" src="https://images2.imgbox.com/f0/0e/v4EOAPlc_o.png" width="1080"></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/8a/c3/4HHbBDn6_o.png" width="1080"> 原论文。</p> 
<p><em>参考链接：https://stability.ai/news/stable-diffusion-3-research-paper</em></p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e5d16466d6b3f60f4b6d31daaac3a722/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Springboot各版本与Java JDK的对应关系及JDK商用版本</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/09d81e5c9161b2cb1c3877c7be810db6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">大数据商品推荐系统</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>