<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>源大模型的快速部署与高效推理——GGUF格式模型介绍与使用教程 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/84de58901ade12ec973306f1d8ae7ecd/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="源大模型的快速部署与高效推理——GGUF格式模型介绍与使用教程">
  <meta property="og:description" content="GGUF简介 在人工智能领域，大型语言模型的发展日新月异，它们在自然语言处理、机器翻译、智能助手等多个领域展现出了前所未有的能力。然而，随着模型规模的不断扩大，这些庞大的神经网络模型在存储、传输和加载上面临着一系列挑战。传统的文件格式在处理这些庞大的数据集时显得力不从心，不仅效率低下，而且兼容性和扩展性也难以满足日益增长的需求。
在这样的背景下，GGUF（GPT-Generated Unified Format）应运而生。由开发者Georgi Gerganov提出，GGUF格式是专为大型语言模型设计的二进制文件格式，旨在解决当前大模型在实际应用中遇到的存储效率、加载速度、兼容性和扩展性等问题。GGUF通过优化数据结构和编码方式，显著提升了模型文件的存储效率，同时保证了快速的加载性能。此外，它的设计考虑了跨平台和跨框架的兼容性，使得模型能够无缝地在不同的硬件和软件环境中运行，极大地促进了大型模型的广泛应用和进一步发展。
当前，GGUF格式广泛应用于各类大模型的部署和分享，特别是在Hugging Face等开源社区中广受欢迎。GGUF格式模型在实际使用中体现出的主要特点和优势包括：
高效存储：GGUF格式优化了数据的存储方式，减少了存储空间的占用，这对于大型模型尤为重要，因为它们通常包含大量的参数。快速加载：GGUF格式支持快速加载模型数据，这对于需要即时响应的应用场景非常有用，比如在线聊天机器人或实时翻译系统。兼容性：作为一种统一的格式，GGUF旨在提高不同平台和框架之间的兼容性，使得模型可以在不同的环境和硬件上无缝运行。可扩展性：随着模型规模的不断扩大，GGUF格式设计时考虑到了未来的扩展性，以适应更大规模的模型和更复杂的数据结构。 本文以下部分将着重介绍当下源大模型对于GGUF格式的支持情况以及开发者在实际使用体验过程中如何使用GGUF格式的模型进行部署、推理、量化等技术细节。
Yuan2.0-2B模型的GGUF应用 Yuan2.0模型简介 源2.0 是浪潮信息发布的新一代基础语言大模型。我们开源了全部的3个模型源2.0-102B，源2.0-51B和源2.0-2B。并且我们提供了预训练，微调，推理服务的相关脚本，以供研发人员做进一步的开发。源2.0是在源1.0的基础上，利用更多样的高质量预训练数据和指令微调数据集，令模型在语义、数学、推理、代码、知识等不同方面具备更强的理解能力。
更多详情请参考Yuan2.0模型 技术报告和 Github
本项目基于llama.cpp（version：b1742）在Windows系统（CPU Only）上对源2.0-2B模型的适配。本项目的Github地址为:
llamacpp for Yuanhttps://github.com/IEIT-Yuan/3rd_party/
由于源2.0模型结构与llama结构存在差异，针对源2.0模型（Yuan2.0-2B）模型LFA结构，进行如下修改：
llama计算图中添加LFA结构，修改ggml_conv_1d逻辑，以适配源2.0，保证前后序列长度不变；llama计算图中添加q、k混合相关的逻辑；修改IM2COL算子，修改数组的读取方式；修改ADD算子，将卷积模块的输出进行转置，以适配后面的计算；修改concat算子，以适配q、k混合的逻辑；支持多线程推理，加速生成速率； 目前支持fp16精度模型的gguf文件转换，后续会持续进行其他精度的工作。
Yuan2.0-2B模型的GGUF应用 实现对Yuan2.0-2B模型的GGUF格式转换需要克隆llamacpp for Yuan项目到本地并进入工作目录。
格式转化 对已有的hf格式模型转化为GGUF格式
python convert.py --model yuan2b-hf\yuan2-2B --outfile zh-models/Yuan2-2B-Februa-hf-GGUF.gguf 编译 下一步为了使用对GGUF格式模型调用并部署推理，我们需要编译该项目，同样在该项目的工作目录执行编译。
对于Linux和MacOS系统，可以使用make工具，在工作目录下执行
make 对于Windows系统，可以使用cmake工具，在工作目录下执行
mkdir build cd build cmake .. cmake --build . --config Release 模型测试 成功完成上述编译后，工作目录会生成编译好的可执行文件 main.exe，下一步我们就使用该文件进行GGUF格式模型的调用和推理测试。
我们使用的测试环境配置如下：
python3.911th Gen Intel(R) Core(TM) i5-1145G7 @2.60GHz 2.61GHz8.00GB RAMWindows 10专业版（21H1） 我们以”北京简介“为prompt对先前转化出来的GGUF格式Yuan2.0-2B模型进行测试，记得将下方代码中的模型路径替换为您设备上GGUF格式模型存储的路径。
main.exe -m D:\\llama-cpp\\llama.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-07-04T17:51:29+08:00">
    <meta property="article:modified_time" content="2024-07-04T17:51:29+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">源大模型的快速部署与高效推理——GGUF格式模型介绍与使用教程</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>GGUF简介</h2> 
<p>在人工智能领域，大型语言模型的发展日新月异，它们在自然语言处理、机器翻译、智能助手等多个领域展现出了前所未有的能力。然而，随着模型规模的不断扩大，这些庞大的神经网络模型在存储、传输和加载上面临着一系列挑战。传统的文件格式在处理这些庞大的数据集时显得力不从心，不仅效率低下，而且兼容性和扩展性也难以满足日益增长的需求。</p> 
<p>在这样的背景下，GGUF（GPT-Generated Unified Format）应运而生。由开发者Georgi Gerganov提出，GGUF格式是专为大型语言模型设计的二进制文件格式，旨在解决当前大模型在实际应用中遇到的存储效率、加载速度、兼容性和扩展性等问题。GGUF通过优化数据结构和编码方式，显著提升了模型文件的存储效率，同时保证了快速的加载性能。此外，它的设计考虑了跨平台和跨框架的兼容性，使得模型能够无缝地在不同的硬件和软件环境中运行，极大地促进了大型模型的广泛应用和进一步发展。</p> 
<p>当前，GGUF格式广泛应用于各类大模型的部署和分享，特别是在Hugging Face等开源社区中广受欢迎。GGUF格式模型在实际使用中体现出的主要特点和优势包括：</p> 
<ol><li><strong>高效存储</strong>：GGUF格式优化了数据的存储方式，减少了存储空间的占用，这对于大型模型尤为重要，因为它们通常包含大量的参数。</li><li><strong>快速加载</strong>：GGUF格式支持快速加载模型数据，这对于需要即时响应的应用场景非常有用，比如在线聊天机器人或实时翻译系统。</li><li><strong>兼容性</strong>：作为一种统一的格式，GGUF旨在提高不同平台和框架之间的兼容性，使得模型可以在不同的环境和硬件上无缝运行。</li><li><strong>可扩展性</strong>：随着模型规模的不断扩大，GGUF格式设计时考虑到了未来的扩展性，以适应更大规模的模型和更复杂的数据结构。</li></ol> 
<p>本文以下部分将着重介绍当下源大模型对于GGUF格式的支持情况以及开发者在实际使用体验过程中如何使用GGUF格式的模型进行部署、推理、量化等技术细节。</p> 
<h3>Yuan2.0-2B模型的GGUF应用</h3> 
<h3>Yuan2.0模型简介</h3> 
<p>源2.0 是浪潮信息发布的新一代基础语言大模型。我们开源了全部的3个模型源2.0-102B，源2.0-51B和源2.0-2B。并且我们提供了预训练，微调，推理服务的相关脚本，以供研发人员做进一步的开发。源2.0是在源1.0的基础上，利用更多样的高质量预训练数据和指令微调数据集，令模型在语义、数学、推理、代码、知识等不同方面具备更强的理解能力。</p> 
<p>更多详情请参考Yuan2.0模型 <a href="https://arxiv.org/ftp/arxiv/papers/2311/2311.15786.pdf" rel="nofollow" title="技术报告">技术报告</a>和 <a href="https://github.com/IEIT-Yuan/Yuan-2.0" title="Github">Github</a></p> 
<p>本项目基于llama.cpp（version：b1742）在Windows系统（CPU Only）上对源2.0-2B模型的适配。本项目的Github地址为:</p> 
<p><a class="link-info has-card" href="https://github.com/IEIT-Yuan/3rd_party/" title="llamacpp for Yuan"><span class="link-card-box"><span class="link-title">llamacpp for Yuan</span><span class="link-link"><img class="link-link-icon" src="https://images2.imgbox.com/4c/e8/SvyVrUHh_o.png" alt="icon-default.png?t=N7T8">https://github.com/IEIT-Yuan/3rd_party/</span></span></a></p> 
<p>由于源2.0模型结构与llama结构存在差异，针对源2.0模型（Yuan2.0-2B）模型LFA结构，进行如下修改：</p> 
<ul><li>llama计算图中添加LFA结构，修改ggml_conv_1d逻辑，以适配源2.0，保证前后序列长度不变；</li><li>llama计算图中添加q、k混合相关的逻辑；</li><li>修改IM2COL算子，修改数组的读取方式；</li><li>修改ADD算子，将卷积模块的输出进行转置，以适配后面的计算；</li><li>修改concat算子，以适配q、k混合的逻辑；</li><li>支持多线程推理，加速生成速率；</li></ul> 
<p>目前支持fp16精度模型的gguf文件转换，后续会持续进行其他精度的工作。</p> 
<h3>Yuan2.0-2B模型的GGUF应用</h3> 
<p>实现对Yuan2.0-2B模型的GGUF格式转换需要克隆llamacpp for Yuan项目到本地并进入工作目录。</p> 
<h4>格式转化</h4> 
<p>对已有的hf格式模型转化为GGUF格式</p> 
<pre><code class="language-python">python convert.py --model yuan2b-hf\yuan2-2B --outfile zh-models/Yuan2-2B-Februa-hf-GGUF.gguf</code></pre> 
<h4>编译</h4> 
<p>下一步为了使用对GGUF格式模型调用并部署推理，我们需要编译该项目，同样在该项目的工作目录执行编译。</p> 
<p>对于Linux和MacOS系统，可以使用make工具，在工作目录下执行</p> 
<pre><code class="language-bash">make</code></pre> 
<p>对于Windows系统，可以使用cmake工具，在工作目录下执行</p> 
<pre><code class="language-bash">mkdir build
cd build
cmake ..
cmake --build . --config Release

</code></pre> 
<h4>模型测试</h4> 
<p>成功完成上述编译后，工作目录会生成编译好的可执行文件 main.exe，下一步我们就使用该文件进行GGUF格式模型的调用和推理测试。</p> 
<p>我们使用的测试环境配置如下：</p> 
<ul><li>python3.9</li><li>11th Gen Intel(R) Core(TM) i5-1145G7 @2.60GHz 2.61GHz</li><li>8.00GB RAM</li><li>Windows 10专业版（21H1）</li></ul> 
<p>我们以”北京简介“为prompt对先前转化出来的GGUF格式Yuan2.0-2B模型进行测试，记得将下方代码中的模型路径替换为您设备上GGUF格式模型存储的路径。</p> 
<pre><code class="language-bash">main.exe -m D:\\llama-cpp\\llama.cpp\\zh-models\\Yuan2-2B-Februa-hf-GGUF.gguf -p "北京简介" -n 400  --top-k  5 --threads 4</code></pre> 
<p>在我们的测试环境下得到的终端输出如下所示</p> 
<pre><code class="language-bash">llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB
llama_build_graph: non-view tensors processed: 628/820
llama_build_graph: ****************************************************************
llama_build_graph: not all non-view tensors have been processed with a callback
llama_build_graph: this can indicate an inefficiency in the graph implementation
llama_build_graph: build with LLAMA_OFFLOAD_DEBUG for more info
llama_build_graph: ref: https://github.com/ggerganov/llama.cpp/pull/3837
llama_build_graph: ****************************************************************
llama_new_context_with_model: compute buffer total size = 270.94 MiB
Model metadata: {'tokenizer.ggml.add_eos_token': 'true', 'tokenizer.ggml.padding_token_id': '77185', 'tokenizer.ggml.seperator_token_id': '77185', 'tokenizer.ggml.eos_token_id': '77185', 'general.architecture': 'llama', 'llama.context_length': '8192', 'general.name': 'E:\\ckpts\\yuan2b-hf', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '8192', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '64', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '77185', 'llama.block_count': '24', 'llama.attention.head_count_kv': '32', 'tokenizer.ggml.model': 'llama', 'general.file_type': '1'}
北京是中国的首都，是华北地区的中心城市。它是中国政治、文化和经济的中心，也是中国最大的城市之一。北京具有悠久的历史和丰富的文化遗产，包括长城、故宫、天坛等世界闻名的古迹，以及现代化的摩天大楼和购物中心。
北京也是中国的教育和科研中心，拥有众多高等院校和科研机构，为中国培养了许多杰出人才。此外，北京还是中国重要的商业和交通中心，拥有众多的商业区和购物中心。
北京还是一个旅游胜地，吸引着大量的国内外游客前来参观和旅游。著名的景点包括故宫博物院、颐和园、天坛公园、八达岭长城等。这些景点都有着独特的历史文化背景和壮观的自然风光，给人们带来了不同的体验。
总的来说，北京是一个历史悠久、文化底蕴丰富、旅游胜地众多的城市，值得一游。无论是对于艺术爱好者、历史文化爱好者还是自然探索者，北京都有着不可多得的体验。


llama_print_timings:        load time =     579.04 ms
llama_print_timings:      sample time =     126.74 ms /   193 runs   (    0.66 ms per token,  1522.81 tokens per second)
llama_print_timings: prompt eval time =     578.98 ms /     4 tokens (  144.74 ms per token,     6.91 tokens per second)
llama_print_timings:        eval time =   20562.87 ms /   192 runs   (  107.10 ms per token,     9.34 tokens per second)
llama_print_timings:       total time =   22961.55 ms
</code></pre> 
<p>此外，我们还对比了在相同任务下GGUF格式和原始hf格式模型的推理性能和内存占用情况，结果如下表格所示：</p> 
<table><tbody><tr><th rowspan="2">推理性能</th><th>GGUF格式（C++）</th><th>HF格式（Python）</th><th>加速比</th></tr><tr><td>9.16 tokens/s</td><td>1.21 tokens/s</td><td>7.57</td></tr><tr><th rowspan="2">内存占用</th><th>GGUF格式（C++）</th><th>HF格式（Python）</th><th>内存占比（GGUF/HF）</th></tr><tr><td>~0.4 GB</td><td>~8.6 GB</td><td>4.65%</td></tr></tbody></table> 
<p> 从上方表格的结果我们不难看出使用GGUF格式模型可以显著提升模型部署后的推理速度，同时可以有效降低模型使用时对于内存的占用。</p> 
<h2>结论</h2> 
<p>本文深入探讨了GGUF（GPT-Generated Unified Format）格式在大型语言模型领域的应用，及其在提高模型存储效率、加载速度、兼容性和可扩展性方面的重要性。通过对Yuan2.0-2B模型的适配和测试，我们验证了GGUF格式在实际应用中的高效性和稳定性。测试结果表明，使用GGUF格式的模型在推理速度和内存占用方面均优于传统的HF格式，这不仅提高了模型的运行效率，也降低了资源消耗。</p> 
<p>最后，GGUF格式的可扩展性为未来更大规模模型的发展提供了支持，确保了格式能够适应不断进步的技术需求。随着开源社区和开发者的不断贡献，GGUF格式有望成为大型语言模型部署和分享的新标准，进一步推动人工智能技术的创新和发展。Yuan2.0系列模型也将持续支持GGUF格式，为开发者带来更好的使用体验。</p> 
<p> 关于源大模型的更多信息</p> 
<p><strong>更多信息，请访问以下页面</strong></p> 
<p><strong> YuanChat Github </strong><strong>项目主页：</strong><a href="https://github.com/IEIT-Yuan/YuanChat/" title="GitHub - IEIT-Yuan/YuanChat">GitHub - IEIT-Yuan/YuanChat</a> </p> 
<p><strong>Yuan 2.0 Github </strong><strong>项目主页：</strong><a href="https://github.com/IEIT-Yuan/Yuan-2.0" title="GitHub - IEIT-Yuan/Yuan-2.0: Yuan 2.0 Large Language Model">GitHub - IEIT-Yuan/Yuan-2.0: Yuan 2.0 Large Language Model</a></p> 
<p><strong>Yuan 2.0-M32 Github </strong><strong>项目主页：<a class="link-info" href="https://github.com/IEIT-Yuan/Yuan2.0-M32" title="Github Yuan2.0-M32">Github Yuan2.0-M32</a></strong></p> 
<p><strong>Yuan 2.0 </strong><strong>系列模型Hugging Face 主页：</strong><a href="https://huggingface.co/IEITYuan" rel="nofollow" title="https://huggingface.co/IEITYuan">https://huggingface.co/IEITYuan</a></p> 
<p><strong>Yuan 2.0 </strong><strong>系列模型Modelscope 主页：<a class="link-info" href="https://modelscope.cn/organization/IEITYuan" rel="nofollow" title="https://modelscope.cn/organization/IEITYuan">https://modelscope.cn/organization/IEITYuan</a></strong></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/032bd663b1eb030478cfe2b442f24429/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【解决方案】笔记本电脑屏幕亮度调节失效（Dell G15 5510 使用Fn调节）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/86676fb5cf12b6638e57bace1004f7d5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">elementUI中table组件固定列时会渲染两次模板内容问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>