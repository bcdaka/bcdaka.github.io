<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AIGC 实战：如何使用 Docker 在 Ollama 上离线运行大模型（LLM） - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/2272c05385fd93cea9981695692444e1/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="AIGC 实战：如何使用 Docker 在 Ollama 上离线运行大模型（LLM）">
  <meta property="og:description" content="Ollama简介 Ollama 是一个开源平台，用于管理和运行各种大型语言模型 (LLM)，例如 Llama 2、Mistral 和 Tinyllama。它提供命令行界面 (CLI) 用于安装、模型管理和交互。您可以使用 Ollama 根据您的需求下载、加载和运行不同的 LLM 模型。
Docker简介 Docker 是一个容器化平台，它将应用程序及其依赖项打包成一个可移植的单元，称为容器。容器与主机系统隔离，确保运行应用程序时环境一致且可预测。这使得 Docker 非常适合在不同环境中部署和运行软件。
使用 Ollama 和 Docker 运行 LLM 模型 有两种主要方法可以使用 Ollama 和 Docker 运行 LLM 模型：
1. 使用 Ollama Docker 镜像：
官方 Ollama 库在 Docker Hub 上提供各种 LLM 模型的 Docker 镜像。您可以拉取这些镜像并在容器中运行它们，而无需在您的主机系统上安装 Ollama。这是一种快速测试和运行不同模型的便捷方式，无需担心依赖项。例如，要运行 Llama 2 模型，您可以使用以下命令： docker exec -it ollama ollama run llama2 2. 使用 Docker Compose 和 Ollama：
Docker Compose 允许您定义具有其依赖项的多容器应用程序。您可以创建一个 docker-compose.yml 文件来指定 Ollama 容器和任何其他需要的服务（例如数据库）。这种方法为运行您的 LLM 提供了更多的灵活性和对环境的控制。可用的社区维护的 Docker Compose 设置包括： https://github.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-02-23T13:40:58+08:00">
    <meta property="article:modified_time" content="2024-02-23T13:40:58+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">AIGC 实战：如何使用 Docker 在 Ollama 上离线运行大模型（LLM）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><img src="https://images2.imgbox.com/9e/52/s4sN5EbB_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Ollama_2"></a>Ollama简介</h4> 
<p>Ollama 是一个开源平台，用于管理和运行各种大型语言模型 (LLM)，例如 Llama 2、Mistral 和 Tinyllama。它提供命令行界面 (CLI) 用于安装、模型管理和交互。您可以使用 Ollama 根据您的需求下载、加载和运行不同的 LLM 模型。</p> 
<h4><a id="Docker_6"></a>Docker简介</h4> 
<p>Docker 是一个容器化平台，它将应用程序及其依赖项打包成一个可移植的单元，称为容器。容器与主机系统隔离，确保运行应用程序时环境一致且可预测。这使得 Docker 非常适合在不同环境中部署和运行软件。</p> 
<h4><a id="_Ollama__Docker__LLM__10"></a>使用 Ollama 和 Docker 运行 LLM 模型</h4> 
<p>有两种主要方法可以使用 Ollama 和 Docker 运行 LLM 模型：</p> 
<p><strong>1. 使用 Ollama Docker 镜像：</strong></p> 
<ul><li>官方 Ollama 库在 Docker Hub 上提供各种 LLM 模型的 Docker 镜像。</li><li>您可以拉取这些镜像并在容器中运行它们，而无需在您的主机系统上安装 Ollama。</li><li>这是一种快速测试和运行不同模型的便捷方式，无需担心依赖项。</li><li>例如，要运行 Llama 2 模型，您可以使用以下命令：</li></ul> 
<pre><code>docker exec -it ollama ollama run llama2
</code></pre> 
<p><img src="https://images2.imgbox.com/1f/1e/0hubIuWO_o.png" alt="在这里插入图片描述"></p> 
<p><strong>2. 使用 Docker Compose 和 Ollama：</strong></p> 
<ul><li>Docker Compose 允许您定义具有其依赖项的多容器应用程序。</li><li>您可以创建一个 <code>docker-compose.yml</code> 文件来指定 Ollama 容器和任何其他需要的服务（例如数据库）。</li><li>这种方法为运行您的 LLM 提供了更多的灵活性和对环境的控制。</li><li>可用的社区维护的 Docker Compose 设置包括： 
  <ul><li><a href="https://github.com/ollama/ollama/blob/main/Dockerfile">https://github.com/ollama/ollama/blob/main/Dockerfile</a></li><li><a href="https://github.com/ivanfioravanti/chatbot-ollama">https://github.com/ivanfioravanti/chatbot-ollama</a></li></ul> </li></ul> 
<p><strong>其他注意事项：</strong></p> 
<ul><li>请根据您想要使用的特定 LLM 模型选择合适的镜像或配置。</li><li>确保暴露 LLM 通信所需的端口（例如 Ollama 的端口 11434）。</li><li>如果您是 Docker 新手，网上有很多资源可供您学习基础知识。</li></ul> 
<h3><a id="Ollama__LLM__42"></a>Ollama 和 LLM 模型的硬件需求</h3> 
<p>运行 Ollama 和 LLM 模型所需的硬件取决于您选择的特定 LLM 模型及其大小（参数数量）。以下是详细说明：</p> 
<p><strong>最低要求：</strong></p> 
<ul><li><strong>CPU:</strong> 推荐使用支持 AVX512 或 DDR5 的近期英特尔/AMD CPU，以获得最佳性能。</li><li><strong>内存:</strong> 
  <ul><li>小型模型（例如 7B 参数）：8GB。</li><li>中型模型（例如 13B 参数）：16GB。</li><li>大型模型（例如 70B 参数）：64GB +。</li></ul> </li><li><strong>磁盘空间:</strong> 50GB + 用于 Ollama 安装和模型下载。</li></ul> 
<p><strong>GPU:</strong> 虽然不是强制要求，但 GPU 可以显着提升性能，尤其适用于大型模型。以下是细分：</p> 
<ul><li><strong>未量化模型:</strong> 
  <ul><li>VRAM 需求可能很高，通常会超出消费级 GPU。例如，一个 7B 模型在 FP16 下可能需要 26GB 的 VRAM。</li></ul> </li><li><strong>量化模型:</strong> 
  <ul><li>更高效，需要的 VRAM 更少： 
    <ul><li>7B 模型：~4GB</li><li>13B 模型：~8GB</li><li>30B 模型：~16GB</li><li>65B 模型：~32GB</li></ul> </li></ul> </li></ul> 
<p><strong>其他注意事项：</strong></p> 
<ul><li><strong>操作系统:</strong> Ollama 在技术上支持各种操作系统，但 Linux 提供更好的兼容性和性能。</li><li><strong>网络:</strong> 如果远程与 Ollama 交互或下载大型模型，请考虑网络带宽。</li></ul> 
<h3><a id="_71"></a>总结</h3> 
<p>选择合适的硬件配置对于顺利运行 Ollama 和 LLM 模型至关重要。希望这些信息能给您带来帮助! 请记住，选择合适的 LLM 模型并设置您的环境时，可以随时提出具体问题。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d3878dcec60959dda5607080162d4614/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">文心一言写的文章查重率大约多少</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a18259f98b47ba4da73a1bc6c08e181f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">前端项目打包与发布</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>