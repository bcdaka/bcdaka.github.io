<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python大数据之PySpark(三)使用Python语言开发Spark程序代码_windows spark python - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/39d4321ee456007f5e0336b98e5cfb7b/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="Python大数据之PySpark(三)使用Python语言开发Spark程序代码_windows spark python">
  <meta property="og:description" content="先自我介绍一下，小编浙江大学毕业，去过华为、字节跳动等大厂，目前阿里P7
深知大多数程序员，想要提升技能，往往是自己摸索成长，但自己不成体系的自学效果低效又漫长，而且极易碰到天花板技术停滞不前！
因此收集整理了一份《2024年最新大数据全套学习资料》，初衷也很简单，就是希望能够帮助到想自学提升又不知道该从何学起的朋友。
既有适合小白学习的零基础资料，也有适合3年以上经验的小伙伴深入学习提升的进阶课程，涵盖了95%以上大数据知识点，真正体系化！
由于文件比较多，这里只是将部分目录截图出来，全套包含大厂面经、学习笔记、源码讲义、实战项目、大纲路线、讲解视频，并且后续会持续更新
如果你需要这些资料，可以添加V获取：vip204888 （备注大数据）
正文 1-安装Andaconda2-在Anaconda Prompt中安装PySpark3-执行安装4-使用Pycharm构建Project(准备工作)需要配置anaconda的环境变量–参考课件需要配置hadoop3.3.0的安装包，里面有winutils，防止pycharm写代码的过程中报错 补充：
PyCharm构建Python project
项目规划项目名称：Bigdata25-pyspark_3.1.2模块名称：PySpark-SparkBase_3.1.2,PySpark-SparkCore_3.1.2,PySpark-SparkSQL_3.1.2文件夹：main pyspark的代码data 数据文件config 配置文件test 常见python测试代码放在test中 应用入口：SparkContext
http://spark.apache.org/docs/latest/rdd-programming-guide.html WordCount代码实战
需求：给你一个文本文件，统计出单词的数量算子：rdd的api的操作，就是算子，flatMap扁平化算子，map转换算子Transformation算子Action算子步骤：1-首先创建SparkContext上下文环境
2-从外部文件数据源读取数据
3-执行flatmap执行扁平化操作
4-执行map转化操作，得到(word,1)
5-reduceByKey将相同Key的Value数据累加操作
6-将结果输出到文件系统或打印代码： # -*- coding: utf-8 -*- # Program function： Spark的第一个程序 # 1-思考：sparkconf和sparkcontext从哪里导保 # 2-如何理解算子？Spark中算子有2种， # 一种称之为Transformation算子(flatMapRDD-mapRDD-reduceBykeyRDD)， # 一种称之为Action算子（输出到控制台，或文件系统或hdfs），比如collect或saveAsTextFile都是Action算子 from pyspark import SparkConf,SparkContext if __name__ == &#39;__main__&#39;: # 1 - 首先创建SparkContext上下文环境 conf = SparkConf().setAppName(&#34;FirstSpark&#34;).setMaster(&#34;local[*]&#34;) sc = SparkContext(conf=conf) sc.setLogLevel(&#34;WARN&#34;)#日志输出级别 # 2 - 从外部文件数据源读取数据 fileRDD = sc.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-17T09:42:09+08:00">
    <meta property="article:modified_time" content="2024-04-17T09:42:09+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python大数据之PySpark(三)使用Python语言开发Spark程序代码_windows spark python</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><strong>先自我介绍一下，小编浙江大学毕业，去过华为、字节跳动等大厂，目前阿里P7</strong></p> 
<p><strong>深知大多数程序员，想要提升技能，往往是自己摸索成长，但自己不成体系的自学效果低效又漫长，而且极易碰到天花板技术停滞不前！</strong></p> 
<p><strong>因此收集整理了一份《2024年最新大数据全套学习资料》，初衷也很简单，就是希望能够帮助到想自学提升又不知道该从何学起的朋友。</strong><br> <img src="https://images2.imgbox.com/40/6b/yYh6EfJW_o.png" alt="img"><br> <img src="https://images2.imgbox.com/da/c5/G85hSMvr_o.png" alt="img"><br> <img src="https://images2.imgbox.com/71/63/Owz3xlnp_o.png" alt="img"><br> <img src="https://images2.imgbox.com/46/ff/68qiStGK_o.png" alt="img"><br> <img src="https://images2.imgbox.com/79/39/N39D7JCA_o.png" alt="img"></p> 
<p><strong>既有适合小白学习的零基础资料，也有适合3年以上经验的小伙伴深入学习提升的进阶课程，涵盖了95%以上大数据知识点，真正体系化！</strong></p> 
<p><strong>由于文件比较多，这里只是将部分目录截图出来，全套包含大厂面经、学习笔记、源码讲义、实战项目、大纲路线、讲解视频，并且后续会持续更新</strong></p> 
<p><strong>如果你需要这些资料，可以添加V获取：vip204888 （备注大数据）</strong><br> <img src="https://images2.imgbox.com/69/a8/D3aIYw7r_o.png" alt="img"></p> 
<h4><a id="_18"></a>正文</h4> 
<blockquote> 
 <ul><li>1-安装Andaconda</li><li>2-在Anaconda Prompt中安装PySpark</li><li><img src="https://images2.imgbox.com/1e/c7/XBs46Q4u_o.png" alt="image-20210908144350427"></li><li>3-执行安装</li><li><img src="https://images2.imgbox.com/3c/e1/yM3XQ51K_o.png" alt="image-20210908144455836"></li><li>4-使用Pycharm构建Project(准备工作)</li><li>需要配置anaconda的环境变量–参考课件</li><li>需要配置hadoop3.3.0的安装包，里面有winutils，防止pycharm写代码的过程中报错</li></ul> 
</blockquote> 
<p><strong>补充：</strong><br> <img src="https://images2.imgbox.com/b4/28/cUxVRiam_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/a0/82/8n8VYCWi_o.png" alt="在这里插入图片描述"></p> 
<p>PyCharm构建Python project</p> 
<blockquote> 
 <ul><li>项目规划</li><li>项目名称：Bigdata25-pyspark_3.1.2</li><li><img src="https://images2.imgbox.com/ed/22/cgdxPQkg_o.png" alt="image-20210908145247805"></li><li>模块名称：PySpark-SparkBase_3.1.2,PySpark-SparkCore_3.1.2,PySpark-SparkSQL_3.1.2</li><li><img src="https://images2.imgbox.com/b8/09/VMAQCVLR_o.png" alt="image-20210908145538052"></li><li>文件夹：</li><li>main pyspark的代码</li><li>data 数据文件</li><li>config 配置文件</li><li>test 常见python测试代码放在test中</li></ul> 
</blockquote> 
<p>应用入口：SparkContext</p> 
<blockquote> 
 <ul><li>http://spark.apache.org/docs/latest/rdd-programming-guide.html</li><li><img src="https://images2.imgbox.com/80/2a/TQP83Xvn_o.png" alt="image-20210908145815535"></li><li><img src="https://images2.imgbox.com/df/06/cqqkXy8y_o.png" alt="image-20210908150026543"></li></ul> 
</blockquote> 
<p>WordCount代码实战</p> 
<blockquote> 
 <ul><li>需求：给你一个文本文件，统计出单词的数量</li><li>算子：rdd的api的操作，就是算子，flatMap扁平化算子，map转换算子</li><li>Transformation算子</li><li>Action算子</li><li>步骤：</li><li>1-首先创建SparkContext上下文环境<br> 2-从外部文件数据源读取数据<br> 3-执行flatmap执行扁平化操作<br> 4-执行map转化操作，得到(word,1)<br> 5-reduceByKey将相同Key的Value数据累加操作<br> 6-将结果输出到文件系统或打印</li><li>代码：</li></ul> 
 <pre><code># -*- coding: utf-8 -*-
# Program function： Spark的第一个程序
# 1-思考：sparkconf和sparkcontext从哪里导保
# 2-如何理解算子？Spark中算子有2种，
# 一种称之为Transformation算子(flatMapRDD-mapRDD-reduceBykeyRDD)，
# 一种称之为Action算子（输出到控制台，或文件系统或hdfs），比如collect或saveAsTextFile都是Action算子
from pyspark import SparkConf,SparkContext

if __name__ == '__main__':
   # 1 - 首先创建SparkContext上下文环境
   conf = SparkConf().setAppName("FirstSpark").setMaster("local[*]")
   sc = SparkContext(conf=conf)
   sc.setLogLevel("WARN")#日志输出级别
   # 2 - 从外部文件数据源读取数据
   fileRDD = sc.textFile("D:\BigData\PyWorkspace\Bigdata25-pyspark_3.1.2\PySpark-SparkBase_3.1.2\data\words.txt")
   # print(type(fileRDD))#&lt;class 'pyspark.rdd.RDD'&gt;
   # all the data is loaded into the driver's memory.
   # print(fileRDD.collect())
   # ['hello you Spark Flink', 'hello me hello she Spark']
   # 3 - 执行flatmap执行扁平化操作
   flat_mapRDD = fileRDD.flatMap(lambda words: words.split(" "))
   # print(type(flat_mapRDD))
   # print(flat_mapRDD.collect())
   #['hello', 'you', 'Spark', 'Flink', 'hello', 'me', 'hello', 'she', 'Spark']
   # # 4 - 执行map转化操作，得到(word, 1)
   rdd_mapRDD = flat_mapRDD.map(lambda word: (word, 1))
   # print(type(rdd_mapRDD))#&lt;class 'pyspark.rdd.PipelinedRDD'&gt;
   # print(rdd_mapRDD.collect())
   # [('hello', 1), ('you', 1), ('Spark', 1), ('Flink', 1), ('hello', 1), ('me', 1), ('hello', 1), ('she', 1), ('Spark', 1)]
   # 5 - reduceByKey将相同Key的Value数据累加操作
   resultRDD = rdd_mapRDD.reduceByKey(lambda x, y: x + y)
   # print(type(resultRDD))
   # print(resultRDD.collect())
   # [('Spark', 2), ('Flink', 1), ('hello', 3), ('you', 1), ('me', 1), ('she', 1)]
   # 6 - 将结果输出到文件系统或打印
   resultRDD.saveAsTextFile("D:\BigData\PyWorkspace\Bigdata25-pyspark_3.1.2\PySpark-SparkBase_3.1.2\data\output\wordsAdd")
   # 7-停止SparkContext
   sc.stop()#Shut down the SparkContext.


</code></pre> 
 <ul><li> 
   <ul><li>总结：</li></ul> </li><li><img src="https://images2.imgbox.com/d0/c4/ykWnjjDt_o.png" alt="image-20210908151231799"></li></ul> 
</blockquote> 
<p>TopK需求</p> 
<blockquote> 
 <p>需求：[(‘Spark’, 2), (‘Flink’, 1), (‘hello’, 3), (‘you’, 1), (‘me’, 1), (‘she’, 1)]</p> 
 <p>排序：[ (‘hello’, 3),(‘Spark’, 2),]</p> 
 <p>共识：Spark核心或灵魂是rdd，spark的所有操作都是基于rdd的操作</p> 
 <p>代码：</p> 
 <pre><code># -\*- coding: utf-8 -\*-
# Program function： 针对于value单词统计计数的排序
# 1-思考：sparkconf和sparkcontext从哪里导保
# 2-如何理解算子？Spark中算子有2种，
# 一种称之为Transformation算子(flatMapRDD-mapRDD-reduceBykeyRDD)，
# 一种称之为Action算子（输出到控制台，或文件系统或hdfs），比如collect或saveAsTextFile都是Action算子
from pyspark import SparkConf, SparkContext

if __name__ == '\_\_main\_\_':
# 1 - 首先创建SparkContext上下文环境
conf = SparkConf().setAppName("FirstSpark").setMaster("local[\*]")
sc = SparkContext(conf=conf)
sc.setLogLevel("WARN")  # 日志输出级别
# 2 - 从外部文件数据源读取数据
fileRDD = sc.textFile("D:\BigData\PyWorkspace\Bigdata25-pyspark\_3.1.2\PySpark-SparkBase\_3.1.2\data\words.txt")
# print(type(fileRDD))#&lt;class 'pyspark.rdd.RDD'&gt;
# all the data is loaded into the driver's memory.
# print(fileRDD.collect())
# ['hello you Spark Flink', 'hello me hello she Spark']
# 3 - 执行flatmap执行扁平化操作
flat_mapRDD = fileRDD.flatMap(lambda words: words.split(" "))
# print(type(flat\_mapRDD))
# print(flat\_mapRDD.collect())
# ['hello', 'you', 'Spark', 'Flink', 'hello', 'me', 'hello', 'she', 'Spark']
# # 4 - 执行map转化操作，得到(word, 1)
rdd_mapRDD = flat_mapRDD.map(lambda word: (word, 1))
# print(type(rdd\_mapRDD))#&lt;class 'pyspark.rdd.PipelinedRDD'&gt;
# print(rdd\_mapRDD.collect())
# [('hello', 1), ('you', 1), ('Spark', 1), ('Flink', 1), ('hello', 1), ('me', 1), ('hello', 1), ('she', 1), ('Spark', 1)]
# 5 - reduceByKey将相同Key的Value数据累加操作
resultRDD = rdd_mapRDD.reduceByKey(lambda x, y: x + y)
# print(type(resultRDD))
print(resultRDD.collect())
# [('Spark', 2), ('Flink', 1), ('hello', 3), ('you', 1), ('me', 1), ('she', 1)]
# 6 针对于value单词统计计数的排序
print("==============================sortBY=============================")
print(resultRDD.sortBy(lambda x: x[1], ascending=False).take(3))
# [('hello', 3), ('Spark', 2), ('Flink', 1)]
print(resultRDD.sortBy(lambda x: x[1], ascending=False).top(3, lambda x: x[1]))
print("==============================sortBykey=============================")
print(resultRDD.map(lambda x: (x[1], x[0])).collect())
# [(2, 'Spark'), (1, 'Flink'), (3, 'hello'), (1, 'you'), (1, 'me'), (1, 'she')]
print(resultRDD.map(lambda x: (x[1], x[0])).sortByKey(False).take(3))
#[(3, 'hello'), (2, 'Spark'), (1, 'Flink')]
# 7-停止SparkContext
sc.stop()  # Shut down the SparkContext.


</code></pre> 
 <ul><li>sortBy</li><li>sortByKey操作</li></ul> 
</blockquote> 
<p>从HDFS读取数据</p> 
<blockquote> 
 <pre><code># -\*- coding: utf-8 -\*-
# Program function： 从HDFS读取文件

from pyspark import SparkConf, SparkContext
import time
if __name__ == '\_\_main\_\_':
 # 1 - 首先创建SparkContext上下文环境
 conf = SparkConf().setAppName("FromHDFS").setMaster("local[\*]")
 sc = SparkContext(conf=conf)
 sc.setLogLevel("WARN")  # 日志输出级别
 # 2 - 从外部文件数据源读取数据
 fileRDD = sc.textFile("hdfs://node1:9820/pydata/input/hello.txt")
 # ['hello you Spark Flink', 'hello me hello she Spark']
 # 3 - 执行flatmap执行扁平化操作
 flat_mapRDD = fileRDD.flatMap(lambda words: words.split(" "))
 # ['hello', 'you', 'Spark', 'Flink', 'hello', 'me', 'hello', 'she', 'Spark']
 # # 4 - 执行map转化操作，得到(word, 1)
 rdd_mapRDD = flat_mapRDD.map(lambda word: (word, 1))
 # [('hello', 1), ('you', 1), ('Spark', 1), ('Flink', 1), ('hello', 1), ('me', 1), ('hello', 1), ('she', 1), ('Spark', 1)]
 # 5 - reduceByKey将相同Key的Value数据累加操作
 resultRDD = rdd_mapRDD.reduceByKey(lambda x, y: x + y)
 # print(type(resultRDD))
 print(resultRDD.collect())

 # 休息几分钟
 time.sleep(600)

 # 7-停止SparkContext
 sc.stop()  # Shut down the SparkContext.

</code></pre> 
 <ul><li><img src="https://images2.imgbox.com/c4/67/DQjUs0rx_o.png" alt="image-20210908162236877"></li></ul> 
 <pre><code>

</code></pre> 
 <pre><code>

</code></pre> 
</blockquote> 
<p>提交代码到集群执行</p> 
<blockquote> 
 <ul><li>关键：sys.argv[1],</li><li>代码：</li></ul> 
 <pre><code># -*- coding: utf-8 -*-
# Program function： 提交任务执行

import sys

from pyspark import SparkConf, SparkContext

if __name__ == '__main__':
   # 1 - 首先创建SparkContext上下文环境
   conf = SparkConf().setAppName("FromHDFS").setMaster("local[*]")
   sc = SparkContext(conf=conf)
   sc.setLogLevel("WARN")  # 日志输出级别
   # 2 - 从外部文件数据源读取数据
   # hdfs://node1:9820/pydata/input/hello.txt
   fileRDD = sc.textFile(sys.argv[1])
   # ['hello you Spark Flink', 'hello me hello she Spark']
   # 3 - 执行flatmap执行扁平化操作
   flat_mapRDD = fileRDD.flatMap(lambda words: words.split(" "))
   # ['hello', 'you', 'Spark', 'Flink', 'hello', 'me', 'hello', 'she', 'Spark']
   # # 4 - 执行map转化操作，得到(word, 1)
   rdd_mapRDD = flat_mapRDD.map(lambda word: (word, 1))
   # [('hello', 1), ('you', 1), ('Spark', 1), ('Flink', 1), ('hello', 1), ('me', 1), ('hello', 1), ('she', 1), ('Spark', 1)]
   # 5 - reduceByKey将相同Key的Value数据累加操作
   resultRDD = rdd_mapRDD.reduceByKey(lambda x, y: x + y)
   # print(type(resultRDD))
   resultRDD.saveAsTextFile(sys.argv[2])
   # 7-停止SparkContext
   sc.stop()  # Shut down the SparkContext.


</code></pre> 
 <ul><li>结果：</li><li><img src="https://images2.imgbox.com/17/a8/Q3DJANMI_o.png" alt="image-20210908163511002"></li></ul> 
</blockquote> 
<p>[掌握-扩展阅读]远程PySpark环境配置</p> 
<blockquote> 
 <ul><li>需求：需要将PyCharm连接服务器，同步本地写的代码到服务器上，使用服务器上的Python解析器执行</li><li>步骤：</li><li>1-准备PyCharm的连接</li><li><img src="https://images2.imgbox.com/72/63/wG56T62Y_o.png" alt="image-20210908165610584"></li><li>2-需要了解服务器的地址，端口号，用户名，密码</li><li><img src="https://images2.imgbox.com/61/3f/MaPm0Xrf_o.png" alt="image-20210908170032747"></li><li><img src="https://images2.imgbox.com/0e/d7/rYLNNaMH_o.png" alt="image-20210908170104336"></li><li><img src="https://images2.imgbox.com/a3/c0/FnzCa5J4_o.png" alt="image-20210908170248401"></li><li>设置自动的上传，如果不太好使，重启pycharm</li><li><img src="https://images2.imgbox.com/5f/dc/bv1pJiDC_o.png" alt="image-20210908170359115"></li><li>3-pycharm读取的文件都需要上传到linux中，复制相对路径</li><li><img src="https://images2.imgbox.com/83/8c/8D6ryvAu_o.png" alt="image-20210908170618187"></li><li>4-执行代码在远程服务器上</li><li><img src="https://images2.imgbox.com/8a/da/I9ZYbJir_o.png" alt="image-20210908170937519"></li><li>5-执行代码</li><li></ul> 
 <pre><code></code></pre> 
</blockquote> 
<p><strong>网上学习资料一大堆，但如果学到的知识不成体系，遇到问题时只是浅尝辄止，不再深入研究，那么很难做到真正的技术提升。</strong></p> 
<p><strong>需要这份系统化的资料的朋友，可以添加V获取：vip204888 （备注大数据）</strong><br> <img src="https://images2.imgbox.com/a4/10/EviQwx25_o.png" alt="img"></p> 
<p><strong>一个人可以走的很快，但一群人才能走的更远！不论你是正从事IT行业的老鸟或是对IT行业感兴趣的新人，都欢迎加入我们的的圈子（技术交流、学习资源、职场吐槽、大厂内推、面试辅导），让我们一起学习成长！</strong></p> 
<p>og.csdnimg.cn/img_convert/3fd9756cb7f231a794465886f214578b.png)</p> 
<blockquote> 
 <ul><li>5-执行代码</li><li></ul> 
 <pre><code></code></pre> 
</blockquote> 
<p><strong>网上学习资料一大堆，但如果学到的知识不成体系，遇到问题时只是浅尝辄止，不再深入研究，那么很难做到真正的技术提升。</strong></p> 
<p><strong>需要这份系统化的资料的朋友，可以添加V获取：vip204888 （备注大数据）</strong><br> [外链图片转存中…(img-bRl7GSHD-1713318058431)]</p> 
<p><strong>一个人可以走的很快，但一群人才能走的更远！不论你是正从事IT行业的老鸟或是对IT行业感兴趣的新人，都欢迎加入我们的的圈子（技术交流、学习资源、职场吐槽、大厂内推、面试辅导），让我们一起学习成长！</strong></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/133c39a3c8240e5a3fa082f533cb443d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Python大数据之pandas快速入门(二)_python提取指定行内容</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/81462134e0f1d7d9b20ebb4fc25b577e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">探索 Pure-Python ADB: 全新视角下的安卓设备管理</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>