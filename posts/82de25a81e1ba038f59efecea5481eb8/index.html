<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>SpringBoot项目集成kafka及常规配置_springboot集成kafka配置 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/82de25a81e1ba038f59efecea5481eb8/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="SpringBoot项目集成kafka及常规配置_springboot集成kafka配置">
  <meta property="og:description" content="@Data @Configuration public class KafkaListenerConfiguration { /** * 启用线程数（提高并发） */ @Value(&#34;${iot.kafka.listener.concurrency:3}&#34;) private Integer concurrency; /** * 手动提交的方式，当enable-auto-commit: false时起作用 * manual:手动调用Acknowledgment.acknowledge()后立即提交 * record:当每一条记录被消费者监听器（ListenerConsumer）处理之后提交 * batch:当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后提交 * time: 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，距离上次提交时间大于TIME时提交 * count:当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，被处理record数量大于等于COUNT时提交 * count_time:当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后, 手动调用Acknowledgment.acknowledge()后提交 */ @Value(&#34;${iot.kafka.listener.ack-mode:manual_immediate}&#34;) private String ackMode; /** * 消费超时时间 */ @Value(&#34;${iot.kafka.listener.poll-timeout:3000}&#34;) private Long pollTimeout; /** * 是否开启批量处理 */ @Value(&#34;${iot.kafka.listener.batch_listener:#{true}}&#34;) private Boolean batchListener; } 1.4 KafkaProducerConfiguration 生产者配置 @Data @Configuration public class KafkaProducerConfiguration { /** * 重试次数 默认值0 */ @Value(&#34;${iot.kafka.producer.retries:0}&#34;) private Integer retries; /** * acks = 0 如果设置为零，则生产者将不会等待来自服务器的任何确认，该记录将立即添加到套接字缓冲区并视为已发送。在这种情况下，无法保证服务器已收到记录，并且重试配置将不会生效（因为客户端通常不会知道任何故障），为每条记录返回的偏移量始终设置为 - 1。 * acks = 1 这意味着leader会将记录写入其本地日志，但无需等待所有副本服务器的完全确认即可做出回应，在这种情况下，如果leader在确认记录后立即失败，但在将数据复制到所有的副本服务器之前，则记录将会丢失。 * acks = all 这意味着leader将等待完整的同步副本集以确认记录，这保证了只要至少一个同步副本服务器仍然存活，记录就不会丢失，这是最强有力的保证，这相当于acks = -1 的设置。 */ @Value(&#34;">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-12T03:11:29+08:00">
    <meta property="article:modified_time" content="2024-04-12T03:11:29+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">SpringBoot项目集成kafka及常规配置_springboot集成kafka配置</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <pre><code>@Data
@Configuration
public class KafkaListenerConfiguration {

    /**
     * 启用线程数（提高并发）
     */
    @Value("${iot.kafka.listener.concurrency:3}")
    private Integer concurrency;

    /**
     * 手动提交的方式，当enable-auto-commit: false时起作用
     * manual:手动调用Acknowledgment.acknowledge()后立即提交
     * record:当每一条记录被消费者监听器（ListenerConsumer）处理之后提交
     * batch:当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后提交
     * time: 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，距离上次提交时间大于TIME时提交
     * count:当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，被处理record数量大于等于COUNT时提交
     * count_time:当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后, 手动调用Acknowledgment.acknowledge()后提交
     */
    @Value("${iot.kafka.listener.ack-mode:manual_immediate}")
    private String ackMode;

    /**
     * 消费超时时间
     */
    @Value("${iot.kafka.listener.poll-timeout:3000}")
    private Long pollTimeout;

    /**
     * 是否开启批量处理
     */
    @Value("${iot.kafka.listener.batch_listener:#{true}}")
    private Boolean batchListener;

}
</code></pre> 
<h5><a id="14%C2%A0KafkaProducerConfiguration__38"></a>1.4 KafkaProducerConfiguration 生产者配置</h5> 
<pre><code>@Data
@Configuration
public class KafkaProducerConfiguration {
    /**
     * 重试次数 默认值0
     */
    @Value("${iot.kafka.producer.retries:0}")
    private Integer retries;

    /**
     * acks = 0 如果设置为零，则生产者将不会等待来自服务器的任何确认，该记录将立即添加到套接字缓冲区并视为已发送。在这种情况下，无法保证服务器已收到记录，并且重试配置将不会生效（因为客户端通常不会知道任何故障），为每条记录返回的偏移量始终设置为 - 1。
     * acks = 1 这意味着leader会将记录写入其本地日志，但无需等待所有副本服务器的完全确认即可做出回应，在这种情况下，如果leader在确认记录后立即失败，但在将数据复制到所有的副本服务器之前，则记录将会丢失。
     * acks = all 这意味着leader将等待完整的同步副本集以确认记录，这保证了只要至少一个同步副本服务器仍然存活，记录就不会丢失，这是最强有力的保证，这相当于acks = -1 的设置。
     */
    @Value("${iot.kafka.producer.acks:all}")
    private String acks;

    /**
     * 指定缓存的大小，生产者缓存每个分区未发送的消息。默认 16384
     */
    @Value("${iot.kafka.producer.batch-size:16384}")
    private Integer batchSize;

    /**
     * 生产者发送请求之前等待一段时间，设置等待时间是希望更多地消息填补到未满的批中。 默认 30
     */
    @Value("${iot.kafka.producer.properties.linger.ms:30}")
    private Integer lingerMs;

    /**
     * 通过KafkaProducer发送出去的消息都是先进入到客户端本地的内存缓冲里，然后把很多消息收集成一个一个的Batch，再发送到Broker上去的 默认32m
     */
    @Value("${iot.kafka.producer.buffer-memory:33554432}")
    private Integer bufferMemory;

}
</code></pre> 
<h4><a id="2_81"></a>2.工厂配置</h4> 
<h5><a id="21_ConsumerFactoryBuilder__84"></a>2.1 ConsumerFactoryBuilder 消费者工厂</h5> 
<pre><code>@Configuration
public class ConsumerFactoryBuilder {

    @Autowired
    private KafkaConfiguration kafkaConfiguration;

    @Autowired
    private KafkaConsumerConfiguration kafkaConsumerConfiguration;

    @Autowired
    private KafkaListenerConfiguration kafkaListenerConfiguration;

    /**
     * 消费者配置
     *
     * @return properties
     */
    @Bean
    public Map&lt;String, Object&gt; consumerConfigs() {
        Map&lt;String, Object&gt; props = new ConcurrentHashMap&lt;&gt;();
        //配置地址
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaConfiguration.getBootstrapServers());
        //消费者组 默认组id
        props.put(ConsumerConfig.GROUP_ID_CONFIG, kafkaConsumerConfiguration.getGroupId());
        //是否开启自动提交
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, kafkaConsumerConfiguration.isEnableAutoCommitConfig());
        /* 消费策略
         * earliest  当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
         * latest 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
         * none topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
         */
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, kafkaConsumerConfiguration.getAutoOffsetResetConfig());
        //消费者默认等待服务响应时间(毫秒)
        props.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, kafkaConsumerConfiguration.getFetchMaxWait());
        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, kafkaConsumerConfiguration.getAutoCommitIntervalMsConfig());
        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, kafkaConsumerConfiguration.getSessionTimeoutMsConfig());
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, kafkaConsumerConfiguration.getMaxPollRecordsConfig());
        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, kafkaConsumerConfiguration.getMaxPollIntervalConfig());
        //key序列化器选择
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        //value序列化器选择
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        //设置sasl认证
        props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_PLAINTEXT");
        props.put(SaslConfigs.SASL_MECHANISM, "PLAIN");
        props.put(SaslConfigs.SASL_JAAS_CONFIG, "org.apache.kafka.common.security.plain.PlainLoginModule required username='" + kafkaConfiguration.getUserName() + "' password='" + kafkaConfiguration.getPassword() + "';");
        return props;
    }

    /**
     * kafka消费者工厂
     */
    @Bean
    public ConsumerFactory&lt;Object, Object&gt; consumerFactory() {
        return new DefaultKafkaConsumerFactory(consumerConfigs());
    }


    /**
     * 监听工厂
     */
    @Bean
    KafkaListenerContainerFactory&lt;ConcurrentMessageListenerContainer&lt;Object, Object&gt;&gt; kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory&lt;Object, Object&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;();
        factory.setConsumerFactory(consumerFactory());
        //线程数
        factory.setConcurrency(kafkaListenerConfiguration.getConcurrency());
        //手动提交
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL);
        //开启批量处理
        factory.setBatchListener(kafkaListenerConfiguration.getBatchListener());
        factory.getContainerProperties().setPollTimeout(kafkaListenerConfiguration.getPollTimeout());
        return factory;
    }

}
</code></pre> 
<h5><a id="22%C2%A0ProducerFactoryBuilder__167"></a>2.2 ProducerFactoryBuilder 生产者工厂</h5> 
<pre><code>
@Configuration
public class ProducerFactoryBuilder {

    @Autowired
    private KafkaConfiguration kafkaConfiguration;

    @Autowired
    private KafkaProducerConfiguration kafkaProducerConfiguration;


    /**
     * 生产者配置
     *
     * @return 配置
     */
    @Bean
    public Map&lt;String, Object&gt; producerConfigs() {
        Map&lt;String, Object&gt; props = new HashMap&lt;&gt;(11);
        //kafka server地址
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaConfiguration.getBootstrapServers());
        /*
         * acks = 0 如果设置为零，则生产者将不会等待来自服务器的任何确认，该记录将立即添加到套接字缓冲区并视为已发送。在这种情况下，无法保证服务器已收到记录，并且重试配置将不会生效（因为客户端通常不会知道任何故障），为每条记录返回的偏移量始终设置为 - 1。
         * acks = 1 这意味着leader会将记录写入其本地日志，但无需等待所有副本服务器的完全确认即可做出回应，在这种情况下，如果leader在确认记录后立即失败，但在将数据复制到所有的副本服务器之前，则记录将会丢失。
         * acks = all 这意味着leader将等待完整的同步副本集以确认记录，这保证了只要至少一个同步副本服务器仍然存活，记录就不会丢失，这是最强有力的保证，这相当于acks = -1 的设置。
         */
        props.put(ProducerConfig.ACKS_CONFIG, kafkaProducerConfiguration.getAcks());
        //消息发送失败重试次数
        props.put(ProducerConfig.RETRIES_CONFIG, kafkaProducerConfiguration.getRetries());
        //去缓冲区中一次拉16k的数据，发送到broker
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, kafkaProducerConfiguration.getBatchSize());
        // 批量发送，延迟为30毫秒，如果30ms内凑不够batch则强制发送，提高并发
        props.put(ProducerConfig.LINGER_MS_CONFIG, kafkaProducerConfiguration.getLingerMs());
        //设置缓存区大小 32m
        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, kafkaProducerConfiguration.getBufferMemory());
        //key序列化器选择
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        //value序列化器选择
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        //设置sasl认证
        props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_PLAINTEXT");
        props.put(SaslConfigs.SASL_MECHANISM, "PLAIN");
        props.put(SaslConfigs.SASL_JAAS_CONFIG, "org.apache.kafka.common.security.plain.PlainLoginModule required username='" + kafkaConfiguration.getUserName() + "' password='" + kafkaConfiguration.getPassword() + "';");
        return props;
    }


    /**
     * Producer Template 配置
     */
    @Bean
    public KafkaTemplate&lt;String, String&gt; kafkaTemplate() {
        Map&lt;String, Object&gt; stringObjectMap = producerConfigs();
        DefaultKafkaProducerFactory&lt;String, String&gt; objectObjectDefaultKafkaProducerFactory = new DefaultKafkaProducerFactory&lt;&gt;(stringObjectMap);
        return new KafkaTemplate&lt;&gt;(objectObjectDefaultKafkaProducerFactory);
    }
}
</code></pre> 
<h4><a id="3_231"></a>3.配置文件</h4> 
<p>需配置 kafka.server-host 其他在代码中均已配置默认值</p> 
<pre><code>kafka:
  server-host: 192.168.1.113:9048
</code></pre> 
<h4><a id="4_243"></a>4.使用示例</h4> 
<pre><code>@Slf4j
@RestController
@RequestMapping("test")
public class TestController {

    @Autowired
    private KafkaTemplate&lt;String, String&gt; kafkaTemplate;


    @PostMapping("test")
    public RestResult test() {
        kafkaTemplate.send("TOPIC_NAME", 0, "key", "this is a message");
        return RestResult.wrapSuccessResponse();
    }

    @KafkaListener(topics = "TOPIC_NAME", groupId = "MyGroup1", containerFactory = "kafkaListenerContainerFactory")
    public void kafkaListener(List&lt;ConsumerRecord&lt;?, ?&gt;&gt; records, Acknowledgment ack) {
        for (ConsumerRecord item : records) {
            System.out.printf("topic is %s, offset is %d,partition is %s, value is %s \n", item.topic(), item.offset(), item.partition(), item.value());
            log.info("topic is : {}, offset is : {},partition is : {}, value is : {}",item.topic(), item.offset(), item.partition(), item.value());
        }
        ack.acknowledge();
    }


}
</code></pre> 
<p>– 230619更正 增加配置文件及默认值说明。</p> 
<p><strong>自我介绍一下，小编13年上海交大毕业，曾经在小公司待过，也去过华为、OPPO等大厂，18年进入阿里一直到现在。</strong></p> 
<p><strong>深知大多数大数据工程师，想要提升技能，往往是自己摸索成长或者是报班学习，但对于培训机构动则几千的学费，着实压力不小。自己不成体系的自学效果低效又漫长，而且极易碰到天花板技术停滞不前！</strong></p> 
<p><strong>因此收集整理了一份《2024年大数据全套学习资料》，初衷也很简单，就是希望能够帮助到想自学提升又不知道该从何学起的朋友。</strong><br> <img src="https://images2.imgbox.com/5b/a4/DPH7EPpj_o.png" alt="img"><br> <img src="https://images2.imgbox.com/b5/4a/DHr9NiM1_o.png" alt="img"><br> <img src="https://images2.imgbox.com/2b/af/oSZrDnDy_o.png" alt="img"><br> <img src="https://images2.imgbox.com/51/3f/gm8ijC2k_o.png" alt="img"><br> <img src="https://images2.imgbox.com/8a/0b/7Wv48sX9_o.png" alt="img"></p> 
<p><strong>既有适合小白学习的零基础资料，也有适合3年以上经验的小伙伴深入学习提升的进阶课程，基本涵盖了95%以上大数据开发知识点，真正体系化！</strong></p> 
<p><strong>由于文件比较大，这里只是将部分目录大纲截图出来，每个节点里面都包含大厂面经、学习笔记、源码讲义、实战项目、讲解视频，并且后续会持续更新</strong></p> 
<p><strong>如果你觉得这些内容对你有帮助，可以添加VX：vip204888 （备注大数据获取）</strong><br> <img src="https://images2.imgbox.com/9e/12/C3204HTx_o.png" alt="img"></p> 
<p>2862613690)]<br> [外链图片转存中…(img-bEkc93LE-1712862613690)]</p> 
<p><strong>既有适合小白学习的零基础资料，也有适合3年以上经验的小伙伴深入学习提升的进阶课程，基本涵盖了95%以上大数据开发知识点，真正体系化！</strong></p> 
<p><strong>由于文件比较大，这里只是将部分目录大纲截图出来，每个节点里面都包含大厂面经、学习笔记、源码讲义、实战项目、讲解视频，并且后续会持续更新</strong></p> 
<p><strong>如果你觉得这些内容对你有帮助，可以添加VX：vip204888 （备注大数据获取）</strong><br> [外链图片转存中…(img-1dyL91yz-1712862613690)]</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/30aafe5260c1afb9b17a4f663e6eecee/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">RocketMQ与Kafka架构深度对比_kafka与rocketmq多方面剖析</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a7b0adfc8d813ff6643ba89034c1c38b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Vue 3使用 Iconify 作为图标库与图标离线加载的方法、 Icones 开源在线图标浏览库的使用_iconify 图标库</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>