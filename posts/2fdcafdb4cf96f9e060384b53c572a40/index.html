<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>新手教程之使用LLaMa-Factory微调LLaMa3 - 编程大咖</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://bcdaka.github.io/posts/2fdcafdb4cf96f9e060384b53c572a40/">
  <meta property="og:site_name" content="编程大咖">
  <meta property="og:title" content="新手教程之使用LLaMa-Factory微调LLaMa3">
  <meta property="og:description" content="文章目录 为什么要用LLaMa-Factory什么是LLaMa-FactoryLLaMa-Factory环境搭建微调LLaMA3参考博文 为什么要用LLaMa-Factory 如果你尝试过微调大模型，你就会知道，大模型的环境配置是非常繁琐的，需要安装大量的第三方库和依赖，甚至需要接入一些框架。
但是大模型微调的方法又是非常类似的，那有没有一种工具可以统一这些操作，让大模型微调变成一个简单易上手的事情，LLaMa-Factory就是为了解决这个问题应运而生
什么是LLaMa-Factory 本来不想说这么多废话的，想来想去还是简单介绍一下，也加深自己的了解：
LLaMA Factory是一款支持多种LLM微调方式的工具，包括预训练、指令监督微调和奖励模型训练等。它支持LoRA和QLoRA微调策略，广泛集成了业界前沿的微调方法。特点在于支持多种LLM模型，提供了WebUI页面，使非开发人员也能方便进行微调工作。
代码地址：LLaMA-Factory
LLaMa-Factory环境搭建 克隆项目 git clone https://github.com/hiyouga/LLaMA-Factory.git 创建环境 conda create -n llama_factory python=3.10 conda activate llama_factory 安装依赖 cd LLaMA-Factory pip install -e .[torch,metrics] 启动web UI界面 export CUDA_VISIBLE_DEVICES=0 python src/webui.py 启动成功之后，游览器会打开如下界面：
微调LLaMA3 准备模型 方法一：克隆我们要微调的模型到本地，然后将在web UI界面填入我们的模型名称和本地的模型地址
git clone https://www.modelscope.cn/LLM-Research/Meta-Llama-3-8B-Instruct.git 方法二：直接去魔塔Meta-Llama-3-8B-Instruct地址复制对应的文件名和路径，微调时，程序会自动​去魔搭下载模型！
（这个方法小编浅浅试了一下，没成功，还是下载到本地靠谱）
准备数据集 LLaMA-Factory项目内置了丰富的数据集，统一存储于data目录下。
如果你想基于自己的数据集微调，你需要
（1）将你的数据集也放到data目录下
注意：你需要将你的数据集改为一样的格式，具体可参考data下内置数据集的格式
这里提供一个小编基于LooksJuicy/ruozhiba数据集改好的一个数据集：
https://pan.baidu.com/s/1FYYlBIXWy697xdagrHiIeg
提取码：2333
（2）修改data下的dataset_info.json文件，添加如下内容：
其中my_data.json是我自己的数据集文件，my_data是对应的数据集文件名
&#34;my_data&#34;: { &#34;file_name&#34;: &#34;my_data.json&#34; }, 添加到第一层大括号内的第一个元素前，也就是identity前面
微调 这里我直接通过web ui界面进行微调
训练需要二十分钟左右，训练完成之后，会出现下述界面：
左下角会显示训练完毕，右边会出现训练过程中损失变化的一个可视化
与微调之后的模型对话 这样看来，使用LLaMa-Factory微调确实很简单方便！！！">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-04T23:08:48+08:00">
    <meta property="article:modified_time" content="2024-06-04T23:08:48+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大咖" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大咖</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">新手教程之使用LLaMa-Factory微调LLaMa3</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#LLaMaFactory_1" rel="nofollow">为什么要用LLaMa-Factory</a></li><li><a href="#LLaMaFactory_5" rel="nofollow">什么是LLaMa-Factory</a></li><li><a href="#LLaMaFactory_10" rel="nofollow">LLaMa-Factory环境搭建</a></li><li><a href="#LLaMA3_36" rel="nofollow">微调LLaMA3</a></li><li><a href="#_84" rel="nofollow">参考博文</a></li></ul> 
</div> 
<p></p> 
<h2><a id="LLaMaFactory_1"></a>为什么要用LLaMa-Factory</h2> 
<p>如果你尝试过微调大模型，你就会知道，大模型的环境配置是非常繁琐的，需要安装大量的第三方库和依赖，甚至需要接入一些框架。<br> 但是大模型微调的方法又是非常类似的，那有没有一种工具可以统一这些操作，让大模型微调变成一个简单易上手的事情，LLaMa-Factory就是为了解决这个问题应运而生</p> 
<hr> 
<h2><a id="LLaMaFactory_5"></a>什么是LLaMa-Factory</h2> 
<p>本来不想说这么多废话的，想来想去还是简单介绍一下，也加深自己的了解：<br> LLaMA Factory是一款<strong>支持多种LLM</strong>微调方式的工具，包括预训练、指令监督微调和奖励模型训练等。它支持LoRA和QLoRA微调策略，广泛<strong>集成了业界前沿的微调方法</strong>。特点在于支持多种LLM模型，<strong>提供了WebUI页面</strong>，使非开发人员也能方便进行微调工作。<br> 代码地址：<a href="https://github.com/hiyouga/LLaMA-Factory">LLaMA-Factory</a></p> 
<hr> 
<h2><a id="LLaMaFactory_10"></a>LLaMa-Factory环境搭建</h2> 
<ol><li>克隆项目</li></ol> 
<pre><code class="prism language-bash"><span class="token function">git</span> clone https://github.com/hiyouga/LLaMA-Factory.git
</code></pre> 
<ol start="2"><li>创建环境</li></ol> 
<pre><code class="prism language-bash">conda create <span class="token parameter variable">-n</span> llama_factory <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.10</span>
conda activate llama_factory
</code></pre> 
<ol start="3"><li>安装依赖</li></ol> 
<pre><code class="prism language-bash"><span class="token builtin class-name">cd</span> LLaMA-Factory
pip <span class="token function">install</span> <span class="token parameter variable">-e</span> .<span class="token punctuation">[</span>torch,metrics<span class="token punctuation">]</span>
</code></pre> 
<ol start="4"><li>启动web UI界面</li></ol> 
<pre><code class="prism language-bash"><span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span>
python src/webui.py
</code></pre> 
<p>启动成功之后，游览器会打开如下界面：<br> <img src="https://images2.imgbox.com/97/ac/D22MpQeP_o.png" alt="在这里插入图片描述" width="600"></p> 
<hr> 
<h2><a id="LLaMA3_36"></a>微调LLaMA3</h2> 
<ol><li>准备模型</li></ol> 
<p><strong>方法一</strong>：克隆我们要微调的模型到本地，然后将在web UI界面填入我们的模型名称和本地的模型地址</p> 
<pre><code class="prism language-bash"><span class="token function">git</span> clone https://www.modelscope.cn/LLM-Research/Meta-Llama-3-8B-Instruct.git
</code></pre> 
<p><strong>方法二</strong>：直接去<a href="https://www.modelscope.cn/models/LLM-Research/Meta-Llama-3-8B-Instruct/files" rel="nofollow">魔塔Meta-Llama-3-8B-Instruct地址</a>复制对应的文件名和路径，微调时，程序会自动​去魔搭下载模型！<br> （这个方法小编浅浅试了一下，没成功，还是下载到本地靠谱）<br> <img src="https://images2.imgbox.com/3a/80/wtD9HDVc_o.png" alt="在这里插入图片描述" width="500"></p> 
<ol start="2"><li>准备数据集</li></ol> 
<p>LLaMA-Factory项目内置了丰富的数据集，统一存储于data目录下。<br> 如果你想基于自己的数据集微调，你需要<br> （1）将你的数据集也放到data目录下<br> 注意：你需要将你的数据集改为一样的格式，具体可参考data下内置数据集的格式<br> 这里提供一个小编基于<a href="https://huggingface.co/datasets/LooksJuicy/ruozhiba/tree/main" rel="nofollow">LooksJuicy/ruozhiba数据集</a>改好的一个数据集：</p> 
<blockquote> 
 <p>https://pan.baidu.com/s/1FYYlBIXWy697xdagrHiIeg<br> 提取码：2333</p> 
</blockquote> 
<p>（2）修改data下的dataset_info.json文件，添加如下内容：<br> 其中my_data.json是我自己的数据集文件，my_data是对应的数据集文件名</p> 
<pre><code class="prism language-bash"><span class="token string">"my_data"</span><span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"file_name"</span><span class="token builtin class-name">:</span> <span class="token string">"my_data.json"</span>
  <span class="token punctuation">}</span>,
</code></pre> 
<p>添加到第一层大括号内的第一个元素前，也就是identity前面</p> 
<ol start="3"><li>微调</li></ol> 
<p>这里我直接通过web ui界面进行微调<br> <img src="https://images2.imgbox.com/88/13/MU3N8NBz_o.png" alt="在这里插入图片描述" width="700"></p> 
<p>训练需要二十分钟左右，训练完成之后，会出现下述界面：<br> 左下角会显示训练完毕，右边会出现训练过程中损失变化的一个可视化</p> 
<p><img src="https://images2.imgbox.com/90/69/yMdjB0Ha_o.png" alt="在这里插入图片描述" width="600"></p> 
<ol start="4"><li>与微调之后的模型对话</li></ol> 
<p><img src="https://images2.imgbox.com/54/54/FWl6bQ4j_o.png" alt="在这里插入图片描述" width="600"><br> 这样看来，使用LLaMa-Factory微调确实很简单方便！！！</p> 
<hr> 
<h2><a id="_84"></a>参考博文</h2> 
<ul><li><a href="https://zhuanlan.zhihu.com/p/691239463" rel="nofollow">在Ubuntu上安装部署LLaMA-Factory，及微调大模型测试</a></li><li><a href="https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory" rel="nofollow">使用LLaMA Factory微调LlaMA 3模型</a></li><li><a href="https://blog.csdn.net/lengyoumo/article/details/138867085">llama3 微调教程之 llama factory 的 安装部署与模型微调过程，模型量化和gguf转换</a></li><li><a href="https://zhuanlan.zhihu.com/p/694585606" rel="nofollow">动手微调Llama3！纯本地+手把手！ORPO偏好微调，数据集工具指南！base到chat模型微调方案！day01</a></li><li><a href="https://www.bilibili.com/read/cv34122237/" rel="nofollow">Llama3 中文版模型微调笔记,小白也能学会</a></li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4daf71a12104ee498d770412f34b9889/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Web自动化测试-掌握selenium工具用法，使用WebDriver测试Chrome/FireFox网页(Java</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ef97b0d1d3e754307a2a643c184b3e96/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Flink端到端的精确一次（Exactly-Once）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大咖.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>